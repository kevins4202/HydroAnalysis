index,text
8580,the classification of characteristics of flood events like peak volume duration and baseflow components is essential for many hydrological applications such as multivariate flood statistics the validation of rainfall runoff models and comparative hydrology in general the basis for estimations of these characteristics is formed by flood event separation it requires an indicator for the time when a flood peak occurs as well as the definition of the beginning and end of a flood event and a subdivision of the total volume into direct and baseflow components however the variable nature of runoff and the multiple processes and impacts that determine rainfall runoff relationships make a separation difficult especially an automation of it we propose a new statistics based flood event separation that was developed to analyse long series of daily discharges automatically to obtain flood events for flood statistics moreover the related flood inducing precipitation is identified allowing the estimation of the flood inducing rainfall and the runoff coefficient with an additional tool to manually check the separation results easily and quickly expert knowledge can be included without much effort the algorithm was applied to seven basins in germany covering alpine mountainous and flatland catchments with different runoff processes in a sensitivity analysis the impact of chosen parameters was evaluated the results show that the algorithm delivers reasonable results for all catchments and only needs manual adjustment for long timeslots with increasing or high baseflow it reliably separates flood events only instead of all runoff events and the estimated beginning and end of an event was shifted in mean by less than one day compared to manual separation keywords flood event separation runoff variation automation 1 introduction the flood peak is the most often considered characteristic in flood statistics and in deterministic rainfall runoff modeling e g as validation criterion for event specific validation although the peak plays a crucial role in assessing e g flood protection measurements the peak alone does not fully characterize a flood event especially for comparative hydrology blöschl et al 2013 not only the peak but also the volume the duration and the shape of the flood event have to be analysed detailed knowledge on all flood characteristics is important e g to apply a flood typology tarasova et al 2019 sikorska et al 2015 merz and blöschl 2003 fischer et al 2019 or to select criteria to evaluate deterministic flood models viglione et al 2009 according to one of the rare definitions of the term flood a flood is a period when discharges are equal or larger than a flood threshold value ozga zielinska 1989 since no unique definition of such a threshold exists it is defined by the objectives of the flood analyses e g by the beginning of inundations navigation problems or by statistically defined values the wmo unesco defined flood in the international glossary of hydrology world meteorological organization wmo 2012 as follows 1 rise usually brief in the water level of a stream or water body to a peak from which the water level recedes at a slower rate 2 relatively high flow as measured by stage height or discharge an important point when comparing event separation methods is the aim of separation basically the methods differ between runoff event separation and flood separation while the first focuses on every increase of runoff e g tarasova et al 2018 diederen et al 2019 the latter focuses on the flood events only for example by separating flood events for given flood peaks e g oppel and mewes 2020 which are characterised by a large increase of runoff see german norm din 4049 1 1992 due to the nature of such events in central europe they are expected to occur seldomly within a year although rare flood events are critical in understanding the nature of processes leading to extreme floods like the ones in 2002 and 2013 in germany and austria and form a basis for flood statistics there are only few methods that are designed especially for the separation of flood events an often used method is based on a peak over threshold approach where the events are separated that are above the chosen threshold kundzewicz 2019 mangini et al 2018 thresholds can be chosen according to the mean runoff or the smallest annual maximum fischer and schumann 2016 this method is often used in the context of flood statistics since it easily delivers the flood peak however event beginning and end are not provided directly instead often additionally baseflow separation methods are applied where it is assumed that a deviation of baseflow the direct runoff indicates a flood event oppel and mewes 2020 though they are originally designed for the separation of runoff events they are suitable in the context of floods when applied to apriori identified flood events e g yue 2000 2001 graszkiewicz et al 2011 bezak et al 2015 e g in combination with the peak over threshold pot approach or a pre defined number of events per year to only separate large peaks karahacane et al 2020 for baseflow estimation different methods exist for example nathan and mcmahon 1990 and tallaksen 1995 propose a recession curve of exponential type to define the point where the direct runoff ends assuming that this component has a higher declining rate than the baseflow blume et al 2007 proposed in their constant k method an approach which is based on the linear storage theory it was further developed by mei and anagnostou 2015 to a hybrid method of the constant k method and recursive digital filter often filter techniques are also used for automated baseflow separation e g chapman 1999 chapman and maxwell 1996 a comparison of manual techniques for baseflow separation with filter algorithms and recession analyses was done by eckhardt 2008 who concluded that the method proposed by eckhardt delivered a more hydrologically plausible estimation of baseflow than the chapman filter and straight line methods a combination of baseflow estimation and recession curve offers the toolbox provided by tang and carey 2017 they estimate the beginning and end of a runoff event by local minima derived from baseflow estimation and also calculate the recession parameters to identify unusual recession curves in order to remove outlier data su et al 2016 state that these digital filter methods for baseflow separation deliver discrepancies between estimated and theoretical baseflow especially for catchments with lower baseflow index bfi which is defined as the ratio of the baseflow to the total discharge in some applications baseflow separation techniques are combined with additional thresholds to make them suitable for runoff event separation for example tarasova et al 2018 combine linear interpolation between local minima see e g piggott et al 2005 aksoy et al 2008 2009 with event identification thresholds the arbitrary choice of parameters and thresholds is a crucial disadvantage of such techniques depending on the choices made very different flood volumes could be separated moreover the application of thresholds often leads to the effect that events in low baseflow periods or general dry periods cannot be detected since they do not exceed these thresholds yet these events may be comparable to those in wet periods concerning direct peak and volume recently application of machine learning techniques offered a new perspective on flood event separation e g thiesen et al 2019 oppel and mewes 2020 these techniques use pattern recognition to separate flood events and hence overcome the drawback of pot based methods that is the application of constant thresholds and the usage of baseflow separation yet machine learning techniques require large samples of labeled and hence apriori separated flood events as training sets these often are obtained by manual separation oppel and mewes 2020 and therefore are rather subjective in conclusion there exist three drawbacks that make most separation algorithms inappropriate for separating flood events suitable for flood statistics the reliance of baseflow separation for identification of beginning and end of flood events the requirement of large samples of already separated events as needed e g for machine learning based methods the inability of identifying flood events also during low baseflow dry conditions in this study a two step method is proposed that aims to overcome these drawbacks it is based on a statistical analysis of long time series of daily discharge series and focuses on the identification of highly variable flood events instead of runoff event separation only a few parameter choices are made that depend on the catchment size whereas different parameter sets are recommended for meso and macroscale catchments the first step consists of the flood event identification based on a moving variance approach in this step beginning end and flood peak time of maximum discharge between beginning and end are determined an additional component of this first step is the opportunity to include expert knowledge by making a posteriori check of the determined flood events with this even complex shapes of flood hydrographs like multiple peaks or pre floods can be separated as a second step the separation of the associated flood inducing precipitation event is proposed so that ultimately all characteristics like peak volume duration start and end point runoff coefficient and flood inducing amount of rainfall of a flood event can be derived moreover the baseflow is estimated with a straight line method e g chow et al 1988 during this step up to now many studies use separated flood events but do not state clearly how these flood events are derived however the quality of the results depends on the quality of input data hence it is important to make the data basis understandable and reproducible we propose a new algorithm that makes flood event separation easily applicable to large data sets easily reproducible when manual check is omitted yet still preserving the advantages of expert knowledge input if needed the inclusion of expert knowledge already has been proposed as a valuable extension by tang and carey 2017 all algorithms are available in an r package floodr free to use with the automated flood event separation methodology consistent flood event separation can be attained 2 data for the proposed flood event separation a time series of daily discharges and information about the catchment size are the only data required the discharge data may include missing values if the instantaneous peaks additionally to daily discharges are available but with lower temporal resolution e g as monthly maxima these can be used with the provided separation algorithm as real measured peak value for improving the flood statistics possible applications of the separated flood events are multivariate peak over threshold approaches or timescale based flood typing fischer et al 2019 for a full characterisation of the flood events including the flood inducing precipitation and e g the runoff coefficient a subsequent analysis of time series of daily precipitation areal precipitation has to be done here seven different basins in germany and austria are considered which vary from alpine and mountainous with rocky grounds to lowland catchments with sandy soil in total 86 gauges measuring daily discharges were available belonging to any of the seven basins fig 1 the catchment sizes vary between 3 4 km2 benzingerode ostharz and 25942 km2 passau ingling inn so that we can ensure inclusion of different catchment sizes and runoff processes in our data sets areal precipitation was estimated by thiessen polygons with a 90 km buffer around each catchment an overview of the most important catchment characteristics is given in fig 2 where it can be seen that the catchment sizes vary between 3 km2 and 26000 km2 with the largest catchment sizes present in the inn basin for this basin with its location partly in the alps the highest elevations of the sample are present the observation periods of daily discharge are quite long for all catchments with often more than 50 years captured the proportion of urbanised area derived from urban and industrial partitions from the corine dataset bossard et al 2000 is relatively small for all catchments such that fast runoff reactions due to sealed soil cannot be expected please note that especially for the large catchments anthropogenic impact can affect the data however as long as the general level or the variability of the discharge is not altered e g by the construction of a dam the methodology proposed here is still applicable applying common statistical change point tests for the mean and the variance we did not detect such shifts or change points in water level or discharge the links for the sources of the data sets are provided in the acknowledgments 3 methodology 3 1 flood event separation for long discharge timeseries due to the lack of data in hourly resolution flood event separation has to be based on daily discharges therefore this demands different assumptions compared to the highly fluctuating hourly discharges for hourly discharges the high dynamics of runoff often make a flood event separation easier since beginning and end are characterised by abrupt changes for daily discharges instead the fluctuation of discharge is smoother and times of abrupt changes are harder to detect here a moving window variance overcomes these lower dynamics of daily discharges basic rules applied for the development of our method are a flood event is a temporally limited exceedance of normal discharges discharges with low variability for every flood event beginning and end can be defined a flood event can be characterised by significantly increased dynamics of discharge especially around the peak the variance of discharges is high the sum of the increasing discharges is similar to the sum of the recession of the flood event but a potential increase of baseflow is considered two distinct variables for consideration can be derived from these rules the lag 1 1 day differences of the daily discharges qdi qi qi 1 with qd1 0 and their variance in a certain time window of length dvar n 2 meaning that dvar is an integer larger than 2 given by va r d var i v a r q i d var 1 q i va r d var 1 v a r d var d var 1 0 where i 1 n indicates the time step and n is the length of the considered time series of discharges the window length dvar is a catchment dependent parameter and is chosen according to the catchment size and the runoff dynamics in general for the catchments considered here a unique window length dvar of 3 days is chosen this choice is evaluated in the sensitivity analysis in a later stage to indicate the period of high dynamics belonging to a flood event the variance has to exceed a certain threshold an often applied range to classify the deviation of certain values in a data set from the remaining ones is the sigma threshold for the approach here a similar threshold t h var is derived such that the values of interest lay above θ 0 standard deviations σ away from the mean t h var va r d var θ var v a r d var this means we detect a significantly higher variation in discharge if the variation is larger than θ standard deviations of the average dvar day variation fig 3 the value of θ has an impact on the sensitivity of the algorithm the smaller θ the more flood events are detected hence it can be chosen based on the desired number of flood events per year for example during the time period of exceedance of the threshold with discharges q 1 q n the maximum discharge is defined as peak of the flood event q t peak the starting point of the flood event is defined as the time step before the peak for which qd is negative for the last time t start max 1 i t peak q d i 0 from this point on an increasing discharge can be observed however not every increase of runoff automatically defines the beginning of a flood event to consider the natural discharge variability that does not belong to the flood event we adjust the starting point fig 4 a with natural variability we refer to the processes not belonging to the extraordinary case of floods that are not part of the daily fluctuation more precisely as long as 1 q t start q t start 1 q t start 1 η η 0 1 we set q t start q t start 1 the parameter η provides a threshold for the relative 1 day discharge difference before beginning with this modification we ensure that the beginning of a flood is indicated by a pronounced increase of discharge of course not every flood event consists of a straight monotone increase of discharge instead short temporally limited increases of the runoff are often mixed with short recession phases this occurs due to variability of precipitation since these initial increases of discharges may contribute substantially to the flood event e g by higher antecedent soil moisture and a stepwise increased level of runoff we want to include such phenomena into the flood event as long as their contribution is significant rather than classified as an independent flood event fig 4b we handle this by adjusting the beginning by t start t start m if max 1 i γ 2 j γ 1 i j q t start i q t start j κ q t peak q t start 2 where m arg max 2 j γ 1 q t start i q t start j κ q t peak q t start 1 i γ i j and γ n 0 i e a positive integer the parameter κ controls the level of significant contribution a pre flood has to have to be part of the flood event this means if there is an increase of the magnitude of at least κ 0 1 of the total peak within γ 1 days before the flood event this pre flood is included in the flood event in fact a local maximum is searched in the range of γ 1 days before the beginning of the flood event the parameter γ can be chosen according to catchment size and climate region of the catchment to define the end of the flood event we need to find the time k t peak for which the sum of one day gradients i t start 1 k q d i becomes approximately zero this would mean that we are at the same discharge level as at the beginning of the flood event however in hydrological theory an increase of the baseflow component during a rainfall runoff event is assumed this was considered by the following definition of the end point of the direct runoff t end arg min t peak 1 t n i t start 1 t q d i 0 i t peak 1 t end ω q d i i t peak 1 t end q d i i t peak 1 t end q d i 1 δ 3 here two criteria are compared first we check if the baseflow from the beginning of the flood event is reached again which is comparable to the straight line method by chow et al 1988 second we check if the next ω n 1 time steps days after the potential end of the flood event would add less than δ 0 1 of the discharge volume of the falling limb if they would be included if either of these criteria is met the end of the flood event is found of course the end is always defined as occurring after the beginning of the flood event a full schematic overview of the algorithm is given in fig 5 there are several assumptions on the temporal variability of baseflow during floods which can be differentiated into two types nathan and mcmahon 1990 those that assume that the baseflow responds to a precipitation event concurrently with direct runoff and those that account for the effects of bank storage and assume that the baseflow recession continues after the time when direct runoff begins the shape of the baseflow hydrograph depends on the geological and geomorphological characteristics of catchments which cannot be specified for heterogeneous watersheds in an objective way in this manuscript we applied the first type of assumption on baseflow the baseflow was estimated with the straight line method between beginning and end of the flood event to avoid any inconsistencies between the separations of single flood events the slope of the straight line baseflow of each flood event was compared with the median slope of baseflow for all flood events of a watershed this comparison with the median was not found to be essential and was only useful to detect very high or low slopes of the rising baseflow where the end point of the flood would have had to be corrected manually this automatic detection can be activated in the algorithm a special case that was not addressed explicitly by the procedure described above are flood events with multiple peaks here we divided such flood events into three groups single floods with unsteady rising and or falling limbs the superposition of two flood events which can be separated and floods which are superimposed on high and long lasting baseflow following successively in a pre defined time span and cannot be separated each consecutive rain event falls on a wet catchment and causes an additional increase of direct runoff to differentiate the first two groups of floods from the third one we define a time span d dur if the flood event has a longer duration than d dur days it probably belongs to the third category using these rules an extraordinary duration of a separated flood event with multiple peaks becomes an indicator for a series of superimposed flood events the parameter d dur can be chosen according to the catchment size and the climatic conditions it can be estimated by evaluating the mean discharge and the duration of exceedance of its mean using this method we found several such series of floods in wet seasons of the year which could not be differentiated into single flood events the long duration of such flood events makes the separation between direct runoff and baseflow difficult according to wmo 2012 direct runoff is the water flow that enters a watercourse after precipitation without delay if the rainfall occurs in short time intervals and hence the runoff cannot decrease to baseflow level the total runoff may well be the result of cumulative precipitation without the existence of a clear and objective distinction between direct runoff and baseflow fig 6 b in this case the baseflow increases over several months here we decided to specify such flood events as a specific type of flood these flood events are important when considering wet soil moisture conditions the second group of floods are separable flood events here the second flood event begins before the recession of the first flood event has ended this leads to a magnification of the second flood event an example is given in fig 6a to estimate the feasibility of separating two successive floods we had to define an independence criterion criteria like this are mainly based on the differences between the heights of two consecutive local maxima the level of the local minimum between them and the time span between two consecutive maxima e g lang et al 1999 zhang et al 2016 bacchi et al 1992 proposed the following rule two subsequent flood events were considered independent if their peaks are separated by at least 20 f where f is the time to peak and the flow decreases by 80 or more between two adjacent peaks the selection of such a time span requires hourly runoff data with regard to the daily averages which are used here and the different reaction times of watersheds we avoided using any time span between consecutive peaks to define their interdependency we applied the following rules proposed by klein 2009 two consecutive local maxima of a hydrograph belong to two independent flood events if the larger peak is at least 5 times higher than the smaller one the larger peak is more than 2 5 times as large as the smallest discharge between both peaks 70 of the smaller peak is larger than the smallest discharge between two consecutive peaks this criterion ensures that both peaks are distinct enough to be treated as two consecutive flood events in this case both flood events have to be separated and the recession of the first has to be reconstructed the end of the first flood event was estimated by the smallest discharge between both peaks to reconstruct the recession we added a recession curve which was estimated from the recession of the second flood using all discharge values smaller than the lowest discharge between both peaks still discharges were kept above the estimated baseflow in fig 6a the local minimum is equal to 79 m3 s 12 03 1915 the same value can be found at 21 03 1915 from this point on the recession is reconstructed that is the value at 13 03 1915 is equal to 22 03 1915 with 71 4 m3 s then the value of 14 03 1915 is equal to 71 4 79 71 4 m3 s and so on to preserve the estimated baseflow with this recession curve we consider the event specific conditions of flood recession which may differ from a mean recession curve after reconstruction of the recession the end of the first wave is defined as the end of the recession and the flood volume and all other flood characteristics like duration or baseflow are updated accordingly the second flood event begins with this smallest discharge value and ends with the end point of the double flood wave fig 6a the volume of the second flood event is reduced by the recession curve of the first one by application of this approach a list of flood events with characteristics like peak volume duration direct runoff and baseflow was obtained with added comments if a superpositioned or superimposed flood event had been detected some of the resulting flood events were separated with a large increase of baseflow up to 300 at the end of the event compared to the beginning these flood events can be categorised into two groups flood events with extreme peaks where the increased baseflow seems to result from a scaling dependence of the algorithm on the peak about two flood events per gauge fig a1 2002 and 2013 events top left and flood events with high impact of snowmelt e g fig a1 1946 and 2005 events top right 3 2 interactive evaluation of separation even with the proposed methodology a totally satisfying separation cannot be ensured in an objective way especially for flood events with multiple peaks and long duration expert knowledge can be a valuable addition to automated algorithms since such a manual inspection of all flood events is rather time costly for large data sets we developed a tool based on r r core team 2018 that displays the flood hydrograph as well as the precipitation and the automatically predefined beginning and end of the flood utilising this tool a manual correction of the separation can be done quickly within the user interface in the browser window fig 7 more precisely the application enables a manual shift of the automatically defined start and ending point of each flood event if any change is performed the respective flood volume and baseflow are adjusted automatically an additional feature enables the user to add comments if the hydrograph shows a specific shape e g multiple peaked after checking all hydrographs the results can be exported in mean between 10 and 20 floods can be checked and corrected per minute with the tool with 2 87 flood events per year in 63 years for gauge wechselburg see table 2 this would require approximately 12 min for all 180 flood events 3 3 flood event precipitation for each flood event the flood inducing rainfall has to be derived in order to estimate flood characteristics like the runoff coefficient since the rising limb of the hydrograph was known we estimated the starting point of the flood inducing precipitation by matching it to the increasing daily rainfall we assumed that any increase of the discharges indicated a time span with flood inducing precipitation per definition the end of a flood event automatically defines the end of the flood inducing rainfall due to catchment reaction times a possible delay of the beginning of the flood event versus the beginning of event precipitation has to be considered we searched for the beginning of the event precipitation in a time window starting b days before the beginning of the flood event here b was defined as max ξ t peak t start 1 where ξ 0 the minimum time lag between beginning of precipitation and flood event can be chosen according to the catchment size during the time window t start b t peak the time series of accumulated precipitation was separated into two subsamples this change point is denoted by k t start b 2 t peak 2 for each of the two subsamples a linear regression is fitted with least squares p i α β j k t i with p i being the cumulative precipitation at time i and i j k fig 8 please note that each window had to have a length of at least 3 days otherwise a regression is not meaningful then the beginning of the flood inducing precipitation k was defined by the change point of the slope of the cumulative precipitation sum in this period the k for which the difference between the slope of both regressions between cumulative precipitation and time was largest k arg max t start b 2 k t peak 2 β k t peak β t start b k 1 a possible delay also applies for the end of the event here the end of event precipitation was defined as one day before the end of the flood event since pre flood precipitation may cause false starting points of event precipitation we compared the estimated beginning with an estimation by a twofold change point consideration of cumulative precipitation which works similar to the first method for this the precipitation time series p t start b p t end was separated into three sub sequences at the splitting points k 1 and k 2 then the point k 1 of the set k 1 k 2 for which β t start b k 1 β k 2 1 t end 2 β k 1 1 k 2 is smallest is chosen as the beginning of the event precipitation this criterion ensures that the second regression is steeper compared to the first and third parts and thus indicates the beginning of the precipitation event finally the results for the two methods were compared and we chose the beginning of event precipitation as the result that delivers the latest point in time before the beginning of the flood event fig 8 shows the estimation of the beginning of a precipitation event for the corresponding flood event for both methods one and two change point method for gauge aue 1 in the mulde basin in this example both approaches give different results the one change point approach defines the starting point at february 24th but the two change point method defines the starting point on february 27th in this case the second change point was chosen due to the smaller distance to the beginning of the flood event if the first change point would be considered only rainfall belonging to another flood event would be included the end of event precipitation was chosen as the day before the end of the flood event considering that direct runoff no longer occurs on the last day of the flood event 3 4 sensitivity of the parameters since we cannot refer to an objective truth a calibration and or validation on observed data is an impossible task workarounds can be used but are difficult and need special caution on the results and their interpretation therefore we used a sensitivity study to show the impact of different parameters on a reference separation which was carried out with manually adjusted parameters and resulted in a good separation quality additionally at the end of this section we introduce one possible goodness criterion which can be used for estimating parameters for new catchments or calibration validation in the sensitivity study the differences of the results between different parameter sets were evaluated for this we applied a reference parameter set and compared the results of the determined flood events with those obtained by other parameter combinations this is done by comparing day by day if any day belongs to a flood event for both sets of parameter choices as a result we obtain the proportion of days that belong to a flood event from one parameter set on flood days resulting from another parameter set one parameter set serves as reference set since it was checked according to the goodness i e a low number of manual corrections in the manual check the change of the respective proportion is then evaluated according to the change in the separation of the flood event for example separated flood events with the length of the moving window of variance d var 3 are compared with separated flood events with d var 5 d var 7 and d var 10 this is done twofold a the proportion of days that belong to separated flood events with d var 3 were compared to days that belong to separated flood events with d var 5 7 or 10 this is denoted as reference and b the proportion of days that belong to separated flood events with d var 5 7 or 10 were compared to days that belong to separated flood events with d var 3 varied this way it can be determined which parameter delivers longer and a larger number of separated flood events a schematic overview of the procedure is given in fig 9 two flood event separations with different choices of the parameter λ λ x and λ y are given with the black squares indicating days that are included in a separated flood event then the reference data set is calculated by counting all days that are included in a flood event for both parameter choices here this would be 12 from originally 22 days separated with λ x for the choice λ y originally 15 days were identified as part of a flood event from which 12 are identical to days identified with λ x this leads to reference equal to 12 22 0 55 and varied equal to 12 15 0 8 these proportions are displayed in boxplots for each parameter in this case this means that 80 of the flood events identified with the varied parameter set are also detected with the reference parameter set but only 55 of the flood events detected with reference parameter set are detected with the varied parameter set this implies that the reference parameter set detects either more or longer flood events additionally the number of separated floods is also compared to further differentiate this implication here the number of each parameter choice is calculated and the relative change to the reference parameter is given with these two results duration as well as number of flood events can be compared for each parameter please note that a comparison of e g statistics of the duration of the flood events is not meaningful since the separated flood events include very different flood types short flood events with high peaks caused by short but heavy rain long duration rainfall events that have a high volume and long duration and also snowmelt floods comparing a mixture of these types by a characteristic like the duration or the volume does not lead to reliable results a comparison of the same flood event for every parameter set cannot be guaranteed since the number of separated events varies e g when changing the variance threshold parameter moreover the full spectrum of possible values for each parameter cannot be considered since separation of full time series for 86 discharge gauges for six parameters with small parameter variation is rather time prohibitive however by the selection of parameter choices given here the general tendency of sensitivity is captured since our interest lies in the separation of floods and not runoff events the final parameters are chosen with this purpose for the separation of runoff events a different parametrisation would be meaningful e g lower thresholds for variance 3 5 calibration of initial parameters as stated above a goodness of fit for flood event separation lacks a consistent and true data foundation to compare the simulated data with to still have an estimate of the goodness of the separation algorithm we introduce a measurement for the goodness of our flood separation by explicitly maximising the number of flood events of high discharge while minimising the number of small runoff events included in the separated sample with the criterion 4 g sep q q t h upper f l o o d q q t h upper max q q t h lower f l o o d q q t h lower t o l lower 0 where q q t h upper is the number of days with discharge above the threshold t h upper and q q t h upper f l o o d is the number of days with discharge above the threshold which are separated as floods hence between the beginning and the end of a flood event separation the same applies to the second bracket but all days with discharge below a certain threshold are summed additionally the tolerance threshold to l lower allows for a certain percentage of flood events below t h lower to obtain more flexibility in the optimisation the range of the goodness therefore lies between 0 and 1 where 1 indicates that all days with discharge above the upper threshold are separated as flood events while a maximum of to l lower is separated as flood events below the lower threshold this ensures that large discharge values are separated while only a small amount of low discharge values e g in low flow periods are separated as flood events the three thresholds are set to the 95 quantile of discharge for the upper threshold and 50 quantile of discharge for the lower threshold while the tolerance is defined as 1 of the number of discharge days below the lower threshold keep in mind that even the three parameters are set a priori they remain an arbitrary choice and can be adjusted by the user if required this goodness measurement is also implemented in the floodr package with an evolutionary search algorithm mebane and sekhon 2011 for an optimisation of parameters users can apply this method to define initial parameters for the flood event separation 3 6 comparison to existing methods as stated above there exist several approaches to separate runoff events however many do not have the focus on floods and the application of flood statistics but on every temporally limited increase of runoff other methodologies depend on a chosen threshold defining a flood but not by the beginning and end e g peak over threshold to obtain a methodology comparable to the approach proposed here which focuses on flood events and their use for flood statistics and defines a flood event according to the variability of the discharge a combination of a flood event identification and the definition of beginning and end is required in the following we compared the results of such a separation method with the results of the method we propose using the parameter choices identified as suitable in section 4 here the flood events obtained directly from the algorithm and therefore without manual inspection were used to ensure a fair comparison since we focus on flood events only we compared the three approaches concerning the number of separated flood events the number of flood events per year and the value of the smallest peak of all separated flood events the number of separated flood events per year should be reasonable compared to the expected number of floods in a year the number of flood events per year is an often used criterion for flood statistics especially peak over threshold pot e g in germany austria and the netherlands on average 2 3 floods per year can be observed blmfuw 2011 rijkswaterstaat waterinfo 2020 of course this number varies for different climatic regions sometimes it is even used to define flood events e g by defining that in average 3 flood events per year should be included in the pot statistics kundzewicz 2019 mangini et al 2018 here it was used to evaluate if the separated events can be classified as floods too great a number of events per year indicates that not only floods but also short increases of runoff have been separated as flood events very often baseflow estimation approaches are coupled with flood identification e g annual maxima to obtain flood event separation e g graszkiewicz et al 2011 bezak et al 2015 shao et al 2020 here we will use three different flood identification thresholds if the daily discharges exceed such a threshold we can assume that a flood event occurred beginning and end of the flood event then can be identified by separation techniques the three thresholds are based on the pot approach where the exceedance of the threshold defines a flood event in pot statistics such an event would be considered for the sample we varied the threshold between three common values 2 and 3 times the long term mean discharge mq and the minimum values of the annual maximum series ams fischer and schumann 2016 these three thresholds are then used to identify flood events beginning and end of the identified flood events are estimated twofold on the basis of digital filters the lyne hollek dfm filter lyne and hollick 1979 filter was used to estimate the baseflow of the daily discharge time series the parameter was set to 0 925 according to nathan and mcmahon 1990 and eckhardt 2008 this is a common choice and was derived by the correlation between the baseflow indices derived using the digital filter and smoothed minima techniques each time the discharge exceeded the estimated baseflow the beginning of a flood event was defined see e g oppel and mewes 2020 the flood event ended when the daily discharge no longer exceeded the baseflow and hence no direct discharge was present this technique is very common to separate direct discharge from baseflow see references above of course other filter techniques could be also applied the method proposed by tarasova et al 2018 first applied a baseflow separation based on digital filters and separated the direct runoff event according to additional separation criteria afterwards the aim of this analysis was to define rainfall runoff events coupled with the pot thresholds it can be applied to identify flood events the resulting separation for the wechselburg gauge can be found as supplement of tarasova et al 2018 the approaches thresholds as well as digital filters are chosen according to their reproducibility recession approaches often lack reproducibility like the approach of blume et al 2007 where no specific method for defining the beginning of the event is given in contrast the filter technique with the estimation approach by eckhardt 2008 can be reproduced easily the same holds true for the pot approaches which do not require any calibration the approach by tarasova et al 2018 is well documented and the given data set in the supplements can be used for direct comparison there already separated events are given 4 results 4 1 sensitivity analyses of the flood event separation method for the application of the proposed algorithm several parameters had to be chosen the window for the moving variance dvar the parameter for the variance threshold θ the three thresholds for defining the beginning of the flood event η γ and κ and two parameters δ and ω to specify the end of the flood event the parameter d dur to specify a time window for superpositioned flood events moreover for the separation of the flood inducing precipitation the buffer b had to be defined by parameter ξ for the parameter d var it is clear that the longer the window for variance the less coherence of days of flood events the term coherence here refers to the number of days that were identified as part of a flood event for both parameter sets either in the case of reference or varied scenario fig 10 can be interpreted as follows the mean coherence of reference decreases with increasing parameter that is the number of flood days identified by dvar 3 is not equal to those days identified by e g dvar 5 however since almost all days identified by dvar 5 as part of a flood are included in dvar 3 see the proportion for varied it can be concluded that the flood events derived by dvar 3 have to be longer than those derived by dvar 5 the number of flood events became smaller 16 1 table 1 and the duration became shorter compared to dvar 5 fig 10 which is expected since a larger window for the variance compensates for small time windows of increased variance if the remaining days show less variability hence an increase of variance for shorter flood events is smoothed out for mesoscale catchments this is a disadvantage and the choice of three days as window length is kept for a time window shorter than three days a calculation of variance is not sensible and hence for our purpose of separating flood events and the mesoscale catchments considered here d var 3 is the best choice of course this choice is a compromise however results in fischer et al 2019 emphasize the goodness of this choice by demonstrating the applicability of the separated flood events for flood typology and flood statistics it is shown how the coherence of flood days is declining with increasing moving windows for large watersheds the coherence of days within flood events estimated with longer windows 5 or 7 days is in mean higher than for smaller watersheds fig 10 this may not affect the mesoscale catchments considered here much and the parameter can be kept constant but for macroscale catchments 10000 km2 an adjustment is meaningful please note that the classes of catchments sizes given in fig 10 were chosen here to ensure reasonable sample sizes of each class when increasing the threshold parameter of variances θ for the σ confidence band a clear reduction of the number of flood days can be observed fig 11 while for θ 0 5 the number of flood events was reduced by 1 3 compared to θ 0 25 and events became shorter for θ 1 in average only 25 of the events obtained with θ 0 25 remained resulting in less than one flood event per year table 1 however for increasing parameter almost all additionally separated flood events were included in the separation set using threshold θ 0 25 as the coherences showed no differences between watersheds of different catchment sizes visually and tested statistically applying the wilcoxon test to the proportion of coherences the selection of the θ 0 25 was scale invariant and applicable for all 86 watersheds fig 11 the same scale invariance was present for the remaining parameters the two parameters considered in the sensitivity analysis so far d var and θ are the most significant parameters and have the largest influence on separation compared to the remaining parameters only small changes of the resulting flood events results in table 1 and in the appendix an increase of these parameters leads to less flood events in mesoscale catchments since short increases in case of d var or small increases in case of θ of variance are ignored in flood event separation table 1 for macroscale catchments an adjustment is recommended since the omitting of such small increases of variance might be desirable for the remaining three choices of parameters defining the beginning and end of a flood event the results are presented in the appendix these parameters changed the number of flood events by less than 10 for most choices table 1 but had an impact on the length of all events the length of flood events can be adjusted by the threshold parameter for the natural variability eq 1 where we varied η between 0 05 and 0 3 the larger η the shorter the flood events fig a2 the number of flood events stays constant since the initial criterion of increased variance is not changed differences in the number of separated flood events mainly occurred due to different handling of superpositioned and superimposed flood events which is impacted by the length of the events due to this finding we chose η 0 1 the remaining two parameters defining the beginning are used to decide if a pre flood occurred that belongs to the flood event for this the criterion given in eq 2 is checked for the sensitivity analyses we varied the parameters γ between 2 and 8 days 1 day being the reference choice and κ between 0 1 and 0 6 where 0 4 was the choice made for the algorithm from the figs a3 and a4 it becomes obvious that the parameters have direct impact on the length of the flood event the larger the parameter γ the number of days to check for a pre flood the longer flood events are separated and the less events are found instead often floods are merged with the choice γ 1 for mesoscale catchments in the german climate pre floods are included that have direct impact on the flood event the smaller the parameter κ the more likely pre floods are included in the flood event however no further decrease of pre floods can be observed from κ 0 5 with the initial choice κ 0 4 we included pre floods as long as they have a contribution to the main event the last two parameters that were chosen for flood event separation have an impact on the definition of the flood event here we used δ in eq 3 to decide if the variation at the end of the flood event ω days is daily fluctuation or still belongs to the recession of the flood event the parameter δ was varied between 0 and 0 4 in the sensitivity analysis since this parameter is only a part of one of two criteria that have to be fulfilled its impact on the results is negligible in fact no difference in the flood event separation can be found in fig a5 for a few flood events however this parameter leads to shorter events the larger it is for about 2 of all flood events it omits erroneous long recessions of the events in the case where the end of the flood event cannot be found correctly due to long and smooth recession so we included it in the algorithm based on this we kept δ 0 2 according to the manual check variation of ω had almost no impact on the number of floods and omitted very long events fig a6 it is similar to δ concerning the impact on flood separation for mesoscale catchments two days for prolongation is a reasonable choice for definition of the event precipitation that is the time window of precipitation belonging to the respective flood event the only subjective choice made was the buffer time for the regression b more precisely the ξ day buffer in the maximum used to calculate b for sensitivity analyses ξ was varied between 3 and 11 days to analyse the impact on the resulting event precipitation this time window was chosen for mesoscale catchments where the reaction time will be less than 11 days however the catchment reaction time can vary depending on the antecedent soil moisture hence the buffer time window should not be too small the analysis method is analogous to the one described above for flood event separation applied to days where event rainfall is defined results for b can be found in fig 12 too short of a buffer led to shorter precipitation events too long of a buffer however did not change the beginning of the precipitation events at all so the algorithm is insensitive to an overestimation of the needed pre period of precipitation and with our choice of ξ 7 days we are acting conservatively finally the parameter d dur had to be chosen we decided to choose d dur 40 days meaning that we expect superimposed events to only occur in separated flood events of a duration of more than 40 days for flood events with smaller duration only superpositioned events occur since the basins considered here belong to the mesoscale this threshold was sufficient for our purpose please note that this decision only has a small impact on the results since for superpositioned as well as superimposed events a similar handling is performed the only difference is the amount of baseflow of the flood events in general this parameter can be estimated e g by evaluating the mean discharge and estimating the mean duration of increased discharge the sensitivity analyses for the parameter choices made in the algorithm demonstrate that our choices can be kept for the application of mesoscale catchments with different choices the length and number of flood events can be changed for our application in the manual check the parameter choices showed valid results i e a small number of manual corrections and hence were kept please note that a sensitivity analysis is not always required for application of the algorithm instead parameters can be adjusted according to the desired outcome for example parameter θ for the variance threshold can be chosen according to the desired number of flood events per year the larger θ the less events are detected the same holds true for d var the remaining parameters when applied for mesoscale catchments can be kept as suggested here or checked in the provided visual inspection tool 4 2 validation of the separation method additionally to the sensitivity analysis a validation was applied to test the goodness by applying eq 4 with the default parameters to the separated flood events and their discharge timeseries fig 13 shows the boxplot of the goodness of fit criterion for all catchments considered here grouped by basin with the parameter choices given above it can be seen that some catchments have large values close to 1 indicating that most of the days with discharge above the 95 quantile have been linked to a flood event while most of the discharge below the median discharge is not separated as a flood event most of the basins respectively their catchments have a goodness between 0 75 and 0 8 the inn basin has catchments with a separation goodness of below 0 5 these are small mountainous and or glacier influenced catchments same holds true for the outliers in the ostharz and westharz basin where both outliers are catchments smaller than 30 km2 here the temporal resolution of daily discharges could be a drawback in the separation 4 3 results of the automated flood event separation the resulting flood events cover most of days of high discharges as already shown in fig 13 a more detailed evaluation of the discharge timeseries can be seen in fig 14 which shows the cumulative discharge for one small and one large catchment for each basin it is demonstrated that the separated flood peaks all lie above the 95 quantile of all daily discharges the days that lie above this threshold but do not belong to a flood peak are mostly those that are separated as part of the flood event and most probably lie in close temporal range to the peak no correlation between the amount of separated high discharge days and the catchment size can be observed since no objectively verifiable flood event separation exists an objective evaluation of the algorithm is not possible one possibility is to change perspective and determine which flood events are not separated correctly this can for example be achieved by using the runoff coefficient which is here defined as the discharge volume of the flood event divided by the rainfall volume of the corresponding rainfall event a runoff coefficient above 1 indicates that the flood separation could have not worked here of course data uncertainty can lead to wrong conclusion but it is a good first estimator for the goodness of the separation method also the missing snowmelt for the calculation of the amount of flood inducing water respectively the runoff coefficient is assumed to produce runoff coefficients above 1 for winter and spring floods especially in the mountainous inn and ost westharz catchments fig 15 shows the runoff coefficient for one small and one large catchment for each basin differentiated by month of peak occurrence it can be seen that the runoff coefficients for the lowland basins cloppenburg and lueneburg are comparably small and low in variation the other basins which are all affected by mountains and snowmelt processes have larger runoff coefficients and a clear monthly pattern emphasizing that so far snowmelt is not included in the calculation of the runoff coefficient here estimations of snowmelt with rainfall runoff models and inclusion of this in the runoff generating amount of water could help correct these runoff coefficients for the total of 2648 flood events in fig 15 151 runoff coefficients can be observed above 1 of which all but 3 occur in the winter and spring season between december and may one flood event inn basin with runoff coefficient equal to 1 22 is influenced by glaciers while the other with runoff coefficients of 1 05 and 1 27 are flood events occurring in august here precipitation and discharge measurements do not correlate and indicate faulty data or very local precipitation events not captured by the precipitation gauges however since we use the manual correction afterwards we can also compare the flood events before and after correction we can assume that the manual correction is very close to the true event separation since hydrological expert knowledge was used this is similar to the assumption made by thiesen et al 2019 who assumed flood events identified by an expert as truth hence we compared the proportion of corrected events for each gauge this is the number of manually corrected flood events divided by all separated events we differentiated between the basins to find systematic regional differences and used the parameters identified in section 4 as meaningful for flood separation the results can be seen in fig 16 in general between 40 and 70 of all flood events had to be corrected manually the fewest correction of all basins was done for flood events in the mulde basin where mostly the end was shifted this is probable caused by the dendritic structure of the basin leading to less complex shapes of the hydrograph between all remaining basins no regional differences could be found since the separation algorithm resulted in a similar proportion of manually corrected flood events this implies that the algorithm works equally well for all basins and is not influenced by e g the topography however the number of corrected events is quite high and brings into question the applicability of the algorithm therefore we also had a look at what was changed and how it was changed by manual correction of the events in particular we compared the beginning and end date before and after correction due to baseflow increase over several months as described above fig 6b for some gauges one or two flood event beginnings or endings were shifted by 100 200 days this was the case only for 1 of all flood events that is why we decided to compare the median of the difference between old and new beginning and end for each catchment to reduce the impact of these single outliers the results can be found in fig 16 there in particular the beginning of flood events was estimated very accurately by the automated algorithm almost no correcting shifts had to be performed manually only one catchment in each of the basins cloppenburg ostharz and westharz had a median difference in beginning of more than one day whereas for inn lueneburg main and mulde in median no corrections of the starting point were necessary the basins cloppenburg westharz and ostharz are located in the central part of germany while ost and westharz are both affected significantly by luv and lee effects of the harz mountains causing e g a rain shadow for parts of the ostharz basin cloppenburg is a rather flat basin this implies that not the catchment characteristics or the topography affects the separation of the event beginning the three gauges in the basins where the beginning of the flood event was systematically estimated off target are with 17 km2 18 km2 and 72 km2 rather small but not the smallest gauges considered a rapid runoff reaction that cannot be handled by our approach cannot be the only reason for the wrong separation other possible impact factors might be the catchment shape soil types or a steep hypsometric curve yet no clear indication for the resulting deviation of the beginning of the flood has been found fig 17 demonstrates that the estimation of the end of a flood event is much more difficult this is in line with studies on event separation before blume et al 2007 however in median the end had to be corrected by one day only for inn and mulde the most mountainous basins in median no correction was necessary the very steep nature of the catchments leads to faster runoff generation and therefore a faster drainage of the catchment and a more abrupt end that is easier to detect in general the evaluation of the corrections shows that almost no significant correction had to be done manually the correction of the end of a flood event by one day does not lead to significant changes in the total volume not shown or duration fig 17 and does not change the peak at all it remains included in the event hence even without manual correction the algorithm delivers meaningful results the manual correction is important in cases where long flood events had to be separated with a duration of several weeks or even months in this case the baseflow behavior changed and the algorithm was not able to differentiate the variance of the flood event from the natural variance for this the manual check is required when adjusting the parameters for the variance threshold it is possible to also separate all runoff events instead of flood events only by reducing the threshold e g θ 0 1 a smaller increase of variance already indicates the beginning of an event this can be of advantage when rainfall runoff events are the purpose of the separation 4 4 comparison to other event separation methods many runoff event separation methods exist but only few can be designed for the separation of flood events to demonstrate the differences to the method proposed here a flood identification threshold is applied afterwards digital filters are used to estimate beginning and end of the corresponding flood event an example of one year of discharge is given in fig 18 the three pot thresholds that are used to identify flood events are given as horizontal lines for each identified event beginning and end is estimated with either the lyne hollek filter where a deviation of the discharge from baseflow indicates direct runoff or with the approach of tarasova et al 2018 for the latter the baseflow is separated with the straight line method as comparison additionally beginning and end of our method is indicated also by using a straight line representing the baseflow please note that our method is independent of the pot thresholds since flood events are defined directly within the algorithm here we used the flood events that were obtained directly from the algorithm meaning that no inspection by an expert was performed to make comparison fair for the given year four pronounced flood events are visible these flood events are detected by our method as well as the pot thresholds for the pot threshold equal to the minimum annual maximum discharge min ams additional 2 flood events have been identified and separated with both methods lyne hollek filter and tarasova et al similar behavior of the algorithms is also visible in fig a7 in the appendix when comparing the separation in terms of beginning and end it becomes obvious that the lyne hollek filter very closely follows the runoff variation this leads to comparably early ends of the flood events which are located clearly during the recession period of discharge sometimes rather close to the peak the approach of tarasova et al 2018 gives comparably longer flood events to our approach but sometimes splits long flood events fig a7 in general more of the pre event runoff is included to demonstrate the differences between the flood separation approaches in fig 19 the flood event duration and the relation between discharge at the end of the flood event and flood peak is compared for each of the methods for the daily mean discharges at wechselburg gauge germany in the years 1951 2011 the end peak relationship is an indicator for how early the flood end is defined it can be seen that the tarasova et al separation method leads to comparably longer duration of flood events despite of the flood identification threshold this can be explained by the double events that are separated by this approach for the double waves for details see tarasova et al 2018 the median duration is 14 days whereas it is 4 days for the single waves in combination this leads to longer flood event duration the lyne hollek filter gives comparably longer duration then our approach mostly due to an earlier beginning fig 18 when comparing the relation between discharge at the end of the flood event and peak discharge it can be seen that the approach of tarasova et al delivers similar results as our approach this implies that their definition of a flood event is similar to our definition the lyne hollek filter instead clearly defines the flood end much earlier for both approaches tarasova et al and lyne hollek filter also the variability of the results is much larger in summary the findings so far indicate that the approach of tarasova et al in combination with the pot thresholds separates flood events with similar definition of the end but shorter duration for single waves and longer duration of double waves compared to our approach the lyne hollek filter in combination with the pot thresholds instead separates flood events with much earlier ends further results are given in table 2 our approach delivered the smallest number of flood events per year compared to the other approaches used here with 2 98 the number of flood events per year is in a range for germany which could be related with a spring flood caused by snowmelt and or high soil moisture and rain at the end of the winter and one or two summer floods typical with a smaller volume but higher peak than the winter flood by adjusting the pot thresholds the number of flood events per year can be altered a comparable number of flood events per year is delivered when the flood is identified with pot approach and the threshold was chosen as 3mq in this case however the peak of the smallest flood detected was much higher with 79 2 m3 s compared to our results of 51 8 m3 s this can be explained by the ability of our approach to detect multiple peaked events these flood events are separated by the pot approach together with the separation by lyne hollek respectively tarasova et al into single peaks and would increase the number of flood events with higher peak discharges this problem of the pot approach can be overcome by combining pot with criteria for independence between flood peaks similar to the criterion by klein 2009 used for our approach where the splitting of a multiple peak event can be omitted lang et al 1999 in general it could be seen that the pot approach can be used to control the number of flood events that is the higher the threshold the more the number of flood events converges to a limit at the same time the use of such a threshold without considering runoff dynamics could lead to the situation that the same flood can be considered as flood or not depending if it occurs at a period with high or low baseflow the results emphasize that our approach delivers similar results as the often applied combination of flood detection thresholds and digital filters however it has the advantage that not a constant threshold has to be defined but instead the runoff dynamics are considered hence the splitting of flood events occurs less often 5 conclusion we propose a new automated statistical algorithm for flood event separation that is based on an analysis of the variance of the daily discharges the aim was to separate flood events that can be used for flood statistics this means only flood events should be separated and their respective characteristics like peak volume and duration should be obtained moreover the associated flood inducing precipitation event was identified additionally for an easy manual check as an intermediate step of the procedure a helpful interactive application is provided especially for use in distinguishing very long flood events with multiple peaks with this two step procedure large data sets can be analysed easily and flood events along with their characteristics such as volume duration event precipitation runoff coefficient and baseflow at beginning and end of the flood event can be obtained for further analyses e g multivariate flood statistic or rainfall runoff modeling an application to catchments in germany and austria of significantly differing natural conditions demonstrated the transferability of the approach to varying types of catchments alpine mountainous or lowlands the parameters in the algorithm were chosen to be suitable for mesoscale catchments the results indicate that the algorithm separates flood events reliably for all catchments the manual check adjusts separation of flood events of long duration and with multiple peaks caused for instance by several rainfall events occurring in short chronology for most flood events a manual adjustment is not necessary in a sensitivity analysis the choice of parameters that define the beginning and end of the flood event and the corresponding precipitation are evaluated the choices were made for mesoscale catchments with the proposed calibration method initial parameter choices can be made by the applicant the method proposed here overcomes the three drawbacks that most separation methods have namely the reliance of baseflow separation for identification of beginning and end of flood events the requirement of large samples of already separated events as needed e g for machine learning based methods the inability of identifying flood events also during low baseflow dry conditions it is important to notice that by changing the respective parameters the proposed method can be adjusted to also separate runoff events and to be applicable to hourly data and or macroscale catchments as long as these are not affected in their runoff dynamics e g by dams this is in contrast to existing methods that are mostly limited to one of these cases and cannot be adjusted to be applicable to other cases yet there are still some model assumptions in the proposed algorithm that are based on subjective choices though these choices can be altered by the user they remain impacted by personal experiences the same holds true for the starting values of the parameters for optimization the proposed expert validation is not required but highly recommended at least for small data sets credit authorship contribution statement svenja fischer methodology writing original draft visualisation andreas schumann writing review editing philipp bühler software visualisation declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements the financial support of the german research foundation deutsche forschungsgesellschaft dfg in terms of the research unit spate for 2416 is gratefully acknowledged moreover the authors would like to thank two anonymous reviewers and stephanie thiesen for their valuable reviews that helped to improve this paper the discharge data for the mulde basin are provided by the state water authority lfulg saxony at www umwelt sachsen de umwelt infosysteme ida for the basins lueneburg cloppenburg and westharz the discharge data are available by the nlwkn of lower saxony at www wasserdaten niedersachsen de cadenza for the main basin and the german parts of the inn basin the discharge data are available at www lfu bayern de wasser wasserstand abfluss while the discharge data for the austrian catchments of the inn are available at ehyd gv at discharge data of ostharz have to be requested from https www umwelt sachsen anhalt de daily time series of precipitation for germany are available from the german weather service dwd at the climate data center cdc cdc dwd de portal respectively from ehyd www ehyd gv at for austria all r codes are available at www github com philippbuehler in the package floodr appendix fig a1 displays 12 separated flood events with highest increase of discharge it can be seen that they mostly consist of extreme peak events or snowmelt events in the following supplemental figures for the sensitivity analysis in section 4 are given fig a7 displays a second year of flood event separation and can be used in addition to fig 18 to compare the different approaches additionally to the results presented in the manuscript the sensitivity of the flood separation algorithm to the temporal resolution of the discharge data was tested for this hourly discharge data were used for the longest time period available in average 15 years for each gauge were available since hourly data is not the primary focus only a short evaluation if the method is capable of separating hourly data has been done for this the flood events derived from daily data at the golzern gauge in the mulde basin between the years 2008 and 2014 have been used an optimisation function bobyqa which is a derivative free bound constrained optimisation has been used to maximise the overlapping between daily and hourly flood events while minimising the non overlapping similar to the sensitivity analysis with a reference and varied parameter set of the 22 flood events derived from daily data 17 events have been separated from hourly values with reasonable beginning and end also additional 16 events have been found which is due to the more variable nature of hourly discharge the most relevant parameters for the event separation with hourly data dvar and θ had their optimal parameters at 14 5 hours and 0 33 respectively while these are promising results from just a simple optimisation a complete evaluation is beyond the scope of this paper the method should not be easily transferred from daily to hourly values because of the different behavior of the runoff data 
8580,the classification of characteristics of flood events like peak volume duration and baseflow components is essential for many hydrological applications such as multivariate flood statistics the validation of rainfall runoff models and comparative hydrology in general the basis for estimations of these characteristics is formed by flood event separation it requires an indicator for the time when a flood peak occurs as well as the definition of the beginning and end of a flood event and a subdivision of the total volume into direct and baseflow components however the variable nature of runoff and the multiple processes and impacts that determine rainfall runoff relationships make a separation difficult especially an automation of it we propose a new statistics based flood event separation that was developed to analyse long series of daily discharges automatically to obtain flood events for flood statistics moreover the related flood inducing precipitation is identified allowing the estimation of the flood inducing rainfall and the runoff coefficient with an additional tool to manually check the separation results easily and quickly expert knowledge can be included without much effort the algorithm was applied to seven basins in germany covering alpine mountainous and flatland catchments with different runoff processes in a sensitivity analysis the impact of chosen parameters was evaluated the results show that the algorithm delivers reasonable results for all catchments and only needs manual adjustment for long timeslots with increasing or high baseflow it reliably separates flood events only instead of all runoff events and the estimated beginning and end of an event was shifted in mean by less than one day compared to manual separation keywords flood event separation runoff variation automation 1 introduction the flood peak is the most often considered characteristic in flood statistics and in deterministic rainfall runoff modeling e g as validation criterion for event specific validation although the peak plays a crucial role in assessing e g flood protection measurements the peak alone does not fully characterize a flood event especially for comparative hydrology blöschl et al 2013 not only the peak but also the volume the duration and the shape of the flood event have to be analysed detailed knowledge on all flood characteristics is important e g to apply a flood typology tarasova et al 2019 sikorska et al 2015 merz and blöschl 2003 fischer et al 2019 or to select criteria to evaluate deterministic flood models viglione et al 2009 according to one of the rare definitions of the term flood a flood is a period when discharges are equal or larger than a flood threshold value ozga zielinska 1989 since no unique definition of such a threshold exists it is defined by the objectives of the flood analyses e g by the beginning of inundations navigation problems or by statistically defined values the wmo unesco defined flood in the international glossary of hydrology world meteorological organization wmo 2012 as follows 1 rise usually brief in the water level of a stream or water body to a peak from which the water level recedes at a slower rate 2 relatively high flow as measured by stage height or discharge an important point when comparing event separation methods is the aim of separation basically the methods differ between runoff event separation and flood separation while the first focuses on every increase of runoff e g tarasova et al 2018 diederen et al 2019 the latter focuses on the flood events only for example by separating flood events for given flood peaks e g oppel and mewes 2020 which are characterised by a large increase of runoff see german norm din 4049 1 1992 due to the nature of such events in central europe they are expected to occur seldomly within a year although rare flood events are critical in understanding the nature of processes leading to extreme floods like the ones in 2002 and 2013 in germany and austria and form a basis for flood statistics there are only few methods that are designed especially for the separation of flood events an often used method is based on a peak over threshold approach where the events are separated that are above the chosen threshold kundzewicz 2019 mangini et al 2018 thresholds can be chosen according to the mean runoff or the smallest annual maximum fischer and schumann 2016 this method is often used in the context of flood statistics since it easily delivers the flood peak however event beginning and end are not provided directly instead often additionally baseflow separation methods are applied where it is assumed that a deviation of baseflow the direct runoff indicates a flood event oppel and mewes 2020 though they are originally designed for the separation of runoff events they are suitable in the context of floods when applied to apriori identified flood events e g yue 2000 2001 graszkiewicz et al 2011 bezak et al 2015 e g in combination with the peak over threshold pot approach or a pre defined number of events per year to only separate large peaks karahacane et al 2020 for baseflow estimation different methods exist for example nathan and mcmahon 1990 and tallaksen 1995 propose a recession curve of exponential type to define the point where the direct runoff ends assuming that this component has a higher declining rate than the baseflow blume et al 2007 proposed in their constant k method an approach which is based on the linear storage theory it was further developed by mei and anagnostou 2015 to a hybrid method of the constant k method and recursive digital filter often filter techniques are also used for automated baseflow separation e g chapman 1999 chapman and maxwell 1996 a comparison of manual techniques for baseflow separation with filter algorithms and recession analyses was done by eckhardt 2008 who concluded that the method proposed by eckhardt delivered a more hydrologically plausible estimation of baseflow than the chapman filter and straight line methods a combination of baseflow estimation and recession curve offers the toolbox provided by tang and carey 2017 they estimate the beginning and end of a runoff event by local minima derived from baseflow estimation and also calculate the recession parameters to identify unusual recession curves in order to remove outlier data su et al 2016 state that these digital filter methods for baseflow separation deliver discrepancies between estimated and theoretical baseflow especially for catchments with lower baseflow index bfi which is defined as the ratio of the baseflow to the total discharge in some applications baseflow separation techniques are combined with additional thresholds to make them suitable for runoff event separation for example tarasova et al 2018 combine linear interpolation between local minima see e g piggott et al 2005 aksoy et al 2008 2009 with event identification thresholds the arbitrary choice of parameters and thresholds is a crucial disadvantage of such techniques depending on the choices made very different flood volumes could be separated moreover the application of thresholds often leads to the effect that events in low baseflow periods or general dry periods cannot be detected since they do not exceed these thresholds yet these events may be comparable to those in wet periods concerning direct peak and volume recently application of machine learning techniques offered a new perspective on flood event separation e g thiesen et al 2019 oppel and mewes 2020 these techniques use pattern recognition to separate flood events and hence overcome the drawback of pot based methods that is the application of constant thresholds and the usage of baseflow separation yet machine learning techniques require large samples of labeled and hence apriori separated flood events as training sets these often are obtained by manual separation oppel and mewes 2020 and therefore are rather subjective in conclusion there exist three drawbacks that make most separation algorithms inappropriate for separating flood events suitable for flood statistics the reliance of baseflow separation for identification of beginning and end of flood events the requirement of large samples of already separated events as needed e g for machine learning based methods the inability of identifying flood events also during low baseflow dry conditions in this study a two step method is proposed that aims to overcome these drawbacks it is based on a statistical analysis of long time series of daily discharge series and focuses on the identification of highly variable flood events instead of runoff event separation only a few parameter choices are made that depend on the catchment size whereas different parameter sets are recommended for meso and macroscale catchments the first step consists of the flood event identification based on a moving variance approach in this step beginning end and flood peak time of maximum discharge between beginning and end are determined an additional component of this first step is the opportunity to include expert knowledge by making a posteriori check of the determined flood events with this even complex shapes of flood hydrographs like multiple peaks or pre floods can be separated as a second step the separation of the associated flood inducing precipitation event is proposed so that ultimately all characteristics like peak volume duration start and end point runoff coefficient and flood inducing amount of rainfall of a flood event can be derived moreover the baseflow is estimated with a straight line method e g chow et al 1988 during this step up to now many studies use separated flood events but do not state clearly how these flood events are derived however the quality of the results depends on the quality of input data hence it is important to make the data basis understandable and reproducible we propose a new algorithm that makes flood event separation easily applicable to large data sets easily reproducible when manual check is omitted yet still preserving the advantages of expert knowledge input if needed the inclusion of expert knowledge already has been proposed as a valuable extension by tang and carey 2017 all algorithms are available in an r package floodr free to use with the automated flood event separation methodology consistent flood event separation can be attained 2 data for the proposed flood event separation a time series of daily discharges and information about the catchment size are the only data required the discharge data may include missing values if the instantaneous peaks additionally to daily discharges are available but with lower temporal resolution e g as monthly maxima these can be used with the provided separation algorithm as real measured peak value for improving the flood statistics possible applications of the separated flood events are multivariate peak over threshold approaches or timescale based flood typing fischer et al 2019 for a full characterisation of the flood events including the flood inducing precipitation and e g the runoff coefficient a subsequent analysis of time series of daily precipitation areal precipitation has to be done here seven different basins in germany and austria are considered which vary from alpine and mountainous with rocky grounds to lowland catchments with sandy soil in total 86 gauges measuring daily discharges were available belonging to any of the seven basins fig 1 the catchment sizes vary between 3 4 km2 benzingerode ostharz and 25942 km2 passau ingling inn so that we can ensure inclusion of different catchment sizes and runoff processes in our data sets areal precipitation was estimated by thiessen polygons with a 90 km buffer around each catchment an overview of the most important catchment characteristics is given in fig 2 where it can be seen that the catchment sizes vary between 3 km2 and 26000 km2 with the largest catchment sizes present in the inn basin for this basin with its location partly in the alps the highest elevations of the sample are present the observation periods of daily discharge are quite long for all catchments with often more than 50 years captured the proportion of urbanised area derived from urban and industrial partitions from the corine dataset bossard et al 2000 is relatively small for all catchments such that fast runoff reactions due to sealed soil cannot be expected please note that especially for the large catchments anthropogenic impact can affect the data however as long as the general level or the variability of the discharge is not altered e g by the construction of a dam the methodology proposed here is still applicable applying common statistical change point tests for the mean and the variance we did not detect such shifts or change points in water level or discharge the links for the sources of the data sets are provided in the acknowledgments 3 methodology 3 1 flood event separation for long discharge timeseries due to the lack of data in hourly resolution flood event separation has to be based on daily discharges therefore this demands different assumptions compared to the highly fluctuating hourly discharges for hourly discharges the high dynamics of runoff often make a flood event separation easier since beginning and end are characterised by abrupt changes for daily discharges instead the fluctuation of discharge is smoother and times of abrupt changes are harder to detect here a moving window variance overcomes these lower dynamics of daily discharges basic rules applied for the development of our method are a flood event is a temporally limited exceedance of normal discharges discharges with low variability for every flood event beginning and end can be defined a flood event can be characterised by significantly increased dynamics of discharge especially around the peak the variance of discharges is high the sum of the increasing discharges is similar to the sum of the recession of the flood event but a potential increase of baseflow is considered two distinct variables for consideration can be derived from these rules the lag 1 1 day differences of the daily discharges qdi qi qi 1 with qd1 0 and their variance in a certain time window of length dvar n 2 meaning that dvar is an integer larger than 2 given by va r d var i v a r q i d var 1 q i va r d var 1 v a r d var d var 1 0 where i 1 n indicates the time step and n is the length of the considered time series of discharges the window length dvar is a catchment dependent parameter and is chosen according to the catchment size and the runoff dynamics in general for the catchments considered here a unique window length dvar of 3 days is chosen this choice is evaluated in the sensitivity analysis in a later stage to indicate the period of high dynamics belonging to a flood event the variance has to exceed a certain threshold an often applied range to classify the deviation of certain values in a data set from the remaining ones is the sigma threshold for the approach here a similar threshold t h var is derived such that the values of interest lay above θ 0 standard deviations σ away from the mean t h var va r d var θ var v a r d var this means we detect a significantly higher variation in discharge if the variation is larger than θ standard deviations of the average dvar day variation fig 3 the value of θ has an impact on the sensitivity of the algorithm the smaller θ the more flood events are detected hence it can be chosen based on the desired number of flood events per year for example during the time period of exceedance of the threshold with discharges q 1 q n the maximum discharge is defined as peak of the flood event q t peak the starting point of the flood event is defined as the time step before the peak for which qd is negative for the last time t start max 1 i t peak q d i 0 from this point on an increasing discharge can be observed however not every increase of runoff automatically defines the beginning of a flood event to consider the natural discharge variability that does not belong to the flood event we adjust the starting point fig 4 a with natural variability we refer to the processes not belonging to the extraordinary case of floods that are not part of the daily fluctuation more precisely as long as 1 q t start q t start 1 q t start 1 η η 0 1 we set q t start q t start 1 the parameter η provides a threshold for the relative 1 day discharge difference before beginning with this modification we ensure that the beginning of a flood is indicated by a pronounced increase of discharge of course not every flood event consists of a straight monotone increase of discharge instead short temporally limited increases of the runoff are often mixed with short recession phases this occurs due to variability of precipitation since these initial increases of discharges may contribute substantially to the flood event e g by higher antecedent soil moisture and a stepwise increased level of runoff we want to include such phenomena into the flood event as long as their contribution is significant rather than classified as an independent flood event fig 4b we handle this by adjusting the beginning by t start t start m if max 1 i γ 2 j γ 1 i j q t start i q t start j κ q t peak q t start 2 where m arg max 2 j γ 1 q t start i q t start j κ q t peak q t start 1 i γ i j and γ n 0 i e a positive integer the parameter κ controls the level of significant contribution a pre flood has to have to be part of the flood event this means if there is an increase of the magnitude of at least κ 0 1 of the total peak within γ 1 days before the flood event this pre flood is included in the flood event in fact a local maximum is searched in the range of γ 1 days before the beginning of the flood event the parameter γ can be chosen according to catchment size and climate region of the catchment to define the end of the flood event we need to find the time k t peak for which the sum of one day gradients i t start 1 k q d i becomes approximately zero this would mean that we are at the same discharge level as at the beginning of the flood event however in hydrological theory an increase of the baseflow component during a rainfall runoff event is assumed this was considered by the following definition of the end point of the direct runoff t end arg min t peak 1 t n i t start 1 t q d i 0 i t peak 1 t end ω q d i i t peak 1 t end q d i i t peak 1 t end q d i 1 δ 3 here two criteria are compared first we check if the baseflow from the beginning of the flood event is reached again which is comparable to the straight line method by chow et al 1988 second we check if the next ω n 1 time steps days after the potential end of the flood event would add less than δ 0 1 of the discharge volume of the falling limb if they would be included if either of these criteria is met the end of the flood event is found of course the end is always defined as occurring after the beginning of the flood event a full schematic overview of the algorithm is given in fig 5 there are several assumptions on the temporal variability of baseflow during floods which can be differentiated into two types nathan and mcmahon 1990 those that assume that the baseflow responds to a precipitation event concurrently with direct runoff and those that account for the effects of bank storage and assume that the baseflow recession continues after the time when direct runoff begins the shape of the baseflow hydrograph depends on the geological and geomorphological characteristics of catchments which cannot be specified for heterogeneous watersheds in an objective way in this manuscript we applied the first type of assumption on baseflow the baseflow was estimated with the straight line method between beginning and end of the flood event to avoid any inconsistencies between the separations of single flood events the slope of the straight line baseflow of each flood event was compared with the median slope of baseflow for all flood events of a watershed this comparison with the median was not found to be essential and was only useful to detect very high or low slopes of the rising baseflow where the end point of the flood would have had to be corrected manually this automatic detection can be activated in the algorithm a special case that was not addressed explicitly by the procedure described above are flood events with multiple peaks here we divided such flood events into three groups single floods with unsteady rising and or falling limbs the superposition of two flood events which can be separated and floods which are superimposed on high and long lasting baseflow following successively in a pre defined time span and cannot be separated each consecutive rain event falls on a wet catchment and causes an additional increase of direct runoff to differentiate the first two groups of floods from the third one we define a time span d dur if the flood event has a longer duration than d dur days it probably belongs to the third category using these rules an extraordinary duration of a separated flood event with multiple peaks becomes an indicator for a series of superimposed flood events the parameter d dur can be chosen according to the catchment size and the climatic conditions it can be estimated by evaluating the mean discharge and the duration of exceedance of its mean using this method we found several such series of floods in wet seasons of the year which could not be differentiated into single flood events the long duration of such flood events makes the separation between direct runoff and baseflow difficult according to wmo 2012 direct runoff is the water flow that enters a watercourse after precipitation without delay if the rainfall occurs in short time intervals and hence the runoff cannot decrease to baseflow level the total runoff may well be the result of cumulative precipitation without the existence of a clear and objective distinction between direct runoff and baseflow fig 6 b in this case the baseflow increases over several months here we decided to specify such flood events as a specific type of flood these flood events are important when considering wet soil moisture conditions the second group of floods are separable flood events here the second flood event begins before the recession of the first flood event has ended this leads to a magnification of the second flood event an example is given in fig 6a to estimate the feasibility of separating two successive floods we had to define an independence criterion criteria like this are mainly based on the differences between the heights of two consecutive local maxima the level of the local minimum between them and the time span between two consecutive maxima e g lang et al 1999 zhang et al 2016 bacchi et al 1992 proposed the following rule two subsequent flood events were considered independent if their peaks are separated by at least 20 f where f is the time to peak and the flow decreases by 80 or more between two adjacent peaks the selection of such a time span requires hourly runoff data with regard to the daily averages which are used here and the different reaction times of watersheds we avoided using any time span between consecutive peaks to define their interdependency we applied the following rules proposed by klein 2009 two consecutive local maxima of a hydrograph belong to two independent flood events if the larger peak is at least 5 times higher than the smaller one the larger peak is more than 2 5 times as large as the smallest discharge between both peaks 70 of the smaller peak is larger than the smallest discharge between two consecutive peaks this criterion ensures that both peaks are distinct enough to be treated as two consecutive flood events in this case both flood events have to be separated and the recession of the first has to be reconstructed the end of the first flood event was estimated by the smallest discharge between both peaks to reconstruct the recession we added a recession curve which was estimated from the recession of the second flood using all discharge values smaller than the lowest discharge between both peaks still discharges were kept above the estimated baseflow in fig 6a the local minimum is equal to 79 m3 s 12 03 1915 the same value can be found at 21 03 1915 from this point on the recession is reconstructed that is the value at 13 03 1915 is equal to 22 03 1915 with 71 4 m3 s then the value of 14 03 1915 is equal to 71 4 79 71 4 m3 s and so on to preserve the estimated baseflow with this recession curve we consider the event specific conditions of flood recession which may differ from a mean recession curve after reconstruction of the recession the end of the first wave is defined as the end of the recession and the flood volume and all other flood characteristics like duration or baseflow are updated accordingly the second flood event begins with this smallest discharge value and ends with the end point of the double flood wave fig 6a the volume of the second flood event is reduced by the recession curve of the first one by application of this approach a list of flood events with characteristics like peak volume duration direct runoff and baseflow was obtained with added comments if a superpositioned or superimposed flood event had been detected some of the resulting flood events were separated with a large increase of baseflow up to 300 at the end of the event compared to the beginning these flood events can be categorised into two groups flood events with extreme peaks where the increased baseflow seems to result from a scaling dependence of the algorithm on the peak about two flood events per gauge fig a1 2002 and 2013 events top left and flood events with high impact of snowmelt e g fig a1 1946 and 2005 events top right 3 2 interactive evaluation of separation even with the proposed methodology a totally satisfying separation cannot be ensured in an objective way especially for flood events with multiple peaks and long duration expert knowledge can be a valuable addition to automated algorithms since such a manual inspection of all flood events is rather time costly for large data sets we developed a tool based on r r core team 2018 that displays the flood hydrograph as well as the precipitation and the automatically predefined beginning and end of the flood utilising this tool a manual correction of the separation can be done quickly within the user interface in the browser window fig 7 more precisely the application enables a manual shift of the automatically defined start and ending point of each flood event if any change is performed the respective flood volume and baseflow are adjusted automatically an additional feature enables the user to add comments if the hydrograph shows a specific shape e g multiple peaked after checking all hydrographs the results can be exported in mean between 10 and 20 floods can be checked and corrected per minute with the tool with 2 87 flood events per year in 63 years for gauge wechselburg see table 2 this would require approximately 12 min for all 180 flood events 3 3 flood event precipitation for each flood event the flood inducing rainfall has to be derived in order to estimate flood characteristics like the runoff coefficient since the rising limb of the hydrograph was known we estimated the starting point of the flood inducing precipitation by matching it to the increasing daily rainfall we assumed that any increase of the discharges indicated a time span with flood inducing precipitation per definition the end of a flood event automatically defines the end of the flood inducing rainfall due to catchment reaction times a possible delay of the beginning of the flood event versus the beginning of event precipitation has to be considered we searched for the beginning of the event precipitation in a time window starting b days before the beginning of the flood event here b was defined as max ξ t peak t start 1 where ξ 0 the minimum time lag between beginning of precipitation and flood event can be chosen according to the catchment size during the time window t start b t peak the time series of accumulated precipitation was separated into two subsamples this change point is denoted by k t start b 2 t peak 2 for each of the two subsamples a linear regression is fitted with least squares p i α β j k t i with p i being the cumulative precipitation at time i and i j k fig 8 please note that each window had to have a length of at least 3 days otherwise a regression is not meaningful then the beginning of the flood inducing precipitation k was defined by the change point of the slope of the cumulative precipitation sum in this period the k for which the difference between the slope of both regressions between cumulative precipitation and time was largest k arg max t start b 2 k t peak 2 β k t peak β t start b k 1 a possible delay also applies for the end of the event here the end of event precipitation was defined as one day before the end of the flood event since pre flood precipitation may cause false starting points of event precipitation we compared the estimated beginning with an estimation by a twofold change point consideration of cumulative precipitation which works similar to the first method for this the precipitation time series p t start b p t end was separated into three sub sequences at the splitting points k 1 and k 2 then the point k 1 of the set k 1 k 2 for which β t start b k 1 β k 2 1 t end 2 β k 1 1 k 2 is smallest is chosen as the beginning of the event precipitation this criterion ensures that the second regression is steeper compared to the first and third parts and thus indicates the beginning of the precipitation event finally the results for the two methods were compared and we chose the beginning of event precipitation as the result that delivers the latest point in time before the beginning of the flood event fig 8 shows the estimation of the beginning of a precipitation event for the corresponding flood event for both methods one and two change point method for gauge aue 1 in the mulde basin in this example both approaches give different results the one change point approach defines the starting point at february 24th but the two change point method defines the starting point on february 27th in this case the second change point was chosen due to the smaller distance to the beginning of the flood event if the first change point would be considered only rainfall belonging to another flood event would be included the end of event precipitation was chosen as the day before the end of the flood event considering that direct runoff no longer occurs on the last day of the flood event 3 4 sensitivity of the parameters since we cannot refer to an objective truth a calibration and or validation on observed data is an impossible task workarounds can be used but are difficult and need special caution on the results and their interpretation therefore we used a sensitivity study to show the impact of different parameters on a reference separation which was carried out with manually adjusted parameters and resulted in a good separation quality additionally at the end of this section we introduce one possible goodness criterion which can be used for estimating parameters for new catchments or calibration validation in the sensitivity study the differences of the results between different parameter sets were evaluated for this we applied a reference parameter set and compared the results of the determined flood events with those obtained by other parameter combinations this is done by comparing day by day if any day belongs to a flood event for both sets of parameter choices as a result we obtain the proportion of days that belong to a flood event from one parameter set on flood days resulting from another parameter set one parameter set serves as reference set since it was checked according to the goodness i e a low number of manual corrections in the manual check the change of the respective proportion is then evaluated according to the change in the separation of the flood event for example separated flood events with the length of the moving window of variance d var 3 are compared with separated flood events with d var 5 d var 7 and d var 10 this is done twofold a the proportion of days that belong to separated flood events with d var 3 were compared to days that belong to separated flood events with d var 5 7 or 10 this is denoted as reference and b the proportion of days that belong to separated flood events with d var 5 7 or 10 were compared to days that belong to separated flood events with d var 3 varied this way it can be determined which parameter delivers longer and a larger number of separated flood events a schematic overview of the procedure is given in fig 9 two flood event separations with different choices of the parameter λ λ x and λ y are given with the black squares indicating days that are included in a separated flood event then the reference data set is calculated by counting all days that are included in a flood event for both parameter choices here this would be 12 from originally 22 days separated with λ x for the choice λ y originally 15 days were identified as part of a flood event from which 12 are identical to days identified with λ x this leads to reference equal to 12 22 0 55 and varied equal to 12 15 0 8 these proportions are displayed in boxplots for each parameter in this case this means that 80 of the flood events identified with the varied parameter set are also detected with the reference parameter set but only 55 of the flood events detected with reference parameter set are detected with the varied parameter set this implies that the reference parameter set detects either more or longer flood events additionally the number of separated floods is also compared to further differentiate this implication here the number of each parameter choice is calculated and the relative change to the reference parameter is given with these two results duration as well as number of flood events can be compared for each parameter please note that a comparison of e g statistics of the duration of the flood events is not meaningful since the separated flood events include very different flood types short flood events with high peaks caused by short but heavy rain long duration rainfall events that have a high volume and long duration and also snowmelt floods comparing a mixture of these types by a characteristic like the duration or the volume does not lead to reliable results a comparison of the same flood event for every parameter set cannot be guaranteed since the number of separated events varies e g when changing the variance threshold parameter moreover the full spectrum of possible values for each parameter cannot be considered since separation of full time series for 86 discharge gauges for six parameters with small parameter variation is rather time prohibitive however by the selection of parameter choices given here the general tendency of sensitivity is captured since our interest lies in the separation of floods and not runoff events the final parameters are chosen with this purpose for the separation of runoff events a different parametrisation would be meaningful e g lower thresholds for variance 3 5 calibration of initial parameters as stated above a goodness of fit for flood event separation lacks a consistent and true data foundation to compare the simulated data with to still have an estimate of the goodness of the separation algorithm we introduce a measurement for the goodness of our flood separation by explicitly maximising the number of flood events of high discharge while minimising the number of small runoff events included in the separated sample with the criterion 4 g sep q q t h upper f l o o d q q t h upper max q q t h lower f l o o d q q t h lower t o l lower 0 where q q t h upper is the number of days with discharge above the threshold t h upper and q q t h upper f l o o d is the number of days with discharge above the threshold which are separated as floods hence between the beginning and the end of a flood event separation the same applies to the second bracket but all days with discharge below a certain threshold are summed additionally the tolerance threshold to l lower allows for a certain percentage of flood events below t h lower to obtain more flexibility in the optimisation the range of the goodness therefore lies between 0 and 1 where 1 indicates that all days with discharge above the upper threshold are separated as flood events while a maximum of to l lower is separated as flood events below the lower threshold this ensures that large discharge values are separated while only a small amount of low discharge values e g in low flow periods are separated as flood events the three thresholds are set to the 95 quantile of discharge for the upper threshold and 50 quantile of discharge for the lower threshold while the tolerance is defined as 1 of the number of discharge days below the lower threshold keep in mind that even the three parameters are set a priori they remain an arbitrary choice and can be adjusted by the user if required this goodness measurement is also implemented in the floodr package with an evolutionary search algorithm mebane and sekhon 2011 for an optimisation of parameters users can apply this method to define initial parameters for the flood event separation 3 6 comparison to existing methods as stated above there exist several approaches to separate runoff events however many do not have the focus on floods and the application of flood statistics but on every temporally limited increase of runoff other methodologies depend on a chosen threshold defining a flood but not by the beginning and end e g peak over threshold to obtain a methodology comparable to the approach proposed here which focuses on flood events and their use for flood statistics and defines a flood event according to the variability of the discharge a combination of a flood event identification and the definition of beginning and end is required in the following we compared the results of such a separation method with the results of the method we propose using the parameter choices identified as suitable in section 4 here the flood events obtained directly from the algorithm and therefore without manual inspection were used to ensure a fair comparison since we focus on flood events only we compared the three approaches concerning the number of separated flood events the number of flood events per year and the value of the smallest peak of all separated flood events the number of separated flood events per year should be reasonable compared to the expected number of floods in a year the number of flood events per year is an often used criterion for flood statistics especially peak over threshold pot e g in germany austria and the netherlands on average 2 3 floods per year can be observed blmfuw 2011 rijkswaterstaat waterinfo 2020 of course this number varies for different climatic regions sometimes it is even used to define flood events e g by defining that in average 3 flood events per year should be included in the pot statistics kundzewicz 2019 mangini et al 2018 here it was used to evaluate if the separated events can be classified as floods too great a number of events per year indicates that not only floods but also short increases of runoff have been separated as flood events very often baseflow estimation approaches are coupled with flood identification e g annual maxima to obtain flood event separation e g graszkiewicz et al 2011 bezak et al 2015 shao et al 2020 here we will use three different flood identification thresholds if the daily discharges exceed such a threshold we can assume that a flood event occurred beginning and end of the flood event then can be identified by separation techniques the three thresholds are based on the pot approach where the exceedance of the threshold defines a flood event in pot statistics such an event would be considered for the sample we varied the threshold between three common values 2 and 3 times the long term mean discharge mq and the minimum values of the annual maximum series ams fischer and schumann 2016 these three thresholds are then used to identify flood events beginning and end of the identified flood events are estimated twofold on the basis of digital filters the lyne hollek dfm filter lyne and hollick 1979 filter was used to estimate the baseflow of the daily discharge time series the parameter was set to 0 925 according to nathan and mcmahon 1990 and eckhardt 2008 this is a common choice and was derived by the correlation between the baseflow indices derived using the digital filter and smoothed minima techniques each time the discharge exceeded the estimated baseflow the beginning of a flood event was defined see e g oppel and mewes 2020 the flood event ended when the daily discharge no longer exceeded the baseflow and hence no direct discharge was present this technique is very common to separate direct discharge from baseflow see references above of course other filter techniques could be also applied the method proposed by tarasova et al 2018 first applied a baseflow separation based on digital filters and separated the direct runoff event according to additional separation criteria afterwards the aim of this analysis was to define rainfall runoff events coupled with the pot thresholds it can be applied to identify flood events the resulting separation for the wechselburg gauge can be found as supplement of tarasova et al 2018 the approaches thresholds as well as digital filters are chosen according to their reproducibility recession approaches often lack reproducibility like the approach of blume et al 2007 where no specific method for defining the beginning of the event is given in contrast the filter technique with the estimation approach by eckhardt 2008 can be reproduced easily the same holds true for the pot approaches which do not require any calibration the approach by tarasova et al 2018 is well documented and the given data set in the supplements can be used for direct comparison there already separated events are given 4 results 4 1 sensitivity analyses of the flood event separation method for the application of the proposed algorithm several parameters had to be chosen the window for the moving variance dvar the parameter for the variance threshold θ the three thresholds for defining the beginning of the flood event η γ and κ and two parameters δ and ω to specify the end of the flood event the parameter d dur to specify a time window for superpositioned flood events moreover for the separation of the flood inducing precipitation the buffer b had to be defined by parameter ξ for the parameter d var it is clear that the longer the window for variance the less coherence of days of flood events the term coherence here refers to the number of days that were identified as part of a flood event for both parameter sets either in the case of reference or varied scenario fig 10 can be interpreted as follows the mean coherence of reference decreases with increasing parameter that is the number of flood days identified by dvar 3 is not equal to those days identified by e g dvar 5 however since almost all days identified by dvar 5 as part of a flood are included in dvar 3 see the proportion for varied it can be concluded that the flood events derived by dvar 3 have to be longer than those derived by dvar 5 the number of flood events became smaller 16 1 table 1 and the duration became shorter compared to dvar 5 fig 10 which is expected since a larger window for the variance compensates for small time windows of increased variance if the remaining days show less variability hence an increase of variance for shorter flood events is smoothed out for mesoscale catchments this is a disadvantage and the choice of three days as window length is kept for a time window shorter than three days a calculation of variance is not sensible and hence for our purpose of separating flood events and the mesoscale catchments considered here d var 3 is the best choice of course this choice is a compromise however results in fischer et al 2019 emphasize the goodness of this choice by demonstrating the applicability of the separated flood events for flood typology and flood statistics it is shown how the coherence of flood days is declining with increasing moving windows for large watersheds the coherence of days within flood events estimated with longer windows 5 or 7 days is in mean higher than for smaller watersheds fig 10 this may not affect the mesoscale catchments considered here much and the parameter can be kept constant but for macroscale catchments 10000 km2 an adjustment is meaningful please note that the classes of catchments sizes given in fig 10 were chosen here to ensure reasonable sample sizes of each class when increasing the threshold parameter of variances θ for the σ confidence band a clear reduction of the number of flood days can be observed fig 11 while for θ 0 5 the number of flood events was reduced by 1 3 compared to θ 0 25 and events became shorter for θ 1 in average only 25 of the events obtained with θ 0 25 remained resulting in less than one flood event per year table 1 however for increasing parameter almost all additionally separated flood events were included in the separation set using threshold θ 0 25 as the coherences showed no differences between watersheds of different catchment sizes visually and tested statistically applying the wilcoxon test to the proportion of coherences the selection of the θ 0 25 was scale invariant and applicable for all 86 watersheds fig 11 the same scale invariance was present for the remaining parameters the two parameters considered in the sensitivity analysis so far d var and θ are the most significant parameters and have the largest influence on separation compared to the remaining parameters only small changes of the resulting flood events results in table 1 and in the appendix an increase of these parameters leads to less flood events in mesoscale catchments since short increases in case of d var or small increases in case of θ of variance are ignored in flood event separation table 1 for macroscale catchments an adjustment is recommended since the omitting of such small increases of variance might be desirable for the remaining three choices of parameters defining the beginning and end of a flood event the results are presented in the appendix these parameters changed the number of flood events by less than 10 for most choices table 1 but had an impact on the length of all events the length of flood events can be adjusted by the threshold parameter for the natural variability eq 1 where we varied η between 0 05 and 0 3 the larger η the shorter the flood events fig a2 the number of flood events stays constant since the initial criterion of increased variance is not changed differences in the number of separated flood events mainly occurred due to different handling of superpositioned and superimposed flood events which is impacted by the length of the events due to this finding we chose η 0 1 the remaining two parameters defining the beginning are used to decide if a pre flood occurred that belongs to the flood event for this the criterion given in eq 2 is checked for the sensitivity analyses we varied the parameters γ between 2 and 8 days 1 day being the reference choice and κ between 0 1 and 0 6 where 0 4 was the choice made for the algorithm from the figs a3 and a4 it becomes obvious that the parameters have direct impact on the length of the flood event the larger the parameter γ the number of days to check for a pre flood the longer flood events are separated and the less events are found instead often floods are merged with the choice γ 1 for mesoscale catchments in the german climate pre floods are included that have direct impact on the flood event the smaller the parameter κ the more likely pre floods are included in the flood event however no further decrease of pre floods can be observed from κ 0 5 with the initial choice κ 0 4 we included pre floods as long as they have a contribution to the main event the last two parameters that were chosen for flood event separation have an impact on the definition of the flood event here we used δ in eq 3 to decide if the variation at the end of the flood event ω days is daily fluctuation or still belongs to the recession of the flood event the parameter δ was varied between 0 and 0 4 in the sensitivity analysis since this parameter is only a part of one of two criteria that have to be fulfilled its impact on the results is negligible in fact no difference in the flood event separation can be found in fig a5 for a few flood events however this parameter leads to shorter events the larger it is for about 2 of all flood events it omits erroneous long recessions of the events in the case where the end of the flood event cannot be found correctly due to long and smooth recession so we included it in the algorithm based on this we kept δ 0 2 according to the manual check variation of ω had almost no impact on the number of floods and omitted very long events fig a6 it is similar to δ concerning the impact on flood separation for mesoscale catchments two days for prolongation is a reasonable choice for definition of the event precipitation that is the time window of precipitation belonging to the respective flood event the only subjective choice made was the buffer time for the regression b more precisely the ξ day buffer in the maximum used to calculate b for sensitivity analyses ξ was varied between 3 and 11 days to analyse the impact on the resulting event precipitation this time window was chosen for mesoscale catchments where the reaction time will be less than 11 days however the catchment reaction time can vary depending on the antecedent soil moisture hence the buffer time window should not be too small the analysis method is analogous to the one described above for flood event separation applied to days where event rainfall is defined results for b can be found in fig 12 too short of a buffer led to shorter precipitation events too long of a buffer however did not change the beginning of the precipitation events at all so the algorithm is insensitive to an overestimation of the needed pre period of precipitation and with our choice of ξ 7 days we are acting conservatively finally the parameter d dur had to be chosen we decided to choose d dur 40 days meaning that we expect superimposed events to only occur in separated flood events of a duration of more than 40 days for flood events with smaller duration only superpositioned events occur since the basins considered here belong to the mesoscale this threshold was sufficient for our purpose please note that this decision only has a small impact on the results since for superpositioned as well as superimposed events a similar handling is performed the only difference is the amount of baseflow of the flood events in general this parameter can be estimated e g by evaluating the mean discharge and estimating the mean duration of increased discharge the sensitivity analyses for the parameter choices made in the algorithm demonstrate that our choices can be kept for the application of mesoscale catchments with different choices the length and number of flood events can be changed for our application in the manual check the parameter choices showed valid results i e a small number of manual corrections and hence were kept please note that a sensitivity analysis is not always required for application of the algorithm instead parameters can be adjusted according to the desired outcome for example parameter θ for the variance threshold can be chosen according to the desired number of flood events per year the larger θ the less events are detected the same holds true for d var the remaining parameters when applied for mesoscale catchments can be kept as suggested here or checked in the provided visual inspection tool 4 2 validation of the separation method additionally to the sensitivity analysis a validation was applied to test the goodness by applying eq 4 with the default parameters to the separated flood events and their discharge timeseries fig 13 shows the boxplot of the goodness of fit criterion for all catchments considered here grouped by basin with the parameter choices given above it can be seen that some catchments have large values close to 1 indicating that most of the days with discharge above the 95 quantile have been linked to a flood event while most of the discharge below the median discharge is not separated as a flood event most of the basins respectively their catchments have a goodness between 0 75 and 0 8 the inn basin has catchments with a separation goodness of below 0 5 these are small mountainous and or glacier influenced catchments same holds true for the outliers in the ostharz and westharz basin where both outliers are catchments smaller than 30 km2 here the temporal resolution of daily discharges could be a drawback in the separation 4 3 results of the automated flood event separation the resulting flood events cover most of days of high discharges as already shown in fig 13 a more detailed evaluation of the discharge timeseries can be seen in fig 14 which shows the cumulative discharge for one small and one large catchment for each basin it is demonstrated that the separated flood peaks all lie above the 95 quantile of all daily discharges the days that lie above this threshold but do not belong to a flood peak are mostly those that are separated as part of the flood event and most probably lie in close temporal range to the peak no correlation between the amount of separated high discharge days and the catchment size can be observed since no objectively verifiable flood event separation exists an objective evaluation of the algorithm is not possible one possibility is to change perspective and determine which flood events are not separated correctly this can for example be achieved by using the runoff coefficient which is here defined as the discharge volume of the flood event divided by the rainfall volume of the corresponding rainfall event a runoff coefficient above 1 indicates that the flood separation could have not worked here of course data uncertainty can lead to wrong conclusion but it is a good first estimator for the goodness of the separation method also the missing snowmelt for the calculation of the amount of flood inducing water respectively the runoff coefficient is assumed to produce runoff coefficients above 1 for winter and spring floods especially in the mountainous inn and ost westharz catchments fig 15 shows the runoff coefficient for one small and one large catchment for each basin differentiated by month of peak occurrence it can be seen that the runoff coefficients for the lowland basins cloppenburg and lueneburg are comparably small and low in variation the other basins which are all affected by mountains and snowmelt processes have larger runoff coefficients and a clear monthly pattern emphasizing that so far snowmelt is not included in the calculation of the runoff coefficient here estimations of snowmelt with rainfall runoff models and inclusion of this in the runoff generating amount of water could help correct these runoff coefficients for the total of 2648 flood events in fig 15 151 runoff coefficients can be observed above 1 of which all but 3 occur in the winter and spring season between december and may one flood event inn basin with runoff coefficient equal to 1 22 is influenced by glaciers while the other with runoff coefficients of 1 05 and 1 27 are flood events occurring in august here precipitation and discharge measurements do not correlate and indicate faulty data or very local precipitation events not captured by the precipitation gauges however since we use the manual correction afterwards we can also compare the flood events before and after correction we can assume that the manual correction is very close to the true event separation since hydrological expert knowledge was used this is similar to the assumption made by thiesen et al 2019 who assumed flood events identified by an expert as truth hence we compared the proportion of corrected events for each gauge this is the number of manually corrected flood events divided by all separated events we differentiated between the basins to find systematic regional differences and used the parameters identified in section 4 as meaningful for flood separation the results can be seen in fig 16 in general between 40 and 70 of all flood events had to be corrected manually the fewest correction of all basins was done for flood events in the mulde basin where mostly the end was shifted this is probable caused by the dendritic structure of the basin leading to less complex shapes of the hydrograph between all remaining basins no regional differences could be found since the separation algorithm resulted in a similar proportion of manually corrected flood events this implies that the algorithm works equally well for all basins and is not influenced by e g the topography however the number of corrected events is quite high and brings into question the applicability of the algorithm therefore we also had a look at what was changed and how it was changed by manual correction of the events in particular we compared the beginning and end date before and after correction due to baseflow increase over several months as described above fig 6b for some gauges one or two flood event beginnings or endings were shifted by 100 200 days this was the case only for 1 of all flood events that is why we decided to compare the median of the difference between old and new beginning and end for each catchment to reduce the impact of these single outliers the results can be found in fig 16 there in particular the beginning of flood events was estimated very accurately by the automated algorithm almost no correcting shifts had to be performed manually only one catchment in each of the basins cloppenburg ostharz and westharz had a median difference in beginning of more than one day whereas for inn lueneburg main and mulde in median no corrections of the starting point were necessary the basins cloppenburg westharz and ostharz are located in the central part of germany while ost and westharz are both affected significantly by luv and lee effects of the harz mountains causing e g a rain shadow for parts of the ostharz basin cloppenburg is a rather flat basin this implies that not the catchment characteristics or the topography affects the separation of the event beginning the three gauges in the basins where the beginning of the flood event was systematically estimated off target are with 17 km2 18 km2 and 72 km2 rather small but not the smallest gauges considered a rapid runoff reaction that cannot be handled by our approach cannot be the only reason for the wrong separation other possible impact factors might be the catchment shape soil types or a steep hypsometric curve yet no clear indication for the resulting deviation of the beginning of the flood has been found fig 17 demonstrates that the estimation of the end of a flood event is much more difficult this is in line with studies on event separation before blume et al 2007 however in median the end had to be corrected by one day only for inn and mulde the most mountainous basins in median no correction was necessary the very steep nature of the catchments leads to faster runoff generation and therefore a faster drainage of the catchment and a more abrupt end that is easier to detect in general the evaluation of the corrections shows that almost no significant correction had to be done manually the correction of the end of a flood event by one day does not lead to significant changes in the total volume not shown or duration fig 17 and does not change the peak at all it remains included in the event hence even without manual correction the algorithm delivers meaningful results the manual correction is important in cases where long flood events had to be separated with a duration of several weeks or even months in this case the baseflow behavior changed and the algorithm was not able to differentiate the variance of the flood event from the natural variance for this the manual check is required when adjusting the parameters for the variance threshold it is possible to also separate all runoff events instead of flood events only by reducing the threshold e g θ 0 1 a smaller increase of variance already indicates the beginning of an event this can be of advantage when rainfall runoff events are the purpose of the separation 4 4 comparison to other event separation methods many runoff event separation methods exist but only few can be designed for the separation of flood events to demonstrate the differences to the method proposed here a flood identification threshold is applied afterwards digital filters are used to estimate beginning and end of the corresponding flood event an example of one year of discharge is given in fig 18 the three pot thresholds that are used to identify flood events are given as horizontal lines for each identified event beginning and end is estimated with either the lyne hollek filter where a deviation of the discharge from baseflow indicates direct runoff or with the approach of tarasova et al 2018 for the latter the baseflow is separated with the straight line method as comparison additionally beginning and end of our method is indicated also by using a straight line representing the baseflow please note that our method is independent of the pot thresholds since flood events are defined directly within the algorithm here we used the flood events that were obtained directly from the algorithm meaning that no inspection by an expert was performed to make comparison fair for the given year four pronounced flood events are visible these flood events are detected by our method as well as the pot thresholds for the pot threshold equal to the minimum annual maximum discharge min ams additional 2 flood events have been identified and separated with both methods lyne hollek filter and tarasova et al similar behavior of the algorithms is also visible in fig a7 in the appendix when comparing the separation in terms of beginning and end it becomes obvious that the lyne hollek filter very closely follows the runoff variation this leads to comparably early ends of the flood events which are located clearly during the recession period of discharge sometimes rather close to the peak the approach of tarasova et al 2018 gives comparably longer flood events to our approach but sometimes splits long flood events fig a7 in general more of the pre event runoff is included to demonstrate the differences between the flood separation approaches in fig 19 the flood event duration and the relation between discharge at the end of the flood event and flood peak is compared for each of the methods for the daily mean discharges at wechselburg gauge germany in the years 1951 2011 the end peak relationship is an indicator for how early the flood end is defined it can be seen that the tarasova et al separation method leads to comparably longer duration of flood events despite of the flood identification threshold this can be explained by the double events that are separated by this approach for the double waves for details see tarasova et al 2018 the median duration is 14 days whereas it is 4 days for the single waves in combination this leads to longer flood event duration the lyne hollek filter gives comparably longer duration then our approach mostly due to an earlier beginning fig 18 when comparing the relation between discharge at the end of the flood event and peak discharge it can be seen that the approach of tarasova et al delivers similar results as our approach this implies that their definition of a flood event is similar to our definition the lyne hollek filter instead clearly defines the flood end much earlier for both approaches tarasova et al and lyne hollek filter also the variability of the results is much larger in summary the findings so far indicate that the approach of tarasova et al in combination with the pot thresholds separates flood events with similar definition of the end but shorter duration for single waves and longer duration of double waves compared to our approach the lyne hollek filter in combination with the pot thresholds instead separates flood events with much earlier ends further results are given in table 2 our approach delivered the smallest number of flood events per year compared to the other approaches used here with 2 98 the number of flood events per year is in a range for germany which could be related with a spring flood caused by snowmelt and or high soil moisture and rain at the end of the winter and one or two summer floods typical with a smaller volume but higher peak than the winter flood by adjusting the pot thresholds the number of flood events per year can be altered a comparable number of flood events per year is delivered when the flood is identified with pot approach and the threshold was chosen as 3mq in this case however the peak of the smallest flood detected was much higher with 79 2 m3 s compared to our results of 51 8 m3 s this can be explained by the ability of our approach to detect multiple peaked events these flood events are separated by the pot approach together with the separation by lyne hollek respectively tarasova et al into single peaks and would increase the number of flood events with higher peak discharges this problem of the pot approach can be overcome by combining pot with criteria for independence between flood peaks similar to the criterion by klein 2009 used for our approach where the splitting of a multiple peak event can be omitted lang et al 1999 in general it could be seen that the pot approach can be used to control the number of flood events that is the higher the threshold the more the number of flood events converges to a limit at the same time the use of such a threshold without considering runoff dynamics could lead to the situation that the same flood can be considered as flood or not depending if it occurs at a period with high or low baseflow the results emphasize that our approach delivers similar results as the often applied combination of flood detection thresholds and digital filters however it has the advantage that not a constant threshold has to be defined but instead the runoff dynamics are considered hence the splitting of flood events occurs less often 5 conclusion we propose a new automated statistical algorithm for flood event separation that is based on an analysis of the variance of the daily discharges the aim was to separate flood events that can be used for flood statistics this means only flood events should be separated and their respective characteristics like peak volume and duration should be obtained moreover the associated flood inducing precipitation event was identified additionally for an easy manual check as an intermediate step of the procedure a helpful interactive application is provided especially for use in distinguishing very long flood events with multiple peaks with this two step procedure large data sets can be analysed easily and flood events along with their characteristics such as volume duration event precipitation runoff coefficient and baseflow at beginning and end of the flood event can be obtained for further analyses e g multivariate flood statistic or rainfall runoff modeling an application to catchments in germany and austria of significantly differing natural conditions demonstrated the transferability of the approach to varying types of catchments alpine mountainous or lowlands the parameters in the algorithm were chosen to be suitable for mesoscale catchments the results indicate that the algorithm separates flood events reliably for all catchments the manual check adjusts separation of flood events of long duration and with multiple peaks caused for instance by several rainfall events occurring in short chronology for most flood events a manual adjustment is not necessary in a sensitivity analysis the choice of parameters that define the beginning and end of the flood event and the corresponding precipitation are evaluated the choices were made for mesoscale catchments with the proposed calibration method initial parameter choices can be made by the applicant the method proposed here overcomes the three drawbacks that most separation methods have namely the reliance of baseflow separation for identification of beginning and end of flood events the requirement of large samples of already separated events as needed e g for machine learning based methods the inability of identifying flood events also during low baseflow dry conditions it is important to notice that by changing the respective parameters the proposed method can be adjusted to also separate runoff events and to be applicable to hourly data and or macroscale catchments as long as these are not affected in their runoff dynamics e g by dams this is in contrast to existing methods that are mostly limited to one of these cases and cannot be adjusted to be applicable to other cases yet there are still some model assumptions in the proposed algorithm that are based on subjective choices though these choices can be altered by the user they remain impacted by personal experiences the same holds true for the starting values of the parameters for optimization the proposed expert validation is not required but highly recommended at least for small data sets credit authorship contribution statement svenja fischer methodology writing original draft visualisation andreas schumann writing review editing philipp bühler software visualisation declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements the financial support of the german research foundation deutsche forschungsgesellschaft dfg in terms of the research unit spate for 2416 is gratefully acknowledged moreover the authors would like to thank two anonymous reviewers and stephanie thiesen for their valuable reviews that helped to improve this paper the discharge data for the mulde basin are provided by the state water authority lfulg saxony at www umwelt sachsen de umwelt infosysteme ida for the basins lueneburg cloppenburg and westharz the discharge data are available by the nlwkn of lower saxony at www wasserdaten niedersachsen de cadenza for the main basin and the german parts of the inn basin the discharge data are available at www lfu bayern de wasser wasserstand abfluss while the discharge data for the austrian catchments of the inn are available at ehyd gv at discharge data of ostharz have to be requested from https www umwelt sachsen anhalt de daily time series of precipitation for germany are available from the german weather service dwd at the climate data center cdc cdc dwd de portal respectively from ehyd www ehyd gv at for austria all r codes are available at www github com philippbuehler in the package floodr appendix fig a1 displays 12 separated flood events with highest increase of discharge it can be seen that they mostly consist of extreme peak events or snowmelt events in the following supplemental figures for the sensitivity analysis in section 4 are given fig a7 displays a second year of flood event separation and can be used in addition to fig 18 to compare the different approaches additionally to the results presented in the manuscript the sensitivity of the flood separation algorithm to the temporal resolution of the discharge data was tested for this hourly discharge data were used for the longest time period available in average 15 years for each gauge were available since hourly data is not the primary focus only a short evaluation if the method is capable of separating hourly data has been done for this the flood events derived from daily data at the golzern gauge in the mulde basin between the years 2008 and 2014 have been used an optimisation function bobyqa which is a derivative free bound constrained optimisation has been used to maximise the overlapping between daily and hourly flood events while minimising the non overlapping similar to the sensitivity analysis with a reference and varied parameter set of the 22 flood events derived from daily data 17 events have been separated from hourly values with reasonable beginning and end also additional 16 events have been found which is due to the more variable nature of hourly discharge the most relevant parameters for the event separation with hourly data dvar and θ had their optimal parameters at 14 5 hours and 0 33 respectively while these are promising results from just a simple optimisation a complete evaluation is beyond the scope of this paper the method should not be easily transferred from daily to hourly values because of the different behavior of the runoff data 
8581,anthropogenic activities are altering flood frequency magnitude distributions along many of the world s large rivers yet isolating the impact of any single factor amongst the multitudes of competing anthropogenic drivers is a persistent challenge the usumacinta river in southeastern mexico provides an opportunity to study the anthropogenic driver of tropical forest conversion in isolation as the long meteorological and discharge records capture the river s response to large scale agricultural expansion without interference from development activities such as dams or channel modifications we analyse continuous daily time series of precipitation temperature and discharge to identify long term trends and employ a novel approach to disentangle the signal of deforestation by normalising daily discharges by 90 day mean precipitation volumes from the contributing area in order to account for climatic variability we also identify an anthropogenic signature of tropical forest conversion at the intra annual scale reproduce this signal using a distributed hydrological model vmod and demonstrate that the continued conversion of tropical forest to agricultural land use will further exacerbate large scale flooding we find statistically significant increasing trends in annual minimum mean and maximum discharges that are not evident in either precipitation or temperature records with mean monthly discharges increasing between 7 and 75 in the past decades model results demonstrate that forest cover loss is responsible for raising the 10 year return peak discharge by 25 while the total conversion of forest to agricultural use would result in an additional 18 rise these findings highlight the need for an integrated basin wide approach to land management that considers the impacts of agricultural expansion on increased flood prevalence and the economic and social costs involved keywords deforestation flooding hydrological modelling flow regime stormflow watershed management 1 introduction global climate change and anthropogenic activities are disrupting flood frequency magnitude distributions along many of the world s large rivers posing threats to human populations and critical infrastructure many competing drivers contribute to modify a river s long term discharge pattern land cover and land use change through deforestation and agricultural expansion urban expansion water abstraction for human consumption and irrigation channel modifications and infrastructural developments as well as shifting precipitation patterns due to climate change bruijnzeel 2004 júnior et al 2015 malmer 1992 each of these drivers contributes its own signal to a river s hydrograph by altering the timing duration and magnitude of peak flows yet evaluating the relative impact of any one of these drivers in isolation is a persistent challenge due to their interconnectedness and constantly varying climatic conditions rogger et al 2016 van dijk et al 2009 the signal derived from forest conversion is particularly difficult to identify and quantify as large scale land cover change historically takes place over long periods amid many competing and overlapping developments and often during times with spatially sparse climate and discharge data bruijnzeel 1990 1993 forest removal changes the pathways of precipitation through altering the processes of interception the proportion reaching the surface infiltration the partition of surface and subsurface and retention the proportion reaching the river network after infiltration replacing dense tropical forest canopy with short vegetation grass shrubs or crops significantly reduces interception losses spracklen et al 2018 compacts soil and removes organic material reducing infiltration rates germer et al 2010 muñoz villers and mcdonnell 2013 and dramatically decreases evapotranspiration rates particularly in the humid tropics spracklen et al 2018 zhang et al 2001 by reducing interception losses evapotranspiration and the infiltration rate of soils the removal of forest increases the proportion of rainfall entering the river network and the rate at which it reaches the network thereby increasing the potential for large scale flooding brown et al 2005 bruijnzeel 1989 fritsch 1993 whilst this has long been understood on the conceptual level blackie and edwards 1979 clark 1987 gilmour et al 1987 and many studies that relate tropical forest conversion to increases in mean river discharge exist at the small experimental scale 1 km2 and lower mesoscale 10 km2 there are relatively few studies that examine the effects at the meso or large scale 100 km2 bruijnzeel 1990 1997 costa et al 2003 fritsch 1993 in addition due to poor data coverage the relative inaccessibility of the study areas and the temporal and spatial heterogeneity of competing factors a number of early large scale studies reported inconclusive or contradictory findings dyhr nielsen 1986 gentry and lopez parodi 1980 qian 1983 richey et al 1989 wilk and plermkamon 2001 despite the complexities and complications that large scale studies of this type have to contend with there is a growing body of research that examines the hydrological impact of tropical forest conversion at the river basin scale brown et al 2005 dias et al 2015 gao et al 2020 gerold and leemhuis 2008 júnior tomasella and rodriguez 2015 karamage et al 2017 molina et al 2012 recha et al 2012 robinet et al 2018 sahin and hall 1996 van der weert 1994 the overwhelming majority of these studies conclude that large scale deforestation increases annual water yields and mean discharges yet the large scale impact on storm flow generation storm flow pathways and how these pertain to flood generation is less clear bruijnzeel 2004 robinet et al 2018 at the global scale bradshaw et al 2007 undertook an analysis of flood severity and found that deforestation increases flood risk in tropical regions yet a subsequent analysis suggested that 83 of the variation may be accounted for by population density as a root cause rather than a direct causal link with removal of trees van dijk et al 2009 it is this persistent challenge of disentangling landscape responses to tropical forest conversion from competing anthropogenic and climate signals that has prohibited the systematic quantification of the impact forest conversion has on long term flood risk bruijnzeel 2004 van dijk et al 2009 here we attempt to fill this gap by clearly demonstrating the impact of forest conversion on peak discharge generation in the absence of competing anthropogenic influences and accounting for climatic variation between our reference time periods to significantly clarify the role tropical forest plays in large scale flooding this is an important contribution as lacking a clear understanding of the role of forest cover in preventing future flood disasters has complicated the planning and implementation of effective land management strategies together with feasible flood mitigation policies calder and aylward 2006 fao and cifor 2005 2 data and methods here we analyse 55 years 1959 2014 of climate and discharge data from the usumacinta river sub basin in southeastern mexico which provides an opportunity to study the hydrological impact of large scale tropical forest conversion in isolation the past 20 years have seen an increase in the severity of flooding in the states of tabasco and chiapas in southeastern mexico atreya et al 2017 gama et al 2010 2011 which encompass the connected river basin of the grijalva and usumacinta rivers unlike the grijalva sub basin which has several large hydropower dams along the main channel the flow of the usumacinta sub basin is unobstructed for the entirety of its course from the guatemalan highlands to the gulf of mexico the absence of large urban settlements and the relative inaccessibility of the terrain stifled development and large scale forest conversion until the 1990 s meaning the 55 year discharge record from 1959 to 2014 captures the hydrological impact of deforestation on an otherwise consistent landscape to identify and quantify an anthropogenic signal of tropical forest conversion in the discharge record of the usumacinta river we analysed continuous daily time series of precipitation temperature and discharge to compare long term trends and accounted for climatic variability between reference periods by examining the ratio of daily discharge to 90 day precipitation totals from across the contributing area we then successfully reproduced this signal using a distributed hydrological model to simulate the response of the study area to wide spread forest conversion vmod lauri et al 2006 and evaluated both the historical and future impact agricultural expansion has on the flood frequency magnitude distribution along the usumacinta river in this paper we employ a novel approach to disentangle climatic variability from within the daily discharge record at the intra annual scale and contribute to the wider global scale debate concerning the role of forests in controlling large flood events especially in the humid tropics 2 1 study area the grijalva and usumacinta are the largest rivers in mexico day et al 2003 both begin in the highlands of guatemala and traverse the states of chiapas and tabasco before converging in the low lying floodplains just 50 km from the gulf of mexico the combined grijalva usumacinta river basin covers some 130 000 km2 and produces mean monthly discharges between 3000 and 6000 m3 s equivalent to around 30 of the total surface runoff of the country areu rangel et al 2019 the grijalva and usumacinta sub basins are separated both hydrographically and socio economically as the grijalva sub basin was the historical focus of development within the region leaving the usumacinta sub basin relatively undisturbed until the 1980s villela and martínez 2018 as such the main population centres areas of industrial oil and natural gas extraction projects of irrigated agriculture and four hydropower dams are all concentrated within the grijalva sub basin however an intensification of agricultural development projects since the 1990s has seen rapid large scale conversion of natural forest cover across much of the usumacinta sub basin driven by agricultural expansion that extends into many of the headwater regions our study focuses on the usumacinta sub basin measured at the boca del cerro gauging station located on the main branch of the usumacinta river outside tenosique fig 1 which is the main urban centre along the river course home to around 50 000 inhabitants ine 2005 we identify four periods within the discharge and climate records of the usumacinta sub basin that correspond to distinct stages of development and draw comparisons between these land cover signatures 2 2 discharge data our analysis of the usumacinta flow regime is based on discharge data collected by the servicio meteorológico nacional smn under the comisión nacional del agua conagua of mexico we use data from the boca del cerro gauging station 30019 fig 1 which is the only station within the usumacinta sub basin to provide a continuous record of average daily discharges m3 s across the entire study period 1959 2014 2 3 climate data observations of daily precipitation maximum temperature and minimum temperature were available at 15 weather stations that collectively form a continuous spatially robust dataset spanning the period 1959 1992 these data were provided by the global historical climatology network ghcn menne et al 2012 and garcía 1977 we infilled a spatial data gap located within guatemala using additional data points taken from the princeton university global meteorological forcing pgf v 1 sheffield goteti and wood 2006 formed of a suite of global observation based datasets with the ncep ncar reanalysis we applied a bias correction to these data against ground data using a multi variable scaling method santander meteorology group 2015 wilcke et al 2013 after 1992 there are few ghcn stations within the usumacinta sub basin that record daily climate observations therefore we supplemented these with a combination of the tropical rainfall measuring mission trmm v7 satellite derived precipitation data and the climate prediction centre cpc global daily temperature data for the period 1999 2018 to preserve model calibration across all time periods we sampled the gridded data at points corresponding to the observation stations used in earlier iterations we used daily composites of the rain gauge adjusted 3 hourly 0 25 degree trmm product 3b42 kummerow et al 1998 that has been shown to reliably reproduce rainfall in the humid tropics and has been used extensively in climate analyses and model forcing ferreira et al 2012 ji 2006 lauri et al 2014 shrivastava 2014 tapiador 2017 wu et al 2015 a comparison of trmm data with gauged observations taken within the grijalva river sub basin showed a strong correlation particularly at monthly timescales nse 0 6 0 82 with 30 day moving average cpc daily temperatures combine the ghcn observation dataset and the climate anomaly monitoring system cams dataset and interpolate them across a 0 5 degree grid fan and dool 2008 which has proven a reliable forcing for climate models nashwan 2019 comparing measurements of max and min temperature to ground observations across the wider grijalva usumacinta river basin for the period 1998 2003 we found a consistent negative bias by the cpc dataset which we again corrected using a scaling factor santander meteorology group 2015 wilcke et al 2013 for our analysis of climate trends across time periods we used spatially averaged data interpolated across the study area from point data used as our model inputs 2 4 model description and set up the environmental impact assessment centre of finland s eia integrated water resources management modelling tool iwrm vmod is a physically based hydrological model distributed across a square grid representation that couples sub models resolving energy and water balances at the grid scale with a 1 d river channel network model that routes outflows between cells vmod first constructs a grid mesh over layered raster inputs representing elevation m soil type flow direction and land cover class it then interpolates daily climate data for each grid cell from input data at discrete points max and min temperature c precipitation mm and calculates potential evapotranspiration pet using the hargreaves samani method hargreaves and samani 1982 before solving energy and mass balances across two subsurface soil layers and surface atmosphere transfers following dingman 1994 runoff generated from each cell is then routed through a 1 d river channel network to give discharge outputs which are calibrated against historical records for a detailed description of the model construction and the governing equations see lauri et al 2006 for elevation data we used srtm 90 m jarvis et al 2008 from which the model inferred flow direction data and the river channel network which we adjusted to ensure alignment with satellite imagery we prepared soil data from the fao world soil map fao 2009 by reclassifying the original classifications into six classes with default parameterisations but later amended these as part of the calibration process we then defined four periods each with a distinct land cover signature ranging from lc1970 with almost total forest cover to lc2014 with just 42 dense forest cover across the study area lc1970 land cover map was inferred from the international satellite land surface climatology project s islscp ii ramankutty et al 2010 historical land cover maps for 1950 and 1970 that showed little deforestation outside of the area around tenosique which corresponds with accounts of land use described in tudela 1989 lc1992 land cover map corresponds to land use classifications described in the central american vegetation land cover classification and conservation status 1992 1993 ccad 1998 lc2004 and lc2014 land cover maps were derived from modis land cover classifications mcd12q1 from 2004 and 2014 respectively friedl and sulla menashe 2019 each of the land cover maps were reclassified from the original interpretations into five classes water forest rain fed cropland pastureland and urban we then aggregated each of the raster inputs to match the model s grid sizing which we set to a resolution of 2 5 2 5 km 2 5 model calibration validation and testing as the focus of this study is to assess the impact of forest conversion on the hydrological regime of the usumacinta river we calibrated our model using 4 periods with distinct land cover signatures to distinguish between behaviours driven by soil type characteristics and those driven by vegetation dynamics our initial calibration was against discharge data for the period 1978 1985 assuming a ubiquitous forest cover although this assumption contradicts our land cover map lc1992 this period has the most abundant and reliable climate data needed for a robust calibration of the soil parameters controlling the timing of runoff we used the period of 1968 1973 as a validation of the initial calibration in a cyclical process to identify the interactions of soil and vegetation effects to best approximate the parameters for each soil type and the forest land cover class across both periods which we then tested against the period 1959 1966 having thus calibrated the soil type and forest land cover parameters we applied the model to the period 2008 2014 where variations in model performance stem entirely from forest land cover alterations assuming consistent soil parameters across time periods whilst maintaining the model calibration from the initial periods 1959 1985 we introduced two additional land cover classes rain fed cropland and pastureland to correct the model deficiencies using 2003 2007 as a validation period for a repetition of the cyclical calibration process this final model calibration including the non forest land cover classes was tested against the periods 1986 1992 and 1999 2003 the initial calibration phase 1968 1986 required all major parameters to be adjusted but primarily focused on hydraulic conductivities horizontal and vertical directions soil layer depths storage capacities weather interpolation values surface runoff coefficients and computational grid values the second phase 2003 2014 focused solely on defining the vegetation characteristics of the non forest types primarily the evapotranspiration and interception parameters as well as the surface model components pertaining to vegetation differences for the calibration phases we used the nash sutcliffe efficiency coefficient nse nash and sutcliffe 1970 as the objective function we then assessed the overall model performance against observed discharge by comparing relative biases of the total annual flow low flow and high flow indices i e the 95th and 5th percentile of the discharge record respectively as well as comparing the mean monthly discharges and distributions of annual maximum minimum and mean discharges across the entire discharge record 1959 2014 2 6 assessing hydrological and climatic changes between periods to assess changes in the hydrological regime of the usumacinta river we first analysed trends in the long term annual maxima minima and mean discharges for the duration of the discharge record we repeated these analyses for mean annual temperature data as well as total annual and seasonal precipitation data where we defined the wettest season as june november and the drier season as december may to examine changes in the intra annual flow regime we divided the discharge and climate records into four periods for comparison each representative of a distinct land cover signature lc1970 used for period 1959 1973 lc1992 1978 1992 lc2004 1999 2007 and lc2014 2008 2014 we compared the mean day of year doy discharges temperatures and precipitation 30 day totals for each period against the long term means across the period of the entire discharge record finally to determine whether variances in mean discharges between lc periods are attributable to alterations in water availability we normalised mean day of year doy discharges by average 90 day precipitation totals scaled by contributing area in the case of lc1970 which has a ubiquitous covering of vegetation the discharge recorded at the boca del cerro gauging station will be directly proportional to the amount of precipitation fallen within the study area less interception evapotranspiration and changes to groundwater storage and alterations to flow due to extraction or dams not applicable in the usumacinta interception evapotranspiration is a function of temperature controlled by vegetation characteristics and changes to groundwater storage can be assumed negligible when summed over multiple years therefore normalising monthly discharge by precipitation totals scaled by area should reveal a consistent proportionality that reduces intra annual variation such that 1 nd i 1 n 1 n d i n 1 n a 1 n 1 90 j i 90 j i p j n where nd i is the normalised discharge for the i th day of the year n is the number of years in the observation record d i n is the discharge m3 s on the i th day of the n th year a is the contributing area m2 and p j n is the precipitation total mm day on the jth day of the n th year nd i then represents the dimensionless after unit conversion factors proportion of discharge to the average amount of water fallen over the entire study area as precipitation in the previous 90 days we find that 90 day precipitation totals are most suitable for removing intra annual variation to consistent proportionalities in this study area were the periods 1978 1992 lc1992 1999 2007 lc2004 and 2008 2014 lc2014 to maintain the same ubiquitous forest cover then the discharge record at the boca del cerro gauging station should display the same proportionality to precipitation as displayed in lc1970 assuming a consistent temperature distribution across time periods any variation from the intra annual normalised discharge pattern displayed in lc1970 will be the result of alterations to the vegetation effects controlling interception evapotranspiration and thus represents an anthropogenic signature of forest conversion to agricultural expansion 2 7 forest conversion scenarios to assess the impact of potential future forest conversion on the hydrological regime of the usumacinta river we developed scenarios that progressively removed forest area according to observed historical patterns projected into the future lc1970 lc1992 lc2004 and lc2014 represent 98 87 3 73 3 and 42 1 forest cover respectively we randomly converted forested pixels from the initial land cover map lc1970 sampled from areas later converted to either crop agriculture or pastureland as displayed in lc1992 lc2004 and lc2014 maintaining the observed proportions of each as displayed in lc2004 the most reliable partition of land classes to infill the proportion of non forest cover for 100 50 forest cover scenarios after which the forest conversion was entirely randomised for 25 and 0 forest cover scenarios we then used these progressive land cover maps to run forest conversion model scenarios across the most complete record of climate forcings 1999 2018 this allowed us to draw direct comparison of the mean doy discharges and hydrological extremes under different projections of forest conversion and to assess the likely impact of continued agricultural expansion on severe flooding along the usumacinta river and by extension the grijalva river 3 results 3 1 long term climate trends and hydrological signals using interpolations of daily climate data and discharge data spanning the entire 55 year study period 1959 2014 we found statistically significant positive trends for the mean annual temperature p value 0 01 fig 2 a and annual maximum mean and 10th percentile low flow discharges p values 0 1 0 04 and 0 01 respectively fig 2c we found no statistically significant trends in neither the total annual precipitation fig 2b the total drier season precipitation nor the total wet season precipitation across the years from a comparison of the mean daily discharges taken for each of the land cover classification periods lc1970 lc2014 there are considerable increases in the first wet season peak jun aug and again in the second peak sep nov fig 3 a whilst the drier season flows look comparatively stable across the periods the proportional increases from the historical base discharge show statistically significant drier season gains positive 7 60 fig 4 a which is not evident in the drier season precipitation between time periods fig 4c the largest proportional change comes at the onset of the wet season in june where mean monthly discharges are 75 larger in lc2014 compared to lc1970 fig 4a whilst in terms of magnitude discharge increases in october are equivalent to those in june with lc2014 exhibiting a mean monthly discharge 900 m3 s larger than that for lc1970 to characterise the different landscape responses under each of the land cover classifications independently of the varying climatic conditions we compared the ratio of daily discharge totals at the mouth of the study area to the mean daily precipitation to have fallen across the entire study area for the previous 90 days the results show an average proportionality between 0 and 1 that is relatively consistent throughout the year fig 3d when compared to the variation displayed in the precipitation totals fig 3c however this proportionality alters dramatically between early and later land cover classifications with lc2004 and lc2014 showing changes of up to 58 may from those of lc1970 fig 4c predominantly in the drier season months where the ratio of mean daily discharge to precipitation has risen from 0 45 to consistently above 0 7 fig 3c 3 2 hydrological model calibration and validation both the initial calibration phase that concentrated on soil characteristics and forest cover parameterisation 1978 85 and the later calibration phase that focused on defining the non forest land cover types 2008 14 display good agreements with observed data each obtaining an nse of 0 80 table 1 the test phases which were not included as part of the calibration procedure display nses of 0 64 and 0 74 respectively the slightly poorer fit to the older test period may in part be due to the reliability of the forcing climate data which was sparser and contained a number of data gaps making the interpolation less consistent overall the modelled discharge series displayed a robust agreement to the observed data set with an nse of 0 76 table 1 in addition to consistently performing better than the sample mean indicated by the nse an important component of hydrological modelling is the faithful reproduction of key aspects of the regime comparing the flow duration curve fig 5 b the distribution of annual means fig 5cmiddle lines and the mean daily discharges fig 5d the model performs well reproducing the distribution of flows characteristic of the usumacinta river however as both the measures of bias table 1 and the distributions of maximum and minimum discharges fig 5c top and bottom attest the model tends to marginally underestimate high flows q5 and overestimate low flows q95 for our purposes the most important component of the model is the representation of hydrological processes affected by forest cover and the impact of forest conversion to agricultural land use on the hydrological regime as the differences in discharge records between land cover classification periods are driven by the combined effects of climatic variability and landscape dynamics normalising the discharge record by variations in climate should reveal a signal of forest conversion to agriculture this signal is present in the observed discharge record where lc2004 and lc2014 display fundamentally different behaviour with respect to the proportion of water reaching the study area outflow fig 4d this same signal is present in the model results fig 6 a which suggests that the model represents the impact of vegetation cover on the hydrological cycle adequately and the effect of forest conversion on discharge re running the model with a homogeneous forest land cover class whilst maintaining the original climate input data produces results that display a more uniform proportionality where the ratio of daily discharge to mean precipitation across the study area more closely resembles that of the historical base case lc1970 fig 6b 3 3 hydrological analysis under future forest conversion scenarios to investigate the impact of forest conversion to agricultural land use on the hydrological regime and flood magnitude frequency distribution of the usumacinta river we ran the calibrated model using climate data from 1999 to 2018 under different scenarios of forest cover representing 100 50 25 and 0 forest cover as well as the 2018 land cover classification we found that each successive forest cover scenario shows a clear increase in discharge throughout the year fig 7 a though the increase is not uniformly proportional fig 7b drier season flows dec may display a larger proportional increase with decreasing forest cover compared to the wet season jun nov the 2014 land cover scenario has a forest cover of 42 yet exhibits discharge patterns that more closely resemble fc25 25 forest cover scenario than fc50 in the drier season months this is most likely due to the proportion of cropland compared to pastureland represented as we maintained the cropland pastureland ratio observed in lc2004 throughout forest cover scenarios while the observations of lc2014 identify a smaller proportion of cropland lastly we fitted a generalized extreme value gev distribution function to the annual maxima for each of the forest cover scenarios including the additional fc625 scenario with 62 5 forest cover to ascertain to what extent agricultural expansion has increased the expected return flood historically and to what extent it is likely to increase it in the future we found that the 10 year return flood under the current land cover classification lc2014 has increased 25 compared to fc100 and that the continued expansion of agricultural land use could increase it a further 10 under the fc25 projection and 18 under the fc0 fig 8 this means that the return period for a record high flood during the study period i e approximately the 2008 peak discharge fig 5a would fall from the current estimate of 22 years under lc2014 to just 8 years under the total forest conversion scenario fc0 4 discussion the hydrological changes evident in our scenarios of forest conversion within the usumacinta river sub basin clearly suggest that forests play an important role in controlling the frequency and magnitude of floods in the humid tropics of southeastern mexico in addition these results may offer broader insights into the hydrological functioning of forests in similar climate conditions and support the assertion that forests play an important role in controlling floods globally here we discuss the implications and processes underlying these results as well as additional drivers and externalities both biophysical and linked socio political issues 4 1 impact of forest conversion on hydrological processes the conversion of forest to short vegetation is typically associated with a large reduction in interception losses over longer time periods spracklen et al 2018 however interception losses vary with precipitation intensity the highest losses are associated with lower intensity events whereas interception losses are unlikely to significantly affect higher intensity precipitation events as the canopy capacity is rapidly exceeded bandeira et al 2018 fleischbein et al 2005 van dijk et al 2009 evapotranspiration is another mechanism by which forests can reduce the proportion of precipitation reaching the river network compared to grass and cropland forests have rates of evapotranspiration between 20 and 80 greater than tropical grasslands schlesinger and jasechko 2014 spracklen et al 2018 von randow et al 2004 zhang et al 2001 as with interception losses the difference in evapotranspiration between forest and shorter vegetation will have a pronounced effect on base flow characteristics and small to mid sized flood peaks resulting from prolonged lower intensity precipitation events than upon floods produced by short duration extreme rainfall brown et al 2005 bathurst et al 2011 the third mechanism by which forest conversion affects the delivery of water to the river network is by altering the permeability of soils and thus the partitioning of surface to subsurface flow the exposure of bare soil to intense rainfall lal 1987 1996 the compaction of topsoil by machinery or grazing gilmour et al 1987 kamaruzaman 1991 and the removal of roots and organic intrusions lal 1983 aina 1984 all contribute to reduce soil permeability and rainfall infiltration after forest conversion bruijnzeel 2004 germer et al 2010 moraes et al 2006 muñoz villers and mcdonnell 2013 unlike the previous two mechanisms a reduction in soil permeability and the associated increase in runoff generation will significantly affect the magnitude and timing of flood peaks particularly those resulting from the most intense precipitation events kamaruzaman 1991 robinet et al 2018 van der plas and bruijnzeel 1993 the combined effect of reduced interception losses and evapotranspiration following forest conversion should be most evident in the drier season when the volume of water returned to the atmosphere is a significant proportion of the total volume that falls across the study area our results clearly support these hypotheses as the ratio of discharge at the study area outflow to the average precipitation across the study area shows a dramatic increase in the drier season months for land cover classifications lc2004 and lc2014 fig 4d the difference between interception and evapotranspiration losses following forest conversion are less significant in the wetter months as evidenced in the more consistent ratio of discharge to precipitation across land cover classes between june and december fig 4d whilst the total volume of water reaching the river network after forest clearing during the wetter season may not alter as significantly as during the drier season the timing and magnitude of peak discharges may shift as a result of increased runoff generation due to reduced soil infiltration rates this may account for the observed increases in discharge during aug october despite there being no significant alteration in precipitation totals fig 4 an analysis of extreme value distributions requires sufficiently long time series data to encompass a range of extremes therefore a direct comparison of historical discharges across each of the land cover classification periods does not yield robust results as the time periods only cover 7 15 years however using our calibrated model we simulated the expected range of annual extremes across a consistent 20 year period 1999 2018 of climate data for each scenario of forest cover fc100 fc0 this allowed us to explore the impact of both historical and future forest conversion on the flood magnitude frequency distribution of the usumacinta and to characterise the role that forests play in mediating large scale flood events our results indicate that the large scale conversion of forests to agriculture has intensified discharge extremes and that continued conversion is likely to exacerbate fluvial flooding in the future whilst the quantification of this impact may only be valid within the usumacinta context the general patterns observed here are indicative of alterations that forest conversion makes to the underlying processes that mediate large scale flooding due to the interconnectedness of the usumacinta and grijalva sub basins and the resemblance of their topographic and climatic characteristics we can reasonably assert that our results will hold true across the wider grijalva usumacinta basin whilst the current flood regime of the grijalva differs significantly from the usumacinta due to the large scale infrastructural developments including four hydropower dams along its course our results nevertheless have important implications for the management of reservoirs if the conversion of forests to agriculture and pastureland in the grijalva upper basin continues reassessing the reservoirs operating levels may be necessary to take into account the shift in landscape response to intense rainfall events and to accommodate more frequent higher magnitude discharge events during the wettest periods 4 2 additional externalities in addition to forest conversion the major biophysical factors that will affect the flood frequency magnitude distribution in the grijalva usumacinta river basin in the coming decades are climatic changes major shifts in agricultural production urban expansion and infrastructural development of these climate change has the most potential to radically alter the hydrological regime of southeastern mexico the expected impact of global warming on future patterns of precipitation across southeastern mexico is a reduction of annual totals and protraction of drier periods with the possibility of increased extreme events during the wetter season fuentes et al 2015 imbach et al 2018 karmalkar et al 2011 less frequent more intense rainfall interspersed with longer dry periods will likely result in a more rapid conveyance of rainfall to the river network increased soil degradation and compaction during dry spells may further reduce the infiltration capacity of soils batey 2009 bruijnzeel 2004 followed by an intensification of precipitation extremes that will generate a higher proportion of runoff potentially overloading the river network capacity and causing widespread flooding each of the biophysical factors affecting the flood regime of the grijalva usumacinta has the potential to either exacerbate or counteract the impact of future climate change but this will depend upon the multifaceted interplay of sociopolitical drivers that shape the land use patterns rural development strategies and plans for urbanization in the region since the 1980s the major agricultural activity that has superseded forest conversion in the grijalva usumacinta river basin is extensive cattle raising which is often practiced not only for the economic value of the herd but also as an indication of land ownership and a form of land speculation kaimowitz and angelsen 2008 tudela 1989 at an average of one head of cattle per hectare the profitability of extensive cattle raising in the humid tropics is considerably lower per unit of area than in many alternative production systems including cacao citrus or intensive agrosilvopastoral systems nahde toral et al 2013 the reason why many farmers prefer extensive cattle raising is that it requires relatively few investments and the labour costs are relatively low tudela 1989 several governmental agricultural development programmes in the region have also promoted cattle raising with subsidies low interest loans and technical assistance a major shift in agricultural and environmental policies that promotes the reforestation of large portions of the upper basins might mitigate some of the negative impacts of forest removal on flood generation bruijnzeel 2004 at present there is an initiative to reforest 1 million hectares of pastures and croplands with fruit and timber trees across the grijalva usumacinta riverbasin by the federal government however the tendency seems to be land conversion to largescale sugar cane african oil palm and gmelina plantations el heraldo de tabasco 3 of april 2020 if this one million hectare reforestation programme succeeded in its goals this could shift the current land cover scenario lc2014 towards the fc625 land cover scenario see fig 8 in terms of high vegetation cover whilst reforestation s capacity to entirely rehabilitate the hydrological functioning of converted land is uncertain bruijnzeel 1997 2004 restoring comparable levels of interception losses and evapotranspiration to pre conversion levels could be expected within 3 5 years malmer 1992 brown et al 1997 accompanied by a significant reduction in peak and storm flows bruijnzeel 1989 a transition from the flow regime associated with lc2014 to fc625 would reduce the occurrence of the expected 20 year return peak discharge to once every 36 years fig 8 such a reduction in flood frequency would have enormous economic and social ramifications as the 2007 flood alone has been estimated to have incurred losses to the state of tabasco between 800 and 3000 million usd kauffer 2013 räsänen et al 2017 ishizawa et al 2017 a thorough investigation of environmental social and economic impacts of basin wide reforestation programmes should be undertaken to assess the long term viability of balancing the cost of reforestation programmes against the associated reductions in flood damage expenses as a result of a severe flood disaster in 1999 the federal government of mexico initiated an integrated flood control program pici in the state of tabasco aparicio et al 2009 however due to a series of delays and budgetary ambiguities many of these projects were abandoned before completion and few were operational when the 2007 flood event occurred perevochtchikova and lezama de la torre 2010 a renewed effort following the devastating 2007 flood saw a number of infrastructural projects completed which appear to have reduced the socioeconomic impacts of the 2010 flood despite its magnitude being larger than that of 2007 ishizawa et al 2017 however an issue with many of these projects is that they divert the flood hazard from one place to another rather than eliminating the risk of serious flooding through levees gate structures embankments and water channels floodwater has been redirected from the more affluent urban areas to socio economically vulnerable rural areas and indigenous communities who are under represented in the political decision making lack the resources to protect themselves from the negative consequences of flooding and to recover quickly after a disaster nygren 2016 in recent years the long term sustainability of technocentric flood control measures has been questioned and there has been a shift towards more integrated flood management approaches with basin wide land use strategies to make room for the river and to enhance residents social resilience to flooding butler and pidgeon 2011 nygren 2016 sletto and nygren 2015 rijke et al 2012 räsänen et al 2017 to avoid future catastrophes in southeastern mexico a broader and more integrated approach to flood management is needed with strategies to reduce the strain on the river network capacity and shift from merely water resources management towards approaches that consider the complex interactions and overlappings between river basin management land use changes hydrological infrastructure coastal zone restoration and flood risk prevention 4 3 limitations of the study and opportunities for future research we focused our study on the usumacinta river because its channel has remained free from infrastructural development and agricultural expansion happened at time when the landscape response to widespread forest conversion was captured in satellite imagery and gauging station records this allowed us to characterise the anthropogenic signal within the discharge record generated from agricultural expansion that would otherwise have been lost amongst competing signals however the benefits derived from the study area s relative remoteness come with certain disadvantages and challenges the main constraint in this research has been data availability and coverage a limitation common to these kinds of studies but particularly difficult in this instance due to the geographical remoteness and the transboundary nature of the usumacinta river the upper reaches of the usumacinta sub basin are located within the boundary of guatemala where there are very few records of climate data and no records of discharge even in the relatively data rich areas within mexico data sets lacked consistency and continuity we acknowledge the limitations and uncertainty that these data constraints place on our work nevertheless supplementing observed data with satellite derived data we were able to consistently replicate discharge records across the entire study period the methods used in this study to explore the processes that shape the landscape s hydrological response to widespread forest conversion would be directly transferable to river basins with similar characteristics across the tropics however our assessment of future flood patterns is not directly transferable to the wider grijalva usumacinta river basin as these will also depend upon the operation of the reservoirs along the grijalva river which issue was outside the scope of this study to comprehensively account for future flood risk within this area a detailed analysis of future trends in discharge that incorporates the current reservoir operating rules is required such a study should also incorporate scenarios of future changes to precipitation and temperature patterns due to climate change to implement a fully integrated flood risk management strategy a more thorough understanding of the changing flood management policies and land uses patterns and their linkages to residents socio economically differentiated vulnerabilities and capabilities of flood resilience would also be needed 5 conclusion this study is one of the few comprehensive assessments that quantifies the impact of widespread tropical forest conversion on river discharge and flood magnitude conducted at the large scale by analysing 55 years of climate and discharge data we identified a signal of forest conversion within the discharge record of the usumacinta river southeastern mexico comparing the proportion of water falling as precipitation to that reaching the study area outflow between stages of deforestation extent we found tropical forest conversion significantly alters the hydrological functioning of the landscape between 2010 and 2020 the net loss of global forests is estimated at 4 7 million ha per year with the large majority of that loss occurring in the tropical regions of latin america and africa fao 2020 in addition to the large carbon source that this loss represents there are a number of ecosystem services essential to humanity s well being that can no longer function such as weather regulation spracklen et al 2018 air purification biodiversity protection and pest control chivian 2002 diaz et al 2006 laurance and williamson 2001 yet despite this governments are reluctant to conserve tropical forests as the economic valuation of these ecosystem services is difficult to quantify and the global demand for timber and agroindustrial products incentivises forest conversion for agriculture and the commercial extraction of forests to rapidly generate capital rudel et al 2009 hosonuma et al 2012 our findings bring to light the potential for tropical forests to play a key role in the mitigation of large flood events and the impact continued deforestation can have on the magnitude and frequency of future flood events across the tropics due to the socio economic costs and environmental impacts these increases in flood magnitude represent such findings may contribute to a holistic evaluation of the benefits derived from conserving forest cover and promote the implementation of integrated flood management approaches that include comprehensive river basin wide land use and resource management practices credit authorship contribution statement alexander j horton conceptualization methodology formal analysis writing original draft writing review editing visualization anja nygren conceptualization writing review editing supervision funding acquisition miguel a diaz perera resources investigation matti kummu conceptualization writing review editing visualization supervision funding acquisition declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements the study was supported by the academy of finland through the watvul project grant no 317320 the emil aaltonen foundation through the eat less water project and the european research council erc under the european union s horizon 2020 research and innovation programme grant agreement no 819202 data used in this study is publicly available as cited in the manuscript and by request from conagua for which we are grateful 
8581,anthropogenic activities are altering flood frequency magnitude distributions along many of the world s large rivers yet isolating the impact of any single factor amongst the multitudes of competing anthropogenic drivers is a persistent challenge the usumacinta river in southeastern mexico provides an opportunity to study the anthropogenic driver of tropical forest conversion in isolation as the long meteorological and discharge records capture the river s response to large scale agricultural expansion without interference from development activities such as dams or channel modifications we analyse continuous daily time series of precipitation temperature and discharge to identify long term trends and employ a novel approach to disentangle the signal of deforestation by normalising daily discharges by 90 day mean precipitation volumes from the contributing area in order to account for climatic variability we also identify an anthropogenic signature of tropical forest conversion at the intra annual scale reproduce this signal using a distributed hydrological model vmod and demonstrate that the continued conversion of tropical forest to agricultural land use will further exacerbate large scale flooding we find statistically significant increasing trends in annual minimum mean and maximum discharges that are not evident in either precipitation or temperature records with mean monthly discharges increasing between 7 and 75 in the past decades model results demonstrate that forest cover loss is responsible for raising the 10 year return peak discharge by 25 while the total conversion of forest to agricultural use would result in an additional 18 rise these findings highlight the need for an integrated basin wide approach to land management that considers the impacts of agricultural expansion on increased flood prevalence and the economic and social costs involved keywords deforestation flooding hydrological modelling flow regime stormflow watershed management 1 introduction global climate change and anthropogenic activities are disrupting flood frequency magnitude distributions along many of the world s large rivers posing threats to human populations and critical infrastructure many competing drivers contribute to modify a river s long term discharge pattern land cover and land use change through deforestation and agricultural expansion urban expansion water abstraction for human consumption and irrigation channel modifications and infrastructural developments as well as shifting precipitation patterns due to climate change bruijnzeel 2004 júnior et al 2015 malmer 1992 each of these drivers contributes its own signal to a river s hydrograph by altering the timing duration and magnitude of peak flows yet evaluating the relative impact of any one of these drivers in isolation is a persistent challenge due to their interconnectedness and constantly varying climatic conditions rogger et al 2016 van dijk et al 2009 the signal derived from forest conversion is particularly difficult to identify and quantify as large scale land cover change historically takes place over long periods amid many competing and overlapping developments and often during times with spatially sparse climate and discharge data bruijnzeel 1990 1993 forest removal changes the pathways of precipitation through altering the processes of interception the proportion reaching the surface infiltration the partition of surface and subsurface and retention the proportion reaching the river network after infiltration replacing dense tropical forest canopy with short vegetation grass shrubs or crops significantly reduces interception losses spracklen et al 2018 compacts soil and removes organic material reducing infiltration rates germer et al 2010 muñoz villers and mcdonnell 2013 and dramatically decreases evapotranspiration rates particularly in the humid tropics spracklen et al 2018 zhang et al 2001 by reducing interception losses evapotranspiration and the infiltration rate of soils the removal of forest increases the proportion of rainfall entering the river network and the rate at which it reaches the network thereby increasing the potential for large scale flooding brown et al 2005 bruijnzeel 1989 fritsch 1993 whilst this has long been understood on the conceptual level blackie and edwards 1979 clark 1987 gilmour et al 1987 and many studies that relate tropical forest conversion to increases in mean river discharge exist at the small experimental scale 1 km2 and lower mesoscale 10 km2 there are relatively few studies that examine the effects at the meso or large scale 100 km2 bruijnzeel 1990 1997 costa et al 2003 fritsch 1993 in addition due to poor data coverage the relative inaccessibility of the study areas and the temporal and spatial heterogeneity of competing factors a number of early large scale studies reported inconclusive or contradictory findings dyhr nielsen 1986 gentry and lopez parodi 1980 qian 1983 richey et al 1989 wilk and plermkamon 2001 despite the complexities and complications that large scale studies of this type have to contend with there is a growing body of research that examines the hydrological impact of tropical forest conversion at the river basin scale brown et al 2005 dias et al 2015 gao et al 2020 gerold and leemhuis 2008 júnior tomasella and rodriguez 2015 karamage et al 2017 molina et al 2012 recha et al 2012 robinet et al 2018 sahin and hall 1996 van der weert 1994 the overwhelming majority of these studies conclude that large scale deforestation increases annual water yields and mean discharges yet the large scale impact on storm flow generation storm flow pathways and how these pertain to flood generation is less clear bruijnzeel 2004 robinet et al 2018 at the global scale bradshaw et al 2007 undertook an analysis of flood severity and found that deforestation increases flood risk in tropical regions yet a subsequent analysis suggested that 83 of the variation may be accounted for by population density as a root cause rather than a direct causal link with removal of trees van dijk et al 2009 it is this persistent challenge of disentangling landscape responses to tropical forest conversion from competing anthropogenic and climate signals that has prohibited the systematic quantification of the impact forest conversion has on long term flood risk bruijnzeel 2004 van dijk et al 2009 here we attempt to fill this gap by clearly demonstrating the impact of forest conversion on peak discharge generation in the absence of competing anthropogenic influences and accounting for climatic variation between our reference time periods to significantly clarify the role tropical forest plays in large scale flooding this is an important contribution as lacking a clear understanding of the role of forest cover in preventing future flood disasters has complicated the planning and implementation of effective land management strategies together with feasible flood mitigation policies calder and aylward 2006 fao and cifor 2005 2 data and methods here we analyse 55 years 1959 2014 of climate and discharge data from the usumacinta river sub basin in southeastern mexico which provides an opportunity to study the hydrological impact of large scale tropical forest conversion in isolation the past 20 years have seen an increase in the severity of flooding in the states of tabasco and chiapas in southeastern mexico atreya et al 2017 gama et al 2010 2011 which encompass the connected river basin of the grijalva and usumacinta rivers unlike the grijalva sub basin which has several large hydropower dams along the main channel the flow of the usumacinta sub basin is unobstructed for the entirety of its course from the guatemalan highlands to the gulf of mexico the absence of large urban settlements and the relative inaccessibility of the terrain stifled development and large scale forest conversion until the 1990 s meaning the 55 year discharge record from 1959 to 2014 captures the hydrological impact of deforestation on an otherwise consistent landscape to identify and quantify an anthropogenic signal of tropical forest conversion in the discharge record of the usumacinta river we analysed continuous daily time series of precipitation temperature and discharge to compare long term trends and accounted for climatic variability between reference periods by examining the ratio of daily discharge to 90 day precipitation totals from across the contributing area we then successfully reproduced this signal using a distributed hydrological model to simulate the response of the study area to wide spread forest conversion vmod lauri et al 2006 and evaluated both the historical and future impact agricultural expansion has on the flood frequency magnitude distribution along the usumacinta river in this paper we employ a novel approach to disentangle climatic variability from within the daily discharge record at the intra annual scale and contribute to the wider global scale debate concerning the role of forests in controlling large flood events especially in the humid tropics 2 1 study area the grijalva and usumacinta are the largest rivers in mexico day et al 2003 both begin in the highlands of guatemala and traverse the states of chiapas and tabasco before converging in the low lying floodplains just 50 km from the gulf of mexico the combined grijalva usumacinta river basin covers some 130 000 km2 and produces mean monthly discharges between 3000 and 6000 m3 s equivalent to around 30 of the total surface runoff of the country areu rangel et al 2019 the grijalva and usumacinta sub basins are separated both hydrographically and socio economically as the grijalva sub basin was the historical focus of development within the region leaving the usumacinta sub basin relatively undisturbed until the 1980s villela and martínez 2018 as such the main population centres areas of industrial oil and natural gas extraction projects of irrigated agriculture and four hydropower dams are all concentrated within the grijalva sub basin however an intensification of agricultural development projects since the 1990s has seen rapid large scale conversion of natural forest cover across much of the usumacinta sub basin driven by agricultural expansion that extends into many of the headwater regions our study focuses on the usumacinta sub basin measured at the boca del cerro gauging station located on the main branch of the usumacinta river outside tenosique fig 1 which is the main urban centre along the river course home to around 50 000 inhabitants ine 2005 we identify four periods within the discharge and climate records of the usumacinta sub basin that correspond to distinct stages of development and draw comparisons between these land cover signatures 2 2 discharge data our analysis of the usumacinta flow regime is based on discharge data collected by the servicio meteorológico nacional smn under the comisión nacional del agua conagua of mexico we use data from the boca del cerro gauging station 30019 fig 1 which is the only station within the usumacinta sub basin to provide a continuous record of average daily discharges m3 s across the entire study period 1959 2014 2 3 climate data observations of daily precipitation maximum temperature and minimum temperature were available at 15 weather stations that collectively form a continuous spatially robust dataset spanning the period 1959 1992 these data were provided by the global historical climatology network ghcn menne et al 2012 and garcía 1977 we infilled a spatial data gap located within guatemala using additional data points taken from the princeton university global meteorological forcing pgf v 1 sheffield goteti and wood 2006 formed of a suite of global observation based datasets with the ncep ncar reanalysis we applied a bias correction to these data against ground data using a multi variable scaling method santander meteorology group 2015 wilcke et al 2013 after 1992 there are few ghcn stations within the usumacinta sub basin that record daily climate observations therefore we supplemented these with a combination of the tropical rainfall measuring mission trmm v7 satellite derived precipitation data and the climate prediction centre cpc global daily temperature data for the period 1999 2018 to preserve model calibration across all time periods we sampled the gridded data at points corresponding to the observation stations used in earlier iterations we used daily composites of the rain gauge adjusted 3 hourly 0 25 degree trmm product 3b42 kummerow et al 1998 that has been shown to reliably reproduce rainfall in the humid tropics and has been used extensively in climate analyses and model forcing ferreira et al 2012 ji 2006 lauri et al 2014 shrivastava 2014 tapiador 2017 wu et al 2015 a comparison of trmm data with gauged observations taken within the grijalva river sub basin showed a strong correlation particularly at monthly timescales nse 0 6 0 82 with 30 day moving average cpc daily temperatures combine the ghcn observation dataset and the climate anomaly monitoring system cams dataset and interpolate them across a 0 5 degree grid fan and dool 2008 which has proven a reliable forcing for climate models nashwan 2019 comparing measurements of max and min temperature to ground observations across the wider grijalva usumacinta river basin for the period 1998 2003 we found a consistent negative bias by the cpc dataset which we again corrected using a scaling factor santander meteorology group 2015 wilcke et al 2013 for our analysis of climate trends across time periods we used spatially averaged data interpolated across the study area from point data used as our model inputs 2 4 model description and set up the environmental impact assessment centre of finland s eia integrated water resources management modelling tool iwrm vmod is a physically based hydrological model distributed across a square grid representation that couples sub models resolving energy and water balances at the grid scale with a 1 d river channel network model that routes outflows between cells vmod first constructs a grid mesh over layered raster inputs representing elevation m soil type flow direction and land cover class it then interpolates daily climate data for each grid cell from input data at discrete points max and min temperature c precipitation mm and calculates potential evapotranspiration pet using the hargreaves samani method hargreaves and samani 1982 before solving energy and mass balances across two subsurface soil layers and surface atmosphere transfers following dingman 1994 runoff generated from each cell is then routed through a 1 d river channel network to give discharge outputs which are calibrated against historical records for a detailed description of the model construction and the governing equations see lauri et al 2006 for elevation data we used srtm 90 m jarvis et al 2008 from which the model inferred flow direction data and the river channel network which we adjusted to ensure alignment with satellite imagery we prepared soil data from the fao world soil map fao 2009 by reclassifying the original classifications into six classes with default parameterisations but later amended these as part of the calibration process we then defined four periods each with a distinct land cover signature ranging from lc1970 with almost total forest cover to lc2014 with just 42 dense forest cover across the study area lc1970 land cover map was inferred from the international satellite land surface climatology project s islscp ii ramankutty et al 2010 historical land cover maps for 1950 and 1970 that showed little deforestation outside of the area around tenosique which corresponds with accounts of land use described in tudela 1989 lc1992 land cover map corresponds to land use classifications described in the central american vegetation land cover classification and conservation status 1992 1993 ccad 1998 lc2004 and lc2014 land cover maps were derived from modis land cover classifications mcd12q1 from 2004 and 2014 respectively friedl and sulla menashe 2019 each of the land cover maps were reclassified from the original interpretations into five classes water forest rain fed cropland pastureland and urban we then aggregated each of the raster inputs to match the model s grid sizing which we set to a resolution of 2 5 2 5 km 2 5 model calibration validation and testing as the focus of this study is to assess the impact of forest conversion on the hydrological regime of the usumacinta river we calibrated our model using 4 periods with distinct land cover signatures to distinguish between behaviours driven by soil type characteristics and those driven by vegetation dynamics our initial calibration was against discharge data for the period 1978 1985 assuming a ubiquitous forest cover although this assumption contradicts our land cover map lc1992 this period has the most abundant and reliable climate data needed for a robust calibration of the soil parameters controlling the timing of runoff we used the period of 1968 1973 as a validation of the initial calibration in a cyclical process to identify the interactions of soil and vegetation effects to best approximate the parameters for each soil type and the forest land cover class across both periods which we then tested against the period 1959 1966 having thus calibrated the soil type and forest land cover parameters we applied the model to the period 2008 2014 where variations in model performance stem entirely from forest land cover alterations assuming consistent soil parameters across time periods whilst maintaining the model calibration from the initial periods 1959 1985 we introduced two additional land cover classes rain fed cropland and pastureland to correct the model deficiencies using 2003 2007 as a validation period for a repetition of the cyclical calibration process this final model calibration including the non forest land cover classes was tested against the periods 1986 1992 and 1999 2003 the initial calibration phase 1968 1986 required all major parameters to be adjusted but primarily focused on hydraulic conductivities horizontal and vertical directions soil layer depths storage capacities weather interpolation values surface runoff coefficients and computational grid values the second phase 2003 2014 focused solely on defining the vegetation characteristics of the non forest types primarily the evapotranspiration and interception parameters as well as the surface model components pertaining to vegetation differences for the calibration phases we used the nash sutcliffe efficiency coefficient nse nash and sutcliffe 1970 as the objective function we then assessed the overall model performance against observed discharge by comparing relative biases of the total annual flow low flow and high flow indices i e the 95th and 5th percentile of the discharge record respectively as well as comparing the mean monthly discharges and distributions of annual maximum minimum and mean discharges across the entire discharge record 1959 2014 2 6 assessing hydrological and climatic changes between periods to assess changes in the hydrological regime of the usumacinta river we first analysed trends in the long term annual maxima minima and mean discharges for the duration of the discharge record we repeated these analyses for mean annual temperature data as well as total annual and seasonal precipitation data where we defined the wettest season as june november and the drier season as december may to examine changes in the intra annual flow regime we divided the discharge and climate records into four periods for comparison each representative of a distinct land cover signature lc1970 used for period 1959 1973 lc1992 1978 1992 lc2004 1999 2007 and lc2014 2008 2014 we compared the mean day of year doy discharges temperatures and precipitation 30 day totals for each period against the long term means across the period of the entire discharge record finally to determine whether variances in mean discharges between lc periods are attributable to alterations in water availability we normalised mean day of year doy discharges by average 90 day precipitation totals scaled by contributing area in the case of lc1970 which has a ubiquitous covering of vegetation the discharge recorded at the boca del cerro gauging station will be directly proportional to the amount of precipitation fallen within the study area less interception evapotranspiration and changes to groundwater storage and alterations to flow due to extraction or dams not applicable in the usumacinta interception evapotranspiration is a function of temperature controlled by vegetation characteristics and changes to groundwater storage can be assumed negligible when summed over multiple years therefore normalising monthly discharge by precipitation totals scaled by area should reveal a consistent proportionality that reduces intra annual variation such that 1 nd i 1 n 1 n d i n 1 n a 1 n 1 90 j i 90 j i p j n where nd i is the normalised discharge for the i th day of the year n is the number of years in the observation record d i n is the discharge m3 s on the i th day of the n th year a is the contributing area m2 and p j n is the precipitation total mm day on the jth day of the n th year nd i then represents the dimensionless after unit conversion factors proportion of discharge to the average amount of water fallen over the entire study area as precipitation in the previous 90 days we find that 90 day precipitation totals are most suitable for removing intra annual variation to consistent proportionalities in this study area were the periods 1978 1992 lc1992 1999 2007 lc2004 and 2008 2014 lc2014 to maintain the same ubiquitous forest cover then the discharge record at the boca del cerro gauging station should display the same proportionality to precipitation as displayed in lc1970 assuming a consistent temperature distribution across time periods any variation from the intra annual normalised discharge pattern displayed in lc1970 will be the result of alterations to the vegetation effects controlling interception evapotranspiration and thus represents an anthropogenic signature of forest conversion to agricultural expansion 2 7 forest conversion scenarios to assess the impact of potential future forest conversion on the hydrological regime of the usumacinta river we developed scenarios that progressively removed forest area according to observed historical patterns projected into the future lc1970 lc1992 lc2004 and lc2014 represent 98 87 3 73 3 and 42 1 forest cover respectively we randomly converted forested pixels from the initial land cover map lc1970 sampled from areas later converted to either crop agriculture or pastureland as displayed in lc1992 lc2004 and lc2014 maintaining the observed proportions of each as displayed in lc2004 the most reliable partition of land classes to infill the proportion of non forest cover for 100 50 forest cover scenarios after which the forest conversion was entirely randomised for 25 and 0 forest cover scenarios we then used these progressive land cover maps to run forest conversion model scenarios across the most complete record of climate forcings 1999 2018 this allowed us to draw direct comparison of the mean doy discharges and hydrological extremes under different projections of forest conversion and to assess the likely impact of continued agricultural expansion on severe flooding along the usumacinta river and by extension the grijalva river 3 results 3 1 long term climate trends and hydrological signals using interpolations of daily climate data and discharge data spanning the entire 55 year study period 1959 2014 we found statistically significant positive trends for the mean annual temperature p value 0 01 fig 2 a and annual maximum mean and 10th percentile low flow discharges p values 0 1 0 04 and 0 01 respectively fig 2c we found no statistically significant trends in neither the total annual precipitation fig 2b the total drier season precipitation nor the total wet season precipitation across the years from a comparison of the mean daily discharges taken for each of the land cover classification periods lc1970 lc2014 there are considerable increases in the first wet season peak jun aug and again in the second peak sep nov fig 3 a whilst the drier season flows look comparatively stable across the periods the proportional increases from the historical base discharge show statistically significant drier season gains positive 7 60 fig 4 a which is not evident in the drier season precipitation between time periods fig 4c the largest proportional change comes at the onset of the wet season in june where mean monthly discharges are 75 larger in lc2014 compared to lc1970 fig 4a whilst in terms of magnitude discharge increases in october are equivalent to those in june with lc2014 exhibiting a mean monthly discharge 900 m3 s larger than that for lc1970 to characterise the different landscape responses under each of the land cover classifications independently of the varying climatic conditions we compared the ratio of daily discharge totals at the mouth of the study area to the mean daily precipitation to have fallen across the entire study area for the previous 90 days the results show an average proportionality between 0 and 1 that is relatively consistent throughout the year fig 3d when compared to the variation displayed in the precipitation totals fig 3c however this proportionality alters dramatically between early and later land cover classifications with lc2004 and lc2014 showing changes of up to 58 may from those of lc1970 fig 4c predominantly in the drier season months where the ratio of mean daily discharge to precipitation has risen from 0 45 to consistently above 0 7 fig 3c 3 2 hydrological model calibration and validation both the initial calibration phase that concentrated on soil characteristics and forest cover parameterisation 1978 85 and the later calibration phase that focused on defining the non forest land cover types 2008 14 display good agreements with observed data each obtaining an nse of 0 80 table 1 the test phases which were not included as part of the calibration procedure display nses of 0 64 and 0 74 respectively the slightly poorer fit to the older test period may in part be due to the reliability of the forcing climate data which was sparser and contained a number of data gaps making the interpolation less consistent overall the modelled discharge series displayed a robust agreement to the observed data set with an nse of 0 76 table 1 in addition to consistently performing better than the sample mean indicated by the nse an important component of hydrological modelling is the faithful reproduction of key aspects of the regime comparing the flow duration curve fig 5 b the distribution of annual means fig 5cmiddle lines and the mean daily discharges fig 5d the model performs well reproducing the distribution of flows characteristic of the usumacinta river however as both the measures of bias table 1 and the distributions of maximum and minimum discharges fig 5c top and bottom attest the model tends to marginally underestimate high flows q5 and overestimate low flows q95 for our purposes the most important component of the model is the representation of hydrological processes affected by forest cover and the impact of forest conversion to agricultural land use on the hydrological regime as the differences in discharge records between land cover classification periods are driven by the combined effects of climatic variability and landscape dynamics normalising the discharge record by variations in climate should reveal a signal of forest conversion to agriculture this signal is present in the observed discharge record where lc2004 and lc2014 display fundamentally different behaviour with respect to the proportion of water reaching the study area outflow fig 4d this same signal is present in the model results fig 6 a which suggests that the model represents the impact of vegetation cover on the hydrological cycle adequately and the effect of forest conversion on discharge re running the model with a homogeneous forest land cover class whilst maintaining the original climate input data produces results that display a more uniform proportionality where the ratio of daily discharge to mean precipitation across the study area more closely resembles that of the historical base case lc1970 fig 6b 3 3 hydrological analysis under future forest conversion scenarios to investigate the impact of forest conversion to agricultural land use on the hydrological regime and flood magnitude frequency distribution of the usumacinta river we ran the calibrated model using climate data from 1999 to 2018 under different scenarios of forest cover representing 100 50 25 and 0 forest cover as well as the 2018 land cover classification we found that each successive forest cover scenario shows a clear increase in discharge throughout the year fig 7 a though the increase is not uniformly proportional fig 7b drier season flows dec may display a larger proportional increase with decreasing forest cover compared to the wet season jun nov the 2014 land cover scenario has a forest cover of 42 yet exhibits discharge patterns that more closely resemble fc25 25 forest cover scenario than fc50 in the drier season months this is most likely due to the proportion of cropland compared to pastureland represented as we maintained the cropland pastureland ratio observed in lc2004 throughout forest cover scenarios while the observations of lc2014 identify a smaller proportion of cropland lastly we fitted a generalized extreme value gev distribution function to the annual maxima for each of the forest cover scenarios including the additional fc625 scenario with 62 5 forest cover to ascertain to what extent agricultural expansion has increased the expected return flood historically and to what extent it is likely to increase it in the future we found that the 10 year return flood under the current land cover classification lc2014 has increased 25 compared to fc100 and that the continued expansion of agricultural land use could increase it a further 10 under the fc25 projection and 18 under the fc0 fig 8 this means that the return period for a record high flood during the study period i e approximately the 2008 peak discharge fig 5a would fall from the current estimate of 22 years under lc2014 to just 8 years under the total forest conversion scenario fc0 4 discussion the hydrological changes evident in our scenarios of forest conversion within the usumacinta river sub basin clearly suggest that forests play an important role in controlling the frequency and magnitude of floods in the humid tropics of southeastern mexico in addition these results may offer broader insights into the hydrological functioning of forests in similar climate conditions and support the assertion that forests play an important role in controlling floods globally here we discuss the implications and processes underlying these results as well as additional drivers and externalities both biophysical and linked socio political issues 4 1 impact of forest conversion on hydrological processes the conversion of forest to short vegetation is typically associated with a large reduction in interception losses over longer time periods spracklen et al 2018 however interception losses vary with precipitation intensity the highest losses are associated with lower intensity events whereas interception losses are unlikely to significantly affect higher intensity precipitation events as the canopy capacity is rapidly exceeded bandeira et al 2018 fleischbein et al 2005 van dijk et al 2009 evapotranspiration is another mechanism by which forests can reduce the proportion of precipitation reaching the river network compared to grass and cropland forests have rates of evapotranspiration between 20 and 80 greater than tropical grasslands schlesinger and jasechko 2014 spracklen et al 2018 von randow et al 2004 zhang et al 2001 as with interception losses the difference in evapotranspiration between forest and shorter vegetation will have a pronounced effect on base flow characteristics and small to mid sized flood peaks resulting from prolonged lower intensity precipitation events than upon floods produced by short duration extreme rainfall brown et al 2005 bathurst et al 2011 the third mechanism by which forest conversion affects the delivery of water to the river network is by altering the permeability of soils and thus the partitioning of surface to subsurface flow the exposure of bare soil to intense rainfall lal 1987 1996 the compaction of topsoil by machinery or grazing gilmour et al 1987 kamaruzaman 1991 and the removal of roots and organic intrusions lal 1983 aina 1984 all contribute to reduce soil permeability and rainfall infiltration after forest conversion bruijnzeel 2004 germer et al 2010 moraes et al 2006 muñoz villers and mcdonnell 2013 unlike the previous two mechanisms a reduction in soil permeability and the associated increase in runoff generation will significantly affect the magnitude and timing of flood peaks particularly those resulting from the most intense precipitation events kamaruzaman 1991 robinet et al 2018 van der plas and bruijnzeel 1993 the combined effect of reduced interception losses and evapotranspiration following forest conversion should be most evident in the drier season when the volume of water returned to the atmosphere is a significant proportion of the total volume that falls across the study area our results clearly support these hypotheses as the ratio of discharge at the study area outflow to the average precipitation across the study area shows a dramatic increase in the drier season months for land cover classifications lc2004 and lc2014 fig 4d the difference between interception and evapotranspiration losses following forest conversion are less significant in the wetter months as evidenced in the more consistent ratio of discharge to precipitation across land cover classes between june and december fig 4d whilst the total volume of water reaching the river network after forest clearing during the wetter season may not alter as significantly as during the drier season the timing and magnitude of peak discharges may shift as a result of increased runoff generation due to reduced soil infiltration rates this may account for the observed increases in discharge during aug october despite there being no significant alteration in precipitation totals fig 4 an analysis of extreme value distributions requires sufficiently long time series data to encompass a range of extremes therefore a direct comparison of historical discharges across each of the land cover classification periods does not yield robust results as the time periods only cover 7 15 years however using our calibrated model we simulated the expected range of annual extremes across a consistent 20 year period 1999 2018 of climate data for each scenario of forest cover fc100 fc0 this allowed us to explore the impact of both historical and future forest conversion on the flood magnitude frequency distribution of the usumacinta and to characterise the role that forests play in mediating large scale flood events our results indicate that the large scale conversion of forests to agriculture has intensified discharge extremes and that continued conversion is likely to exacerbate fluvial flooding in the future whilst the quantification of this impact may only be valid within the usumacinta context the general patterns observed here are indicative of alterations that forest conversion makes to the underlying processes that mediate large scale flooding due to the interconnectedness of the usumacinta and grijalva sub basins and the resemblance of their topographic and climatic characteristics we can reasonably assert that our results will hold true across the wider grijalva usumacinta basin whilst the current flood regime of the grijalva differs significantly from the usumacinta due to the large scale infrastructural developments including four hydropower dams along its course our results nevertheless have important implications for the management of reservoirs if the conversion of forests to agriculture and pastureland in the grijalva upper basin continues reassessing the reservoirs operating levels may be necessary to take into account the shift in landscape response to intense rainfall events and to accommodate more frequent higher magnitude discharge events during the wettest periods 4 2 additional externalities in addition to forest conversion the major biophysical factors that will affect the flood frequency magnitude distribution in the grijalva usumacinta river basin in the coming decades are climatic changes major shifts in agricultural production urban expansion and infrastructural development of these climate change has the most potential to radically alter the hydrological regime of southeastern mexico the expected impact of global warming on future patterns of precipitation across southeastern mexico is a reduction of annual totals and protraction of drier periods with the possibility of increased extreme events during the wetter season fuentes et al 2015 imbach et al 2018 karmalkar et al 2011 less frequent more intense rainfall interspersed with longer dry periods will likely result in a more rapid conveyance of rainfall to the river network increased soil degradation and compaction during dry spells may further reduce the infiltration capacity of soils batey 2009 bruijnzeel 2004 followed by an intensification of precipitation extremes that will generate a higher proportion of runoff potentially overloading the river network capacity and causing widespread flooding each of the biophysical factors affecting the flood regime of the grijalva usumacinta has the potential to either exacerbate or counteract the impact of future climate change but this will depend upon the multifaceted interplay of sociopolitical drivers that shape the land use patterns rural development strategies and plans for urbanization in the region since the 1980s the major agricultural activity that has superseded forest conversion in the grijalva usumacinta river basin is extensive cattle raising which is often practiced not only for the economic value of the herd but also as an indication of land ownership and a form of land speculation kaimowitz and angelsen 2008 tudela 1989 at an average of one head of cattle per hectare the profitability of extensive cattle raising in the humid tropics is considerably lower per unit of area than in many alternative production systems including cacao citrus or intensive agrosilvopastoral systems nahde toral et al 2013 the reason why many farmers prefer extensive cattle raising is that it requires relatively few investments and the labour costs are relatively low tudela 1989 several governmental agricultural development programmes in the region have also promoted cattle raising with subsidies low interest loans and technical assistance a major shift in agricultural and environmental policies that promotes the reforestation of large portions of the upper basins might mitigate some of the negative impacts of forest removal on flood generation bruijnzeel 2004 at present there is an initiative to reforest 1 million hectares of pastures and croplands with fruit and timber trees across the grijalva usumacinta riverbasin by the federal government however the tendency seems to be land conversion to largescale sugar cane african oil palm and gmelina plantations el heraldo de tabasco 3 of april 2020 if this one million hectare reforestation programme succeeded in its goals this could shift the current land cover scenario lc2014 towards the fc625 land cover scenario see fig 8 in terms of high vegetation cover whilst reforestation s capacity to entirely rehabilitate the hydrological functioning of converted land is uncertain bruijnzeel 1997 2004 restoring comparable levels of interception losses and evapotranspiration to pre conversion levels could be expected within 3 5 years malmer 1992 brown et al 1997 accompanied by a significant reduction in peak and storm flows bruijnzeel 1989 a transition from the flow regime associated with lc2014 to fc625 would reduce the occurrence of the expected 20 year return peak discharge to once every 36 years fig 8 such a reduction in flood frequency would have enormous economic and social ramifications as the 2007 flood alone has been estimated to have incurred losses to the state of tabasco between 800 and 3000 million usd kauffer 2013 räsänen et al 2017 ishizawa et al 2017 a thorough investigation of environmental social and economic impacts of basin wide reforestation programmes should be undertaken to assess the long term viability of balancing the cost of reforestation programmes against the associated reductions in flood damage expenses as a result of a severe flood disaster in 1999 the federal government of mexico initiated an integrated flood control program pici in the state of tabasco aparicio et al 2009 however due to a series of delays and budgetary ambiguities many of these projects were abandoned before completion and few were operational when the 2007 flood event occurred perevochtchikova and lezama de la torre 2010 a renewed effort following the devastating 2007 flood saw a number of infrastructural projects completed which appear to have reduced the socioeconomic impacts of the 2010 flood despite its magnitude being larger than that of 2007 ishizawa et al 2017 however an issue with many of these projects is that they divert the flood hazard from one place to another rather than eliminating the risk of serious flooding through levees gate structures embankments and water channels floodwater has been redirected from the more affluent urban areas to socio economically vulnerable rural areas and indigenous communities who are under represented in the political decision making lack the resources to protect themselves from the negative consequences of flooding and to recover quickly after a disaster nygren 2016 in recent years the long term sustainability of technocentric flood control measures has been questioned and there has been a shift towards more integrated flood management approaches with basin wide land use strategies to make room for the river and to enhance residents social resilience to flooding butler and pidgeon 2011 nygren 2016 sletto and nygren 2015 rijke et al 2012 räsänen et al 2017 to avoid future catastrophes in southeastern mexico a broader and more integrated approach to flood management is needed with strategies to reduce the strain on the river network capacity and shift from merely water resources management towards approaches that consider the complex interactions and overlappings between river basin management land use changes hydrological infrastructure coastal zone restoration and flood risk prevention 4 3 limitations of the study and opportunities for future research we focused our study on the usumacinta river because its channel has remained free from infrastructural development and agricultural expansion happened at time when the landscape response to widespread forest conversion was captured in satellite imagery and gauging station records this allowed us to characterise the anthropogenic signal within the discharge record generated from agricultural expansion that would otherwise have been lost amongst competing signals however the benefits derived from the study area s relative remoteness come with certain disadvantages and challenges the main constraint in this research has been data availability and coverage a limitation common to these kinds of studies but particularly difficult in this instance due to the geographical remoteness and the transboundary nature of the usumacinta river the upper reaches of the usumacinta sub basin are located within the boundary of guatemala where there are very few records of climate data and no records of discharge even in the relatively data rich areas within mexico data sets lacked consistency and continuity we acknowledge the limitations and uncertainty that these data constraints place on our work nevertheless supplementing observed data with satellite derived data we were able to consistently replicate discharge records across the entire study period the methods used in this study to explore the processes that shape the landscape s hydrological response to widespread forest conversion would be directly transferable to river basins with similar characteristics across the tropics however our assessment of future flood patterns is not directly transferable to the wider grijalva usumacinta river basin as these will also depend upon the operation of the reservoirs along the grijalva river which issue was outside the scope of this study to comprehensively account for future flood risk within this area a detailed analysis of future trends in discharge that incorporates the current reservoir operating rules is required such a study should also incorporate scenarios of future changes to precipitation and temperature patterns due to climate change to implement a fully integrated flood risk management strategy a more thorough understanding of the changing flood management policies and land uses patterns and their linkages to residents socio economically differentiated vulnerabilities and capabilities of flood resilience would also be needed 5 conclusion this study is one of the few comprehensive assessments that quantifies the impact of widespread tropical forest conversion on river discharge and flood magnitude conducted at the large scale by analysing 55 years of climate and discharge data we identified a signal of forest conversion within the discharge record of the usumacinta river southeastern mexico comparing the proportion of water falling as precipitation to that reaching the study area outflow between stages of deforestation extent we found tropical forest conversion significantly alters the hydrological functioning of the landscape between 2010 and 2020 the net loss of global forests is estimated at 4 7 million ha per year with the large majority of that loss occurring in the tropical regions of latin america and africa fao 2020 in addition to the large carbon source that this loss represents there are a number of ecosystem services essential to humanity s well being that can no longer function such as weather regulation spracklen et al 2018 air purification biodiversity protection and pest control chivian 2002 diaz et al 2006 laurance and williamson 2001 yet despite this governments are reluctant to conserve tropical forests as the economic valuation of these ecosystem services is difficult to quantify and the global demand for timber and agroindustrial products incentivises forest conversion for agriculture and the commercial extraction of forests to rapidly generate capital rudel et al 2009 hosonuma et al 2012 our findings bring to light the potential for tropical forests to play a key role in the mitigation of large flood events and the impact continued deforestation can have on the magnitude and frequency of future flood events across the tropics due to the socio economic costs and environmental impacts these increases in flood magnitude represent such findings may contribute to a holistic evaluation of the benefits derived from conserving forest cover and promote the implementation of integrated flood management approaches that include comprehensive river basin wide land use and resource management practices credit authorship contribution statement alexander j horton conceptualization methodology formal analysis writing original draft writing review editing visualization anja nygren conceptualization writing review editing supervision funding acquisition miguel a diaz perera resources investigation matti kummu conceptualization writing review editing visualization supervision funding acquisition declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements the study was supported by the academy of finland through the watvul project grant no 317320 the emil aaltonen foundation through the eat less water project and the european research council erc under the european union s horizon 2020 research and innovation programme grant agreement no 819202 data used in this study is publicly available as cited in the manuscript and by request from conagua for which we are grateful 
8582,this study evaluates the potential of assimilating phenology observations using a direct insertion di method by constraining the modeled terrestrial carbon dynamics with synthetic observations of vegetation condition specifically observations of leaf area index lai are assimilated in the noah multi parameterization noah mp land surface model across the continental united states during a 5 year period an observing system simulation experiment osse was developed to understand and quantify the model response to assimilating lai information through di when the input precipitation is strongly biased this is particularly significant in data poor regions like africa and south asia where satellite and re analysis products known to be affected by significant biases are the only available precipitation data to drive a land surface model results show a degradation in surface and rootzone soil moisture after assimilating lai within noah mp but an improvement in intercepted liquid water and evapotranspiration with respect to the open loop simulation a free run with no lai assimilation in terms of carbon and energy variables net ecosystem exchange amount of carbon in shallow soil and surface soil temperature are improved by the lai di although canopy sensible heat is degraded overall the assimilation of lai has larger impact in terms of reduced systematic and random errors over the great plains cropland shrubland and grassland moreover lai da shows a greater improvement when the input precipitation is affected by a positive wet bias than the opposite case in which precipitation shows a dry bias keywords data assimilation direct insertion leaf area index dynamic vegetation model dvm land surface model lsm land information systems lis 1 introduction vegetation supports critical functions in the biosphere as it regulates both the biogeochemical water carbon and nitrogen and energy cycles from the local to the global scale vegetation also strongly affects soil characteristics including soil volume chemistry and texture which has a feedback on various vegetation characteristics including productivity and structure kumi boateng et al 2012 vegetation dynamics are therefore crucial when modeling the land surface littell et al 2011 dynamic vegetation models dvms have been designed to represent structural and functional variables that control land surface energy carbon nutrient and water budgets wullschleger et al 2014 peterson et al 2014 dvms and land surface models lsms have been combined in the past to improve the estimation of water carbon and energy cycle processes clark et al 2011 dai et al 2003 for instance the noah multi parameterization lsm hereinafter noah mp developed by niu et al 2011 uses multiple options for key land hydrologic processes together with a module that allocates carbon to various parts of vegetation and soil carbon pools satellite observations offer a valid alternative for monitoring vegetation globally producing maps of indices such the leaf area index lai defined as the one sided leaf surface area measured over unit ground and the normalized difference vegetation index ndvi based on spectral reflectance measurements acquired in the red and near infrared regions lai has been proven to be a useful indicator of the exchange of water vapor and co2 between the vegetation canopy and atmosphere xiao et al 2016 albergel et al 2017 whereas ndvi is an indicator of the density of green vegetation on a patch of land yang et al 2012 for example the moderate resolution imaging spectroradiometer modis has been acquiring data in 36 spectral bands since 2000 rees and danks 2007 at resolutions of 500 1000 m every 4 8 days and the advanced very high resolution radiometer avhrr tucker et al 2005 produces global maps of lai at a resolution of 4 km every 10 days nevertheless satellite based observations often have gaps in their spatial and temporal coverage mainly due to cloud coverage in order to fill in such gaps and guarantee continuous time series observations are commonly merged with model simulations data assimilation is a well known technique for optimally combining the information from such observations and model estimates based on their respective uncertainties reichle 2008 land data assimilation systems ldass have been successfully used in the past decades to merge satellite observations of soil moisture and surface temperature into lsms e g reichle et al 2008 reichle 2008 maggioni and houser 2017 in the recent past a few attempts applied the same concept to vegetation observations and vegetation dynamics models for instance albergel et al 2010 assimilated observations of lai and surface soil moisture ssm within an lsm the joint assimilation of ssm and lai was tested at the local scale in western france and showed a positive impact on the estimation of carbon water and heat fluxes barbu et al 2011 developed an lsm to simulate photosynthesis processes surface carbon fluxes and vegetation biomass and jointly assimilated soil moisture and lai data in another study by albergel et al 2017 a global land data assimilation system ldas monde was applied over europe and the mediterranean basin to improve land surface variable estimation when ssm and lai satellite derived observations were assimilated using a simplified extended kalman filter ldas monde was more effective in estimating soil moisture in the top soil layers but model sensitivity to ssm decreased with depth and had almost no impact below 60 cm kumar et al 2019 conducted an experiment for assessing the impact of assimilating satellite based lai using an ensemble kalman filter over the continental u s conus in the noah mp lsm results demonstrated that lai da has a beneficial impact on the simulation of key water budget terms i e soil moisture evapotranspiration terrestrial water storage and streamflow and carbon fluxes i e gross primary production and net ecosystem exchange when compared to a large suite of ground based reference datasets ling et al 2019 assimilated global land surface satellite glass lai data using an ensemble adjustment kalman filter technique globally results showed that the assimilation was able to reduce the bias in the lai especially in low latitude regions from 5 m2 m2 to 1 m2 m2 another recent work by rajib et al 2020 assimilated modis lai data across eastern iowa in the united states and showed improvements in the estimation of root zone soil moisture and water quality indicators this work builds upon these past studies while also introducing several novelties first off it investigates the impacts of assimilating phenology observations in a land data assimilation system when the precipitation input data are strongly either positively or negatively biased this is particularly crucial for assessing the hydrological conditions in data poor regions like africa and south asia where precipitation information relies on satellite based and model re analysis products which are well known to be affected by severe biases koutsouris et al 2016 singh and xiaosheng 2019 steinschneider et al 2019 for instance ghatak et al 2018 showed biases up to 70 in a suite of precipitation datasets both satellite based and model re analyses that were compared to a gauge based product across south asia yoon et al 2019 compared ten different satellite based and re analysis precipitation datasets across high mountain asia and found average biases of 20 second this work investigates the efficiency of di as the data assimilation approach i e the lai model state is directly replaced by observations whenever the latter become available although a very simple method di does not require any unbiasedness assumptions to operate in an optimal mode as opposed to more sophisticated techniques such as kalman filters since biases in atmospheric forcing and precipitation in particular are often large and unknown blind bias in data poor regions di represents a unique approach to assess the potential of improving the estimation of land surface variables by assimilating vegetation observations being the first attempt to assimilate lai using di under strongly biased precipitation input this work presents a synthetic and therefore fully controlled experiment across conus to assess the potential of such approach 2 methodology this study proposes an observing system simulation experiment osse to better understand and quantify the noah mp model response to assimilating lai information using di the experiment focuses on conus from 250n 1250w to 530n 670w fig 1 during 2011 2015 the nasa land information system framework lis kumar et al 2006 which includes several land surface models and a data assimilation system has been used in this experiment and noah mp described in detail in section 2 1 is chosen as the lsm 2 1 noah mp the noah mp model has a semi tile sub grid scheme in which the canopy layer is separated from the land surface niu et al 2011 the shortwave radiation transfer is computed over the entire grid cell while longwave radiation latent heat sensible heat and ground heat fluxes are computed separately over two tiles a fractional vegetated area fveg and a fractional bare ground area 1 fveg multiple options are available in noah mp for surface water infiltration runoff groundwater transfer and storage dynamic vegetation canopy resistance and frozen soil physics niu and yang 2007 specifically the prognostic vegetation growth combines a ball berry photosynthesis based stomatal resistance ball et al 1987 with a dvm dickinson 1983 that allocates carbon to various parts of vegetation leaf stem wood and root and soil carbon pools fast and slow in our experiment noah mp is forced with the north american land data assimilation system second phase nldas 2 xia et al 2012 nldas 2 is an upgraded version of the first phase of the multi institution nldas 1 mitchell 2004 project which was initiated to provide coupled atmosphere ocean land models with reliable initial land surface states for improving weather predictions xia et al 2012 it has a time window of 40 years 1979 present 1 8 spatial resolution and hourly temporal resolution noah mp is spun up for 30 years 1981 2010 and run at a 15 min time step model daily output are considered in this study 2 2 the observing system simulation experiment synthetic experiments e g osses are useful to quantitively assess the potential impact of land data assimilation systems before they are developed and deployed hoffman and atlas 2016 osses are mostly designed to investigate data assimilation ideas and have the advantage of being controlled experiments in which the reference or truth is known in an osse the observations are simulated by a model rather than being real observations masutani et al 2010 such observations are drawn from a perturbation free run the nature run nr which serves as the baseline simulation commonly referred to as synthetic truth to which the output from all the other runs are compared to similarly in our experiment described by the framework in fig 2 the output from the nr which is forced with the original unperturbed nldas atmospheric forcing dataset is considered as the synthetic truth in terms of not only lai but also for all the other water carbon and energy variables of interest in order to produce the synthetic observations the lai output from the nature run nr lai is aggregated to daily scale and perturbed by a multiplicative random error model with zero mean and a standard deviation of 0 1 to mimic the uncertainties associated with real observations this is a simplistic way of representing such uncertainties and more complex error models e g seasonally dependent could be investigated in future studies that make use of actual satellite products next two sets of open loop ol simulations i e no assimilation are run by perturbing the nldas 2 precipitation to half and double of the original nldas 2 data to generate two extreme cases the one in which nldas 2 has a dry bias and the one in which nldas 2 has a wet bias this is obtained by multiplying the precipitation input by a constant scaling factor i e 0 5 and 2 0 for creating the dry and wet conditions respectively such simple multiplicative error model assumes that the given dataset has perfect detection neither false alarms nor missed events i e no rain rainy pixels will remain no rain rainy pixels in the perturbed forcing dataset this is a strong assumption for re analysis precipitation and for satellite based products as well which are well known to be affected by detection errors nevertheless a series of articles by maggioni et al 2011 2012 2013 showed minimal improvement in soil moisture simulations when a more complex precipitation error model that simulates both false alarm and missed events was considered in a land data assimilation system thus a simple error characterization is chosen for this study but future work should investigate this issue further finally two data assimilation da runs wet and dry are performed by directly replacing the lai model state with observations obtained by the perturbed nr lai in a di da the observation is fully trusted kumar et al 2008 and model estimates are replaced by the observations whenever the latter are available reichle 2008 in this case the model is replaced by the synthetic lai observations every day at 1 00 am di one of the simplest data assimilation methods was chosen in this experiment mainly because it does not require any unbiased model or observation furthermore di is computationally efficient and easy to implement as the updating algorithm does not account for system dynamics or measurement statistics walker et al 2003 in the past di has been successfully used to assimilate snow cover freeze thaw state soil moisture surface albedo in land surface models as well as radiances in aerosols transport models alavi et al 2010 kumar et al 2020 liston et al 1999 sun 2004 weaver et al 2007 xue et al 2019 in this work we propose to apply the same concept to vegetation data by not only updating lai but also the modeled leaf biomass computed by dividing the lai value by the specific leaf area the root mass and stem mass are not updated in this experiment 2 3 evaluation variables and metrics daily water carbon and energy variables listed in table 1 from the ol and da output for both wet and dry conditions are compared to the corresponding true output from the nr run in terms of water variables we consider surface soil moisture ssm m3 m3 defined in noah mp as the water content in the top 10 cm of the soil column rootzone soil moisture rzsm m3 m3 defined as the water content in the top 100 cm of soil intercepted liquid water ilw mm defined as the amount of precipitation that does not reach the soil but that instead gets absorbed by canopy plants and forest floor and total evapotranspiration et kg m2s defined as the total amount of water released to the atmosphere from land and plant as carbon and energy variables we select net ecosystem exchange nee g m2s defined as the amount of carbon exchange between plant and atmosphere short lived carbon in shallow soil css g m2 defined as the short lived carbon pool which is the summation of total leaf and root turnover and total amount of dead leaf canopy sensible heat csh w m2 defined as the total amount of heat transferred to the air from ground and vegetation and surface soil temperature sst k defined as the temperature of the soil in the top 10 cm two performance metrics are considered the relative bias rb and the normalized unbiased root mean square error nubrmse metrics are computed at each grid cell separately for all the experiments ol and da during the 5 year study period rb is an estimate of the systematic error and defined here as the normalized difference between the estimated value either from ol or da and the reference nr variable 1 rb x x nr n x nr n 100 where x represents the output variable from either ol or da runs either from dry or wet conditions xnr is the corresponding nr variable and n is the number of days in order to investigate the random error alone we use the nubrmse following the definition of bhuiyan et al 2018 falck et al 2018 and zhang et al 2020 in which we first remove the bias from the ol and da output variables and then normalize by the reference i e nr values 2 nubrmse x x n x nr x nr n 2 n x nr n 100 normalized metrics allow for easier comparison among variables of different nature and units in this work both rb and nubrmse are shown as percentage values and computed for every model pixel across the study area 3 results changes in lai due to da with respect to the corresponding ol run are shown in fig 3 in the dry condition the application of da introduces additional vegetation to the system to balance the negative bias in the input precipitation in other words the model tends towards the dry side but the observations see more vegetation than the one estimated by the model whereas the wet condition case shows the exact opposite i e lower lai in the da run both in the dry and wet condition the central part of conus is particularly impacted by lai da some areas over the eastern part of conus also show large changes due to the application of lai da such areas mainly correspond to shrublands and croplands fig 1 this is due to the fact that plants characterized by smaller roots are more affected by precipitation whereas plants with longer roots i e woodlands and forests are more resistant to changes in precipitation because they can pull water from the deeper layer of the soil the overall systematic and random errors of the eight variables simulated by the ol or da runs with respect to the reference nr output are computed for both wet and dry conditions rb and nubrmse are averaged across conus and presented in tables 2 and 3 respectively together with differences between the ol and da runs which highlight whether da improves positive sign or worsens negative sign the corresponding ol run ssm rzsm and csh show a degradation with respect to the ol in the average rb after lai is assimilated under with larger degradation under dry condition similarly lai da worsens the nubrmse of soil moisture with a larger degradation in the dry condition for rzsm but with no difference between dry and wet condition for ssm and an improvement in csh on the other hand ilw et nee and css improve after the application of lai da in terms of both systematic and random error with a consistently higher improvement in the wet condition compared to the dry condition with the exception for sst whose statistics only slightly change after the implementation of the lai da scheme in order to further investigate these overall performances we investigate time series during one selected year i e 2013 for averages of all select variables across conus we also assess the spatial variability of the improvement degradation in the evaluation metrics introduced in section 2 3 by showing maps of differences in rb and nubrmse as a last analysis we explore the dependency of the model performance as a function of four main vegetation covers forest and woodland shrubland grassland and cropland results are presented in sub section 3 1 for the water variables ssm rzsm ilw and et and in sub section 3 2 for the carbon and energy variables nee css csh and sst and further discussed in section 4 3 1 water variables as shown in the time series in fig 4 da pushes the modeled ssm and rzsm in the opposite direction with respect to the nr degrading the original ol model simulation this is corroborated by the maps of rb and nubrmse differences in figs 5 and 6 respectively presenting a clear degradation in soil moisture both ssm and rzsm in several regions of the continental us nevertheless some other areas do show an improvement thanks to da e g ssm rb in the southeastern us under wet condition overall the degradation in soil moisture is consistently less pronounced under wet condition in the dry experiment noah mp is fed by only half of the nldas 2 precipitation and there is no additional water source lai da introduces more vegetation to the system and that results in more water absorption which worsens the estimation of soil moisture on the contrary in the wet condition experiment precipitation is doubled adding more water to the system when the soil saturates soil moisture reaches its maximum value and no longer changes which translates into smaller variability in soil moisture under the wet condition compared to the dry condition experiment as shown by the time series in fig 4 and more clearly by the maps in figs 5 and 6 the da simulations of ilw and et improve compared to their corresponding ol runs moving them towards the synthetic truth a decrease in rb is evident across conus for both variables and under both precipitation bias conditions fig 5 however some locations in the south and northeastern us present a degradation in nubrmse fig 6 in the dry and wet condition experiments showing that the lai da cannot correct for the random error in all cases fig 7 summarizes the performance of the proposed da scheme as a function of land cover in general lai da degrades soil moisture but improves ilw and et across all vegetation covers under wet and dry conditions and in terms of both systematic and random errors for ssm ilw and et forest and woodland areas are less impacted by the lai da compared to the other vegetation covers forest and woodland regions have vegetation with deeper roots while shrubland grassland and cropland are characterized by plants with shallower roots the lack of rain dry condition experiment makes the plants with smaller roots dry out and die whereas plants with longer roots can still survive by pulling water from the deeper soil bonan 2002 thus lai da does not have a strong impact on water variables in forest and woodland areas although it does in other regions like the great plains rzsm in the dry condition and in forest and woodland areas shows a larger impact compared to other variables because long rooted plants would pull even more water from the rootzone while precipitation is reduced 3 2 carbon and energy variables all four carbon and energy variables nee css csh and sst present an overall improvement after the application of da in terms of rb and nubrmse tables 2 and 3 fig 4 shows how nee and css are consistently pushed towards the nr by the da scheme during the whole year specifically nee decreases in june july when the lai is the highest which indicates less carbon in the atmosphere and more vegetation on the ground css presents an increasing trend which demonstrates that total amount of living carbon in the soil increases over during the year especially after fall when the leaf and root turnover is the highest however da csh has a noticeable degradation during summer under both dry and wet condition differences between ol sst and da sst are minimal as already highlighted in tables 2 and 3 with an improvement during the summer ol sst is higher than the nr during the summer in the dry run and vice versa in the wet run fig 4 this happens because a decrease increase in vegetation which is due to less or more precipitation in the model input increases decreases soil temperature sst da shows slight degradation in the winter when vegetation is less abundant and soil temperature depends not only on air temperature but also on ssm which degrades after the application of lai da figs 8 and 9 confirm the overall improvement in the selected carbon and energy variables however rb seems to increase after da in a region in the southwestern us and the nee nubrmse has a similar behavior in some areas in the northeastern and northwestern us csh shows a degradation in rb in both dry and wet experiments over the eastern and central part of us but an improvement in nubrmse almost all over conus such degradation in csh corresponds to the same locations where soil moisture shows the largest degradation after da as highlighted in section 2 3 csh is the combined heat flux from ground and vegetation to the atmosphere with the ground to air heat flux being the dominant factor bonan 2002 the heat flux from the ground is highly correlated with the soil water content which is part of the reason why csh worsens after da hwang 1985 guan et al 2009 the change in sst once again is minimal with some regions showing no change between the ol and da runs and some others presenting a slight improvement or a slight degradation this latter is only evident in the nubrmse under wet condition and specifically over forest woodland and cropland areas as demonstrated in fig 10 da in wet condition reduces the vegetation amount modeled by noah mp which would bring sst up however in the northern part of the us sst is more affected by air temperature rather than the surface condition in forest woodland and cropland areas all the variables show an improvement after the application of lai da except for csh in the dry condition fig 10 for all the carbon and energy variables forest and woodland areas are the least impacted after the application of da just like for the water variables 4 discussion in the proposed osse we assimilate daily synthetic observations of lai that mimic temporally smoothed modis observations commonly available every 8 days obtaining satellite observations at such frequency is challenging due to frequent cloud coverage during winters thus the proposed experiment and associated results should be considered as a best case scenario in which lai observations are consistently available every 8 days assimilating observations every 8 days may cause instability in the model dynamics so an alternative approach is proposed here to interpolate between observations and update the model state more often i e every day when actual satellite based lai observations are assimilated in the lsm and ground based measurements are used as reference for validation purposes the lai da performance may differ to the one presented in the results section and what looks like a degradation in the osse may be an improvement in the actual data experiment for instance in this experiment the lai da could not largely improve the rb and nubrmse associated with surface and root zone soil moisture but this may be different if real satellite products are merged within an lsm especially in irrigated fields that are not modeled within the current noah mp framework as a matter of fact kumar et al 2019 show an improvement not only in et but also in rzsm thanks to the assimilation of satellite based lai within a lsm using an ensemble kalman filter across the central plains in us mainly characterized by crops future work should therefore analyze lai soil moisture feedback in the model simulations during different seasons to illustrate why lai assimilation degrades soil moisture estimates in some regions this would be particularly useful for future ecohydrology data assimilation studies systematic errors quantified by rb are reduced thanks to lai da for most variables considered in this study except for ssm rzsm and csh in both dry and wet condition experiments and across the whole study area in terms of random errors nubrmse da improves most of the variables for both wet and dry conditions except the ssm and rzsm with some regions also showing a degradation in nee similarly the work by kumar et al 2019 mentioned above shows an improvement in nee except in the northwestern region of the us and areas around the great lakes in general the improvement in both rb and nubrmse is larger when the precipitation forcing has a positive bias wet condition than in the dry condition experiment this is because in the latter case the amount of water available in the system is limited but the assimilation of lai observations introduces vegetation which causes more root water uptake from the soil and worsens the soil moisture estimation in the osse proposed here some water variables e g et and ilw show an improvement both in dry and wet bias conditions though the improvement under dry biases is marginal these results corroborate what shown in the work recently published by zhang et al 2020 who performed an osse at the global scale to assimilate lai with an ensemble kalman filter technique and observed only marginal improvements in soil moisture contents a degradation in ssm under dry precipitation conditions and significant improvements in et in wet bias conditions furthermore we evaluate the lai da impact on carbon and energy variables most of which except csh improve across the entire domain under both wet and dry bias conditions results show that croplands grasslands and shrublands are the most impacted by the application of lai da as highlighted in figs 7 and 10 as plants with smaller roots are more affected by precipitation which is perturbed in the ol and da simulations while plants with longer roots that characterize woodlands and forests can pull water from the deeper soil and are therefore more resistant to changes in precipitation this also aligns with the findings by zhang et al 2020 who showed a larger impact of the lai da in regions characterized by short root plants when compared to forests and woodlands in summary di of lai can improve almost all the carbon and energy variables under both wet and dry conditions and across different vegetation covers nevertheless the proposed da scheme was not able to improve surface and rootzone soil moisture soil moisture plays a fundamental role in land surface processes and in the water carbon and energy cycles as it i controls the partitioning of the energy incident on the land surface ii is a storage component for precipitation and radiation and therefore impact cloud formation precipitation runoff and evapotranspiration and iii governs several feedbacks at the local regional and global scale e g soil moisture temperature and soil moisture precipitation previous work on the joint assimilation of lai and soil moisture observations has shown promising improvements in both water and carbon dioxide fluxes albergel et al 2010 albergel et al 2017 barbu et al 2011 and bonan et al 2019 however these studies have only focused on limited domain or for a specific crop type thus future research work should look at dual assimilation methods that merge different types of observations e g surface soil moisture and lai if the goal is to improve land surface processes as a whole 5 conclusions this work proposes a synthetic experiment to investigate the impact of assimilating lai observations within a land surface model through di the experiment is performed across the continental us during a 5 year time period january 2011 december 2015 adopting the noah mp lsm forced with nldas 2 meteorological forcing data the efficiency of lai da is investigated in terms of several water carbon and energy states and fluxes under strongly both negatively and positively biased precipitation forcing although di is a very naïve da approach it does not require any unbiasedness assumptions to optimally operate as opposed to the more sophisticated kalman filter which is critical since biases in atmospheric forcing and precipitation in particular are often large and unknown in summary this study demonstrates how assimilating lai observations through the simplest da method available i e direct insertion has the potential to improve lsm estimation of some water e g ilw carbon e g nee and energy e g sst variables these improvements are often a function of the land cover as demonstrated in this work thus future work should be directed towards a more in depth investigation of such dependence to identify what vegetation types would gain the most from assimilating lai observations however as results from this study demonstrate that soil moisture variables would not benefit from the assimilation of lai alone the combined assimilation of both lai and surface soil moisture observations should also be explored this work opens new research directions as real observational data e g modis and glass products available globally could be assimilated in the proposed da framework which could then be expanded to the global scale and benefit several regions of the world where ground monitoring of water energy and carbon states and fluxes is limited but extremely important e g amazon forest high mountain asia the efficiency of the proposed di that adopts a simple multiplicative precipitation error model should be also compared to more sophisticated techniques e g ensemble kalman filter and more complex precipitation error models that simulate false alarms and missed events in the original products declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements this work was supported by the nasa modeling analysis and prediction program map program award number 80nssc17k0109 all the model computations were run using the argo cluster administered by the office of research computing at george mason university va http orc gmu edu author contribution a r performed all analyses and wrote the manuscript x z developed the code for the lai data assimilation within the lis framework v m and p h conceptualized the experiment s v and d m provided the lis 7 2 version code configuration files and helped with mode debugging all co authors contributed to the interpretation of results 
8582,this study evaluates the potential of assimilating phenology observations using a direct insertion di method by constraining the modeled terrestrial carbon dynamics with synthetic observations of vegetation condition specifically observations of leaf area index lai are assimilated in the noah multi parameterization noah mp land surface model across the continental united states during a 5 year period an observing system simulation experiment osse was developed to understand and quantify the model response to assimilating lai information through di when the input precipitation is strongly biased this is particularly significant in data poor regions like africa and south asia where satellite and re analysis products known to be affected by significant biases are the only available precipitation data to drive a land surface model results show a degradation in surface and rootzone soil moisture after assimilating lai within noah mp but an improvement in intercepted liquid water and evapotranspiration with respect to the open loop simulation a free run with no lai assimilation in terms of carbon and energy variables net ecosystem exchange amount of carbon in shallow soil and surface soil temperature are improved by the lai di although canopy sensible heat is degraded overall the assimilation of lai has larger impact in terms of reduced systematic and random errors over the great plains cropland shrubland and grassland moreover lai da shows a greater improvement when the input precipitation is affected by a positive wet bias than the opposite case in which precipitation shows a dry bias keywords data assimilation direct insertion leaf area index dynamic vegetation model dvm land surface model lsm land information systems lis 1 introduction vegetation supports critical functions in the biosphere as it regulates both the biogeochemical water carbon and nitrogen and energy cycles from the local to the global scale vegetation also strongly affects soil characteristics including soil volume chemistry and texture which has a feedback on various vegetation characteristics including productivity and structure kumi boateng et al 2012 vegetation dynamics are therefore crucial when modeling the land surface littell et al 2011 dynamic vegetation models dvms have been designed to represent structural and functional variables that control land surface energy carbon nutrient and water budgets wullschleger et al 2014 peterson et al 2014 dvms and land surface models lsms have been combined in the past to improve the estimation of water carbon and energy cycle processes clark et al 2011 dai et al 2003 for instance the noah multi parameterization lsm hereinafter noah mp developed by niu et al 2011 uses multiple options for key land hydrologic processes together with a module that allocates carbon to various parts of vegetation and soil carbon pools satellite observations offer a valid alternative for monitoring vegetation globally producing maps of indices such the leaf area index lai defined as the one sided leaf surface area measured over unit ground and the normalized difference vegetation index ndvi based on spectral reflectance measurements acquired in the red and near infrared regions lai has been proven to be a useful indicator of the exchange of water vapor and co2 between the vegetation canopy and atmosphere xiao et al 2016 albergel et al 2017 whereas ndvi is an indicator of the density of green vegetation on a patch of land yang et al 2012 for example the moderate resolution imaging spectroradiometer modis has been acquiring data in 36 spectral bands since 2000 rees and danks 2007 at resolutions of 500 1000 m every 4 8 days and the advanced very high resolution radiometer avhrr tucker et al 2005 produces global maps of lai at a resolution of 4 km every 10 days nevertheless satellite based observations often have gaps in their spatial and temporal coverage mainly due to cloud coverage in order to fill in such gaps and guarantee continuous time series observations are commonly merged with model simulations data assimilation is a well known technique for optimally combining the information from such observations and model estimates based on their respective uncertainties reichle 2008 land data assimilation systems ldass have been successfully used in the past decades to merge satellite observations of soil moisture and surface temperature into lsms e g reichle et al 2008 reichle 2008 maggioni and houser 2017 in the recent past a few attempts applied the same concept to vegetation observations and vegetation dynamics models for instance albergel et al 2010 assimilated observations of lai and surface soil moisture ssm within an lsm the joint assimilation of ssm and lai was tested at the local scale in western france and showed a positive impact on the estimation of carbon water and heat fluxes barbu et al 2011 developed an lsm to simulate photosynthesis processes surface carbon fluxes and vegetation biomass and jointly assimilated soil moisture and lai data in another study by albergel et al 2017 a global land data assimilation system ldas monde was applied over europe and the mediterranean basin to improve land surface variable estimation when ssm and lai satellite derived observations were assimilated using a simplified extended kalman filter ldas monde was more effective in estimating soil moisture in the top soil layers but model sensitivity to ssm decreased with depth and had almost no impact below 60 cm kumar et al 2019 conducted an experiment for assessing the impact of assimilating satellite based lai using an ensemble kalman filter over the continental u s conus in the noah mp lsm results demonstrated that lai da has a beneficial impact on the simulation of key water budget terms i e soil moisture evapotranspiration terrestrial water storage and streamflow and carbon fluxes i e gross primary production and net ecosystem exchange when compared to a large suite of ground based reference datasets ling et al 2019 assimilated global land surface satellite glass lai data using an ensemble adjustment kalman filter technique globally results showed that the assimilation was able to reduce the bias in the lai especially in low latitude regions from 5 m2 m2 to 1 m2 m2 another recent work by rajib et al 2020 assimilated modis lai data across eastern iowa in the united states and showed improvements in the estimation of root zone soil moisture and water quality indicators this work builds upon these past studies while also introducing several novelties first off it investigates the impacts of assimilating phenology observations in a land data assimilation system when the precipitation input data are strongly either positively or negatively biased this is particularly crucial for assessing the hydrological conditions in data poor regions like africa and south asia where precipitation information relies on satellite based and model re analysis products which are well known to be affected by severe biases koutsouris et al 2016 singh and xiaosheng 2019 steinschneider et al 2019 for instance ghatak et al 2018 showed biases up to 70 in a suite of precipitation datasets both satellite based and model re analyses that were compared to a gauge based product across south asia yoon et al 2019 compared ten different satellite based and re analysis precipitation datasets across high mountain asia and found average biases of 20 second this work investigates the efficiency of di as the data assimilation approach i e the lai model state is directly replaced by observations whenever the latter become available although a very simple method di does not require any unbiasedness assumptions to operate in an optimal mode as opposed to more sophisticated techniques such as kalman filters since biases in atmospheric forcing and precipitation in particular are often large and unknown blind bias in data poor regions di represents a unique approach to assess the potential of improving the estimation of land surface variables by assimilating vegetation observations being the first attempt to assimilate lai using di under strongly biased precipitation input this work presents a synthetic and therefore fully controlled experiment across conus to assess the potential of such approach 2 methodology this study proposes an observing system simulation experiment osse to better understand and quantify the noah mp model response to assimilating lai information using di the experiment focuses on conus from 250n 1250w to 530n 670w fig 1 during 2011 2015 the nasa land information system framework lis kumar et al 2006 which includes several land surface models and a data assimilation system has been used in this experiment and noah mp described in detail in section 2 1 is chosen as the lsm 2 1 noah mp the noah mp model has a semi tile sub grid scheme in which the canopy layer is separated from the land surface niu et al 2011 the shortwave radiation transfer is computed over the entire grid cell while longwave radiation latent heat sensible heat and ground heat fluxes are computed separately over two tiles a fractional vegetated area fveg and a fractional bare ground area 1 fveg multiple options are available in noah mp for surface water infiltration runoff groundwater transfer and storage dynamic vegetation canopy resistance and frozen soil physics niu and yang 2007 specifically the prognostic vegetation growth combines a ball berry photosynthesis based stomatal resistance ball et al 1987 with a dvm dickinson 1983 that allocates carbon to various parts of vegetation leaf stem wood and root and soil carbon pools fast and slow in our experiment noah mp is forced with the north american land data assimilation system second phase nldas 2 xia et al 2012 nldas 2 is an upgraded version of the first phase of the multi institution nldas 1 mitchell 2004 project which was initiated to provide coupled atmosphere ocean land models with reliable initial land surface states for improving weather predictions xia et al 2012 it has a time window of 40 years 1979 present 1 8 spatial resolution and hourly temporal resolution noah mp is spun up for 30 years 1981 2010 and run at a 15 min time step model daily output are considered in this study 2 2 the observing system simulation experiment synthetic experiments e g osses are useful to quantitively assess the potential impact of land data assimilation systems before they are developed and deployed hoffman and atlas 2016 osses are mostly designed to investigate data assimilation ideas and have the advantage of being controlled experiments in which the reference or truth is known in an osse the observations are simulated by a model rather than being real observations masutani et al 2010 such observations are drawn from a perturbation free run the nature run nr which serves as the baseline simulation commonly referred to as synthetic truth to which the output from all the other runs are compared to similarly in our experiment described by the framework in fig 2 the output from the nr which is forced with the original unperturbed nldas atmospheric forcing dataset is considered as the synthetic truth in terms of not only lai but also for all the other water carbon and energy variables of interest in order to produce the synthetic observations the lai output from the nature run nr lai is aggregated to daily scale and perturbed by a multiplicative random error model with zero mean and a standard deviation of 0 1 to mimic the uncertainties associated with real observations this is a simplistic way of representing such uncertainties and more complex error models e g seasonally dependent could be investigated in future studies that make use of actual satellite products next two sets of open loop ol simulations i e no assimilation are run by perturbing the nldas 2 precipitation to half and double of the original nldas 2 data to generate two extreme cases the one in which nldas 2 has a dry bias and the one in which nldas 2 has a wet bias this is obtained by multiplying the precipitation input by a constant scaling factor i e 0 5 and 2 0 for creating the dry and wet conditions respectively such simple multiplicative error model assumes that the given dataset has perfect detection neither false alarms nor missed events i e no rain rainy pixels will remain no rain rainy pixels in the perturbed forcing dataset this is a strong assumption for re analysis precipitation and for satellite based products as well which are well known to be affected by detection errors nevertheless a series of articles by maggioni et al 2011 2012 2013 showed minimal improvement in soil moisture simulations when a more complex precipitation error model that simulates both false alarm and missed events was considered in a land data assimilation system thus a simple error characterization is chosen for this study but future work should investigate this issue further finally two data assimilation da runs wet and dry are performed by directly replacing the lai model state with observations obtained by the perturbed nr lai in a di da the observation is fully trusted kumar et al 2008 and model estimates are replaced by the observations whenever the latter are available reichle 2008 in this case the model is replaced by the synthetic lai observations every day at 1 00 am di one of the simplest data assimilation methods was chosen in this experiment mainly because it does not require any unbiased model or observation furthermore di is computationally efficient and easy to implement as the updating algorithm does not account for system dynamics or measurement statistics walker et al 2003 in the past di has been successfully used to assimilate snow cover freeze thaw state soil moisture surface albedo in land surface models as well as radiances in aerosols transport models alavi et al 2010 kumar et al 2020 liston et al 1999 sun 2004 weaver et al 2007 xue et al 2019 in this work we propose to apply the same concept to vegetation data by not only updating lai but also the modeled leaf biomass computed by dividing the lai value by the specific leaf area the root mass and stem mass are not updated in this experiment 2 3 evaluation variables and metrics daily water carbon and energy variables listed in table 1 from the ol and da output for both wet and dry conditions are compared to the corresponding true output from the nr run in terms of water variables we consider surface soil moisture ssm m3 m3 defined in noah mp as the water content in the top 10 cm of the soil column rootzone soil moisture rzsm m3 m3 defined as the water content in the top 100 cm of soil intercepted liquid water ilw mm defined as the amount of precipitation that does not reach the soil but that instead gets absorbed by canopy plants and forest floor and total evapotranspiration et kg m2s defined as the total amount of water released to the atmosphere from land and plant as carbon and energy variables we select net ecosystem exchange nee g m2s defined as the amount of carbon exchange between plant and atmosphere short lived carbon in shallow soil css g m2 defined as the short lived carbon pool which is the summation of total leaf and root turnover and total amount of dead leaf canopy sensible heat csh w m2 defined as the total amount of heat transferred to the air from ground and vegetation and surface soil temperature sst k defined as the temperature of the soil in the top 10 cm two performance metrics are considered the relative bias rb and the normalized unbiased root mean square error nubrmse metrics are computed at each grid cell separately for all the experiments ol and da during the 5 year study period rb is an estimate of the systematic error and defined here as the normalized difference between the estimated value either from ol or da and the reference nr variable 1 rb x x nr n x nr n 100 where x represents the output variable from either ol or da runs either from dry or wet conditions xnr is the corresponding nr variable and n is the number of days in order to investigate the random error alone we use the nubrmse following the definition of bhuiyan et al 2018 falck et al 2018 and zhang et al 2020 in which we first remove the bias from the ol and da output variables and then normalize by the reference i e nr values 2 nubrmse x x n x nr x nr n 2 n x nr n 100 normalized metrics allow for easier comparison among variables of different nature and units in this work both rb and nubrmse are shown as percentage values and computed for every model pixel across the study area 3 results changes in lai due to da with respect to the corresponding ol run are shown in fig 3 in the dry condition the application of da introduces additional vegetation to the system to balance the negative bias in the input precipitation in other words the model tends towards the dry side but the observations see more vegetation than the one estimated by the model whereas the wet condition case shows the exact opposite i e lower lai in the da run both in the dry and wet condition the central part of conus is particularly impacted by lai da some areas over the eastern part of conus also show large changes due to the application of lai da such areas mainly correspond to shrublands and croplands fig 1 this is due to the fact that plants characterized by smaller roots are more affected by precipitation whereas plants with longer roots i e woodlands and forests are more resistant to changes in precipitation because they can pull water from the deeper layer of the soil the overall systematic and random errors of the eight variables simulated by the ol or da runs with respect to the reference nr output are computed for both wet and dry conditions rb and nubrmse are averaged across conus and presented in tables 2 and 3 respectively together with differences between the ol and da runs which highlight whether da improves positive sign or worsens negative sign the corresponding ol run ssm rzsm and csh show a degradation with respect to the ol in the average rb after lai is assimilated under with larger degradation under dry condition similarly lai da worsens the nubrmse of soil moisture with a larger degradation in the dry condition for rzsm but with no difference between dry and wet condition for ssm and an improvement in csh on the other hand ilw et nee and css improve after the application of lai da in terms of both systematic and random error with a consistently higher improvement in the wet condition compared to the dry condition with the exception for sst whose statistics only slightly change after the implementation of the lai da scheme in order to further investigate these overall performances we investigate time series during one selected year i e 2013 for averages of all select variables across conus we also assess the spatial variability of the improvement degradation in the evaluation metrics introduced in section 2 3 by showing maps of differences in rb and nubrmse as a last analysis we explore the dependency of the model performance as a function of four main vegetation covers forest and woodland shrubland grassland and cropland results are presented in sub section 3 1 for the water variables ssm rzsm ilw and et and in sub section 3 2 for the carbon and energy variables nee css csh and sst and further discussed in section 4 3 1 water variables as shown in the time series in fig 4 da pushes the modeled ssm and rzsm in the opposite direction with respect to the nr degrading the original ol model simulation this is corroborated by the maps of rb and nubrmse differences in figs 5 and 6 respectively presenting a clear degradation in soil moisture both ssm and rzsm in several regions of the continental us nevertheless some other areas do show an improvement thanks to da e g ssm rb in the southeastern us under wet condition overall the degradation in soil moisture is consistently less pronounced under wet condition in the dry experiment noah mp is fed by only half of the nldas 2 precipitation and there is no additional water source lai da introduces more vegetation to the system and that results in more water absorption which worsens the estimation of soil moisture on the contrary in the wet condition experiment precipitation is doubled adding more water to the system when the soil saturates soil moisture reaches its maximum value and no longer changes which translates into smaller variability in soil moisture under the wet condition compared to the dry condition experiment as shown by the time series in fig 4 and more clearly by the maps in figs 5 and 6 the da simulations of ilw and et improve compared to their corresponding ol runs moving them towards the synthetic truth a decrease in rb is evident across conus for both variables and under both precipitation bias conditions fig 5 however some locations in the south and northeastern us present a degradation in nubrmse fig 6 in the dry and wet condition experiments showing that the lai da cannot correct for the random error in all cases fig 7 summarizes the performance of the proposed da scheme as a function of land cover in general lai da degrades soil moisture but improves ilw and et across all vegetation covers under wet and dry conditions and in terms of both systematic and random errors for ssm ilw and et forest and woodland areas are less impacted by the lai da compared to the other vegetation covers forest and woodland regions have vegetation with deeper roots while shrubland grassland and cropland are characterized by plants with shallower roots the lack of rain dry condition experiment makes the plants with smaller roots dry out and die whereas plants with longer roots can still survive by pulling water from the deeper soil bonan 2002 thus lai da does not have a strong impact on water variables in forest and woodland areas although it does in other regions like the great plains rzsm in the dry condition and in forest and woodland areas shows a larger impact compared to other variables because long rooted plants would pull even more water from the rootzone while precipitation is reduced 3 2 carbon and energy variables all four carbon and energy variables nee css csh and sst present an overall improvement after the application of da in terms of rb and nubrmse tables 2 and 3 fig 4 shows how nee and css are consistently pushed towards the nr by the da scheme during the whole year specifically nee decreases in june july when the lai is the highest which indicates less carbon in the atmosphere and more vegetation on the ground css presents an increasing trend which demonstrates that total amount of living carbon in the soil increases over during the year especially after fall when the leaf and root turnover is the highest however da csh has a noticeable degradation during summer under both dry and wet condition differences between ol sst and da sst are minimal as already highlighted in tables 2 and 3 with an improvement during the summer ol sst is higher than the nr during the summer in the dry run and vice versa in the wet run fig 4 this happens because a decrease increase in vegetation which is due to less or more precipitation in the model input increases decreases soil temperature sst da shows slight degradation in the winter when vegetation is less abundant and soil temperature depends not only on air temperature but also on ssm which degrades after the application of lai da figs 8 and 9 confirm the overall improvement in the selected carbon and energy variables however rb seems to increase after da in a region in the southwestern us and the nee nubrmse has a similar behavior in some areas in the northeastern and northwestern us csh shows a degradation in rb in both dry and wet experiments over the eastern and central part of us but an improvement in nubrmse almost all over conus such degradation in csh corresponds to the same locations where soil moisture shows the largest degradation after da as highlighted in section 2 3 csh is the combined heat flux from ground and vegetation to the atmosphere with the ground to air heat flux being the dominant factor bonan 2002 the heat flux from the ground is highly correlated with the soil water content which is part of the reason why csh worsens after da hwang 1985 guan et al 2009 the change in sst once again is minimal with some regions showing no change between the ol and da runs and some others presenting a slight improvement or a slight degradation this latter is only evident in the nubrmse under wet condition and specifically over forest woodland and cropland areas as demonstrated in fig 10 da in wet condition reduces the vegetation amount modeled by noah mp which would bring sst up however in the northern part of the us sst is more affected by air temperature rather than the surface condition in forest woodland and cropland areas all the variables show an improvement after the application of lai da except for csh in the dry condition fig 10 for all the carbon and energy variables forest and woodland areas are the least impacted after the application of da just like for the water variables 4 discussion in the proposed osse we assimilate daily synthetic observations of lai that mimic temporally smoothed modis observations commonly available every 8 days obtaining satellite observations at such frequency is challenging due to frequent cloud coverage during winters thus the proposed experiment and associated results should be considered as a best case scenario in which lai observations are consistently available every 8 days assimilating observations every 8 days may cause instability in the model dynamics so an alternative approach is proposed here to interpolate between observations and update the model state more often i e every day when actual satellite based lai observations are assimilated in the lsm and ground based measurements are used as reference for validation purposes the lai da performance may differ to the one presented in the results section and what looks like a degradation in the osse may be an improvement in the actual data experiment for instance in this experiment the lai da could not largely improve the rb and nubrmse associated with surface and root zone soil moisture but this may be different if real satellite products are merged within an lsm especially in irrigated fields that are not modeled within the current noah mp framework as a matter of fact kumar et al 2019 show an improvement not only in et but also in rzsm thanks to the assimilation of satellite based lai within a lsm using an ensemble kalman filter across the central plains in us mainly characterized by crops future work should therefore analyze lai soil moisture feedback in the model simulations during different seasons to illustrate why lai assimilation degrades soil moisture estimates in some regions this would be particularly useful for future ecohydrology data assimilation studies systematic errors quantified by rb are reduced thanks to lai da for most variables considered in this study except for ssm rzsm and csh in both dry and wet condition experiments and across the whole study area in terms of random errors nubrmse da improves most of the variables for both wet and dry conditions except the ssm and rzsm with some regions also showing a degradation in nee similarly the work by kumar et al 2019 mentioned above shows an improvement in nee except in the northwestern region of the us and areas around the great lakes in general the improvement in both rb and nubrmse is larger when the precipitation forcing has a positive bias wet condition than in the dry condition experiment this is because in the latter case the amount of water available in the system is limited but the assimilation of lai observations introduces vegetation which causes more root water uptake from the soil and worsens the soil moisture estimation in the osse proposed here some water variables e g et and ilw show an improvement both in dry and wet bias conditions though the improvement under dry biases is marginal these results corroborate what shown in the work recently published by zhang et al 2020 who performed an osse at the global scale to assimilate lai with an ensemble kalman filter technique and observed only marginal improvements in soil moisture contents a degradation in ssm under dry precipitation conditions and significant improvements in et in wet bias conditions furthermore we evaluate the lai da impact on carbon and energy variables most of which except csh improve across the entire domain under both wet and dry bias conditions results show that croplands grasslands and shrublands are the most impacted by the application of lai da as highlighted in figs 7 and 10 as plants with smaller roots are more affected by precipitation which is perturbed in the ol and da simulations while plants with longer roots that characterize woodlands and forests can pull water from the deeper soil and are therefore more resistant to changes in precipitation this also aligns with the findings by zhang et al 2020 who showed a larger impact of the lai da in regions characterized by short root plants when compared to forests and woodlands in summary di of lai can improve almost all the carbon and energy variables under both wet and dry conditions and across different vegetation covers nevertheless the proposed da scheme was not able to improve surface and rootzone soil moisture soil moisture plays a fundamental role in land surface processes and in the water carbon and energy cycles as it i controls the partitioning of the energy incident on the land surface ii is a storage component for precipitation and radiation and therefore impact cloud formation precipitation runoff and evapotranspiration and iii governs several feedbacks at the local regional and global scale e g soil moisture temperature and soil moisture precipitation previous work on the joint assimilation of lai and soil moisture observations has shown promising improvements in both water and carbon dioxide fluxes albergel et al 2010 albergel et al 2017 barbu et al 2011 and bonan et al 2019 however these studies have only focused on limited domain or for a specific crop type thus future research work should look at dual assimilation methods that merge different types of observations e g surface soil moisture and lai if the goal is to improve land surface processes as a whole 5 conclusions this work proposes a synthetic experiment to investigate the impact of assimilating lai observations within a land surface model through di the experiment is performed across the continental us during a 5 year time period january 2011 december 2015 adopting the noah mp lsm forced with nldas 2 meteorological forcing data the efficiency of lai da is investigated in terms of several water carbon and energy states and fluxes under strongly both negatively and positively biased precipitation forcing although di is a very naïve da approach it does not require any unbiasedness assumptions to optimally operate as opposed to the more sophisticated kalman filter which is critical since biases in atmospheric forcing and precipitation in particular are often large and unknown in summary this study demonstrates how assimilating lai observations through the simplest da method available i e direct insertion has the potential to improve lsm estimation of some water e g ilw carbon e g nee and energy e g sst variables these improvements are often a function of the land cover as demonstrated in this work thus future work should be directed towards a more in depth investigation of such dependence to identify what vegetation types would gain the most from assimilating lai observations however as results from this study demonstrate that soil moisture variables would not benefit from the assimilation of lai alone the combined assimilation of both lai and surface soil moisture observations should also be explored this work opens new research directions as real observational data e g modis and glass products available globally could be assimilated in the proposed da framework which could then be expanded to the global scale and benefit several regions of the world where ground monitoring of water energy and carbon states and fluxes is limited but extremely important e g amazon forest high mountain asia the efficiency of the proposed di that adopts a simple multiplicative precipitation error model should be also compared to more sophisticated techniques e g ensemble kalman filter and more complex precipitation error models that simulate false alarms and missed events in the original products declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements this work was supported by the nasa modeling analysis and prediction program map program award number 80nssc17k0109 all the model computations were run using the argo cluster administered by the office of research computing at george mason university va http orc gmu edu author contribution a r performed all analyses and wrote the manuscript x z developed the code for the lai data assimilation within the lis framework v m and p h conceptualized the experiment s v and d m provided the lis 7 2 version code configuration files and helped with mode debugging all co authors contributed to the interpretation of results 
8583,many hydrologic and hydraulic h h engineering applications require spatial rainfall distribution over a watershed but point precipitation frequency estimates such as those provided by noaa atlas 14 are only applicable for relatively small areas for larger areas areal reduction factors arfs are commonly used to transform a point precipitation frequency estimate of a given duration and frequency to a corresponding areal estimate the most common source of arfs for the united states is technical paper 29 tp 29 published in 1958 although there have been significant increases in record length and types of available data and several new methods for computing arfs have been proposed over the last several decades this study applied up to date precipitation data products and analysis methods with a watershed based approach to investigate factors that affect arf variabilities and to compare arfs across multiple us hydrologic regions our overall findings are in line with other recent studies showing that arfs decrease with increasing area increase with increasing duration and decrease with increasing return period in particular we found a strong geographical variability across different us hydrologic regions suggesting that arf are specific to regional climate patterns and geographical characteristics and should not be applied arbitrarily to other locations the results also reveal the importance of record length especially for long return period arfs the study demonstrates the need to improve arfs with new data and methods to support more reliable areal precipitation frequency estimates for h h applications keywords areal reduction factor arf precipitation frequency analysis design rainfall 1 introduction areal precipitation frequency estimates e g t year rainfall depth are widely used to inform hydrologic and hydraulic h h modeling for flood hazard applications e g inundation mapping infrastructure protection flood risk mitigation ideally such estimates can be developed from precipitation frequency analysis pfa based on long term rain gauge observations throughout a watershed to avoid going through the entire process of pfa including rain gauge data collection and processing annual maxima searching probabilistic density function fitting goodness of fit test and regionalization h h engineers have often opted to use pre calculated t year rainfall depths from existing pfa products such as the national oceanic and atmospheric administration noaa atlas 14 bonnin et al 2004 and other volumes in conjunction with areal reduction factors arfs to obtain basin average rainfall estimates spatiotemporal distributions can then be applied to the basin average rainfall estimates to form rainfall hyetographs for input into event based h h modeling applications this arf approach has been used in many hydrologic applications including stormwater management e g mgndct 2012 cwcb 2006 and dam safety assessment e g usbr 2004 arf is generally defined as the ratio of areal rainfall depth to point based rainfall depth it can be a fixed geographical area or fixed area estimate that describes the features of maximum precipitation depth in a specific watershed which may contain only a portion of a storm or sum of multiple storms or a storm centered estimate that describes the spatiotemporal evolution of an individual storm for deterministic analysis in general the concept of the fixed area approach is more consistent with pfa and can provide more conservative estimates omolayo 1993 the us weather bureau technical paper no 29 tp 29 us weather bureau 1957 represents the first systematic development of arfs for the us providing estimates for durations up to 24 h and for area sizes up to 1 036 km2 the tp 29 analysis was based on 20 rain gauges from several dense networks with an average record length of about 11 years owing mainly to the limited data length and small study areas the tp 29 arf curves do not characterize any dependence on return period season or geographic location although the tp 29 arf chart was published decades ago it is still used in many engineering applications today several arf methods have been proposed since tp 29 was published but none have been wildey applied in the us reviews and comparison of selected arf methods can be found in svensson and jones 2010 pietersen et al 2015 and pavlovic et al 2016 these review studies have generally classified arf methods into the following categories empirical methods fit a statistical relationship to the collected arf samples from the study area empirical methods originated from tp 29 us weather bureau 1957 leclerc and schaake 1972 with subsequent products including the united kingdom studies nerc 1975 bell 1976 koutsoyiannis and xanthopoulos 1999 national weather service tr 24 myers and zehr 1980 zehr and myers 1984 hydrological atlas of switzerland grebner et al 1998 and australian rainfall and runoff guidelines nathan and weinmann 2016 spatial correlation methods are based on spatial correlation of rainfall fields relying on assumptions of isotropy i e spatial correlation does not vary significantly along a specific direction orientation and particular statistical distributions of the rainfall process examples include rodriguez iturbe and mejía 1974 omolayo 1989 and sivapalan and blöschl 1998 spatial and temporal scaling methods use the concepts of dynamic scaling multifractal and statistical self affinity to reflect the scaling properties of rainfall in space and time examples include de michele et al 2001 and veneziano and langousis 2005 extreme value theory methods extend the generalized extreme value gev distribution widely used in point frequency analysis to areal extreme precipitation examples include durrans et al 2002 and overeem et al 2010 generally speaking most of these previous studies focused on understanding arf variability due to the choice of methodology largely ignoring influences of factors such precipitation dataset properties seasonality and climate geographic setting spatial aggregation of point precipitation i e either from gauge records or from gridded datasets in these studies was typically based on inter gauge distance or simplified spatial windows e g square n by n grids or circular windows considering the end use of arfs to support watershed h h modeling a hydrology focused spatial aggregation method based on watershed geometry is worth exploring in addition with the availability of more abundant precipitation data products it will be of interest to understand how arfs may be improved using more up to date datasets these considerations motivate this study we use the precipitation information from multiple gauge and grid based precipitation products to investigate the influence of dataset choice and to support inter regional comparisons this study looked at factors influencing arf from two perspectives 1 an analysis in a single region ohio river basin to investigate the influence of using different precipitation datasets and 2 an analysis conducted across all hydrologic regions of the conterminous us conus using one selected precipitation product to explore the influence of climate and geographic setting a precipitation annual maximum series ams searching technique based on watershed boundary and connectivity is introduced to identify point and areal ams across different data sources durations spatial aggregation units seasons and hydrologic regions we then use the de michele et al 2001 dynamic scaling model to construct regionally smoothed arf curves for inter comparison and discuss the major factors affecting arf variability and their implications for h h application this paper is organized as follows section 2 provides an overview of the study area methodology and data section 3 presents results and associated discussion and section 4 summarizes and concludes the study 2 data and methods the overall study design is summarized in table 1 which includes the study domains data and methods to examine arf variabilities we only consider fixed area arf in this study storm centered arf was found to be less conservative for estimating areal precipitation frequency omolayo 1993 and is not evaluated since there is no fully objective way to assign storm types to historical precipitation data the influence of storm type is not explicitly explored instead seasonality is used as the main proxy to demonstrate the potential influence of storm type on arf because storm types also correlate to geographic setting and climate the variability over conus will reflect to some extent difference in storm types each factor is discussed further in this section 2 1 study domains two spatial domain scales are considered in this study the first is a regional domain focusing on the ohio river basin orb usgs hydrologic region 05 which was selected due to the availability of relatively good precipitation data coverage and the region s setting as a source of original data used in developing the tp 29 arfs the second spatial scale extends across all usgs hydrologic regions in the conus usgs et al 2013 maps showing the orb and the 18 conus hydrologic regions are provided in fig 1 2 2 data sources to explore the arf variabilities associated with different data sources five precipitation products are considered in this study summarized in table 2 based on the nature of each product they can be classified into three categories gauge only precipitation data refers to the direct measurements taken at gauge stations despite potential issues such as undercatch sieck et al 2007 mekonnen et al 2015 gauge only precipitation data in general has the highest point measurement accuracy however spatial disaggregation of gauge precipitation measurements into watersheds is not a trivial task since one needs to consider various topographic orographic and local adjustments especially in complex terrain in this study we used hourly observations from the national centers for environmental information ncei dsi 3240 dataset overall there are over 300 dsi 3240 stations with more than 30 years of record in and surrounding orb with density 1400 km2 per station gauge driven precipitation products refer to the gridded products that are primarily based on gauge observations during the gridding process topographic orographic or statistical adjustments are made compared with gauge only precipitation data sets gauge driven precipitation products are easier to use and quite popular in many h h applications we selected three commonly used products in this study including 1 daymet thornton et al 1997 maintained by oak ridge national laboratory ornl provides daily gridded precipitation estimates since 1980 throughout north america at a 1 km horizontal resolution 2 prism parameter elevation regressions on independent slopes model daly et al 1994 produced by oregon state university osu provides daily gridded precipitation estimates since 1981 throughout conus at a 1 24 4 km horizontal resolution 3 livneh et al 2015 precipitation and other hydrometeorological variables produced by the university of colorado at boulder ucb provides daily gridded precipitation estimates for 1950 2013 for the conus mexico and the part of canada south of 53 n at a 1 16 6 km horizontal resolution one main difference among the gauge driven gridded products is how precipitation is spatially distributed over complex terrain prism spatially distributes precipitation using precipitation elevation regressions daymet spatially distributes precipitation through an iterative station density algorithm livneh spatially distributes precipitation over complex terrain using a satellite based estimate of peak snow water equivalent and also uses prism climatology for bias correction an intercomparison of gridded precipitation data estimates provided by prism daymet and livneh for the western us by henn et al 2017 suggested that the greatest absolute differences in annual total precipitation occurs in maritime mountain ranges and high elevation areas of the western us 200 mm year or greater on average around 5 60 radar driven precipitation products merge radar reflectivity and other supporting gauge observations to estimate rainfall depth recent studies such as lombardo et al 2006 olivera et al 2008 pavlovic et al 2016 and kim et al 2019 have started to explore the development of arf using radar based information although radar can capture the spatial distribution of extreme storms that cannot be measured by conventional gauges the accuracy of radar based precipitation products can be limited by nonlinear reflectivity rainfall relationships variations in vertical reflectivity blockages and spatial and temporal sampling aghakouchak et al 2010 therefore further data assimilation with gauge observations is needed in this study we used the national center for environmental prediction ncep stage iv quantitative precipitation estimate data set st4 lin 2011 that is available in 4 km horizontal resolution since 2002 the st4 data merges raw radar based estimates with automatic rainfall gauge observations and is further quality controlled by several noaa river forecasting centers rfcs an evaluation performed by gourley et al 2010 suggests that st4 has the highest correlation coefficient with gauge observations among various gridded precipitation products among the gridded datasets daymet has the finest spatial resolution 1 km followed by prism and st4 4 km and livneh 6 km despite livneh s coarser spatial resolution it has the longest record 64 years from 1950 to 2013 followed by daymet 38 years from 1980 to 2017 prism 37 years from 1981 to 2017 and st4 16 years from 2002 to 2017 since both spatial resolution and data length are important features influencing arf trade offs exist in selecting the most appropriate precipitation product to support arf calculation in the regional orb assessment we compare the variability of arf using all five precipitation products we further extend the assessment of prism given the wide usage in many hydrologic studies to other conus hydrologic regions to explore the arf variability across different geographical locations 2 3 ams identification the first step of arf calculation is to identify maximum precipitation from either the ams or partial duration series pds approaches to be consistent with the prominent literature including more recent atlas 14 volumes the ams approach is used in this study also similar to atlas 14 ams are searched for each calendar year january to december instead of each water year october to september while some ams may be different when searched by water year these differences should not lead to very different probability density function fitting results and hence the overall influence is limited since the main purpose of arf is to estimate areal precipitation depth for h h applications for watersheds and catchments the ams should also be identified in a consistent manner however previous arf studies often used simplified spatial units e g square n by n grids or circular windows that do not have specific hydrologic meanings to overcome this limitation in this study we propose to identify ams across different area sizes using the usgs hydrologic units usgs et al 2013 the usgs hydrologic unit code huc is a hierarchical labeling structure to organize us watersheds across different sizes taking orb huc02 id 05 as an example it contains 14 huc04 14 000 85 000 km2 21 huc06 4 400 54 000 km2 and 120 huc08 290 840 km2 using hydrologic units to identify ams would naturally capture the underlying watershed boundaries and river connectivity fig 2 provides a further illustration of ams identification using prism data for year 2002 in orb fig 2 a and b show the annual maximum precipitation identified independently at each prism grid cell and the corresponding annual maximum day the different timing suggests that these maximum values correspond to different events and hence may not be used jointly for areal frequency analysis to identify the areal maximum precipitation depth one needs to first spatially average precipitation data across a spatial unit and then identify the annual maximum fig 2 c d and e show the areal annual maximum precipitation identified at each huc04 huc06 and huc08 unit as expected with increasing area sizes huc08 to huc04 the average precipitation depths decrease suggesting the need for areal reduction through arf in testing this huc based ams identification approach a larger data gap for large area samples was observed i e there are fewer huc04s than huc06s and huc08s to address this issue and increase the ams samples to cover a wider range of watershed sizes the connectivity between huc08s is used to develop additional accumulated huc units hucac taking huc08 05090203 as an example using the connectivity between huc08s fig 1 b we may identify all upstream huc08s contributing to 05090203 and merge them as a new hucac unit for ams searching fig 2 f following this concept hucac is identified for each of the huc08s based on their upstream connectivity when an hucac is identical to an existing huc04 huc06 and huc08 it is not used to avoid double sampling a total of 46 hucac 4 600 420 000 km2 is identified in orb a histogram showing all huc unit area sizes is provided in fig 2 g note that seven huc06s were excluded in this region since they have the same spatial extent with their corresponding huc04s and would have otherwise been double counted to effectively summarize gridded precipitation prism daymet livneh and st4 a conversion table is established indicating what grid points should be included in a specific huc unit this conversion table is then used to spatially average all hourly and daily gridded precipitation into different huc based precipitation for ams identification for dsi 3240 all hourly gauge data in and surrounding orb are spatially interpolated at each prism grid location while the gauge data are ground based and should ideally include topographic adjustments e g elevation lapse rate these considerations are not accounted for during spatial interpolation this is acceptable in most of the orb given its relatively flat terrain but may be unacceptable for watersheds with significant topographic variation in such cases proper topography informed adjustment approaches such as those used in prism daymet and livneh should be followed overall as summarized in table 1 ams are searched for different data durations spatial units and seasons all season january through december warm season may through october cool season january through april and november through december in orb for the conus assessment we conduct ams searching using prism as the only data source in addition we neglect the huc08 connectivity across different hydrologic regions e g from region 14 upper colorado to region 15 lower colorado so that we may derive and compare arf specifically for each hydrologic region 2 4 sample arf estimation after ams has been comprehensively searched for all huc units and each grid point the next step is to calculate the sample arf at each huc these sample arfs would represent the best available watershed specific arf estimates at each huc the arf samples across all hucs can then be grouped and jointly fitted into a regional arf model for more generalized representation let p grid y g represent the annual maximum precipitation at year y and grid cell g and p huc y represent the annual maximum precipitation at year y for a particular huc considering all grid cells in the huc unit the first type of sample arf arf avg can be defined as 1 arf avg p huc a v g p grid a v g 2 p huc a v g y 1 n y p huc y n y 3 p grid a v g y 1 n y g 1 n g p grid y g n y n g where n y is the total number of years and n g is the total number of grid cells in the huc unit following the definition arf avg presents the ratio of average areal ams to the average grid based ams these formulations do not account for the potential influence of precipitation frequency on arf and are similar to the approach used in early arf studies e g tp 29 approximately speaking arf avg defined in this fashion would have a return period around 2 years another type of sample arf involves probability density function pdf fitting so that frequency return period can be introduced into the arf calculation assuming that the gev distribution can be a suitable pdf in this case the t year arf tyr can be defined as 4 arf tyr p huc t y r p grid t y r a v g 5 p huc t y r gev 1 1 1 t μ huc γ huc κ huc 6 p grid t y r a v g g 1 n g gev 1 1 1 t μ g γ g κ g n g where gev 1 represents the inverse of gev μ huc γ huc κ huc represent the gev parameters estimated at the selected huc unit using the huc based ams and μ g γ g κ g represent the gev parameters estimated at each grid cell using the grid based ams as an example the fitted gev parameters of daily prism precipitation for each huc unit in orb are shown in fig 3 while both μ huc and γ huc are scale dependent and show similar patterns across all huc units no clear pattern is found for κ huc clearly other pdfs may be considered in place of gev to provide frequency estimates but epistemic uncertainty due to pdf choice is not coinsidered in this study the maximum likelihood approach is used to estimate the gev parameters after parameter fitting the kolmogorov smirnov ks test is used to examine the goodness of fit at a 5 significance level if a specific case fails to pass the ks test the data point is disregarded for further arf model fitting overall the average arf from eq 1 and the 10 and 100 year estimates from eq 4 are used for regional arf model fitting when calculating individual arf samples it is possible to obtain an arf sample above 1 00 due the artifact of pdf fitting for both huc and grid based ams in these cases an upper arf limit of 1 00 was set for regional arf model development note that frequency can also be introduced into arf from other approaches e g overeem et al 2010 however regardless of which approach is used the limited data record would likely be the greatest hurdle to estimating long return period precipitation and arf even with around 60 years of long term records from livneh and dsi 3240 data sets there may not be sufficient data to support precipitation and arf estimation for return periods greater than 100 year without substantial uncertainty which is associated with extrapolating return period estimates well beyond the record length in addition nonstationarity in a changing environment would add further complication to the frequency analysis these more involved issues are noted but not examined in this study 2 5 arf model fitting the final step is to fit all arf samples in a hydrologic region with a generalized arf model among various potential arf models we select the dynamic scaling model by de michele et al 2001 in this study when comparing several different arf models not shown here we found that the de michele et al 2001 model is generally stable can fit well in a variety of hydrologic regions and has a good underlying theory on spatial and temporal rainfall scaling the good performance of the de michele et al 2001 model was also reported by pavlovic et al 2016 the de michele et al 2001 model is a function of area a km2 or mi2 and duration d hours with four parameters b v w and z 7 a r f a d 1 ω a z d b υ b to fit these four parameters the root mean square error rmse between arf samples and arf model is minimized using the nelder mead simplex algorithm lagarias et al 1998 implemted in the fminsearch function in matlab the performance of fitting is also evaluated by the nash sutcliffe efficiency nse mccuen et al 2006 coefficient given eq 7 is not a function of return period fitting is performed separately for each frequency level of interest i e average ams 10 and 100 year to help understand the uncertainty associated with sampling arfs across different huc units we further quantify the resampling uncertainty through 1 000 bootstrap replications efron and tibshirani 1994 during each round of bootstrap the arf samples calculated from eqs 1 and 4 are randomly re sampled to the same sample size and re fitted with eq 7 the 95 uncertainty bound from lower 2 5 to higher 97 5 quantiles is then identified to estimate the uncertainty because the focus in this study is to explore and reveal the variabilities of arf associated a variety of external non method related factors we only select one arf model for interregional comparison and do not focus on testing and selecting different arf models in each of the hydrologic regions for site specific studies it would be useful to test and compare different arf models to select the most appropriate model for the application 3 results and discussion 3 1 effect of area duration and return period fig 4 shows arfs derived from prism precipitation in orb as a function of area for different durations 1 day 2 day 3 day and frequency levels 2 year 10 year 100 year arf samples calculated at huc units are marked in colored dots while the fitted regional arf curves are marked in colored lines the nse values are also provided and can be used to evaluate the reasonableness of derived regional arf curves in general the results are consistent with common understanding that arf decreases with increasing area and increases with increasing duration the results clearly indicate that arf decreases with increasing return period which was not considered in the conventional tp 29 arf curves the results also show that the variability of arf samples increases with increasing return periods and hence leads to poorer fitting results particularly poor at the 100 year return level in this case the t year arf samples calculated by eqs 4 6 are based on 37 years of prism ams the larger variability and degraded nse values suggest that they may not be sufficient to support the 100 year return level calculation the issue largely relates to the limited observed data record length available from the precipitation products leading to high uncertainty in the model results the huc based ams search approach can effectively increase the number of arf samples across a wide range of area sizes and better enable arf estimation for large area sizes greater than 1 036 km2 provided by tp 29 however given the use of huc08 as the minimum unit our analysis did not include arf samples in the smaller sub huc08 area range e g from 0 to 1000 km2 in fig 2 g this simplification should not affect our results since the arf samples in these smaller areas are close to 1 are likely to have relatively lower variability and have much less influence on the overall arf curve fitting to enhance this step one may consider using an even smaller huc unit e g huc12 as the minimum unit for ams searching we did not adopt huc12 as the minimum searching unit since it is computationally intensive for a conus scale study to gain further insights into the variability of subdaily arf fig 5 provides a comparison of 10 year arf using dsi 3240 hourly precipitation across 9 different durations 1 hr 2 hr 3 hr 4 hr 6 hr 12 hr 18 hr 1 day 2 day 3 day dsi 3240 is selected for this comparison given its hourly resolution and relatively long record similar to the findings reported by nerc 1975 the subdaily arf decreases with decreasing durations note that each duration is fit separately instead of fitting a single set of arf parameters across all durations because the fitting performance was unacceptable when trying to fit a single parameter set across both longer 1 2 and 3 day and shorter less than 1 day durations the implication for derivation of a generalized arf model is that one may need to develop separate models for long medium and short durations the subdaily analysis can also be performed using the radar driven hourly st4 precipitation but the limited st4 period of record would lead to more noisy results especially at longer return periods 3 2 effect of data source fig 6 provides a comparison of 1 day arf and nse fitting statistics in orb across different precipitation data sources prism daymet st4 livneh and dsi 3240 and frequency levels the st4 reveals overall lower arfs than other data sources with worse overall performance the models using daymet livneh prism and dsi 3240 data produce more similar results the consistent trends among different precipitation data products suggest that the arf variability is not particularly sensitive to data type we note that the difference between st4 based arf and the other precipitation products increases with increasing of return period this is likely a result of shorter st4 data record length while st4 provides hourly data and can better represent spatial variability it only has 16 years of record as compared to 37 64 years of record for other precipitation products the overall results suggest that data length is a more important factor to consider especially for the calculation of longer return period arfs 3 3 effect of seasonality fig 7 provides a comparison of 1 day 10 year prism arf in orb across different seasons the fitting performance of arf across different seasons is largely similar with cool season having slightly smaller nse the results indicate that the warm season arf is close to all season arf while cool season arf has a much higher value the similarity between warm and all seasons suggests that the annual extreme precipitation in orb mainly occurs during warm season the differences in warm and cool seasons arf can be explained by their respective controlling extreme precipitation processes in orb the major extreme precipitation events during warm season are meso scale convective storms that are generally smaller in size and have larger spatial variability leading to smaller arf on the other hand the major extreme precipitation events in orb during cool season are mostly large scale frontal systems with relatively smaller spatial variability as compared to warm season convective storms that leads to larger arf for h h applications such as simulation of rain on snow during cool season the results suggest that a specific cool season arf may be needed considering the different regional climate characteristics the definition of seasons may need to be adjusted which deserve further exploration 3 4 effect of geographic location fig 8 shows fitted arf curves across the 18 conus hydrologic regions for 1 day duration and 10 year return period the maximum area plotted for each region represents the largest huc unit analyzed in the region the results demonstrate that while arf values are somewhat consistent for small area sizes e g arfs are within about 4 for areas below 100 km2 the variation increases for larger areas for example at 10 000 km2 the fitted arfs range from approximately 0 90 in the new england region region 01 to 0 76 in the souris red rainy region 09 and texas gulf region 12 regions this regional variation is illustrated in fig 9 which generally shows that lower arfs are found in the central us with higher arfs found in the eastern and western us several factors could contribute to this pattern but it is expected that regional climate coastal proximity topographical influences and frequency of certain storm types intensities could influence ams values across regions and influence arf estimates it is worth noting that the results in fig 9 are based on 37 years of prism data thus the fitting performance summarized in table 3 is relatively poor in longer return periods the nse values vary regionally with the lowest values found in regions 01 new england 02 mid atlantic 03 south atlantic gulf 04 great lakes and 16 great basin while regions 01 02 and 04 are all in the northeastern us a clear explanation for the performance results is not apparent and would require further investigation the relatively poorer performance in certain regions can be explained by the wider variability in the calculated huc unit arfs one possible explanation for the wider variability is that these regions contain multiple separate river systems e g non hydrologically connected huc04s and hence involves larger variabilities it is also possible that terrain effects could play a role and that a mixture of more extreme precipitation processes e g from coastal hurricanes to topographically enhanced precipitation regardless the results clearly demonstrate a large variability of arf across different geographical regions this finding suggests that the existing arf formulations from published literature are location specific and application of estimates in one region may not be generalizable to other regions this finding is consistent with mineo et al 2018 which compared the derived arf estimates for the lazio italy region with several published arf estimates and reported substantial over or under estimation therefore arf derived in distinct geographical and climate regions should not be directly transported during h h application 3 5 uncertainty quantification in addition to assessing how various factors may affect arf values we further quantify the sampling uncertainty of arfs across different huc units through 1 000 bootstrap replications efron and tibshirani 1994 taking prism precipitation in orb as an example i e fig 4 fig 10 illustrates the median value and 95 uncertainty bounds of 1 day 2 day 3 day and 2 year 10 year 100 year arfs the uncertainty bounds are very narrow for 2 year arfs and become wider for 10 year and 100 year arfs despite the spread of arf samples in fig 4 it is encouraging to see that the bootstrap uncertainty bounds are narrow in most of the cases this can be due to two possible reasons 1 a large number 194 of huc units is used during the fitting thereby reducing the uncertainty and 2 stability provided by the de michele et al 2001 model the uncertainty bounds behave consistently with the fitting statistics in which a better fitting can lead to a narrower uncertainty bound the uncertainty evaluated through bootstrap represents another view of arf variability considering all factors that affect arf e g methods data sources geographical locations seasonality in depth arf uncertainty analysis is needed to support further applications such integrated arf uncertainty analysis is beyond the scope of this pilot study but warrants further evaluation in the future research 3 6 comparison with tp 29 finally we compare 1 day orb arf with the conventional tp 29 values us weather bureau 1957 in fig 11 based on the multi data source arfs shown in fig 6 we consider the area range of tp 29 0 to 1 000 km2 for a more detailed view although our huc08 based analysis does not capture many data points in the 0 1 000 km2 range given that the de michele et al 2001 arf model has a mathematical upper bound at 1 when area equals 0 the fitting with a much broader set of arf samples across different areas can still yield reasonable results in the 0 1 000 km2 area range the 100 year st4 curve is not included in fig 11 given its poor fit shown in fig 6 c considering the effects of data source and geographical variability discussed in the prior sections it is not surprising to see differences between tp 29 and arf curves derived for orb in this case the 1 day tp 29 curve is closest to the 10 year st4 curve however it should be noted that while the decreasing trend of tp 29 is generally consistent in the range from 0 to 400 km2 the tp 29 1 day arf curve starts to flatten out in the range from 400 to 1 000 km2 considering that tp 29 was derived based on limited stations it is likely that tp 29 did not capture the arf trend in the larger area sizes combined with the spatial variability shown in fig 8 the results suggest that the tp 29 arf should not be considered applicable across different study areas without further diagnosis and evaluation 4 summary and conclusions utilizing a huc based ams searching technique this study investigates factors influencing arf including area duration return period data source seasonality and geographical location in general our results are consistent with previous studies that found arf decreases with increasing area and increases with increasing duration our results clearly suggest that arf decreases with increasing return period which was not addressed in the conventional tp 29 arf curves this finding supports the need to develop frequency return period specific arf for h h application however the results also suggest that the performance of high return level arf is strongly controlled by the length of historic observation e g the poor performance of 100 year st4 arf fitting given only 16 years of observation therefore when using a purely observation based approach such as the one used in this study it is questionable if one has sufficient ability and confidence to derive high return level areal extreme rainfall and arf estimates to support h h application this limitation should be clearly acknowledged when selecting and designing probabilistic flood hazard assessment unless the data sufficiency issue is addressed high return level arfs should be estimated using more conservative and reliable low return period arfs to overcome the data barrier it is worth exploring how and if proper process based numerical weather modeling could be used to support the characterization of probabilistic large scale spatiotemporal storm structures and the estimation of fixed geographical area high return level arfs regarding data sources while the effect of data sources is relatively smaller than duration and return periods non negligible differences are still found the gridded precipitation products e g daymet prism livneh are easy to use but given their limited temporal resolution they cannot be used to derive subdaily arf the radar driven precipitation product st4 can better capture high resolution spatiotemporal storm structure however given its limited period of record it provides the worst high return level arf model fitting across all precipitation products while gauge data dsi 3240 are harder to process the associated arf model fitting performance is among the best in orb with further consideration of topography informed spatial interpolation there is value to consider gauge data based approaches in future site specific arf studies regarding seasonality the results suggest that the warm season arf is close to all season arf while cool season arf is generally higher the similarity between warm and all seasons indicates that the annual maximum precipitation in orb mainly occurs during warm season the differences in warm and cool seasons arf can be explained by their respective controlling extreme precipitation processes in most regions the major extreme precipitation events during warm season are meso scale convective storms that area generally smaller in size and have larger spatial variability leading to smaller arf on the other hand the major extreme precipitation events in most regions during cool season are mostly large scale frontal systems with relatively smaller spatial variability as compared to warm season convective storms that leads to larger arf for h h applications such as simulation of rain on snow during cool season cool season arf will be needed regarding geographical variability it is found that arfs are lower in the central us and higher in eastern and western us texas gulf region 12 souris red rainy region 09 are generally the lowest among all regions the results clearly suggest a strong geographical variability associated with arf therefore arf values produced from previous studies e g tp 29 should not be indiscriminately used at different geographical locations this study demonstrates a need to derive region or watershed specific arf for robust probabilistic flood hazard evaluation while the results of this study suggest that the advance of precipitation products and more advanced arf methods can enable more defensible arf estimates for h h applications several major challenges are also identified for instance long return period areal extreme rainfall and arf derivation is found to be one major challenge long return period in this case refers to when the desired return period is much longer than the period of records of the supporting precipitation data e g deriving 1000 year extreme rainfall estimate based on 66 years of data in addition potential changes in climatic conditions may further complicate this data record challenge from this perspective there is a need to explore suitable methods for the derivation of long return period areal extreme rainfall and arf through the assistance of process based numerical weather simulation overall this study demonstrates the needs to improve arfs with new data and methods for more reliable areal extreme precipitation estimates to support h h applications credit authorship contribution statement shih chieh kao conceptualization methodology software formal analysis investigation resources writing original draft visualization scott t deneale conceptualization methodology formal analysis investigation writing original draft visualization project administration elena yegorova conceptualization methodology writing review editing supervision funding acquisition joseph kanney conceptualization methodology writing review editing meredith l carr conceptualization methodology writing review editing declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgment this work was supported by the us nuclear regulatory commission nrc office of nuclear regulatory research as part of the nrc probabilistic flood hazard assessment research program sck and std are employees of ut battelle llc under contract de ac05 00or22725 with the us department of energy accordingly the us government retains and the publisher by accepting the article for publication acknowledges that the us government retains a nonexclusive irrevocable worldwide license to publish or reproduce the published form of this manuscript or allow others to do so for us government purposes all data used in this study was obtained from public sources which are identified in the paper this report was prepared as an account of work sponsored by an agency of the us government neither the us government nor any agency thereof nor any of their employees makes any warranty express or implied or assumes any legal liability or responsibility for the accuracy completeness or usefulness of any information apparatus product or process disclosed or represents that its use would not infringe privately owned rights reference herein to any specific commercial product process or service by trade name trademark manufacturer or otherwise does not necessarily constitute or imply its endorsement recommendation or favoring by the us government or any agency thereof the views and opinions of authors expressed herein do not necessarily state or reflect those of the us government or any agency thereof 
8583,many hydrologic and hydraulic h h engineering applications require spatial rainfall distribution over a watershed but point precipitation frequency estimates such as those provided by noaa atlas 14 are only applicable for relatively small areas for larger areas areal reduction factors arfs are commonly used to transform a point precipitation frequency estimate of a given duration and frequency to a corresponding areal estimate the most common source of arfs for the united states is technical paper 29 tp 29 published in 1958 although there have been significant increases in record length and types of available data and several new methods for computing arfs have been proposed over the last several decades this study applied up to date precipitation data products and analysis methods with a watershed based approach to investigate factors that affect arf variabilities and to compare arfs across multiple us hydrologic regions our overall findings are in line with other recent studies showing that arfs decrease with increasing area increase with increasing duration and decrease with increasing return period in particular we found a strong geographical variability across different us hydrologic regions suggesting that arf are specific to regional climate patterns and geographical characteristics and should not be applied arbitrarily to other locations the results also reveal the importance of record length especially for long return period arfs the study demonstrates the need to improve arfs with new data and methods to support more reliable areal precipitation frequency estimates for h h applications keywords areal reduction factor arf precipitation frequency analysis design rainfall 1 introduction areal precipitation frequency estimates e g t year rainfall depth are widely used to inform hydrologic and hydraulic h h modeling for flood hazard applications e g inundation mapping infrastructure protection flood risk mitigation ideally such estimates can be developed from precipitation frequency analysis pfa based on long term rain gauge observations throughout a watershed to avoid going through the entire process of pfa including rain gauge data collection and processing annual maxima searching probabilistic density function fitting goodness of fit test and regionalization h h engineers have often opted to use pre calculated t year rainfall depths from existing pfa products such as the national oceanic and atmospheric administration noaa atlas 14 bonnin et al 2004 and other volumes in conjunction with areal reduction factors arfs to obtain basin average rainfall estimates spatiotemporal distributions can then be applied to the basin average rainfall estimates to form rainfall hyetographs for input into event based h h modeling applications this arf approach has been used in many hydrologic applications including stormwater management e g mgndct 2012 cwcb 2006 and dam safety assessment e g usbr 2004 arf is generally defined as the ratio of areal rainfall depth to point based rainfall depth it can be a fixed geographical area or fixed area estimate that describes the features of maximum precipitation depth in a specific watershed which may contain only a portion of a storm or sum of multiple storms or a storm centered estimate that describes the spatiotemporal evolution of an individual storm for deterministic analysis in general the concept of the fixed area approach is more consistent with pfa and can provide more conservative estimates omolayo 1993 the us weather bureau technical paper no 29 tp 29 us weather bureau 1957 represents the first systematic development of arfs for the us providing estimates for durations up to 24 h and for area sizes up to 1 036 km2 the tp 29 analysis was based on 20 rain gauges from several dense networks with an average record length of about 11 years owing mainly to the limited data length and small study areas the tp 29 arf curves do not characterize any dependence on return period season or geographic location although the tp 29 arf chart was published decades ago it is still used in many engineering applications today several arf methods have been proposed since tp 29 was published but none have been wildey applied in the us reviews and comparison of selected arf methods can be found in svensson and jones 2010 pietersen et al 2015 and pavlovic et al 2016 these review studies have generally classified arf methods into the following categories empirical methods fit a statistical relationship to the collected arf samples from the study area empirical methods originated from tp 29 us weather bureau 1957 leclerc and schaake 1972 with subsequent products including the united kingdom studies nerc 1975 bell 1976 koutsoyiannis and xanthopoulos 1999 national weather service tr 24 myers and zehr 1980 zehr and myers 1984 hydrological atlas of switzerland grebner et al 1998 and australian rainfall and runoff guidelines nathan and weinmann 2016 spatial correlation methods are based on spatial correlation of rainfall fields relying on assumptions of isotropy i e spatial correlation does not vary significantly along a specific direction orientation and particular statistical distributions of the rainfall process examples include rodriguez iturbe and mejía 1974 omolayo 1989 and sivapalan and blöschl 1998 spatial and temporal scaling methods use the concepts of dynamic scaling multifractal and statistical self affinity to reflect the scaling properties of rainfall in space and time examples include de michele et al 2001 and veneziano and langousis 2005 extreme value theory methods extend the generalized extreme value gev distribution widely used in point frequency analysis to areal extreme precipitation examples include durrans et al 2002 and overeem et al 2010 generally speaking most of these previous studies focused on understanding arf variability due to the choice of methodology largely ignoring influences of factors such precipitation dataset properties seasonality and climate geographic setting spatial aggregation of point precipitation i e either from gauge records or from gridded datasets in these studies was typically based on inter gauge distance or simplified spatial windows e g square n by n grids or circular windows considering the end use of arfs to support watershed h h modeling a hydrology focused spatial aggregation method based on watershed geometry is worth exploring in addition with the availability of more abundant precipitation data products it will be of interest to understand how arfs may be improved using more up to date datasets these considerations motivate this study we use the precipitation information from multiple gauge and grid based precipitation products to investigate the influence of dataset choice and to support inter regional comparisons this study looked at factors influencing arf from two perspectives 1 an analysis in a single region ohio river basin to investigate the influence of using different precipitation datasets and 2 an analysis conducted across all hydrologic regions of the conterminous us conus using one selected precipitation product to explore the influence of climate and geographic setting a precipitation annual maximum series ams searching technique based on watershed boundary and connectivity is introduced to identify point and areal ams across different data sources durations spatial aggregation units seasons and hydrologic regions we then use the de michele et al 2001 dynamic scaling model to construct regionally smoothed arf curves for inter comparison and discuss the major factors affecting arf variability and their implications for h h application this paper is organized as follows section 2 provides an overview of the study area methodology and data section 3 presents results and associated discussion and section 4 summarizes and concludes the study 2 data and methods the overall study design is summarized in table 1 which includes the study domains data and methods to examine arf variabilities we only consider fixed area arf in this study storm centered arf was found to be less conservative for estimating areal precipitation frequency omolayo 1993 and is not evaluated since there is no fully objective way to assign storm types to historical precipitation data the influence of storm type is not explicitly explored instead seasonality is used as the main proxy to demonstrate the potential influence of storm type on arf because storm types also correlate to geographic setting and climate the variability over conus will reflect to some extent difference in storm types each factor is discussed further in this section 2 1 study domains two spatial domain scales are considered in this study the first is a regional domain focusing on the ohio river basin orb usgs hydrologic region 05 which was selected due to the availability of relatively good precipitation data coverage and the region s setting as a source of original data used in developing the tp 29 arfs the second spatial scale extends across all usgs hydrologic regions in the conus usgs et al 2013 maps showing the orb and the 18 conus hydrologic regions are provided in fig 1 2 2 data sources to explore the arf variabilities associated with different data sources five precipitation products are considered in this study summarized in table 2 based on the nature of each product they can be classified into three categories gauge only precipitation data refers to the direct measurements taken at gauge stations despite potential issues such as undercatch sieck et al 2007 mekonnen et al 2015 gauge only precipitation data in general has the highest point measurement accuracy however spatial disaggregation of gauge precipitation measurements into watersheds is not a trivial task since one needs to consider various topographic orographic and local adjustments especially in complex terrain in this study we used hourly observations from the national centers for environmental information ncei dsi 3240 dataset overall there are over 300 dsi 3240 stations with more than 30 years of record in and surrounding orb with density 1400 km2 per station gauge driven precipitation products refer to the gridded products that are primarily based on gauge observations during the gridding process topographic orographic or statistical adjustments are made compared with gauge only precipitation data sets gauge driven precipitation products are easier to use and quite popular in many h h applications we selected three commonly used products in this study including 1 daymet thornton et al 1997 maintained by oak ridge national laboratory ornl provides daily gridded precipitation estimates since 1980 throughout north america at a 1 km horizontal resolution 2 prism parameter elevation regressions on independent slopes model daly et al 1994 produced by oregon state university osu provides daily gridded precipitation estimates since 1981 throughout conus at a 1 24 4 km horizontal resolution 3 livneh et al 2015 precipitation and other hydrometeorological variables produced by the university of colorado at boulder ucb provides daily gridded precipitation estimates for 1950 2013 for the conus mexico and the part of canada south of 53 n at a 1 16 6 km horizontal resolution one main difference among the gauge driven gridded products is how precipitation is spatially distributed over complex terrain prism spatially distributes precipitation using precipitation elevation regressions daymet spatially distributes precipitation through an iterative station density algorithm livneh spatially distributes precipitation over complex terrain using a satellite based estimate of peak snow water equivalent and also uses prism climatology for bias correction an intercomparison of gridded precipitation data estimates provided by prism daymet and livneh for the western us by henn et al 2017 suggested that the greatest absolute differences in annual total precipitation occurs in maritime mountain ranges and high elevation areas of the western us 200 mm year or greater on average around 5 60 radar driven precipitation products merge radar reflectivity and other supporting gauge observations to estimate rainfall depth recent studies such as lombardo et al 2006 olivera et al 2008 pavlovic et al 2016 and kim et al 2019 have started to explore the development of arf using radar based information although radar can capture the spatial distribution of extreme storms that cannot be measured by conventional gauges the accuracy of radar based precipitation products can be limited by nonlinear reflectivity rainfall relationships variations in vertical reflectivity blockages and spatial and temporal sampling aghakouchak et al 2010 therefore further data assimilation with gauge observations is needed in this study we used the national center for environmental prediction ncep stage iv quantitative precipitation estimate data set st4 lin 2011 that is available in 4 km horizontal resolution since 2002 the st4 data merges raw radar based estimates with automatic rainfall gauge observations and is further quality controlled by several noaa river forecasting centers rfcs an evaluation performed by gourley et al 2010 suggests that st4 has the highest correlation coefficient with gauge observations among various gridded precipitation products among the gridded datasets daymet has the finest spatial resolution 1 km followed by prism and st4 4 km and livneh 6 km despite livneh s coarser spatial resolution it has the longest record 64 years from 1950 to 2013 followed by daymet 38 years from 1980 to 2017 prism 37 years from 1981 to 2017 and st4 16 years from 2002 to 2017 since both spatial resolution and data length are important features influencing arf trade offs exist in selecting the most appropriate precipitation product to support arf calculation in the regional orb assessment we compare the variability of arf using all five precipitation products we further extend the assessment of prism given the wide usage in many hydrologic studies to other conus hydrologic regions to explore the arf variability across different geographical locations 2 3 ams identification the first step of arf calculation is to identify maximum precipitation from either the ams or partial duration series pds approaches to be consistent with the prominent literature including more recent atlas 14 volumes the ams approach is used in this study also similar to atlas 14 ams are searched for each calendar year january to december instead of each water year october to september while some ams may be different when searched by water year these differences should not lead to very different probability density function fitting results and hence the overall influence is limited since the main purpose of arf is to estimate areal precipitation depth for h h applications for watersheds and catchments the ams should also be identified in a consistent manner however previous arf studies often used simplified spatial units e g square n by n grids or circular windows that do not have specific hydrologic meanings to overcome this limitation in this study we propose to identify ams across different area sizes using the usgs hydrologic units usgs et al 2013 the usgs hydrologic unit code huc is a hierarchical labeling structure to organize us watersheds across different sizes taking orb huc02 id 05 as an example it contains 14 huc04 14 000 85 000 km2 21 huc06 4 400 54 000 km2 and 120 huc08 290 840 km2 using hydrologic units to identify ams would naturally capture the underlying watershed boundaries and river connectivity fig 2 provides a further illustration of ams identification using prism data for year 2002 in orb fig 2 a and b show the annual maximum precipitation identified independently at each prism grid cell and the corresponding annual maximum day the different timing suggests that these maximum values correspond to different events and hence may not be used jointly for areal frequency analysis to identify the areal maximum precipitation depth one needs to first spatially average precipitation data across a spatial unit and then identify the annual maximum fig 2 c d and e show the areal annual maximum precipitation identified at each huc04 huc06 and huc08 unit as expected with increasing area sizes huc08 to huc04 the average precipitation depths decrease suggesting the need for areal reduction through arf in testing this huc based ams identification approach a larger data gap for large area samples was observed i e there are fewer huc04s than huc06s and huc08s to address this issue and increase the ams samples to cover a wider range of watershed sizes the connectivity between huc08s is used to develop additional accumulated huc units hucac taking huc08 05090203 as an example using the connectivity between huc08s fig 1 b we may identify all upstream huc08s contributing to 05090203 and merge them as a new hucac unit for ams searching fig 2 f following this concept hucac is identified for each of the huc08s based on their upstream connectivity when an hucac is identical to an existing huc04 huc06 and huc08 it is not used to avoid double sampling a total of 46 hucac 4 600 420 000 km2 is identified in orb a histogram showing all huc unit area sizes is provided in fig 2 g note that seven huc06s were excluded in this region since they have the same spatial extent with their corresponding huc04s and would have otherwise been double counted to effectively summarize gridded precipitation prism daymet livneh and st4 a conversion table is established indicating what grid points should be included in a specific huc unit this conversion table is then used to spatially average all hourly and daily gridded precipitation into different huc based precipitation for ams identification for dsi 3240 all hourly gauge data in and surrounding orb are spatially interpolated at each prism grid location while the gauge data are ground based and should ideally include topographic adjustments e g elevation lapse rate these considerations are not accounted for during spatial interpolation this is acceptable in most of the orb given its relatively flat terrain but may be unacceptable for watersheds with significant topographic variation in such cases proper topography informed adjustment approaches such as those used in prism daymet and livneh should be followed overall as summarized in table 1 ams are searched for different data durations spatial units and seasons all season january through december warm season may through october cool season january through april and november through december in orb for the conus assessment we conduct ams searching using prism as the only data source in addition we neglect the huc08 connectivity across different hydrologic regions e g from region 14 upper colorado to region 15 lower colorado so that we may derive and compare arf specifically for each hydrologic region 2 4 sample arf estimation after ams has been comprehensively searched for all huc units and each grid point the next step is to calculate the sample arf at each huc these sample arfs would represent the best available watershed specific arf estimates at each huc the arf samples across all hucs can then be grouped and jointly fitted into a regional arf model for more generalized representation let p grid y g represent the annual maximum precipitation at year y and grid cell g and p huc y represent the annual maximum precipitation at year y for a particular huc considering all grid cells in the huc unit the first type of sample arf arf avg can be defined as 1 arf avg p huc a v g p grid a v g 2 p huc a v g y 1 n y p huc y n y 3 p grid a v g y 1 n y g 1 n g p grid y g n y n g where n y is the total number of years and n g is the total number of grid cells in the huc unit following the definition arf avg presents the ratio of average areal ams to the average grid based ams these formulations do not account for the potential influence of precipitation frequency on arf and are similar to the approach used in early arf studies e g tp 29 approximately speaking arf avg defined in this fashion would have a return period around 2 years another type of sample arf involves probability density function pdf fitting so that frequency return period can be introduced into the arf calculation assuming that the gev distribution can be a suitable pdf in this case the t year arf tyr can be defined as 4 arf tyr p huc t y r p grid t y r a v g 5 p huc t y r gev 1 1 1 t μ huc γ huc κ huc 6 p grid t y r a v g g 1 n g gev 1 1 1 t μ g γ g κ g n g where gev 1 represents the inverse of gev μ huc γ huc κ huc represent the gev parameters estimated at the selected huc unit using the huc based ams and μ g γ g κ g represent the gev parameters estimated at each grid cell using the grid based ams as an example the fitted gev parameters of daily prism precipitation for each huc unit in orb are shown in fig 3 while both μ huc and γ huc are scale dependent and show similar patterns across all huc units no clear pattern is found for κ huc clearly other pdfs may be considered in place of gev to provide frequency estimates but epistemic uncertainty due to pdf choice is not coinsidered in this study the maximum likelihood approach is used to estimate the gev parameters after parameter fitting the kolmogorov smirnov ks test is used to examine the goodness of fit at a 5 significance level if a specific case fails to pass the ks test the data point is disregarded for further arf model fitting overall the average arf from eq 1 and the 10 and 100 year estimates from eq 4 are used for regional arf model fitting when calculating individual arf samples it is possible to obtain an arf sample above 1 00 due the artifact of pdf fitting for both huc and grid based ams in these cases an upper arf limit of 1 00 was set for regional arf model development note that frequency can also be introduced into arf from other approaches e g overeem et al 2010 however regardless of which approach is used the limited data record would likely be the greatest hurdle to estimating long return period precipitation and arf even with around 60 years of long term records from livneh and dsi 3240 data sets there may not be sufficient data to support precipitation and arf estimation for return periods greater than 100 year without substantial uncertainty which is associated with extrapolating return period estimates well beyond the record length in addition nonstationarity in a changing environment would add further complication to the frequency analysis these more involved issues are noted but not examined in this study 2 5 arf model fitting the final step is to fit all arf samples in a hydrologic region with a generalized arf model among various potential arf models we select the dynamic scaling model by de michele et al 2001 in this study when comparing several different arf models not shown here we found that the de michele et al 2001 model is generally stable can fit well in a variety of hydrologic regions and has a good underlying theory on spatial and temporal rainfall scaling the good performance of the de michele et al 2001 model was also reported by pavlovic et al 2016 the de michele et al 2001 model is a function of area a km2 or mi2 and duration d hours with four parameters b v w and z 7 a r f a d 1 ω a z d b υ b to fit these four parameters the root mean square error rmse between arf samples and arf model is minimized using the nelder mead simplex algorithm lagarias et al 1998 implemted in the fminsearch function in matlab the performance of fitting is also evaluated by the nash sutcliffe efficiency nse mccuen et al 2006 coefficient given eq 7 is not a function of return period fitting is performed separately for each frequency level of interest i e average ams 10 and 100 year to help understand the uncertainty associated with sampling arfs across different huc units we further quantify the resampling uncertainty through 1 000 bootstrap replications efron and tibshirani 1994 during each round of bootstrap the arf samples calculated from eqs 1 and 4 are randomly re sampled to the same sample size and re fitted with eq 7 the 95 uncertainty bound from lower 2 5 to higher 97 5 quantiles is then identified to estimate the uncertainty because the focus in this study is to explore and reveal the variabilities of arf associated a variety of external non method related factors we only select one arf model for interregional comparison and do not focus on testing and selecting different arf models in each of the hydrologic regions for site specific studies it would be useful to test and compare different arf models to select the most appropriate model for the application 3 results and discussion 3 1 effect of area duration and return period fig 4 shows arfs derived from prism precipitation in orb as a function of area for different durations 1 day 2 day 3 day and frequency levels 2 year 10 year 100 year arf samples calculated at huc units are marked in colored dots while the fitted regional arf curves are marked in colored lines the nse values are also provided and can be used to evaluate the reasonableness of derived regional arf curves in general the results are consistent with common understanding that arf decreases with increasing area and increases with increasing duration the results clearly indicate that arf decreases with increasing return period which was not considered in the conventional tp 29 arf curves the results also show that the variability of arf samples increases with increasing return periods and hence leads to poorer fitting results particularly poor at the 100 year return level in this case the t year arf samples calculated by eqs 4 6 are based on 37 years of prism ams the larger variability and degraded nse values suggest that they may not be sufficient to support the 100 year return level calculation the issue largely relates to the limited observed data record length available from the precipitation products leading to high uncertainty in the model results the huc based ams search approach can effectively increase the number of arf samples across a wide range of area sizes and better enable arf estimation for large area sizes greater than 1 036 km2 provided by tp 29 however given the use of huc08 as the minimum unit our analysis did not include arf samples in the smaller sub huc08 area range e g from 0 to 1000 km2 in fig 2 g this simplification should not affect our results since the arf samples in these smaller areas are close to 1 are likely to have relatively lower variability and have much less influence on the overall arf curve fitting to enhance this step one may consider using an even smaller huc unit e g huc12 as the minimum unit for ams searching we did not adopt huc12 as the minimum searching unit since it is computationally intensive for a conus scale study to gain further insights into the variability of subdaily arf fig 5 provides a comparison of 10 year arf using dsi 3240 hourly precipitation across 9 different durations 1 hr 2 hr 3 hr 4 hr 6 hr 12 hr 18 hr 1 day 2 day 3 day dsi 3240 is selected for this comparison given its hourly resolution and relatively long record similar to the findings reported by nerc 1975 the subdaily arf decreases with decreasing durations note that each duration is fit separately instead of fitting a single set of arf parameters across all durations because the fitting performance was unacceptable when trying to fit a single parameter set across both longer 1 2 and 3 day and shorter less than 1 day durations the implication for derivation of a generalized arf model is that one may need to develop separate models for long medium and short durations the subdaily analysis can also be performed using the radar driven hourly st4 precipitation but the limited st4 period of record would lead to more noisy results especially at longer return periods 3 2 effect of data source fig 6 provides a comparison of 1 day arf and nse fitting statistics in orb across different precipitation data sources prism daymet st4 livneh and dsi 3240 and frequency levels the st4 reveals overall lower arfs than other data sources with worse overall performance the models using daymet livneh prism and dsi 3240 data produce more similar results the consistent trends among different precipitation data products suggest that the arf variability is not particularly sensitive to data type we note that the difference between st4 based arf and the other precipitation products increases with increasing of return period this is likely a result of shorter st4 data record length while st4 provides hourly data and can better represent spatial variability it only has 16 years of record as compared to 37 64 years of record for other precipitation products the overall results suggest that data length is a more important factor to consider especially for the calculation of longer return period arfs 3 3 effect of seasonality fig 7 provides a comparison of 1 day 10 year prism arf in orb across different seasons the fitting performance of arf across different seasons is largely similar with cool season having slightly smaller nse the results indicate that the warm season arf is close to all season arf while cool season arf has a much higher value the similarity between warm and all seasons suggests that the annual extreme precipitation in orb mainly occurs during warm season the differences in warm and cool seasons arf can be explained by their respective controlling extreme precipitation processes in orb the major extreme precipitation events during warm season are meso scale convective storms that are generally smaller in size and have larger spatial variability leading to smaller arf on the other hand the major extreme precipitation events in orb during cool season are mostly large scale frontal systems with relatively smaller spatial variability as compared to warm season convective storms that leads to larger arf for h h applications such as simulation of rain on snow during cool season the results suggest that a specific cool season arf may be needed considering the different regional climate characteristics the definition of seasons may need to be adjusted which deserve further exploration 3 4 effect of geographic location fig 8 shows fitted arf curves across the 18 conus hydrologic regions for 1 day duration and 10 year return period the maximum area plotted for each region represents the largest huc unit analyzed in the region the results demonstrate that while arf values are somewhat consistent for small area sizes e g arfs are within about 4 for areas below 100 km2 the variation increases for larger areas for example at 10 000 km2 the fitted arfs range from approximately 0 90 in the new england region region 01 to 0 76 in the souris red rainy region 09 and texas gulf region 12 regions this regional variation is illustrated in fig 9 which generally shows that lower arfs are found in the central us with higher arfs found in the eastern and western us several factors could contribute to this pattern but it is expected that regional climate coastal proximity topographical influences and frequency of certain storm types intensities could influence ams values across regions and influence arf estimates it is worth noting that the results in fig 9 are based on 37 years of prism data thus the fitting performance summarized in table 3 is relatively poor in longer return periods the nse values vary regionally with the lowest values found in regions 01 new england 02 mid atlantic 03 south atlantic gulf 04 great lakes and 16 great basin while regions 01 02 and 04 are all in the northeastern us a clear explanation for the performance results is not apparent and would require further investigation the relatively poorer performance in certain regions can be explained by the wider variability in the calculated huc unit arfs one possible explanation for the wider variability is that these regions contain multiple separate river systems e g non hydrologically connected huc04s and hence involves larger variabilities it is also possible that terrain effects could play a role and that a mixture of more extreme precipitation processes e g from coastal hurricanes to topographically enhanced precipitation regardless the results clearly demonstrate a large variability of arf across different geographical regions this finding suggests that the existing arf formulations from published literature are location specific and application of estimates in one region may not be generalizable to other regions this finding is consistent with mineo et al 2018 which compared the derived arf estimates for the lazio italy region with several published arf estimates and reported substantial over or under estimation therefore arf derived in distinct geographical and climate regions should not be directly transported during h h application 3 5 uncertainty quantification in addition to assessing how various factors may affect arf values we further quantify the sampling uncertainty of arfs across different huc units through 1 000 bootstrap replications efron and tibshirani 1994 taking prism precipitation in orb as an example i e fig 4 fig 10 illustrates the median value and 95 uncertainty bounds of 1 day 2 day 3 day and 2 year 10 year 100 year arfs the uncertainty bounds are very narrow for 2 year arfs and become wider for 10 year and 100 year arfs despite the spread of arf samples in fig 4 it is encouraging to see that the bootstrap uncertainty bounds are narrow in most of the cases this can be due to two possible reasons 1 a large number 194 of huc units is used during the fitting thereby reducing the uncertainty and 2 stability provided by the de michele et al 2001 model the uncertainty bounds behave consistently with the fitting statistics in which a better fitting can lead to a narrower uncertainty bound the uncertainty evaluated through bootstrap represents another view of arf variability considering all factors that affect arf e g methods data sources geographical locations seasonality in depth arf uncertainty analysis is needed to support further applications such integrated arf uncertainty analysis is beyond the scope of this pilot study but warrants further evaluation in the future research 3 6 comparison with tp 29 finally we compare 1 day orb arf with the conventional tp 29 values us weather bureau 1957 in fig 11 based on the multi data source arfs shown in fig 6 we consider the area range of tp 29 0 to 1 000 km2 for a more detailed view although our huc08 based analysis does not capture many data points in the 0 1 000 km2 range given that the de michele et al 2001 arf model has a mathematical upper bound at 1 when area equals 0 the fitting with a much broader set of arf samples across different areas can still yield reasonable results in the 0 1 000 km2 area range the 100 year st4 curve is not included in fig 11 given its poor fit shown in fig 6 c considering the effects of data source and geographical variability discussed in the prior sections it is not surprising to see differences between tp 29 and arf curves derived for orb in this case the 1 day tp 29 curve is closest to the 10 year st4 curve however it should be noted that while the decreasing trend of tp 29 is generally consistent in the range from 0 to 400 km2 the tp 29 1 day arf curve starts to flatten out in the range from 400 to 1 000 km2 considering that tp 29 was derived based on limited stations it is likely that tp 29 did not capture the arf trend in the larger area sizes combined with the spatial variability shown in fig 8 the results suggest that the tp 29 arf should not be considered applicable across different study areas without further diagnosis and evaluation 4 summary and conclusions utilizing a huc based ams searching technique this study investigates factors influencing arf including area duration return period data source seasonality and geographical location in general our results are consistent with previous studies that found arf decreases with increasing area and increases with increasing duration our results clearly suggest that arf decreases with increasing return period which was not addressed in the conventional tp 29 arf curves this finding supports the need to develop frequency return period specific arf for h h application however the results also suggest that the performance of high return level arf is strongly controlled by the length of historic observation e g the poor performance of 100 year st4 arf fitting given only 16 years of observation therefore when using a purely observation based approach such as the one used in this study it is questionable if one has sufficient ability and confidence to derive high return level areal extreme rainfall and arf estimates to support h h application this limitation should be clearly acknowledged when selecting and designing probabilistic flood hazard assessment unless the data sufficiency issue is addressed high return level arfs should be estimated using more conservative and reliable low return period arfs to overcome the data barrier it is worth exploring how and if proper process based numerical weather modeling could be used to support the characterization of probabilistic large scale spatiotemporal storm structures and the estimation of fixed geographical area high return level arfs regarding data sources while the effect of data sources is relatively smaller than duration and return periods non negligible differences are still found the gridded precipitation products e g daymet prism livneh are easy to use but given their limited temporal resolution they cannot be used to derive subdaily arf the radar driven precipitation product st4 can better capture high resolution spatiotemporal storm structure however given its limited period of record it provides the worst high return level arf model fitting across all precipitation products while gauge data dsi 3240 are harder to process the associated arf model fitting performance is among the best in orb with further consideration of topography informed spatial interpolation there is value to consider gauge data based approaches in future site specific arf studies regarding seasonality the results suggest that the warm season arf is close to all season arf while cool season arf is generally higher the similarity between warm and all seasons indicates that the annual maximum precipitation in orb mainly occurs during warm season the differences in warm and cool seasons arf can be explained by their respective controlling extreme precipitation processes in most regions the major extreme precipitation events during warm season are meso scale convective storms that area generally smaller in size and have larger spatial variability leading to smaller arf on the other hand the major extreme precipitation events in most regions during cool season are mostly large scale frontal systems with relatively smaller spatial variability as compared to warm season convective storms that leads to larger arf for h h applications such as simulation of rain on snow during cool season cool season arf will be needed regarding geographical variability it is found that arfs are lower in the central us and higher in eastern and western us texas gulf region 12 souris red rainy region 09 are generally the lowest among all regions the results clearly suggest a strong geographical variability associated with arf therefore arf values produced from previous studies e g tp 29 should not be indiscriminately used at different geographical locations this study demonstrates a need to derive region or watershed specific arf for robust probabilistic flood hazard evaluation while the results of this study suggest that the advance of precipitation products and more advanced arf methods can enable more defensible arf estimates for h h applications several major challenges are also identified for instance long return period areal extreme rainfall and arf derivation is found to be one major challenge long return period in this case refers to when the desired return period is much longer than the period of records of the supporting precipitation data e g deriving 1000 year extreme rainfall estimate based on 66 years of data in addition potential changes in climatic conditions may further complicate this data record challenge from this perspective there is a need to explore suitable methods for the derivation of long return period areal extreme rainfall and arf through the assistance of process based numerical weather simulation overall this study demonstrates the needs to improve arfs with new data and methods for more reliable areal extreme precipitation estimates to support h h applications credit authorship contribution statement shih chieh kao conceptualization methodology software formal analysis investigation resources writing original draft visualization scott t deneale conceptualization methodology formal analysis investigation writing original draft visualization project administration elena yegorova conceptualization methodology writing review editing supervision funding acquisition joseph kanney conceptualization methodology writing review editing meredith l carr conceptualization methodology writing review editing declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgment this work was supported by the us nuclear regulatory commission nrc office of nuclear regulatory research as part of the nrc probabilistic flood hazard assessment research program sck and std are employees of ut battelle llc under contract de ac05 00or22725 with the us department of energy accordingly the us government retains and the publisher by accepting the article for publication acknowledges that the us government retains a nonexclusive irrevocable worldwide license to publish or reproduce the published form of this manuscript or allow others to do so for us government purposes all data used in this study was obtained from public sources which are identified in the paper this report was prepared as an account of work sponsored by an agency of the us government neither the us government nor any agency thereof nor any of their employees makes any warranty express or implied or assumes any legal liability or responsibility for the accuracy completeness or usefulness of any information apparatus product or process disclosed or represents that its use would not infringe privately owned rights reference herein to any specific commercial product process or service by trade name trademark manufacturer or otherwise does not necessarily constitute or imply its endorsement recommendation or favoring by the us government or any agency thereof the views and opinions of authors expressed herein do not necessarily state or reflect those of the us government or any agency thereof 
8584,floods have caused severe destruction and affected communities in different ways throughout history flood events are being exacerbated by climate change and hence it is increasingly necessary to have a more accurate understanding of various aspects of flood hazard particularly for pedestrians the focus of this study is therefore to investigate different criteria to assess the flood hazard for pedestrians and to propose improvements in assessing such hazards the revised mechanics based approach reported herein gives results based on a full physical analysis of the forces acting on a body and can be universally applied as the method can be fine tuned for different region of the world the results from flood hazard assessments can be used to design evacuation plans improve resilience of sites prone to flooding and plan more resilient future developments extreme flood events in the uk and documented for boscastle 2004 and borth 2012 were used as case studies two approaches were considered including i a mechanics based approach and ii an experimental based approach with the criteria for the stability of pedestrians in floods being compared for the criteria used by regulatory authorities in australia spain uk and usa the results obtained in this study demonstrate that the mechanics based methods are preferable in determining flood hazard rating assessments keywords extreme flood events flood modelling flood hazard flood risk management shock capturing models human stability in floodwaters 1 introduction of all the natural hazards that occur world wide flood events are historically recognised as being one of the most devastating often leading to significant loss of life bellos et al 2020 bracken et al 2016 percival and teeuw 2019 svetlana et al 2015 in specific regions of the world climate change in combination with an increase in population and increasing urbanisation coastal and riverside cities has made the impact of flooding on people and economic assets even more dramatic guerriero et al 2020 milanesi et al 2016 wang et al 2018 although it is clearly impossible to reduce the flood risk of any river basin to zero creutin et al 2013 it is increasingly important to minimise so far as possible the impact of flood events this can be done by implementing various flood mitigation methodologies fox rogers et al 2016 and developing preparedness and response actions such as emergency evacuation plans bodoque et al 2019 2016 pedestrians walking in flooded areas is one of the two major causes of death associated with flood events arrighi et al 2019 shabanikiya et al 2014 thus one of the fundamental aspects of flood risk management is to assess the hazard posed by floods to pedestrians generally pedestrians tend to underestimate the impact that a flood flow can have on the human body especially for shallow water depths and high flow velocities this aspect jointly with the nature of extreme floods in specific areas e g flash floods in alpine environments steep catchments and urban environments makes floods very dangerous for pedestrians moreover most people and businesses consider flood hazard defence schemes to provide a complete safeguard against flooding when to the contrary the protection is often only partial stevens et al 2010 internationally various authorities have often adopted flood hazard assessment methods as suggested by earlier studies started in the 1970s in providing a significant step forward to ensure the safety of pedestrians during floods however it is important to note that there is currently a lack of standardisation between countries in terms of assessing flood hazard from a pedestrian protection perspective in many countries flood hazard assessment for pedestrians is not updated to recently available methods and in some cases is not considered at all recent advancements in understanding the stability of pedestrians in flood events together with more readily available data and more accurate modelling resources means that the flood hazard assessment from a pedestrian protection perspective can be improved and should be considered internationally in a more unified manner the available literature reports different approaches and methodologies to assess the hazard to pedestrians in flooding waters but there is general agreement on the two main possible failure mechanisms of people stability including toppling and sliding arrighi et al 2017 jonkman and penning rowsell 2008 furthermore two main approaches have been increasingly used to assess the stability of people in floodwaters the first is based on empirical or semi quantitative criteria and the second is based on formulae derived from a mechanics based approach and supported by experimental data authorities worldwide have assessed the flood hazard from the perspective of pedestrian protection using different methods available cox et al 2010 priest et al 2016 in 1988 the bureau of reclamation of the united states department of the interior published a report entitled downstream hazard classification guidelines u s department of the interior 1988 with the intent being to provide some guidelines to assess flood hazard due to possible dam break flows these guidelines provided different graphs in order to identify and assess the hazard for pedestrians and houses associated with a flood event due a dam breach event and allowed hazard quantification for the following categories passenger vehicles adult pedestrian routes and child pedestrians routes in 1996 the general directorate of the hydraulics works and water quality of the spanish environmental ministry published its technical guidelines for dam classification based on the potential risk of failure ministerio de medio ambiente de espana 1996 in these guidelines graphs were provided which correlate depth velocity relationships with the danger derived from floods due a dam break event and allowed hazard quantification ramsbottom et al 2003 and ramsbottom et al 2006 developed an empirically based method for evaluating flood hazard for the department for the environment flood and rural affairs defra and the uk environmental agency ea the authors tested various empirical formulae using datasets available in the literature and proposed an approach that considers the likelihood of flooding the probability of exposure to a flood event and the probability that people exposed to the considered event would be seriously or even fatally injured the publication of a report on australian rainfall and runoff by cox et al 2010 updated australia s guidelines on safety of pedestrians in floodwaters the authors reviewed their previous work and re analysed all of the available datasets enabling new guidelines to be produced for the safety of pedestrians in floodwaters and using depth velocity relationships the main focus of this research study is to compare the performance of a revised and improved mechanics based method mbm against various empirical methodologies adopted by authorities in some countries particularly from the perspective of pedestrian protection and to highlight the potential inconsistencies between the empirical methods this benchmarking enables the pros and cons of the various methods to be assessed and facilitates a more universal and scientific approach to flood hazard assessment of pedestrians moving in floodwaters with the scope of contributing to improving the flood hazard evaluation especially for the case of flash floods and based on a pedestrian protection perspective two cases studies from the uk have been considered in the benchmarking analysis these sites include i boscastle a tourist village in the south west of england affected on 16th august 2004 by an extreme flash flood which has been widely studied due to the availability of data and the impacts of the flash flood and ii the borth region in west wales where on the 9th june 2012 a heavy rainfall event caused a flash flood to occur with the site being an important tourist resort with tourism being crucial to the local economy for both sites pedestrians hazard levels were important due to the nature of the sites the novelty of this particular study is the inclusion in the comparison of a revised and improved mbm which considers the effects of ground slope and includes updated parameter values of the key characteristics of a typical european human body these flood events were also considered to be significantly different in terms of their intensity with the aim of the study being to investigate the dependency of the reliability of the results for different flood conditions and using a range of different assessment methods 2 case studies sites 2 1 boscastle boscastle is a small touristic village located at the end of a narrow and steep catchment in cornwall uk fig 1 on 16th august 2004 an intense rainfall event occurred over the north coast of cornwall with up to 200 mm of rain fell in about 5 h over a 20 km2 catchment area hr wallingford 2005 roca and davison 2010 this extreme rainfall event caused a flash flood fig 2 that severely affected the village and its population causing extensive damages which were widely reported during the event streets were inundated by over 2 m of water xia et al 2011a and people had to be rescued from cars and rooftops the extent of the flash flood can briefly summarized as follow 100 people were airlifted six buildings collapsed due to the strong force of the flood water and over 70 properties were flooded 79 cars were washed away into the harbour the two local bridges were blocked the lower bridge collapsed and had to be reconstructed after the event environment agency 2004 rowe 2004 damages were of the order of several million pounds without considering psychological consequences suffered by people affected by trauma due to consequences of the flood rowe 2004 characteristics of both basin and flash flood reported above made boscastle an ideal case study for many flash flood modelling studies the domain for this study covers a surface of 0 156 km2 235 m wide and 665 m long that has been divided in square cells of 1 m2 each lidar laser imaging detection and ranging data collected during a survey undertaken by the environmental agency post the flood event was used to represent the topography of the domain a constant value of manning s roughness coefficient equal to 0 040 been used along the whole domain kvočka et al 2015 xia et al 2011b peak discharge of the event was estimated to be about 180 m3 s as shown in fig 3 therefore based on flood estimation handbook feh statistical and rainfall runoff methods the frequency of the flood event was estimated in the order of 1 400 years hr wallingford 2005 roca and davison 2010 calibration and validation of the hydrodynamic model has been undertaken in some detail and has been reported previously falconer et al 2012 kvočka et al 2017 2015 xia et al 2011b 2 2 borth borth is a coastal village located in west wales uk fig 1 and part of the dyfi biosphere which is the only unesco biosphere reserve in wales the village it is also part of dyfi national nature reserve and it is situated along the welsh coast path there are many touristic attractions and facilities in borth and surrounding area such as caravan parks camping site golf club zoo seasonal festival and carnival make the area important for the local economy this area is suited at the final part of river leri catchment which is a relatively small steep catchment on 9th june 2012 heavy rain caused a flash flood down the cambrian mountains causing a severe flood in tal y bont dol y bont borth and surrounding area fig 2 about 60 properties and caravan parks in those areas had been evacuated this evens has been reported as the biggest flooding in living memory foulds et al 2012 large areas of the floodplain have been developed as camping and caravan sites thus these are classified as high exposure areas due to the large number of temporary residents exposed to flood risk the events in june 2012 highlights the need for accurate flood hazard assessment and appropriate flood defences to reduce the impact of such events or even more disastrous events similar to the one happens in spain during a flash flood in 2007 were 87 people died in a campsite foulds et al 2012 the domain of this study covers an area of 63 km2 9 km long and 7 km wide and include borth tal y bont and dol y bont areas topographic data used to set up the hydrodynamic model have been extracted from a 2 m lidar upstream boundary condition was flow entering the domain through two main rivers namely river leri and river cuelan the simulated flood event was a 1 100 year flood event with a discharge peak of 64 5 m3 s for river leri and 19 1 m3 s for river cuelan kvočka et al 2018 downstream boundary condition was set to the water levels in the dify estuary roughness parameters were assigned on the basis of kvočka et al 2018 to the floodplain it has been assigned the value of 0 05 the value of 0 04 was assigned for the river channel calibration and validation of the model has been undertaken in detail as reported by kvočka et al 2018 3 numerical modelling of flash flood events the flood events considered as case study in this work were simulated using the divast tvd model in order to obtain flood characteristics as flow depth and velocity to be used to assess flood hazard to pedestrian divast tvd was developed in the hydro environmental research centre hrc at cardiff university and has been used for a number of flood modelling studies ahmadian et al 2018 hunter et al 2008 kvočka et al 2017 liang et al 2007a 2007b neelz and pender 2013 divast tvd is based on the finite difference scheme the algorithm is fully conservative and is based on a standard maccormack scheme enhanced with a symmetric five points total variation diminishing tvd shock capturing algorithm mingham et al 2001 the tvd algorithm allows discontinuities to be captured as occurring for trans and super critical river flows without generating spurious oscillations kalita 2016 the shock capturing feature of divast tvd makes this model ideal for modelling short and steep catchments where trans and super critical flows can occur for high rainfall events further details of the divast tvd model are given in liang et al 2007a 4 assessing flood hazard to pedestrians in this section a brief overview of methods to assess flood hazard for pedestrians and the methods considered for benchmarking hazard analysis are presented especially a revised mbm is benchmarked against methods adopted by authorities in the usa method a australia method b the uk method c and spain method d as well as the empirical method method e proposed by martínez gomariz et al 2016 these methods are summarised below and their performance in assessing the flood hazard to pedestrians have been compared and discussed in section 5 the methods used for the benchmarking have been selected from those available in the literature the selection of the methods has been based on consideration of i the methodology adopted with benchmark comparisons being undertaken between the different assessment methods to establish if there is any scope for improvement in the method and ii the validity of the method in term of the methodology adopted by the authorities and the novelty of the analysis as reported in the literature methods a b c and d are used by government organisations with method e being regarded as a state of the art empirical approach 4 1 methodologies to assess flood hazard for pedestrian early studies of foster and cox 1973 showed that instability of the children in floodwaters depended on a combination of physical dynamic and emotional factors moreover the results also showed that the predominant failure mechanism was sliding abt et al 1989 conducted a series of tests with human subjects and a monolith placed in a flume their study demonstrating the importance of toppling mechanism karvonen et al 2000 carried out further tests with humans through the rescdam project with their findings showing that depending on the person s weight and height the critical depth and velocity product ranged from 0 64 m2 s to 1 29 m2 s to overcome the limitations of experimental activities involving people various authors have proposed several conceptual modelling techniques and with differing degrees of simplification in order to describe the complex phenomenon of pedestrian stability in floodwaters some of these studies and findings are summarised below jonkman and penning rowsell 2008 analysed their experimental results and found that the sliding mechanism was more dangerous than previous studies had suggested another interesting finding of their work was that sliding mechanism was the dominant mechanism of failure in shallow water depths and high flow velocities as typically occurs for the case of flash floods in urban environments moreover the authors also found that the simplified approach used to evaluate instability which was generally presented as the product of depth h and velocity v had a physical connection with the toppling mechanism but that a better descriptor of the sliding mechanism was the product h v2 xia et al 2014a proposed a mbm the methodology considered the failure mechanisms of both toppling and sliding and included the effects of ground slope and a non uniform upstream velocity profile acting on the human body moreover the analysis included the forces acting on a body when immersed in water such as buoyancy friction drag normal reaction and gravitational forces the resulting formulation parameters were calibrated using flume experiments and datasets available in the literature later this methodology was further extended to include experiments for a range of bed slope conditions xia et al 2014b another important feature of this mbm approach was the inclusion of the body shape characteristics in the analysis through the addition of coefficients describing the typical features of a human body milanesi et al 2016 tried to overcome the bias inherent in tests conducted in controlled laboratory conditions e g trained subjects gained experience during testing and presence of safety equipment as well as dealing with the bias of tests conducted on scaled models e g the dummy cannot continuously adjust its posture the dummy is not affected by psychological factors etc the authors proposed a new methodology that extrapolated the flow characteristics from videos available on the web with the results being verified through observations the authors suggested studying the stability problem in a statistical framework rather than in a deterministic manner and accordingly proposed a methodology that identified the stability surface and relative thresholds in probabilistic terms arrighi et al 2017 highlighted the importance of including the body characteristics in analysing the interaction between a human body and the hydrodynamics in a flood event in other words the interaction depends on the portions and shape of the body that are in contact with the floodwater as well as the flow characteristics recently chen et al 2018 expanded the work of xia et al 2014a to include the effects of adjustments to a human body in a flood and they revised some of the key parameters in order to consider american and european body characteristics however the corresponding formulae were only obtained for toppling and did not include the effects of bed slope 4 2 mechanics based method it is first necessary to evaluate the incipient velocity for pedestrians in order to determine their instability in floodwaters the incipient velocity is defined in a similar manner to the incipient velocity of sediment particles in sediment transport formulation and is the velocity at which a person loses stability in floodwaters through the mechanisms of sliding or toppling before starting to move with the flow the sliding failure mechanism is given as xia et al 2014a 1 u c α h f h p β m p ρ f h p h f a 1 h f h p b 1 a 2 m p b 2 h p 2 for a sloping terrain the toppling failure mechanism is given as xia et al 2014a 2 u c α h f h p β m p h f 2 ρ f cos θ γ sin θ a 1 h p 2 b 1 h f h p a 2 m p b 2 where uc incipient velocity hf water depth m hp height of pedestrian m mp weight of pedestrian kg ρf density of water kg m3 α and β empirical coefficients and a1 a2 b1 b2 coefficients defining the characteristic features of a human body e g mass height and volume of the full body and body segments such as legs torso arms etc as shown in table 1 θ angle of the sloping ground and γ correction constant it is possible to determine a flood hazard rating fhr parameter by considering both failure mechanisms as follows 3 fhr m i n 1 u u c where u flow velocity and uc incipient velocity which is the minimum velocity of either utoppling or usliding further details of this approach can be found in xia et al 2014a therefore it is possible to calculate the precise threshold conditions for different age and gender groups as well as taking account of differences in body characteristics depending on the country etc milanesi et al 2015 xia et al 2014b as noted by gonzález riancho et al 2013 slope is an important factor which can significantly affect the flood hazard assessment for pedestrians one of the refinements of this research study has been the inclusion of the term related to the slope in eq 2 in previous studies this term has generally been omitted for simplicity the term relative to the slope in eq 2 is represented through the additional term given by cos θ γ sin θ in which θ represents the slope angle of the ground in this study the incipient velocity equations have been included in the finite difference model outlined previously where the bed elevation was stored at the centre of each grid cell ahmadian et al 2018 kvočka et al 2017 liang et al 2007a 2007b the slope of each computational cell was calculated by first evaluating the ground slope between the centre of the cell and the centre of the four neighbouring cells as shown in fig 3 the highest slope calculated was then selected as the slope to be used for the value of θ in eq 2 in this way the most adverse situation is considered in a precautionary approach in the proposed formulae for the stability of a pedestrian since both case studies are located in the uk the values used for the characteristics of a pedestrian are based on the typical dimensions of an average british person except for α and β since these are not available at the moment and as given in table 1 the parameters α and β depend on several factors such as the shape of the human body pedestrian s ability to adjust his her position in order to maintain stability in floodwaters and the drag and friction coefficients between the pedestrian and the ground surface typical values for α and β are different when considering toppling or sliding and allow the calibration of the method using both tests using real pedestrians and dummies 4 3 method a method a uses graphs to determine the flood hazard for pedestrians the u s department of the interior 1988 provided one graph for adults i e a person over 1 5 m in height and 54 kg in weight fig 4 a and one graph for children fig 4b details of the classifications of the hazard ratio can be founded in u s department of the interior 1988 4 4 method b cox et al 2010 in revising the stability thresholds for the australian guidelines used experimental data to establish different levels of hazard based on the product of depth and velocity as shown in fig 5 the authors proposed four different thresholds based on different depth and velocity products there was also a limiting depth and velocity considered for both children and adults the limiting velocity was 3 0 m s for both children and adults while limiting depths were 0 5 m and 1 2 m for children and adults respectively these values represent the depth and velocity thresholds for extreme danger details of the depth velocity relationships and the relative fhr for the different categories can be found in the classifications of the hazard ratio first given in cox et al 2010 4 5 method c a mathematical expression using an empirically based method is widely used in the uk ramsbottom et al 2006 ramsbottom et al 2003 and is given as 4 hr d v 0 5 d f where hr is the flood hazard rating d water depth m v velocity of flow m s and df is a factor which depends on the threat posed by debris which assumes a value of 0 0 5 or 1 ramsbottom et al 2006 details of the classifications of the hazard ratio can be found in ramsbottom et al 2006 4 6 method d similar to the guidelines provided by the us technical guidelines for dam classification based on the potential risk of failure ministerio de medio ambiente de espana 1996 two graphs were developed fig 6 to assess the flood hazard for the case of a dam failure in spain the graphs in fig 6 show the depth velocity relationships and the associated flood hazard levels for a urban areas and b rural or non urban areas the graphs are based on the product of the depth and velocity from these results it can be seen that there is good correlation between the graphs of the pedestrian route for adults using method a fig 4 a and the graph for the unurbanized area of method d fig 6b from this comparison the graph for the unurbanized area will be used hereafter rather than the pedestrian graph of method a since the graph for method d is more conservative a description and classification of the hazard level based on this method is reported in ministerio de medio ambiente de espana 1996 4 7 method e martínez gomariz et al 2016 derived an empirical equation for pedestrians based on results obtained from experiments with human subjects of different ages and gender the authors have merged these new data with previous data published by russo in 2009 in order to obtain more instability conditions this new merged dataset has been used to define the lower limit function expressed by eq 5 martínez gomariz et al 2016 thus depending on the value of the product of the water depth i e d in eq 5 and the flow velocity i e v in eq 5 it is possible to determine the stability of a pedestrian as shown in fig 7 5 d v 0 22 m 2 s 1 a classification of the hazard level can be found in martínez gomariz et al 2016 5 results the results of the benchmark studies for the revised mbm and the other methods shown individually in sections 5 1 5 5 the scope of this benchmarking study is therefore to highlight the improvements obtained using the revised mbm as compared to previous studies all the figures relative to simulation time 340 min for boscastle and 720 min for borth are reported as supplement material 5 1 benchmark between revised mbm and method a fig 8 shows the benchmark results between the revised mbm and method a in terms of predictions of the fhr for boscastle and borth respectively from the comparisons it can be seen that when using the revised mbm rather than method a there is a greater extension of the areas with an extreme fhr in considering the boscastle site method a assessed 29 54 less area characterised by extreme fhr at simulation time st 200 min fig 8b and 3 51 less at st 340 min similarly for the borth site method a assessed 28 96 and 48 71 less extreme fhr areas at st 420 min fig 8d and st 720 min respectively 5 2 benchmark between revised mbm and method b in fig 9 the results are benchmarked between the revised mbm and method b for the sites at boscastle and borth respectively it can be seen from the results that when using method b there are less regions characterised with an extreme fhr in particular there is 55 60 and 15 27 less area for st 200 min fig 9b and 340 min respectively for borth there are the 28 95 at st 420 min fig 9d and 48 71 at st 720 min less areas of extreme fhr when using method b instead of the revised mbm 5 3 benchmark between revised mbm and method c fig 10 shows benchmarked results between the revised mbm and method c for boscastle and borth sites respectively it can be seen that when using method c there is a reduction in the extreme fhr compared to the results obtained using the revised mbm for boscastle the difference is 76 93 at st 200 min fig 10b and 27 04 at st 340 min for the borth case study the difference is 83 60 at st 420 min fig 10d and 81 65 at st 720 min 5 4 benchmark between revised mbm and method d the benchmark results between those for the revised mbm and method d are shown in fig 11 for the boscastle and borth sites it can be seen that when using the revised mbm there is a greater extension of the areas categorised as extreme fhr for the boscastle case study when using method d there are 29 69 and 3 69 less extreme fhr areas at st 200 min fig 11b and at st 340 min respectively for borth then method d shows 36 06 and 47 94 less extreme fhr areas at st 420 min fig 11d and st 720 min respectively 5 5 benchmark between revised mbm and method e fig 12 shows the benchmark comparisons between the revised mbm and method e the results show a greater extension of extreme fhr when using method e in particular for the boscastle site the increase in area is 10 02 at st 200 min fig 12b and 6 19 at st 340 min for the borth case study method e assesses increases of 15 30 at st 420 min fig 12d and 11 06 at st 720 min 6 discussion the results presented in this study have shown that the empirical methods except for the method e generally underestimate the fhr results for extreme flood events musolino et al 2020 russo et al 2014 when compared with the revised mbm approach tables 2 and 3 in comparing the results in tables 2 and 3 at st 200 min and st 420 min respectively it can be seen that the difference is very similar in comparison with the results reported in tables 2 and 3 at st 320 min and st 720 min respectively when st is close to the peak of the flood event then the differences are noticeably different this is explained by the fact that the two flood events are different in terms of intensity with the boscastle flood event being a 1 400 years flood event and borth being a 1 100 years flood event see sections 2 1 and 2 2 hence if the value of the water depths and flow velocities are relatively large then all of the assessment methods tend to give similar assessments of the stability thresholds which have been already largely exceeded in comparing the results of the revised mbm and method e it is noted that the differences in the fhr areas are close for both case studies in considering the two sts the difference between the revised mbm and method e is 3 83 for boscastle and 4 26 for borth with no such big differences when considering the benchmark with the other methods at different sts especially for the boscastle case study this observation means that the two methods give reliable results no matter how extreme the flood event is so far there are no data available on the instability of a pedestrian in a real flood i e instability data obtained during a real flood event data available are only from experiments which take account of some of the most important factors leading to instability this makes validation of different methods and to a higher degree comparisons of the performance of the methods and uncertainty associated with the predictions more difficult to assess moreover different studies have highlighted that it is necessary to include the full physical characteristics of the flood in order to accurately assess the flood hazard from the perspective of pedestrian protection in events characterised by deep flood waters high flow velocities and sudden variations in the flow regime such those occurring in flash floods arrighi et al 2017 kvočka et al 2016 milanesi et al 2015 musolino et al 2020 this leads to further consideration of the mechanics based methods such as the revised mbm presented herein furthermore this study has highlighted some of the inconsistencies between the different empirical methods and the revised mbm for various physical characteristics which confirms the caution needed in an empirical method alone thus it is important to use an appropriate assessment method since if an emergency evacuation plan needs to be activated for local residents then it is important to undertake the planning as soon as possible in order to implement the safest evacuation pathways in contrast if the fhr predictions are not reasonably accurate then any evacuation plans can be erroneous and could have serious consequences the difference in the predictions is thought to be due to the following reasons the revised mbm approach is defined as being a product of the submerged depth and the square of the free stream velocity while the empirical methods are based on the product of the depth and velocity this later approach is inconsistent with an analysis of the hydrodynamic forces on a stationary body generally the difference in the results are covered by experimental coefficients at low velocities however for these case studies and similar extreme flood events the difference in the hazard assessment is expected to be considerably higher when the velocity is well in excess of unity as is generally the case for most extreme flood events thus when assessing extreme flood events which are often also characterised by deeper floodwaters higher flow velocities and sudden variations in the flow regime necessitate the inclusion of a full physical analysis as for the revised mbm approach as aforementioned furthermore the revised mbm approach considers all of the forces acting on a pedestrian moving in floodwaters including the effects of the ground slope both velocity and slope are relevant factors to be considered when assessing flood events especially in steep catchments methods a and d assess the fhr to produce very similar graphs and hence the results are similar method a allows a characterization between the thresholds for adults and children whereas method d does not include this distinction moreover both methods have been developed for dam failures and therefore consider very specific flood characteristics i e a rapid change in depth as well as velocity moreover the graphs leave some areas to the judgment of the individual which could be misleading furthermore considering that the key flood characteristics considered i e velocity and water depth then a distinction between the urban and non urban areas may not be adequate russo et al 2014 this suggests that the methods a and d need to be updated as suggested by martínez gomariz et al 2016 and russo et al 2014 when using method b further explanation of the lower fhr threshold is explained by the fact that cox et al 2010 in updating the previous thresholds used a database which included extensive scatter in the data the data were obtained from experimental campaigns which were conducted with inconsistent procedures thereby increasing the potential for errors such as gaining experience of the tested subjects use of safety equipment not including slope effects etc arrighi et al 2017 russo et al 2013 the differences in of the predicted fhr values using method c as highlighted in fig 10 and the revised mbm can be explained by the fact that the revised mbm approach considers the square of the velocity in its formulation as mentioned previously the difference in the results are also explained by the following limitations highlighted by cox et al 2010 i the available datasets have been averaged regardless of the influence of the training that the subjects gained repeating the same task during the tests due to the averaged data the final formula includes the effects of training in formulating the results since the majority of pedestrians would not have any experience in moving in floodwaters then the assumption of any form of training cannot be considered as valid ii there is no particular experiment supporting the proposed values for the debris factor iii the authors did not include any upper depth limit which means that a large depth and a low velocity would not necessarily be considered as dangerous but this may be the case since once a pedestrian starts to float then the person becomes unstable moreover milanesi et al 2015 pointed out that by considering the nature of the empirical approximation function as purely regressive it is not possible to truly connect hazard level and physical effects so there is no relationship between hazard levels with physical aspects of pedestrians e g no different thresholds for age body size and shape the authors of method c also pointed out in their work is that the expression they proposed is based on experience of flood hazard estimation it is recognised that the expression appears rather arbitrary and refinement of this relationship is proposed in phase 2 based on a more detailed assessment of previous work together with possible new research ramsbottom et al 2003 in phase 2 ramsbottom et al 2006 refined the expression but only for the part relative to the debris factor since at the time studies relative to the use of the square of the velocity were not available for method e despite the good results obtained when using this method some limitations are present firstly the experimental method does not offer the possibility to characterise different body characteristics this means that the method needs to be tailored for different areas in the world where body characteristics can be very different by repeating the experiments similarly it is not possible to obtain thresholds for different categories i e adults and children inside a specific geographic group secondly the authors focused their attention on flow cases with a high velocity and shallow depth so neglecting the toppling failure mechanism which occurs more frequently in deeper flows the limitations and results reported herein for all of the methods benchmarked against the revised mbm suggest that the existing frameworks widely used can be improved using a more physics based methodology as presented in this study the historical case studies reported in this study were related to two specific return period flood events namely 1in 400 years for boscastle and 1 in 100 years for borth however in assessing the flood hazard of a specific area from a pedestrian protection perspective different return periods should be considered creating multiple aggregated scenarios considering different return periods offers more insight dankers and feyen 2009 menne and murray 2013 yin et al 2013 in pedestrian protection perspective and can better support the design optimisation of evacuation plans based on multiple aggregated scenarios in order to undertake this improvement floods with different return periods should be simulated and multiple flood hazard scenarios considered for pedestrian protection based on a different set of characteristics for each return period finally the evacuation plans could then be aggregated and produced as a function of the return period this is beyond the scope of this paper and is recommended to be considered in future studies 7 conclusions in this study the main methods used internationally and reported in the literature have been benchmarked against the mechanics based approach with the aim of investigating the scope for improving the fhr from a pedestrian protection perspective when considering extreme flood events such as flash floods the comparisons reported herein have highlighted that the empirical methods have limitations in acquire reliable thresholds of human stability in flood waters although the method used by martínez gomariz et al 2016 have shown very similar predictions to the revised mbm method the method lacks the capability to include human body characteristics in calculating the threshold velocity and or depth this means that the method needs to be calibrated by extensive experiments in different regions and it cannot be used for different groups of people with different body types and capabilities e g adults children and less mobile senior citizens moreover the approaches widely used by authorities were considered not to be sufficiently accurate in terms of assessing human stability thresholds in floodwaters and a revision to these methods should be considered in using most recent methodologies as for the revised mbm approach this study proposed a revised mbm which has included the most recent available body shape parameter values and the effects of the ground slope in the formulation these additional parameters have allowed improved accuracy in the determination of the physics based threshold levels which should lead to enhanced safety of pedestrians moving through evacuation routes during extreme flood events the revised mbm approach proposed herein has a key limitation in terms of the availability of data relating to the body shape parameters if these data are not available then a detailed characterization for the study area cannot be undertaken with a relatively high degree of accuracy particularly since generic body shape data then has to be used moreover the impact on the flood hazard assessment due to psychological and behavioural response has been not considered in the formulation with these aspects being recommendations for future works further research is also required on developing new flood hazard maps based on the most critical pedestrian category for the study area and considering different flood return periods as proposed in section 5 credit authorship contribution statement g musolino conceptualization methodology software writing original draft writing review editing visualization data curation r ahmadian supervision conceptualization methodology project administration funding acquisition resources writing review editing r a falconer supervision writing review editing declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments the research reported herein is funded by the engineering and physical sciences research council through centre for doctoral training in water informatics science and engineering wise cdt with grant number ep l016214 1 and the royal academy of engineering through urban flooding research policy impact programme with grant number uufrip 100031 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j hydroa 2020 100067 appendix a supplementary data the following are the supplementary data to this article supplementary data 1 supplementary data 2 supplementary data 3 supplementary data 4 supplementary data 5 supplementary data 6 supplementary data 7 supplementary data 8 supplementary data 9 supplementary data 10 supplementary data 11 
8584,floods have caused severe destruction and affected communities in different ways throughout history flood events are being exacerbated by climate change and hence it is increasingly necessary to have a more accurate understanding of various aspects of flood hazard particularly for pedestrians the focus of this study is therefore to investigate different criteria to assess the flood hazard for pedestrians and to propose improvements in assessing such hazards the revised mechanics based approach reported herein gives results based on a full physical analysis of the forces acting on a body and can be universally applied as the method can be fine tuned for different region of the world the results from flood hazard assessments can be used to design evacuation plans improve resilience of sites prone to flooding and plan more resilient future developments extreme flood events in the uk and documented for boscastle 2004 and borth 2012 were used as case studies two approaches were considered including i a mechanics based approach and ii an experimental based approach with the criteria for the stability of pedestrians in floods being compared for the criteria used by regulatory authorities in australia spain uk and usa the results obtained in this study demonstrate that the mechanics based methods are preferable in determining flood hazard rating assessments keywords extreme flood events flood modelling flood hazard flood risk management shock capturing models human stability in floodwaters 1 introduction of all the natural hazards that occur world wide flood events are historically recognised as being one of the most devastating often leading to significant loss of life bellos et al 2020 bracken et al 2016 percival and teeuw 2019 svetlana et al 2015 in specific regions of the world climate change in combination with an increase in population and increasing urbanisation coastal and riverside cities has made the impact of flooding on people and economic assets even more dramatic guerriero et al 2020 milanesi et al 2016 wang et al 2018 although it is clearly impossible to reduce the flood risk of any river basin to zero creutin et al 2013 it is increasingly important to minimise so far as possible the impact of flood events this can be done by implementing various flood mitigation methodologies fox rogers et al 2016 and developing preparedness and response actions such as emergency evacuation plans bodoque et al 2019 2016 pedestrians walking in flooded areas is one of the two major causes of death associated with flood events arrighi et al 2019 shabanikiya et al 2014 thus one of the fundamental aspects of flood risk management is to assess the hazard posed by floods to pedestrians generally pedestrians tend to underestimate the impact that a flood flow can have on the human body especially for shallow water depths and high flow velocities this aspect jointly with the nature of extreme floods in specific areas e g flash floods in alpine environments steep catchments and urban environments makes floods very dangerous for pedestrians moreover most people and businesses consider flood hazard defence schemes to provide a complete safeguard against flooding when to the contrary the protection is often only partial stevens et al 2010 internationally various authorities have often adopted flood hazard assessment methods as suggested by earlier studies started in the 1970s in providing a significant step forward to ensure the safety of pedestrians during floods however it is important to note that there is currently a lack of standardisation between countries in terms of assessing flood hazard from a pedestrian protection perspective in many countries flood hazard assessment for pedestrians is not updated to recently available methods and in some cases is not considered at all recent advancements in understanding the stability of pedestrians in flood events together with more readily available data and more accurate modelling resources means that the flood hazard assessment from a pedestrian protection perspective can be improved and should be considered internationally in a more unified manner the available literature reports different approaches and methodologies to assess the hazard to pedestrians in flooding waters but there is general agreement on the two main possible failure mechanisms of people stability including toppling and sliding arrighi et al 2017 jonkman and penning rowsell 2008 furthermore two main approaches have been increasingly used to assess the stability of people in floodwaters the first is based on empirical or semi quantitative criteria and the second is based on formulae derived from a mechanics based approach and supported by experimental data authorities worldwide have assessed the flood hazard from the perspective of pedestrian protection using different methods available cox et al 2010 priest et al 2016 in 1988 the bureau of reclamation of the united states department of the interior published a report entitled downstream hazard classification guidelines u s department of the interior 1988 with the intent being to provide some guidelines to assess flood hazard due to possible dam break flows these guidelines provided different graphs in order to identify and assess the hazard for pedestrians and houses associated with a flood event due a dam breach event and allowed hazard quantification for the following categories passenger vehicles adult pedestrian routes and child pedestrians routes in 1996 the general directorate of the hydraulics works and water quality of the spanish environmental ministry published its technical guidelines for dam classification based on the potential risk of failure ministerio de medio ambiente de espana 1996 in these guidelines graphs were provided which correlate depth velocity relationships with the danger derived from floods due a dam break event and allowed hazard quantification ramsbottom et al 2003 and ramsbottom et al 2006 developed an empirically based method for evaluating flood hazard for the department for the environment flood and rural affairs defra and the uk environmental agency ea the authors tested various empirical formulae using datasets available in the literature and proposed an approach that considers the likelihood of flooding the probability of exposure to a flood event and the probability that people exposed to the considered event would be seriously or even fatally injured the publication of a report on australian rainfall and runoff by cox et al 2010 updated australia s guidelines on safety of pedestrians in floodwaters the authors reviewed their previous work and re analysed all of the available datasets enabling new guidelines to be produced for the safety of pedestrians in floodwaters and using depth velocity relationships the main focus of this research study is to compare the performance of a revised and improved mechanics based method mbm against various empirical methodologies adopted by authorities in some countries particularly from the perspective of pedestrian protection and to highlight the potential inconsistencies between the empirical methods this benchmarking enables the pros and cons of the various methods to be assessed and facilitates a more universal and scientific approach to flood hazard assessment of pedestrians moving in floodwaters with the scope of contributing to improving the flood hazard evaluation especially for the case of flash floods and based on a pedestrian protection perspective two cases studies from the uk have been considered in the benchmarking analysis these sites include i boscastle a tourist village in the south west of england affected on 16th august 2004 by an extreme flash flood which has been widely studied due to the availability of data and the impacts of the flash flood and ii the borth region in west wales where on the 9th june 2012 a heavy rainfall event caused a flash flood to occur with the site being an important tourist resort with tourism being crucial to the local economy for both sites pedestrians hazard levels were important due to the nature of the sites the novelty of this particular study is the inclusion in the comparison of a revised and improved mbm which considers the effects of ground slope and includes updated parameter values of the key characteristics of a typical european human body these flood events were also considered to be significantly different in terms of their intensity with the aim of the study being to investigate the dependency of the reliability of the results for different flood conditions and using a range of different assessment methods 2 case studies sites 2 1 boscastle boscastle is a small touristic village located at the end of a narrow and steep catchment in cornwall uk fig 1 on 16th august 2004 an intense rainfall event occurred over the north coast of cornwall with up to 200 mm of rain fell in about 5 h over a 20 km2 catchment area hr wallingford 2005 roca and davison 2010 this extreme rainfall event caused a flash flood fig 2 that severely affected the village and its population causing extensive damages which were widely reported during the event streets were inundated by over 2 m of water xia et al 2011a and people had to be rescued from cars and rooftops the extent of the flash flood can briefly summarized as follow 100 people were airlifted six buildings collapsed due to the strong force of the flood water and over 70 properties were flooded 79 cars were washed away into the harbour the two local bridges were blocked the lower bridge collapsed and had to be reconstructed after the event environment agency 2004 rowe 2004 damages were of the order of several million pounds without considering psychological consequences suffered by people affected by trauma due to consequences of the flood rowe 2004 characteristics of both basin and flash flood reported above made boscastle an ideal case study for many flash flood modelling studies the domain for this study covers a surface of 0 156 km2 235 m wide and 665 m long that has been divided in square cells of 1 m2 each lidar laser imaging detection and ranging data collected during a survey undertaken by the environmental agency post the flood event was used to represent the topography of the domain a constant value of manning s roughness coefficient equal to 0 040 been used along the whole domain kvočka et al 2015 xia et al 2011b peak discharge of the event was estimated to be about 180 m3 s as shown in fig 3 therefore based on flood estimation handbook feh statistical and rainfall runoff methods the frequency of the flood event was estimated in the order of 1 400 years hr wallingford 2005 roca and davison 2010 calibration and validation of the hydrodynamic model has been undertaken in some detail and has been reported previously falconer et al 2012 kvočka et al 2017 2015 xia et al 2011b 2 2 borth borth is a coastal village located in west wales uk fig 1 and part of the dyfi biosphere which is the only unesco biosphere reserve in wales the village it is also part of dyfi national nature reserve and it is situated along the welsh coast path there are many touristic attractions and facilities in borth and surrounding area such as caravan parks camping site golf club zoo seasonal festival and carnival make the area important for the local economy this area is suited at the final part of river leri catchment which is a relatively small steep catchment on 9th june 2012 heavy rain caused a flash flood down the cambrian mountains causing a severe flood in tal y bont dol y bont borth and surrounding area fig 2 about 60 properties and caravan parks in those areas had been evacuated this evens has been reported as the biggest flooding in living memory foulds et al 2012 large areas of the floodplain have been developed as camping and caravan sites thus these are classified as high exposure areas due to the large number of temporary residents exposed to flood risk the events in june 2012 highlights the need for accurate flood hazard assessment and appropriate flood defences to reduce the impact of such events or even more disastrous events similar to the one happens in spain during a flash flood in 2007 were 87 people died in a campsite foulds et al 2012 the domain of this study covers an area of 63 km2 9 km long and 7 km wide and include borth tal y bont and dol y bont areas topographic data used to set up the hydrodynamic model have been extracted from a 2 m lidar upstream boundary condition was flow entering the domain through two main rivers namely river leri and river cuelan the simulated flood event was a 1 100 year flood event with a discharge peak of 64 5 m3 s for river leri and 19 1 m3 s for river cuelan kvočka et al 2018 downstream boundary condition was set to the water levels in the dify estuary roughness parameters were assigned on the basis of kvočka et al 2018 to the floodplain it has been assigned the value of 0 05 the value of 0 04 was assigned for the river channel calibration and validation of the model has been undertaken in detail as reported by kvočka et al 2018 3 numerical modelling of flash flood events the flood events considered as case study in this work were simulated using the divast tvd model in order to obtain flood characteristics as flow depth and velocity to be used to assess flood hazard to pedestrian divast tvd was developed in the hydro environmental research centre hrc at cardiff university and has been used for a number of flood modelling studies ahmadian et al 2018 hunter et al 2008 kvočka et al 2017 liang et al 2007a 2007b neelz and pender 2013 divast tvd is based on the finite difference scheme the algorithm is fully conservative and is based on a standard maccormack scheme enhanced with a symmetric five points total variation diminishing tvd shock capturing algorithm mingham et al 2001 the tvd algorithm allows discontinuities to be captured as occurring for trans and super critical river flows without generating spurious oscillations kalita 2016 the shock capturing feature of divast tvd makes this model ideal for modelling short and steep catchments where trans and super critical flows can occur for high rainfall events further details of the divast tvd model are given in liang et al 2007a 4 assessing flood hazard to pedestrians in this section a brief overview of methods to assess flood hazard for pedestrians and the methods considered for benchmarking hazard analysis are presented especially a revised mbm is benchmarked against methods adopted by authorities in the usa method a australia method b the uk method c and spain method d as well as the empirical method method e proposed by martínez gomariz et al 2016 these methods are summarised below and their performance in assessing the flood hazard to pedestrians have been compared and discussed in section 5 the methods used for the benchmarking have been selected from those available in the literature the selection of the methods has been based on consideration of i the methodology adopted with benchmark comparisons being undertaken between the different assessment methods to establish if there is any scope for improvement in the method and ii the validity of the method in term of the methodology adopted by the authorities and the novelty of the analysis as reported in the literature methods a b c and d are used by government organisations with method e being regarded as a state of the art empirical approach 4 1 methodologies to assess flood hazard for pedestrian early studies of foster and cox 1973 showed that instability of the children in floodwaters depended on a combination of physical dynamic and emotional factors moreover the results also showed that the predominant failure mechanism was sliding abt et al 1989 conducted a series of tests with human subjects and a monolith placed in a flume their study demonstrating the importance of toppling mechanism karvonen et al 2000 carried out further tests with humans through the rescdam project with their findings showing that depending on the person s weight and height the critical depth and velocity product ranged from 0 64 m2 s to 1 29 m2 s to overcome the limitations of experimental activities involving people various authors have proposed several conceptual modelling techniques and with differing degrees of simplification in order to describe the complex phenomenon of pedestrian stability in floodwaters some of these studies and findings are summarised below jonkman and penning rowsell 2008 analysed their experimental results and found that the sliding mechanism was more dangerous than previous studies had suggested another interesting finding of their work was that sliding mechanism was the dominant mechanism of failure in shallow water depths and high flow velocities as typically occurs for the case of flash floods in urban environments moreover the authors also found that the simplified approach used to evaluate instability which was generally presented as the product of depth h and velocity v had a physical connection with the toppling mechanism but that a better descriptor of the sliding mechanism was the product h v2 xia et al 2014a proposed a mbm the methodology considered the failure mechanisms of both toppling and sliding and included the effects of ground slope and a non uniform upstream velocity profile acting on the human body moreover the analysis included the forces acting on a body when immersed in water such as buoyancy friction drag normal reaction and gravitational forces the resulting formulation parameters were calibrated using flume experiments and datasets available in the literature later this methodology was further extended to include experiments for a range of bed slope conditions xia et al 2014b another important feature of this mbm approach was the inclusion of the body shape characteristics in the analysis through the addition of coefficients describing the typical features of a human body milanesi et al 2016 tried to overcome the bias inherent in tests conducted in controlled laboratory conditions e g trained subjects gained experience during testing and presence of safety equipment as well as dealing with the bias of tests conducted on scaled models e g the dummy cannot continuously adjust its posture the dummy is not affected by psychological factors etc the authors proposed a new methodology that extrapolated the flow characteristics from videos available on the web with the results being verified through observations the authors suggested studying the stability problem in a statistical framework rather than in a deterministic manner and accordingly proposed a methodology that identified the stability surface and relative thresholds in probabilistic terms arrighi et al 2017 highlighted the importance of including the body characteristics in analysing the interaction between a human body and the hydrodynamics in a flood event in other words the interaction depends on the portions and shape of the body that are in contact with the floodwater as well as the flow characteristics recently chen et al 2018 expanded the work of xia et al 2014a to include the effects of adjustments to a human body in a flood and they revised some of the key parameters in order to consider american and european body characteristics however the corresponding formulae were only obtained for toppling and did not include the effects of bed slope 4 2 mechanics based method it is first necessary to evaluate the incipient velocity for pedestrians in order to determine their instability in floodwaters the incipient velocity is defined in a similar manner to the incipient velocity of sediment particles in sediment transport formulation and is the velocity at which a person loses stability in floodwaters through the mechanisms of sliding or toppling before starting to move with the flow the sliding failure mechanism is given as xia et al 2014a 1 u c α h f h p β m p ρ f h p h f a 1 h f h p b 1 a 2 m p b 2 h p 2 for a sloping terrain the toppling failure mechanism is given as xia et al 2014a 2 u c α h f h p β m p h f 2 ρ f cos θ γ sin θ a 1 h p 2 b 1 h f h p a 2 m p b 2 where uc incipient velocity hf water depth m hp height of pedestrian m mp weight of pedestrian kg ρf density of water kg m3 α and β empirical coefficients and a1 a2 b1 b2 coefficients defining the characteristic features of a human body e g mass height and volume of the full body and body segments such as legs torso arms etc as shown in table 1 θ angle of the sloping ground and γ correction constant it is possible to determine a flood hazard rating fhr parameter by considering both failure mechanisms as follows 3 fhr m i n 1 u u c where u flow velocity and uc incipient velocity which is the minimum velocity of either utoppling or usliding further details of this approach can be found in xia et al 2014a therefore it is possible to calculate the precise threshold conditions for different age and gender groups as well as taking account of differences in body characteristics depending on the country etc milanesi et al 2015 xia et al 2014b as noted by gonzález riancho et al 2013 slope is an important factor which can significantly affect the flood hazard assessment for pedestrians one of the refinements of this research study has been the inclusion of the term related to the slope in eq 2 in previous studies this term has generally been omitted for simplicity the term relative to the slope in eq 2 is represented through the additional term given by cos θ γ sin θ in which θ represents the slope angle of the ground in this study the incipient velocity equations have been included in the finite difference model outlined previously where the bed elevation was stored at the centre of each grid cell ahmadian et al 2018 kvočka et al 2017 liang et al 2007a 2007b the slope of each computational cell was calculated by first evaluating the ground slope between the centre of the cell and the centre of the four neighbouring cells as shown in fig 3 the highest slope calculated was then selected as the slope to be used for the value of θ in eq 2 in this way the most adverse situation is considered in a precautionary approach in the proposed formulae for the stability of a pedestrian since both case studies are located in the uk the values used for the characteristics of a pedestrian are based on the typical dimensions of an average british person except for α and β since these are not available at the moment and as given in table 1 the parameters α and β depend on several factors such as the shape of the human body pedestrian s ability to adjust his her position in order to maintain stability in floodwaters and the drag and friction coefficients between the pedestrian and the ground surface typical values for α and β are different when considering toppling or sliding and allow the calibration of the method using both tests using real pedestrians and dummies 4 3 method a method a uses graphs to determine the flood hazard for pedestrians the u s department of the interior 1988 provided one graph for adults i e a person over 1 5 m in height and 54 kg in weight fig 4 a and one graph for children fig 4b details of the classifications of the hazard ratio can be founded in u s department of the interior 1988 4 4 method b cox et al 2010 in revising the stability thresholds for the australian guidelines used experimental data to establish different levels of hazard based on the product of depth and velocity as shown in fig 5 the authors proposed four different thresholds based on different depth and velocity products there was also a limiting depth and velocity considered for both children and adults the limiting velocity was 3 0 m s for both children and adults while limiting depths were 0 5 m and 1 2 m for children and adults respectively these values represent the depth and velocity thresholds for extreme danger details of the depth velocity relationships and the relative fhr for the different categories can be found in the classifications of the hazard ratio first given in cox et al 2010 4 5 method c a mathematical expression using an empirically based method is widely used in the uk ramsbottom et al 2006 ramsbottom et al 2003 and is given as 4 hr d v 0 5 d f where hr is the flood hazard rating d water depth m v velocity of flow m s and df is a factor which depends on the threat posed by debris which assumes a value of 0 0 5 or 1 ramsbottom et al 2006 details of the classifications of the hazard ratio can be found in ramsbottom et al 2006 4 6 method d similar to the guidelines provided by the us technical guidelines for dam classification based on the potential risk of failure ministerio de medio ambiente de espana 1996 two graphs were developed fig 6 to assess the flood hazard for the case of a dam failure in spain the graphs in fig 6 show the depth velocity relationships and the associated flood hazard levels for a urban areas and b rural or non urban areas the graphs are based on the product of the depth and velocity from these results it can be seen that there is good correlation between the graphs of the pedestrian route for adults using method a fig 4 a and the graph for the unurbanized area of method d fig 6b from this comparison the graph for the unurbanized area will be used hereafter rather than the pedestrian graph of method a since the graph for method d is more conservative a description and classification of the hazard level based on this method is reported in ministerio de medio ambiente de espana 1996 4 7 method e martínez gomariz et al 2016 derived an empirical equation for pedestrians based on results obtained from experiments with human subjects of different ages and gender the authors have merged these new data with previous data published by russo in 2009 in order to obtain more instability conditions this new merged dataset has been used to define the lower limit function expressed by eq 5 martínez gomariz et al 2016 thus depending on the value of the product of the water depth i e d in eq 5 and the flow velocity i e v in eq 5 it is possible to determine the stability of a pedestrian as shown in fig 7 5 d v 0 22 m 2 s 1 a classification of the hazard level can be found in martínez gomariz et al 2016 5 results the results of the benchmark studies for the revised mbm and the other methods shown individually in sections 5 1 5 5 the scope of this benchmarking study is therefore to highlight the improvements obtained using the revised mbm as compared to previous studies all the figures relative to simulation time 340 min for boscastle and 720 min for borth are reported as supplement material 5 1 benchmark between revised mbm and method a fig 8 shows the benchmark results between the revised mbm and method a in terms of predictions of the fhr for boscastle and borth respectively from the comparisons it can be seen that when using the revised mbm rather than method a there is a greater extension of the areas with an extreme fhr in considering the boscastle site method a assessed 29 54 less area characterised by extreme fhr at simulation time st 200 min fig 8b and 3 51 less at st 340 min similarly for the borth site method a assessed 28 96 and 48 71 less extreme fhr areas at st 420 min fig 8d and st 720 min respectively 5 2 benchmark between revised mbm and method b in fig 9 the results are benchmarked between the revised mbm and method b for the sites at boscastle and borth respectively it can be seen from the results that when using method b there are less regions characterised with an extreme fhr in particular there is 55 60 and 15 27 less area for st 200 min fig 9b and 340 min respectively for borth there are the 28 95 at st 420 min fig 9d and 48 71 at st 720 min less areas of extreme fhr when using method b instead of the revised mbm 5 3 benchmark between revised mbm and method c fig 10 shows benchmarked results between the revised mbm and method c for boscastle and borth sites respectively it can be seen that when using method c there is a reduction in the extreme fhr compared to the results obtained using the revised mbm for boscastle the difference is 76 93 at st 200 min fig 10b and 27 04 at st 340 min for the borth case study the difference is 83 60 at st 420 min fig 10d and 81 65 at st 720 min 5 4 benchmark between revised mbm and method d the benchmark results between those for the revised mbm and method d are shown in fig 11 for the boscastle and borth sites it can be seen that when using the revised mbm there is a greater extension of the areas categorised as extreme fhr for the boscastle case study when using method d there are 29 69 and 3 69 less extreme fhr areas at st 200 min fig 11b and at st 340 min respectively for borth then method d shows 36 06 and 47 94 less extreme fhr areas at st 420 min fig 11d and st 720 min respectively 5 5 benchmark between revised mbm and method e fig 12 shows the benchmark comparisons between the revised mbm and method e the results show a greater extension of extreme fhr when using method e in particular for the boscastle site the increase in area is 10 02 at st 200 min fig 12b and 6 19 at st 340 min for the borth case study method e assesses increases of 15 30 at st 420 min fig 12d and 11 06 at st 720 min 6 discussion the results presented in this study have shown that the empirical methods except for the method e generally underestimate the fhr results for extreme flood events musolino et al 2020 russo et al 2014 when compared with the revised mbm approach tables 2 and 3 in comparing the results in tables 2 and 3 at st 200 min and st 420 min respectively it can be seen that the difference is very similar in comparison with the results reported in tables 2 and 3 at st 320 min and st 720 min respectively when st is close to the peak of the flood event then the differences are noticeably different this is explained by the fact that the two flood events are different in terms of intensity with the boscastle flood event being a 1 400 years flood event and borth being a 1 100 years flood event see sections 2 1 and 2 2 hence if the value of the water depths and flow velocities are relatively large then all of the assessment methods tend to give similar assessments of the stability thresholds which have been already largely exceeded in comparing the results of the revised mbm and method e it is noted that the differences in the fhr areas are close for both case studies in considering the two sts the difference between the revised mbm and method e is 3 83 for boscastle and 4 26 for borth with no such big differences when considering the benchmark with the other methods at different sts especially for the boscastle case study this observation means that the two methods give reliable results no matter how extreme the flood event is so far there are no data available on the instability of a pedestrian in a real flood i e instability data obtained during a real flood event data available are only from experiments which take account of some of the most important factors leading to instability this makes validation of different methods and to a higher degree comparisons of the performance of the methods and uncertainty associated with the predictions more difficult to assess moreover different studies have highlighted that it is necessary to include the full physical characteristics of the flood in order to accurately assess the flood hazard from the perspective of pedestrian protection in events characterised by deep flood waters high flow velocities and sudden variations in the flow regime such those occurring in flash floods arrighi et al 2017 kvočka et al 2016 milanesi et al 2015 musolino et al 2020 this leads to further consideration of the mechanics based methods such as the revised mbm presented herein furthermore this study has highlighted some of the inconsistencies between the different empirical methods and the revised mbm for various physical characteristics which confirms the caution needed in an empirical method alone thus it is important to use an appropriate assessment method since if an emergency evacuation plan needs to be activated for local residents then it is important to undertake the planning as soon as possible in order to implement the safest evacuation pathways in contrast if the fhr predictions are not reasonably accurate then any evacuation plans can be erroneous and could have serious consequences the difference in the predictions is thought to be due to the following reasons the revised mbm approach is defined as being a product of the submerged depth and the square of the free stream velocity while the empirical methods are based on the product of the depth and velocity this later approach is inconsistent with an analysis of the hydrodynamic forces on a stationary body generally the difference in the results are covered by experimental coefficients at low velocities however for these case studies and similar extreme flood events the difference in the hazard assessment is expected to be considerably higher when the velocity is well in excess of unity as is generally the case for most extreme flood events thus when assessing extreme flood events which are often also characterised by deeper floodwaters higher flow velocities and sudden variations in the flow regime necessitate the inclusion of a full physical analysis as for the revised mbm approach as aforementioned furthermore the revised mbm approach considers all of the forces acting on a pedestrian moving in floodwaters including the effects of the ground slope both velocity and slope are relevant factors to be considered when assessing flood events especially in steep catchments methods a and d assess the fhr to produce very similar graphs and hence the results are similar method a allows a characterization between the thresholds for adults and children whereas method d does not include this distinction moreover both methods have been developed for dam failures and therefore consider very specific flood characteristics i e a rapid change in depth as well as velocity moreover the graphs leave some areas to the judgment of the individual which could be misleading furthermore considering that the key flood characteristics considered i e velocity and water depth then a distinction between the urban and non urban areas may not be adequate russo et al 2014 this suggests that the methods a and d need to be updated as suggested by martínez gomariz et al 2016 and russo et al 2014 when using method b further explanation of the lower fhr threshold is explained by the fact that cox et al 2010 in updating the previous thresholds used a database which included extensive scatter in the data the data were obtained from experimental campaigns which were conducted with inconsistent procedures thereby increasing the potential for errors such as gaining experience of the tested subjects use of safety equipment not including slope effects etc arrighi et al 2017 russo et al 2013 the differences in of the predicted fhr values using method c as highlighted in fig 10 and the revised mbm can be explained by the fact that the revised mbm approach considers the square of the velocity in its formulation as mentioned previously the difference in the results are also explained by the following limitations highlighted by cox et al 2010 i the available datasets have been averaged regardless of the influence of the training that the subjects gained repeating the same task during the tests due to the averaged data the final formula includes the effects of training in formulating the results since the majority of pedestrians would not have any experience in moving in floodwaters then the assumption of any form of training cannot be considered as valid ii there is no particular experiment supporting the proposed values for the debris factor iii the authors did not include any upper depth limit which means that a large depth and a low velocity would not necessarily be considered as dangerous but this may be the case since once a pedestrian starts to float then the person becomes unstable moreover milanesi et al 2015 pointed out that by considering the nature of the empirical approximation function as purely regressive it is not possible to truly connect hazard level and physical effects so there is no relationship between hazard levels with physical aspects of pedestrians e g no different thresholds for age body size and shape the authors of method c also pointed out in their work is that the expression they proposed is based on experience of flood hazard estimation it is recognised that the expression appears rather arbitrary and refinement of this relationship is proposed in phase 2 based on a more detailed assessment of previous work together with possible new research ramsbottom et al 2003 in phase 2 ramsbottom et al 2006 refined the expression but only for the part relative to the debris factor since at the time studies relative to the use of the square of the velocity were not available for method e despite the good results obtained when using this method some limitations are present firstly the experimental method does not offer the possibility to characterise different body characteristics this means that the method needs to be tailored for different areas in the world where body characteristics can be very different by repeating the experiments similarly it is not possible to obtain thresholds for different categories i e adults and children inside a specific geographic group secondly the authors focused their attention on flow cases with a high velocity and shallow depth so neglecting the toppling failure mechanism which occurs more frequently in deeper flows the limitations and results reported herein for all of the methods benchmarked against the revised mbm suggest that the existing frameworks widely used can be improved using a more physics based methodology as presented in this study the historical case studies reported in this study were related to two specific return period flood events namely 1in 400 years for boscastle and 1 in 100 years for borth however in assessing the flood hazard of a specific area from a pedestrian protection perspective different return periods should be considered creating multiple aggregated scenarios considering different return periods offers more insight dankers and feyen 2009 menne and murray 2013 yin et al 2013 in pedestrian protection perspective and can better support the design optimisation of evacuation plans based on multiple aggregated scenarios in order to undertake this improvement floods with different return periods should be simulated and multiple flood hazard scenarios considered for pedestrian protection based on a different set of characteristics for each return period finally the evacuation plans could then be aggregated and produced as a function of the return period this is beyond the scope of this paper and is recommended to be considered in future studies 7 conclusions in this study the main methods used internationally and reported in the literature have been benchmarked against the mechanics based approach with the aim of investigating the scope for improving the fhr from a pedestrian protection perspective when considering extreme flood events such as flash floods the comparisons reported herein have highlighted that the empirical methods have limitations in acquire reliable thresholds of human stability in flood waters although the method used by martínez gomariz et al 2016 have shown very similar predictions to the revised mbm method the method lacks the capability to include human body characteristics in calculating the threshold velocity and or depth this means that the method needs to be calibrated by extensive experiments in different regions and it cannot be used for different groups of people with different body types and capabilities e g adults children and less mobile senior citizens moreover the approaches widely used by authorities were considered not to be sufficiently accurate in terms of assessing human stability thresholds in floodwaters and a revision to these methods should be considered in using most recent methodologies as for the revised mbm approach this study proposed a revised mbm which has included the most recent available body shape parameter values and the effects of the ground slope in the formulation these additional parameters have allowed improved accuracy in the determination of the physics based threshold levels which should lead to enhanced safety of pedestrians moving through evacuation routes during extreme flood events the revised mbm approach proposed herein has a key limitation in terms of the availability of data relating to the body shape parameters if these data are not available then a detailed characterization for the study area cannot be undertaken with a relatively high degree of accuracy particularly since generic body shape data then has to be used moreover the impact on the flood hazard assessment due to psychological and behavioural response has been not considered in the formulation with these aspects being recommendations for future works further research is also required on developing new flood hazard maps based on the most critical pedestrian category for the study area and considering different flood return periods as proposed in section 5 credit authorship contribution statement g musolino conceptualization methodology software writing original draft writing review editing visualization data curation r ahmadian supervision conceptualization methodology project administration funding acquisition resources writing review editing r a falconer supervision writing review editing declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments the research reported herein is funded by the engineering and physical sciences research council through centre for doctoral training in water informatics science and engineering wise cdt with grant number ep l016214 1 and the royal academy of engineering through urban flooding research policy impact programme with grant number uufrip 100031 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j hydroa 2020 100067 appendix a supplementary data the following are the supplementary data to this article supplementary data 1 supplementary data 2 supplementary data 3 supplementary data 4 supplementary data 5 supplementary data 6 supplementary data 7 supplementary data 8 supplementary data 9 supplementary data 10 supplementary data 11 
