index,text
25530,widely used term weighting schemes and machine learning ml classifiers with default parameter settings were assessed for their performance when applied to environmental big data analysis five term weighting schemes term frequency tf tf inverse document frequency tf idf best match 25 bm25 tf inverse gravity moment tf igm and tf idf inverse class frequency tf idf icf and five different ml classifiers support vector machine svm naive bayes nb logistic regression lr random forest rf and extreme gradient boosting xgboost were tested the optimal text classification scheme and classifier were tf idf icf and lr respectively based on evaluation criteria their combination resulted in the best performance of all scheme and classifier combinations for the full environmental data analysis category classification performance differed according to the environmental section climate air water or waste garbage with the best performance being achieved for climate and the poorest for water this demonstrated the importance of selecting term weighting schemes and ml classifiers in human generated environmental big data analysis accordingly the study was designed to evaluate the text classification performance of various term weighting schemes for environmental big data using common ml algorithms i e support vector machine svm naive bayes nb logistic regression lr random forest rf and extreme gradient boosting xgboost news articles served as human generated environmental big data because such articles have well structured and filtered data compared to social media and blogs they therefore provide reliable objective information when applying term weighting schemes additionally news articles are commonly used as textual resources to evaluate the performance of term weighting schemes using term weighting schemes and ml algorithms in the analysis process the present study followed specific steps 1 using a web article crawling method digital environmental news articles from the past 11 years were downloaded from a korean news platform 2 articles were classified into four categories of major environmental issues i e climate air water and waste garbage 3 text mining preprocessing served to extract nouns from each document 4 meaningful nouns were defined using the term weighing algorithm under each term weighting scheme and 5 using several ml classifiers classification performance was evaluated fig 1 accordingly the present study demonstrates the significance of selecting the term weighting scheme with ml classifiers by comparing different term weighting schemes i e tf tf idf bm25 tf igm and term frequency inverse document frequency inverse class frequency tf idf icf this research was supported by the basic science research program through the national research foundation of korea nrf funded by the ministry of education grant no nrf 2020r1a6a1a03042742 nrf 2021r1i1a1a01060891 and the ministry of science and ict grant no nrf 2021r1c1c1004179 this research was supported by the basic science research program through the national research foundation of korea nrf funded by the ministry of education grant no nrf 2020r1a6a1a03042742 nrf 2021r1i1a1a01060891 and the ministry of science and ict grant no nrf 2021r1c1c1004179 0 https doi org 10 15223 policy 017 https doi org 10 15223 policy 037 https doi org 10 15223 policy 012 https doi org 10 15223 policy 029 https doi org 10 15223 policy 004 item s1364 8152 22 00236 5 s1364815222002365 1 s2 0 s1364815222002365 10 1016 j envsoft 2022 105536 271872 2022 10 04t10 15 39 535193z 2022 11 01 2022 11 30 1 s2 0 s1364815222002365 main pdf https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s1364815222002365 main application pdf 0857126805863aafaeeafef9ca275159 main pdf main pdf pdf true 3722144 main 11 1 s2 0 s1364815222002365 main 1 png https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s1364815222002365 preview image png 35ab82f5d217bb83a0a27bf98ef8538f main 1 png main 1 png png 61376 849 656 image web pdf 1 1 s2 0 s1364815222002365 gr4 jpg https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s1364815222002365 gr4 downsampled image jpeg 75d7e214409d7427a0c167945e00d5ab gr4 jpg gr4 gr4 jpg jpg 189894 452 691 image downsampled 1 s2 0 s1364815222002365 gr3 jpg https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s1364815222002365 gr3 downsampled image jpeg e258c9ad32739313763e9c40e34591ff gr3 jpg gr3 gr3 jpg jpg 108540 298 535 image downsampled 1 s2 0 s1364815222002365 gr2 jpg https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s1364815222002365 gr2 downsampled image jpeg 037aa8bad5cd55ac27580ff73a15378e gr2 jpg gr2 gr2 jpg jpg 87914 250 535 image downsampled 1 s2 0 s1364815222002365 gr1 jpg https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s1364815222002365 gr1 downsampled image jpeg c708d693305de362d1177e045b6cd44e gr1 jpg gr1 gr1 jpg jpg 174595 475 535 image downsampled 1 s2 0 s1364815222002365 gr4 sml https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s1364815222002365 gr4 thumbnail image gif 9c64e55a92447ebec085b87c8021414b gr4 sml gr4 gr4 sml sml 86852 143 219 image thumbnail 1 s2 0 s1364815222002365 gr3 sml https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s1364815222002365 gr3 thumbnail image gif f8c1aa71b55c9c9debc397fb915a7d99 gr3 sml gr3 gr3 sml sml 75159 122 219 image thumbnail 1 s2 0 s1364815222002365 gr2 sml https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s1364815222002365 gr2 thumbnail image gif 65b41e9e81e6e466e07c3aae3321ee00 gr2 sml gr2 gr2 sml sml 69370 102 219 image thumbnail 1 s2 0 s1364815222002365 gr1 sml https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s1364815222002365 gr1 thumbnail image gif ae0c70480c769b771c60f05f90e5bebc gr1 sml gr1 gr1 sml sml 82436 164 185 image thumbnail 1 s2 0 s1364815222002365 gr4 lrg jpg https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s1364815222002365 highres image jpeg 4c05e92d197275937ffa9d4597ba2c5c gr4 lrg jpg gr4 gr4 lrg jpg jpg 1060103 2002 3059 image high res 1 s2 0 s1364815222002365 gr3 lrg jpg https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s1364815222002365 highres image jpeg e401e8657eaa4691ba0213a953eb899c gr3 lrg jpg gr3 gr3 lrg jpg jpg 338235 1321 2370 image high res 1 s2 0 s1364815222002365 gr2 lrg jpg https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s1364815222002365 highres image jpeg 5e3cc5f27de24e9fe4738621baa12eef gr2 lrg jpg gr2 gr2 lrg jpg jpg 214025 1107 2371 image high res 1 s2 0 s1364815222002365 gr1 lrg jpg https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s1364815222002365 highres image jpeg eeebb94786054c246274981e064baca8 gr1 lrg jpg gr1 gr1 lrg jpg jpg 681060 2103 2370 image high res 1 s2 0 s1364815222002365 si23 svg https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s1364815222002365 image svg xml 628ea3dc0ee36820b61d2acf43fa9fba si23 svg si23 si23 svg svg 98549 altimg 1 s2 0 s1364815222002365 si19 svg https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s1364815222002365 image svg xml 5928f2ab62fd495258b65ebb19d9d4fe si19 svg si19 si19 svg svg 15608 altimg 1 s2 0 s1364815222002365 si14 svg https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s1364815222002365 image svg xml 85796a300ab5c630a142415000aad0be si14 svg si14 si14 svg svg 31536 altimg 1 s2 0 s1364815222002365 si9 svg https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s1364815222002365 image svg xml c96dbb89d043a11227ddbf0fb86f0933 si9 svg si9 si9 svg svg 3907 altimg 1 s2 0 s1364815222002365 si16 svg https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s1364815222002365 image svg xml d786a35ea45ca6a9eaaccb7d7f9fe031 si16 svg si16 si16 svg svg 15626 altimg 1 s2 0 s1364815222002365 si7 svg https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s1364815222002365 image svg xml 3750e4c0c87ba7f505a9f9fe6e76cbef si7 svg si7 si7 svg svg 105194 altimg 1 s2 0 s1364815222002365 si15 svg https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s1364815222002365 image svg xml 68a3c4158180d723fc5da1b88a3edb46 si15 svg si15 si15 svg svg 51095 altimg 1 s2 0 s1364815222002365 si17 svg https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s1364815222002365 image svg xml 4c7e2ae313459acfb767ee9c3a02434f si17 svg si17 si17 svg svg 13781 altimg 1 s2 0 s1364815222002365 si2 svg https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s1364815222002365 image svg xml 74e5b33f74abf8508d1bd9706f048c93 si2 svg si2 si2 svg svg 30393 altimg 1 s2 0 s1364815222002365 si21 svg https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s1364815222002365 image svg xml d9feec93ca998a618a8005f8b740ccbe si21 svg si21 si21 svg svg 74205 altimg 1 s2 0 s1364815222002365 si22 svg https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s1364815222002365 image svg xml e10246815fbdb09a04e9ad9659beba81 si22 svg si22 si22 svg svg 82700 altimg 1 s2 0 s1364815222002365 si12 svg https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s1364815222002365 image svg xml 0ecd882c2d3dbbd4b6f669b6b9f107a2 si12 svg si12 si12 svg svg 20442 altimg 1 s2 0 s1364815222002365 si24 svg https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s1364815222002365 image svg xml 17db8be95700327020b1d9ed7a6f59d2 si24 svg si24 si24 svg svg 182190 altimg 1 s2 0 s1364815222002365 si6 svg https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s1364815222002365 image svg xml 27b738ac4db044b10aa9389a831dd072 si6 svg si6 si6 svg svg 22444 altimg 1 s2 0 s1364815222002365 si18 svg https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s1364815222002365 image svg xml 47f07e1d549540a4e4a970312840364c si18 svg si18 si18 svg svg 21859 altimg 1 s2 0 s1364815222002365 si3 svg https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s1364815222002365 image svg xml 34e9d4113051ae2034cc35d8c841cee8 si3 svg si3 si3 svg svg 9194 altimg 1 s2 0 s1364815222002365 si20 svg https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s1364815222002365 image svg xml 5a90c36a3a39eac9329865649d3e9d30 si20 svg si20 si20 svg svg 78936 altimg 1 s2 0 s1364815222002365 si4 svg https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s1364815222002365 image svg xml 1f739bf8667a685d28ffc40f41d3a5a7 si4 svg si4 si4 svg svg 13886 altimg 1 s2 0 s1364815222002365 si1 svg https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s1364815222002365 image svg xml c4fb246768c531ab1e4a731238ca7b72 si1 svg si1 si1 svg svg 85658 altimg 1 s2 0 s1364815222002365 si5 svg https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s1364815222002365 image svg xml 4d6c54d37721c83017eb2ead1c308e10 si5 svg si5 si5 svg svg 99235 altimg 1 s2 0 s1364815222002365 si13 svg https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s1364815222002365 image svg xml 094f132ad5490aeeda8b82dff357cdd4 si13 svg si13 si13 svg svg 116353 altimg 1 s2 0 s1364815222002365 si8 svg https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s1364815222002365 image svg xml 5cdd24238a749822624650b469a6174c si8 svg si8 si8 svg svg 13569 altimg 1 s2 0 s1364815222002365 si11 svg https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s1364815222002365 image svg xml 6535c83ca1b342d4979d068038c5bd95 si11 svg si11 si11 svg svg 115121 altimg 1 s2 0 s1364815222002365 si10 svg https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s1364815222002365 image svg xml 9fec1b537a6d05503d98315a3b9cfaed si10 svg si10 si10 svg svg 4346 altimg 1 s2 0 s1364815222002365 am pdf https s3 eu west 1 amazonaws com prod ucs content store eu west content egi 108qf2q15d8 main application pdf b8c55019a1ee4c8852204590faa9217a am pdf am am pdf pdf false 1128005 aam pdf enso 105536 105536 s1364 8152 22 00236 5 10 1016 j envsoft 2022 105536 elsevier ltd fig 1 schematic diagram of this study fig 1 fig 2 data distribution of each category for training and validation datasets fig 2 fig 3 weighted f1 score for each term weighting scheme in the different categories for the validation dataset using the five machine learning ml classifiers fig 3 fig 4 f1 scores of each term weighting scheme in different categories for the validation dataset using the five ml classifiers fig 4 table 1 article classification for environmental news by keyword for each environmental category in seoul south korea table 1 categories keywords training dataset article validation dataset article total articles air air pollution air quality fine dust yellow sand exhaust smog air ozone sulfur monoxide nitrogen oxide sulfur oxide acid rain visibility dioxin atmosphere discharge specific air pollution volatile organic compounds diesel particle air in the room flue gas clean fuel incinerator benzene background concentration dust 15 994 1 978 012 words 5 331 498 212 words 21 325 2 476 224 words climate climate greenhouse gases global warming carbon dioxide carbon heavy snow heavy rain flood drought typhon sea level heat island abnormally high temperature localized heavy rain tidal wave tsunami desertification green growth 8 511 1 330 164 words 2 838 337 769 words 11 349 1 667 933 words water algal bloom water quality wastewater four main rivers river pollution ecological stream clean water pure water aquatic ecology river ecology water environment buffer strip tn fish mortality tp sewer manure septic tank water saving rainwater management inundation water supply fry discharge stream restoration river restoration wetland shoals of fish stream stench unauthorized discharge dirking water water deficit water saving water waste water bill aquanomics water reuse recycle water water treatment seawater desalination 5 490 852 124 words 1 372 206 861 words 6 862 1 058 985 words waste garbage garbage waste recycling garbage collection disposal system landfill food waste leachate illegal trash dispose of plastic waste ocean disposal waste resources waste heat upcycling abandoned metal material flow analysis leaching test resource recirculation waste electric and electronic end of life vehicle 3 717 700 335 words 1 240 179 358 words 4 957 879 693 words total number of downloaded news articles 33 712 4 867 037 words 10 781 1 215 798words 44 493 6 082 835 words table 2 hyperparameters of machine learning ml classifiers used in this study table 2 ml classifiers hyperparameters definition parameters svm kernel type specifies the kernel type to use in the algorithm rbf c regularization parameter 1 0 degree degree of the polynomial kernel function 3 lr solver algorithm to use in the optimization problem saga penalty specify the norm of the penalty elasticnet multi class define the multiclass classification multinomial nb alpha additive smoothing parameter 1 0 rf n estimators the number of tresses in the forest 100 min samples split the minimum number of samples required to split an intern node 2 max faatures the number of features to consider when looking for the best split auto xgboot booster the type of model gbtree max depth the depth of tree 6 sampling method the method to use to sample the training instances uniform table 3 contingency table for the target category ci table 3 confusion matrix predicted c i yes predicted c i no actual c i e g climate yes true positive tp a false negative fn b actual c i no false positive fp c true negative tn d a tp the number of documents that belong to actual ci resulting in predicted ci b fn the number of documents that belong to actual ci not resulting in predicted ci c fp the number of documents that do not belong to actual ci resulting in predicted ci d tn the number of documents that do not belong to actual ci not resulting in predicted ci table 4 evaluation results for each term weighting scheme with the five machine learning ml classifiers using the validation dataset table 4 statistics ml classifiers term weighting schemes tf tf idf bm25 tf igm tf idf icf accuracy svm 0 57 0 72 0 41 0 62 0 74 lr 0 69 0 72 0 61 0 74 0 75 rf 0 62 0 62 0 63 0 62 0 63 nb 0 69 0 68 0 67 0 60 0 68 xgboost 0 65 0 66 0 65 0 65 0 65 precision svm 0 51 0 68 0 24 0 57 0 72 lr 0 64 0 67 0 55 0 69 0 71 rf 0 57 0 57 0 58 0 57 0 58 nb 0 64 0 64 0 63 0 59 0 65 xgboost 0 60 0 61 0 60 0 60 0 61 recall svm 0 46 0 67 0 24 0 57 0 69 lr 0 68 0 72 0 57 0 75 0 76 rf 0 57 0 57 0 58 0 57 0 58 nb 0 67 0 66 0 67 0 59 0 65 xgboost 0 61 0 62 0 60 0 61 0 61 f1 score svm 0 48 0 68 0 22 0 57 0 70 lr 0 65 0 69 0 59 0 71 0 73 rf 0 57 0 57 0 58 0 57 0 58 nb 0 60 0 64 0 63 0 59 0 64 xgboost 0 61 0 62 0 60 0 60 0 61 note svm indicates support vector machine nb indicates naive bayes lr indicates logistic regression rf indicates random forest xgboost indicates extreme gradient boosting tf indicates term frequency tf idf indicates term frequency inverse document frequency bm25 indicates best match 25 tf igm indicates term frequency inverse gravity moment and tf idf icf indicates term frequency inverse document frequency inverse class frequency table 5 evaluation results of the precision and recall of each term weighting scheme in different categories for the validation dataset using the five ml classifiers table 5 ml classifiers term weighting schemes precision recall air climate water waste garbage air climate water waste garbage svm tf 0 49 0 63 0 43 0 50 0 43 0 77 0 33 0 33 tf idf 0 63 0 79 0 64 0 67 0 61 0 83 0 61 0 63 bm25 0 18 0 49 0 12 0 16 0 09 0 78 0 06 0 05 tf igm 0 52 0 74 0 46 0 56 0 51 0 75 0 45 0 56 tf idf icf 0 66 0 79 0 67 0 73 0 66 0 84 0 60 0 66 lr tf 0 63 0 86 0 56 0 50 0 63 0 73 0 66 0 70 tf idf 0 65 0 88 0 60 0 56 0 65 0 75 0 72 0 77 bm25 0 50 0 78 0 46 0 46 0 51 0 72 0 51 0 53 tf igm 0 68 0 89 0 61 0 59 0 68 0 76 0 73 0 81 tf idf icf 0 69 0 90 0 64 0 62 0 71 0 76 0 76 0 83 rf tf 0 50 0 74 0 47 0 58 0 50 0 75 0 46 0 58 tf idf 0 50 0 73 0 49 0 58 0 50 0 75 0 46 0 58 bm25 0 51 0 74 0 49 0 59 0 52 0 75 0 47 0 57 tf igm 0 52 0 74 0 47 0 56 0 52 0 75 0 45 0 57 tf idf icf 0 52 0 73 0 50 0 59 0 52 0 75 0 46 0 58 nb tf 0 53 0 75 0 41 0 44 0 60 0 79 0 59 0 59 tf idf 0 77 0 61 0 50 0 74 0 78 0 60 0 57 0 61 bm25 0 74 0 57 0 57 0 79 0 78 0 59 0 58 0 58 tf igm 0 71 0 44 0 32 0 90 0 75 0 53 0 41 0 44 tf idf icf 0 79 0 58 0 68 0 55 0 77 0 64 0 48 0 72 xgboost tf 0 77 0 55 0 52 0 57 0 76 0 55 0 51 0 61 tf idf 0 77 0 54 0 53 0 61 0 77 0 55 0 53 0 62 bm25 0 76 0 53 0 52 0 59 0 76 0 54 0 51 0 60 tf igm 0 76 0 55 0 51 0 57 0 76 0 54 0 51 0 60 tf idf icf 0 75 0 55 0 53 0 60 0 75 0 54 0 51 0 62 comparative study of term weighting schemes for environmental big data using machine learning jungjin kim a han ul kim b jan adamowski c shadi hatami c hanseok jeong a d a institute of environmental technology seoul national university of science and technology seoul 01811 republic of korea institute of environmental technology seoul national university of science and technology seoul 01811 republic of korea institute of environmental technology seoul national university of science and technology seoul 01811 republic of korea b department of applied artificial intelligence seoul national university of science and technology seoul 01811 republic of korea department of applied artificial intelligence seoul national university of science and technology seoul 01811 republic of korea department of applied artificial intelligence seoul national university of science and technology seoul 01811 republic of korea c department of bioresource engineering mcgill university ste anne de bellevue quebec canada department of bioresource engineering mcgill university ste anne de bellevue quebec canada department of bioresource engineering mcgill university ste anne de bellevue quebec canada d department of environmental engineering seoul national university of science and technology seoul 01811 republic of korea department of environmental engineering seoul national university of science and technology seoul 01811 republic of korea department of environmental engineering seoul national university of science and technology seoul 01811 republic of korea corresponding author 120 1 chungun hall 232 gongneung ro nowon gu seoul 01811 republic of korea 120 1 chungun hall 232 gongneung ro nowon gu seoul 01811 republic of korea widely used term weighting schemes and machine learning ml classifiers with default parameter settings were assessed for their performance when applied to environmental big data analysis five term weighting schemes term frequency tf tf inverse document frequency tf idf best match 25 bm25 tf inverse gravity moment tf igm and tf idf inverse class frequency tf idf icf and five different ml classifiers support vector machine svm naive bayes nb logistic regression lr random forest rf and extreme gradient boosting xgboost were tested the optimal text classification scheme and classifier were tf idf icf and lr respectively based on evaluation criteria their combination resulted in the best performance of all scheme and classifier combinations for the full environmental data analysis category classification performance differed according to the environmental section climate air water or waste garbage with the best performance being achieved for climate and the poorest for water this demonstrated the importance of selecting term weighting schemes and ml classifiers in human generated environmental big data analysis keywords text classification environmental digital news term weighting schemes feature selection data availability data will be made available on request 1 introduction the explosion of machine and human generated data as well as advances in computing technology have led to the current era of big data big data and its analysis have been widely applied in various fields including health care dash et al 2019 marketing lies 2019 agriculture xiao et al 2021 banking hung et al 2020 administration andrews 2019 and education khan and alqahtani 2020 to identify hidden patterns unknown correlations and user trends decision makers and researchers can enhance the benefits of their actions increase transparency and allow for better results and decisions by using valuable information drawn from big data analytics in particular big data is increasingly being called upon in environmental management machine generated remote sensing data provide real time information concerning various observations allowing for better representations of the environment human generated data offer new possibilities for applications in environmental management for example roby et al 2018 developed the articulate tool to collate material from the news media addressing a topic of interest in the field of environmental issues and evaluate the most important events related to that topic morss et al 2017 recently demonstrated how hazardous weather could be predicted using twitter datasets similarly kryvasheyeu et al 2016 examined the relationship between the real and perceived threats for disaster damage using twitter s message stream in another interesting application chen et al 2016 examined the frequency of articles on floods and wetlands from the new york times to validate a sociological model and better understand human and water interactions despite these promising case studies extracting valuable information from this surge of data to make informed decisions remains a great challenge human generated data is very difficult to interpret because of its unstructured nature and improved data processing techniques are therefore needed to facilitate the widespread use of human generated data in environmental management text mining is a key technique used to analyze collections of textual human generated data this technique captures useful data by uncovering textual information within unstructured data by identifying hidden patterns correlations and other insights using natural language processing nlp many studies have applied text mining techniques to extract meaningful information from online datasets deshmukh and phalnikar 2018 jiao et al 2021 li and liu 2018 li et al 2017 mendez et al 2019 rivera et al 2014 term weighting schemes present another essential text mining process for extracting and classifying environmental information from human generated data sources such schemes quantify terms in a document and employ features such as the frequency of stemmed words to define the importance of certain words in the document and corpus el khair 2009 kim and kim 2016 sabbah et al 2017 recent studies have supported the utility of term weighted schemes for various elements of data analysis chum et al 2008 reported these methods to be effective in text classification and duplicate image detection as well as for information retrieval additionally domeniconi et al 2014 showed that applying an appropriate term weighting scheme for text categorization resulted in a considerable boost in classification effectiveness to reach informed decisions in environmental management contexts decision makers need to extract and then prioritize information from human generated data using term weighting schemes however no comprehensive studies have been conducted to explore the performance of various term weighting schemes in environmental big data analysis several term weighting approaches have been used to derive the frequency and distribution of words in digital textual data documents domeniconi et al 2014 the two types of term weighting methods are unsupervised and supervised in the former traditional weighting methods are based on term distribution within documents while they are effective in information retrieval they do not consider document distribution in training documents usually applied to evaluate the importance of a term in a document of a specific class supervised term weighting schemes in contrast use the membership of training documents to generate categories however these schemes are limited to the classification of two classes as they only consider the distribution of terms in two classes term frequency tf and term frequency inverse document frequency tf idf are standard term weighting approaches commonly used for text classification according to beel et al 2017 83 of text based research papers reported using the tf idf scheme modified term weighting schemes such as best match 25 bm25 term frequency inverse gravity moment tf igm modified distinguishing feature selector based on term frequency dfs based tf modified distinguishing feature selector based on modified term frequency mdfs based mtf deep feature weighting dfw and lm dirichlet were developed to improve prediction performance cer et al 2018 chen et al 2021a 2021b jiang et al 2016 to assign appropriate weights to each term in a document or corpus using different term metrics various term weighting schemes have been applied to news articles according to text type corpus size and types of queries however no consensus has emerged regarding the optimal method to classify environmental issues from digital environmental news articles because the environmental category varies according to the application of the term weighting scheme several term weighing schemes with machine learning ml classifiers need to be tested to evaluate their classification performance on environmental data accordingly the study was designed to evaluate the text classification performance of various term weighting schemes for environmental big data using common ml algorithms i e support vector machine svm naive bayes nb logistic regression lr random forest rf and extreme gradient boosting xgboost news articles served as human generated environmental big data because such articles have well structured and filtered data compared to social media and blogs they therefore provide reliable objective information when applying term weighting schemes additionally news articles are commonly used as textual resources to evaluate the performance of term weighting schemes using term weighting schemes and ml algorithms in the analysis process the present study followed specific steps 1 using a web article crawling method digital environmental news articles from the past 11 years were downloaded from a korean news platform 2 articles were classified into four categories of major environmental issues i e climate air water and waste garbage 3 text mining preprocessing served to extract nouns from each document 4 meaningful nouns were defined using the term weighing algorithm under each term weighting scheme and 5 using several ml classifiers classification performance was evaluated fig 1 accordingly the present study demonstrates the significance of selecting the term weighting scheme with ml classifiers by comparing different term weighting schemes i e tf tf idf bm25 tf igm and term frequency inverse document frequency inverse class frequency tf idf icf 2 materials and methods 2 1 experimental setup 2 1 1 data collection online newspapers provide long searchable and in depth representations of topics reported on by the media accordingly this study used digital environmental news information over a period of 11 years january 1 2010 december 31 2020 related to the seoul area in south korea the naver korean portal site was used to collect digital data because it provides all news articles published by mainstream media and internet news publishers in the country a web crawler technique was used to download a total of 44 493 digital news articles related to environmental topics from the portal site the web crawler was programmed with a script to automatically retrieve web page information and insert it into a local repository using a python program additional detailed information concerning the web crawler process can be found in kausar et al 2013 starting with an initial set of urls as seed urls the web crawler downloaded the web pages for the seed urls and then extracted new links present in the downloaded pages the extracted web pages were stored in a defined storage folder so that they could be retrieved later if necessary after the web crawling process the downloaded digital news was then classified into four categories i e climate air water and waste garbage based on the search keywords representing each environmental condition table 1 and fig 2 the entire dataset was divided into 11 349 climate related 21 325 air related 6 862 water related and 4 957 waster garbage related news articles 2 1 2 text mining preprocessing korean language characteristics include many variations on roots the importance of distinguishing postposition and editing and compound nouns with spaces these characteristics make natural language processing more challenging in korean than in english the korean natural language konlpy package park and cho 2014 a natural language text mining package in python was used to phrase the korean sentences a potential term was extracted via the elimination of stop words stemming morphological analysis and part of speech pos tagging the process of stop word elimination removed special characters punctuation marks numbers advertising additional white spaces and unnecessary words for example names news corporations url links and city names because the meaning of nouns changes depending on the combination of korean characters a user dictionary for proper nouns related to environmental issues was created and applied to recognize environmental terms in the noun extraction process finally meaningful nouns were obtained to predict the news categories using konlpy under alternative term weighting schemes and ml classifiers 2 1 3 training and validation design the word vector sparse matrix space from the digital environmental news data was extracted using the term weighting schemes to generate a feature vector space for each scheme every text vector of each sample was formed and provided to the ml classifiers for training the total sample data were divided into a training set 75 of the total data and a validation set 25 of the total data stratified random sampling with stratification on the target variable was used to include the comparable target variable distribution in the training and validation sets five ml classifiers were trained with each training sample dataset from the different term weighting schemes after the training was completed the validation dataset was used to analyze the performances of the five classifiers and five term weighting schemes then their effectiveness was evaluated for the classification of environmental issues 2 2 term weighting schemes feature selection using a term weighting scheme is the preprocessing step in text classification that assigns appropriate weights to each term in all documents this step is significant to improve the efficiency of an ml classifier since it highlights the most discriminatory term in each class alsmadi and hoon 2019 several term weighting techniques have been proposed for text classification in this study the widely used term weighting schemes tf tf idf tf igm tf idf icf and bm25 were used to assign the weight to each word in the news articles under each environmental category and applied as baseline approaches to compare their performances 2 2 1 term frequency tf tf is the simplest and most used term weighting scheme it is a local weighting scheme because it counts how many times a term is present in a document tf assumes that the more frequently a term is in a document the more essential that term is it can be formulated as follows sanz et al 2014 t f t i d k t f t i d k i 1 n t f t i d k 1 where n is the number of chosen terms and t f t i d k is the tf of the term t i in the document d k 2 2 2 term frequency inverse document frequency tf idf tf idf is one of the most popular term weighting schemes for text classification it can evaluate the importance of a word or phrase in the text in a document chen et al 2016 tf idf assumes that a term that occurs less frequently in the corpus is more important in the document the tf idf value of a term t in document d is the dot product of the tf and idf of that term tf indicates the occurrence of related terms and idf is the collected frequency factor values of the related terms for the entire document the tf idf weighting formula is as follows sanz et al 2014 tf idf t i t f t i d k log n d f t i 2 where n is the total number of documents in the entire dataset and d f t i is the number of documents in which t i occurs 2 2 3 term frequency inverse gravity moment tf igm tf igm is a recently proposed term weighting scheme it integrates the tf and igm of the terms as a collection frequency factor igm is a statistical model that has been adapted in term weighting schemes by chen et al 2016 this model estimates the interclass distribution of terms by considering their document frequencies and distinguishing powers tf igm can be calculated as chen et al 2016 tf igm t i t f t i d k 1 γ f i 1 r 1 c f i r r 3 where f i r is the class specific document frequency of t i and γ is a balance parameter ranging from 5 0 to 9 0 during the igm weighting process related document frequencies are sorted in descending order and multiplied by rank values expressed with r r 1 2 3 c 2 2 4 term frequency inverse document frequency inverse class frequency tf idf icf tf idf icf uses the class information of terms in its term weighting process inverse class frequency icf is added to the term frequency inverse document frequency tf idf information in the weighting strategy of this scheme ren and sohrab 2013 icf calculates the ratio of the number of documents and the total number of documents that contain the term t i tf idf icf does not use a globalization policy in terms of weighting because it estimates one weighting score for all terms tf idf icf can be formulated as ren and sohrab 2013 tf idf icf t i t f t i d k 1 log n d f t i 1 log c c f t i 4 here c f t i indicates the number of documents where the term t i occurs and c is the total number of documents in the entire dataset 2 2 5 best match 25 bm25 bm25 was developed using probabilistic theory and is a good performing term weighting scheme that incorporates the tf idf and length normalization of a given document garcia 2016 bm25 ranks documents according to their relevant results using a bag of words retrieval function it considers the entire length of the document under evaluation as well as the frequency of the query terms zhu 2016 the bm25 can be computed as garcia 2016 bm 25 t i f i j f i j k k 1 1 f i j f i j k 1 1 b b d l j a v e d l k 1 1 5 where k k 1 b 6 b 1 b b d l j a v e d l 7 here f i j is the frequency of the term i in the document j d l j is the document length of document j a v e d l is the average document length k is an attenuation factor b is the document length normalization function b is a normalization parameter and k 1 1 is a scaling factor 2 3 machine learning ml classifiers ml classifiers have recently been proposed for automatic text classification based on a training set of labeled documents ml classifiers assign predefined category labels to each document based on likelihood ml classifiers have been extensively applied and rapid progress has been achieved in several research fields using svm nb lr rf and xgboost in our study each ml classifier assigned predefined environmental category labels e g air climate water waste garbage to each article it recognized a pattern that recognized the relation among the keywords group and environmental category labels of the training data after the training was done ml classifiers forecasted the environmental category labels on the keywords group of validation datasets more detailed information concerning these classifiers is given below table 2 shows the hyperparameters that were used in this study most parameters of ml classifiers were set to the default values provided by the program setup based on recommendations from the literature and experience the default values of ml classifiers occasionally work well for specific datasets elgeldawi et al 2021 tran et al 2022 and lorena et al 2011 suggested the use of the default parameter values to allow a fair performance comparison of different ml classifiers therefore the optimization of parameters was not applied because the goal of our study was to compare the potential performance of the term weighting schemes and evaluate the applicability of the text mining processing in korean even with ml classifiers using default parameters 2 3 1 support vector machine svm svm is a powerful supervised learning function used to find the decision surface that maximizes the margin between data points of two classes its basic classification method is the structural risk minimization principle from computation learning vapnik 1991 and it can be divided into linear and nonlinear methods according to its kernel functions the learning process from a training set aims to find a straight hyperplane that distinguishes positive cases from negative cases the hyperplane with the maximum margin is the optimal line separating the maximized distance between the nearest data points of positive and negative classes in the svm the advantages of svm include the ability to handle extremely large feature spaces redundant features and high dimensional feature vectors therefore an aggressive feature selection method is not necessary when svm is applied several studies have indicated that svm achieves high performance and accurate classification analysis for text classification lee et al 2012 wang et al 2006 2 3 2 naive bayes nb nb is a formal classifier based on the bayes theorem with strong independence assumptions rennie et al 2003 it has been widely used for text classification because of its lower computational cost nb can be effectively trained in a supervised setting to determine classification parameters with a relatively small amount of training data due to the nature of the probability model this classifier has been proven to be a suitable and adaptive method in many complex real word classification applications aphinyanaphongs et al 2014 ur rahman and harding 2012 the advantage of nb is that it requires a small amount of training data to determine the classification parameters additionally its simplicity means that training computational efficiency is linear in both the number of instances and attributes 2 3 3 logistic regression lr lr is a regression analysis that predicts categorical outcomes using a certain predictor logistic functions can be used to find the probabilities of the possible outcomes under a supervised classification algorithm the supervised classification algorithm is used to classify each term in the categories according to their importance for text classification lr analyzes the coefficient for each term and estimates the class of the text in a document of word vectors by recognizing the vectors containing variables pranckevičius and marcinkevičius 2016 shah et al 2020 2 3 4 random forest rf developed by breima 2001 rf is an ensemble learning algorithm consisting of the aggregation of several decision trees it results in the reduction of the variable when compared to a single decision tree using a standard classification or regression tree and a decrease in gini impurity each tree is contracted depending on a bootstrap sample drawn randomly from the entire dataset breima 2001 only a given number of variables of the randomly selected terms are considered splitting candidates when creating each tree at each split the prediction rule is hard to determine with rf because of the substantial number of trees consequently rf is commonly referred to as a black box algorithm it is one of the best classification algorithms because it classifies many decision trees in training sets and provides the class outputs according to each tree 2 3 5 extreme gradient boosting xgboost the xgboost algorithm is an improved version of the gradient boosting decision tree it combines multiple decision trees with lower accuracy into a tool with higher accuracy chen and guestrin 2016 gradient descent in the generation of each tree is used in this algorithm where the direction of the minimum given objective function is integrated into xgboost depending on the tree generated in the previous step the prediction model is constructed by continuously decreasing the loss error via the interaction of multiple decision trees each decision tree splits the nodes according to the regression tree criteria a commonly used function for xgboost is the least squared log loss 2 4 performance evaluation the effectiveness of the ml classifiers and term weighting schemes was evaluated using performance metrics including accuracy precision recall and the f1 score the evaluations were conducted using a contingency table of calculations for the target category and its components based on table 3 precision is a measure of the correctly predicted positive cases from all identified documents that should be assigned to the target category recall is the proportion of the correctly predicted positive cases from all actual positive document cases accuracy is the proportion of all correctly predicted classes for the target category c i the f1 score is the average of precision and recall prasetyo 2014 which provides a better representation of the incorrectly predicted cases than the accuracy evaluation metric the weighted f1 score is the weight of the f1 score of each category divided by the amount of data in that class the weighted fi score estimates the f1 score for each category independently and then adds them together using a weight that depends on the number of true labels for each class precision recall accuracy the f1 score and the weighted f1 score were estimated as follows sokolova and lapalme 2009 p r e c i s i o n t p c i t p c i f p c i 8 r e c a l l t p c i t p c i f n c i 9 a c c u a r c y t p c i t n c i t p c i f p c i t n c i f n c i 10 f 1 s c o r e 2 p r e c i s i o n r e c a l l p r e c i s i o n r e c a l l 11 w e i g h t e d f 1 s c o r e 2 p r e c i s i o n r e c a l l p r e c i s i o n r e c a l l c i t o t a l d a t a 12 3 results 3 1 performance of the term weighting schemes for all categories five term weighting schemes were compared to evaluate their text classification performance with the five ml classifiers e g svm lr rf nb and xgboost in general the tf idf icf scheme had a higher weighted f1 score for the text classification of the entire environmental dataset whereas the bm25 scheme had the lowest performance when compared with the other term weighting schemes fig 3 the performance evaluation of the ml classifiers indicated that the lr classifier provided more reliable outcomes for environmental data classification when a suitable term weighting scheme was employed the svm classifier had a higher performance than the rf nb and xgboost classifiers however the selection of the term weighting scheme needs to be carefully considered to obtain a higher classification performance with the lr and svm classifiers because performance is significantly altered by the selected scheme the rf nb and xgboost had lower classification performances than lr and svm but provided similar evaluation results regardless of the term weighting schemes therefore one needs to be more vigilant in using svm and lr classifiers for text classification a more detailed analysis of the term weighting schemes with the ml classifiers indicates that the tf idf icf scheme with the lr classifier had the highest accuracy recall and f1 score with values of 0 75 0 76 and 0 73 respectively table 4 the highest precision with a value of 0 72 was obtained with the svm classifier the tf idf icf scheme consistently provided better performance with all ml classifiers except the xgboost classifier the tf igm scheme with the lr classifier provided the second best performance with an accuracy of 0 74 precision of 69 recall of 0 57 and an f1 score of 0 71 however the tf idf scheme generally performed better than the tf igm scheme when the other ml classifiers were applied the bm25 scheme had the worst performance with the svm lr and nb classifiers when compared with the other term weighting schemes however the bm25 scheme had a better performance relative to the tf scheme with the rf classifier and the tf idf icf scheme with the xgboost classifier in addition for the weighted f1 score the tf idf icf scheme had a higher value with the svm and lr classifiers and a lower value with the xgboost classifier the bm25 scheme with the svm classifier and the tf scheme with the lr classifier had the lowest weighted f1 scores in addition even though the overall weighted f1 scores of the rf and xgboost classifiers indicated lower performances when compared with the svm lr and nb classifiers they returned similar experimental performance results with all the term weighting schemes our results indicate that the svm and lr classifiers returned a higher classification performance for the environmental data and were highly sensitive with respect to performance variations when the bm25 and tf schemes were used conversely the rf nb and xgboost classifiers were less sensitive for the selection of term weighting schemes such that similar classification performances were estimated when the tf tf idf bm25 tf igm and tf idf icf schemes were applied to the environmental news articles therefore the selection of the ml classifier should be carefully considered when text classification of environmental data is performed especially using the bm25 and tf schemes 3 2 performance of the term weighting schemes and ml classifiers for each category after the performance evaluation for the entire environmental dataset the classification performance for the considered environmental category according to each term weighting scheme with svm lr rf nb and xgboost was compared using precision recall and the f1 score table 5 and fig 4 in the air section the tf idf icf scheme with the lr classifier obtained the highest precision of 0 69 recall of 0 71 and f1 score of 0 70 relative to the tf idf icf scheme with other ml classifiers and other term weighting schemes with ml classifiers the bm25 scheme with the svm classifier obtained the lowest precision of 0 18 recall of 0 09 and f1 score of 0 12 this result indicates that the bm25 scheme with the svm classifier is not an effective method for the classification of environmental news articles in the air category however the bm25 scheme with the nb classifier had a higher f1 score for classifying the air related environmental category when compared with the other term weighting schemes with ml classifiers fig 4 a the tf scheme with the lr and rf classifiers predicted a lower f1 score for the text classification of the air category than the other term weighting schemes in the climate section tf idf icf obtained the highest precision of 0 90 with the lr classifier and the highest recall of 0 84 with the svm classifier when compared with the tf idf icf scheme with other ml classifiers and other term weighting schemes with classifiers whereas bm25 obtained the lowest precision of 0 49 with the svm classifier and the lowest recall of 0 72 with the lr classifier table 5 the tf idf scheme with the nb classifier obtained the highest f1 score of 0 83 whereas the bm25 scheme with the svm classifier obtained the lowest f1 score of 0 61 fig 4 b the tf idf and if idf icf schemes performed better than the other schemes regardless of the ml classifier the svm and lr classifiers had high variability according to the term weighting scheme in general the climate environmental issue was well predicted and analyzed for all term weighting schemes and ml classifiers achieving high precision recall and f1 score values relative to the other environmental issues i e air water and waste garbage this relative accuracy is because the climate related words in korean usually use proper nouns which facilitated the classification of text terms the water section had the highest precision 0 68 under the tf idf icf with nb classifier and the highest recall 0 76 under the tf idf icf with lr classifier relative to other term weighting schemes with ml classifiers respectively the bm25 scheme with the svm classifier returned the lowest precision of 0 12 and recall of 0 06 and the tf scheme with the lr classifier had a lower f1 score compared with the other term weighting schemes fig 4 c finally in the waste garbage section the tf igm scheme with nb classifier had the highest precision of 0 90 while the tf idf icf scheme with lr classifier had the highest recall of 0 83 from all term weighting schemes with ml classifiers the xgboost classifier had similar f1 scores for all term weighting schemes fig 4 d these results imply that the climate section was well classified from the environmental news articles regardless of the term weighting scheme or ml classifier the bm25 and tf schemes with the svm and lr classifiers showed a highly sensitive performance when classifying the air water and waste garbage categories finally results indicate that it was difficult to classify interconnected environmental issues to each category using bm25 and tf schemes in tandem with the svm and lr classifiers 4 discussion this study obtained interesting and differing results for text classification of environmental data in comparison to previous text classification outcomes related to the prediction performance of term weighting schemes and ml classifiers this study presents the following novel and noteworthy outcomes with respect to the use of term weighting schemes and ml classifiers for environmental text classification first the type of term weighting scheme and ml classifier matters the performance of the text classification for environmental information is highly related to the combination of the term weighting scheme and ml classifier the classification performance of the different term weighting schemes for each environmental category using five ml classifiers was evaluated the highest and lowest classification performances were for the tf idf icf and bm25 schemes respectively these results are with the svm classifier relative to results obtained using the other ml classifiers for each term weighting scheme this implies that the selection of the term weighting scheme must be carefully considered when using the svm classifier which has the highest sensitivity among classifiers in addition tf idf icf outperformed other term weighting schemes when used with the svm lr and rf classifiers all the term weighting schemes showed a similar performance with the rf and xgboost classifiers regarding performance with the ml classifiers the bm25 scheme showed greater variability than the tf tf idf tg igm and tf idf icf schemes these results indicate the importance of the selection of the term weighting schemes and ml classifiers when classifying environmental information when the nb rf and xgboost classifiers were used the classification performance was similar for all the term weighting schemes such that the estimated results were valid for environmental management applications even though the overall performance was lower than when using the other ml classifiers however the svm classifier showed contrasting performance results for text classification depending on the term weighting scheme the tf idf icf scheme with the svm classifier can provide more reliable classification information for environmental management to users and the bm25 scheme with the svm classifier may provide meaningless information because of its low accuracy second the environmental category matters the text classification performance of the climate category had the highest f1 score with similar values regardless of the term weighting scheme or ml classifier this is because the keywords for the climate section had clear meanings and included mostly proper nouns in korean which makes it easier to distinguish their term weighting solution regardless of the term weighting method however in the air waste garbage and water sections the f1 score changed depending on the type of term weighing scheme with the svm lg and nb classifiers because their environmental impacts were interconnected with each other for example the if idf icf scheme with the svm and lr classifiers had a high f1 score for the air and waste garbage sections however when the nb classifier was applied the bm25 and tf igm schemes had higher f1 scores for the water and waste garbage sections respectively these results indicate that an integrated ml classifier approach is needed to improve the classification performance for individual environmental categories that said the sizes of defined categories in this study were unbalanced future work should therefore be conducted to investigate how performance is impacted by unbalanced categories finally this study showed that language matters language differences greatly affect the text classification performance most term weighting schemes and ml classifiers have been developed based on english previous text classification results written in english indicated that the bm25 scheme has a higher performance than the tf idf scheme for news articles blog posts and research papers marwah and bell 2020 the tf igm scheme with the svm classifier has also been reported to have a better classification performance for news articles than the tf idf and if idf icf schemes dogan and uysal 2020 the classification performance of these schemes did not indicate starkly different results according to the ml classifier however jiang et al 2021 reported that the most effective combination of a term weighting method and classifiers is different depending on the language characteristics of english and chinese datasets our study indicated a similar tendency as that of jiang et al 2021 it is difficult to conclude which term weighting scheme and ml classifier constitute the best method for environmental text classification in written korean because korean has many variations on roots and compound nouns with spaces also limited studies have been conducted to compare the classification performance of environmental textual data for different languages additional studies are needed to develop an optimization method for term weighting schemes for written korean and to evaluate how korean and english words will affect the text classification performance when using the same information our results provide compelling evidence that the tf idf icf scheme with the svm and lr classifiers achieves better classification results of environmental datasets than the other term weighting schemes note that the experiments executed with the rf classifier showed a similar classification performance for all the term weighting schemes even if the estimated performance was less than those with the other classifiers this indicates that the computation of the svm and lr classifiers can achieve more reliable classification performances even though they are far more sensitive to the term weighting scheme 5 conclusions in this study we analyzed five term weighting schemes i e tf tf idf bm25 tf igm and tf idf icf in combination with five ml classifiers i e svm lr rf nb and xgboost to analyze textual big data consisting of environmental news articles from our analyses we identified the following three outcomes first the selection of the term weighting scheme and ml classifier matters the weighted f1 score which is the most important criterion for the text classification performance was used to evaluate the results for the term weighting schemes and ml classifiers for the entire environmental news dataset the tf idf icf scheme in combination with the five ml classifiers often offered better performance for the text classification of the entire environmental dataset than did other schemes conversely the lr classifier provided an appropriate selection for the entire environmental data classification when the optimal term weighting scheme was implemented in particular the implementation results for the term weighting schemes with the ml classifiers showed that the tf idf icf scheme with the lr classifier attained the highest weighted f1 score for the entire dataset followed by the tf igm scheme with the lr classifier and the tf idf scheme with the svm classifier the classification performance of the nb xgboost and rf classifiers remained consistent achieving similar weighted f1 scores which were lower than those achieved with the svm and lr classifiers regardless of the term weighting schemes second the environmental category matters the performance of each environmental category was compared for the five term weighting schemes with the five ml classifiers based on the f1 score the climate section outperformed all other sections for all term weighting schemes and ml classifiers the tf idf icf scheme using the svm and lg classifiers performed best for the classification of the water waste garbage and air sections the bm25 and tf igm schemes with the nb classifier are also promising approaches for classifying the air and waste garbage sections respectively the rf and xgboost classifiers presented similar f1 scores for all term weighting schemes in each environmental category third the language matters our study was conducted on environmental news article data written in korean the analysis results for the entire dataset and each environmental section indicated that the text classification performances varied according to the term weighting scheme and ml classifiers because korean has many variations on roots and compound nouns with spaces this study demonstrated that the selection of the term weighting scheme and ml classifier significantly affects the text classification performance when using human generated environmental big data additional studies are needed to explore how korean and english words affect the text classification performance when using the same information as well as nationwide comparative studies that apply term weighting schemes and classifiers with larger datasets this information will provide new insights concerning the text classification of environmental big data software and data availability name of software korean morpheme analysis description a python script for morpheme analysis of environmental news data in korea including the user s defined korean environmental dictionary and stop word text file developer and contact information jungjin kim kimjj82 seoultech ac kr program language python 3 7 system requirements linux python package numpy pandas matplotlib nltk xgboost keras rank bm25 textvec scikit learn collections konlpy csv availability korean morpheme analysis source codes and input news data can be freely downloaded from https drive google com drive folders 1fiodd5k swp1xdq4sa0hmbsbzrknj53f usp sharing declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments this research was supported by the basic science research program through the national research foundation of korea nrf funded by the ministry of education grant no nrf 2020r1a6a1a03042742 nrf 2021r1i1a1a01060891 and the ministry of science and ict grant no nrf 2021r1c1c1004179 
25530,widely used term weighting schemes and machine learning ml classifiers with default parameter settings were assessed for their performance when applied to environmental big data analysis five term weighting schemes term frequency tf tf inverse document frequency tf idf best match 25 bm25 tf inverse gravity moment tf igm and tf idf inverse class frequency tf idf icf and five different ml classifiers support vector machine svm naive bayes nb logistic regression lr random forest rf and extreme gradient boosting xgboost were tested the optimal text classification scheme and classifier were tf idf icf and lr respectively based on evaluation criteria their combination resulted in the best performance of all scheme and classifier combinations for the full environmental data analysis category classification performance differed according to the environmental section climate air water or waste garbage with the best performance being achieved for climate and the poorest for water this demonstrated the importance of selecting term weighting schemes and ml classifiers in human generated environmental big data analysis accordingly the study was designed to evaluate the text classification performance of various term weighting schemes for environmental big data using common ml algorithms i e support vector machine svm naive bayes nb logistic regression lr random forest rf and extreme gradient boosting xgboost news articles served as human generated environmental big data because such articles have well structured and filtered data compared to social media and blogs they therefore provide reliable objective information when applying term weighting schemes additionally news articles are commonly used as textual resources to evaluate the performance of term weighting schemes using term weighting schemes and ml algorithms in the analysis process the present study followed specific steps 1 using a web article crawling method digital environmental news articles from the past 11 years were downloaded from a korean news platform 2 articles were classified into four categories of major environmental issues i e climate air water and waste garbage 3 text mining preprocessing served to extract nouns from each document 4 meaningful nouns were defined using the term weighing algorithm under each term weighting scheme and 5 using several ml classifiers classification performance was evaluated fig 1 accordingly the present study demonstrates the significance of selecting the term weighting scheme with ml classifiers by comparing different term weighting schemes i e tf tf idf bm25 tf igm and term frequency inverse document frequency inverse class frequency tf idf icf this research was supported by the basic science research program through the national research foundation of korea nrf funded by the ministry of education grant no nrf 2020r1a6a1a03042742 nrf 2021r1i1a1a01060891 and the ministry of science and ict grant no nrf 2021r1c1c1004179 this research was supported by the basic science research program through the national research foundation of korea nrf funded by the ministry of education grant no nrf 2020r1a6a1a03042742 nrf 2021r1i1a1a01060891 and the ministry of science and ict grant no nrf 2021r1c1c1004179 0 https doi org 10 15223 policy 017 https doi org 10 15223 policy 037 https doi org 10 15223 policy 012 https doi org 10 15223 policy 029 https doi org 10 15223 policy 004 item s1364 8152 22 00236 5 s1364815222002365 1 s2 0 s1364815222002365 10 1016 j envsoft 2022 105536 271872 2022 10 04t10 15 39 535193z 2022 11 01 2022 11 30 1 s2 0 s1364815222002365 main pdf https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s1364815222002365 main application pdf 0857126805863aafaeeafef9ca275159 main pdf main pdf pdf true 3722144 main 11 1 s2 0 s1364815222002365 main 1 png https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s1364815222002365 preview image png 35ab82f5d217bb83a0a27bf98ef8538f main 1 png main 1 png png 61376 849 656 image web pdf 1 1 s2 0 s1364815222002365 gr4 jpg https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s1364815222002365 gr4 downsampled image jpeg 75d7e214409d7427a0c167945e00d5ab gr4 jpg gr4 gr4 jpg jpg 189894 452 691 image downsampled 1 s2 0 s1364815222002365 gr3 jpg https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s1364815222002365 gr3 downsampled image jpeg e258c9ad32739313763e9c40e34591ff gr3 jpg gr3 gr3 jpg jpg 108540 298 535 image downsampled 1 s2 0 s1364815222002365 gr2 jpg https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s1364815222002365 gr2 downsampled image jpeg 037aa8bad5cd55ac27580ff73a15378e gr2 jpg gr2 gr2 jpg jpg 87914 250 535 image downsampled 1 s2 0 s1364815222002365 gr1 jpg https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s1364815222002365 gr1 downsampled image jpeg c708d693305de362d1177e045b6cd44e gr1 jpg gr1 gr1 jpg jpg 174595 475 535 image downsampled 1 s2 0 s1364815222002365 gr4 sml https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s1364815222002365 gr4 thumbnail image gif 9c64e55a92447ebec085b87c8021414b gr4 sml gr4 gr4 sml sml 86852 143 219 image thumbnail 1 s2 0 s1364815222002365 gr3 sml https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s1364815222002365 gr3 thumbnail image gif f8c1aa71b55c9c9debc397fb915a7d99 gr3 sml gr3 gr3 sml sml 75159 122 219 image thumbnail 1 s2 0 s1364815222002365 gr2 sml https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s1364815222002365 gr2 thumbnail image gif 65b41e9e81e6e466e07c3aae3321ee00 gr2 sml gr2 gr2 sml sml 69370 102 219 image thumbnail 1 s2 0 s1364815222002365 gr1 sml https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s1364815222002365 gr1 thumbnail image gif ae0c70480c769b771c60f05f90e5bebc gr1 sml gr1 gr1 sml sml 82436 164 185 image thumbnail 1 s2 0 s1364815222002365 gr4 lrg jpg https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s1364815222002365 highres image jpeg 4c05e92d197275937ffa9d4597ba2c5c gr4 lrg jpg gr4 gr4 lrg jpg jpg 1060103 2002 3059 image high res 1 s2 0 s1364815222002365 gr3 lrg jpg https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s1364815222002365 highres image jpeg e401e8657eaa4691ba0213a953eb899c gr3 lrg jpg gr3 gr3 lrg jpg jpg 338235 1321 2370 image high res 1 s2 0 s1364815222002365 gr2 lrg jpg https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s1364815222002365 highres image jpeg 5e3cc5f27de24e9fe4738621baa12eef gr2 lrg jpg gr2 gr2 lrg jpg jpg 214025 1107 2371 image high res 1 s2 0 s1364815222002365 gr1 lrg jpg https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s1364815222002365 highres image jpeg eeebb94786054c246274981e064baca8 gr1 lrg jpg gr1 gr1 lrg jpg jpg 681060 2103 2370 image high res 1 s2 0 s1364815222002365 si23 svg https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s1364815222002365 image svg xml 628ea3dc0ee36820b61d2acf43fa9fba si23 svg si23 si23 svg svg 98549 altimg 1 s2 0 s1364815222002365 si19 svg https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s1364815222002365 image svg xml 5928f2ab62fd495258b65ebb19d9d4fe si19 svg si19 si19 svg svg 15608 altimg 1 s2 0 s1364815222002365 si14 svg https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s1364815222002365 image svg xml 85796a300ab5c630a142415000aad0be si14 svg si14 si14 svg svg 31536 altimg 1 s2 0 s1364815222002365 si9 svg https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s1364815222002365 image svg xml c96dbb89d043a11227ddbf0fb86f0933 si9 svg si9 si9 svg svg 3907 altimg 1 s2 0 s1364815222002365 si16 svg https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s1364815222002365 image svg xml d786a35ea45ca6a9eaaccb7d7f9fe031 si16 svg si16 si16 svg svg 15626 altimg 1 s2 0 s1364815222002365 si7 svg https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s1364815222002365 image svg xml 3750e4c0c87ba7f505a9f9fe6e76cbef si7 svg si7 si7 svg svg 105194 altimg 1 s2 0 s1364815222002365 si15 svg https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s1364815222002365 image svg xml 68a3c4158180d723fc5da1b88a3edb46 si15 svg si15 si15 svg svg 51095 altimg 1 s2 0 s1364815222002365 si17 svg https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s1364815222002365 image svg xml 4c7e2ae313459acfb767ee9c3a02434f si17 svg si17 si17 svg svg 13781 altimg 1 s2 0 s1364815222002365 si2 svg https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s1364815222002365 image svg xml 74e5b33f74abf8508d1bd9706f048c93 si2 svg si2 si2 svg svg 30393 altimg 1 s2 0 s1364815222002365 si21 svg https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s1364815222002365 image svg xml d9feec93ca998a618a8005f8b740ccbe si21 svg si21 si21 svg svg 74205 altimg 1 s2 0 s1364815222002365 si22 svg https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s1364815222002365 image svg xml e10246815fbdb09a04e9ad9659beba81 si22 svg si22 si22 svg svg 82700 altimg 1 s2 0 s1364815222002365 si12 svg https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s1364815222002365 image svg xml 0ecd882c2d3dbbd4b6f669b6b9f107a2 si12 svg si12 si12 svg svg 20442 altimg 1 s2 0 s1364815222002365 si24 svg https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s1364815222002365 image svg xml 17db8be95700327020b1d9ed7a6f59d2 si24 svg si24 si24 svg svg 182190 altimg 1 s2 0 s1364815222002365 si6 svg https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s1364815222002365 image svg xml 27b738ac4db044b10aa9389a831dd072 si6 svg si6 si6 svg svg 22444 altimg 1 s2 0 s1364815222002365 si18 svg https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s1364815222002365 image svg xml 47f07e1d549540a4e4a970312840364c si18 svg si18 si18 svg svg 21859 altimg 1 s2 0 s1364815222002365 si3 svg https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s1364815222002365 image svg xml 34e9d4113051ae2034cc35d8c841cee8 si3 svg si3 si3 svg svg 9194 altimg 1 s2 0 s1364815222002365 si20 svg https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s1364815222002365 image svg xml 5a90c36a3a39eac9329865649d3e9d30 si20 svg si20 si20 svg svg 78936 altimg 1 s2 0 s1364815222002365 si4 svg https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s1364815222002365 image svg xml 1f739bf8667a685d28ffc40f41d3a5a7 si4 svg si4 si4 svg svg 13886 altimg 1 s2 0 s1364815222002365 si1 svg https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s1364815222002365 image svg xml c4fb246768c531ab1e4a731238ca7b72 si1 svg si1 si1 svg svg 85658 altimg 1 s2 0 s1364815222002365 si5 svg https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s1364815222002365 image svg xml 4d6c54d37721c83017eb2ead1c308e10 si5 svg si5 si5 svg svg 99235 altimg 1 s2 0 s1364815222002365 si13 svg https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s1364815222002365 image svg xml 094f132ad5490aeeda8b82dff357cdd4 si13 svg si13 si13 svg svg 116353 altimg 1 s2 0 s1364815222002365 si8 svg https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s1364815222002365 image svg xml 5cdd24238a749822624650b469a6174c si8 svg si8 si8 svg svg 13569 altimg 1 s2 0 s1364815222002365 si11 svg https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s1364815222002365 image svg xml 6535c83ca1b342d4979d068038c5bd95 si11 svg si11 si11 svg svg 115121 altimg 1 s2 0 s1364815222002365 si10 svg https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s1364815222002365 image svg xml 9fec1b537a6d05503d98315a3b9cfaed si10 svg si10 si10 svg svg 4346 altimg 1 s2 0 s1364815222002365 am pdf https s3 eu west 1 amazonaws com prod ucs content store eu west content egi 108qf2q15d8 main application pdf b8c55019a1ee4c8852204590faa9217a am pdf am am pdf pdf false 1128005 aam pdf enso 105536 105536 s1364 8152 22 00236 5 10 1016 j envsoft 2022 105536 elsevier ltd fig 1 schematic diagram of this study fig 1 fig 2 data distribution of each category for training and validation datasets fig 2 fig 3 weighted f1 score for each term weighting scheme in the different categories for the validation dataset using the five machine learning ml classifiers fig 3 fig 4 f1 scores of each term weighting scheme in different categories for the validation dataset using the five ml classifiers fig 4 table 1 article classification for environmental news by keyword for each environmental category in seoul south korea table 1 categories keywords training dataset article validation dataset article total articles air air pollution air quality fine dust yellow sand exhaust smog air ozone sulfur monoxide nitrogen oxide sulfur oxide acid rain visibility dioxin atmosphere discharge specific air pollution volatile organic compounds diesel particle air in the room flue gas clean fuel incinerator benzene background concentration dust 15 994 1 978 012 words 5 331 498 212 words 21 325 2 476 224 words climate climate greenhouse gases global warming carbon dioxide carbon heavy snow heavy rain flood drought typhon sea level heat island abnormally high temperature localized heavy rain tidal wave tsunami desertification green growth 8 511 1 330 164 words 2 838 337 769 words 11 349 1 667 933 words water algal bloom water quality wastewater four main rivers river pollution ecological stream clean water pure water aquatic ecology river ecology water environment buffer strip tn fish mortality tp sewer manure septic tank water saving rainwater management inundation water supply fry discharge stream restoration river restoration wetland shoals of fish stream stench unauthorized discharge dirking water water deficit water saving water waste water bill aquanomics water reuse recycle water water treatment seawater desalination 5 490 852 124 words 1 372 206 861 words 6 862 1 058 985 words waste garbage garbage waste recycling garbage collection disposal system landfill food waste leachate illegal trash dispose of plastic waste ocean disposal waste resources waste heat upcycling abandoned metal material flow analysis leaching test resource recirculation waste electric and electronic end of life vehicle 3 717 700 335 words 1 240 179 358 words 4 957 879 693 words total number of downloaded news articles 33 712 4 867 037 words 10 781 1 215 798words 44 493 6 082 835 words table 2 hyperparameters of machine learning ml classifiers used in this study table 2 ml classifiers hyperparameters definition parameters svm kernel type specifies the kernel type to use in the algorithm rbf c regularization parameter 1 0 degree degree of the polynomial kernel function 3 lr solver algorithm to use in the optimization problem saga penalty specify the norm of the penalty elasticnet multi class define the multiclass classification multinomial nb alpha additive smoothing parameter 1 0 rf n estimators the number of tresses in the forest 100 min samples split the minimum number of samples required to split an intern node 2 max faatures the number of features to consider when looking for the best split auto xgboot booster the type of model gbtree max depth the depth of tree 6 sampling method the method to use to sample the training instances uniform table 3 contingency table for the target category ci table 3 confusion matrix predicted c i yes predicted c i no actual c i e g climate yes true positive tp a false negative fn b actual c i no false positive fp c true negative tn d a tp the number of documents that belong to actual ci resulting in predicted ci b fn the number of documents that belong to actual ci not resulting in predicted ci c fp the number of documents that do not belong to actual ci resulting in predicted ci d tn the number of documents that do not belong to actual ci not resulting in predicted ci table 4 evaluation results for each term weighting scheme with the five machine learning ml classifiers using the validation dataset table 4 statistics ml classifiers term weighting schemes tf tf idf bm25 tf igm tf idf icf accuracy svm 0 57 0 72 0 41 0 62 0 74 lr 0 69 0 72 0 61 0 74 0 75 rf 0 62 0 62 0 63 0 62 0 63 nb 0 69 0 68 0 67 0 60 0 68 xgboost 0 65 0 66 0 65 0 65 0 65 precision svm 0 51 0 68 0 24 0 57 0 72 lr 0 64 0 67 0 55 0 69 0 71 rf 0 57 0 57 0 58 0 57 0 58 nb 0 64 0 64 0 63 0 59 0 65 xgboost 0 60 0 61 0 60 0 60 0 61 recall svm 0 46 0 67 0 24 0 57 0 69 lr 0 68 0 72 0 57 0 75 0 76 rf 0 57 0 57 0 58 0 57 0 58 nb 0 67 0 66 0 67 0 59 0 65 xgboost 0 61 0 62 0 60 0 61 0 61 f1 score svm 0 48 0 68 0 22 0 57 0 70 lr 0 65 0 69 0 59 0 71 0 73 rf 0 57 0 57 0 58 0 57 0 58 nb 0 60 0 64 0 63 0 59 0 64 xgboost 0 61 0 62 0 60 0 60 0 61 note svm indicates support vector machine nb indicates naive bayes lr indicates logistic regression rf indicates random forest xgboost indicates extreme gradient boosting tf indicates term frequency tf idf indicates term frequency inverse document frequency bm25 indicates best match 25 tf igm indicates term frequency inverse gravity moment and tf idf icf indicates term frequency inverse document frequency inverse class frequency table 5 evaluation results of the precision and recall of each term weighting scheme in different categories for the validation dataset using the five ml classifiers table 5 ml classifiers term weighting schemes precision recall air climate water waste garbage air climate water waste garbage svm tf 0 49 0 63 0 43 0 50 0 43 0 77 0 33 0 33 tf idf 0 63 0 79 0 64 0 67 0 61 0 83 0 61 0 63 bm25 0 18 0 49 0 12 0 16 0 09 0 78 0 06 0 05 tf igm 0 52 0 74 0 46 0 56 0 51 0 75 0 45 0 56 tf idf icf 0 66 0 79 0 67 0 73 0 66 0 84 0 60 0 66 lr tf 0 63 0 86 0 56 0 50 0 63 0 73 0 66 0 70 tf idf 0 65 0 88 0 60 0 56 0 65 0 75 0 72 0 77 bm25 0 50 0 78 0 46 0 46 0 51 0 72 0 51 0 53 tf igm 0 68 0 89 0 61 0 59 0 68 0 76 0 73 0 81 tf idf icf 0 69 0 90 0 64 0 62 0 71 0 76 0 76 0 83 rf tf 0 50 0 74 0 47 0 58 0 50 0 75 0 46 0 58 tf idf 0 50 0 73 0 49 0 58 0 50 0 75 0 46 0 58 bm25 0 51 0 74 0 49 0 59 0 52 0 75 0 47 0 57 tf igm 0 52 0 74 0 47 0 56 0 52 0 75 0 45 0 57 tf idf icf 0 52 0 73 0 50 0 59 0 52 0 75 0 46 0 58 nb tf 0 53 0 75 0 41 0 44 0 60 0 79 0 59 0 59 tf idf 0 77 0 61 0 50 0 74 0 78 0 60 0 57 0 61 bm25 0 74 0 57 0 57 0 79 0 78 0 59 0 58 0 58 tf igm 0 71 0 44 0 32 0 90 0 75 0 53 0 41 0 44 tf idf icf 0 79 0 58 0 68 0 55 0 77 0 64 0 48 0 72 xgboost tf 0 77 0 55 0 52 0 57 0 76 0 55 0 51 0 61 tf idf 0 77 0 54 0 53 0 61 0 77 0 55 0 53 0 62 bm25 0 76 0 53 0 52 0 59 0 76 0 54 0 51 0 60 tf igm 0 76 0 55 0 51 0 57 0 76 0 54 0 51 0 60 tf idf icf 0 75 0 55 0 53 0 60 0 75 0 54 0 51 0 62 comparative study of term weighting schemes for environmental big data using machine learning jungjin kim a han ul kim b jan adamowski c shadi hatami c hanseok jeong a d a institute of environmental technology seoul national university of science and technology seoul 01811 republic of korea institute of environmental technology seoul national university of science and technology seoul 01811 republic of korea institute of environmental technology seoul national university of science and technology seoul 01811 republic of korea b department of applied artificial intelligence seoul national university of science and technology seoul 01811 republic of korea department of applied artificial intelligence seoul national university of science and technology seoul 01811 republic of korea department of applied artificial intelligence seoul national university of science and technology seoul 01811 republic of korea c department of bioresource engineering mcgill university ste anne de bellevue quebec canada department of bioresource engineering mcgill university ste anne de bellevue quebec canada department of bioresource engineering mcgill university ste anne de bellevue quebec canada d department of environmental engineering seoul national university of science and technology seoul 01811 republic of korea department of environmental engineering seoul national university of science and technology seoul 01811 republic of korea department of environmental engineering seoul national university of science and technology seoul 01811 republic of korea corresponding author 120 1 chungun hall 232 gongneung ro nowon gu seoul 01811 republic of korea 120 1 chungun hall 232 gongneung ro nowon gu seoul 01811 republic of korea widely used term weighting schemes and machine learning ml classifiers with default parameter settings were assessed for their performance when applied to environmental big data analysis five term weighting schemes term frequency tf tf inverse document frequency tf idf best match 25 bm25 tf inverse gravity moment tf igm and tf idf inverse class frequency tf idf icf and five different ml classifiers support vector machine svm naive bayes nb logistic regression lr random forest rf and extreme gradient boosting xgboost were tested the optimal text classification scheme and classifier were tf idf icf and lr respectively based on evaluation criteria their combination resulted in the best performance of all scheme and classifier combinations for the full environmental data analysis category classification performance differed according to the environmental section climate air water or waste garbage with the best performance being achieved for climate and the poorest for water this demonstrated the importance of selecting term weighting schemes and ml classifiers in human generated environmental big data analysis keywords text classification environmental digital news term weighting schemes feature selection data availability data will be made available on request 1 introduction the explosion of machine and human generated data as well as advances in computing technology have led to the current era of big data big data and its analysis have been widely applied in various fields including health care dash et al 2019 marketing lies 2019 agriculture xiao et al 2021 banking hung et al 2020 administration andrews 2019 and education khan and alqahtani 2020 to identify hidden patterns unknown correlations and user trends decision makers and researchers can enhance the benefits of their actions increase transparency and allow for better results and decisions by using valuable information drawn from big data analytics in particular big data is increasingly being called upon in environmental management machine generated remote sensing data provide real time information concerning various observations allowing for better representations of the environment human generated data offer new possibilities for applications in environmental management for example roby et al 2018 developed the articulate tool to collate material from the news media addressing a topic of interest in the field of environmental issues and evaluate the most important events related to that topic morss et al 2017 recently demonstrated how hazardous weather could be predicted using twitter datasets similarly kryvasheyeu et al 2016 examined the relationship between the real and perceived threats for disaster damage using twitter s message stream in another interesting application chen et al 2016 examined the frequency of articles on floods and wetlands from the new york times to validate a sociological model and better understand human and water interactions despite these promising case studies extracting valuable information from this surge of data to make informed decisions remains a great challenge human generated data is very difficult to interpret because of its unstructured nature and improved data processing techniques are therefore needed to facilitate the widespread use of human generated data in environmental management text mining is a key technique used to analyze collections of textual human generated data this technique captures useful data by uncovering textual information within unstructured data by identifying hidden patterns correlations and other insights using natural language processing nlp many studies have applied text mining techniques to extract meaningful information from online datasets deshmukh and phalnikar 2018 jiao et al 2021 li and liu 2018 li et al 2017 mendez et al 2019 rivera et al 2014 term weighting schemes present another essential text mining process for extracting and classifying environmental information from human generated data sources such schemes quantify terms in a document and employ features such as the frequency of stemmed words to define the importance of certain words in the document and corpus el khair 2009 kim and kim 2016 sabbah et al 2017 recent studies have supported the utility of term weighted schemes for various elements of data analysis chum et al 2008 reported these methods to be effective in text classification and duplicate image detection as well as for information retrieval additionally domeniconi et al 2014 showed that applying an appropriate term weighting scheme for text categorization resulted in a considerable boost in classification effectiveness to reach informed decisions in environmental management contexts decision makers need to extract and then prioritize information from human generated data using term weighting schemes however no comprehensive studies have been conducted to explore the performance of various term weighting schemes in environmental big data analysis several term weighting approaches have been used to derive the frequency and distribution of words in digital textual data documents domeniconi et al 2014 the two types of term weighting methods are unsupervised and supervised in the former traditional weighting methods are based on term distribution within documents while they are effective in information retrieval they do not consider document distribution in training documents usually applied to evaluate the importance of a term in a document of a specific class supervised term weighting schemes in contrast use the membership of training documents to generate categories however these schemes are limited to the classification of two classes as they only consider the distribution of terms in two classes term frequency tf and term frequency inverse document frequency tf idf are standard term weighting approaches commonly used for text classification according to beel et al 2017 83 of text based research papers reported using the tf idf scheme modified term weighting schemes such as best match 25 bm25 term frequency inverse gravity moment tf igm modified distinguishing feature selector based on term frequency dfs based tf modified distinguishing feature selector based on modified term frequency mdfs based mtf deep feature weighting dfw and lm dirichlet were developed to improve prediction performance cer et al 2018 chen et al 2021a 2021b jiang et al 2016 to assign appropriate weights to each term in a document or corpus using different term metrics various term weighting schemes have been applied to news articles according to text type corpus size and types of queries however no consensus has emerged regarding the optimal method to classify environmental issues from digital environmental news articles because the environmental category varies according to the application of the term weighting scheme several term weighing schemes with machine learning ml classifiers need to be tested to evaluate their classification performance on environmental data accordingly the study was designed to evaluate the text classification performance of various term weighting schemes for environmental big data using common ml algorithms i e support vector machine svm naive bayes nb logistic regression lr random forest rf and extreme gradient boosting xgboost news articles served as human generated environmental big data because such articles have well structured and filtered data compared to social media and blogs they therefore provide reliable objective information when applying term weighting schemes additionally news articles are commonly used as textual resources to evaluate the performance of term weighting schemes using term weighting schemes and ml algorithms in the analysis process the present study followed specific steps 1 using a web article crawling method digital environmental news articles from the past 11 years were downloaded from a korean news platform 2 articles were classified into four categories of major environmental issues i e climate air water and waste garbage 3 text mining preprocessing served to extract nouns from each document 4 meaningful nouns were defined using the term weighing algorithm under each term weighting scheme and 5 using several ml classifiers classification performance was evaluated fig 1 accordingly the present study demonstrates the significance of selecting the term weighting scheme with ml classifiers by comparing different term weighting schemes i e tf tf idf bm25 tf igm and term frequency inverse document frequency inverse class frequency tf idf icf 2 materials and methods 2 1 experimental setup 2 1 1 data collection online newspapers provide long searchable and in depth representations of topics reported on by the media accordingly this study used digital environmental news information over a period of 11 years january 1 2010 december 31 2020 related to the seoul area in south korea the naver korean portal site was used to collect digital data because it provides all news articles published by mainstream media and internet news publishers in the country a web crawler technique was used to download a total of 44 493 digital news articles related to environmental topics from the portal site the web crawler was programmed with a script to automatically retrieve web page information and insert it into a local repository using a python program additional detailed information concerning the web crawler process can be found in kausar et al 2013 starting with an initial set of urls as seed urls the web crawler downloaded the web pages for the seed urls and then extracted new links present in the downloaded pages the extracted web pages were stored in a defined storage folder so that they could be retrieved later if necessary after the web crawling process the downloaded digital news was then classified into four categories i e climate air water and waste garbage based on the search keywords representing each environmental condition table 1 and fig 2 the entire dataset was divided into 11 349 climate related 21 325 air related 6 862 water related and 4 957 waster garbage related news articles 2 1 2 text mining preprocessing korean language characteristics include many variations on roots the importance of distinguishing postposition and editing and compound nouns with spaces these characteristics make natural language processing more challenging in korean than in english the korean natural language konlpy package park and cho 2014 a natural language text mining package in python was used to phrase the korean sentences a potential term was extracted via the elimination of stop words stemming morphological analysis and part of speech pos tagging the process of stop word elimination removed special characters punctuation marks numbers advertising additional white spaces and unnecessary words for example names news corporations url links and city names because the meaning of nouns changes depending on the combination of korean characters a user dictionary for proper nouns related to environmental issues was created and applied to recognize environmental terms in the noun extraction process finally meaningful nouns were obtained to predict the news categories using konlpy under alternative term weighting schemes and ml classifiers 2 1 3 training and validation design the word vector sparse matrix space from the digital environmental news data was extracted using the term weighting schemes to generate a feature vector space for each scheme every text vector of each sample was formed and provided to the ml classifiers for training the total sample data were divided into a training set 75 of the total data and a validation set 25 of the total data stratified random sampling with stratification on the target variable was used to include the comparable target variable distribution in the training and validation sets five ml classifiers were trained with each training sample dataset from the different term weighting schemes after the training was completed the validation dataset was used to analyze the performances of the five classifiers and five term weighting schemes then their effectiveness was evaluated for the classification of environmental issues 2 2 term weighting schemes feature selection using a term weighting scheme is the preprocessing step in text classification that assigns appropriate weights to each term in all documents this step is significant to improve the efficiency of an ml classifier since it highlights the most discriminatory term in each class alsmadi and hoon 2019 several term weighting techniques have been proposed for text classification in this study the widely used term weighting schemes tf tf idf tf igm tf idf icf and bm25 were used to assign the weight to each word in the news articles under each environmental category and applied as baseline approaches to compare their performances 2 2 1 term frequency tf tf is the simplest and most used term weighting scheme it is a local weighting scheme because it counts how many times a term is present in a document tf assumes that the more frequently a term is in a document the more essential that term is it can be formulated as follows sanz et al 2014 t f t i d k t f t i d k i 1 n t f t i d k 1 where n is the number of chosen terms and t f t i d k is the tf of the term t i in the document d k 2 2 2 term frequency inverse document frequency tf idf tf idf is one of the most popular term weighting schemes for text classification it can evaluate the importance of a word or phrase in the text in a document chen et al 2016 tf idf assumes that a term that occurs less frequently in the corpus is more important in the document the tf idf value of a term t in document d is the dot product of the tf and idf of that term tf indicates the occurrence of related terms and idf is the collected frequency factor values of the related terms for the entire document the tf idf weighting formula is as follows sanz et al 2014 tf idf t i t f t i d k log n d f t i 2 where n is the total number of documents in the entire dataset and d f t i is the number of documents in which t i occurs 2 2 3 term frequency inverse gravity moment tf igm tf igm is a recently proposed term weighting scheme it integrates the tf and igm of the terms as a collection frequency factor igm is a statistical model that has been adapted in term weighting schemes by chen et al 2016 this model estimates the interclass distribution of terms by considering their document frequencies and distinguishing powers tf igm can be calculated as chen et al 2016 tf igm t i t f t i d k 1 γ f i 1 r 1 c f i r r 3 where f i r is the class specific document frequency of t i and γ is a balance parameter ranging from 5 0 to 9 0 during the igm weighting process related document frequencies are sorted in descending order and multiplied by rank values expressed with r r 1 2 3 c 2 2 4 term frequency inverse document frequency inverse class frequency tf idf icf tf idf icf uses the class information of terms in its term weighting process inverse class frequency icf is added to the term frequency inverse document frequency tf idf information in the weighting strategy of this scheme ren and sohrab 2013 icf calculates the ratio of the number of documents and the total number of documents that contain the term t i tf idf icf does not use a globalization policy in terms of weighting because it estimates one weighting score for all terms tf idf icf can be formulated as ren and sohrab 2013 tf idf icf t i t f t i d k 1 log n d f t i 1 log c c f t i 4 here c f t i indicates the number of documents where the term t i occurs and c is the total number of documents in the entire dataset 2 2 5 best match 25 bm25 bm25 was developed using probabilistic theory and is a good performing term weighting scheme that incorporates the tf idf and length normalization of a given document garcia 2016 bm25 ranks documents according to their relevant results using a bag of words retrieval function it considers the entire length of the document under evaluation as well as the frequency of the query terms zhu 2016 the bm25 can be computed as garcia 2016 bm 25 t i f i j f i j k k 1 1 f i j f i j k 1 1 b b d l j a v e d l k 1 1 5 where k k 1 b 6 b 1 b b d l j a v e d l 7 here f i j is the frequency of the term i in the document j d l j is the document length of document j a v e d l is the average document length k is an attenuation factor b is the document length normalization function b is a normalization parameter and k 1 1 is a scaling factor 2 3 machine learning ml classifiers ml classifiers have recently been proposed for automatic text classification based on a training set of labeled documents ml classifiers assign predefined category labels to each document based on likelihood ml classifiers have been extensively applied and rapid progress has been achieved in several research fields using svm nb lr rf and xgboost in our study each ml classifier assigned predefined environmental category labels e g air climate water waste garbage to each article it recognized a pattern that recognized the relation among the keywords group and environmental category labels of the training data after the training was done ml classifiers forecasted the environmental category labels on the keywords group of validation datasets more detailed information concerning these classifiers is given below table 2 shows the hyperparameters that were used in this study most parameters of ml classifiers were set to the default values provided by the program setup based on recommendations from the literature and experience the default values of ml classifiers occasionally work well for specific datasets elgeldawi et al 2021 tran et al 2022 and lorena et al 2011 suggested the use of the default parameter values to allow a fair performance comparison of different ml classifiers therefore the optimization of parameters was not applied because the goal of our study was to compare the potential performance of the term weighting schemes and evaluate the applicability of the text mining processing in korean even with ml classifiers using default parameters 2 3 1 support vector machine svm svm is a powerful supervised learning function used to find the decision surface that maximizes the margin between data points of two classes its basic classification method is the structural risk minimization principle from computation learning vapnik 1991 and it can be divided into linear and nonlinear methods according to its kernel functions the learning process from a training set aims to find a straight hyperplane that distinguishes positive cases from negative cases the hyperplane with the maximum margin is the optimal line separating the maximized distance between the nearest data points of positive and negative classes in the svm the advantages of svm include the ability to handle extremely large feature spaces redundant features and high dimensional feature vectors therefore an aggressive feature selection method is not necessary when svm is applied several studies have indicated that svm achieves high performance and accurate classification analysis for text classification lee et al 2012 wang et al 2006 2 3 2 naive bayes nb nb is a formal classifier based on the bayes theorem with strong independence assumptions rennie et al 2003 it has been widely used for text classification because of its lower computational cost nb can be effectively trained in a supervised setting to determine classification parameters with a relatively small amount of training data due to the nature of the probability model this classifier has been proven to be a suitable and adaptive method in many complex real word classification applications aphinyanaphongs et al 2014 ur rahman and harding 2012 the advantage of nb is that it requires a small amount of training data to determine the classification parameters additionally its simplicity means that training computational efficiency is linear in both the number of instances and attributes 2 3 3 logistic regression lr lr is a regression analysis that predicts categorical outcomes using a certain predictor logistic functions can be used to find the probabilities of the possible outcomes under a supervised classification algorithm the supervised classification algorithm is used to classify each term in the categories according to their importance for text classification lr analyzes the coefficient for each term and estimates the class of the text in a document of word vectors by recognizing the vectors containing variables pranckevičius and marcinkevičius 2016 shah et al 2020 2 3 4 random forest rf developed by breima 2001 rf is an ensemble learning algorithm consisting of the aggregation of several decision trees it results in the reduction of the variable when compared to a single decision tree using a standard classification or regression tree and a decrease in gini impurity each tree is contracted depending on a bootstrap sample drawn randomly from the entire dataset breima 2001 only a given number of variables of the randomly selected terms are considered splitting candidates when creating each tree at each split the prediction rule is hard to determine with rf because of the substantial number of trees consequently rf is commonly referred to as a black box algorithm it is one of the best classification algorithms because it classifies many decision trees in training sets and provides the class outputs according to each tree 2 3 5 extreme gradient boosting xgboost the xgboost algorithm is an improved version of the gradient boosting decision tree it combines multiple decision trees with lower accuracy into a tool with higher accuracy chen and guestrin 2016 gradient descent in the generation of each tree is used in this algorithm where the direction of the minimum given objective function is integrated into xgboost depending on the tree generated in the previous step the prediction model is constructed by continuously decreasing the loss error via the interaction of multiple decision trees each decision tree splits the nodes according to the regression tree criteria a commonly used function for xgboost is the least squared log loss 2 4 performance evaluation the effectiveness of the ml classifiers and term weighting schemes was evaluated using performance metrics including accuracy precision recall and the f1 score the evaluations were conducted using a contingency table of calculations for the target category and its components based on table 3 precision is a measure of the correctly predicted positive cases from all identified documents that should be assigned to the target category recall is the proportion of the correctly predicted positive cases from all actual positive document cases accuracy is the proportion of all correctly predicted classes for the target category c i the f1 score is the average of precision and recall prasetyo 2014 which provides a better representation of the incorrectly predicted cases than the accuracy evaluation metric the weighted f1 score is the weight of the f1 score of each category divided by the amount of data in that class the weighted fi score estimates the f1 score for each category independently and then adds them together using a weight that depends on the number of true labels for each class precision recall accuracy the f1 score and the weighted f1 score were estimated as follows sokolova and lapalme 2009 p r e c i s i o n t p c i t p c i f p c i 8 r e c a l l t p c i t p c i f n c i 9 a c c u a r c y t p c i t n c i t p c i f p c i t n c i f n c i 10 f 1 s c o r e 2 p r e c i s i o n r e c a l l p r e c i s i o n r e c a l l 11 w e i g h t e d f 1 s c o r e 2 p r e c i s i o n r e c a l l p r e c i s i o n r e c a l l c i t o t a l d a t a 12 3 results 3 1 performance of the term weighting schemes for all categories five term weighting schemes were compared to evaluate their text classification performance with the five ml classifiers e g svm lr rf nb and xgboost in general the tf idf icf scheme had a higher weighted f1 score for the text classification of the entire environmental dataset whereas the bm25 scheme had the lowest performance when compared with the other term weighting schemes fig 3 the performance evaluation of the ml classifiers indicated that the lr classifier provided more reliable outcomes for environmental data classification when a suitable term weighting scheme was employed the svm classifier had a higher performance than the rf nb and xgboost classifiers however the selection of the term weighting scheme needs to be carefully considered to obtain a higher classification performance with the lr and svm classifiers because performance is significantly altered by the selected scheme the rf nb and xgboost had lower classification performances than lr and svm but provided similar evaluation results regardless of the term weighting schemes therefore one needs to be more vigilant in using svm and lr classifiers for text classification a more detailed analysis of the term weighting schemes with the ml classifiers indicates that the tf idf icf scheme with the lr classifier had the highest accuracy recall and f1 score with values of 0 75 0 76 and 0 73 respectively table 4 the highest precision with a value of 0 72 was obtained with the svm classifier the tf idf icf scheme consistently provided better performance with all ml classifiers except the xgboost classifier the tf igm scheme with the lr classifier provided the second best performance with an accuracy of 0 74 precision of 69 recall of 0 57 and an f1 score of 0 71 however the tf idf scheme generally performed better than the tf igm scheme when the other ml classifiers were applied the bm25 scheme had the worst performance with the svm lr and nb classifiers when compared with the other term weighting schemes however the bm25 scheme had a better performance relative to the tf scheme with the rf classifier and the tf idf icf scheme with the xgboost classifier in addition for the weighted f1 score the tf idf icf scheme had a higher value with the svm and lr classifiers and a lower value with the xgboost classifier the bm25 scheme with the svm classifier and the tf scheme with the lr classifier had the lowest weighted f1 scores in addition even though the overall weighted f1 scores of the rf and xgboost classifiers indicated lower performances when compared with the svm lr and nb classifiers they returned similar experimental performance results with all the term weighting schemes our results indicate that the svm and lr classifiers returned a higher classification performance for the environmental data and were highly sensitive with respect to performance variations when the bm25 and tf schemes were used conversely the rf nb and xgboost classifiers were less sensitive for the selection of term weighting schemes such that similar classification performances were estimated when the tf tf idf bm25 tf igm and tf idf icf schemes were applied to the environmental news articles therefore the selection of the ml classifier should be carefully considered when text classification of environmental data is performed especially using the bm25 and tf schemes 3 2 performance of the term weighting schemes and ml classifiers for each category after the performance evaluation for the entire environmental dataset the classification performance for the considered environmental category according to each term weighting scheme with svm lr rf nb and xgboost was compared using precision recall and the f1 score table 5 and fig 4 in the air section the tf idf icf scheme with the lr classifier obtained the highest precision of 0 69 recall of 0 71 and f1 score of 0 70 relative to the tf idf icf scheme with other ml classifiers and other term weighting schemes with ml classifiers the bm25 scheme with the svm classifier obtained the lowest precision of 0 18 recall of 0 09 and f1 score of 0 12 this result indicates that the bm25 scheme with the svm classifier is not an effective method for the classification of environmental news articles in the air category however the bm25 scheme with the nb classifier had a higher f1 score for classifying the air related environmental category when compared with the other term weighting schemes with ml classifiers fig 4 a the tf scheme with the lr and rf classifiers predicted a lower f1 score for the text classification of the air category than the other term weighting schemes in the climate section tf idf icf obtained the highest precision of 0 90 with the lr classifier and the highest recall of 0 84 with the svm classifier when compared with the tf idf icf scheme with other ml classifiers and other term weighting schemes with classifiers whereas bm25 obtained the lowest precision of 0 49 with the svm classifier and the lowest recall of 0 72 with the lr classifier table 5 the tf idf scheme with the nb classifier obtained the highest f1 score of 0 83 whereas the bm25 scheme with the svm classifier obtained the lowest f1 score of 0 61 fig 4 b the tf idf and if idf icf schemes performed better than the other schemes regardless of the ml classifier the svm and lr classifiers had high variability according to the term weighting scheme in general the climate environmental issue was well predicted and analyzed for all term weighting schemes and ml classifiers achieving high precision recall and f1 score values relative to the other environmental issues i e air water and waste garbage this relative accuracy is because the climate related words in korean usually use proper nouns which facilitated the classification of text terms the water section had the highest precision 0 68 under the tf idf icf with nb classifier and the highest recall 0 76 under the tf idf icf with lr classifier relative to other term weighting schemes with ml classifiers respectively the bm25 scheme with the svm classifier returned the lowest precision of 0 12 and recall of 0 06 and the tf scheme with the lr classifier had a lower f1 score compared with the other term weighting schemes fig 4 c finally in the waste garbage section the tf igm scheme with nb classifier had the highest precision of 0 90 while the tf idf icf scheme with lr classifier had the highest recall of 0 83 from all term weighting schemes with ml classifiers the xgboost classifier had similar f1 scores for all term weighting schemes fig 4 d these results imply that the climate section was well classified from the environmental news articles regardless of the term weighting scheme or ml classifier the bm25 and tf schemes with the svm and lr classifiers showed a highly sensitive performance when classifying the air water and waste garbage categories finally results indicate that it was difficult to classify interconnected environmental issues to each category using bm25 and tf schemes in tandem with the svm and lr classifiers 4 discussion this study obtained interesting and differing results for text classification of environmental data in comparison to previous text classification outcomes related to the prediction performance of term weighting schemes and ml classifiers this study presents the following novel and noteworthy outcomes with respect to the use of term weighting schemes and ml classifiers for environmental text classification first the type of term weighting scheme and ml classifier matters the performance of the text classification for environmental information is highly related to the combination of the term weighting scheme and ml classifier the classification performance of the different term weighting schemes for each environmental category using five ml classifiers was evaluated the highest and lowest classification performances were for the tf idf icf and bm25 schemes respectively these results are with the svm classifier relative to results obtained using the other ml classifiers for each term weighting scheme this implies that the selection of the term weighting scheme must be carefully considered when using the svm classifier which has the highest sensitivity among classifiers in addition tf idf icf outperformed other term weighting schemes when used with the svm lr and rf classifiers all the term weighting schemes showed a similar performance with the rf and xgboost classifiers regarding performance with the ml classifiers the bm25 scheme showed greater variability than the tf tf idf tg igm and tf idf icf schemes these results indicate the importance of the selection of the term weighting schemes and ml classifiers when classifying environmental information when the nb rf and xgboost classifiers were used the classification performance was similar for all the term weighting schemes such that the estimated results were valid for environmental management applications even though the overall performance was lower than when using the other ml classifiers however the svm classifier showed contrasting performance results for text classification depending on the term weighting scheme the tf idf icf scheme with the svm classifier can provide more reliable classification information for environmental management to users and the bm25 scheme with the svm classifier may provide meaningless information because of its low accuracy second the environmental category matters the text classification performance of the climate category had the highest f1 score with similar values regardless of the term weighting scheme or ml classifier this is because the keywords for the climate section had clear meanings and included mostly proper nouns in korean which makes it easier to distinguish their term weighting solution regardless of the term weighting method however in the air waste garbage and water sections the f1 score changed depending on the type of term weighing scheme with the svm lg and nb classifiers because their environmental impacts were interconnected with each other for example the if idf icf scheme with the svm and lr classifiers had a high f1 score for the air and waste garbage sections however when the nb classifier was applied the bm25 and tf igm schemes had higher f1 scores for the water and waste garbage sections respectively these results indicate that an integrated ml classifier approach is needed to improve the classification performance for individual environmental categories that said the sizes of defined categories in this study were unbalanced future work should therefore be conducted to investigate how performance is impacted by unbalanced categories finally this study showed that language matters language differences greatly affect the text classification performance most term weighting schemes and ml classifiers have been developed based on english previous text classification results written in english indicated that the bm25 scheme has a higher performance than the tf idf scheme for news articles blog posts and research papers marwah and bell 2020 the tf igm scheme with the svm classifier has also been reported to have a better classification performance for news articles than the tf idf and if idf icf schemes dogan and uysal 2020 the classification performance of these schemes did not indicate starkly different results according to the ml classifier however jiang et al 2021 reported that the most effective combination of a term weighting method and classifiers is different depending on the language characteristics of english and chinese datasets our study indicated a similar tendency as that of jiang et al 2021 it is difficult to conclude which term weighting scheme and ml classifier constitute the best method for environmental text classification in written korean because korean has many variations on roots and compound nouns with spaces also limited studies have been conducted to compare the classification performance of environmental textual data for different languages additional studies are needed to develop an optimization method for term weighting schemes for written korean and to evaluate how korean and english words will affect the text classification performance when using the same information our results provide compelling evidence that the tf idf icf scheme with the svm and lr classifiers achieves better classification results of environmental datasets than the other term weighting schemes note that the experiments executed with the rf classifier showed a similar classification performance for all the term weighting schemes even if the estimated performance was less than those with the other classifiers this indicates that the computation of the svm and lr classifiers can achieve more reliable classification performances even though they are far more sensitive to the term weighting scheme 5 conclusions in this study we analyzed five term weighting schemes i e tf tf idf bm25 tf igm and tf idf icf in combination with five ml classifiers i e svm lr rf nb and xgboost to analyze textual big data consisting of environmental news articles from our analyses we identified the following three outcomes first the selection of the term weighting scheme and ml classifier matters the weighted f1 score which is the most important criterion for the text classification performance was used to evaluate the results for the term weighting schemes and ml classifiers for the entire environmental news dataset the tf idf icf scheme in combination with the five ml classifiers often offered better performance for the text classification of the entire environmental dataset than did other schemes conversely the lr classifier provided an appropriate selection for the entire environmental data classification when the optimal term weighting scheme was implemented in particular the implementation results for the term weighting schemes with the ml classifiers showed that the tf idf icf scheme with the lr classifier attained the highest weighted f1 score for the entire dataset followed by the tf igm scheme with the lr classifier and the tf idf scheme with the svm classifier the classification performance of the nb xgboost and rf classifiers remained consistent achieving similar weighted f1 scores which were lower than those achieved with the svm and lr classifiers regardless of the term weighting schemes second the environmental category matters the performance of each environmental category was compared for the five term weighting schemes with the five ml classifiers based on the f1 score the climate section outperformed all other sections for all term weighting schemes and ml classifiers the tf idf icf scheme using the svm and lg classifiers performed best for the classification of the water waste garbage and air sections the bm25 and tf igm schemes with the nb classifier are also promising approaches for classifying the air and waste garbage sections respectively the rf and xgboost classifiers presented similar f1 scores for all term weighting schemes in each environmental category third the language matters our study was conducted on environmental news article data written in korean the analysis results for the entire dataset and each environmental section indicated that the text classification performances varied according to the term weighting scheme and ml classifiers because korean has many variations on roots and compound nouns with spaces this study demonstrated that the selection of the term weighting scheme and ml classifier significantly affects the text classification performance when using human generated environmental big data additional studies are needed to explore how korean and english words affect the text classification performance when using the same information as well as nationwide comparative studies that apply term weighting schemes and classifiers with larger datasets this information will provide new insights concerning the text classification of environmental big data software and data availability name of software korean morpheme analysis description a python script for morpheme analysis of environmental news data in korea including the user s defined korean environmental dictionary and stop word text file developer and contact information jungjin kim kimjj82 seoultech ac kr program language python 3 7 system requirements linux python package numpy pandas matplotlib nltk xgboost keras rank bm25 textvec scikit learn collections konlpy csv availability korean morpheme analysis source codes and input news data can be freely downloaded from https drive google com drive folders 1fiodd5k swp1xdq4sa0hmbsbzrknj53f usp sharing declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments this research was supported by the basic science research program through the national research foundation of korea nrf funded by the ministry of education grant no nrf 2020r1a6a1a03042742 nrf 2021r1i1a1a01060891 and the ministry of science and ict grant no nrf 2021r1c1c1004179 
25531,urban flooding has become an increasingly frequent and fatal natural hazard and numerical modeling techniques play a vital role in its prediction and management we review urban flood numerical simulations by systematically summarizing the calculation methods of surface runoff drainage systems and coupled models following the review accuracy and computational efficiency are found to be the two key areas hindering the quality improvement of urban flood models so an investigation of the key trends in the improvement of model accuracy and computational efficiency is conducted it is found that the 1d 2d coupling model finite volume method unstructured meshing method and hybrid parallel computing applications are the most effective strategies furthermore the complex coupling of models and the lack of validation data are still crucial challenges in the development of urban flood modeling this result can be used as a guideline for hydrologists in choosing the proper method of urban flood numerical simulation according to the task keywords urban flooding numerical simulation runoff model drainage system coupling model 1 introduction environmental changes have precipitated the frequently extreme weather that has occurred in recent decades climate change produces extreme rainfall which in turn leads to flooding li et al 2022 mou et al 2020 zhu et al 2020 increases in precipitation urbanization and changes in topographic characteristics have led to a sharp rise in the occurrence of natural hazards urban floods soil erosion etc an ee s et al 2016 particularly increasing urbanization and urban populations have led to frequent urban flooding semadeni davies et al 2008 mu et al 2021 accordingly urban flood forecasts and early warning systems are crucial in preventing disaster damage and urban flood models are important to establish those wang et al 2022 cao et al 2022 therefore reviewing the numerical simulation technology for urban floods is critical for study of urban floods in the future the core science of urban flood numerical simulation consists of urban hydrology hydrodynamic mechanisms and their coupling salvadore et al 2015 generally an urban flood model comprises generating runoff confluence and flood inundation analysis modules fatichi et al 2016 according to the specific modeling mechanism employed each module adopts the hydrologic hydrodynamic or simplified method to conduct calculations liu et al 2015 the model development process of the urban flood simulation is divided into three stages the experience exploration stage proposing the classical hydrology theoretical methods the algorithm innovation stage innovating of simulation methods and algorithms for urban hydrology and hydrodynamic processes and the comprehensive integration stage integrating of new algorithms and new data pre and post processing technologies using the simulation platform bach et al 2014 fletcher et al 2013 wang and wang 2018 historical research has been conducted on many specific urban flood events su et al 2019 established an urban pluvial flood simulation model based on the diffusing wave approximation of shallow water equations moreover hasan et al 2019 integrated hydrology and hydraulic models to simulate the urban flood events in the aur river catchment in malaysia rabori and ghazavi 2018 applied the storm water management model to estimate urban flooding in semiarid areas zanjan city in northwest iran also to benefit from the advantages and characteristics of various models researchers have established two dimensional 2d coupled hydrodynamic models to study the storage and blocking effects of buildings in urban flood modeling huang et al 2014 similarly to overcome the drawbacks of each individual modeling approach zhang 2015 introduced a new integrated flood modeling tool in urban areas by coupling a hydrodynamic model with a hydrological model a review of these developed urban flood simulation technologies provides collective experience and offers insights that guides the development of future urban flood models despite active research in this field feng et al 2020 liu et al 2020 zhang et al 2022 the rapid and accurate modeling of urban flooding remains a major challenge for hydrology and hydraulics research owing to the complexity of the urban flooding process luo et al 2021 zhu et al 2022 therefore retrospective studies on urban flood models have been conducted to assist in developing suitable flood models researchers reviewed 45 laboratory based urban flooding studies to help computational modelers detect the most appropriate data sets for validating their numerical methods mignot et al 2019 furthermore nkwunonwo et al 2020 reviewed the current status of flood modeling for managing urban flood risk in developing countries and qi et al 2021 provided a systematic overview of applying urban flood modeling approaches from the perspective of urban flood strategy design sequences guo et al 2021 presented a comprehensive review of advanced urban flood models and the emerging approaches for predicting urban surface water flooding driven by intense rainfall bulti and abebe 2020 provided an overview of prevailing flood modeling approaches in view of their potentials and limitations for modeling pluvial flooding in urban settings however the general applicability of various methods for the numerical simulation of urban rainfall flow and flooding has not been reviewed and the associated model calculation methods have not been systematically summarized or compared this is an opportune time for a review of the literature as urban flood modeling methods have evolved rapidly in recent years and they are central to studies of flood mitigation in this study historical literature is used to analyze the construction modules of urban flood models and the hydrological and hydrodynamic numerical simulation methods of each module are summarized based on the review of urban flood numerical simulation and the development analysis of future models we provide some suggestions for improving the quality of future urban flood models the novelty of the present study lies in the comprehensive and systematic comparison of the calculation methods adopted by various models and the advantages disadvantages and applicability of the models are summarized considering the specific application scenarios of the methods and especially detailed ways to improve the model are proposed the research is the comprehensive methodological discussions that are aimed to support flood modeling in different regions the results offer an extensive overview of urban flood numerical simulation and will provide reference materials for national and local emergency departments and researchers 2 methodology fig 1 shows this study s research framework which includes three main parts systematic review research content and methods and discussion of future urban flood numerical simulations first we collected historical documents and literature and conducted a retrospective study on three aspects the urban surface runoff model urban drainage numerical simulation and the coupling model second we used the literature review and comparative analysis to describe the technology of urban flood numerical simulation in four aspects type selection calculation method applicability advantages and disadvantages third the direction of future developments and improvements in urban flood numerical simulation was discussed our study s implications were provided to facilitate the research of the urban flood model 3 overview of urban surface runoff model simulating the urban surface runoff yield and confluence process is a crucial part of establishing an urban flood model this section outlines the urban surface runoff model based on hydrologic modeling methods and discusses the development of the urban surface runoff model based on hydrodynamics 3 1 surface runoff yield and confluence model based on hydrologic method urban surface runoff yield is defined as the process of deducting losses from rainfall to form net rainfall rainfall losses include plant interception infiltration bottomland filling and evaporation but are primarily caused by infiltration note that the characteristics of urban surface runoff are different from those of general basins and the differences are reflected in the following the underlying surface of the city is intricate and the surface rainfall yield is not uniform the urban surface rainfall usually lasts for a short period of time and yields large the rainfall loss runoff yield and runoff retention time on urban surface are quite different from those in general watersheds owing to the complexity of the underlying surface of a city including impermeable and permeable areas the hydrologic methods applicable to calculating urban surface runoff yield include the runoff coefficient method soil conservation service curve number scs cn method mishra and singh 2003 infiltration curve deduction method saturation excess runoff method and φ indicator method there are various models for calculating runoff yield with advantages and disadvantages in current urban hydrology research the relative efficiency or accuracy of a particular method with respect to the others cannot be determined in a definite manner table 1 summarizes the characteristics and applicability of relevant runoff yield models to facilitate the researchers selection surface rainwater confluence is the process of runoff from the surface into a pipe network or channel as hydrologic methods employ a system based idea they simulate surface confluence by establishing the relationship between the input and output of the urban system common methods for achieving this include the reasoning formula isochrones instantaneous unit hydrograph linear reservoir and non linear reservoir methods kidd 1978 used the observation data of urban surface runoff in britain sweden and other countries to compare the methods of calculating surface confluence the results show that the simulation effect of the non linear reservoir method is better and the calculation effect of the instantaneous unit hydrograph method is poor table 1 defines their different applications when there are few urban surface runoff data the nonlinear reservoir method is more suitable for calculating surface confluence in areas with high precision requirements 3 2 numerical simulation of urban surface runoff based on hydrodynamic method specific flow rates and water depth distributions are obtained using the hydrodynamic method because it has a clear physical meaning accordingly the hydrodynamic method directly solves the saint venant equations or its simplified form based on the microscopic physical laws to obtain a more detailed urban surface runoff process than the hydrological method indeed the hydrodynamic method which better reflects the surface runoff of more complex terrain is suitable for areas without hydrological data and is more scalable the numerical models of urban surface runoff based on the hydrodynamic method include one dimensional 1d saint venant equations two dimensional 2d shallow water equations swes 1d 2d shallow water equations swes three dimensional 3d navier stokes equations boussinesq equations kinematic wave equations diffusion wave equations and the porosity model table 1 lists the application of these numerical models though the navier stokes equations best describe complex surface runoff solving these equations at the actual space time scale has been considered impossible meng et al 2019 they are employed in theoretical analyses rather than in actual engineering applications however the simplified forms of the navier stokes equations such as the 1d saint venant equations or 2d swes have been used in various flooding simulations su et al 2019 the 1d hydrodynamic model has obvious limitations in calculating the irregular regional confluence of urban surfaces in recent years urban surface runoff modeling is mostly based on 2d or 1d 2d swes however solving the 2d swes remains a complex project to reduce the computational complexity and running time the researchers simplified the 2d swes omitted the convective acceleration and used the kinematic wave or diffusion wave equations to simulate the flood runoff in urban areas based on the typical characteristics of urban surface runoff then we discuss numerical simulation studies of streets buildings and sudden flooding in detail 3 2 1 simulating complex street runoff the main cause of street flooding is the insufficient water intake capacity of the drainage system in recent years street network flood models have been considered steady or unsteady flow with single or multiple intersections there has been some in depth research on street surface flow mark et al 2004 bazin et al 2017 mignot et al 2013 the saint venant equations are applied to a drainage network or simply a street though the simulation results are inaccurate for irregular street geometries or when rainwater crosses a curb in such situations since the water flow is no longer in 1d the validity of the 1d flow field hypothesis will be uncertain mark et al 2004 for densely populated traffic intersections the fully 2d swes are required to realize high precision flood simulations in addition to including more accurate descriptions of hydraulic factors the 2d swes simulate local complex water flow phenomena indeed since overflow phenomena occur near street intersections and obstacles special research into these situations was required such research included the use of 2d swes to simulate the flooding of intersections with obstacles and sidewalks bazin et al 2017 and the use of a 2d numerical model with a simple turbulence model to simulate the effects of obstacles mignot et al 2013 and sidewalks bazin 2013 on crossroad flooding notably the latter models considered the turbulence effect and were found to be much simpler than an equivalent 3d model 3 2 2 surface runoff around buildings the buildings and building groups present in urban areas change the direction of rainwater runoff because of their water blocking effect which must be considered in any urban flood model commonly used methods for doing so include local friction building blocks porosity and mesh adjustment the local friction method of considering the building water blocking effect is realized by increasing the roughness coefficient of the building group mesh this method is easy to model can be executed relatively quickly and can be used with any type of mesh however it is less accurate than other methods for example when other meshes display a reasonable inundation depth the inundation depth of the building group mesh tends to be abnormally high this may be caused by floods flowing into and subsequently being stored in a building when the water level outside the building is higher than at the entrance to correct the inaccurate reduction of the surface water depth owing to this water storage effect huang et al 2014 proposed a manning s friction coefficient correction method that considered the water storage effect when the water level reached the building threshold in this approach buildings were conceptualized as a region with the same geometry as the mesh cell located at cell center s see fig 2 without affecting the flux through the boundary shared with its neighbor cells due to these complications the local friction methods were suitable for situations in which there were no detailed architectural geometry data building blocks provide a suitable replication of a building s blocking effect and do not reduce the volume of rain schubert and sanders 2012 however this method requires a fine mesh to generate each building shape as a block therefore it requires a high computational power making it costly and impractical guinot et al 2017 proposed the dual integral porosity dip model which includes several improvements to the integral porosity ip model in addition to simulating the blocking effect of the building the dip model achieves closure of the previously introduced ip model by defining an edge based model solution boundary of computational cell in addition to a cell based model solution interior of a computational cell transient momentum dissipation is used to introduce an important control when shaping the direction in which buildings within an array impede flow however the dip model has difficulty in determining the optimal parameter values over a large area thus reducing the application value of the resulting simulation nevertheless both the dip and ip models fail to some extent to reproduce the preferential directions of the flow field in the transient phase calibrating and applying shallow water porosity models remains in its infancy therefore further research is required on model development lee et al 2016 used high resolution data from light detection ranging lidar measurements to describe complex urban geographic features and developed a sophisticated urban flood model the model separated the building community mesh from the road network mesh and artificially adjusted the height of the two grids that induce flow from the building group to the road network and showed better simulation accuracy 3 2 3 sudden floods for many scales of flooding the 1d or 2d shallow water model is sufficient to provide an approximate simulation however sudden flood disasters such as dam break tsunamis and mountain torrents have complex mechanism processes such as vertical turbulence flow vortex flow and spiral flow flood models based on 1d 2d swes cannot approximate the motion of these types of floods their basic assumptions are hydrostatic pressure distribution and no vertical acceleration however sudden floods flow in three directions longitudinal lateral and vertical and vertical acceleration has a significant effect on the pressure distribution which renders the assumptions of the modeling equations invalid wang et al 2016 there have been four types of numerical simulation methods based on various equations developed by researchers for sudden flood disasters such as tsunamis 1 nonlinear shallow water equations such as the tunami n2 model most model comcot model etc 2 boussinesq equation such as funwave and cornell university s culwave 3 fully nonlinear potential flow theory 4 the navier stokes equations among these model types the first two are used to simulate tsunami inundation a review of recent relevant literature tehranirad et al 2011 pophet et al 2011 tissier et al 2011 indicates that the high order boussinesq water wave model has been found to provide better nonlinear and dispersion characteristics than the shallow water equations and is used in the dynamic study of sudden flooding disasters such as tsunamis the boussinesq equation describes short wave dynamics considering the dispersion and non hydrostatic pressure distribution of the wave therefore this equation has been used to simulate the evolution of waves on an uneven formation the interaction between waves and structures etc zerihun and fenton 2006 fuhrman et al 2005 also recent studies have shown that the boussinesq equation is superior to the shallow water equations in predicting surface oscillation modes and secondary wave characteristics chang et al 2014 however solving the boussinesq equation is time consuming considering that 3d numerical simulation of the flood evolution process has been an important subject to researchers methods for reducing the cost of calculating and optimizing the numerical model in each stage of sudden flooding should be the focus of future work however this review focuses on general scale urban flooding events caused by heavy rains and does not specifically address large scale sudden flooding in summary table 2 shows the selection rules of urban surface runoff modeling methods discussed in this section based on hydrological and hydrodynamics hydrological modeling is fast easy to implement and suitable for areas with limited data hydrodynamic modeling is stable accurate and versatile and is used in more detailed urban flood simulations the simplified method simplified from the hydrodynamic method runs faster than the hydrodynamic method and the simulation results are more accurate than the hydrological method which is suitable for large scale applications 4 urban drainage numerical simulation technology many urban areas are prone to flooding during summer rainstorms because the drainage networks in the older parts of the city are too small limiting the drainage capacity when heavy rain occurs the drainage pipe network is overloaded and the rainwater cannot be discharged in time causing flooding inside the city furthermore defects in the design of the drainage system causes rainwater from the pipe network to back up into the city during a rainstorm causing additional flooding therefore the study of urban drainage systems is essential in the numerical simulation of urban flooding 4 1 numerical simulation of pipeline flow in drainage network in general the designer of a drainage network uses the urban topographic map and planning map to select the optimal gravity flow drainage arrangement according to the terrain so that most of the urban rainwater discharge is based on gravity a gravity based drainage network employs open channel flow pressure flow free surface pressurized flow and mixed free surface pressure flow to move water depending on its arrangement general open channel flow is described by a 1d open channel flow model whereas a pressure flow model is required to simulate the pressure flow state the early use of pipe network convergence is based on the instantaneous unit hydrograph method and muskingum method but such methods are less accurate another type of pipeline network flow simulation method employs simplified saint venant equations to develop kinematic diffusive and inertial waves that provides more accurate results than hydrologic methods however this approach is not suitable for simulating mixed free surface pressure flow currently the combination of the 1d saint venant equations with the preissmann slot method is used for pipe network flow simulation an et al 2018 wang et al 2019 maranzoni et al 2015 in this approach the preissmann slot method was used to unify the open channel flow then the solution of the pressurized flow was inserted into the saint venant equations to simulate the transient free surface and the presence of pressure flow the problem that arose in this approach was that the preissmann slot scheme did not simulate the behavior of supercritical flows meanwhile the widths of different flow regimes could not be consistently described making it difficult to unify the flow equations in response to the above problems fan et al 2017 proposed a method of treating supercritical flows as subcritical flows an approximate simulation of supercritical flows was achieved by expanding the flow term to reduce the influence of the momentum equation liu et al 2016 proposed the use of an equivalent width to revise the equation for the narrow slot width in the preissmann slot method the objective was to convert the density change in the pipe network flows into the overcurrent equivalent area change introduce the equivalent area and convert the continuous equation form of the pressurized flows into the non pressure open channel form realizing the effective numerical simulation of the mixed free surface pressure flows beyond theory based model formulation practical implementation with the preissmann slot scheme has been a problematic issue in the past and has required significant research effort to resolve for example choosing an inappropriate slot width when implementing the preissmann slot scheme might lead to model instability vasconcelos et al 2006 therefore researchers proposed a new node calculation algorithm to model the drainage network li et al 2018 that evaluated the coupling conditions between pipelines and pipeline intersections to select a two component pressure approach tpa that supports pipeline flow calculation then the algorithm used the 2d swes to calculate the flow at the junction fig 3 shows a diagram of the tpa at a connection point between three pipes in fig 3 1 p1 and p2 were assumed to be inflow pipes while p3 was the outflow pipe based on the pipe layout the junction domain was approximated using an irregular grid cell as indicated in 2 the inflow from the two inlet pipes p1 and p2 was mixed in the junction and then discharged into the outflow pipe p3 the junction was idealized into a 2d domain in 3 and subsequently the junction flow was described using fully 2d swes to reinforce the strict conservation of mass and momentum across the interface between the pipes and the junctions the flow obtained from the tpa calculation was directly used to provide the interface flux on the 2d domain the 2d cell edge that was not connected to the pipe was set to the closed state by applying a no flow boundary condition this method had superior capabilities in complex transient flow simulation between the free surface of the pipeline and the stressed state and avoided the complex boundary conditions required by the conventional tpa model indeed by dynamically coupling the 1d tpa model with the 2d connection model through boundary conditions a new pipe network model that provided greater computational efficiency and stability for the modeling of large drainage pipe networks was provided furthermore urban drainage systems typically include river and drainage networks water can be moved from the drainage network to the river network through gravity or pressure drainage gravity drainage is based on the difference in height between the pipe network end and the water level of the river the flow form of pipeline based on pressure drainage is usually pumping that is regulated by a water pump the water first flows into a pipeline and is then pumped into the river by a pumping station the closer the target unit is to a pumping station the higher its drainage efficiency based on the above characteristics li et al 2016 proposed a radius modeling method to calculate pumping drainage flow this method determines the drainage coefficient by setting the pumping influence radius of the pumping station to obtain the drainage formula describing the water exchange from the pipeline to the river channel 4 2 numerical simulation of roof drainage systems methods for simulating building roof drainage were divided into two categories unorganized and organized drainage yan 2012 unorganized drainage known as free fall directly drained rainwater off the roof from the cornice to the ground where it then flowed into ditches storm drains or through surface runoff into the surrounding streets gardens and other areas organized drainage directed rainwater off the roof and into a drainage system where it was released onto the ground or to an underground drainage network wright et al 2006 also buildings were equipped with rainwater retention systems in which the rainwater was temporarily stored for later drainage vesuviano and stovin 2013 once the runoff generated on the roof exceeded the design capacity of the drainage system the excess runoff might remain on the roof in the retention system or spill to the ground as unorganized drainage arthur et al 2005 for example a flat building roof with retaining walls could function as a storage pool to limit runoff in excess of the capacity of the organized drainage system and the drainage flow equation was obtained accordingly when the water level on the roof rose to a level higher than the crest of the retaining wall the excess runoff flowed directly to the surface as unorganized drainage arthur and wright 2005 therefore the roof drainage capacity of a certain area was designed according to the type of roofs and the assumed rainfall intensity it could be assumed that the roof drainage capacity per square meter was similar for all buildings within a certain area chang et al 2015 green roofing offered an effective way to mitigate urban flooding by reducing the need for drainage capacity in a given area it consisted of three main components a vegetation layer a cultivation substrate layer and a water storage drainage layer placed on a waterproof membrane like rainfall retention using walls the green roof was designed to distribute storm runoff over a longer period of time however most of the rain was trapped on the roof and it released the retained precipitation back into the atmosphere through evaporation helping to relieve the problem of limited urban drainage capacity to model the drainage effect of green roofs soulis et al 2017 proposed a comprehensive model employing a physics based hydrus 1d numerical simulation model combined with a simple conceptual model on the one hand the conceptual model assumed that rainwater entered the green roof system from the surface of the substrate and then left via evaporation through the substrate plant transpiration or drainage layer runoff then the runoff from the green roof reaching the gutter was modeled as conventional runoff on the other hand the hydrus model considered the green roof to consist of two integrated reservoirs the first reservoir captured the rainfall interception in the vegetation layer and the second captured the infiltration process through the substrate and storage drainage layers this combined model required less computation and was applied to different locations and conditions 4 3 uncertainty related to operating conditions of urban drainage system to capture the sources of uncertainty in urban drainage systems most research has focused on input output data deletic et al 2012 as well as model structures and parameters while ignoring the uncertainties associated with specific operating conditions a common problem was the blockage at a drainage system inlet its probability depended on the type and location of the inlet season type of weather and applied cleaning regime for example a drainage inlet located near trees could be blocked by fallen leaves whereas a drainage inlet located in a business district or at a concavity in the terrain could be blocked by debris accordingly blockage was more common in late summer or late fall and during strong winds or heavy rains thus the problem of drainage inlet blocking was an important consideration when attempting to realistically model urban drainage systems leitão et al 2017 investigated simulating random drainage inlet blockage and proposed a three step method for generating flood hazard maps first identify drainage inlets that were prone to blockage by geospatial analysis second conduct a monte carlo simulation of the inlets that are prone to blockage followed by flood simulation third draw a flood disaster map based on the results of the simulation in addition to the problem of blocked drainage inlets uncertainty arose in the operating conditions of a drainage system because of other factors such as blocked drainage pipes or outlets further research into uncertainty analyses related to these factors should be conducted 5 coupled models model coupling is the process of computational interaction of different models or modules in time and space a 2d model is used to calculate rainwater runoff in urban surface areas such as squares residential areas and wide roads whereas a 1d model is used to calculate flows with distinct 1d features such as underground pipe networks and river drainage system therefore a complete urban groundwater runoff model was constructed by establishing a water exchange between 2d and 1d models using a coupling model 5 1 estimating exchange flow to capture the flow exchange between models the coupling model is divided into modules describing the interaction between the drainage pipe system and river network the interaction between the road network and drainage network and the interaction between the urban area and surrounding environment the drainage pipe system and river network are connected by a pumping station or a gate the road network and drainage pipeline are connected by a drainage ditch or storm drain li et al 2016 described the water exchange between the area of the community and surrounding environment in the following forms within the community areas between the community areas and the road network and between the community areas and the drainage system the coupling among the modules not only captures the interaction between surface and subsurface components but also divides the surface open channel into community areas road network and river network flow the main exchange in an urban flood model is between the surface flow and the drainage network three flow patterns occur in this coupling model 1 drainage into an unpressurized system 2 drainage into a pressurized system or 3 drainage from the system to the surface these three patterns correspond to free flow submerged flow and overflow respectively fig 4 and the latter two states are pressurized fernández pato and garcía navarro 2018 fan et al 2017 rubinato et al 2017 the coupling model that describes the exchange of surface runoff with flow in the drainage network generalizes the exchange channel that connects the two networks e g drainage ditch or storm drain as a wide roof weir then the flow exchange direction was determined by the relative magnitudes of the heads on each side of the weir the commonly used flow exchange equations are the weir and orifice equations the weir equation is used to calculate free flow it assumes that the amount of water flowing into a pipe is less than the free space in the pipe or feeding gutter the orifice equation is suitable for calculating submerged flows it converts water pressure into a corresponding water depth to calculate the exchange flow note that these two methods have uncertainties in calculating exchange flows which in turn affect the accuracy of urban flood modeling these uncertainties have been attributed to the use of empirical hydraulic equations the weir and orifice equations which require accurate determination of the discharge coefficient to achieve connectivity at the interface the uncertainties of these coefficients arise from the dependence of the hydraulic head loss coefficient on the different flow patterns of drainage and overflow therefore if the classic orifice plate sum was employed the corresponding flow coefficient had to be selected carefully and the values of at least the drainage and overflow coefficients had to be different bazin et al 2014 djordjević et al 2005 however owing to the lack of sufficiently high resolution local data there was poor understanding and quantifying of the appropriate range for these flow coefficients to date the flow coefficient in these empirical equations has been determined using experimental calibration and verification fu et al 2018 djordjević et al 2013 spencer 2013 rubinato et al 2017 currently there is no specific theory or widely accepted method for accurately describing the overflow situation in this coupling model although the methods proposed in the literature are stable there is no evidence as to which is the most suitable therefore it remains necessary to further study the flow interaction mechanism improve calculation methods and collect more detailed meteorological data to improve the coupling model performance 5 2 coupling structure analysis the most common coupling model used in simulating urban floods describes the interaction between the surface runoff model and the drainage network flow model through a storm drain or another orifice to calculate the interaction flow of this coupling model the relationship between the two at the connection must first be determined this can be accomplished using the node information of the pipe network model and the mesh information of the 2d surface runoff model therefore the analysis process fig 5 using the coupling model between the surface runoff and underground pipe network flow is undertaken fernández pato and garcía navarro 2018 1 input the required data describing the surface runoff model and underground pipe network flow model including the spatial topographic data and attribute parameter data 2 determine and input the flow exchange relationship between the surface runoff model and the underground pipe network flow model at their connection three patterns in fig 4 3 calculate the exchange flow between the surface runoff model and underground pipe network flow model at their connection 4 calculate and update the surface runoff and underground pipe flow 5 if the end calculation condition is met go to step 6 otherwise go back to step 3 6 output the analysis result however when modeling real floods a lack of comprehensive field data means that the coupling model can only be partially verified in particular the equation governing the exchange between surface and groundwater flows is uncertain therefore further research is required to explore the accurate application of the coupling model between the surface runoff and underground pipe network 6 summary and improving urban flood numerical simulation models this section provides a comprehensive summary of the numerical simulation of urban flooding based on the various models methods and influencing factors discussed simultaneously two directions for improving urban flood models are proposed to improve the model accuracy and to accelerate the model computation speed the advantages disadvantages and adaptability of various calculation methods and model types are analyzed and the latest research progress are discussed 6 1 summary of urban flood numerical simulation the urban flood numerical model consisted of four calculation modules urban surface runoff yield surface runoff pipe network flow and the interaction between surface subsurface flow table 1 details the subroutines and typical applications of the urban flood numerical simulation model the core of urban runoff yield simulation research lies in the runoff yield mechanisms of permeable and impervious surfaces and the runoff yield simulation method on permeable surfaces for impervious areas the saturation excess runoff method is suitable for areas requiring higher accuracy and the runoff coefficient method or the scs cn method is used for general accuracy in permeable areas the infiltration curve method is used for areas requiring a higher precision and the runoff coefficient method and the φ indicator method are used in general precision areas simulating urban surface runoff is divided into two aspects the surface convergence model and complex area simulation street building sudden flood table 1 lists the various numerical simulation models and their corresponding applicable conditions also calculating the pipe network confluence of urban drainage systems is a key part of establishing the urban flood model the 1d saint venant equation and the preissmann slot method are pipeline flow calculation models used and the 2d swes and the tpa method are used at pipeline intersections the interaction between surface and underground runoff is established from three aspects road network and drainage pipeline river network and drainage pipeline and residential area and drainage pipeline table 1 lists the detailed calculation methods and their corresponding applicability 6 2 improving model accuracy there is a great deal of unavoidable uncertainty in the process of urban flood modeling which affects the accuracy of a numerical simulation however the resulting error could be reduced by identifying the source of uncertainty in the model and then reducing its amplitude in the modeling of urban flooding uncertainty arises from teng et al 2017 1 numerical performance when solving the model 2 uncertainty of the model structure 3 uncertainty of the model input 4 inherent random events related to natural phenomena the uncertainties described in 1 2 and 3 represent types of cognitive uncertainty that could be addressed by the modeling method therefore the numerical structural and input errors caused by such uncertainties could be reduced by improving the describing and measuring methods therefore errors arising from the uncertainty described in 4 required a larger analysis of natural phenomena and were not discussed within the model specific scope of this review numerical errors described in 1 were related to the applied numerical method and were addressed by selecting the discrete method used to solve the governing equations or the type of model meshing structural model errors described in 2 were caused by the limitations of the governing equations owing to assumptions regarding the mathematical description of natural phenomena they were addressed by appropriately selecting a modeling method and its corresponding mathematical model input errors described in 3 were introduced in the input data and calibration parameters and were addressed by reconstructing the data and improving other data methods 6 2 1 improved method of reducing numerical error 6 2 1 1 selection of discrete equation solution method in recent decades a great deal of research has been conducted to establish numerical methods for solving 2d hydrodynamic flow models based on current mathematical theory 2d swes which are hyperbolic partial differential equations can only be solved using numerical methods at present the most commonly used discrete methods for solving such equations are divided into the finite difference method finite volume method and finite element method table 3 compares the specific advantages and disadvantages of each of these methods the finite difference method is simple but it is not suitable for the cumbersome processing of irregular regions this method is suitable for 1d dynamic flow simulations commonly in the four point implicit preissmann scheme this format provides high stability high accuracy and excellent calculation efficiency however it is not suitable for solving problems such as rapids or critical flows or for processing dry beaches and shallow water depths the finite volume method is the commonly used discrete method for solving the 2d swes which is more accurate than the finite difference method moukalled et al 2016 however if the time term is in an explicit format a small time step is required reducing the calculation efficiency furthermore when the terrain drastically changes and the mesh unit scales are different limiting the small time step is apparent the implicit format allows the use of a larger time step but its solution incurs heavy computational burdens and programming difficulties accordingly fan et al 2017 adopted the implicit two time step method to establish a coupling model for urban flooding that improved the computational efficiency of the 2d model by considering accuracy and solved the limitation of requiring the same time step in both the 1d and 2d models the finite element method had a high accuracy and was more suitable for dealing with complex regions because of the challenges associated with its representation and complexity the finite element method was yet to be applied to simulating flow at this time waltz et al 2014 cotter and thuburn 2014 however its unique flexibility means that it remains a potential resource of considerable value for future development and will play an important role in high precision simulations 6 2 1 2 selecting mesh type the correct selection of mesh type and resolution is necessary to obtain an accurate solution with rapid convergence and reduce numerical errors when simulating urban flooding in areas with variable terrain a fine mesh might be required to resolve important flow paths thereby minimizing input data errors for areas with more uniform and simple terrain a coarser mesh might be more suitable kim et al 2014 pointed out two critical considerations when selecting the appropriate mesh resolution 1 the time step depends on the mesh resolution which is determined by the cfl conditions convergence judgment conditions named after courant friedrichs and lewy 2 the higher the resolution of the mesh the higher the accuracy but the higher the computational cost as well meshes are classified as structured unstructured and mixed mesh table 3 shows the advantages and disadvantages structured meshes have small memory and low cost and their application scope is narrow unstructured meshes are slower to generate and are suitable for areas with complex terrain a mixed mesh combines two meshes in simulations the original advantage of unstructured meshes was the automatic generation of internal nodes but later researchers have identified far greater benefits including region adaptive encryption and meshing of any complex region making unstructured meshes highly popular kim et al 2014 analyzed the influence of mesh type on the calculation accuracy and requirements of the 2d godunov type flood model by considering the cartesian grids unstructured grids of triangular cells and mixed meshes of triangular and quadrilateral cells they found that each type of mesh provided advantages in different situations in rectangular channel geometries cartesian and quadrilateral grids reduce numerical errors more effectively than triangular grids in applications involving complex terrain and irregular area boundaries unstructured mesh designs were advantageous in recent years the meshless method has become a research hotspot in the field of computational mechanics as it easily uses coordinates to simulate the flow field in high dimensional and complex areas a key focus of development efforts current literature on the use of meshless methods to solve shallow water equations is based on smoothed particle hydrodynamics sph sun et al 2013 darbani et al 2011 xia and liang 2016 liang et al 2015 other than the method mentioned there are few meshless simulation methods for general urban flooding 6 2 2 selecting mathematical model type the high precision simulation of urban flooding requires detailed data such as river flood levels street flooding processes and local flood flow rates commonly selected hydrodynamic models for employing these data include the 1d 2d and 3d models as well as coupled models this section details several types of equations commonly used to simulate the flow of water in these models table 4 compares the advantages disadvantages and applicability of various model types the 1d models have a simple structure and high computational efficiency the 1d saint venant equations were a set of partial differential equations that describe the laws of motion of 1d morphologies such as those in closed channels or pipes and other shallow water bodies with free surfaces such as open surface floodplains aldrighetti 2007 the 2d models have a higher accuracy stronger applicability and flexibility but are less computationally efficient and time consuming the 1d 2d model combines the advantages of both but with complex coupling conditions the 2d swes represent the current in a flood zone as a 2d field and assume that the water depth is shallow compared to the other dimensions the use of the 2d swes to describe the surface runoff combined with the use of the 1d saint venant equations to describe the underground pipe network flow in a single model was referred to as the coupled 2d hydrodynamic method and was used to simulate urban flooding rangari et al 2018 salvan et al 2016 li and hou 2010 the 3d model s theory was complex and the research immature it is an important development direction for future urban flood simulation 6 2 3 applying data processing technology the uncertainty of the input and the model validation data lead to uncertainty in the model input this type of uncertainty depended on the accuracy of the instruments used to collect the data and the inherent randomness of the data collection process today the rise of emerging data processing techniques offers promising prospects for reducing model input uncertainty 1 uncertainty of input data the use of modern data acquisition technologies such as lidar have increased the complexity and scope of the application by improving resolution while reducing workload wang et al 2018 however a higher resolution does not necessarily improve accuracy owing to the existence of various sources of uncertainty in reality therefore overconfidence in high resolution data should be avoided by further processing such data typical processing methods employ probability based statistics to describe uncertainties and the use of monte carlo methods to implement them 2 uncertainty of model validation data at present a relatively complete urban flood model measures inundation range and water depth however when used for real floods such models can only be partially verified due to a lack of comprehensive field data most existing water depth measurements that can be used for verification were based on stereo two media photogrammetry cao et al 2019 and multispectral images su et al 2015 retrieved from water depth research but these measurement methods were more suitable for deeper water than typically associated with urban rain flooding intelligent measurement technologies have yet to be developed for shallower water an ideal method would employ photos of the stagnant water formed after rainfall and employ artificial intelligence technologies such as image processing and machine learning ml to determine the range and depth of stagnant waterlogged points then this information could be used to verify the flood model s results using ml technology to process data to validate models offered a promising direction for reducing uncertainty establishing commonly used hydrological models was complicated and limited by the region making it difficult to popularize ml methods are data hungry models that relied on much fine data karpatne et al 2017 the working mode could be divided into two parts model training learning and model simulation at present there was no unified data storage and sharing system in the hydrology field and it was difficult to obtain high quality data sets therefore the most accurate model in physics could be selected as the training data set of ml methods and the model would obtain more real and effective results an integrated model suitable for solving hydrological water resource problems was constructed by organically combining ml methods with hydrological physical models which provided a reliable way for establishing of flood models and predicting floods zhu et al 2020 6 3 accelerating model calculation speed most research that has been conducted to increase model calculation speed has attempted to reduce running time by either simplifying the governing equations or applying a type of acceleration technology the three familiar types of governing equations the saint venant equations shallow water equations and navier stokes equations were mathematically related to a certain extent these three equations are momentum conservation equations of fluid motion and the relationship and differences between them could be clearly distinguished according to their underlying hypotheses thus they were derived to obtain increasingly simplified relationships after introducing basic assumptions for simplicity the navier stokes equations could be simplified to obtain the shallow water equations which could in turn be simplified to obtain the saint venant equations furthermore there were many possible simplifications of the hydrodynamic model such as ignoring the inertia term in the equations of motion to obtain a diffusive wave or ignoring both the inertia and pressure terms to consider the effect of friction and bottom slope obtaining a kinematic wave diffusive and kinematic waves as well as other simplified forms of the hydrodynamic model could often efficiently approximate flow with sufficient precision and their simplified calculation was convenient for use in practical applications such simplifications accuracy lay between that of the hydrologic model and the pure hydrodynamic model making them suitable for either general modeling or emergency flood forecasting daganzo 2005 su et al 2019 to obtain accurate simulation results it was important to use high resolution digital elevation models dem or digital surface models dsm obtained through lidar measurements to describe complex urban geographic features however high resolution models require significant computing power and time the use of parallel computing technology offered a method to improve the calculation speed and efficiency of flood modeling neal et al 2010 compared three different types of parallelization openmp open multi processing which is an application programming interface api for multithreading on shared memory computing platforms mpi message passing interface kernel which is an api for passing data across distributed memory to enable parallel computing across different machines and gpu graphical processing unit the processor type that uses apis such as cuda to use the architectural advantages of this processor type for accelerated computation they determined that openmp was the easiest means of parallelizing the methods code in the flood models but could only work on a single host and could not be used for parallel computing between multiple hosts mpi provided excellent scalability when there was access to many cores but its performance was affected by the speed of the communication network ranging from simple ethernet to high speed interconnections for modern supercomputers and it was not easy to program the flood modeling code with gpu acceleration was faster and more power efficient than standard code on a single core but it took a great deal of time to develop thus each parallelization method had advantages and disadvantages see table 5 various researchers have proposed hybrid parallelization methods that combine the above methods as an effective solution for complex and detailed modeling conditions noh et al 2018 used a hybrid parallel code consisting of openmp and mpi to calculate shallow water equations and determined that the results were obtained more efficiently than when using openmp or mpi individually especially for super high resolution simulations dazzi et al 2018 proposed an efficient gpu acceleration method based on the local time stepping lts strategy this method was used to rapidly solve 2d swes on a non uniform structural mesh liang et al 2015 proposed a gpu accelerated sph model to solve the 2d swes sph swe with variable smooth lengths combined with the quadtrees neighbor search method this lagrangian meshless computing method was more efficient than a traditional mesh based parallel algorithm in future the smoothed particle hydrodynamics sph models should be extended to run on multiple gpus to improve their computational efficiency note that the reduction of computation time was achieved by either reducing complexity or increasing processing capabilities but improper application in either direction affected the model s accuracy therefore in practical applications researchers should ensure that any accelerated operations yielded results within the required accuracy range in this section we systematically summarized the methods to improve the numerical simulation model of urban flood from two aspects improving the accuracy of the model and speeding up the calculation speed the model s accuracy could be improved by reducing the numerical error reasonably selecting the type of mathematical model and applying data processing technology generally the model s running time was reduced by simplifying the control equation or applying acceleration technology to speed up the calculation speed model improvement research was crucial to the sustainable development of urban flood numerical simulation 7 future research directions to develop more accurate and widely applicable urban flood numerical simulation technology future research should focus on the following problems complex coupling between different flood water sources the complex relationships between the five types of water sources upstream runoff river and drainage overflow regional rainfall and tsunamis which may or may not be coupled additional factors had to be considered when modeling urban flooding caused by these different water sources and the coupling between these water sources made the task of calculating and verifying models more difficult therefore it is an essential future research direction to comprehensively and systematically discuss the formation mechanism of urban floods from large catchment areas particularly researchers should focus on the mechanism of urban road flooding which were the core means of alleviating urban flood disasters at present sponge city measures have been proposed and applied in flood mitigation luo et al 2022 zha et al 2021 coupling of hydrologic and hydrodynamic models hydrodynamic models were difficult to apply to flood simulations in large scale basins due to their high computational costs whereas hydrologic models were difficult to apply to flood simulations in complex areas owing to their less detailed physical processes for large scale watersheds with complex terrain coupling the hydrodynamic model with the hydrologic model could overcome the shortcomings of each modeling method to provide effective regional flood simulation while reducing calculation costs future research could be further developed from the aspects of model coupling mode structure efficiency and connectivity development of 3d models it was necessary to develop 3d models to capture the vertical features of sudden flood disasters including vertical turbulence vortexes or spiral flows such as tsunamis dam break mountain torrents etc note that the development of a 3d model required careful attention to calculating feasibility as well as issues such as free surface flow high order turbulence and the crossing of coastlines a particularly exciting field of 3d model development was the use of particle models these models have many advantages among others they can represent small scale features and do not need spatial discretization st germain et al 2014 developing particle based 3d flood models remains in the initial stages but they have excellent prospects for being applied to urban flood simulations additionally virtual reality technology as the back end application of the 3d model is also being gradually developed which could provide a more intuitive and realistic flood simulation process improvements of model accuracy and operation efficiency the need for faster and more accurate fluid solvers and quantifying and reducing uncertainty in the future urban flood modeling will be improved in terms of acceleration technology data processing methods and high precision input data meanwhile the understanding of urban hydrophysical processes need to be strengthened the related conclusions generalized and in depth mechanism research conducted in particular related theories and technologies such as surface water groundwater interaction processes 3d hydrodynamic models meshless sph swe models accelerating gpu and finite element methods need to be further developed acquiring field data to verify the model at present a relatively complete urban flood model could simulate the submergence range and water depth but when modeling real floods the lack of comprehensive field data made it difficult to verify these results at this stage a variety of emerging data collection technologies have been developed and applied the combined effect of rainfall monitoring stations remote sensing and radar technology could capture accurate rainfall information and provide data to support flood forecasting also ai technologies such as ml have been applied to predicting and modeling urban flooding for example the degree and depth of ponding during rainfall events were analyzed by collecting photos and using image processing technology at the ponding point in future emerging data collection and processing technologies will be developed to verify simulated urban floods and to provide support to predict urban floods 8 conclusion urban flooding has become increasingly frequent with ongoing economic development accelerating urbanization and climate change thus developing urban flood models has become an important issue in preventing floods and reducing disasters accordingly this review summarized the research and development of methods for the numerical simulation of urban flooding and is intended to provide a scientific basis for future research into methods for assessing urban flood risks early warning forecasts and suggesting policies the research on advancing flood models have contributed to reducing risk minimizing the loss of human life and reducing property damage associated with floods this review is summarized as a comparative study of various methods for the numerical simulation of urban floods was undertaken detailing numerical analysis methods for addressing urban street runoff building barriers underground pipe network flow modeling problems and feature analysis the hydrological modeling method is simple and fast but the accuracy is not high so it is suitable for emergency urban prediction the 1d 2d coupling model based on hydrodynamics has become one of the important choices of urban flood modeling because of its high precision urban flood simulation two directions for improving the model were identified increased accuracy and faster computation speed and the various factors influencing these objectives were compared and analyzed the 1d 2d coupling model finite volume method unstructured meshing method and hybrid parallel computing applications were highlighted as important options for urban flood modeling due to their applicability and mature theoretical development prospects for the future development of urban flood numerical simulation were proposed it was determined that future research will focus on high precision refinement multi process and emerging data in the future the most promising areas for further development include comprehensive hydro hydraulic modeling 3d hydrodynamics modeling sph swe modeling based on the grid etc declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgment this study is supported by national key r d program of china 2018yfe0103800 one hundred talent plan of shaanxi province international collaborative research of disaster prevention research institute of kyoto university 2019w 02 and general project of shaanxi provincial key r d program social development field 2021sf 454 many thanks is given to the reviewers for their great comments 
25531,urban flooding has become an increasingly frequent and fatal natural hazard and numerical modeling techniques play a vital role in its prediction and management we review urban flood numerical simulations by systematically summarizing the calculation methods of surface runoff drainage systems and coupled models following the review accuracy and computational efficiency are found to be the two key areas hindering the quality improvement of urban flood models so an investigation of the key trends in the improvement of model accuracy and computational efficiency is conducted it is found that the 1d 2d coupling model finite volume method unstructured meshing method and hybrid parallel computing applications are the most effective strategies furthermore the complex coupling of models and the lack of validation data are still crucial challenges in the development of urban flood modeling this result can be used as a guideline for hydrologists in choosing the proper method of urban flood numerical simulation according to the task keywords urban flooding numerical simulation runoff model drainage system coupling model 1 introduction environmental changes have precipitated the frequently extreme weather that has occurred in recent decades climate change produces extreme rainfall which in turn leads to flooding li et al 2022 mou et al 2020 zhu et al 2020 increases in precipitation urbanization and changes in topographic characteristics have led to a sharp rise in the occurrence of natural hazards urban floods soil erosion etc an ee s et al 2016 particularly increasing urbanization and urban populations have led to frequent urban flooding semadeni davies et al 2008 mu et al 2021 accordingly urban flood forecasts and early warning systems are crucial in preventing disaster damage and urban flood models are important to establish those wang et al 2022 cao et al 2022 therefore reviewing the numerical simulation technology for urban floods is critical for study of urban floods in the future the core science of urban flood numerical simulation consists of urban hydrology hydrodynamic mechanisms and their coupling salvadore et al 2015 generally an urban flood model comprises generating runoff confluence and flood inundation analysis modules fatichi et al 2016 according to the specific modeling mechanism employed each module adopts the hydrologic hydrodynamic or simplified method to conduct calculations liu et al 2015 the model development process of the urban flood simulation is divided into three stages the experience exploration stage proposing the classical hydrology theoretical methods the algorithm innovation stage innovating of simulation methods and algorithms for urban hydrology and hydrodynamic processes and the comprehensive integration stage integrating of new algorithms and new data pre and post processing technologies using the simulation platform bach et al 2014 fletcher et al 2013 wang and wang 2018 historical research has been conducted on many specific urban flood events su et al 2019 established an urban pluvial flood simulation model based on the diffusing wave approximation of shallow water equations moreover hasan et al 2019 integrated hydrology and hydraulic models to simulate the urban flood events in the aur river catchment in malaysia rabori and ghazavi 2018 applied the storm water management model to estimate urban flooding in semiarid areas zanjan city in northwest iran also to benefit from the advantages and characteristics of various models researchers have established two dimensional 2d coupled hydrodynamic models to study the storage and blocking effects of buildings in urban flood modeling huang et al 2014 similarly to overcome the drawbacks of each individual modeling approach zhang 2015 introduced a new integrated flood modeling tool in urban areas by coupling a hydrodynamic model with a hydrological model a review of these developed urban flood simulation technologies provides collective experience and offers insights that guides the development of future urban flood models despite active research in this field feng et al 2020 liu et al 2020 zhang et al 2022 the rapid and accurate modeling of urban flooding remains a major challenge for hydrology and hydraulics research owing to the complexity of the urban flooding process luo et al 2021 zhu et al 2022 therefore retrospective studies on urban flood models have been conducted to assist in developing suitable flood models researchers reviewed 45 laboratory based urban flooding studies to help computational modelers detect the most appropriate data sets for validating their numerical methods mignot et al 2019 furthermore nkwunonwo et al 2020 reviewed the current status of flood modeling for managing urban flood risk in developing countries and qi et al 2021 provided a systematic overview of applying urban flood modeling approaches from the perspective of urban flood strategy design sequences guo et al 2021 presented a comprehensive review of advanced urban flood models and the emerging approaches for predicting urban surface water flooding driven by intense rainfall bulti and abebe 2020 provided an overview of prevailing flood modeling approaches in view of their potentials and limitations for modeling pluvial flooding in urban settings however the general applicability of various methods for the numerical simulation of urban rainfall flow and flooding has not been reviewed and the associated model calculation methods have not been systematically summarized or compared this is an opportune time for a review of the literature as urban flood modeling methods have evolved rapidly in recent years and they are central to studies of flood mitigation in this study historical literature is used to analyze the construction modules of urban flood models and the hydrological and hydrodynamic numerical simulation methods of each module are summarized based on the review of urban flood numerical simulation and the development analysis of future models we provide some suggestions for improving the quality of future urban flood models the novelty of the present study lies in the comprehensive and systematic comparison of the calculation methods adopted by various models and the advantages disadvantages and applicability of the models are summarized considering the specific application scenarios of the methods and especially detailed ways to improve the model are proposed the research is the comprehensive methodological discussions that are aimed to support flood modeling in different regions the results offer an extensive overview of urban flood numerical simulation and will provide reference materials for national and local emergency departments and researchers 2 methodology fig 1 shows this study s research framework which includes three main parts systematic review research content and methods and discussion of future urban flood numerical simulations first we collected historical documents and literature and conducted a retrospective study on three aspects the urban surface runoff model urban drainage numerical simulation and the coupling model second we used the literature review and comparative analysis to describe the technology of urban flood numerical simulation in four aspects type selection calculation method applicability advantages and disadvantages third the direction of future developments and improvements in urban flood numerical simulation was discussed our study s implications were provided to facilitate the research of the urban flood model 3 overview of urban surface runoff model simulating the urban surface runoff yield and confluence process is a crucial part of establishing an urban flood model this section outlines the urban surface runoff model based on hydrologic modeling methods and discusses the development of the urban surface runoff model based on hydrodynamics 3 1 surface runoff yield and confluence model based on hydrologic method urban surface runoff yield is defined as the process of deducting losses from rainfall to form net rainfall rainfall losses include plant interception infiltration bottomland filling and evaporation but are primarily caused by infiltration note that the characteristics of urban surface runoff are different from those of general basins and the differences are reflected in the following the underlying surface of the city is intricate and the surface rainfall yield is not uniform the urban surface rainfall usually lasts for a short period of time and yields large the rainfall loss runoff yield and runoff retention time on urban surface are quite different from those in general watersheds owing to the complexity of the underlying surface of a city including impermeable and permeable areas the hydrologic methods applicable to calculating urban surface runoff yield include the runoff coefficient method soil conservation service curve number scs cn method mishra and singh 2003 infiltration curve deduction method saturation excess runoff method and φ indicator method there are various models for calculating runoff yield with advantages and disadvantages in current urban hydrology research the relative efficiency or accuracy of a particular method with respect to the others cannot be determined in a definite manner table 1 summarizes the characteristics and applicability of relevant runoff yield models to facilitate the researchers selection surface rainwater confluence is the process of runoff from the surface into a pipe network or channel as hydrologic methods employ a system based idea they simulate surface confluence by establishing the relationship between the input and output of the urban system common methods for achieving this include the reasoning formula isochrones instantaneous unit hydrograph linear reservoir and non linear reservoir methods kidd 1978 used the observation data of urban surface runoff in britain sweden and other countries to compare the methods of calculating surface confluence the results show that the simulation effect of the non linear reservoir method is better and the calculation effect of the instantaneous unit hydrograph method is poor table 1 defines their different applications when there are few urban surface runoff data the nonlinear reservoir method is more suitable for calculating surface confluence in areas with high precision requirements 3 2 numerical simulation of urban surface runoff based on hydrodynamic method specific flow rates and water depth distributions are obtained using the hydrodynamic method because it has a clear physical meaning accordingly the hydrodynamic method directly solves the saint venant equations or its simplified form based on the microscopic physical laws to obtain a more detailed urban surface runoff process than the hydrological method indeed the hydrodynamic method which better reflects the surface runoff of more complex terrain is suitable for areas without hydrological data and is more scalable the numerical models of urban surface runoff based on the hydrodynamic method include one dimensional 1d saint venant equations two dimensional 2d shallow water equations swes 1d 2d shallow water equations swes three dimensional 3d navier stokes equations boussinesq equations kinematic wave equations diffusion wave equations and the porosity model table 1 lists the application of these numerical models though the navier stokes equations best describe complex surface runoff solving these equations at the actual space time scale has been considered impossible meng et al 2019 they are employed in theoretical analyses rather than in actual engineering applications however the simplified forms of the navier stokes equations such as the 1d saint venant equations or 2d swes have been used in various flooding simulations su et al 2019 the 1d hydrodynamic model has obvious limitations in calculating the irregular regional confluence of urban surfaces in recent years urban surface runoff modeling is mostly based on 2d or 1d 2d swes however solving the 2d swes remains a complex project to reduce the computational complexity and running time the researchers simplified the 2d swes omitted the convective acceleration and used the kinematic wave or diffusion wave equations to simulate the flood runoff in urban areas based on the typical characteristics of urban surface runoff then we discuss numerical simulation studies of streets buildings and sudden flooding in detail 3 2 1 simulating complex street runoff the main cause of street flooding is the insufficient water intake capacity of the drainage system in recent years street network flood models have been considered steady or unsteady flow with single or multiple intersections there has been some in depth research on street surface flow mark et al 2004 bazin et al 2017 mignot et al 2013 the saint venant equations are applied to a drainage network or simply a street though the simulation results are inaccurate for irregular street geometries or when rainwater crosses a curb in such situations since the water flow is no longer in 1d the validity of the 1d flow field hypothesis will be uncertain mark et al 2004 for densely populated traffic intersections the fully 2d swes are required to realize high precision flood simulations in addition to including more accurate descriptions of hydraulic factors the 2d swes simulate local complex water flow phenomena indeed since overflow phenomena occur near street intersections and obstacles special research into these situations was required such research included the use of 2d swes to simulate the flooding of intersections with obstacles and sidewalks bazin et al 2017 and the use of a 2d numerical model with a simple turbulence model to simulate the effects of obstacles mignot et al 2013 and sidewalks bazin 2013 on crossroad flooding notably the latter models considered the turbulence effect and were found to be much simpler than an equivalent 3d model 3 2 2 surface runoff around buildings the buildings and building groups present in urban areas change the direction of rainwater runoff because of their water blocking effect which must be considered in any urban flood model commonly used methods for doing so include local friction building blocks porosity and mesh adjustment the local friction method of considering the building water blocking effect is realized by increasing the roughness coefficient of the building group mesh this method is easy to model can be executed relatively quickly and can be used with any type of mesh however it is less accurate than other methods for example when other meshes display a reasonable inundation depth the inundation depth of the building group mesh tends to be abnormally high this may be caused by floods flowing into and subsequently being stored in a building when the water level outside the building is higher than at the entrance to correct the inaccurate reduction of the surface water depth owing to this water storage effect huang et al 2014 proposed a manning s friction coefficient correction method that considered the water storage effect when the water level reached the building threshold in this approach buildings were conceptualized as a region with the same geometry as the mesh cell located at cell center s see fig 2 without affecting the flux through the boundary shared with its neighbor cells due to these complications the local friction methods were suitable for situations in which there were no detailed architectural geometry data building blocks provide a suitable replication of a building s blocking effect and do not reduce the volume of rain schubert and sanders 2012 however this method requires a fine mesh to generate each building shape as a block therefore it requires a high computational power making it costly and impractical guinot et al 2017 proposed the dual integral porosity dip model which includes several improvements to the integral porosity ip model in addition to simulating the blocking effect of the building the dip model achieves closure of the previously introduced ip model by defining an edge based model solution boundary of computational cell in addition to a cell based model solution interior of a computational cell transient momentum dissipation is used to introduce an important control when shaping the direction in which buildings within an array impede flow however the dip model has difficulty in determining the optimal parameter values over a large area thus reducing the application value of the resulting simulation nevertheless both the dip and ip models fail to some extent to reproduce the preferential directions of the flow field in the transient phase calibrating and applying shallow water porosity models remains in its infancy therefore further research is required on model development lee et al 2016 used high resolution data from light detection ranging lidar measurements to describe complex urban geographic features and developed a sophisticated urban flood model the model separated the building community mesh from the road network mesh and artificially adjusted the height of the two grids that induce flow from the building group to the road network and showed better simulation accuracy 3 2 3 sudden floods for many scales of flooding the 1d or 2d shallow water model is sufficient to provide an approximate simulation however sudden flood disasters such as dam break tsunamis and mountain torrents have complex mechanism processes such as vertical turbulence flow vortex flow and spiral flow flood models based on 1d 2d swes cannot approximate the motion of these types of floods their basic assumptions are hydrostatic pressure distribution and no vertical acceleration however sudden floods flow in three directions longitudinal lateral and vertical and vertical acceleration has a significant effect on the pressure distribution which renders the assumptions of the modeling equations invalid wang et al 2016 there have been four types of numerical simulation methods based on various equations developed by researchers for sudden flood disasters such as tsunamis 1 nonlinear shallow water equations such as the tunami n2 model most model comcot model etc 2 boussinesq equation such as funwave and cornell university s culwave 3 fully nonlinear potential flow theory 4 the navier stokes equations among these model types the first two are used to simulate tsunami inundation a review of recent relevant literature tehranirad et al 2011 pophet et al 2011 tissier et al 2011 indicates that the high order boussinesq water wave model has been found to provide better nonlinear and dispersion characteristics than the shallow water equations and is used in the dynamic study of sudden flooding disasters such as tsunamis the boussinesq equation describes short wave dynamics considering the dispersion and non hydrostatic pressure distribution of the wave therefore this equation has been used to simulate the evolution of waves on an uneven formation the interaction between waves and structures etc zerihun and fenton 2006 fuhrman et al 2005 also recent studies have shown that the boussinesq equation is superior to the shallow water equations in predicting surface oscillation modes and secondary wave characteristics chang et al 2014 however solving the boussinesq equation is time consuming considering that 3d numerical simulation of the flood evolution process has been an important subject to researchers methods for reducing the cost of calculating and optimizing the numerical model in each stage of sudden flooding should be the focus of future work however this review focuses on general scale urban flooding events caused by heavy rains and does not specifically address large scale sudden flooding in summary table 2 shows the selection rules of urban surface runoff modeling methods discussed in this section based on hydrological and hydrodynamics hydrological modeling is fast easy to implement and suitable for areas with limited data hydrodynamic modeling is stable accurate and versatile and is used in more detailed urban flood simulations the simplified method simplified from the hydrodynamic method runs faster than the hydrodynamic method and the simulation results are more accurate than the hydrological method which is suitable for large scale applications 4 urban drainage numerical simulation technology many urban areas are prone to flooding during summer rainstorms because the drainage networks in the older parts of the city are too small limiting the drainage capacity when heavy rain occurs the drainage pipe network is overloaded and the rainwater cannot be discharged in time causing flooding inside the city furthermore defects in the design of the drainage system causes rainwater from the pipe network to back up into the city during a rainstorm causing additional flooding therefore the study of urban drainage systems is essential in the numerical simulation of urban flooding 4 1 numerical simulation of pipeline flow in drainage network in general the designer of a drainage network uses the urban topographic map and planning map to select the optimal gravity flow drainage arrangement according to the terrain so that most of the urban rainwater discharge is based on gravity a gravity based drainage network employs open channel flow pressure flow free surface pressurized flow and mixed free surface pressure flow to move water depending on its arrangement general open channel flow is described by a 1d open channel flow model whereas a pressure flow model is required to simulate the pressure flow state the early use of pipe network convergence is based on the instantaneous unit hydrograph method and muskingum method but such methods are less accurate another type of pipeline network flow simulation method employs simplified saint venant equations to develop kinematic diffusive and inertial waves that provides more accurate results than hydrologic methods however this approach is not suitable for simulating mixed free surface pressure flow currently the combination of the 1d saint venant equations with the preissmann slot method is used for pipe network flow simulation an et al 2018 wang et al 2019 maranzoni et al 2015 in this approach the preissmann slot method was used to unify the open channel flow then the solution of the pressurized flow was inserted into the saint venant equations to simulate the transient free surface and the presence of pressure flow the problem that arose in this approach was that the preissmann slot scheme did not simulate the behavior of supercritical flows meanwhile the widths of different flow regimes could not be consistently described making it difficult to unify the flow equations in response to the above problems fan et al 2017 proposed a method of treating supercritical flows as subcritical flows an approximate simulation of supercritical flows was achieved by expanding the flow term to reduce the influence of the momentum equation liu et al 2016 proposed the use of an equivalent width to revise the equation for the narrow slot width in the preissmann slot method the objective was to convert the density change in the pipe network flows into the overcurrent equivalent area change introduce the equivalent area and convert the continuous equation form of the pressurized flows into the non pressure open channel form realizing the effective numerical simulation of the mixed free surface pressure flows beyond theory based model formulation practical implementation with the preissmann slot scheme has been a problematic issue in the past and has required significant research effort to resolve for example choosing an inappropriate slot width when implementing the preissmann slot scheme might lead to model instability vasconcelos et al 2006 therefore researchers proposed a new node calculation algorithm to model the drainage network li et al 2018 that evaluated the coupling conditions between pipelines and pipeline intersections to select a two component pressure approach tpa that supports pipeline flow calculation then the algorithm used the 2d swes to calculate the flow at the junction fig 3 shows a diagram of the tpa at a connection point between three pipes in fig 3 1 p1 and p2 were assumed to be inflow pipes while p3 was the outflow pipe based on the pipe layout the junction domain was approximated using an irregular grid cell as indicated in 2 the inflow from the two inlet pipes p1 and p2 was mixed in the junction and then discharged into the outflow pipe p3 the junction was idealized into a 2d domain in 3 and subsequently the junction flow was described using fully 2d swes to reinforce the strict conservation of mass and momentum across the interface between the pipes and the junctions the flow obtained from the tpa calculation was directly used to provide the interface flux on the 2d domain the 2d cell edge that was not connected to the pipe was set to the closed state by applying a no flow boundary condition this method had superior capabilities in complex transient flow simulation between the free surface of the pipeline and the stressed state and avoided the complex boundary conditions required by the conventional tpa model indeed by dynamically coupling the 1d tpa model with the 2d connection model through boundary conditions a new pipe network model that provided greater computational efficiency and stability for the modeling of large drainage pipe networks was provided furthermore urban drainage systems typically include river and drainage networks water can be moved from the drainage network to the river network through gravity or pressure drainage gravity drainage is based on the difference in height between the pipe network end and the water level of the river the flow form of pipeline based on pressure drainage is usually pumping that is regulated by a water pump the water first flows into a pipeline and is then pumped into the river by a pumping station the closer the target unit is to a pumping station the higher its drainage efficiency based on the above characteristics li et al 2016 proposed a radius modeling method to calculate pumping drainage flow this method determines the drainage coefficient by setting the pumping influence radius of the pumping station to obtain the drainage formula describing the water exchange from the pipeline to the river channel 4 2 numerical simulation of roof drainage systems methods for simulating building roof drainage were divided into two categories unorganized and organized drainage yan 2012 unorganized drainage known as free fall directly drained rainwater off the roof from the cornice to the ground where it then flowed into ditches storm drains or through surface runoff into the surrounding streets gardens and other areas organized drainage directed rainwater off the roof and into a drainage system where it was released onto the ground or to an underground drainage network wright et al 2006 also buildings were equipped with rainwater retention systems in which the rainwater was temporarily stored for later drainage vesuviano and stovin 2013 once the runoff generated on the roof exceeded the design capacity of the drainage system the excess runoff might remain on the roof in the retention system or spill to the ground as unorganized drainage arthur et al 2005 for example a flat building roof with retaining walls could function as a storage pool to limit runoff in excess of the capacity of the organized drainage system and the drainage flow equation was obtained accordingly when the water level on the roof rose to a level higher than the crest of the retaining wall the excess runoff flowed directly to the surface as unorganized drainage arthur and wright 2005 therefore the roof drainage capacity of a certain area was designed according to the type of roofs and the assumed rainfall intensity it could be assumed that the roof drainage capacity per square meter was similar for all buildings within a certain area chang et al 2015 green roofing offered an effective way to mitigate urban flooding by reducing the need for drainage capacity in a given area it consisted of three main components a vegetation layer a cultivation substrate layer and a water storage drainage layer placed on a waterproof membrane like rainfall retention using walls the green roof was designed to distribute storm runoff over a longer period of time however most of the rain was trapped on the roof and it released the retained precipitation back into the atmosphere through evaporation helping to relieve the problem of limited urban drainage capacity to model the drainage effect of green roofs soulis et al 2017 proposed a comprehensive model employing a physics based hydrus 1d numerical simulation model combined with a simple conceptual model on the one hand the conceptual model assumed that rainwater entered the green roof system from the surface of the substrate and then left via evaporation through the substrate plant transpiration or drainage layer runoff then the runoff from the green roof reaching the gutter was modeled as conventional runoff on the other hand the hydrus model considered the green roof to consist of two integrated reservoirs the first reservoir captured the rainfall interception in the vegetation layer and the second captured the infiltration process through the substrate and storage drainage layers this combined model required less computation and was applied to different locations and conditions 4 3 uncertainty related to operating conditions of urban drainage system to capture the sources of uncertainty in urban drainage systems most research has focused on input output data deletic et al 2012 as well as model structures and parameters while ignoring the uncertainties associated with specific operating conditions a common problem was the blockage at a drainage system inlet its probability depended on the type and location of the inlet season type of weather and applied cleaning regime for example a drainage inlet located near trees could be blocked by fallen leaves whereas a drainage inlet located in a business district or at a concavity in the terrain could be blocked by debris accordingly blockage was more common in late summer or late fall and during strong winds or heavy rains thus the problem of drainage inlet blocking was an important consideration when attempting to realistically model urban drainage systems leitão et al 2017 investigated simulating random drainage inlet blockage and proposed a three step method for generating flood hazard maps first identify drainage inlets that were prone to blockage by geospatial analysis second conduct a monte carlo simulation of the inlets that are prone to blockage followed by flood simulation third draw a flood disaster map based on the results of the simulation in addition to the problem of blocked drainage inlets uncertainty arose in the operating conditions of a drainage system because of other factors such as blocked drainage pipes or outlets further research into uncertainty analyses related to these factors should be conducted 5 coupled models model coupling is the process of computational interaction of different models or modules in time and space a 2d model is used to calculate rainwater runoff in urban surface areas such as squares residential areas and wide roads whereas a 1d model is used to calculate flows with distinct 1d features such as underground pipe networks and river drainage system therefore a complete urban groundwater runoff model was constructed by establishing a water exchange between 2d and 1d models using a coupling model 5 1 estimating exchange flow to capture the flow exchange between models the coupling model is divided into modules describing the interaction between the drainage pipe system and river network the interaction between the road network and drainage network and the interaction between the urban area and surrounding environment the drainage pipe system and river network are connected by a pumping station or a gate the road network and drainage pipeline are connected by a drainage ditch or storm drain li et al 2016 described the water exchange between the area of the community and surrounding environment in the following forms within the community areas between the community areas and the road network and between the community areas and the drainage system the coupling among the modules not only captures the interaction between surface and subsurface components but also divides the surface open channel into community areas road network and river network flow the main exchange in an urban flood model is between the surface flow and the drainage network three flow patterns occur in this coupling model 1 drainage into an unpressurized system 2 drainage into a pressurized system or 3 drainage from the system to the surface these three patterns correspond to free flow submerged flow and overflow respectively fig 4 and the latter two states are pressurized fernández pato and garcía navarro 2018 fan et al 2017 rubinato et al 2017 the coupling model that describes the exchange of surface runoff with flow in the drainage network generalizes the exchange channel that connects the two networks e g drainage ditch or storm drain as a wide roof weir then the flow exchange direction was determined by the relative magnitudes of the heads on each side of the weir the commonly used flow exchange equations are the weir and orifice equations the weir equation is used to calculate free flow it assumes that the amount of water flowing into a pipe is less than the free space in the pipe or feeding gutter the orifice equation is suitable for calculating submerged flows it converts water pressure into a corresponding water depth to calculate the exchange flow note that these two methods have uncertainties in calculating exchange flows which in turn affect the accuracy of urban flood modeling these uncertainties have been attributed to the use of empirical hydraulic equations the weir and orifice equations which require accurate determination of the discharge coefficient to achieve connectivity at the interface the uncertainties of these coefficients arise from the dependence of the hydraulic head loss coefficient on the different flow patterns of drainage and overflow therefore if the classic orifice plate sum was employed the corresponding flow coefficient had to be selected carefully and the values of at least the drainage and overflow coefficients had to be different bazin et al 2014 djordjević et al 2005 however owing to the lack of sufficiently high resolution local data there was poor understanding and quantifying of the appropriate range for these flow coefficients to date the flow coefficient in these empirical equations has been determined using experimental calibration and verification fu et al 2018 djordjević et al 2013 spencer 2013 rubinato et al 2017 currently there is no specific theory or widely accepted method for accurately describing the overflow situation in this coupling model although the methods proposed in the literature are stable there is no evidence as to which is the most suitable therefore it remains necessary to further study the flow interaction mechanism improve calculation methods and collect more detailed meteorological data to improve the coupling model performance 5 2 coupling structure analysis the most common coupling model used in simulating urban floods describes the interaction between the surface runoff model and the drainage network flow model through a storm drain or another orifice to calculate the interaction flow of this coupling model the relationship between the two at the connection must first be determined this can be accomplished using the node information of the pipe network model and the mesh information of the 2d surface runoff model therefore the analysis process fig 5 using the coupling model between the surface runoff and underground pipe network flow is undertaken fernández pato and garcía navarro 2018 1 input the required data describing the surface runoff model and underground pipe network flow model including the spatial topographic data and attribute parameter data 2 determine and input the flow exchange relationship between the surface runoff model and the underground pipe network flow model at their connection three patterns in fig 4 3 calculate the exchange flow between the surface runoff model and underground pipe network flow model at their connection 4 calculate and update the surface runoff and underground pipe flow 5 if the end calculation condition is met go to step 6 otherwise go back to step 3 6 output the analysis result however when modeling real floods a lack of comprehensive field data means that the coupling model can only be partially verified in particular the equation governing the exchange between surface and groundwater flows is uncertain therefore further research is required to explore the accurate application of the coupling model between the surface runoff and underground pipe network 6 summary and improving urban flood numerical simulation models this section provides a comprehensive summary of the numerical simulation of urban flooding based on the various models methods and influencing factors discussed simultaneously two directions for improving urban flood models are proposed to improve the model accuracy and to accelerate the model computation speed the advantages disadvantages and adaptability of various calculation methods and model types are analyzed and the latest research progress are discussed 6 1 summary of urban flood numerical simulation the urban flood numerical model consisted of four calculation modules urban surface runoff yield surface runoff pipe network flow and the interaction between surface subsurface flow table 1 details the subroutines and typical applications of the urban flood numerical simulation model the core of urban runoff yield simulation research lies in the runoff yield mechanisms of permeable and impervious surfaces and the runoff yield simulation method on permeable surfaces for impervious areas the saturation excess runoff method is suitable for areas requiring higher accuracy and the runoff coefficient method or the scs cn method is used for general accuracy in permeable areas the infiltration curve method is used for areas requiring a higher precision and the runoff coefficient method and the φ indicator method are used in general precision areas simulating urban surface runoff is divided into two aspects the surface convergence model and complex area simulation street building sudden flood table 1 lists the various numerical simulation models and their corresponding applicable conditions also calculating the pipe network confluence of urban drainage systems is a key part of establishing the urban flood model the 1d saint venant equation and the preissmann slot method are pipeline flow calculation models used and the 2d swes and the tpa method are used at pipeline intersections the interaction between surface and underground runoff is established from three aspects road network and drainage pipeline river network and drainage pipeline and residential area and drainage pipeline table 1 lists the detailed calculation methods and their corresponding applicability 6 2 improving model accuracy there is a great deal of unavoidable uncertainty in the process of urban flood modeling which affects the accuracy of a numerical simulation however the resulting error could be reduced by identifying the source of uncertainty in the model and then reducing its amplitude in the modeling of urban flooding uncertainty arises from teng et al 2017 1 numerical performance when solving the model 2 uncertainty of the model structure 3 uncertainty of the model input 4 inherent random events related to natural phenomena the uncertainties described in 1 2 and 3 represent types of cognitive uncertainty that could be addressed by the modeling method therefore the numerical structural and input errors caused by such uncertainties could be reduced by improving the describing and measuring methods therefore errors arising from the uncertainty described in 4 required a larger analysis of natural phenomena and were not discussed within the model specific scope of this review numerical errors described in 1 were related to the applied numerical method and were addressed by selecting the discrete method used to solve the governing equations or the type of model meshing structural model errors described in 2 were caused by the limitations of the governing equations owing to assumptions regarding the mathematical description of natural phenomena they were addressed by appropriately selecting a modeling method and its corresponding mathematical model input errors described in 3 were introduced in the input data and calibration parameters and were addressed by reconstructing the data and improving other data methods 6 2 1 improved method of reducing numerical error 6 2 1 1 selection of discrete equation solution method in recent decades a great deal of research has been conducted to establish numerical methods for solving 2d hydrodynamic flow models based on current mathematical theory 2d swes which are hyperbolic partial differential equations can only be solved using numerical methods at present the most commonly used discrete methods for solving such equations are divided into the finite difference method finite volume method and finite element method table 3 compares the specific advantages and disadvantages of each of these methods the finite difference method is simple but it is not suitable for the cumbersome processing of irregular regions this method is suitable for 1d dynamic flow simulations commonly in the four point implicit preissmann scheme this format provides high stability high accuracy and excellent calculation efficiency however it is not suitable for solving problems such as rapids or critical flows or for processing dry beaches and shallow water depths the finite volume method is the commonly used discrete method for solving the 2d swes which is more accurate than the finite difference method moukalled et al 2016 however if the time term is in an explicit format a small time step is required reducing the calculation efficiency furthermore when the terrain drastically changes and the mesh unit scales are different limiting the small time step is apparent the implicit format allows the use of a larger time step but its solution incurs heavy computational burdens and programming difficulties accordingly fan et al 2017 adopted the implicit two time step method to establish a coupling model for urban flooding that improved the computational efficiency of the 2d model by considering accuracy and solved the limitation of requiring the same time step in both the 1d and 2d models the finite element method had a high accuracy and was more suitable for dealing with complex regions because of the challenges associated with its representation and complexity the finite element method was yet to be applied to simulating flow at this time waltz et al 2014 cotter and thuburn 2014 however its unique flexibility means that it remains a potential resource of considerable value for future development and will play an important role in high precision simulations 6 2 1 2 selecting mesh type the correct selection of mesh type and resolution is necessary to obtain an accurate solution with rapid convergence and reduce numerical errors when simulating urban flooding in areas with variable terrain a fine mesh might be required to resolve important flow paths thereby minimizing input data errors for areas with more uniform and simple terrain a coarser mesh might be more suitable kim et al 2014 pointed out two critical considerations when selecting the appropriate mesh resolution 1 the time step depends on the mesh resolution which is determined by the cfl conditions convergence judgment conditions named after courant friedrichs and lewy 2 the higher the resolution of the mesh the higher the accuracy but the higher the computational cost as well meshes are classified as structured unstructured and mixed mesh table 3 shows the advantages and disadvantages structured meshes have small memory and low cost and their application scope is narrow unstructured meshes are slower to generate and are suitable for areas with complex terrain a mixed mesh combines two meshes in simulations the original advantage of unstructured meshes was the automatic generation of internal nodes but later researchers have identified far greater benefits including region adaptive encryption and meshing of any complex region making unstructured meshes highly popular kim et al 2014 analyzed the influence of mesh type on the calculation accuracy and requirements of the 2d godunov type flood model by considering the cartesian grids unstructured grids of triangular cells and mixed meshes of triangular and quadrilateral cells they found that each type of mesh provided advantages in different situations in rectangular channel geometries cartesian and quadrilateral grids reduce numerical errors more effectively than triangular grids in applications involving complex terrain and irregular area boundaries unstructured mesh designs were advantageous in recent years the meshless method has become a research hotspot in the field of computational mechanics as it easily uses coordinates to simulate the flow field in high dimensional and complex areas a key focus of development efforts current literature on the use of meshless methods to solve shallow water equations is based on smoothed particle hydrodynamics sph sun et al 2013 darbani et al 2011 xia and liang 2016 liang et al 2015 other than the method mentioned there are few meshless simulation methods for general urban flooding 6 2 2 selecting mathematical model type the high precision simulation of urban flooding requires detailed data such as river flood levels street flooding processes and local flood flow rates commonly selected hydrodynamic models for employing these data include the 1d 2d and 3d models as well as coupled models this section details several types of equations commonly used to simulate the flow of water in these models table 4 compares the advantages disadvantages and applicability of various model types the 1d models have a simple structure and high computational efficiency the 1d saint venant equations were a set of partial differential equations that describe the laws of motion of 1d morphologies such as those in closed channels or pipes and other shallow water bodies with free surfaces such as open surface floodplains aldrighetti 2007 the 2d models have a higher accuracy stronger applicability and flexibility but are less computationally efficient and time consuming the 1d 2d model combines the advantages of both but with complex coupling conditions the 2d swes represent the current in a flood zone as a 2d field and assume that the water depth is shallow compared to the other dimensions the use of the 2d swes to describe the surface runoff combined with the use of the 1d saint venant equations to describe the underground pipe network flow in a single model was referred to as the coupled 2d hydrodynamic method and was used to simulate urban flooding rangari et al 2018 salvan et al 2016 li and hou 2010 the 3d model s theory was complex and the research immature it is an important development direction for future urban flood simulation 6 2 3 applying data processing technology the uncertainty of the input and the model validation data lead to uncertainty in the model input this type of uncertainty depended on the accuracy of the instruments used to collect the data and the inherent randomness of the data collection process today the rise of emerging data processing techniques offers promising prospects for reducing model input uncertainty 1 uncertainty of input data the use of modern data acquisition technologies such as lidar have increased the complexity and scope of the application by improving resolution while reducing workload wang et al 2018 however a higher resolution does not necessarily improve accuracy owing to the existence of various sources of uncertainty in reality therefore overconfidence in high resolution data should be avoided by further processing such data typical processing methods employ probability based statistics to describe uncertainties and the use of monte carlo methods to implement them 2 uncertainty of model validation data at present a relatively complete urban flood model measures inundation range and water depth however when used for real floods such models can only be partially verified due to a lack of comprehensive field data most existing water depth measurements that can be used for verification were based on stereo two media photogrammetry cao et al 2019 and multispectral images su et al 2015 retrieved from water depth research but these measurement methods were more suitable for deeper water than typically associated with urban rain flooding intelligent measurement technologies have yet to be developed for shallower water an ideal method would employ photos of the stagnant water formed after rainfall and employ artificial intelligence technologies such as image processing and machine learning ml to determine the range and depth of stagnant waterlogged points then this information could be used to verify the flood model s results using ml technology to process data to validate models offered a promising direction for reducing uncertainty establishing commonly used hydrological models was complicated and limited by the region making it difficult to popularize ml methods are data hungry models that relied on much fine data karpatne et al 2017 the working mode could be divided into two parts model training learning and model simulation at present there was no unified data storage and sharing system in the hydrology field and it was difficult to obtain high quality data sets therefore the most accurate model in physics could be selected as the training data set of ml methods and the model would obtain more real and effective results an integrated model suitable for solving hydrological water resource problems was constructed by organically combining ml methods with hydrological physical models which provided a reliable way for establishing of flood models and predicting floods zhu et al 2020 6 3 accelerating model calculation speed most research that has been conducted to increase model calculation speed has attempted to reduce running time by either simplifying the governing equations or applying a type of acceleration technology the three familiar types of governing equations the saint venant equations shallow water equations and navier stokes equations were mathematically related to a certain extent these three equations are momentum conservation equations of fluid motion and the relationship and differences between them could be clearly distinguished according to their underlying hypotheses thus they were derived to obtain increasingly simplified relationships after introducing basic assumptions for simplicity the navier stokes equations could be simplified to obtain the shallow water equations which could in turn be simplified to obtain the saint venant equations furthermore there were many possible simplifications of the hydrodynamic model such as ignoring the inertia term in the equations of motion to obtain a diffusive wave or ignoring both the inertia and pressure terms to consider the effect of friction and bottom slope obtaining a kinematic wave diffusive and kinematic waves as well as other simplified forms of the hydrodynamic model could often efficiently approximate flow with sufficient precision and their simplified calculation was convenient for use in practical applications such simplifications accuracy lay between that of the hydrologic model and the pure hydrodynamic model making them suitable for either general modeling or emergency flood forecasting daganzo 2005 su et al 2019 to obtain accurate simulation results it was important to use high resolution digital elevation models dem or digital surface models dsm obtained through lidar measurements to describe complex urban geographic features however high resolution models require significant computing power and time the use of parallel computing technology offered a method to improve the calculation speed and efficiency of flood modeling neal et al 2010 compared three different types of parallelization openmp open multi processing which is an application programming interface api for multithreading on shared memory computing platforms mpi message passing interface kernel which is an api for passing data across distributed memory to enable parallel computing across different machines and gpu graphical processing unit the processor type that uses apis such as cuda to use the architectural advantages of this processor type for accelerated computation they determined that openmp was the easiest means of parallelizing the methods code in the flood models but could only work on a single host and could not be used for parallel computing between multiple hosts mpi provided excellent scalability when there was access to many cores but its performance was affected by the speed of the communication network ranging from simple ethernet to high speed interconnections for modern supercomputers and it was not easy to program the flood modeling code with gpu acceleration was faster and more power efficient than standard code on a single core but it took a great deal of time to develop thus each parallelization method had advantages and disadvantages see table 5 various researchers have proposed hybrid parallelization methods that combine the above methods as an effective solution for complex and detailed modeling conditions noh et al 2018 used a hybrid parallel code consisting of openmp and mpi to calculate shallow water equations and determined that the results were obtained more efficiently than when using openmp or mpi individually especially for super high resolution simulations dazzi et al 2018 proposed an efficient gpu acceleration method based on the local time stepping lts strategy this method was used to rapidly solve 2d swes on a non uniform structural mesh liang et al 2015 proposed a gpu accelerated sph model to solve the 2d swes sph swe with variable smooth lengths combined with the quadtrees neighbor search method this lagrangian meshless computing method was more efficient than a traditional mesh based parallel algorithm in future the smoothed particle hydrodynamics sph models should be extended to run on multiple gpus to improve their computational efficiency note that the reduction of computation time was achieved by either reducing complexity or increasing processing capabilities but improper application in either direction affected the model s accuracy therefore in practical applications researchers should ensure that any accelerated operations yielded results within the required accuracy range in this section we systematically summarized the methods to improve the numerical simulation model of urban flood from two aspects improving the accuracy of the model and speeding up the calculation speed the model s accuracy could be improved by reducing the numerical error reasonably selecting the type of mathematical model and applying data processing technology generally the model s running time was reduced by simplifying the control equation or applying acceleration technology to speed up the calculation speed model improvement research was crucial to the sustainable development of urban flood numerical simulation 7 future research directions to develop more accurate and widely applicable urban flood numerical simulation technology future research should focus on the following problems complex coupling between different flood water sources the complex relationships between the five types of water sources upstream runoff river and drainage overflow regional rainfall and tsunamis which may or may not be coupled additional factors had to be considered when modeling urban flooding caused by these different water sources and the coupling between these water sources made the task of calculating and verifying models more difficult therefore it is an essential future research direction to comprehensively and systematically discuss the formation mechanism of urban floods from large catchment areas particularly researchers should focus on the mechanism of urban road flooding which were the core means of alleviating urban flood disasters at present sponge city measures have been proposed and applied in flood mitigation luo et al 2022 zha et al 2021 coupling of hydrologic and hydrodynamic models hydrodynamic models were difficult to apply to flood simulations in large scale basins due to their high computational costs whereas hydrologic models were difficult to apply to flood simulations in complex areas owing to their less detailed physical processes for large scale watersheds with complex terrain coupling the hydrodynamic model with the hydrologic model could overcome the shortcomings of each modeling method to provide effective regional flood simulation while reducing calculation costs future research could be further developed from the aspects of model coupling mode structure efficiency and connectivity development of 3d models it was necessary to develop 3d models to capture the vertical features of sudden flood disasters including vertical turbulence vortexes or spiral flows such as tsunamis dam break mountain torrents etc note that the development of a 3d model required careful attention to calculating feasibility as well as issues such as free surface flow high order turbulence and the crossing of coastlines a particularly exciting field of 3d model development was the use of particle models these models have many advantages among others they can represent small scale features and do not need spatial discretization st germain et al 2014 developing particle based 3d flood models remains in the initial stages but they have excellent prospects for being applied to urban flood simulations additionally virtual reality technology as the back end application of the 3d model is also being gradually developed which could provide a more intuitive and realistic flood simulation process improvements of model accuracy and operation efficiency the need for faster and more accurate fluid solvers and quantifying and reducing uncertainty in the future urban flood modeling will be improved in terms of acceleration technology data processing methods and high precision input data meanwhile the understanding of urban hydrophysical processes need to be strengthened the related conclusions generalized and in depth mechanism research conducted in particular related theories and technologies such as surface water groundwater interaction processes 3d hydrodynamic models meshless sph swe models accelerating gpu and finite element methods need to be further developed acquiring field data to verify the model at present a relatively complete urban flood model could simulate the submergence range and water depth but when modeling real floods the lack of comprehensive field data made it difficult to verify these results at this stage a variety of emerging data collection technologies have been developed and applied the combined effect of rainfall monitoring stations remote sensing and radar technology could capture accurate rainfall information and provide data to support flood forecasting also ai technologies such as ml have been applied to predicting and modeling urban flooding for example the degree and depth of ponding during rainfall events were analyzed by collecting photos and using image processing technology at the ponding point in future emerging data collection and processing technologies will be developed to verify simulated urban floods and to provide support to predict urban floods 8 conclusion urban flooding has become increasingly frequent with ongoing economic development accelerating urbanization and climate change thus developing urban flood models has become an important issue in preventing floods and reducing disasters accordingly this review summarized the research and development of methods for the numerical simulation of urban flooding and is intended to provide a scientific basis for future research into methods for assessing urban flood risks early warning forecasts and suggesting policies the research on advancing flood models have contributed to reducing risk minimizing the loss of human life and reducing property damage associated with floods this review is summarized as a comparative study of various methods for the numerical simulation of urban floods was undertaken detailing numerical analysis methods for addressing urban street runoff building barriers underground pipe network flow modeling problems and feature analysis the hydrological modeling method is simple and fast but the accuracy is not high so it is suitable for emergency urban prediction the 1d 2d coupling model based on hydrodynamics has become one of the important choices of urban flood modeling because of its high precision urban flood simulation two directions for improving the model were identified increased accuracy and faster computation speed and the various factors influencing these objectives were compared and analyzed the 1d 2d coupling model finite volume method unstructured meshing method and hybrid parallel computing applications were highlighted as important options for urban flood modeling due to their applicability and mature theoretical development prospects for the future development of urban flood numerical simulation were proposed it was determined that future research will focus on high precision refinement multi process and emerging data in the future the most promising areas for further development include comprehensive hydro hydraulic modeling 3d hydrodynamics modeling sph swe modeling based on the grid etc declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgment this study is supported by national key r d program of china 2018yfe0103800 one hundred talent plan of shaanxi province international collaborative research of disaster prevention research institute of kyoto university 2019w 02 and general project of shaanxi provincial key r d program social development field 2021sf 454 many thanks is given to the reviewers for their great comments 
25532,fragmentation affects river ecosystems worldwide by dampening the movement and dispersal of aquatic organisms and material such as sediment water nutrients across the river network in this paper we develop river connectivity indices to explain biodiversity patterns prioritize reaches that need habitat restoration and barriers that need improvement we provide a general framework for calculating connectivity indices by disentangling the contribution of the river network s physical setup structural connectivity from the process driven and biota related contribution functional connectivity to facilitate the calculations the r package riverconn is introduced a prioritization of habitats and barriers is carried out for the ebro river north west iberian peninsula using indices setups accounting for different classes of organisms and dispersal traits resulting prioritizations are very diverse riverconn can support scientists and managers working on riverscape planning and population and community ecology by providing a means to compute and compare a wide array of fragmentation indices keywords river network fragmentation river network connectivity barriers improvement prioritization network analysis 1 introduction free flowing rivers host significant biodiversity he et al 2021 provide a wide array of ecosystem services such as water provisioning nutrient and sediment transport and support to fisheries harvests böck et al 2018 grill et al 2019 longitudinal river fragmentation is recognized as one of the major threats to freshwater systems worldwide dudgeon et al 2006 fullerton et al 2010 the causes of longitudinal fragmentation are multiple including the construction of barriers belletti et al 2020 duarte et al 2021 point source pollution araujo et al 2018 stream and riparian habitat modification baldan et al 2021 fuller et al 2015 and water withdrawals baumgartner et al 2022 longitudinally fragmented rivers are composed of successions of isolated sections where the free movement of water sediments organic matter nutrients energy and organisms is impeded jumani et al 2020 biotic impacts of fragmentation include the decline of migratory fish populations birnie gauvin et al 2017 genetic drifting of isolated populations inoue and berg 2017 and increased local extinction risks fagan 2002 thus when habitat restoration measures are implemented without adequate consideration to river network connectivity they might fail because of the inaccessibility of the recreated habitat brederveld et al 2011 river networks are particularly sensitive to connectivity losses even local disconnections can lead to large scale impacts due to the hierarchic and dendritic nature of such systems campbell grant et al 2007 duarte et al 2019 river network topologies display few possible connections for water mediated dispersal of organisms altermatt 2013 cañedo argüelles et al 2015 tonkin et al 2018 and increased distances between points along the network that are geographically close ver hoef and erin 2010 assessing the connectivity of river networks and its changes with increasing fragmentation has become a key task for scientists and practitioners recent works have developed a wide array of indices to quantify longitudinal fragmentation with different complexities jumani et al 2020 including the explicit modeling of the network structure of the landscape saura and pascual hortal 2007 even simple indices can describe the structural connectivity i e the set of all possible pathways linking different reaches in the landscape cote et al 2009 grill et al 2015 voutsa et al 2021 at multiple scales duarte et al 2021 this concept was expanded with the inclusion of functional connectivity i e the component of connectivity that can be explained by biotic factors such as organisms dispersal traits and mobility branco et al 2014 rodeles et al 2019 2021 both structural and functional connectivity indices have been used to explain biodiversity patterns in highly modified catchments perkin and gido 2012 prioritize habitat patches that need improvement saura et al 2014 saura and pascual hortal 2007 and or explain declines in fish populations barbarossa et al 2020 van puijenbroek et al 2019 some indices were developed to account explicitly for the directional nature of river systems for studies with anadromous fish rodeles et al 2019 asymmetric indices that account for the river network directionality can be used also for classes of organism with asymmetric dispersal patterns such as macroinvertebrates e g active aerial such as coleoptera or trichoptera in the adult phase or passive drifters such as gasteropoda or bivalva tachet et al 2000 empirical evidence of the impacts of fragmentation on macroinvertebrates exists cañedo argüelles et al 2020 monaghan et al 2005 wang et al 2019 but the use of connectivity indices for this class of organism is still overlooked moreover despite the large number of existing indices a unifying framework to examine and quantify river connectivity is still missing such a framework would support scientists and managers to perform an informed selection of the most suitable index for a specific task and to examine critically the implicit assumptions behind each index formulation also based on the decision trees presented in jumani et al 2020 to mitigate river fragmentation the permeability hence after passability of barriers can be improved to allow for the passage of at least some groups of organisms king et al 2017 this can be accomplished via structural modifications to the barrier e g with the construction of fishpasses to facilitate upstream movement and migration the removal of obsolete obstructions birnie gauvin et al 2020 and the improvement of the streambed or the riparian habitat existing studies have used different fragmentation indices to support such actions via the identification of barriers with the highest reconnection potential buddendorf et al 2019 the identification of optimal removal sequences branco et al 2014 including in the analysis also economic constrains king et al 2017 however tools and frameworks that include structural and functional information in barriers prioritization are still missing given the gaps identified above we propose an integrative framework that aims at i generalizing existing connectivity indices ii providing a straightforward parametrization of several structural and functional connectivity indices iii implementing algorithms for prioritization of barriers for passability improvement to this end we present riverconn an r package that allows for a flexible implementation of several graph based connectivity indices and implements algorithms to prioritize reaches and barriers with the aim of improving catchment scale connectivity as a proof of applicability we calculate and compare several riverconn indices to the ebro catchment ne spain 2 methods 2 1 conceptual background algorithms to estimate river network connectivity 2 1 1 river network as a graph following erős et al 2012 we conceptualize the river network as a graph g e v i e a collection of edges e and vertices v csardi and nepusz 2006 in the example provided in this paper edges or links represent either barriers or confluences between reaches while vertices or nodes represent reaches i e river sections having relatively uniform conditions reaches are river sections located either between two confluences or between two sequential barriers in this conceptualization the resulting network is a directed loop less graph whose root is the catchment outlet jumani et al 2020 an alternative characterization where confluences are vertices and reaches are edges is also possible borthagaray et al 2020 but might be of limited utility when multiple habitat patches within the same reach are considered in fact river networks can also be defined on a finer scale for instance on channels units like riffle pool sections or species specific habitat patches where vertices may represent habitat patches and links are elements representing the potential of dispersal between two habitat patches erős et al 2012 2 1 2 generalized connectivity index a generalized connectivity index for river networks can be defined based on pascual hortal and saura 2006 for every pair of reaches i and j in the catchment the dispersal probability i i j see eq 2 3 is defined as the probability that an organism originally located in reach i can disperse to reach j under the assumption of steady state behavior of the system the dependence from time can be dropped hence the dispersal probability is only a function of the path subgraph connecting nodes i and j based on the probability of connectivity saura and pascual hortal 2007 we propose a catchment level index of connectivity the catchment connectivity index cci defined as the weighted sum of the dispersal probability 2 1 c c i i 1 n j 1 n i i j w i w j w 2 where w i and w j are the weights of reaches i and j respectively and w the sum of the weights over the n reaches a reach level index of connectivity the reach connectivity index rci can be defined for reach i in the catchment by dropping the first summation in equation 2 1 2 2 r c i i j 1 n i i j w j w reaches weights can represent either abiotic properties such as length area and volume or biotic suitability scores dependent on a target organism such as the weighted useable length or area i e the length area of the reach weighted with habitat quality or any other variable of interest yi et al 2017 when directionality is included cci and rci can be further decomposed by considering separately connections entering or exiting each reach following rodeles et al 2021 we factorize the dispersal probability i into a component describing the structural connectivity the presence and arrangement of barriers and a component describing functional connectivity species dispersal 2 3 i i j c i j b i j where c i j depends exclusively on spatial configuration and number of barriers and b i j depends on the spatial configuration of reaches and the dispersal capacity of the species both cci and rci range between 0 and 1 catchments with higher cci values have higher levels of longitudinal connectivity meaning habitats are reachable more easily cote et al 2009 reaches with higher rci are more reachable from other reaches in the catchment cci or rci indices approaching 0 indicate a more fragmented riverscape 2 1 3 barriers fragmentation each barrier can be characterized with two passability parameters the upstream and downstream passability p u and p d respectively represent the probability that an organism crosses the barrier from downstream to upstream or from upstream to downstream if the directionality of the system is ignored the equivalent passability for the barrier m can be defined cote et al 2009 2 4 p m e q p m u p m d alternatively if the directionality of the system is considered the equivalent passability of the barrier m can be defined as 2 5 p m e q p m u i f m i s c r o s s e d w h i l e m o v i n g u p s t r e a m i n t h e i j p a t h p m d i f m i s c r o s s e d w h i l e m o v i n g d o w n s t r e a m i n t h e i j p a t h based on the barriers equivalent passability the barrier fragmentation index bfi can be defined as the weighted sum of the passabilities of all the r barriers in the catchment jumani et al 2022 2 6 b f i m 1 r 1 p m e q w m w where w m are the barriers weights bfi ranges from 0 to 1 with higher values indicating higher fragmentation the combined passability of all the barriers in the subgraph connecting reaches i and j results from the aggregation of the equivalent passabilities of all the k barriers located in the path cote et al 2009 2 7 c i j m 1 k p m e q 2 1 4 dispersal fragmentation the contribution of species dispersal to the dispersal probability is a function of the distance between each pair of reaches the effect of the directionality is accounted by calculating separately the dispersal for the fraction of the path between reaches i and j that proceeds upstream and downstream d i j u and d i j d are the total distances travelled upstream and downstream respectively the dependence of b i j fom the distance can be expressed through a dispersal kernel eqs 2 8 and 2 10 rodeles et al 2021 or through a threshold on the distance eqs 2 9 and 2 11 borthagaray et al 2015 asymmetric dispersal can be also accounted for eqs 2 10 and 2 11 2 8 b i j p d d i j 2 9 b i j 1 w h e n d i j t r 0 o t h e r w i s e 2 10 b i j p d u d i j u p d d d i j d 2 11 b i j 1 w h e n d i j u t r u a n d d i j d t r u 0 o t h e r w i s e where p d p d u and p d d are the dispersal kernel parameters dependent on the mobility of the organism closer to 1 for highly mobile organisms and t r t r u and t r d are the dispersal thresholds a value of p d u 0 or t r u 0 corresponds to organisms that are passive drifters and cannot move upstream e g gasteropoda or bivalva the distance d i j can be both a geometric distance or a landscape friction weighted distance in the latter case the inverse of the reach scale habitat suitability score can be used as a multiplicative factor to simulate higher friction of the landscape inoue and berg 2017 then the effective distance would coincide with the geometric distance only when the whole habitat of the reach is suitable being bigger otherwise 2 1 5 barriers prioritization both cci and rci can be used to prioritize barriers based on the relative contribution to landscape fragmentation a leave one out approach is commonly used saura and pascual hortal 2007 2 12 d c c i m c c i s t a r t m c c i s t a r t c c i s t a r t 100 where c c i s t a r t is the value of the index calculated with the current setup and c c i s t a r t m is the value of the index calculated after removing the barrier m an analogous index can be defined for rci and bfi values of d c c i m d r c i m and d b f i m range between 0 when the barrier m has no effect over fragmentation and infinite when the landscape is fully fragmented cci 0 and all the landscape fragmentation can be attributed to m rankings for d c c i m and d r c i m can be used to select barriers based on the contribution to landscape fragmentation the same approach can be used also to calculate the connectivity gain when a certain sequence of barriers is removed in a leave many out approach 2 1 6 comparison and integration with existing indices and software the cci can be easily parametrized to cover a wide array of already existing indices table 1 if the reach length is used as weights no dispersal limitation is considered and symmetric organisms movement the dendritic connectivity index is obtained cote et al 2009 using the reach area instead of the length yields the probability of connectivity index saura and pascual hortal 2007 under the same settings including the dispersal limitation term yields the population connectivity index rodeles et al 2021 using the river impoundment volume yields the river fragmentation index grill et al 2015 using binary dispersal probabilities e g setting a distance threshold on dispersal and a binary barrier passability yields the integral index of connectivity saura and pascual hortal 2007 using the stream order as nodes weights yields the stream continuity index shao et al 2020 calculating rci based on the catchment outlet yields the breeding habitat connectivity index for anadromous fish rodeles et al 2019 using the weighted suitable length as reach weight imposing no constrain to biotic movement and setting barriers passabilities to zero yields the residual core length fuller et al 2015 recently proposed indices such as the catchment area fragmentation index and the catchment area and rainfall fragmentation index can be also calculated based on the bfi specifying the upstream catchment area or annual rainfall as barrier weight jumani et al 2022 using upstream cumulative statistics as node weights can be facilitated by a wide array of software packages e g the river network toolkit duarte et al 2019 the graph structure allows for the calculation of centrality metrics such as the betweeness centrality bodin and saura 2010 via routines implemented in the igraph r package other software packages provide graph based calculations of connectivity index for instance conefor bodin and saura 2010 pascual hortal and saura 2006 implements several connectivity indices and habitat patches prioritization in directional landscapes but requires the explicit definition of the dispersal probabilities for cases different than simple dispersal kernels while riverconn allows for the automatic calculation of the dispersal probability the software fipex oldford 2020 allows for the calculation of the dendritic connectivity index but it does not provide the possibility of defining the functional component of the dispersal probability while riverconn does 2 2 riverconn package features the riverconn package relies on the functionalities of the igraph package csardi and nepusz 2006 all riverconn functions are designed to accept as input an igraph object which can be easily created in r either from a list of vertices and edges from an adjacency matrix kolaczyk and csárdi 2014 or directly from a shapefile through the package shp2graph lu et al 2018 the riverconn documentation shows a general workflow to generate an igraph object based on commonly available geospatial datasets the function set graph directionality assigns the directionality to the graph based on the outlet position the main function of the package is index calculation table 2 that calculates rci cci or bfi equations 2 1 2 2 and 2 6 based on the input graph and its attributes under a variety of settings decided by the user for instance the functional and the structural connectivity terms can be selected or dropped from the calculations the way directionality is dealt with can be specified the dispersal functions and related parametrizations can be defined functions that allow to separately calculate c i j and b i j are also exported from the package for diagnostic purposes c ij fun for equation 2 6 and b ij fun for equations 2 7 and 2 8 11 network based calculations e g the identification of the shortest path between each pair of reaches are performed using the highly efficient routines from the package dodgr padgham 2019 even though the index calculation function was designed with the directed loop less graph in mind different structures such as lattice like networks can also be used the function d index calculation calculates the improvement in the connectvitiy index when selected barriers are removed equation 2 9 the function t index calculation calculates the index change when the graph attributes are sequentially modified e g to simulate some time dynamics the functions t weights sequencer and t passability sequencer are provided to generate the metadata needed temporal changes of nodes weights or barriers passabilities based on simple objects of class data frame the function d index calculation can be resource consuming for large networks the calculation of the dcci indices for the case study in this paper network size 650 reaches took approximately 4 min on a single core of a laptop equipped with 11th gen intel core i7 3 30 ghz and 32 gb ram the option to parallelize calculations on multiple cores is also available 2 3 application of riverconn to the ebro river we used the ebro river ne iberian peninsula as a case study for implementing the model we used the hydrosheds river network for the ebro catchment fig 1 lehner and processes 2013 to limit the size of the graph we retained only those sections of the river network having an upstream area greater than 100 km2 we downloaded barriers shapefiles from the confederación hidrografica del ebro website https www chebro es and snapped maximum snapping distance 1 km them to the pruned river network fig 1 we inspected visually the results to ensure snapped barriers were retained in the proper sub catchment due to the pruning of the network 97 barriers out of the 224 in the inventory were retained for the analysis based on the river network and the barriers data we generated a graph using the package igraph we included the length of each reach units 10 km and its elevation obtained from a digital elevation model and the elevatr package units m a m s l as edges attributes we set the upstream and downstream passabilities of barriers to 0 1 and 0 8 respectively adding them to the graph as vertex attributes for those vertices categorized as barriers we set the passability of confluences to 1 the size of the resulting graph was 650 edges reaches we used two univariate theoretical habitat suitability curves based on elevation of each reach to account for different species tolerances fig 1 we defined habitat suitability scores for an organism preferring low elevation streams and for an organism preferring higher elevation streams finally we used the habitat suitability scores to calculate the weighted useable length of each reach edge fig 1 a step by step tutorial is available at https damianobaldan github io riverconn tutorial we calculated the rci each reach function index calculation and dcci for each barrier function d index calculation in the determination of the directionality for i i j in rci we used outbound connections for each reach both outputs were used to prioritize reaches and dams for restoration we interpret the rci as the potential of each reach to act as dispersal source for the colonization of the river network high ranked reaches rank one is the highest have high potential for restoration we interpret the dcci as an index prioritizing those barriers whose removal or improvement would lead to highest increases in cci thus barriers that are ranked higher rank one is the highest yield higher improvements in catchment connectivity pascual hortal and saura 2006 we repeated both assessments for 8 different setups tables 1 and 3 a dendritic connectivity index with symmetric dams passabilities b dendritic connectivity index with asymmetric dams passabilities c integral index of connectivity dams are not passable and a threshold on dispersal distance is used d integral index of connectivity with uniform reach weights equal to one e lowland species with active aquatic dispersal fish f upland species with active aquatic dispersal fish g lowland species with passive aquatic dispersal invertebrate larval stage h upland species with passive aquatic dispersal invertebrate larval stage bivalve additonally for dcci we calculated also i catchment area fragmentation index we plotted the spatial distribution of reaches and barriers rankings and compared the prioritization using spearman s rank coefficient 3 results obtained values for rci range between 5 10 5 and 0 38 with higher values for dci like indices and lower values for pci like indices the spatial distribution of rci differs for the different setups fig 2 the symmetric dci setup is higher for reaches located in the downstream sections of the catchment fig 2a while the asymmetric setup has higher values for reaches located in the mid to upstream sections fig 2b the iic setup weights more reaches located in the main stem and in the main channels fig 2c and d the pci setup for lowland fish has higher values for few sections in the main stem fig 2e while the setup for upland preferring fish has higher values for headwater sections in the southern and northern parts of the catchment fig 2f the pci setup for lowland preferring passive drifter weights uniformly the downstream sections of the river fig 2g while the setup for the upland passive drifter weights more the headwaters fig 2h obtained values for cci and bfi ranged between 0 005 and 0 29 with higher values for dci like indices and cafi and lower values for pci like indices the spatial distribution of prioritized barriers varies with the cci index setup fig 3 the dci setups both symmetric and asymmetric assign high priorities to barriers located in the mid section of the main stem of the ebro river fig 3a and b the iic setup prioritizes barriers located in the main stem fig 3c while the setup with uniform weights prioritizes more upstream barriers fig 3d the pci setup for lowland preferring fish has higher values for few sections in the main stem fig 3e while the setup for upland fish has higher values for headwater sections in the southern and northern parts of the catchment fig 3f the pci setup for lowland preferring passive drifter weights uniformly the downstream sections of the river fig 3g while the setup for the upland passive drifter weights more the headwaters fig 3h the cafi prioritizes the most downstream barriers fig 3i the correlation of both rci and the barriers prioritization is highly variable for rci the spearman s correlation ranges from negative values 0 46 for scenarios e h to high congruence 0 89 for scenarios a e fig 4 for the barrier s prioritization with dcci spearman s correlation ranges between low congruence 0 15 for e h to high congruence 0 94 for a e fig 5 there seems to be a partial overlap between rci and dcci as barriers located in reaches with higher rci are also ranked higher in the prioritization 4 discussion 4 1 use of the package for population ecology restoring river connectivity is increasingly considered as an option to improve the health of freshwater populations and the resilience of metapopulations tickner et al 2020 beneficial impacts of connectivity restoration have been documented for many fish species birnie gauvin et al 2020 king et al 2017 sun et al 2022 however development of general connectivity biota causal links to inform restoration planning is complicated because of species specific dispersal and movement traits baldan et al 2020 for instance the response of organisms with limited mobility to increased connectivity might be limited in this regard the analysis of historical connectivity losses and the related effects on biota can be beneficial hall et al 2011 mattocks et al 2017 historic populations data can be reconstructed from public datasets e g rivfishtime carvajal quintero et al 2021 while qualitative methods can be used where data are not available duarte et al 2022 historic connectivity biota relationships can be used to validate connectivity indices especially when developed on neighbouring catchments with different degree of fragmentation the establishment of connectivity biota causal pathways and the validation of connectivity metrics can then be used to support population management schick and lindley 2007 functional and structural connectivity are influenced by seasonal to multiannual dynamics fullerton et al 2010 pont et al 2015 including the spatial structure of the river network orientation properties and modes of operation of barriers species traits and spatiotemporal scales considered climate change can affect the structure of the mesohabitat and indirectly affect the dispersal potential of organisms baldan et al 2021 holyoak et al 2020 effects can be even more drastic in rivers experiencing intermittency datry et al 2014 jaeger et al 2014 thus time dependent estimates of connectivity indices might be needed to fully capture the dynamics of the system if the relaxation time of the system i e the time the system needs to reach steady state is smaller than the time span considered castillo escrivà et al 2020 the problem can be considered by a sequence of steady states with the static equations implemented in riverconn still adequate in this case connectivity indices can be calculated for each state of the system separately and synthetic indicators can be developed to represent the connectivity aspects cid et al 2020 for highly dynamic systems such as small sized catchments and or highly intermittent different methodological approaches such as individual based models or high frequency monitoring pineda morante et al 2022 might be needed to better capture spatial or temporal connectivity patterns at the expense of higher data collection efforts and computational capacities in these cases connectivity patterns and barriers distribution might also be understood differently as fragmentation by drought can strongly vary in relation with river morphology drought intensity or riffle and pool distribution cañedo argüelles et al 2020 sarremejane et al 2017 the use of actual observation based connectivity metrics can also be feasible for such smaller scales applications jumani et al 2020 the riverconn package can be used for calculating several connectivity indices and their temporal development and therefore can be of great support for population level analyses at the moment riverconn implements only equations for calculating estimates of longitudinal connectivity however other dimensions of connectivity exist cañedo argüelles et al 2015 grill et al 2019 tonkin et al 2018 lateral connectivity river floodplains vertical connectivity groundwater river atmosphere and temporal connectivity constancy of the flow the implementation of lateral and vertical connectivity would require a modification of the base conceptualization of the river network as a directed loopless tree towards patch based spatial graphs erős et al 2012 flow related dependencies the temporal connectivity aspect could still be included in riverconn by quantifying the reach or meshohabitat scale as a function of flow characteristics kakouei et al 2018 in this regard indicators of hydrological alteration are a useful set of metrics to synthetize a wide range of hydrographs characteristics in scalar properties for spatial analyses olden and poff 2003 4 2 use of the package for community ecology in fragmented landscapes organism dispersal plays a key role in determining the local community structure borthagaray et al 2015 cañedo argüelles et al 2015 connectivity defined as the degree to which the landscape facilitates the movement of individuals among habitat patches economo and keitt 2010 uroy et al 2021 is a key factor affecting dispersal connectivity determined by landscape structure i e the spatial arrangement of communities and species dispersal strategies is widely used to assess communities isolation borthagaray et al 2020 horváth et al 2019 pineda morante et al 2022 accordingly metacommunity theory predicts local richness and diversity to be highly influenced by the degree of isolation of the community heino et al 2015 tonkin et al 2018 landscape fragmentation is a factor influencing the structuring of freshwater communities díaz et al 2021 horváth et al 2019 perkin and gido 2012 thus deriving metrics describing communities centrality and isolation is a key task in current community ecology pineda morante et al 2022 recent development allowed to account for the directional and dendritic nature of the river network peterson et al 2013 but still much work is needed to include longitudinal barriers indices as spatial covariates of aquatic communities wang et al 2019 the rci can be computed for each reach where community data is available and can be used as a predictor in multivariate community analysis the effect of fragmentation on biotic communities is expected to be mediated by the spatial scales and extents analyzed mahlum et al 2014 furthermore the use of such metrics could complement spatial covariates based on euclidean distance river based distance and flow connected distance ver hoef and erin 2010 in a variance partitioning algorithm to quantify the effect of longitudinal fragmentation on biotic communities and beta diversity cañedo argüelles et al 2020 schmera et al 2018 consequently the implementation of connectivity indices for community level studies would contribute to better quantify barrier impacts at higher organizational levels and across different organisms 4 3 use of the package to support riverscape planning our results show how the use of different connectivity indices based on different assumptions on the calculation of structural and functional connectivity can lead to diverse outcomes in the prioritization of reaches and barriers the reaches prioritization based on the dci has higher values for section of the catchment where few barriers exist located more upstream when directionality is accounted for accordingly barriers whose removal would increase the connected length are prioritized jumani et al 2022 the dci is length based and therefore it is supposed to be sensitive to the river network cropping therefore the cropping of the headwater reaches might result in overlooking the connectedness of the headwaters to the river network the cafi partially solves this issue by using area based weights less sensitive to river network delineation jumani et al 2022 however cafi is not a network based index thus it might neglect interactions between barriers placement that are accounted for in dci and pci cote et al 2009 rodeles et al 2021 the package riverconn can explicitly consider the assumptions underlying the definition of each connectivity index improving the transparency of the prioritization process multi criteria analysis has the potential to generate compromise indices based on a large set of indices calculated under different setups further optimization algorithms can be used to identify optimal barriers or habitat improvement sequences under economic constraints king et al 2017 o hanley 2011 the functions implemented in riverconn can be used to support the delineation of relevant sub catchments for conservation planning moilanen et al 2008 methods for prioritizing sub catchment for freshwater biodiversity conservation based on optimization algorithms deal with connectivity with simplistic approaches such as distance penalties beger et al 2010 or the unobstructed distance upstream a barrier mcmanamay et al 2019 modeling frameworks that include graph based connectivity were recently proposed erős et al 2018 the use of a wide array of connectivity indices can better incorporate multiple connectivity facets into freshwater conservation planning by explicitly including in the models organism specific dispersal traits rodeles et al 2021 planning and implementation of conservation actions such as habitat improvement is also carried out at the meso habitat scale 1 100 m wegscheider et al 2020 in our graph based conceptualization of the river network we defined a node as a whole reach this definition allows to consider only attributes averaged over a reach and might not be enough when the analysis seeks to follow restoration actions occurring at finer scales however any reach of interest could be further split in a sequence of nodes corresponding to sequences of different mesohabitats erős et al 2012 therefore riverconn conceptualization of the river can be adapted to the specific study case and scale allowing for fragmentation assessments by explicitly considering different scales for instance first a broad assessment can be conducted at the catchment scale then a detailed analysis on a smaller area of interest for instance a dammed reach and the surrounding floodplains can be performed erős et al 2012 5 conclusions in this paper we re conceptualized many indices used to assess river network fragmentation under a unifying framework and we presented an r package riverconn that allows for the calculation of such indices based on a graph based landscape conceptualization and widely available geospatial data functions implemented in the package can support among others i conservation planning via the identification and prioritization of barriers and habitats for connectivity improvement at the catchment scale ii population ecology via the possibility to analyze historic patterns of fragmentation and iii community ecology offering the possibility to generate barriers dependent spatial covariates for community analysis software availability name of the software riverconn developer damiano baldan aut cre david cunillera montcusì ctb andrea funk ctb contact information damiano baldan91 gmail com year first available 2022 program language r cost free software availability https cran r project org package riverconn last stable version https github com damianobaldan riverconn development version program size 1 93 mb declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements db and af acknowledge support from the christian doppler society cd laboratory for meta ecosystem dynamics in riverine landscapes dcm was supported by the mecodisper project ctm 2017 89295 p funded by the spanish ministerio de economía industria y competitividad mineco agencia estatal de investigación aei and co funded by the european regional development fund erdf and by european union nextgenerationeu ministry of universities and recovery transformation and resilience plan through a call from universitat de girona we are thankful to the two anonymous reviewers whose comments improved the manuscript 
25532,fragmentation affects river ecosystems worldwide by dampening the movement and dispersal of aquatic organisms and material such as sediment water nutrients across the river network in this paper we develop river connectivity indices to explain biodiversity patterns prioritize reaches that need habitat restoration and barriers that need improvement we provide a general framework for calculating connectivity indices by disentangling the contribution of the river network s physical setup structural connectivity from the process driven and biota related contribution functional connectivity to facilitate the calculations the r package riverconn is introduced a prioritization of habitats and barriers is carried out for the ebro river north west iberian peninsula using indices setups accounting for different classes of organisms and dispersal traits resulting prioritizations are very diverse riverconn can support scientists and managers working on riverscape planning and population and community ecology by providing a means to compute and compare a wide array of fragmentation indices keywords river network fragmentation river network connectivity barriers improvement prioritization network analysis 1 introduction free flowing rivers host significant biodiversity he et al 2021 provide a wide array of ecosystem services such as water provisioning nutrient and sediment transport and support to fisheries harvests böck et al 2018 grill et al 2019 longitudinal river fragmentation is recognized as one of the major threats to freshwater systems worldwide dudgeon et al 2006 fullerton et al 2010 the causes of longitudinal fragmentation are multiple including the construction of barriers belletti et al 2020 duarte et al 2021 point source pollution araujo et al 2018 stream and riparian habitat modification baldan et al 2021 fuller et al 2015 and water withdrawals baumgartner et al 2022 longitudinally fragmented rivers are composed of successions of isolated sections where the free movement of water sediments organic matter nutrients energy and organisms is impeded jumani et al 2020 biotic impacts of fragmentation include the decline of migratory fish populations birnie gauvin et al 2017 genetic drifting of isolated populations inoue and berg 2017 and increased local extinction risks fagan 2002 thus when habitat restoration measures are implemented without adequate consideration to river network connectivity they might fail because of the inaccessibility of the recreated habitat brederveld et al 2011 river networks are particularly sensitive to connectivity losses even local disconnections can lead to large scale impacts due to the hierarchic and dendritic nature of such systems campbell grant et al 2007 duarte et al 2019 river network topologies display few possible connections for water mediated dispersal of organisms altermatt 2013 cañedo argüelles et al 2015 tonkin et al 2018 and increased distances between points along the network that are geographically close ver hoef and erin 2010 assessing the connectivity of river networks and its changes with increasing fragmentation has become a key task for scientists and practitioners recent works have developed a wide array of indices to quantify longitudinal fragmentation with different complexities jumani et al 2020 including the explicit modeling of the network structure of the landscape saura and pascual hortal 2007 even simple indices can describe the structural connectivity i e the set of all possible pathways linking different reaches in the landscape cote et al 2009 grill et al 2015 voutsa et al 2021 at multiple scales duarte et al 2021 this concept was expanded with the inclusion of functional connectivity i e the component of connectivity that can be explained by biotic factors such as organisms dispersal traits and mobility branco et al 2014 rodeles et al 2019 2021 both structural and functional connectivity indices have been used to explain biodiversity patterns in highly modified catchments perkin and gido 2012 prioritize habitat patches that need improvement saura et al 2014 saura and pascual hortal 2007 and or explain declines in fish populations barbarossa et al 2020 van puijenbroek et al 2019 some indices were developed to account explicitly for the directional nature of river systems for studies with anadromous fish rodeles et al 2019 asymmetric indices that account for the river network directionality can be used also for classes of organism with asymmetric dispersal patterns such as macroinvertebrates e g active aerial such as coleoptera or trichoptera in the adult phase or passive drifters such as gasteropoda or bivalva tachet et al 2000 empirical evidence of the impacts of fragmentation on macroinvertebrates exists cañedo argüelles et al 2020 monaghan et al 2005 wang et al 2019 but the use of connectivity indices for this class of organism is still overlooked moreover despite the large number of existing indices a unifying framework to examine and quantify river connectivity is still missing such a framework would support scientists and managers to perform an informed selection of the most suitable index for a specific task and to examine critically the implicit assumptions behind each index formulation also based on the decision trees presented in jumani et al 2020 to mitigate river fragmentation the permeability hence after passability of barriers can be improved to allow for the passage of at least some groups of organisms king et al 2017 this can be accomplished via structural modifications to the barrier e g with the construction of fishpasses to facilitate upstream movement and migration the removal of obsolete obstructions birnie gauvin et al 2020 and the improvement of the streambed or the riparian habitat existing studies have used different fragmentation indices to support such actions via the identification of barriers with the highest reconnection potential buddendorf et al 2019 the identification of optimal removal sequences branco et al 2014 including in the analysis also economic constrains king et al 2017 however tools and frameworks that include structural and functional information in barriers prioritization are still missing given the gaps identified above we propose an integrative framework that aims at i generalizing existing connectivity indices ii providing a straightforward parametrization of several structural and functional connectivity indices iii implementing algorithms for prioritization of barriers for passability improvement to this end we present riverconn an r package that allows for a flexible implementation of several graph based connectivity indices and implements algorithms to prioritize reaches and barriers with the aim of improving catchment scale connectivity as a proof of applicability we calculate and compare several riverconn indices to the ebro catchment ne spain 2 methods 2 1 conceptual background algorithms to estimate river network connectivity 2 1 1 river network as a graph following erős et al 2012 we conceptualize the river network as a graph g e v i e a collection of edges e and vertices v csardi and nepusz 2006 in the example provided in this paper edges or links represent either barriers or confluences between reaches while vertices or nodes represent reaches i e river sections having relatively uniform conditions reaches are river sections located either between two confluences or between two sequential barriers in this conceptualization the resulting network is a directed loop less graph whose root is the catchment outlet jumani et al 2020 an alternative characterization where confluences are vertices and reaches are edges is also possible borthagaray et al 2020 but might be of limited utility when multiple habitat patches within the same reach are considered in fact river networks can also be defined on a finer scale for instance on channels units like riffle pool sections or species specific habitat patches where vertices may represent habitat patches and links are elements representing the potential of dispersal between two habitat patches erős et al 2012 2 1 2 generalized connectivity index a generalized connectivity index for river networks can be defined based on pascual hortal and saura 2006 for every pair of reaches i and j in the catchment the dispersal probability i i j see eq 2 3 is defined as the probability that an organism originally located in reach i can disperse to reach j under the assumption of steady state behavior of the system the dependence from time can be dropped hence the dispersal probability is only a function of the path subgraph connecting nodes i and j based on the probability of connectivity saura and pascual hortal 2007 we propose a catchment level index of connectivity the catchment connectivity index cci defined as the weighted sum of the dispersal probability 2 1 c c i i 1 n j 1 n i i j w i w j w 2 where w i and w j are the weights of reaches i and j respectively and w the sum of the weights over the n reaches a reach level index of connectivity the reach connectivity index rci can be defined for reach i in the catchment by dropping the first summation in equation 2 1 2 2 r c i i j 1 n i i j w j w reaches weights can represent either abiotic properties such as length area and volume or biotic suitability scores dependent on a target organism such as the weighted useable length or area i e the length area of the reach weighted with habitat quality or any other variable of interest yi et al 2017 when directionality is included cci and rci can be further decomposed by considering separately connections entering or exiting each reach following rodeles et al 2021 we factorize the dispersal probability i into a component describing the structural connectivity the presence and arrangement of barriers and a component describing functional connectivity species dispersal 2 3 i i j c i j b i j where c i j depends exclusively on spatial configuration and number of barriers and b i j depends on the spatial configuration of reaches and the dispersal capacity of the species both cci and rci range between 0 and 1 catchments with higher cci values have higher levels of longitudinal connectivity meaning habitats are reachable more easily cote et al 2009 reaches with higher rci are more reachable from other reaches in the catchment cci or rci indices approaching 0 indicate a more fragmented riverscape 2 1 3 barriers fragmentation each barrier can be characterized with two passability parameters the upstream and downstream passability p u and p d respectively represent the probability that an organism crosses the barrier from downstream to upstream or from upstream to downstream if the directionality of the system is ignored the equivalent passability for the barrier m can be defined cote et al 2009 2 4 p m e q p m u p m d alternatively if the directionality of the system is considered the equivalent passability of the barrier m can be defined as 2 5 p m e q p m u i f m i s c r o s s e d w h i l e m o v i n g u p s t r e a m i n t h e i j p a t h p m d i f m i s c r o s s e d w h i l e m o v i n g d o w n s t r e a m i n t h e i j p a t h based on the barriers equivalent passability the barrier fragmentation index bfi can be defined as the weighted sum of the passabilities of all the r barriers in the catchment jumani et al 2022 2 6 b f i m 1 r 1 p m e q w m w where w m are the barriers weights bfi ranges from 0 to 1 with higher values indicating higher fragmentation the combined passability of all the barriers in the subgraph connecting reaches i and j results from the aggregation of the equivalent passabilities of all the k barriers located in the path cote et al 2009 2 7 c i j m 1 k p m e q 2 1 4 dispersal fragmentation the contribution of species dispersal to the dispersal probability is a function of the distance between each pair of reaches the effect of the directionality is accounted by calculating separately the dispersal for the fraction of the path between reaches i and j that proceeds upstream and downstream d i j u and d i j d are the total distances travelled upstream and downstream respectively the dependence of b i j fom the distance can be expressed through a dispersal kernel eqs 2 8 and 2 10 rodeles et al 2021 or through a threshold on the distance eqs 2 9 and 2 11 borthagaray et al 2015 asymmetric dispersal can be also accounted for eqs 2 10 and 2 11 2 8 b i j p d d i j 2 9 b i j 1 w h e n d i j t r 0 o t h e r w i s e 2 10 b i j p d u d i j u p d d d i j d 2 11 b i j 1 w h e n d i j u t r u a n d d i j d t r u 0 o t h e r w i s e where p d p d u and p d d are the dispersal kernel parameters dependent on the mobility of the organism closer to 1 for highly mobile organisms and t r t r u and t r d are the dispersal thresholds a value of p d u 0 or t r u 0 corresponds to organisms that are passive drifters and cannot move upstream e g gasteropoda or bivalva the distance d i j can be both a geometric distance or a landscape friction weighted distance in the latter case the inverse of the reach scale habitat suitability score can be used as a multiplicative factor to simulate higher friction of the landscape inoue and berg 2017 then the effective distance would coincide with the geometric distance only when the whole habitat of the reach is suitable being bigger otherwise 2 1 5 barriers prioritization both cci and rci can be used to prioritize barriers based on the relative contribution to landscape fragmentation a leave one out approach is commonly used saura and pascual hortal 2007 2 12 d c c i m c c i s t a r t m c c i s t a r t c c i s t a r t 100 where c c i s t a r t is the value of the index calculated with the current setup and c c i s t a r t m is the value of the index calculated after removing the barrier m an analogous index can be defined for rci and bfi values of d c c i m d r c i m and d b f i m range between 0 when the barrier m has no effect over fragmentation and infinite when the landscape is fully fragmented cci 0 and all the landscape fragmentation can be attributed to m rankings for d c c i m and d r c i m can be used to select barriers based on the contribution to landscape fragmentation the same approach can be used also to calculate the connectivity gain when a certain sequence of barriers is removed in a leave many out approach 2 1 6 comparison and integration with existing indices and software the cci can be easily parametrized to cover a wide array of already existing indices table 1 if the reach length is used as weights no dispersal limitation is considered and symmetric organisms movement the dendritic connectivity index is obtained cote et al 2009 using the reach area instead of the length yields the probability of connectivity index saura and pascual hortal 2007 under the same settings including the dispersal limitation term yields the population connectivity index rodeles et al 2021 using the river impoundment volume yields the river fragmentation index grill et al 2015 using binary dispersal probabilities e g setting a distance threshold on dispersal and a binary barrier passability yields the integral index of connectivity saura and pascual hortal 2007 using the stream order as nodes weights yields the stream continuity index shao et al 2020 calculating rci based on the catchment outlet yields the breeding habitat connectivity index for anadromous fish rodeles et al 2019 using the weighted suitable length as reach weight imposing no constrain to biotic movement and setting barriers passabilities to zero yields the residual core length fuller et al 2015 recently proposed indices such as the catchment area fragmentation index and the catchment area and rainfall fragmentation index can be also calculated based on the bfi specifying the upstream catchment area or annual rainfall as barrier weight jumani et al 2022 using upstream cumulative statistics as node weights can be facilitated by a wide array of software packages e g the river network toolkit duarte et al 2019 the graph structure allows for the calculation of centrality metrics such as the betweeness centrality bodin and saura 2010 via routines implemented in the igraph r package other software packages provide graph based calculations of connectivity index for instance conefor bodin and saura 2010 pascual hortal and saura 2006 implements several connectivity indices and habitat patches prioritization in directional landscapes but requires the explicit definition of the dispersal probabilities for cases different than simple dispersal kernels while riverconn allows for the automatic calculation of the dispersal probability the software fipex oldford 2020 allows for the calculation of the dendritic connectivity index but it does not provide the possibility of defining the functional component of the dispersal probability while riverconn does 2 2 riverconn package features the riverconn package relies on the functionalities of the igraph package csardi and nepusz 2006 all riverconn functions are designed to accept as input an igraph object which can be easily created in r either from a list of vertices and edges from an adjacency matrix kolaczyk and csárdi 2014 or directly from a shapefile through the package shp2graph lu et al 2018 the riverconn documentation shows a general workflow to generate an igraph object based on commonly available geospatial datasets the function set graph directionality assigns the directionality to the graph based on the outlet position the main function of the package is index calculation table 2 that calculates rci cci or bfi equations 2 1 2 2 and 2 6 based on the input graph and its attributes under a variety of settings decided by the user for instance the functional and the structural connectivity terms can be selected or dropped from the calculations the way directionality is dealt with can be specified the dispersal functions and related parametrizations can be defined functions that allow to separately calculate c i j and b i j are also exported from the package for diagnostic purposes c ij fun for equation 2 6 and b ij fun for equations 2 7 and 2 8 11 network based calculations e g the identification of the shortest path between each pair of reaches are performed using the highly efficient routines from the package dodgr padgham 2019 even though the index calculation function was designed with the directed loop less graph in mind different structures such as lattice like networks can also be used the function d index calculation calculates the improvement in the connectvitiy index when selected barriers are removed equation 2 9 the function t index calculation calculates the index change when the graph attributes are sequentially modified e g to simulate some time dynamics the functions t weights sequencer and t passability sequencer are provided to generate the metadata needed temporal changes of nodes weights or barriers passabilities based on simple objects of class data frame the function d index calculation can be resource consuming for large networks the calculation of the dcci indices for the case study in this paper network size 650 reaches took approximately 4 min on a single core of a laptop equipped with 11th gen intel core i7 3 30 ghz and 32 gb ram the option to parallelize calculations on multiple cores is also available 2 3 application of riverconn to the ebro river we used the ebro river ne iberian peninsula as a case study for implementing the model we used the hydrosheds river network for the ebro catchment fig 1 lehner and processes 2013 to limit the size of the graph we retained only those sections of the river network having an upstream area greater than 100 km2 we downloaded barriers shapefiles from the confederación hidrografica del ebro website https www chebro es and snapped maximum snapping distance 1 km them to the pruned river network fig 1 we inspected visually the results to ensure snapped barriers were retained in the proper sub catchment due to the pruning of the network 97 barriers out of the 224 in the inventory were retained for the analysis based on the river network and the barriers data we generated a graph using the package igraph we included the length of each reach units 10 km and its elevation obtained from a digital elevation model and the elevatr package units m a m s l as edges attributes we set the upstream and downstream passabilities of barriers to 0 1 and 0 8 respectively adding them to the graph as vertex attributes for those vertices categorized as barriers we set the passability of confluences to 1 the size of the resulting graph was 650 edges reaches we used two univariate theoretical habitat suitability curves based on elevation of each reach to account for different species tolerances fig 1 we defined habitat suitability scores for an organism preferring low elevation streams and for an organism preferring higher elevation streams finally we used the habitat suitability scores to calculate the weighted useable length of each reach edge fig 1 a step by step tutorial is available at https damianobaldan github io riverconn tutorial we calculated the rci each reach function index calculation and dcci for each barrier function d index calculation in the determination of the directionality for i i j in rci we used outbound connections for each reach both outputs were used to prioritize reaches and dams for restoration we interpret the rci as the potential of each reach to act as dispersal source for the colonization of the river network high ranked reaches rank one is the highest have high potential for restoration we interpret the dcci as an index prioritizing those barriers whose removal or improvement would lead to highest increases in cci thus barriers that are ranked higher rank one is the highest yield higher improvements in catchment connectivity pascual hortal and saura 2006 we repeated both assessments for 8 different setups tables 1 and 3 a dendritic connectivity index with symmetric dams passabilities b dendritic connectivity index with asymmetric dams passabilities c integral index of connectivity dams are not passable and a threshold on dispersal distance is used d integral index of connectivity with uniform reach weights equal to one e lowland species with active aquatic dispersal fish f upland species with active aquatic dispersal fish g lowland species with passive aquatic dispersal invertebrate larval stage h upland species with passive aquatic dispersal invertebrate larval stage bivalve additonally for dcci we calculated also i catchment area fragmentation index we plotted the spatial distribution of reaches and barriers rankings and compared the prioritization using spearman s rank coefficient 3 results obtained values for rci range between 5 10 5 and 0 38 with higher values for dci like indices and lower values for pci like indices the spatial distribution of rci differs for the different setups fig 2 the symmetric dci setup is higher for reaches located in the downstream sections of the catchment fig 2a while the asymmetric setup has higher values for reaches located in the mid to upstream sections fig 2b the iic setup weights more reaches located in the main stem and in the main channels fig 2c and d the pci setup for lowland fish has higher values for few sections in the main stem fig 2e while the setup for upland preferring fish has higher values for headwater sections in the southern and northern parts of the catchment fig 2f the pci setup for lowland preferring passive drifter weights uniformly the downstream sections of the river fig 2g while the setup for the upland passive drifter weights more the headwaters fig 2h obtained values for cci and bfi ranged between 0 005 and 0 29 with higher values for dci like indices and cafi and lower values for pci like indices the spatial distribution of prioritized barriers varies with the cci index setup fig 3 the dci setups both symmetric and asymmetric assign high priorities to barriers located in the mid section of the main stem of the ebro river fig 3a and b the iic setup prioritizes barriers located in the main stem fig 3c while the setup with uniform weights prioritizes more upstream barriers fig 3d the pci setup for lowland preferring fish has higher values for few sections in the main stem fig 3e while the setup for upland fish has higher values for headwater sections in the southern and northern parts of the catchment fig 3f the pci setup for lowland preferring passive drifter weights uniformly the downstream sections of the river fig 3g while the setup for the upland passive drifter weights more the headwaters fig 3h the cafi prioritizes the most downstream barriers fig 3i the correlation of both rci and the barriers prioritization is highly variable for rci the spearman s correlation ranges from negative values 0 46 for scenarios e h to high congruence 0 89 for scenarios a e fig 4 for the barrier s prioritization with dcci spearman s correlation ranges between low congruence 0 15 for e h to high congruence 0 94 for a e fig 5 there seems to be a partial overlap between rci and dcci as barriers located in reaches with higher rci are also ranked higher in the prioritization 4 discussion 4 1 use of the package for population ecology restoring river connectivity is increasingly considered as an option to improve the health of freshwater populations and the resilience of metapopulations tickner et al 2020 beneficial impacts of connectivity restoration have been documented for many fish species birnie gauvin et al 2020 king et al 2017 sun et al 2022 however development of general connectivity biota causal links to inform restoration planning is complicated because of species specific dispersal and movement traits baldan et al 2020 for instance the response of organisms with limited mobility to increased connectivity might be limited in this regard the analysis of historical connectivity losses and the related effects on biota can be beneficial hall et al 2011 mattocks et al 2017 historic populations data can be reconstructed from public datasets e g rivfishtime carvajal quintero et al 2021 while qualitative methods can be used where data are not available duarte et al 2022 historic connectivity biota relationships can be used to validate connectivity indices especially when developed on neighbouring catchments with different degree of fragmentation the establishment of connectivity biota causal pathways and the validation of connectivity metrics can then be used to support population management schick and lindley 2007 functional and structural connectivity are influenced by seasonal to multiannual dynamics fullerton et al 2010 pont et al 2015 including the spatial structure of the river network orientation properties and modes of operation of barriers species traits and spatiotemporal scales considered climate change can affect the structure of the mesohabitat and indirectly affect the dispersal potential of organisms baldan et al 2021 holyoak et al 2020 effects can be even more drastic in rivers experiencing intermittency datry et al 2014 jaeger et al 2014 thus time dependent estimates of connectivity indices might be needed to fully capture the dynamics of the system if the relaxation time of the system i e the time the system needs to reach steady state is smaller than the time span considered castillo escrivà et al 2020 the problem can be considered by a sequence of steady states with the static equations implemented in riverconn still adequate in this case connectivity indices can be calculated for each state of the system separately and synthetic indicators can be developed to represent the connectivity aspects cid et al 2020 for highly dynamic systems such as small sized catchments and or highly intermittent different methodological approaches such as individual based models or high frequency monitoring pineda morante et al 2022 might be needed to better capture spatial or temporal connectivity patterns at the expense of higher data collection efforts and computational capacities in these cases connectivity patterns and barriers distribution might also be understood differently as fragmentation by drought can strongly vary in relation with river morphology drought intensity or riffle and pool distribution cañedo argüelles et al 2020 sarremejane et al 2017 the use of actual observation based connectivity metrics can also be feasible for such smaller scales applications jumani et al 2020 the riverconn package can be used for calculating several connectivity indices and their temporal development and therefore can be of great support for population level analyses at the moment riverconn implements only equations for calculating estimates of longitudinal connectivity however other dimensions of connectivity exist cañedo argüelles et al 2015 grill et al 2019 tonkin et al 2018 lateral connectivity river floodplains vertical connectivity groundwater river atmosphere and temporal connectivity constancy of the flow the implementation of lateral and vertical connectivity would require a modification of the base conceptualization of the river network as a directed loopless tree towards patch based spatial graphs erős et al 2012 flow related dependencies the temporal connectivity aspect could still be included in riverconn by quantifying the reach or meshohabitat scale as a function of flow characteristics kakouei et al 2018 in this regard indicators of hydrological alteration are a useful set of metrics to synthetize a wide range of hydrographs characteristics in scalar properties for spatial analyses olden and poff 2003 4 2 use of the package for community ecology in fragmented landscapes organism dispersal plays a key role in determining the local community structure borthagaray et al 2015 cañedo argüelles et al 2015 connectivity defined as the degree to which the landscape facilitates the movement of individuals among habitat patches economo and keitt 2010 uroy et al 2021 is a key factor affecting dispersal connectivity determined by landscape structure i e the spatial arrangement of communities and species dispersal strategies is widely used to assess communities isolation borthagaray et al 2020 horváth et al 2019 pineda morante et al 2022 accordingly metacommunity theory predicts local richness and diversity to be highly influenced by the degree of isolation of the community heino et al 2015 tonkin et al 2018 landscape fragmentation is a factor influencing the structuring of freshwater communities díaz et al 2021 horváth et al 2019 perkin and gido 2012 thus deriving metrics describing communities centrality and isolation is a key task in current community ecology pineda morante et al 2022 recent development allowed to account for the directional and dendritic nature of the river network peterson et al 2013 but still much work is needed to include longitudinal barriers indices as spatial covariates of aquatic communities wang et al 2019 the rci can be computed for each reach where community data is available and can be used as a predictor in multivariate community analysis the effect of fragmentation on biotic communities is expected to be mediated by the spatial scales and extents analyzed mahlum et al 2014 furthermore the use of such metrics could complement spatial covariates based on euclidean distance river based distance and flow connected distance ver hoef and erin 2010 in a variance partitioning algorithm to quantify the effect of longitudinal fragmentation on biotic communities and beta diversity cañedo argüelles et al 2020 schmera et al 2018 consequently the implementation of connectivity indices for community level studies would contribute to better quantify barrier impacts at higher organizational levels and across different organisms 4 3 use of the package to support riverscape planning our results show how the use of different connectivity indices based on different assumptions on the calculation of structural and functional connectivity can lead to diverse outcomes in the prioritization of reaches and barriers the reaches prioritization based on the dci has higher values for section of the catchment where few barriers exist located more upstream when directionality is accounted for accordingly barriers whose removal would increase the connected length are prioritized jumani et al 2022 the dci is length based and therefore it is supposed to be sensitive to the river network cropping therefore the cropping of the headwater reaches might result in overlooking the connectedness of the headwaters to the river network the cafi partially solves this issue by using area based weights less sensitive to river network delineation jumani et al 2022 however cafi is not a network based index thus it might neglect interactions between barriers placement that are accounted for in dci and pci cote et al 2009 rodeles et al 2021 the package riverconn can explicitly consider the assumptions underlying the definition of each connectivity index improving the transparency of the prioritization process multi criteria analysis has the potential to generate compromise indices based on a large set of indices calculated under different setups further optimization algorithms can be used to identify optimal barriers or habitat improvement sequences under economic constraints king et al 2017 o hanley 2011 the functions implemented in riverconn can be used to support the delineation of relevant sub catchments for conservation planning moilanen et al 2008 methods for prioritizing sub catchment for freshwater biodiversity conservation based on optimization algorithms deal with connectivity with simplistic approaches such as distance penalties beger et al 2010 or the unobstructed distance upstream a barrier mcmanamay et al 2019 modeling frameworks that include graph based connectivity were recently proposed erős et al 2018 the use of a wide array of connectivity indices can better incorporate multiple connectivity facets into freshwater conservation planning by explicitly including in the models organism specific dispersal traits rodeles et al 2021 planning and implementation of conservation actions such as habitat improvement is also carried out at the meso habitat scale 1 100 m wegscheider et al 2020 in our graph based conceptualization of the river network we defined a node as a whole reach this definition allows to consider only attributes averaged over a reach and might not be enough when the analysis seeks to follow restoration actions occurring at finer scales however any reach of interest could be further split in a sequence of nodes corresponding to sequences of different mesohabitats erős et al 2012 therefore riverconn conceptualization of the river can be adapted to the specific study case and scale allowing for fragmentation assessments by explicitly considering different scales for instance first a broad assessment can be conducted at the catchment scale then a detailed analysis on a smaller area of interest for instance a dammed reach and the surrounding floodplains can be performed erős et al 2012 5 conclusions in this paper we re conceptualized many indices used to assess river network fragmentation under a unifying framework and we presented an r package riverconn that allows for the calculation of such indices based on a graph based landscape conceptualization and widely available geospatial data functions implemented in the package can support among others i conservation planning via the identification and prioritization of barriers and habitats for connectivity improvement at the catchment scale ii population ecology via the possibility to analyze historic patterns of fragmentation and iii community ecology offering the possibility to generate barriers dependent spatial covariates for community analysis software availability name of the software riverconn developer damiano baldan aut cre david cunillera montcusì ctb andrea funk ctb contact information damiano baldan91 gmail com year first available 2022 program language r cost free software availability https cran r project org package riverconn last stable version https github com damianobaldan riverconn development version program size 1 93 mb declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements db and af acknowledge support from the christian doppler society cd laboratory for meta ecosystem dynamics in riverine landscapes dcm was supported by the mecodisper project ctm 2017 89295 p funded by the spanish ministerio de economía industria y competitividad mineco agencia estatal de investigación aei and co funded by the european regional development fund erdf and by european union nextgenerationeu ministry of universities and recovery transformation and resilience plan through a call from universitat de girona we are thankful to the two anonymous reviewers whose comments improved the manuscript 
25533,forest microclimate can buffer biotic responses to summer heat waves which are expected to become more extreme under climate warming prediction of forest microclimate is limited because meteorological observation standards seldom include situations inside forests we use extreme gradient boosting a machine learning technique to predict the microclimate of forest sites in brandenburg germany using seasonal data comprising weather features the analysis was amended by applying a shapley additive explanation to show the interaction effect of variables and individualised feature attributions we evaluate model performance in comparison to artificial neural networks random forest support vector machine and multi linear regression after implementing a feature selection an ensemble approach was applied to combine individual models for each forest and improve robustness over a given single prediction model the resulting model can be applied to translate climate change scenarios into temperatures inside forests to assess temperature related ecosystem services provided by forests keywords cooling effect machine learning ensemble method ecosystem services 1 introduction forests cover 27 percent of the land surface of the earth and house 80 percent of terrestrial biodiversity lessa et al 2020 many species live below the forest canopy de frenne et al 2019 humans also benefit from a variety of services provided by forests the regulation of temperature as one of the forest ecosystem services has been scarcely addressed in science forest microclimate is recognised as an important moderator of biotic responses and its recent incline driven by climate changes is expected to have a significant impact on forest dwelling species zellweger et al 2019 a large proportion of solar radiation is already converted into heat energy at the canopy surface while the understory remains shaded and cool in addition trees exploit a large volume of soil and the water it contains driving evapotranspiration which further cools the air below the canopy thus forest canopies can reduce the impact of above canopy temperatures on sub canopy ecological processes during the day de frenne et al 2019 at night heat storage in the tree mass limited long wave radiation and ceased transpiration result in a warmer microclimate compared to open landscape peng et al 2014 the daytime cooling effect is desirable for many species and humans enabling them to take advantage of cooler temperatures during unsuitably warm conditions outside forests which are detrimental to health and well being temperature measurements inside forests are not taken by default as part of meteorological observation networks the little data available suggest a nonlinear response of sub canopy temperatures compared to above canopy temperatures the temperature alteration of the canopy is influenced by several factors such as the structure of the canopy the season the tree species and forest management bramer et al 2018 air temperature near forests referred to as the macroclimate in most studies is influenced by the distance from the forest zellweger et al 2019 for this reason world meteorological organization guidelines suggest that synoptic stations where microclimate variables are recorded should be installed sufficiently far from sheltering trees srivastava 2009 as a result data recorded by stations represent the general atmospheric circulation components of the region but give no insight into temperature regimes within forests or other forms of vegetation that deviate from the standard conditions of ground based weather observations duraiappah et al 2005 thus climate projections that rely exclusively on macroclimate data disregard the potential impact of microclimates on biodiversity and ecosystem functioning there is a need for suitable tools and techniques to analyse microclimate variables and to describe the dynamics of temperatures inside forests physical and conceptual models contribute to our understanding of the global climate system and enable researchers to predict or simulate climate at various scales martínez et al 2012 in contrast to long term macroclimate data which are ubiquitously available and freely accessible microclimate data in forests are subject to restricted access payne et al 2000 for this reason we seek to simulate microclimatic conditions inside forests building on existing temperature gradients from the macroclimate to the microclimate we hypothesise that we can predict the temperature inside a forest by using meteorological data from regional weather stations located near the forest of interest lenoir et al 2017 introduced the latest methods used to model the climatic component associated with microclimatic stability over time at very fine resolutions the study suggested that spatiotemporal quantification of forest temperature is key to addressing the responses of forest organisms to climate and land use change however the temperature at ground level may differ substantially from the atmospheric temperature due to local land cover vegetation structure shading topography and slope orientation de frenne et al 2013 attempts to model forest microclimates are rare and there is often a lack of appropriate data available to facilitate model calibration and validation kearney and porter 2017 maclean et al 2019 fewer public synoptic stations exist inside forests after all meteorological stations involve costly facilities and the need to maintain services in contrast to traditional statistical methods a number of machine learning algorithms have been used to predict and classify data offering significant insights mohaimenul islam et al 2018 machine learning ml methods provide an alternative approach that can predict the uncertainty of a weather forecast given the large scale atmospheric state at initialisation pasini et al 2017 ml methods operate without human assistance but require proper training to ensure that adequate data are used ml has proven suitable in various nonlinear interactions of variables as well as in pattern recognition in large data sets the family of boosting methods comprises ml techniques for a wide range of practical applications used to build non parametric regression or classification models generated under supervision natekin and knoll 2013 the extreme gradient boosting xgboost method is a scalable machine learning system of the boosting family its output is processed ten times faster than other existing methods on a single machine by considering the memory of the setting chen and guestrin 2016 in this study results of xgboost method will compare with other ml methods such as artificial neural networks ann random forest rf support vector machin regression svr in general a limited comparison of linear and tree based learners indicates that tree models perform substantially better joharestani et al 2019 beside of applying the xgboost method and comparing its results with other ml methods we used shapley additive explanations shap to interpret the output of the model and to analyse the importance of individual features being black box models ml models are unable to illustrate how predicted values come about by using shap we can transform model results into understandable interpretable rules as a unified approach shap is applied to visualise and describe the complex causal relations between driving factors and the range of the target value rathi 2019 which in this study is the temperature inside the forest this study focuses on predicting temperature dynamics inside a forest using the most available data outside the forest to achieve this we use data from weather stations located near the forest under investigation and we test four different ml methods for their performance we address three research questions i can we simulate temperature dynamics inside a forest using macroclimate data from public meteorological stations ii which is the most suitable ml approach how does it compare to classical multi linear regression and iii can shap values increase model transparency and help us interpret the dynamics of forest temperature 2 material and method 2 1 study area brandenburg with a land surface area of 29 640 km2 is the third largest state in germany almost two thirds of the state s territory is covered with soils that have a low water holding capacity of less than 140 mm gutzler et al 2015 agriculture is dominated by large farm enterprises which are four times larger average of 238 ha than the average farm size for the whole of germany paired with a high mechanisation rate this state focuses on agricultural profit maximisation gutzler et al 2015 the federal state of brandenburg has a high percentage of protected areas in the form of national parks and biosphere reserves 8 based on the selected results of the third national forest soil inventory brandenburg is one of germany s most forested states 37 of the state area forest areas mainly contain pure stands of scots pine pinus sylvestris riek et al 2015 overall brandenburg features typical elements of the north german lowlands exhibiting a mixture of different sized forest patches 38 2 of forest patches are 20 ha 16 1 are 1000 ha riek et al 2015 in addition the agricultural landscape includes diverse arrangements of agricultural fields grassland groves small water bodies and landscape elements linked by land cover gradients transition zones brandenburg has a relatively dry temperate climate the water balance strongly influences forest growth and vitality the measurements for this study were conducted at six forest sites covering the entire state of brandenburg from north to south fig 1 during the warm period may june july and august of the years 2016 2019 2 2 data 2 2 1 microclimate data the meteorological data for the six forest sites in brandenburg were recorded by the landeskompetenzzentrum forest eberswalde state competence centre for forestry lfe https forst brandenburg de lfb de lfe for each site data were collected at two stations one inside and one outside the forest however in this study we used their data only from the inside of the six forests being subject to regular health status checks the sites represent intensive monitoring core plots of the international cooperative programme on assessment and monitoring of air pollution effects on forests icp forests level ii program the hourly meteorological variables used for this study comprise average temperature c relative humidity global radiation w m2 and precipitation mm data have been recorded at four scots pine forests 1202 1203 1204 1205 since 1996 and later also at two oak forests 1208 1209 in our study we consider data from all of those forests only between 2016 and 2019 fig 1 the hourly surface air temperature from 1 may to 31 august later referred to as the warm period of the year over the study period is summarised in table 1 during the warm period of the years 2016 2018 the maximum temperature recorded at the northernmost station 1202 was 31 7 c a maximum temperature of 33 3 c was measured at station 1205 in the south the warmest year in the period of the study was 2018 when the mean temperature of the warm period selected in this study in this year was 1 5 2 k warmer than 2016 and 2017 respectively table 1 2 2 2 macroclimate data in this study we selected nine weather stations with the available meteorological variables during the years 2016 2018 recorded at hourly resolution by the deutscher wetterdienst german meteorological service dwd fig 1 recorded data at each station provided complete time series of weather variables required as our model input four basic variables that determine the dynamics of the thermal environment air temperature mean radiation soil temperature and air relative humidity were considered for each of the six forest sites we selected dwd stations that were located within a minimum of 10 to a maximum of 80 km distance to the relevant site fig 1 the temperature and relative humidity recordings taken by dwd and lfe refer to a height of 2 m above the ground table 2 provides detailed information on all the forest stations and the nearby dwd stations selected for our study during the warm period the northernmost dwd station zehdenick located near forest stations 1202 1203 and 1209 see table 2 featured an average maximum air temperature of 33 8 c while wiesenburg in the south had a maximum temperature of 34 7 c table 3 based on data from our selected weather stations the average maximum temperature during the warm period of the study at stations located in the north 33 6 c was almost one degree lower than at stations in the south 34 8 c details of individual dwd stations are provided in the supplementary material tables s1 s2 2 3 methods we follow a three step methodology applied to predict air temperature inside a forest during the warm period using data available from stations in the forests and also weather stations in brandenburg first we tested four different ml methods extreme gradient boosting xgboost artificial neural network ann random forest rf support vector machine regression svr and also multi linear regression mlr as a statistical approach for their performance on a limited data set dwivedi 2020 ferreira et al 2019 in the second step we applied the shap statistical approach to one of the ml methods xgboost as a common method to provide interpretion to the result of a ml process finally we conclude the study by combining the individual xgboost models derived for each forest station into one general approach for the prediction of inside forest temperatures in brandenburg using dwd station data only for this purpose we apply the xgboost models from the nearest dwd stations as an ensemble by building a new model using weighted contributions from the individual xgboost models 2 3 1 xgboost boosting refers to a class of learning algorithms that fit models by combining base models with basic functions to improve the fit of the final model schapire and freund 2013 the ensemble of simple models with relatively low accuracy friedman 2001 develops a scalable implementation which can capture deep interactions and are less prone to outliers chen and guestrin 2016 the gradient boosting framework which includes an efficient linear model solver and a tree learning algorithm is used to train the model the boosting algorithm supports various objective functions such as regression classification and ranking xgboost an open source package 1 1 https github com dmlc xgboost provides state of the art results on a wide range of problems including climate prediction zheng and wu 2019 xiaoming ma et al 2020 xgboost with a scalable tree boosting system runs more than ten times faster than existing popular solutions on a single machine chen and guestrin 2016 xgboost has plenty of parameters which make it a complex model besides it requires hyperparameters to reduce the risk of overfitting and reducing the prediction variability two main hyperparameters to prevent the overfitting of xgboost are the number of iterations n estimators and the learning rate learning rate n estimators refers to model complexity increasing this parameter can lead to a robust model but it can still overfit at a point the number of iterations controls the degree of fit and thus affects the best value for the learning rate and vice versa keeping the learning rate as small as possible often improves generalisation performance decreasing the learning rate can considerably improve model performance shi et al 2019 the regularisation term suggested by friedman 2001 also helps users to avoid overfitting and controls the complexity of the model adinets et al 2020 during the tuning process model regularisation parameters such as lambda reg lambda and alpha reg alpha should be set according to the desired regularisation weight to enhance model performance tuning parameters in xgboost are provided in table 4 as follows gamma to control regularisation colsample bytree the number of variables provided for a tree min child weight in the classification task the tree splitting stops when the leaf node has a minimum sum of instance weight lower than min child weight max depth which controls maximum depth of a tree and subsample which controls the number of observations provided for a tree 2 3 2 support vector machine regression support vector regression svr is an extension of the popular classification of support vector machines svm following the original form svr works by performing a nonlinear mapping of the data from the input space to a higher dimensional feature space where linear regression can then be performed schölkopf and smola 2018 to improve the model accuracy initial parameters need to be tuned parameters such as kernels c and gamma are the most important ones to be concerned kernel specifies the kernel type of algorithm and useful in non linear separation problems c as the regularisation parameter must be strictly positive to prevent overfitting gamma defines the influence of an acceptable line of separation tuning parameters in svr are provided in table 5 2 3 3 artificial neural network artificial neural network ann multi layer perceptrons are applied in this study to map the relationships between input variables and dependent output variables an ann is composed of an input layer any number of hidden layers and an output layer some parameters i e the number of training and testing data learning rate number of hidden layers and processing function used affect the accuracy reliability and effectiveness of the neural network asiltürk and çunkaş 2011 a hidden layer consists of a number of neurons that transform the input information into the output layer to use stewart 2019a in each layer an action function decides how to compute the input values of a layer into output values learning rate controls the step size for a model to minimize the loss function a higher learning rate makes the model learn faster but may miss the minimum loss function and arrive somewhere close to it rendy 2021 batch size defines the number of training samples that are propagated through the network stewart 2019b epochs refer to the number of times that the entire training dataset is passed through the model rendy 2021 table 6 provides typical tuning parameters in anns 2 3 4 random forest random forest rf is an ensemble of decision tree algorithms which commonly performs well across a wide range of classification and regression predictive modelling problems a prediction of a regression problem is the average of the prediction across the trees in the ensemble random forest involves constructing a large number of decision trees from bootstrap samples from the training dataset brownlee 2021 in the case of a rf hyperparameters include the number of decision trees in the forest and the number of features considered by each tree when splitting a node the most important hyperparameters to tune for the rf are n estimators max samples n features and max depth table 7 the n estimators represented the number of trees which its value stabilizes the model performance stabilizes the max samples control the percentage of the size of the training dataset to train each decision tree during training the input data are split at each node that the parameters of split functions become optimized to fit with the training set the max features argument set the number of features for each split point another hyperparameter is the depth of the decision trees to be specified via the max depth argument high values of max depth indicate overfitting the training data 2 3 5 multi linear regression multi linear regression mlr is the most basic and commonly used type for predictive analysis it is a statistical approach to represent the relationship between a dependent variable and a given set of independent variables mlr attempts to model the relationship between two or more features and a response by fitting a linear equation to observed data 2 3 6 time lag prediction of a sequence of values using only values observed from the past causes the common time series problem of multistep ahead prediction cheng et al 2006 unlike regular classification models where training examples depend on each other the time series model requires information on what happened in the past at any point in time due to the distance between weather stations and forest patches in our study a time lag of several hours will appear in the model results when using data from a remote weather station to predict the temperature in the forest station consequently the input variables are the lagged time series values and the predicted value is the next value for one step ahead forecasting ahmed et al 2010 in the parameter prediction approach time series constitute a sequence of observations x x1 x2 xt each observation x t is recorded with a particular timestamp t xt p is used to show the segment of the time series x xt p xt p 1 xt cheng et al 2006 multistep ahead prediction involves predicting a sequence of h future values xt h t 1 given its p past observations xt t p 1 cheng et al 2006 in this study data are collected from two different sources and a sequence of observations from one station is used to forecast a sequence of temperatures at another station regarding the hourly resolution of the data from both sources and the distance between forest patches and weather stations we expected our model to perform well when considering right time lag 2 3 7 model validation in our experiment data from the dwd stations were used to simulate air temperature inside the forest the same validation process was applied for both datasets table 8 for model validation data were divided into training and testing sets 75 of the data three years data was used as the training set and the remaining 25 as the testing set one year data the training set contains data from the warm season may june july and august for three consecutive years 2016 2017 and 2018 the same warm period of the year 2019 was used to test the models all computations required for this paper were run on python version 3 6 8 xgboost provides various objective functions for training and the corresponding metric for performance monitoring in the field of meteorology air quality and climate research studies the root mean squared error rmse has been used as a standard statistical metric to measure model performance chai and oceanic 2014 in our study we used rmse and in addition mean bias error mbe and willmott s index of agreement dr as common metrics to evaluate training and validation of the model rmse measures the average magnitude of the error and attributes more weight to the larger deviations mbe indicates the systematic bias of the model it is primarily used to decide if any steps need to be taken to correct the model bias in contrast to the two difference based indices the willmott index assesses model accuracy on the bias of variance comparison willmott et al 2012 it is often added to evaluate the overall pattern of a time series 2 3 8 shap values shap values shapley additive explanation provide unique consistent and locally accurate attribution values chen and guestrin 2016 xgboost itself has a feature attribution which is not yet computed for every single prediction or an entire dataset to explain a model s overall behaviour in the model predictions the importance of each value is typically attributed to each input feature existing feature attribution methods often navigate features compared to the wrong direction lundberg et al 2019 the shap algorithm is based on statistical analysis and proposes exponential improvements in run time improved clustering performance and better identification of influential features since shap values are unique to each prediction they provide a richer visual representation in shap summary plots individualised feature attributions are leveraged to represent the range and distribution of a feature s impact on output and how the feature s value relates to its impact 2 3 9 ensemble modelling ensemble modelling reduces the generalisation error of model prediction from multiple diverse base models and improves overall performance kotu and deshpande 2015 here we apply the model to each individual forest station m1 mn the final ensemble model f w m is obtained by combining all individual models with different influential weights w1 wn i e 1 f w m i 1 n w i m i ensemble modelling enables models that perform better during validation to be weighted higher making a greater contribution to the final ensemble prediction the nelder mead method powell 1973 is used to estimate weighting the weighted average ensemble approach enables multiple models to contribute to a prediction in proportion to their trust or estimated performance this often leads to improved generalisability and robustness over a given single prediction model kotu and deshpande 2015 3 results in this section first we compare air temperature inside the forest with available air temperature from nearby weather stations from the open agricultural landscape during the warm period then we undertake a detailed comparison of the results of each forest station with paired weather stations for ease of use we borrowed the main body number of each station from lfe 1202 1203 1204 1205 1208 and 1209 the results of the xgboost regression are compared with ann rf svr and mlr finally xgboost models applied for each dwd station with plausible validation at each forest station are combined ensemble method to create a single higher performing model 3 1 comparing data from inside and outside the forest in our study the hourly resolution data from meteorological parameters inside the forest and also from weather stations have approved a cooler climate as compared to the surrounding agricultural landscape see fig 2 the temperature inside the forest during the day is more moderated compared to the hot ambient temperatures recorded by weather stations we considered two different time spots that express the hottest 15 00 and coolest temperatures 3 00 supplementary fig s1 s5 similar behaviour was observed for other forest stations at these two time spots across brandenburg the temperature inside the forest is always lower than the temperature at the open air stations relative humidity inside the forest is higher during midday than open air stations while the values inside and outside the forests during the night are relatively similar 3 2 model training 3 2 1 hyperparameter selection the methods used in this work require some degree of hyperparameter selection within the model architecture hyperparameters were selected separately for each application of the ml models for nine weather stations and based on validation performance we used package default values for most of the hyperparameters in general the performance of xgboost ann rf and svr was not particularly sensitive to hyperparameter tuning particularly when each model was individually applied for nineteen different datasets of combined weather and forest stations 3 2 2 model performance for simulating temperatures inside forests selected meteorological variables air temperature relative humidity global radiation and soil temperature in 5 cm depth provided by dwd as well as time lag are used as input for all models the selected variables were checked with different time lags in a range of 1 10 h due to the existing distances between forest stations and weather stations outside xgboost had better performance with time lags of 2 h and no significant improvement was achieved by increasing time lags the results of the testing of all methods at all forest sites with remote weather stations are shown in fig 3 more details are provided in the supplementary material table s3 three statistical criteria were computed over the given period to evaluate the performance of all models rmse mbe and willmott s refined index of agreement dr in almost all stations there was no significant differences in rmse between xgboost and mlr 0 1 but xgboost achieved lower rmse compared with svr and rf methods 2 7 and 7 7 respectively ann had higher rmse than xgboost in average 13 7 especially at the weather station in cottbus almost at all stations results of dr were close to 1 and xgboost had relatively better dr than ann rf and svr 1 7 0 2 and 0 7 respectively but it was almost similar to mlr 0 06 the results of mbe demonstrate that the systematic bias was similar for xgboost rf mlr and svm in average 0 2 but higher for ann 0 4 two forest sites 1209 and 1203 oak forests and scots pine are just 3 4 km apart and are assigned to the same dwd weather stations angermünde zehdenick and grünow the dwd station grünow is almost twice as far 42 6 km from its assigned forest station 1203 scots pine than the two other assigned dwd stations results of xgboost for both lfe stations were in an acceptable range of rmse 1 6 c and 1 5 c for scots pine and oak forests respectively the mbe of the station was the most underestimated 29 0 and 40 1 for scots pine and oak forests respectively the same pattern was observed when we applied other methods for this weather station near to both types of forests moreover at the oak forest xgboost had slightly better results than scots pine see supplementary table s3 3 3 shap values and key features related to forest temperatures as a statistical method shap adds interpretability to the output of xgboost a summary plot of all models for each forest station provides an overview of how all predictors interact four top ranked features out of twelve four main features and three different time lags 0 1 and 2 h are shown in descending order in table 9 the values in the horizontal axis indicate positive and negative relationships with the target variable in the shap summary plot dots represent data used for training see fig 4 air temperature at 2 m and soil temperature at 5 cm depth with no time lag were identified as the first two high ranked features 19 and 17 out of 19 models respectively global radiation mainly with zero or 1 h time lag was in the third and fourth place the humidity had the least contribution to the prediction of the target value in this study the summary plot of the grünow station near station 1203 featuring a relatively low model performance is shown infig 4 temperatures with no time lag and 1 h have high impact on the prediction of temperature in this forest global radiation comes as the next important feature see fig 4a unlike station 1203 the results of three models at station 1209 indicate the high impact of soil temperature and global radiation on the target value table 7 in the summary plot of the remote grünow station soil temperature with and without time lags are among the most influential features fig 4b 3 4 model ensemble for inside forest temperatures the results of the ensemble applied only for xgboost models at all stations are presented in table 10 one new learning algorithm was applied to make a new model by taking a weighted version of the contributing models the performance of the ensemble method was evaluated only by rmse and then compared with the same statistical criteria of xgboost performance the results of the ensemble model are on average 18 better than each individual contributing model at stations 1203 and 1209 the remote grünow weather station has a much lower weighting than the other two assigned stations based on the ensemble method 9 and 5 respectively xgboost had the highest performance at zehdenick for both 1203 and 1209 stations and this station received the highest weight of contribution for the ensemble model for both forest stations 63 and 72 respectively 4 discussion this study explored the suitability of using macro meteorological variables in terms of predicting the temperature inside forests during warm periods independent climate variables such as global radiation wm2 relative air humidity air temperature c and soil temperature c indicated the existence of a significant influence on the spatial temperature gradient between the forest station and its nearby weather stations we applied xgboost an efficient implementation of gradient boosting that can be used for regression predictive modelling to capture this gradient using the regional microclimate variables and recorded temperature under the forest canopy the results obtained from xgboost were stable and scalable even at forest stations that had nonlinear interaction with variables from nearby weather stations in the following sections we will discuss the efficiency of the ml method for predicting temperature inside forests and address the significance of topography and weather availability for improving model performance 4 1 the ml approach different machine learning methods have been applied for various weather forecasting purposes for instance support vector machines svm have been used for temperature prediction and classification with an optimal length of previous days holmstrom et al 2016 radhika and shashi 2009 in those studies the limited scope of previous days was considered for model training also the prediction was performed at the same location ml techniques have also been used for complex environmental numerical modelling to develop highly accurate and fast emulations for time consuming model physics components krasnopolsky and fox rabinovitz 2006 we evaluated the results of xgboost ann rf and also mlr at all different weather and forest stations in our study xgboost provides better results than the other ml methods at most of the forest stations that we considered which is why we consider xgboost to be the preferred method we trained all methods to simulate temperature inside the forest for a warm period of three years from data obtained from different ambient ecosystems provided that the data exhibit a regular pattern all methods except ann performed well and xgboost showed a slightly better performance than rf and svr on our simple experimental data set mlr showed a solid performance on a par with the ml methods but for complex and very large data sets it would run into practical scaling barriers where ml methods excel to provide fast results for the practitioner however when different types of forest scots pines and oak are considered xgboost did not excel any better performance than the other methods regardless of the numbers of the forest s types diversity in our study two oak forests and 4 scots pine ones all models showed a similar performance at both types of forests in general the types of forest might have negligible influence on the performance of models in this study but it was not necessary to modify the set of initial model parameters at individual forest stations because all models identified the forest type s differences during the training process in this study we did not expand the given set of parameters and optimized the performance of the model for all stations irrespective of forest type all applied methods except ann performed well at aggregating information from weather stations with different geographical distances and elevations the ensemble method applied to xgboost models combining the results from all the dwd weather stations with different elevations and distances near to the forest provides more precise prediction results than each of the prediction methods alone the contribution weight of individual models even at exceptional weather stations that performed poorly emphasises the importance of distance and elevation the ensemble method assigned relatively high weight of contribution to stations with a high elevation e g station lindenberg for forest station 1208 more remote stations e g grünow for forest stations 1203 and 1209 contributed with a lower weight to the final ensemble model in spite of the diverse topography of the dwd stations the training process in both models justified differences also given weights in ensemble method to each model approved that models trained with data from exceptional topographical characteristics stations might have lower contribution in the final model other parameters like wind speed and wind direction appear to have a role in the accuracy of the model while their impacts on variables such as air temperature and global radiation reflect their impact indirectly on the model results it should be noted that the ensemble method can also link several models e g xgboost and svr with the corresponding stations if it was necessary then this can lead to a reduction in the rsme however the ensemble method introduces additional parameters to the model which have their own uncertainty 4 2 using weather data to predict forest temperature dynamics trees and their cooling effect have a direct impact on the local microclimate during hot periods such as the recent severe heatwave of 2018 recorded in central europe schuldt et al 2020 however the response of trees to extreme heatwaves e g sustained transpirational cooling is uncertain but important for the ecosystem function of regulating temperature if higher summer temperatures occur more regularly trees adapt by increasing their leaf thermal tolerance gorsel et al 2016 drake et al 2018 teskey et al 2015 research on assessing the local cooling effects of trees during warm periods by using experimental studies and modelling has emphasised the critical role played by measurement data rahman et al 2017 li et al 2015 in these studies a lack of adequate measurement data was the main obstacle to quantifying the temperature gradient between the forest canopy and the surrounding area this necessitates the use of methods that extract more information from limited data to bridge this gap we used data from a larger number of meteorological sites as model input instead of selecting only one site as a reference which is the approach usually taken in cooling effect studies cohen et al 2012 we adapted xgboost ann rf svr and also mlr so that we could use macroclimatic data from those sites and their particular spatial features to predict nonlinear temperature in different forests compared to the results of the other methods xgboost provided anefficient outcome in terms of analysing other features which had indirect impact on simulating temperature such as topography one of the limitations of this study was the existence of several meteorological sites that had all of the required variables available there were many sites near the forest where some variables such as global radiation soil temperature in different depths wind direction and speed were not recorded due to the strong impact of global radiation on forest temperatures in the model a denser observation of global radiation at meteorological stations would be required to further improve prediction abilities for temperatures inside forests 4 3 applying shap for feature analysis it is essential to use an explanation method to interpret machine learning predictions we present and analyse xgboost results with shap which offers a powerful and insightful measure of the model feature shap a human grounded evaluation method has been well received in explainable artificial intelligence xai and related communities weerts et al 2019 shap summary plots have been applied recently with ml methods and especially xgboost to analyse the driver relationships hidden in the black box model batunacun et al 2020 parsa et al 2020 lim and chi 2019 kernelshap has been used only for multi class classification models based on the svm and random forest classifier algorithm unfortunately this visual interpretation method has not yet been adapted for svr and rf in the pre processing based on the model results we considered a set of weather variables temperature humidity global radiation and soil temperature with a time lag of 2 h for the feature analysis the shap summary plot was sufficiently powerful to rank features by their importance in simulating forest temperature and to show the extent of their influence and whether they have a positive or negative impact within data with different time lags between 0 and 2 h shap values proved that air temperature with 0 time lag and soil temperature mainly with 0 and 2 h time lag were the most important features four top ranked features in the xgboost model table 7 this confirms our expectations as very similar energy converting processes prevail inside and outside a forest given that vegetation covered areas are compared soil temperature is a damped and lagged function of the land surface temperature and therefore a less volatile heat indicator air temperature is forced by the same energy source but is additionally affected by laterally transported air volumes through wind our results show that the relation of inside forest temperatures and the temperature of the surrounding air volume ranges over several dekametres humidity was also identified as a top ranking feature with the lowest influences on the target value 9 table 7 air humidity is mostly a result of the water that is transpired through the plant or evaporates directly from the soil its phase changes consume energy which results in a lowering of the surrounding temperature however since much of the transpiration in a forest happens at the canopy level and wind speed is low the humidity inside the forest is not very dynamic hence its contribution to temperature changes is expected to be low global radiation with a time lag was the next most important variable after air and soil temperatures 17 also here due to the almost complete shading of the forest ground by the trees much of the solar energy conversion in the forest happens at the canopy level only in scattered older forests with light transmitting canopies incoming solar radiation may play a more important role in explaining inside forest temperatures 5 conclusion in this study machine learning helped us to extract more information from standard weather data for simulating microclimate variables we explored the power of machine learning to generate a well performing model for forest sub canopy temperature the results from the xgboost model were interpreted visually and comprehensively using shap the high performance of xgboost supports its ability to predict weather data in our study case moreover the consistent performance of the model also confirms the robust behaviour of this method in temperature prediction the next step in line with the objectives of this project could involve using geospatial images from satellites with temporal and spatial information to achieve a better simulation of forest sub canopy temperature furthermore shap offers a well suited tool for interpreting results from complex algorithms such as xgboost besides being capable of evaluating the importance and direction of a feature s impact on the output of a model the method can also extract complex and nonlinear joint impacts of features on the output of a model in particular our study reveals the impact of distance and elevation on weather stations features that are captured by ml techniques the cooler temperature regime inside forests during the summer months as compared to the surrounding agricultural landscape has been documented in our study the temperature regime inside forests can be well explained by weather variables obtained from standard meteorological stations this fact makes it possible to predict inside forest temperatures for ecology research and ecosystem service assessments such as the provisioning of cool air during summer heat waves based on easily available weather data from standard meteorological stations this information can be a useful addendum to data that is used to evaluate the value of forests in landscape planning or other political processes declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments the investigations were conducted within the agricultural system of the future dakis digital agricultural knowledge and information system project which was funded by the federal ministry of education and research germany bmbf förderprogramm agrarsysteme der zukunft 031b0729a we thank the landeskompetenzzentrum forest eberswalde lfe for supplying us with data from meteorological stations inside and outside forests in brandenburg appendix a supplementary data the following is the supplementary data to this article multimedia component 1 multimedia component 1 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2022 105466 
25533,forest microclimate can buffer biotic responses to summer heat waves which are expected to become more extreme under climate warming prediction of forest microclimate is limited because meteorological observation standards seldom include situations inside forests we use extreme gradient boosting a machine learning technique to predict the microclimate of forest sites in brandenburg germany using seasonal data comprising weather features the analysis was amended by applying a shapley additive explanation to show the interaction effect of variables and individualised feature attributions we evaluate model performance in comparison to artificial neural networks random forest support vector machine and multi linear regression after implementing a feature selection an ensemble approach was applied to combine individual models for each forest and improve robustness over a given single prediction model the resulting model can be applied to translate climate change scenarios into temperatures inside forests to assess temperature related ecosystem services provided by forests keywords cooling effect machine learning ensemble method ecosystem services 1 introduction forests cover 27 percent of the land surface of the earth and house 80 percent of terrestrial biodiversity lessa et al 2020 many species live below the forest canopy de frenne et al 2019 humans also benefit from a variety of services provided by forests the regulation of temperature as one of the forest ecosystem services has been scarcely addressed in science forest microclimate is recognised as an important moderator of biotic responses and its recent incline driven by climate changes is expected to have a significant impact on forest dwelling species zellweger et al 2019 a large proportion of solar radiation is already converted into heat energy at the canopy surface while the understory remains shaded and cool in addition trees exploit a large volume of soil and the water it contains driving evapotranspiration which further cools the air below the canopy thus forest canopies can reduce the impact of above canopy temperatures on sub canopy ecological processes during the day de frenne et al 2019 at night heat storage in the tree mass limited long wave radiation and ceased transpiration result in a warmer microclimate compared to open landscape peng et al 2014 the daytime cooling effect is desirable for many species and humans enabling them to take advantage of cooler temperatures during unsuitably warm conditions outside forests which are detrimental to health and well being temperature measurements inside forests are not taken by default as part of meteorological observation networks the little data available suggest a nonlinear response of sub canopy temperatures compared to above canopy temperatures the temperature alteration of the canopy is influenced by several factors such as the structure of the canopy the season the tree species and forest management bramer et al 2018 air temperature near forests referred to as the macroclimate in most studies is influenced by the distance from the forest zellweger et al 2019 for this reason world meteorological organization guidelines suggest that synoptic stations where microclimate variables are recorded should be installed sufficiently far from sheltering trees srivastava 2009 as a result data recorded by stations represent the general atmospheric circulation components of the region but give no insight into temperature regimes within forests or other forms of vegetation that deviate from the standard conditions of ground based weather observations duraiappah et al 2005 thus climate projections that rely exclusively on macroclimate data disregard the potential impact of microclimates on biodiversity and ecosystem functioning there is a need for suitable tools and techniques to analyse microclimate variables and to describe the dynamics of temperatures inside forests physical and conceptual models contribute to our understanding of the global climate system and enable researchers to predict or simulate climate at various scales martínez et al 2012 in contrast to long term macroclimate data which are ubiquitously available and freely accessible microclimate data in forests are subject to restricted access payne et al 2000 for this reason we seek to simulate microclimatic conditions inside forests building on existing temperature gradients from the macroclimate to the microclimate we hypothesise that we can predict the temperature inside a forest by using meteorological data from regional weather stations located near the forest of interest lenoir et al 2017 introduced the latest methods used to model the climatic component associated with microclimatic stability over time at very fine resolutions the study suggested that spatiotemporal quantification of forest temperature is key to addressing the responses of forest organisms to climate and land use change however the temperature at ground level may differ substantially from the atmospheric temperature due to local land cover vegetation structure shading topography and slope orientation de frenne et al 2013 attempts to model forest microclimates are rare and there is often a lack of appropriate data available to facilitate model calibration and validation kearney and porter 2017 maclean et al 2019 fewer public synoptic stations exist inside forests after all meteorological stations involve costly facilities and the need to maintain services in contrast to traditional statistical methods a number of machine learning algorithms have been used to predict and classify data offering significant insights mohaimenul islam et al 2018 machine learning ml methods provide an alternative approach that can predict the uncertainty of a weather forecast given the large scale atmospheric state at initialisation pasini et al 2017 ml methods operate without human assistance but require proper training to ensure that adequate data are used ml has proven suitable in various nonlinear interactions of variables as well as in pattern recognition in large data sets the family of boosting methods comprises ml techniques for a wide range of practical applications used to build non parametric regression or classification models generated under supervision natekin and knoll 2013 the extreme gradient boosting xgboost method is a scalable machine learning system of the boosting family its output is processed ten times faster than other existing methods on a single machine by considering the memory of the setting chen and guestrin 2016 in this study results of xgboost method will compare with other ml methods such as artificial neural networks ann random forest rf support vector machin regression svr in general a limited comparison of linear and tree based learners indicates that tree models perform substantially better joharestani et al 2019 beside of applying the xgboost method and comparing its results with other ml methods we used shapley additive explanations shap to interpret the output of the model and to analyse the importance of individual features being black box models ml models are unable to illustrate how predicted values come about by using shap we can transform model results into understandable interpretable rules as a unified approach shap is applied to visualise and describe the complex causal relations between driving factors and the range of the target value rathi 2019 which in this study is the temperature inside the forest this study focuses on predicting temperature dynamics inside a forest using the most available data outside the forest to achieve this we use data from weather stations located near the forest under investigation and we test four different ml methods for their performance we address three research questions i can we simulate temperature dynamics inside a forest using macroclimate data from public meteorological stations ii which is the most suitable ml approach how does it compare to classical multi linear regression and iii can shap values increase model transparency and help us interpret the dynamics of forest temperature 2 material and method 2 1 study area brandenburg with a land surface area of 29 640 km2 is the third largest state in germany almost two thirds of the state s territory is covered with soils that have a low water holding capacity of less than 140 mm gutzler et al 2015 agriculture is dominated by large farm enterprises which are four times larger average of 238 ha than the average farm size for the whole of germany paired with a high mechanisation rate this state focuses on agricultural profit maximisation gutzler et al 2015 the federal state of brandenburg has a high percentage of protected areas in the form of national parks and biosphere reserves 8 based on the selected results of the third national forest soil inventory brandenburg is one of germany s most forested states 37 of the state area forest areas mainly contain pure stands of scots pine pinus sylvestris riek et al 2015 overall brandenburg features typical elements of the north german lowlands exhibiting a mixture of different sized forest patches 38 2 of forest patches are 20 ha 16 1 are 1000 ha riek et al 2015 in addition the agricultural landscape includes diverse arrangements of agricultural fields grassland groves small water bodies and landscape elements linked by land cover gradients transition zones brandenburg has a relatively dry temperate climate the water balance strongly influences forest growth and vitality the measurements for this study were conducted at six forest sites covering the entire state of brandenburg from north to south fig 1 during the warm period may june july and august of the years 2016 2019 2 2 data 2 2 1 microclimate data the meteorological data for the six forest sites in brandenburg were recorded by the landeskompetenzzentrum forest eberswalde state competence centre for forestry lfe https forst brandenburg de lfb de lfe for each site data were collected at two stations one inside and one outside the forest however in this study we used their data only from the inside of the six forests being subject to regular health status checks the sites represent intensive monitoring core plots of the international cooperative programme on assessment and monitoring of air pollution effects on forests icp forests level ii program the hourly meteorological variables used for this study comprise average temperature c relative humidity global radiation w m2 and precipitation mm data have been recorded at four scots pine forests 1202 1203 1204 1205 since 1996 and later also at two oak forests 1208 1209 in our study we consider data from all of those forests only between 2016 and 2019 fig 1 the hourly surface air temperature from 1 may to 31 august later referred to as the warm period of the year over the study period is summarised in table 1 during the warm period of the years 2016 2018 the maximum temperature recorded at the northernmost station 1202 was 31 7 c a maximum temperature of 33 3 c was measured at station 1205 in the south the warmest year in the period of the study was 2018 when the mean temperature of the warm period selected in this study in this year was 1 5 2 k warmer than 2016 and 2017 respectively table 1 2 2 2 macroclimate data in this study we selected nine weather stations with the available meteorological variables during the years 2016 2018 recorded at hourly resolution by the deutscher wetterdienst german meteorological service dwd fig 1 recorded data at each station provided complete time series of weather variables required as our model input four basic variables that determine the dynamics of the thermal environment air temperature mean radiation soil temperature and air relative humidity were considered for each of the six forest sites we selected dwd stations that were located within a minimum of 10 to a maximum of 80 km distance to the relevant site fig 1 the temperature and relative humidity recordings taken by dwd and lfe refer to a height of 2 m above the ground table 2 provides detailed information on all the forest stations and the nearby dwd stations selected for our study during the warm period the northernmost dwd station zehdenick located near forest stations 1202 1203 and 1209 see table 2 featured an average maximum air temperature of 33 8 c while wiesenburg in the south had a maximum temperature of 34 7 c table 3 based on data from our selected weather stations the average maximum temperature during the warm period of the study at stations located in the north 33 6 c was almost one degree lower than at stations in the south 34 8 c details of individual dwd stations are provided in the supplementary material tables s1 s2 2 3 methods we follow a three step methodology applied to predict air temperature inside a forest during the warm period using data available from stations in the forests and also weather stations in brandenburg first we tested four different ml methods extreme gradient boosting xgboost artificial neural network ann random forest rf support vector machine regression svr and also multi linear regression mlr as a statistical approach for their performance on a limited data set dwivedi 2020 ferreira et al 2019 in the second step we applied the shap statistical approach to one of the ml methods xgboost as a common method to provide interpretion to the result of a ml process finally we conclude the study by combining the individual xgboost models derived for each forest station into one general approach for the prediction of inside forest temperatures in brandenburg using dwd station data only for this purpose we apply the xgboost models from the nearest dwd stations as an ensemble by building a new model using weighted contributions from the individual xgboost models 2 3 1 xgboost boosting refers to a class of learning algorithms that fit models by combining base models with basic functions to improve the fit of the final model schapire and freund 2013 the ensemble of simple models with relatively low accuracy friedman 2001 develops a scalable implementation which can capture deep interactions and are less prone to outliers chen and guestrin 2016 the gradient boosting framework which includes an efficient linear model solver and a tree learning algorithm is used to train the model the boosting algorithm supports various objective functions such as regression classification and ranking xgboost an open source package 1 1 https github com dmlc xgboost provides state of the art results on a wide range of problems including climate prediction zheng and wu 2019 xiaoming ma et al 2020 xgboost with a scalable tree boosting system runs more than ten times faster than existing popular solutions on a single machine chen and guestrin 2016 xgboost has plenty of parameters which make it a complex model besides it requires hyperparameters to reduce the risk of overfitting and reducing the prediction variability two main hyperparameters to prevent the overfitting of xgboost are the number of iterations n estimators and the learning rate learning rate n estimators refers to model complexity increasing this parameter can lead to a robust model but it can still overfit at a point the number of iterations controls the degree of fit and thus affects the best value for the learning rate and vice versa keeping the learning rate as small as possible often improves generalisation performance decreasing the learning rate can considerably improve model performance shi et al 2019 the regularisation term suggested by friedman 2001 also helps users to avoid overfitting and controls the complexity of the model adinets et al 2020 during the tuning process model regularisation parameters such as lambda reg lambda and alpha reg alpha should be set according to the desired regularisation weight to enhance model performance tuning parameters in xgboost are provided in table 4 as follows gamma to control regularisation colsample bytree the number of variables provided for a tree min child weight in the classification task the tree splitting stops when the leaf node has a minimum sum of instance weight lower than min child weight max depth which controls maximum depth of a tree and subsample which controls the number of observations provided for a tree 2 3 2 support vector machine regression support vector regression svr is an extension of the popular classification of support vector machines svm following the original form svr works by performing a nonlinear mapping of the data from the input space to a higher dimensional feature space where linear regression can then be performed schölkopf and smola 2018 to improve the model accuracy initial parameters need to be tuned parameters such as kernels c and gamma are the most important ones to be concerned kernel specifies the kernel type of algorithm and useful in non linear separation problems c as the regularisation parameter must be strictly positive to prevent overfitting gamma defines the influence of an acceptable line of separation tuning parameters in svr are provided in table 5 2 3 3 artificial neural network artificial neural network ann multi layer perceptrons are applied in this study to map the relationships between input variables and dependent output variables an ann is composed of an input layer any number of hidden layers and an output layer some parameters i e the number of training and testing data learning rate number of hidden layers and processing function used affect the accuracy reliability and effectiveness of the neural network asiltürk and çunkaş 2011 a hidden layer consists of a number of neurons that transform the input information into the output layer to use stewart 2019a in each layer an action function decides how to compute the input values of a layer into output values learning rate controls the step size for a model to minimize the loss function a higher learning rate makes the model learn faster but may miss the minimum loss function and arrive somewhere close to it rendy 2021 batch size defines the number of training samples that are propagated through the network stewart 2019b epochs refer to the number of times that the entire training dataset is passed through the model rendy 2021 table 6 provides typical tuning parameters in anns 2 3 4 random forest random forest rf is an ensemble of decision tree algorithms which commonly performs well across a wide range of classification and regression predictive modelling problems a prediction of a regression problem is the average of the prediction across the trees in the ensemble random forest involves constructing a large number of decision trees from bootstrap samples from the training dataset brownlee 2021 in the case of a rf hyperparameters include the number of decision trees in the forest and the number of features considered by each tree when splitting a node the most important hyperparameters to tune for the rf are n estimators max samples n features and max depth table 7 the n estimators represented the number of trees which its value stabilizes the model performance stabilizes the max samples control the percentage of the size of the training dataset to train each decision tree during training the input data are split at each node that the parameters of split functions become optimized to fit with the training set the max features argument set the number of features for each split point another hyperparameter is the depth of the decision trees to be specified via the max depth argument high values of max depth indicate overfitting the training data 2 3 5 multi linear regression multi linear regression mlr is the most basic and commonly used type for predictive analysis it is a statistical approach to represent the relationship between a dependent variable and a given set of independent variables mlr attempts to model the relationship between two or more features and a response by fitting a linear equation to observed data 2 3 6 time lag prediction of a sequence of values using only values observed from the past causes the common time series problem of multistep ahead prediction cheng et al 2006 unlike regular classification models where training examples depend on each other the time series model requires information on what happened in the past at any point in time due to the distance between weather stations and forest patches in our study a time lag of several hours will appear in the model results when using data from a remote weather station to predict the temperature in the forest station consequently the input variables are the lagged time series values and the predicted value is the next value for one step ahead forecasting ahmed et al 2010 in the parameter prediction approach time series constitute a sequence of observations x x1 x2 xt each observation x t is recorded with a particular timestamp t xt p is used to show the segment of the time series x xt p xt p 1 xt cheng et al 2006 multistep ahead prediction involves predicting a sequence of h future values xt h t 1 given its p past observations xt t p 1 cheng et al 2006 in this study data are collected from two different sources and a sequence of observations from one station is used to forecast a sequence of temperatures at another station regarding the hourly resolution of the data from both sources and the distance between forest patches and weather stations we expected our model to perform well when considering right time lag 2 3 7 model validation in our experiment data from the dwd stations were used to simulate air temperature inside the forest the same validation process was applied for both datasets table 8 for model validation data were divided into training and testing sets 75 of the data three years data was used as the training set and the remaining 25 as the testing set one year data the training set contains data from the warm season may june july and august for three consecutive years 2016 2017 and 2018 the same warm period of the year 2019 was used to test the models all computations required for this paper were run on python version 3 6 8 xgboost provides various objective functions for training and the corresponding metric for performance monitoring in the field of meteorology air quality and climate research studies the root mean squared error rmse has been used as a standard statistical metric to measure model performance chai and oceanic 2014 in our study we used rmse and in addition mean bias error mbe and willmott s index of agreement dr as common metrics to evaluate training and validation of the model rmse measures the average magnitude of the error and attributes more weight to the larger deviations mbe indicates the systematic bias of the model it is primarily used to decide if any steps need to be taken to correct the model bias in contrast to the two difference based indices the willmott index assesses model accuracy on the bias of variance comparison willmott et al 2012 it is often added to evaluate the overall pattern of a time series 2 3 8 shap values shap values shapley additive explanation provide unique consistent and locally accurate attribution values chen and guestrin 2016 xgboost itself has a feature attribution which is not yet computed for every single prediction or an entire dataset to explain a model s overall behaviour in the model predictions the importance of each value is typically attributed to each input feature existing feature attribution methods often navigate features compared to the wrong direction lundberg et al 2019 the shap algorithm is based on statistical analysis and proposes exponential improvements in run time improved clustering performance and better identification of influential features since shap values are unique to each prediction they provide a richer visual representation in shap summary plots individualised feature attributions are leveraged to represent the range and distribution of a feature s impact on output and how the feature s value relates to its impact 2 3 9 ensemble modelling ensemble modelling reduces the generalisation error of model prediction from multiple diverse base models and improves overall performance kotu and deshpande 2015 here we apply the model to each individual forest station m1 mn the final ensemble model f w m is obtained by combining all individual models with different influential weights w1 wn i e 1 f w m i 1 n w i m i ensemble modelling enables models that perform better during validation to be weighted higher making a greater contribution to the final ensemble prediction the nelder mead method powell 1973 is used to estimate weighting the weighted average ensemble approach enables multiple models to contribute to a prediction in proportion to their trust or estimated performance this often leads to improved generalisability and robustness over a given single prediction model kotu and deshpande 2015 3 results in this section first we compare air temperature inside the forest with available air temperature from nearby weather stations from the open agricultural landscape during the warm period then we undertake a detailed comparison of the results of each forest station with paired weather stations for ease of use we borrowed the main body number of each station from lfe 1202 1203 1204 1205 1208 and 1209 the results of the xgboost regression are compared with ann rf svr and mlr finally xgboost models applied for each dwd station with plausible validation at each forest station are combined ensemble method to create a single higher performing model 3 1 comparing data from inside and outside the forest in our study the hourly resolution data from meteorological parameters inside the forest and also from weather stations have approved a cooler climate as compared to the surrounding agricultural landscape see fig 2 the temperature inside the forest during the day is more moderated compared to the hot ambient temperatures recorded by weather stations we considered two different time spots that express the hottest 15 00 and coolest temperatures 3 00 supplementary fig s1 s5 similar behaviour was observed for other forest stations at these two time spots across brandenburg the temperature inside the forest is always lower than the temperature at the open air stations relative humidity inside the forest is higher during midday than open air stations while the values inside and outside the forests during the night are relatively similar 3 2 model training 3 2 1 hyperparameter selection the methods used in this work require some degree of hyperparameter selection within the model architecture hyperparameters were selected separately for each application of the ml models for nine weather stations and based on validation performance we used package default values for most of the hyperparameters in general the performance of xgboost ann rf and svr was not particularly sensitive to hyperparameter tuning particularly when each model was individually applied for nineteen different datasets of combined weather and forest stations 3 2 2 model performance for simulating temperatures inside forests selected meteorological variables air temperature relative humidity global radiation and soil temperature in 5 cm depth provided by dwd as well as time lag are used as input for all models the selected variables were checked with different time lags in a range of 1 10 h due to the existing distances between forest stations and weather stations outside xgboost had better performance with time lags of 2 h and no significant improvement was achieved by increasing time lags the results of the testing of all methods at all forest sites with remote weather stations are shown in fig 3 more details are provided in the supplementary material table s3 three statistical criteria were computed over the given period to evaluate the performance of all models rmse mbe and willmott s refined index of agreement dr in almost all stations there was no significant differences in rmse between xgboost and mlr 0 1 but xgboost achieved lower rmse compared with svr and rf methods 2 7 and 7 7 respectively ann had higher rmse than xgboost in average 13 7 especially at the weather station in cottbus almost at all stations results of dr were close to 1 and xgboost had relatively better dr than ann rf and svr 1 7 0 2 and 0 7 respectively but it was almost similar to mlr 0 06 the results of mbe demonstrate that the systematic bias was similar for xgboost rf mlr and svm in average 0 2 but higher for ann 0 4 two forest sites 1209 and 1203 oak forests and scots pine are just 3 4 km apart and are assigned to the same dwd weather stations angermünde zehdenick and grünow the dwd station grünow is almost twice as far 42 6 km from its assigned forest station 1203 scots pine than the two other assigned dwd stations results of xgboost for both lfe stations were in an acceptable range of rmse 1 6 c and 1 5 c for scots pine and oak forests respectively the mbe of the station was the most underestimated 29 0 and 40 1 for scots pine and oak forests respectively the same pattern was observed when we applied other methods for this weather station near to both types of forests moreover at the oak forest xgboost had slightly better results than scots pine see supplementary table s3 3 3 shap values and key features related to forest temperatures as a statistical method shap adds interpretability to the output of xgboost a summary plot of all models for each forest station provides an overview of how all predictors interact four top ranked features out of twelve four main features and three different time lags 0 1 and 2 h are shown in descending order in table 9 the values in the horizontal axis indicate positive and negative relationships with the target variable in the shap summary plot dots represent data used for training see fig 4 air temperature at 2 m and soil temperature at 5 cm depth with no time lag were identified as the first two high ranked features 19 and 17 out of 19 models respectively global radiation mainly with zero or 1 h time lag was in the third and fourth place the humidity had the least contribution to the prediction of the target value in this study the summary plot of the grünow station near station 1203 featuring a relatively low model performance is shown infig 4 temperatures with no time lag and 1 h have high impact on the prediction of temperature in this forest global radiation comes as the next important feature see fig 4a unlike station 1203 the results of three models at station 1209 indicate the high impact of soil temperature and global radiation on the target value table 7 in the summary plot of the remote grünow station soil temperature with and without time lags are among the most influential features fig 4b 3 4 model ensemble for inside forest temperatures the results of the ensemble applied only for xgboost models at all stations are presented in table 10 one new learning algorithm was applied to make a new model by taking a weighted version of the contributing models the performance of the ensemble method was evaluated only by rmse and then compared with the same statistical criteria of xgboost performance the results of the ensemble model are on average 18 better than each individual contributing model at stations 1203 and 1209 the remote grünow weather station has a much lower weighting than the other two assigned stations based on the ensemble method 9 and 5 respectively xgboost had the highest performance at zehdenick for both 1203 and 1209 stations and this station received the highest weight of contribution for the ensemble model for both forest stations 63 and 72 respectively 4 discussion this study explored the suitability of using macro meteorological variables in terms of predicting the temperature inside forests during warm periods independent climate variables such as global radiation wm2 relative air humidity air temperature c and soil temperature c indicated the existence of a significant influence on the spatial temperature gradient between the forest station and its nearby weather stations we applied xgboost an efficient implementation of gradient boosting that can be used for regression predictive modelling to capture this gradient using the regional microclimate variables and recorded temperature under the forest canopy the results obtained from xgboost were stable and scalable even at forest stations that had nonlinear interaction with variables from nearby weather stations in the following sections we will discuss the efficiency of the ml method for predicting temperature inside forests and address the significance of topography and weather availability for improving model performance 4 1 the ml approach different machine learning methods have been applied for various weather forecasting purposes for instance support vector machines svm have been used for temperature prediction and classification with an optimal length of previous days holmstrom et al 2016 radhika and shashi 2009 in those studies the limited scope of previous days was considered for model training also the prediction was performed at the same location ml techniques have also been used for complex environmental numerical modelling to develop highly accurate and fast emulations for time consuming model physics components krasnopolsky and fox rabinovitz 2006 we evaluated the results of xgboost ann rf and also mlr at all different weather and forest stations in our study xgboost provides better results than the other ml methods at most of the forest stations that we considered which is why we consider xgboost to be the preferred method we trained all methods to simulate temperature inside the forest for a warm period of three years from data obtained from different ambient ecosystems provided that the data exhibit a regular pattern all methods except ann performed well and xgboost showed a slightly better performance than rf and svr on our simple experimental data set mlr showed a solid performance on a par with the ml methods but for complex and very large data sets it would run into practical scaling barriers where ml methods excel to provide fast results for the practitioner however when different types of forest scots pines and oak are considered xgboost did not excel any better performance than the other methods regardless of the numbers of the forest s types diversity in our study two oak forests and 4 scots pine ones all models showed a similar performance at both types of forests in general the types of forest might have negligible influence on the performance of models in this study but it was not necessary to modify the set of initial model parameters at individual forest stations because all models identified the forest type s differences during the training process in this study we did not expand the given set of parameters and optimized the performance of the model for all stations irrespective of forest type all applied methods except ann performed well at aggregating information from weather stations with different geographical distances and elevations the ensemble method applied to xgboost models combining the results from all the dwd weather stations with different elevations and distances near to the forest provides more precise prediction results than each of the prediction methods alone the contribution weight of individual models even at exceptional weather stations that performed poorly emphasises the importance of distance and elevation the ensemble method assigned relatively high weight of contribution to stations with a high elevation e g station lindenberg for forest station 1208 more remote stations e g grünow for forest stations 1203 and 1209 contributed with a lower weight to the final ensemble model in spite of the diverse topography of the dwd stations the training process in both models justified differences also given weights in ensemble method to each model approved that models trained with data from exceptional topographical characteristics stations might have lower contribution in the final model other parameters like wind speed and wind direction appear to have a role in the accuracy of the model while their impacts on variables such as air temperature and global radiation reflect their impact indirectly on the model results it should be noted that the ensemble method can also link several models e g xgboost and svr with the corresponding stations if it was necessary then this can lead to a reduction in the rsme however the ensemble method introduces additional parameters to the model which have their own uncertainty 4 2 using weather data to predict forest temperature dynamics trees and their cooling effect have a direct impact on the local microclimate during hot periods such as the recent severe heatwave of 2018 recorded in central europe schuldt et al 2020 however the response of trees to extreme heatwaves e g sustained transpirational cooling is uncertain but important for the ecosystem function of regulating temperature if higher summer temperatures occur more regularly trees adapt by increasing their leaf thermal tolerance gorsel et al 2016 drake et al 2018 teskey et al 2015 research on assessing the local cooling effects of trees during warm periods by using experimental studies and modelling has emphasised the critical role played by measurement data rahman et al 2017 li et al 2015 in these studies a lack of adequate measurement data was the main obstacle to quantifying the temperature gradient between the forest canopy and the surrounding area this necessitates the use of methods that extract more information from limited data to bridge this gap we used data from a larger number of meteorological sites as model input instead of selecting only one site as a reference which is the approach usually taken in cooling effect studies cohen et al 2012 we adapted xgboost ann rf svr and also mlr so that we could use macroclimatic data from those sites and their particular spatial features to predict nonlinear temperature in different forests compared to the results of the other methods xgboost provided anefficient outcome in terms of analysing other features which had indirect impact on simulating temperature such as topography one of the limitations of this study was the existence of several meteorological sites that had all of the required variables available there were many sites near the forest where some variables such as global radiation soil temperature in different depths wind direction and speed were not recorded due to the strong impact of global radiation on forest temperatures in the model a denser observation of global radiation at meteorological stations would be required to further improve prediction abilities for temperatures inside forests 4 3 applying shap for feature analysis it is essential to use an explanation method to interpret machine learning predictions we present and analyse xgboost results with shap which offers a powerful and insightful measure of the model feature shap a human grounded evaluation method has been well received in explainable artificial intelligence xai and related communities weerts et al 2019 shap summary plots have been applied recently with ml methods and especially xgboost to analyse the driver relationships hidden in the black box model batunacun et al 2020 parsa et al 2020 lim and chi 2019 kernelshap has been used only for multi class classification models based on the svm and random forest classifier algorithm unfortunately this visual interpretation method has not yet been adapted for svr and rf in the pre processing based on the model results we considered a set of weather variables temperature humidity global radiation and soil temperature with a time lag of 2 h for the feature analysis the shap summary plot was sufficiently powerful to rank features by their importance in simulating forest temperature and to show the extent of their influence and whether they have a positive or negative impact within data with different time lags between 0 and 2 h shap values proved that air temperature with 0 time lag and soil temperature mainly with 0 and 2 h time lag were the most important features four top ranked features in the xgboost model table 7 this confirms our expectations as very similar energy converting processes prevail inside and outside a forest given that vegetation covered areas are compared soil temperature is a damped and lagged function of the land surface temperature and therefore a less volatile heat indicator air temperature is forced by the same energy source but is additionally affected by laterally transported air volumes through wind our results show that the relation of inside forest temperatures and the temperature of the surrounding air volume ranges over several dekametres humidity was also identified as a top ranking feature with the lowest influences on the target value 9 table 7 air humidity is mostly a result of the water that is transpired through the plant or evaporates directly from the soil its phase changes consume energy which results in a lowering of the surrounding temperature however since much of the transpiration in a forest happens at the canopy level and wind speed is low the humidity inside the forest is not very dynamic hence its contribution to temperature changes is expected to be low global radiation with a time lag was the next most important variable after air and soil temperatures 17 also here due to the almost complete shading of the forest ground by the trees much of the solar energy conversion in the forest happens at the canopy level only in scattered older forests with light transmitting canopies incoming solar radiation may play a more important role in explaining inside forest temperatures 5 conclusion in this study machine learning helped us to extract more information from standard weather data for simulating microclimate variables we explored the power of machine learning to generate a well performing model for forest sub canopy temperature the results from the xgboost model were interpreted visually and comprehensively using shap the high performance of xgboost supports its ability to predict weather data in our study case moreover the consistent performance of the model also confirms the robust behaviour of this method in temperature prediction the next step in line with the objectives of this project could involve using geospatial images from satellites with temporal and spatial information to achieve a better simulation of forest sub canopy temperature furthermore shap offers a well suited tool for interpreting results from complex algorithms such as xgboost besides being capable of evaluating the importance and direction of a feature s impact on the output of a model the method can also extract complex and nonlinear joint impacts of features on the output of a model in particular our study reveals the impact of distance and elevation on weather stations features that are captured by ml techniques the cooler temperature regime inside forests during the summer months as compared to the surrounding agricultural landscape has been documented in our study the temperature regime inside forests can be well explained by weather variables obtained from standard meteorological stations this fact makes it possible to predict inside forest temperatures for ecology research and ecosystem service assessments such as the provisioning of cool air during summer heat waves based on easily available weather data from standard meteorological stations this information can be a useful addendum to data that is used to evaluate the value of forests in landscape planning or other political processes declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments the investigations were conducted within the agricultural system of the future dakis digital agricultural knowledge and information system project which was funded by the federal ministry of education and research germany bmbf förderprogramm agrarsysteme der zukunft 031b0729a we thank the landeskompetenzzentrum forest eberswalde lfe for supplying us with data from meteorological stations inside and outside forests in brandenburg appendix a supplementary data the following is the supplementary data to this article multimedia component 1 multimedia component 1 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2022 105466 
25534,snow cover is a key hydrological variable critical to understanding water cycles and informing management decisions around resource extraction and recreational activities remote sensing open access data and cloud based computing platforms are two innovative tools for snow cover estimation in this paper we present snowwarp a processing framework that uses google earth engine and the r programming languages to combine landsat 30 m with modis 500 m satellite imagery and produce daily 30 m spatial resolution snow cover data anywhere globally snowwarp was applied in an alpine catchment in northern italy from 2000 2019 and validated using hydrometeorological datasets strong correlations between snow cover and ground data were found with correlations in terms of r up to 0 84 for temperature 0 17 for precipitation 0 74 for snow depth and 0 43 for streamflow the snowwarp tool is an open source framework enabling users to map fine spatial and temporal dynamics of snow cover to the ecosystem and hydrological monitoring keywords snow cover landsat modis google earth engine javascript r software availability name of tool snowwarp developers ethan berman saverio francini nicholas c coops year first available 2021 hardware required basic computer requirements the r software https www r project org orfeo toolbox https www orfeo toolbox org and a google earth engine account https earthengine google com source code availability codes are available on github https github com bermane snowwarp data availability snowwarp analyzes open access landsat and modis data both freely available on google earth engine https developers google com earth engine datasets catalog landsat https developers google com earth engine datasets catalog modis 006 mod10a1 cost free program languages r and javascript 1 introduction snow is an essential hydrological variable affecting runoff production spring recharge and water availability in mountain environments e g lucianetti et al 2020 penna et al 2016 global climatic hydrological and biogeochemical processes are highly dependent on seasonal snow cover takala et al 2011 informed ecosystem management requires understanding the extent of seasonal snow cover as it impacts a range of ecosystem services including water resources provisioning habitat availability for wildlife species and human recreational use of the landscape the spatial extent and temporal persistence of snow cover have also been used as indicators of shifting climate trends and global warming saavedra et al 2016 lemke et al 2007 under a changing climate the timing and extent of seasonal snow cover are uncertain and accurate snow cover mapping is increasingly important to understand snow dynamics berman et al 2018 corbari et al 2022 accordingly snow cover was included as one of the 50 essential climate variables by the global observing system for climate gcos accurate mapping of snow cover is complex due to its dynamic nature and variable spatial and temporal distribution rittger et al 2021 snow can be characterized by many variables including fractional and binary snow covered area fsca and bsca albedo liquid water content depth of the snowpack and snow water equivalent frei et al 2012 in particular bsca maps report the presence or absence of snow in each pixel whereas fsca indicates the percentage of a pixel covered by snow and is considered an important variable for many applications especially when working with moderate resolution imagery gascoin et al 2019 bsca and fsca maps have been generated since the 1960s ramsay 1998 and in recent years various statistical and machine learning approaches have been tested to improve these products kuter 2021 several snow cover datasets from optical remote sensing imagery are freely available at different spatial and temporal resolutions the moderate resolution imaging spectroradiometer modis onboard the terra and aqua satellite platforms with a 250 m to 1 km spatial resolution is the most frequently employed sensor in snow cover mapping due to its high temporal resolution and sensitivity to regions of the electromagnetic spectrum relevant for snow cover detection for instance the modis mod10a1 v6 snow cover daily global 500 m product contains snow cover snow albedo and quality assessment qa data snow cover data are based on a snow mapping algorithm that employs the normalized difference snow index ndsi defined as the normalized difference between green and swir1 landsat bands hall et al 2016 however 500 m spatial resolution is too coarse for many applications especially in mountain regions blöschl 1999 the landsat 4 9 missions provide snow related products for the united states and conterminous regions at 30 m spatial resolution but with a 16 day revisiting time selkowitz and forster 2016 there is an inability to capture snow cover dynamics daily weekly or even monthly in areas with frequent cloud cover for selected regions of the globe the theia snow collection provides on demand sca products at 20 m spatial resolution based on high temporal frequency sentinel 2 and landsat 8 observations gascoin et al 2019 at a lower spatial resolution 1 km but with global coverage there are snow extent datasets based on modis and viirs visible infrared imaging radiometer suite with 375 775 m spatial resolution data https modis snow ice gsfc nasa gov such as the globsnow product from the european space agency www globsnow info the tradeoff between temporal frequency and spatial resolution has been partially solved by data fusion approaches which exploit the strengths of various snow cover datasets for example modis daily observations can be fused with landsat 30 m spatial resolution data allowing for daily fine scale monitoring of snow dynamics based on these two datasets rittger et al 2021 developed a physically based spectral mixture analysis approach and a two stage random forest algorithm to produce daily 30 m fsca maps the algorithm was tested in the us sierra nevada with an overall accuracy of 97 and very little variation in performance between seasons mityok et al 2018 developed a fusion algorithm named modsat ndsi modis and landsat s ndsi that uses the ndsi metric common to both datasets to generate long term trend ndsi curves for each pixel landsat ndsi values are then resampled and overlaid with daily modis ndsi imagery to calculate the final ndsi values their approach was applied across south central british columbia with an overall accuracy of 90 berman et al 2018 developed the snowwarp algorithm which uses dynamic time warping dtw to obtain daily estimates of fsca at 30 m resolution by matching year to year patterns of modis ndsi and rearranging historical landsat fsca to create a denser time series snowwarp bsca maps were derived from 2000 to 2018 using fsca values with a threshold of 15 which has previously been determined as the snow detection limit for the modscag algorithm painter et al 2016 the data were tested against a network of time lapse cameras and snow pillows and results indicated high accuracy with the root mean squared error of fsca product ranging from 31 3 to 68 3 while the f score of the bsca ranged from 87 7 to 98 6 these three approaches have all shown promise in their ability to accurately map fine spatial and temporal scale snow cover yet are currently not available for users in an open source framework fine scale snow cover maps have proven to be an important indicator to understanding a variety of ecological processes and thus its availability to the broader applied research community would be of large benefit for example in the field of wildlife ecology janousek et al 2021 utilized snowwarp fsca maps to model elk aggregation patterns with fsca explaining 28 of the variability similarly rickbeil et al 2020 used snowwarp outputs to evaluate how spring cover dynamics together with early season forage availability altered grizzly bear behavior post den emergence with snow dynamics explaining 45 of the variation in spring activity date berman et al 2019 showed that the use of fsca improved models of grizzly bear habitat selection by 60 given the importance of snow cover in a variety of ecological modeling endeavors the aim of this paper is twofold first to describe an open source framework to enable users to derive daily fsca and bsca maps anywhere in the world we developed this new framework by improving the original implementation of snowwarp and creating a tool combining google earth engine cloud based computing resources gorelick et al 2017 with a new r package second with this new tool we produced a time series of 20 years 2000 2019 of bsca calculated over a mountain catchment in northern italy where extensive meteorological and snow based ground information is available including air temperature precipitation snow depth and streamflow this time series allows the assessment and demonstration of the snowwarp robustness and utility for environmental monitoring and hydrometeorological applications 2 snowwarp background and open source implementation fsca and bsca maps and accompanying statistics date of snow accumulation date of snowmelt and the number of days with snow cover in a year can be generated using the snowwarp tool fig 1 which combines google earth engine gee gorelick et al 2017 and the r programming environment r core team 2017 data can be downloaded for any period between august 1st 2000 and july 31st 2021 daily at 30 m spatial resolution with new data added at the end of each annual time step july 31st operationally the tool is organized as follows a download the snowwarp r package https github com bermane snowwarp and orfeo toolbox https www orfeo toolbox org b organize and pre process data in gee which involves a study area shapefile being uploaded and outputs provided as geotiff data in wgs84 projection at 30 m spatial resolution c download the pre processed data d process snowwarp daily fsca in r and extract key snowwarp annual statistics a detailed description of the tool as well as steps and the information needed to execute it are available on github https github com bermane snowwarp together with complete r and gee javascript codes the r script itself was translated from the original in matlab for which details are provided in berman et al 2018 and summarised below for completeness snowwarp products are derived by leveraging daily modis snow cover data and dynamic time warping dtw which is used to re order historical landsat observations to account for inter annual variability based on the method developed by berman et al 2018 in the first step of the new snowwarp package gee is used to pre process landsat and modis data more specifically imagery acquired by landsat 5 7 and 8 with a cloud cover threshold 70 is used images with cloud cover 70 are excluded because they are more prone to geographical location errors due to the challenges of performing geometrical corrections when ground control points are obscured white et al 2014 the remaining clouds in landsat imagery were then masked by the cfmask algorithm foga et al 2017 modis snow cover data is already provided on gee with clouds masked out hall et al 2016 next the ndsi is calculated from green and short wave infrared landsat reflectance values as follows 1 ndsi green swir1 green swir1 for modis a daily global and 500 meters resolution ndsi product is already available on gee hall et al 2016 linear regression is then applied to both landsat and modis ndsi data to calculate fsca following the regression formula introduced by aalstad et al 2020 2 fsca 1 45 ndsi 0 01 this regression formula was developed by finding the correlation between modis ndsi and fsca derived from landsat 7 salomonson and appel 2004 following the methods of berman et al 2018 fsca from modis and landsat is then adjusted fscaadj to account for the viewable gap fraction as follows 3 fscaadj fsca 1 fcc where fcc is the fractional canopy cover data ranging between 0 and 1 indicating the portion of the pixel area covered by trees sexton et al 2013 as a last step fsca values 100 are set to 100 and values 0 set to 0 landsat and modis fsca data are packaged and automatically downloaded by the end user using the r package provided which allows running the dtw algorithm as detailed in berman et al 2018 dtw is used to combine modis fsca 500 m resolution and 1 day revisitation time and landsat fsca 30 m resolution and 16 day revisitation time to obtain a time series of 30 m resolution daily fsca snow data dtw was originally developed for spoken word recognition sakoe and chiba 1978 and has been commonly used to define patterns between complementary time series in remote sensing baumann et al 2017 petitjean et al 2012 mcconnell et al 1991 3 study area the snowwarp tool was applied in the cordevole river catchment 109 km2 located in the italian dolomites eastern alps fig 2 elevations range from 1016 to 3152 m above sea level covering approximately 10890 ha corresponding to approximately 121 000 snowwarp 30 30 m pixels fig 2 in this area hydrometeorological data are collected and managed by the agency for environmental protection of the veneto region arpav the average yearly precipitation is 1220 mm average monthly temperatures range from 5 7 c in january and 14 1 c in july the high elevations and the geographical location in the internal portion of the southern alps determine that about half of the mean annual precipitation falls as snow generally from late october to april penna et al 2017 snowmelt usually starts in march and can last until july soils are moderately deep with a good drainage capacity the dominant land cover classes at higher elevations are alpine grassland shrubs and rocks while at lower elevations there are forests agriculture and small urban areas guastini et al 2019 4 data daily bsca maps were generated using the snowwarp tool from 1st august 2000 31st july 2020 over the study area daily snow covered area sca was then calculated in hectares as the sum of pixels areas with bsca true unlike bsca and fsca the sca product is not an image it is a value that indicates the total amount of hectares covered by snow over a defined study area time series of air temperature and precipitation were obtained with thiessen 1911 interpolation from data collected in five different local stations arabba passo pordoi caprile malga ciapela and passo falzarego included in or surrounding the study area snow depth was obtained by averaging values from two local stations monti alti ornella and cima pradazzo while streamflow data were collected at the saviner outlet station fig 2 table 1 describes the main features of the hydrometeorological data to analyze the relationships between sca and ground hydrometeorological variables we used the cross correlation function ccf derrick and thomas 2004 and the running correlation coefficient rcc zhao et al 2018 for periods with data availability the ccf provides a measure of association between signals when two time series are cross correlated a measure of temporal similarity is achieved for instance in the relationship between two time series x and y the series y may be related to past lags of the x series ccf helps identify lags of the x variable that might be useful predictors of y rcc is obtained by measuring the pearson correlation in a small portion of the signal 30 days in this case and repeating the process along with a moving window until the entire signal is covered 5 results 5 1 snowwarp benchmark and products the generation of the bsca time series for the study area involved the processing of 1356 landsat and 7249 modis images automatically downloaded from gee as tif files in 5 minutes landsat images were acquired at about 10 a m while modis at about 2 p m landsat 30 m spatial resolution and modis 500 m spatial resolution were stored as tif files the snowwarp r tool took 4 hours to process using a workstation with 32gb ram and 36 cores the fsca landsat data require about 50mb of storage space while fsca modis about 2 5mb the sca and the ground data time series have different time extent and number of observations with sca covering 20 years while ground data covering a shorter period fig 3 all time series showed an annual periodicity and the expected seasonal and annual variability the sca ranged between 164 10890 ha with an average value across the time series of 7300 days of 5159 ha or about 47 of the study area fig 4 presents four examples of sca daily maps selected on four different days of march 2017 during a period with no snowfall events a decrease in the sca over time is evident ranging from 98 10736 ha 01 03 2017 to 83 9112 ha 10 03 2017 64 7020 ha 20 03 2017 and finally 46 5056 ha 31 03 2017 5 2 correlation between sca and ground data the relationship between sca and temperature was analyzed for the period 1st january 2005 16th november 2017 table 1 during this period the ccf value fig 5 bottom ranged from a high negative correlation equal to 0 84 with a lag of zero days to a moderate positive correlation of 0 53 with a lag of 50 the rcc fig 4 top left computed with a 30 day moving window shows a clear seasonal pattern which is even more evident focusing on just the 2015 2017 period fig 5 top right high negative correlation values were observed when snow cover reaches its maximum and temperatures are minimum and no lag occurs and vice versa at this point the correlation decreases as the snow cover plateaus a smaller negative peak is observed at the end of the snow melting period with minimum sca and maximum temperatures the relationship between sca and precipitation was analyzed from 1st january 2005 16th november 2017 i e the period for which precipitation data were available table 1 the ccf range was close to zero between 0 09 with 50 days lag and 0 17 with 6 days lag fig 6 bottom it is worth noting however that there is no distinction in input data between solid and liquid precipitation looking at the rcc fig 6 top left high positive values are observed in fall when rainfall in this climate is typically abundant and if the temperature is sufficiently low snowfall occurs then correlations decrease while the sca increases when the area is fully covered by snow no correlation is observed as precipitation is solid and determines only an increase of snow depth without a corresponding increase in sca fig 6 top right the relationship between sca and snow depth was analyzed for the period 1st october 2009 31st december 2016 table 1 the ccf fig 7 bottom shows a maximum value equal to 0 74 with a time lag of 8 days and a negative minimum value of 0 64 with a 36 day lag the latter possibly indicating the time lag between snow melting starting at lower elevations and a consistent decrease in snow depth the rcc values fig 7 top computed with a 30 day moving window are always positive and maximum values are observed when sca reaches its maximum extent then the correlation decreases to reach a minimum at the end of the snow season with minimum sca it is interesting to note the very low correlation peaks occurring with small snowfall events that cause an increase of sca but do not cause snow accumulation these events tend to occur during early fall when temperatures are still well above zero in large areas of the catchment and differences occur along the elevation gradient precipitation may occur as snow only at higher elevations thus leading to an increase in sca while as rain at lower elevations leading to decreasing values of snow depth the relationship between sca and streamflow at the saviner outlet is reported for the period 1st january 2005 16th november 2017 table 1 streamflow is the result of complex interactions among different variables especially in alpine areas in addition to precipitation and snowmelt cochand et al 2019 camporese et al 2014 zuecco et al 2019 for this reason the ccf correlations between streamflow and sca were low when compared to previously observed correlations more specifically ccf analysis fig 8 bottom shows a maximum negative correlation equal to 0 43 with 15 days lag and a positive correlation of 0 38 with 50 days lag the rcc fig 8 top computed with a 30 day moving window shows high positive peaks in autumn when streamflow is high due to large rainfall events and then switching to snowfall when air temperatures drop a high negative peak is observed at the end of the snowmelt period when sca is minimum and streamflow is high due to the snowmelt contribution in addition to the ccf and rcc analysis the links between sca change and streamflow were graphically explored for selected years and months to corroborate the existing relationships between sca and hydrometeorological variables for example in the first days of march 2010 fig 9 top left snow starts to melt as sca decreases but streamflow markedly increases only after 15 days when snow at higher elevation also melts providing larger contributions to streamflow this example agrees with the 15 day lag observed in the corresponding ccf streamflow relationship fig 8 conversely in march 2014 fig 9 mid left a more direct relationship between snowmelt and streamflow increase was observed in the first days of the month interestingly in the second part of the month the snow ceases to melt and accordingly streamflow decreases in the last five days of march 2014 the snow begins again to melt smaller sca and similarly the streamflow begins to increase similar observations can be drawn from the other panels in fig 9 for march 2017 april 2007 2010 and 2015 6 discussion this work demonstrates a framework for applied users to compute daily 30 m fsca and bsca maps using the snowwarp algorithm which is the first time this novel high resolution data is readily available and with global coverage we have additionally provided an example of its application in a snow dominated study area in the italian alps the algorithm on which the snowwarp tool is based relies on a landsat modis data fusion approach which represents the most recent advancement in data fusion and allows for better exploitation of the strengths of different remote sensing platforms the original algorithm was robustly tested and previously applied in previous studies berman et al 2018 janousek et al 2021 rickbeil et al 2020 here a slightly modified version is presented to i employ datasets available globally and ii use open source software the procedure to generate fsca bsca and sca products is now easy to be applied on open access resources namely r and gee and allows the users to specify the area of interest and extract annual statistics overall this research illustrates that the snowwarp tool enables monitoring of daily snow dynamics from 2000 onward and is accessible to applied users without the need for large computing resources or complex code this feature is common to most cloud computing based hydrological products exploiting gee castelli and bresci 2019 which makes the application of complex algorithms over very large areas possible francini et al 2021 to confirm the robustness and potential uses of data generated by snowwarp the relationships between sca and meteorological and streamflow data were explored over several years in most cases the results showed intuitive explanations for the dynamics of the variables including the time lag occurring between changes in one variable and effects on another one as well as expected seasonal patterns the correlation between sca and temperature showed very large negative values in winter when temperatures are at a minimum and the snow cover extent is large fig 5 the correlation between sca and snow depth was also very high and always positive with larger correlation values starting in the late fall in correspondence to the onset of snow accumulation lasting until the melt period fig 7 these observations are consistent with the typical behavior of snow dominated mountain catchments kirchner et al 2020 zuecco et al 2019 penna et al 2017 staudinger et al 2017 cochand et al 2019 indicating that the snowwarp sca product is reliable and accurate in depicting sca temporal patterns some low correlations were observed during early winter short snowfall events which increased the snow covered area but had a limited impact on the depth of the snow layer the correlation between sca and precipitation generally was moderate with both positive and negative values due to the lack of distinction between snow vs rain precipitation fig 6 the observed seasonal correlation patterns are in line with what is expected for snow dominated catchments in temperate climates marin et al 2020 diodato et al 2022 with positive peaks occurring in fall at the onset of the snowpack formation when most of the precipitation falls as snow and negative peaks occurring in the summer period the relationship between sca and streamflow is complex and as expected the time series correlations were not particularly strong fig 7 on the other hand the observed correlation peaks were well explained by the snow accumulation and melting seasonal dynamics major positive peaks in the correlation between sca and streamflow were registered in summer when there is no snow on the ground sca 0 and precipitation and streamflow are minimal alternatively major negative peaks in correlation were registered during the snowmelt period when sca decreases quickly and streamflow increases further tests are needed to better investigate this relationship which highlights the potential of the tool to provide input data and reduce prediction uncertainty in hydrological models di marco et al 2021 several applications can benefit from the availability of snow cover area data produced by the snowwarp package for example specific ecology and conservation questions could be better answered using high resolution snow cover data examples of applications related to wildlife were presented by janousek et al 2021 berman et al 2019 and rickbeil et al 2020 who exploited the dataset produced with the original berman et al 2018 algorithm several researchers have also investigated the role of snow cover changes in influencing vegetation growth and dynamics on the tibetan plateau wang et al 2018 exploited satellite observations to show how snow cover changes promoted vegetation growth in drier areas over the alps modis data provided evidence that the relationship between snow cover and vegetation strongly depends on altitude and helped to understand the relative importance of snow and snowmelt as a driver of other climate variables asam et al 2018 furthermore snow cover shows to be an important parameter for the evolution of biodiversity patterns in the arctic region as shown by the simulations based on earth observation data carried out by niittynen et al 2018 for hydrology snow volume derived from snow cover and depth information helped to predict winter and early spring discharge in watersheds in canada and the us dyer 2008 in addition a strong correlation between snow cover and discharge was found in the northwest himalayas to assess the variability of snowmelt related flooding sharma et al 2012 in alpine catchments guastini et al 2019 zuecco et al 2019 streamflow response was linked to snow cover variation during the snowmelt period in snow dominated catchments snow cover data derived from modis proved useful to develop streamflow estimations and runoff forecasts such as in upper euphrates basin sorman et al 2019 in the italian alps the snow cover maps reduced predictive streamflow uncertainty in hydrological modeling di marco et al 2021 they were also used to validate methodologies for forecasting hydropower production corbari et al 2022 and to calibrate snow accumulation and melt models he et al 2014 ceperley et al 2020 lastly software such as snowwarp can be applied to evaluate climate change impacts on the most globally vulnerable snowpack systems saavedra et al 2018 hammond et al 2018 7 conclusions in this paper we i demonstrated the open source snowwarp tool ii exploited gee capabilities to assist with data preparation and initial processing of remote sensing imagery iii tested how snow cover dynamics were related to different hydrometeorological variables in a case study in the italian alps the snowwarp tool exploits the gee cloud computing platform functionalities and combines landsat and modis satellite imagery through the dtw algorithm it delivers daily snow cover datasets at a 30 meter spatial resolution over areas of interest globally however to date it is challenging to apply snowwarp globally and produce stable global snow cover data due to the computational time and storage requirements of such high resolution data to solve this issue future research should aim to implement the dtw currently implemented in our r package on gee moreover high correlations between sca and ground data suggest that the product is reliable and useful but more solid accuracy assessments should be performed across different global regions high resolution snow cover data from snowwarp can contribute to numerous applications especially when the production of data is facilitated by an easy open access based framework such as the tool presented here this is especially true in remote regions where it is difficult to acquire ground data or where the availability of long time series of snow cover can support the development of mitigation strategies for dangerous phenomena such as flooding or species loss declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments the authors thank the agency for environmental protection of the veneto region arpav for the meteorological hydrometric and snow depth data for the catchment cordevole at saviner the original snowwarp model development was supported by the grizzly paw project nserc file crdpj 486175 15 grantee n c coops frm ubc in collaboration with fri research and partners 
25534,snow cover is a key hydrological variable critical to understanding water cycles and informing management decisions around resource extraction and recreational activities remote sensing open access data and cloud based computing platforms are two innovative tools for snow cover estimation in this paper we present snowwarp a processing framework that uses google earth engine and the r programming languages to combine landsat 30 m with modis 500 m satellite imagery and produce daily 30 m spatial resolution snow cover data anywhere globally snowwarp was applied in an alpine catchment in northern italy from 2000 2019 and validated using hydrometeorological datasets strong correlations between snow cover and ground data were found with correlations in terms of r up to 0 84 for temperature 0 17 for precipitation 0 74 for snow depth and 0 43 for streamflow the snowwarp tool is an open source framework enabling users to map fine spatial and temporal dynamics of snow cover to the ecosystem and hydrological monitoring keywords snow cover landsat modis google earth engine javascript r software availability name of tool snowwarp developers ethan berman saverio francini nicholas c coops year first available 2021 hardware required basic computer requirements the r software https www r project org orfeo toolbox https www orfeo toolbox org and a google earth engine account https earthengine google com source code availability codes are available on github https github com bermane snowwarp data availability snowwarp analyzes open access landsat and modis data both freely available on google earth engine https developers google com earth engine datasets catalog landsat https developers google com earth engine datasets catalog modis 006 mod10a1 cost free program languages r and javascript 1 introduction snow is an essential hydrological variable affecting runoff production spring recharge and water availability in mountain environments e g lucianetti et al 2020 penna et al 2016 global climatic hydrological and biogeochemical processes are highly dependent on seasonal snow cover takala et al 2011 informed ecosystem management requires understanding the extent of seasonal snow cover as it impacts a range of ecosystem services including water resources provisioning habitat availability for wildlife species and human recreational use of the landscape the spatial extent and temporal persistence of snow cover have also been used as indicators of shifting climate trends and global warming saavedra et al 2016 lemke et al 2007 under a changing climate the timing and extent of seasonal snow cover are uncertain and accurate snow cover mapping is increasingly important to understand snow dynamics berman et al 2018 corbari et al 2022 accordingly snow cover was included as one of the 50 essential climate variables by the global observing system for climate gcos accurate mapping of snow cover is complex due to its dynamic nature and variable spatial and temporal distribution rittger et al 2021 snow can be characterized by many variables including fractional and binary snow covered area fsca and bsca albedo liquid water content depth of the snowpack and snow water equivalent frei et al 2012 in particular bsca maps report the presence or absence of snow in each pixel whereas fsca indicates the percentage of a pixel covered by snow and is considered an important variable for many applications especially when working with moderate resolution imagery gascoin et al 2019 bsca and fsca maps have been generated since the 1960s ramsay 1998 and in recent years various statistical and machine learning approaches have been tested to improve these products kuter 2021 several snow cover datasets from optical remote sensing imagery are freely available at different spatial and temporal resolutions the moderate resolution imaging spectroradiometer modis onboard the terra and aqua satellite platforms with a 250 m to 1 km spatial resolution is the most frequently employed sensor in snow cover mapping due to its high temporal resolution and sensitivity to regions of the electromagnetic spectrum relevant for snow cover detection for instance the modis mod10a1 v6 snow cover daily global 500 m product contains snow cover snow albedo and quality assessment qa data snow cover data are based on a snow mapping algorithm that employs the normalized difference snow index ndsi defined as the normalized difference between green and swir1 landsat bands hall et al 2016 however 500 m spatial resolution is too coarse for many applications especially in mountain regions blöschl 1999 the landsat 4 9 missions provide snow related products for the united states and conterminous regions at 30 m spatial resolution but with a 16 day revisiting time selkowitz and forster 2016 there is an inability to capture snow cover dynamics daily weekly or even monthly in areas with frequent cloud cover for selected regions of the globe the theia snow collection provides on demand sca products at 20 m spatial resolution based on high temporal frequency sentinel 2 and landsat 8 observations gascoin et al 2019 at a lower spatial resolution 1 km but with global coverage there are snow extent datasets based on modis and viirs visible infrared imaging radiometer suite with 375 775 m spatial resolution data https modis snow ice gsfc nasa gov such as the globsnow product from the european space agency www globsnow info the tradeoff between temporal frequency and spatial resolution has been partially solved by data fusion approaches which exploit the strengths of various snow cover datasets for example modis daily observations can be fused with landsat 30 m spatial resolution data allowing for daily fine scale monitoring of snow dynamics based on these two datasets rittger et al 2021 developed a physically based spectral mixture analysis approach and a two stage random forest algorithm to produce daily 30 m fsca maps the algorithm was tested in the us sierra nevada with an overall accuracy of 97 and very little variation in performance between seasons mityok et al 2018 developed a fusion algorithm named modsat ndsi modis and landsat s ndsi that uses the ndsi metric common to both datasets to generate long term trend ndsi curves for each pixel landsat ndsi values are then resampled and overlaid with daily modis ndsi imagery to calculate the final ndsi values their approach was applied across south central british columbia with an overall accuracy of 90 berman et al 2018 developed the snowwarp algorithm which uses dynamic time warping dtw to obtain daily estimates of fsca at 30 m resolution by matching year to year patterns of modis ndsi and rearranging historical landsat fsca to create a denser time series snowwarp bsca maps were derived from 2000 to 2018 using fsca values with a threshold of 15 which has previously been determined as the snow detection limit for the modscag algorithm painter et al 2016 the data were tested against a network of time lapse cameras and snow pillows and results indicated high accuracy with the root mean squared error of fsca product ranging from 31 3 to 68 3 while the f score of the bsca ranged from 87 7 to 98 6 these three approaches have all shown promise in their ability to accurately map fine spatial and temporal scale snow cover yet are currently not available for users in an open source framework fine scale snow cover maps have proven to be an important indicator to understanding a variety of ecological processes and thus its availability to the broader applied research community would be of large benefit for example in the field of wildlife ecology janousek et al 2021 utilized snowwarp fsca maps to model elk aggregation patterns with fsca explaining 28 of the variability similarly rickbeil et al 2020 used snowwarp outputs to evaluate how spring cover dynamics together with early season forage availability altered grizzly bear behavior post den emergence with snow dynamics explaining 45 of the variation in spring activity date berman et al 2019 showed that the use of fsca improved models of grizzly bear habitat selection by 60 given the importance of snow cover in a variety of ecological modeling endeavors the aim of this paper is twofold first to describe an open source framework to enable users to derive daily fsca and bsca maps anywhere in the world we developed this new framework by improving the original implementation of snowwarp and creating a tool combining google earth engine cloud based computing resources gorelick et al 2017 with a new r package second with this new tool we produced a time series of 20 years 2000 2019 of bsca calculated over a mountain catchment in northern italy where extensive meteorological and snow based ground information is available including air temperature precipitation snow depth and streamflow this time series allows the assessment and demonstration of the snowwarp robustness and utility for environmental monitoring and hydrometeorological applications 2 snowwarp background and open source implementation fsca and bsca maps and accompanying statistics date of snow accumulation date of snowmelt and the number of days with snow cover in a year can be generated using the snowwarp tool fig 1 which combines google earth engine gee gorelick et al 2017 and the r programming environment r core team 2017 data can be downloaded for any period between august 1st 2000 and july 31st 2021 daily at 30 m spatial resolution with new data added at the end of each annual time step july 31st operationally the tool is organized as follows a download the snowwarp r package https github com bermane snowwarp and orfeo toolbox https www orfeo toolbox org b organize and pre process data in gee which involves a study area shapefile being uploaded and outputs provided as geotiff data in wgs84 projection at 30 m spatial resolution c download the pre processed data d process snowwarp daily fsca in r and extract key snowwarp annual statistics a detailed description of the tool as well as steps and the information needed to execute it are available on github https github com bermane snowwarp together with complete r and gee javascript codes the r script itself was translated from the original in matlab for which details are provided in berman et al 2018 and summarised below for completeness snowwarp products are derived by leveraging daily modis snow cover data and dynamic time warping dtw which is used to re order historical landsat observations to account for inter annual variability based on the method developed by berman et al 2018 in the first step of the new snowwarp package gee is used to pre process landsat and modis data more specifically imagery acquired by landsat 5 7 and 8 with a cloud cover threshold 70 is used images with cloud cover 70 are excluded because they are more prone to geographical location errors due to the challenges of performing geometrical corrections when ground control points are obscured white et al 2014 the remaining clouds in landsat imagery were then masked by the cfmask algorithm foga et al 2017 modis snow cover data is already provided on gee with clouds masked out hall et al 2016 next the ndsi is calculated from green and short wave infrared landsat reflectance values as follows 1 ndsi green swir1 green swir1 for modis a daily global and 500 meters resolution ndsi product is already available on gee hall et al 2016 linear regression is then applied to both landsat and modis ndsi data to calculate fsca following the regression formula introduced by aalstad et al 2020 2 fsca 1 45 ndsi 0 01 this regression formula was developed by finding the correlation between modis ndsi and fsca derived from landsat 7 salomonson and appel 2004 following the methods of berman et al 2018 fsca from modis and landsat is then adjusted fscaadj to account for the viewable gap fraction as follows 3 fscaadj fsca 1 fcc where fcc is the fractional canopy cover data ranging between 0 and 1 indicating the portion of the pixel area covered by trees sexton et al 2013 as a last step fsca values 100 are set to 100 and values 0 set to 0 landsat and modis fsca data are packaged and automatically downloaded by the end user using the r package provided which allows running the dtw algorithm as detailed in berman et al 2018 dtw is used to combine modis fsca 500 m resolution and 1 day revisitation time and landsat fsca 30 m resolution and 16 day revisitation time to obtain a time series of 30 m resolution daily fsca snow data dtw was originally developed for spoken word recognition sakoe and chiba 1978 and has been commonly used to define patterns between complementary time series in remote sensing baumann et al 2017 petitjean et al 2012 mcconnell et al 1991 3 study area the snowwarp tool was applied in the cordevole river catchment 109 km2 located in the italian dolomites eastern alps fig 2 elevations range from 1016 to 3152 m above sea level covering approximately 10890 ha corresponding to approximately 121 000 snowwarp 30 30 m pixels fig 2 in this area hydrometeorological data are collected and managed by the agency for environmental protection of the veneto region arpav the average yearly precipitation is 1220 mm average monthly temperatures range from 5 7 c in january and 14 1 c in july the high elevations and the geographical location in the internal portion of the southern alps determine that about half of the mean annual precipitation falls as snow generally from late october to april penna et al 2017 snowmelt usually starts in march and can last until july soils are moderately deep with a good drainage capacity the dominant land cover classes at higher elevations are alpine grassland shrubs and rocks while at lower elevations there are forests agriculture and small urban areas guastini et al 2019 4 data daily bsca maps were generated using the snowwarp tool from 1st august 2000 31st july 2020 over the study area daily snow covered area sca was then calculated in hectares as the sum of pixels areas with bsca true unlike bsca and fsca the sca product is not an image it is a value that indicates the total amount of hectares covered by snow over a defined study area time series of air temperature and precipitation were obtained with thiessen 1911 interpolation from data collected in five different local stations arabba passo pordoi caprile malga ciapela and passo falzarego included in or surrounding the study area snow depth was obtained by averaging values from two local stations monti alti ornella and cima pradazzo while streamflow data were collected at the saviner outlet station fig 2 table 1 describes the main features of the hydrometeorological data to analyze the relationships between sca and ground hydrometeorological variables we used the cross correlation function ccf derrick and thomas 2004 and the running correlation coefficient rcc zhao et al 2018 for periods with data availability the ccf provides a measure of association between signals when two time series are cross correlated a measure of temporal similarity is achieved for instance in the relationship between two time series x and y the series y may be related to past lags of the x series ccf helps identify lags of the x variable that might be useful predictors of y rcc is obtained by measuring the pearson correlation in a small portion of the signal 30 days in this case and repeating the process along with a moving window until the entire signal is covered 5 results 5 1 snowwarp benchmark and products the generation of the bsca time series for the study area involved the processing of 1356 landsat and 7249 modis images automatically downloaded from gee as tif files in 5 minutes landsat images were acquired at about 10 a m while modis at about 2 p m landsat 30 m spatial resolution and modis 500 m spatial resolution were stored as tif files the snowwarp r tool took 4 hours to process using a workstation with 32gb ram and 36 cores the fsca landsat data require about 50mb of storage space while fsca modis about 2 5mb the sca and the ground data time series have different time extent and number of observations with sca covering 20 years while ground data covering a shorter period fig 3 all time series showed an annual periodicity and the expected seasonal and annual variability the sca ranged between 164 10890 ha with an average value across the time series of 7300 days of 5159 ha or about 47 of the study area fig 4 presents four examples of sca daily maps selected on four different days of march 2017 during a period with no snowfall events a decrease in the sca over time is evident ranging from 98 10736 ha 01 03 2017 to 83 9112 ha 10 03 2017 64 7020 ha 20 03 2017 and finally 46 5056 ha 31 03 2017 5 2 correlation between sca and ground data the relationship between sca and temperature was analyzed for the period 1st january 2005 16th november 2017 table 1 during this period the ccf value fig 5 bottom ranged from a high negative correlation equal to 0 84 with a lag of zero days to a moderate positive correlation of 0 53 with a lag of 50 the rcc fig 4 top left computed with a 30 day moving window shows a clear seasonal pattern which is even more evident focusing on just the 2015 2017 period fig 5 top right high negative correlation values were observed when snow cover reaches its maximum and temperatures are minimum and no lag occurs and vice versa at this point the correlation decreases as the snow cover plateaus a smaller negative peak is observed at the end of the snow melting period with minimum sca and maximum temperatures the relationship between sca and precipitation was analyzed from 1st january 2005 16th november 2017 i e the period for which precipitation data were available table 1 the ccf range was close to zero between 0 09 with 50 days lag and 0 17 with 6 days lag fig 6 bottom it is worth noting however that there is no distinction in input data between solid and liquid precipitation looking at the rcc fig 6 top left high positive values are observed in fall when rainfall in this climate is typically abundant and if the temperature is sufficiently low snowfall occurs then correlations decrease while the sca increases when the area is fully covered by snow no correlation is observed as precipitation is solid and determines only an increase of snow depth without a corresponding increase in sca fig 6 top right the relationship between sca and snow depth was analyzed for the period 1st october 2009 31st december 2016 table 1 the ccf fig 7 bottom shows a maximum value equal to 0 74 with a time lag of 8 days and a negative minimum value of 0 64 with a 36 day lag the latter possibly indicating the time lag between snow melting starting at lower elevations and a consistent decrease in snow depth the rcc values fig 7 top computed with a 30 day moving window are always positive and maximum values are observed when sca reaches its maximum extent then the correlation decreases to reach a minimum at the end of the snow season with minimum sca it is interesting to note the very low correlation peaks occurring with small snowfall events that cause an increase of sca but do not cause snow accumulation these events tend to occur during early fall when temperatures are still well above zero in large areas of the catchment and differences occur along the elevation gradient precipitation may occur as snow only at higher elevations thus leading to an increase in sca while as rain at lower elevations leading to decreasing values of snow depth the relationship between sca and streamflow at the saviner outlet is reported for the period 1st january 2005 16th november 2017 table 1 streamflow is the result of complex interactions among different variables especially in alpine areas in addition to precipitation and snowmelt cochand et al 2019 camporese et al 2014 zuecco et al 2019 for this reason the ccf correlations between streamflow and sca were low when compared to previously observed correlations more specifically ccf analysis fig 8 bottom shows a maximum negative correlation equal to 0 43 with 15 days lag and a positive correlation of 0 38 with 50 days lag the rcc fig 8 top computed with a 30 day moving window shows high positive peaks in autumn when streamflow is high due to large rainfall events and then switching to snowfall when air temperatures drop a high negative peak is observed at the end of the snowmelt period when sca is minimum and streamflow is high due to the snowmelt contribution in addition to the ccf and rcc analysis the links between sca change and streamflow were graphically explored for selected years and months to corroborate the existing relationships between sca and hydrometeorological variables for example in the first days of march 2010 fig 9 top left snow starts to melt as sca decreases but streamflow markedly increases only after 15 days when snow at higher elevation also melts providing larger contributions to streamflow this example agrees with the 15 day lag observed in the corresponding ccf streamflow relationship fig 8 conversely in march 2014 fig 9 mid left a more direct relationship between snowmelt and streamflow increase was observed in the first days of the month interestingly in the second part of the month the snow ceases to melt and accordingly streamflow decreases in the last five days of march 2014 the snow begins again to melt smaller sca and similarly the streamflow begins to increase similar observations can be drawn from the other panels in fig 9 for march 2017 april 2007 2010 and 2015 6 discussion this work demonstrates a framework for applied users to compute daily 30 m fsca and bsca maps using the snowwarp algorithm which is the first time this novel high resolution data is readily available and with global coverage we have additionally provided an example of its application in a snow dominated study area in the italian alps the algorithm on which the snowwarp tool is based relies on a landsat modis data fusion approach which represents the most recent advancement in data fusion and allows for better exploitation of the strengths of different remote sensing platforms the original algorithm was robustly tested and previously applied in previous studies berman et al 2018 janousek et al 2021 rickbeil et al 2020 here a slightly modified version is presented to i employ datasets available globally and ii use open source software the procedure to generate fsca bsca and sca products is now easy to be applied on open access resources namely r and gee and allows the users to specify the area of interest and extract annual statistics overall this research illustrates that the snowwarp tool enables monitoring of daily snow dynamics from 2000 onward and is accessible to applied users without the need for large computing resources or complex code this feature is common to most cloud computing based hydrological products exploiting gee castelli and bresci 2019 which makes the application of complex algorithms over very large areas possible francini et al 2021 to confirm the robustness and potential uses of data generated by snowwarp the relationships between sca and meteorological and streamflow data were explored over several years in most cases the results showed intuitive explanations for the dynamics of the variables including the time lag occurring between changes in one variable and effects on another one as well as expected seasonal patterns the correlation between sca and temperature showed very large negative values in winter when temperatures are at a minimum and the snow cover extent is large fig 5 the correlation between sca and snow depth was also very high and always positive with larger correlation values starting in the late fall in correspondence to the onset of snow accumulation lasting until the melt period fig 7 these observations are consistent with the typical behavior of snow dominated mountain catchments kirchner et al 2020 zuecco et al 2019 penna et al 2017 staudinger et al 2017 cochand et al 2019 indicating that the snowwarp sca product is reliable and accurate in depicting sca temporal patterns some low correlations were observed during early winter short snowfall events which increased the snow covered area but had a limited impact on the depth of the snow layer the correlation between sca and precipitation generally was moderate with both positive and negative values due to the lack of distinction between snow vs rain precipitation fig 6 the observed seasonal correlation patterns are in line with what is expected for snow dominated catchments in temperate climates marin et al 2020 diodato et al 2022 with positive peaks occurring in fall at the onset of the snowpack formation when most of the precipitation falls as snow and negative peaks occurring in the summer period the relationship between sca and streamflow is complex and as expected the time series correlations were not particularly strong fig 7 on the other hand the observed correlation peaks were well explained by the snow accumulation and melting seasonal dynamics major positive peaks in the correlation between sca and streamflow were registered in summer when there is no snow on the ground sca 0 and precipitation and streamflow are minimal alternatively major negative peaks in correlation were registered during the snowmelt period when sca decreases quickly and streamflow increases further tests are needed to better investigate this relationship which highlights the potential of the tool to provide input data and reduce prediction uncertainty in hydrological models di marco et al 2021 several applications can benefit from the availability of snow cover area data produced by the snowwarp package for example specific ecology and conservation questions could be better answered using high resolution snow cover data examples of applications related to wildlife were presented by janousek et al 2021 berman et al 2019 and rickbeil et al 2020 who exploited the dataset produced with the original berman et al 2018 algorithm several researchers have also investigated the role of snow cover changes in influencing vegetation growth and dynamics on the tibetan plateau wang et al 2018 exploited satellite observations to show how snow cover changes promoted vegetation growth in drier areas over the alps modis data provided evidence that the relationship between snow cover and vegetation strongly depends on altitude and helped to understand the relative importance of snow and snowmelt as a driver of other climate variables asam et al 2018 furthermore snow cover shows to be an important parameter for the evolution of biodiversity patterns in the arctic region as shown by the simulations based on earth observation data carried out by niittynen et al 2018 for hydrology snow volume derived from snow cover and depth information helped to predict winter and early spring discharge in watersheds in canada and the us dyer 2008 in addition a strong correlation between snow cover and discharge was found in the northwest himalayas to assess the variability of snowmelt related flooding sharma et al 2012 in alpine catchments guastini et al 2019 zuecco et al 2019 streamflow response was linked to snow cover variation during the snowmelt period in snow dominated catchments snow cover data derived from modis proved useful to develop streamflow estimations and runoff forecasts such as in upper euphrates basin sorman et al 2019 in the italian alps the snow cover maps reduced predictive streamflow uncertainty in hydrological modeling di marco et al 2021 they were also used to validate methodologies for forecasting hydropower production corbari et al 2022 and to calibrate snow accumulation and melt models he et al 2014 ceperley et al 2020 lastly software such as snowwarp can be applied to evaluate climate change impacts on the most globally vulnerable snowpack systems saavedra et al 2018 hammond et al 2018 7 conclusions in this paper we i demonstrated the open source snowwarp tool ii exploited gee capabilities to assist with data preparation and initial processing of remote sensing imagery iii tested how snow cover dynamics were related to different hydrometeorological variables in a case study in the italian alps the snowwarp tool exploits the gee cloud computing platform functionalities and combines landsat and modis satellite imagery through the dtw algorithm it delivers daily snow cover datasets at a 30 meter spatial resolution over areas of interest globally however to date it is challenging to apply snowwarp globally and produce stable global snow cover data due to the computational time and storage requirements of such high resolution data to solve this issue future research should aim to implement the dtw currently implemented in our r package on gee moreover high correlations between sca and ground data suggest that the product is reliable and useful but more solid accuracy assessments should be performed across different global regions high resolution snow cover data from snowwarp can contribute to numerous applications especially when the production of data is facilitated by an easy open access based framework such as the tool presented here this is especially true in remote regions where it is difficult to acquire ground data or where the availability of long time series of snow cover can support the development of mitigation strategies for dangerous phenomena such as flooding or species loss declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments the authors thank the agency for environmental protection of the veneto region arpav for the meteorological hydrometric and snow depth data for the catchment cordevole at saviner the original snowwarp model development was supported by the grizzly paw project nserc file crdpj 486175 15 grantee n c coops frm ubc in collaboration with fri research and partners 
