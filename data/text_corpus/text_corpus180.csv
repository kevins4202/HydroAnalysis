index,text
900,particle filters pfs have received increasing attention by researchers from different disciplines including the hydro geosciences as an effective tool to improve model predictions in nonlinear and non gaussian dynamical systems the implication of dual state and parameter estimation using the pfs in hydrology has evolved since 2005 from the pf sir sampling importance resampling to pf mcmc markov chain monte carlo and now to the most effective and robust framework through evolutionary pf approach based on genetic algorithm ga and mcmc the so called epfm in this framework the prior distribution undergoes an evolutionary process based on the designed mutation and crossover operators of ga the merit of this approach is that the particles move to an appropriate position by using the ga optimization and then the number of effective particles is increased by means of mcmc whereby the particle degeneracy is avoided and the particle diversity is improved in this study the usefulness and effectiveness of the proposed epfm is investigated by applying the technique on a conceptual and highly nonlinear hydrologic model over four river basins located in different climate and geographical regions of the united states both synthetic and real case studies demonstrate that the epfm improves both the state and parameter estimation more effectively and reliably as compared with the pf mcmc keywords particle filters markov chain monte carlo genetic algorithm hydrologic prediction 1 introduction accurate and reliable estimation of prognostic variables such as streamflow and soil moisture has always been one of the main challenges for hydrologists although hydrologic modeling can provide estimates of these quantities the simulation results are potentially biased or erroneous given the following uncertainties 1 forcing data uncertainty due to the limitation of measurements and spatio temporal representativeness of the data 2 parameter uncertainty due to conceptualization of the model and non uniqueness of parameters 3 model structural uncertainty due to the imperfect representation of a real system 4 initial and boundary condition uncertainty therefore hydrologic predictions are better generated within a probabilistic framework providing a mechanism to estimate the uncertainties involved in all layers of hydrologic predictions moradkhani et al 2012 most often this is performed through bayesian inference bayesian methods have been well acknowledged and used in numerous efforts to estimate the uncertainties in hydrologic model predictions e g kuczera and parent 1998 marshal et al 2004 moradkhani et al 2005 dechant and moradkhani 2014 yan et al 2015 pathiraja et al 2016a pathiraja et al 2016b data assimilation da has been recognized as one of the effective methods to improve hydrologic prediction currently the most widely used da technique in the hydrologic community is the ensemble kalman filter enkf reichle et al 2002 crow and wood 2003 de lannoy et al 2007 although the successful application of the enkf and its variants has been reported in hydrologic literature this technique has some inherent features resulting in sub optimal performance these include the gaussian assumption of errors linear updating rule within the enkf and violation of water balance that limit its superiority moradkhani et al 2005 matgen et al 2010 noh et al 2011 plaza et al 2012 dechant and moradkhani 2012 yan and moradkhani 2016 given these concerns data assimilation by means of particle filter pf as a viable alternative to the enkf has garnered increasing attention in literature e g noh et al 2011 moradkhani et al 2012 montzka et al 2013 dong et al 2015 yan et al 2017 the pf approach can relax the gaussian assumption of error distributions by potentially characterizing multimodal or skewed distribution in state variables and parameters therefore it can provide a thorough representation of the posterior distribution for a given nonlinear and non gaussian system dechant and moradkhani 2012 presented a detailed performance assessment between the enkf and pf and found more robust and effective performance of the pf with respect to the enkf verified by both deterministic and probabilistic measures van leeuwen 2009 provided a comprehensive review of the pf and its applications in geophysical systems despite the success of the pf one potential limitation has been the particle degeneracy particle degeneracy occurs when most of the particles have negligible weight such that the variance of importance weights increases over time and the particles lose their ability to correctly approximate the posterior distribution in order to mitigate the particle degeneracy problem several procedures have been proposed noh et al 2011 proposed the lagged filtering approach to preserve the sample diversity andrieu et al 2010 proposed the mcmc technique within the pf to reduce weight degeneracy moradkhani et al 2012 re designed the pf mcmc by integrating the variable variance multiplier vvm leisenring and moradkhani 2012 for the appropriate and objective perturbation of observation and parameters and then conducted the combined state parameter estimation using the pf mcmc on the other hand intelligent search and optimization methods categorized as metaheuristic algorithms mas in computer science literature have also been used to mitigate the degeneracy problem this includes genetic algorithm ga higuchi 1997 kwok et al 2005 park et al 2009 evolution strategy es uosaki et al 2003 uosaki et al 2004 particle swarm optimization pso wang et al 2006 li et al 2013 ant colony optimization aco xu et al 2009 park et al 2010 zhu et al 2010 immune genetic algorithm iga han et al 2011 and inverse weed optimization iwo ahmadi et al 2012 among these attempts ga has received more attention and is known as a more effective method to combine with the pf to prevent the particle degeneracy kwok et al 2005 proposed an evolutionary pf where the genetic operators were performed on the prior particles however the procedure only included the crossover operator heris and khaloozadeh 2014 incorporated the non dominated sorting ga ii nsga ii within the pf framework to improve the estimation performance this strategy is only feasible for simple models due to large computational burden since an optimization problem should be solved in every iteration literature suggests that instead of formulating pf into a ga optimization problem incorporating only the evolutionary concept of this approach within the pf has more potential to improve its performance while making it computationally less intensive for example yin and zhu 2015 used ga operators i e crossover and mutation within the pf to modify the small weight particles and they concluded that ga could be an appropriate choice to combine with pf in order to obtain better posterior distributions more specifically in the entire hybrid ga and pf models developed by far the researchers have proposed different formulations to set up ga parameters crossover and mutation although these approaches could enhance the effectiveness of the pfs in different applications the methods appear too subjective in terms of ga parameter selection this means an inappropriate selection of these parameters may degrade the assimilation performance in addition although ga regulates the particle weights properly it may lead to sub optimal performance since it is possible that the shuffled particles after the ga operators move outside the posterior distribution to cope with these deficiencies in this study we combine mcmc with ga algorithm hereafter referred to as ga mcmc and use it within the importance sampling step of the pf mcmc model moradkhani et al 2012 the presented algorithm is named the evolutionary particle filter with mcmc epfm the developed method not only reduces the subjectivity of ga with respect to its parameter selection but also enables the user to further tackle the particle degeneracy and sample impoverishment problems to improve the hydrologic predictions the rest of the paper is organized as follows in section 2 the theory and details of the proposed epfm are described in section 3 the synthetic and real case studies are presented and the results are discussed finally the conclusion of this paper is provided in section 4 2 theory and methods 2 1 sequential bayesian estimation following previous work by moradkhani 2008 the differential equations that describe the generic nonlinear dynamic system are described as follows 1 x t f x t 1 u t θ ω t 2 y t h x t υ t where x t r n is a vector of the uncertain state variables at time t ut is the uncertain forcing data θ r d is a vector of model parameters y t r m is a vector of observation data ω t represents the model error and υ t is the measurement error in most cases ω t and υ t are assumed as white noises with mean zero and covariance qt and rt respectively furthermore the two noises ω t and υ t are assumed to be independent since moradkhani et al 2005 provided a detailed literature on the sequential bayesian filtering formalism in this paper only a concise introduction is presented based on the bayes law the posterior distribution of the state variables at time t is as follows 3 p x t y 1 t p x t y 1 t 1 y t p y t x t p x t y 1 t 1 p y t y 1 t 1 p y t x t p x t y 1 t 1 p y t x t p x t y 1 t 1 d x t 4 p x t y 1 t 1 p x t x t 1 y 1 t 1 d x t 1 p x t x t 1 p x t 1 y 1 t 1 d x t 1 where p yt xt is the likelihood for time step t p xt y 1 t 1 is the prior distribution and p yt y 1 t 1 is the normalization factor the marginal likelihood function p y 1 t can be computed as 5 p y 1 t p y 1 p y t y 1 t 1 where the normalization factor p yt y 1 t 1 is calculated as follows 6 p y t y 1 t 1 p y t x t y 1 t 1 d x t p y t x t p x t y 1 t 1 d x t the multi dimensional integration and analytical solution of 3 is only feasible for special cases such as the linear systems with gaussian noise therefore for practical reasons the posterior distribution is approximated using a set of random replicates with associated weights 2 2 particle filter markov chain monte carlo pf mcmc the pf mcmc moradkhani et al 2012 is an extension of the pf sir sampling importance resampling the application of mcmc to the pf leads to a more complete estimation of posteriors and reduces the risk of parameter impoverishment the pf mcmc consists of two steps 1 generating ensemble model state predictions and parameters and 2 updating predictions and parameters when new observations become available this leads to the posterior in 3 which is approximated as 7 p x t y 1 t i 1 n w i δ x t x t i where w i is the posterior weight of the ith particle δ is the dirac delta function and n is the number of particles the normalized weights are calculated using 8 w i w i p y t x t i θ t i i 1 n w i p y t x t i θ t i where w i is the prior particle weights and the p y t x t i θ t i can be computed from the likelihood l y t x t i θ t i for simplicity a gaussian likelihood can be used to estimate l y t x t i θ t i 9 l y t x t i θ t i 1 2 π m r t e x p 1 2 y t h x t i t r t 1 y t h x t i to obtain approximate samples from p xt y 1 t a resampling operation is necessary the sir algorithm is suggested to resample the particles with a probability greater than the uniform probability this algorithm eliminates the particles with lower weights while retaining the particles with higher weights after application of the sir algorithm all the particle weights are set to 1 n because particles with large weights are likely to be drawn multiple times during resampling this may result in a loss of diversity among particles the problem known as sample impoverishment to avoid the sample impoverishment a perturbation of the resampled parameters is recommended then a proposal distribution is formed to generate proposed parameters θ t i p 10 θ t i p θ t i ɛ t i ɛ t i n 0 s t v a r θ t i where θ t i is the parameters after sir v a r θ t i is the variance of the prior parameters at the current time step and st is a small tuning time variant parameter to reject the parameter samples θ t i p that move outside the filtering posterior distribution a metropolis acceptance ratio α is used to determine whether to accept the proposed parameters 11 α min 1 p x t i p θ t i p y 1 t p x t i θ t i y 1 t where p x t i p θ t i p y 1 t is the proposed joint probability distribution 12 p x t i p θ t i p y 1 t p y 1 t x t i p θ t i p p x t i p θ t i p y 1 t 1 p θ t i p y 1 t 1 13 x t i p f x t 1 i u t i θ t i p where x t i p is a sample from the proposal state distribution at time step t and u t i is the perturbed forcing data associated with the ith particle it is noted that we perturb the forcing data by n times prior to generating the proposal distribution therefore it is necessary to use the same perturbed forcing data u t i to calculate the proposed state x t i p unlike the variants of the kalman filters this algorithm does not adjust the state variables and therefore preserves the water balance since the optimal tuning factor st is unknown in a sequential framework it is beneficial to treat variable s t as a time variant quantity and estimate it automatically moradkhani et al 2012 modified the variable variance multiplier vvm method leisenring and moradkhani 2012 to automatically obtain the most fitting tuning factor st in 11 2 3 evolutionary pf mcmc epfm here we propose an approach relying on hybrid ga and mcmc techniques to pre process the ensemble members within the pf mcmc method moradkhani et al 2012 the developed methodology integrates the mcmc algorithm with the evolutionary concept of ga to enhance the performance of particle filtering the proposed algorithm is illustrated in fig 1 the main idea behind this method is to construct a more informative prior which leads to a more reliable posterior distribution it is important to mention that in this framework the mcmc technique is used twice once during the importance sampling step when it is combined with the ga to obtain the desirable proposal state distribution proposed ga mcmc framework which will be discussed in section 2 3 1 green dashed line shown in fig 1 and later in the resampling step for parameter updating as discussed in section 2 2 in fact what distinguishes the epfm from the pf mcmc is the utilization of ga mcmc in the importance sampling step of the pf mcmc approach here the hybrid ga mcmc technique and how it is used within the pf mcmc model are explained in detail 2 3 1 hybrid ga mcmc the ga mcmc is a new strategy that can be applied to pf to simultaneously mitigate the particle degeneration by increasing the particle diversity and enhance the accuracy and reliability of data assimilation the gas are adaptive heuristic search methods based on principles of natural evolution and genetics the ga encodes the decision variables of a problem into finite length strings of alphabets holland 1975 the strings also referred to as chromosomes are the candidate solutions to the search problem a chromosome is composed of a sequence of genes from a certain alphabet an alphabet could consist of continuous values binary digits integers symbols matrices etc the representation of chromosomes depends on how the problem is structured in the ga fig 2 demonstrates a color coding of the proposed ga mcmc process as it is seen from figs 1 and 2 we use two basic operators of ga crossover and mutation to combine particles with each other it is worth mentioning that each particle x t r n in epfm algorithm is looked upon as a chromosome in ga terminology each chromosome is comprised of number of genes or in similar term each particle is made of number of state variables to do the crossover the chromosomes should be selected from the ensemble based on darwin s evolution theory the best chromosomes should survive and produce new offspring there are many approaches to select the best chromosomes for instance roulette wheel selection tournament selection rank selection boltzman selection and some others in the current research the roulette wheel selection as a simple selection method is chosen and briefly described here since parent chromosomes are selected according to their fitness values we need to assign a fitness value for each chromosome therefore those chromosomes that have significant fitness values will be more likely to be selected as the parent chromosomes the value of weights as an appropriate indication of ensemble member quality can be directly used as the fitness value hence the particle weight is defined as the fitness value as shown in eq 14 14 f t i w t i it is necessary for the chromosomes to be arranged in the descending order of their fitness values then a roulette wheel selection should be implemented to select parent chromosomes this selection method assumes that the probability of a selection is proportional to the fitness of a chromosome in this study each chromosome is characterized by its fitness value f t i i 1 2 n thus the selection probability of the ith chromosome is given as 15 p t i f t i f t i this method is analogous to a roulette wheel in which each slice is proportional in size to the fitness value f t i of each chromosome the selection of a chromosome is equivalent to randomly choosing a point on the wheel and locating the corresponding sector in this method a line segment of length i 1 n f t i out of consecutive sectors of length f t i i 1 2 n is constructed and using a random number r 0 r i 1 n f t i the corresponding sector is identified resulting in the respective chromosome lipowski and lipowska 2012 hence the chromosomes with larger weights will have higher chance to generate new offspring chromosomes while the chromosomes with smaller weights will have less chance to do so this inspires to reduce the number of particles with small weights and find more particles with large weights this property of ga not only intensifies the diversity of population to prevent the particle degeneracy but also improves the reliability and accuracy of model estimation by providing a more realistic form of posterior distribution up to this point the proposed methodology allows us to know how to select the parent particles for the crossover operation step 2 shown in fig 2 there are numerous methods in genetic algorithm to do a crossover there are one point and two point crossovers for binary encoding situation and arithmetic and discrete crossover for float encoding situation janikow and michalewicz 1991 magalhaes mendes 2013 generally the application of genetic operators depends on the problem defined in this study the arithmetic crossover which is widely used in the context of evolutionary pf is adopted for the crossover operation park et al 2009 yin and zhu 2015 in the arithmetic crossover a pair of new particles offspring is generated by a linear combination of a pair of parent particles it is noted that the roulette wheel selection is implemented based on a crossover probability ρ c such that it specifies how many particles are to be nominated for the crossover operation step 2 shown in fig 2 the formulation of the arithmetic crossover operator can be expressed as 16 x t 1 i ξ x t 1 i 1 ξ x t 1 j 17 x t 1 j 1 ξ x t 1 i ξ x t 1 j where x t 1 i and x t 1 j are the parent particles the red and yellow particles shown in step 3 of fig 2 x t 1 i and x t 1 j are the pair of new offspring particle the black and blue particles in step 4 of fig 2 and ξ is a uniform random value in the range of 0 1 this parameter plays a pivotal role in transferring information from the parent particle to offspring particle it should be noted that if ξ 1 the crossover will not occur however if ξ 0 more information will be transferred from x t 1 j to x t 1 i x t 1 i to x t 1 j to further promote diversity of the particles a mutation strategy is designed this process alters each chromosome particle through changing its gene details of step 5 in fig 2 it is realized by the eq 18 that x t 1 k and x t 1 k are the particles before and after mutation process respectively the black and purple particles in step 5 and 6 of fig 2 the mutation operation is executed with the appropriate mutation probability ρ m for instance if ρ m 0 1 this means that 10 of the newly generated offspring crossed over particles are randomly selected to do the mutation operation step 5 shown in fig 2 it should be noted that the mutated gene is chosen randomly along the chromosomes see detail of step 5 in fig 2 18 x t 1 k x t 1 k η x t 1 k x t 1 i x t 1 j η n 0 ψ v a r x t 1 k where n 0 ψ v a r x t 1 k represents a random sample from the gaussian distribution with mean zero and variance ψ v a r x t 1 k where v a r x t 1 k is the variance of the prior states at the time t 1 and ψ is a small tuning parameter here we use gaussian mutation process in which ψ should be tuned according to different systems in this study it is preferred to use 0 01 as the value of the tuning parameter therefore the new proposal state x t 1 i p is generated through the aforementioned crossover and mutation process the number of proposal states is the same as the initial ensemble n this is highlighted in fig 2 illustrating eight particles in both the initial ensemble and the refined ensemble in the epfm the proposal states can be viewed as a new generation of offspring the ensuing task is to come up with an approach to accept or reject the proposal particles this study uses the mcmc algorithm as an effective technique to decide which offspring should survive or be eliminated the mcmc move is illustrated in fig 2 and the pdf of the proposed joint state parameters p x t i p θ t i y 1 t is estimated as 19 x t i p f x t 1 i p u t i θ t i 20 p x t i p θ t i y 1 t p y t x t i p θ t i p x t i p θ t i y 1 t 1 p θ t i y 1 t 1 where p y t x t i p θ t i is calculated based on the same likelihood function used in eq 9 to calculate the proposal state pdf p x t i p θ t i y 1 t 1 an assumption is made that the proposal states are assumed to fit marginal gaussian distributions with a mean of μ t and a variance of σ t 2 although a joint distribution would be preferred in this scenario the marginal priors are selected because the states have nonlinear relationships and thus have a joint distribution that is difficult to fit to calculate the proposal pdf based on the gaussian distribution weighted mean and variance of the filtering posterior must be calculated as follows 21 x t i f x t 1 i u t i θ t i 22 μ t w t 1 i x t i 23 σ t 2 w t 1 i x t i μ t 2 the joint pdf of the proposal and prior states are then compared via the metropolis acceptance ratio α to determine the acceptance probability in 24 24 α min 1 p x t i p θ t i y 1 t p x t i θ t i y 1 t min 1 p y 1 t x t i p θ t i p x t i p θ t i y 1 t 1 p y 1 t x t i θ t i p x t i θ t i y 1 t 1 the acceptance and rejection of the new states is shown in fig 2 this acceptance rejection process ensures that in each time step an appropriate prior state distribution is constructed leading to a better estimation of the posterior distribution this concept is the core part of this proposed epfm technique which is formulated using the ga mcmc algorithm 3 streamflow prediction results in this study synthetic and three real data experiments are performed to compare the effectiveness and robustness of the proposed epfm with those of the pf mcmc the sacramento soil moisture accounting model sac sma is used here to simulate the streamflow at four different basins the sac sma model which was first introduced by burnash 1974 is referred to as the spatially lumped continuous soil moisture model and represents each basin vertically by two soil zones an upper zone and a lower zone the upper zone models the short term storage capacity while the lower zone accounts for long term groundwater storage this model is used to generate daily streamflow from daily potential evapotranspiration pet and precipitation data model parameters are tabulated in table 1 and the model state variables are summarized in table 2 in the da setting precipitation and pet are assumed to have lognormal and normal error distributions with a relative error of 25 respectively using these values it is presumed that meteorological observations errors stemming from spatial heterogeneity inherent in these variables and sensor errors can be accounted for dechant and moradkhani 2012 also the streamflow observation errors are assumed to be normally distributed with a 15 relative error for a synthetic study the model is assumed perfect with no structural error while for a real case study the model error is assumed to follow a normal distribution with a relative error of 25 this study assumes that all errors are uncorrelated and applies them with the same magnitude in both the pf mcmc and epfm in order to provide an assessment of the epfm performance both deterministic and probabilistic measures are used in this study including the nash sutcliffe efficiency nse kling gupta efficiency kge mean absolute bias mab reliability r and 95 exceedance ratio er95 since these measures have been described by a large and sufficient body of literature only a brief definition of each is provided here nse has been widely used for calibration and evaluation of hydrological models since it was developed in 1970 this measure determines the relative magnitude of residual variance noise compared to the observed data variance information nse can range from to 1 with nse 1 as perfect fit between observation and simulation a successor of this metric called kge developed by gupta et al 2009 has also been recently used as a robust measure in analyzing the accuracy of hydrological simulations this metric measures the euclidean distance in a three dimensional space between the ideal point 1 1 1 and the pearson product moment correlation coefficient relative variability standard deviation and ratio of the average observation and simulation over the analysis period similar to nse kge varies from to 1 with kge 1 being a perfect fit between the observed and simulated values mean absolute bias mab is the magnitude of the bias for a given estimate the perfect value of this metric is zero meaning that no bias is observed between the simulated and observed data the exceedance ratio at 95 percentile er95 is an indicator by which the spread of the ensemble is evaluated dechant and moradkhani 2012 it is noted that the ideal value for er95 is 5 an er95 greater than 5 indicates the predictive distribution is too narrow while less than 5 indicates the predictive distribution is too wide the r value represents the reliability with zero as the worst and 1 as the best value this value is associated with the predictive quantile quantile plot q q plot which reflects its proximity to uniform distribution dechant and moradkhani 2012 3 1 synthetic case study we first conduct a synthetic case study to demonstrate the usefulness and applicability of the epfm for streamflow prediction with the refined prior distributions it is hypothesized that the epfm could lead to a more accurate and reliable streamflow prediction in this paper leaf river basin located in the southern mississippi was considered for a synthetic study this basin has an area of 743 square miles with the prevailing climate of humid subtropical characterized by mild temperate winters and dry summers and precipitation that is fairly well distributed throughout the year synthetic streamflow observations were generated by adding gaussian noise 15 relative error to streamflow simulations the synthetic streamflow data represent noisy measurements that are sequentially assimilated into the sac sma model the initial state variables and parameter sets were generated by latin hypercube sampling lhs method resulting in a more uniform ensemble spread over all possible parameter permutations and therefore reducing the sampling variance streamflow prediction over four years of analysis using both filters is shown in fig 3 note that this figure is reported based on an ensemble size of 100 the assimilation results are also numerically shown in this figure indicating that the epfm outperforms the pf mcmc according to both deterministic and probabilistic measures it is observed that the epfm outperforms the pf mcmc based on both deterministic and probabilistic measures these results suggest that the proposed epfm produces a more accurate expected value than the pf mcmc according to deterministic measures the probabilistic measures also show the improved performance of epfm in streamflow prediction for example the reliability r increases from 0 62 for the pf mcmc to 0 91 for the epfm indicating a more reliable ensemble prediction by the epfm to analyze the spread of the ensemble prediction the er95 is used the er95 will be 5 for an ideal distribution er95 greater than 5 suggests that the distribution is too narrow meaning that the observations are falling outside the ensemble range at 95 percentile and er95 less than 5 suggests that the distribution is too wide the er95 of pf mcmc is 27 indicating that the pf mcmc remains overconfident due to particle degeneracy however the er95 of epfm at 9 is closer to the optimal value suggesting a more accurate characterization of the uncertainty in order to further investigate the ability of proposed epfm method to estimate the posterior distribution and explore the scalability of epfm i e ability to consistently outperform pf mcmc across ensemble sizes the analysis was further performed over five different ensemble sizes of 50 100 200 500 and 1000 fig 4 depicts the results for both deterministic and probabilistic performance measures based on the five different ensemble sizes for the synthetic experiment over the six subplots in fig 4 similar trends can be observed first it is noted that with increasing the ensemble size the da performances improve for both approaches for example in the epfm the kge increases from 0 83 to 0 95 and the reliability r increases from 0 77 to 0 96 as the ensemble size increase from 50 to 1000 these results are in accordance with previous studies whereas large ensemble size leads to better estimation of the posterior and reduction of weight degeneration dechant and moradkhani 2012 yan et al 2015 secondly compared with the pf mcmc the proposed epfm produces smaller mean absolute bias and greater nse kge r values regardless of the ensemble size these results suggest that the epfm produces a more accurate expected value and a more reliable ensemble prediction than the pf mcmc at all ensemble sizes it is also noted that the er95 of pf mcmc is greater than 5 at all ensemble sizes meaning slightly overconfident prediction fig 4 summarizes the computational demand seconds of both algorithms although the epfm is more accurate than the pf mcmc the computational complexity is larger for the same ensemble size however epfm can provide the same level of accuracy in prediction for much smaller ensemble size e g 100 as compared to the pf mcmc with the ensemble size of 1000 meaning that with less computational demand it can provide comparable performance with the pf mcmc this issue further highlights that by using ga mcmc technique in the importance sampling step of the pf we can effectively improve the assimilation results without a need to increase the ensemble size therefore the developed method is suitable for assimilation of high dimensional problems where large ensemble sizes would have been needed in former versions of particle filtering this promising result is also in accordance with literature suggesting that the improvements of importance sampling in pfs might provide the potential for data assimilation application in large scale systems van leeuwen 2009 in a summary in terms of both deterministic and probabilistic measures the proposed epfm outperforms pf mcmc regardless of the ensemble size this finding comes at no surprise since the epfm is formulated in an intelligent framework where the ga can shuffle and increase diversity of the particles during the filtering process the mcmc move is then used to reject the shuffled particles that move outside the posterior distribution ensuring that shuffled particles contribute to improved performance 3 2 real case study in addition to the synthetic study three real data experiments are also performed in three basins located in different climate and geographical conditions to fully examine the performance of the proposed epfm the daily precipitation potential evapotranspiration and streamflow data for the following three river basins are acquired from the model parameter estimation experiment mopex project duan et al 2006 1 the chehalis river basin with an area of 895 square miles is the second largest basin in washington state this region has a mostly oceanic climate with wet winters autumns and springs and relatively dry summers such that it receives more than 4100 mm precipitation annually making it the wettest area in washington state average temperatures over much of this region in january range from 7 c to 9 c and in july vary from 7 c to 27 c 2 the indian creek watershed is located in the klamath national forest and drains into the klamath river in california state this watershed encompasses the area of 739 square miles and is located predominantly in california with a small northern portion extending into oregon state indian creek watershed with mean annual precipitation of 762 mm is dominated by semi arid climate conditions with summer temperatures of 24 c to 41 c while in the winter it can drop as low as 9 c 3 the carson river watershed originates from the alpine county in california with an area of nearly 3966 square miles 85 of which lies in nevada state the prevalent climate of this basin is arid and hot with sparsely distributed precipitation throughout the year this watershed has an annual precipitation of 127 mm while annual evaporation exceeds 1524 mm more hydro climatic information about the aforementioned three basins is provided in table 3 fig 5 summarizes both deterministic kge nse and mab and probabilistic reliability and er95 measures for five different ensemble sizes for the three basins also in this figure the computational demand of both algorithms is provided expectedly the use of mcmc twice in the epfm method demands more computation however as seen in both deterministic and probabilistic measures the epfm provides more accurate and reliable predictions than pf mcmc for all basins regardless of the ensemble size further support for these results is provided in fig 6 where the streamflow prediction and its uncertainty interval is shown for the chehalis river basin as an example similar to the synthetic study the epfm improves the pf mcmc performance by 7 and 6 in terms of nse and kge respectively for the probabilistic measurement the reliability is improved from 0 67 of pf mcmc to 0 90 of epfm with an increase of about 15 to further interpret the ga mcmc step within the epfm the comparisons of both epfm priors and posteriors seem necessary fig 7 presents the prior and posterior distributions at three daily time steps t 50 431 and 761 for the chehalis river basin in wa two subplots are included for each time step the first subplot a1 b1 and c1 consists of the likelihood function the prior pdf before the ga mcmc and the epfm refined prior pdf after the ga mcmc this subplot also illustrates the prior pdf for the pf ga model the model for which we used only ga algorithm without mcmc in the importance sampling step this analysis was performed to further investigate why the filtering benefits from the combination of mcmc with ga the second subplot a2 b2 and c2 compares the estimated posteriors using the priors with and without the ga mcmc step this subplot also represents the posterior distributions based on the pf ga model only in cases of a and c it is observed that in the pf prior pdfs a large percentage of particles end up far from the observations and they possess negligible weights as a result after the resampling the posteriors provide low skill predictions instead the epfm can provide more informative priors whereas the shuffled particles are drawn in the vicinity of the observations using the ga mcmc step it is noted that many more particles now end up close to the observations by implementing this framework resulting in posteriors with higher accuracy and reliability more specifically the ga mcmc provides the possibility of eliminating most of the small weight particles and replacing them with the new particles with larger weights which are effectively used in generating the posterior distribution this procedure also enhances the diversity of particles when the assimilation is implemented and consequently the degeneracy problem can be mitigated moreover this figure indicates that although using the ga in the pf ga could improve the diversity of particles it does not necessarily guarantee that the shuffled particles are properly representing the prior distribution for instance in fig 7 b1 pf ga prior pdf is even not as good as pf mcmc prior pdf indicating that single ga algorithm could not enhance the prior distribution and consequently resulted in an inaccurate posterior distribution similarly other subplots show no evidence of promising results when pf ga model is implemented the two parameters of crossover and mutation probability need to be tuned when ga is implemented the choice of these parameters depends on the specific problem and this choice may affect the efficiency of ga this issue plays an important role in many applications where ga is combined with the pf the researchers have proposed different formulations to define the aforementioned probabilities in their studies park et al 2007 han et al 2011 however those approaches could not be considered as a universal method to determine the ga parameters since they were designed for a specific application and case study hence it cannot be generalized this issue was also corroborated in the current study where different sets of ga parameters were investigated in implementing the pf ga in this paper by using ga mcmc algorithm within the importance sampling step of the pf mcmc not only can the subjective choice of ga parameters be overcome but also the effectiveness of the pf mcmc method is improved therefore the crossover and mutation probabilities can take any values that is changing these parameters does not exacerbate the results and the epfm always outperforms its counterparts i e pf ga and pf mcmc in summary the motivation of this study was to overcome the particle degeneracy and sample impoverishment problems that the earlier versions of pf have been more susceptible to the focus of the epfm is on increasing the particle diversity which is achieved through mutation and cross over steps in ga however if some particles move far away from the background states the mcmc will capture those and discard from the pool of particles 4 streamflow forecasting results data assimilation plays a very important role in both weather forecasting and hydrologic forecasting by improving model initial conditions in addition to streamflow prediction analysis the applicability and usefulness of the proposed epfm algorithm against pf mcmc algorithm for one day and five day ahead streamflow forecasting was also investigated in the current research for both synthetic and real case studies at each time step we used the posterior distributions updated by the epfm and pf mcmc to generate the streamflow forecasts table 4 summarizes the epfm and pf mcmc model performance statistics for one day and five day ahead forecasting experiments over all case studies used in this study we found that for both lead times the proposed epfm data assimilation method provided more accurate and reliable results than the pf mcmc method this result suggests the superiority of epfm to its ascendant pf mcmc and the potential for operational streamflow forecasting as seen the forecasting skills of both methods decrease from one day to five day lead time this is mainly due to the effect of initial condition on forecasting skill li et al 2009 dechant and moradkhani 2011 yuan et al 2016 since flood events are main concern for emergency managers here we further analyze this issue to this end we compared the epfm and pf mcmc for one day ahead streamflow forecasting skill during the flood season for a synthetic days 200 to 400 and for a real case study days 300 to 600 the real case study considered here is chehalis river basin in wa the majority of precipitation in this basin falls as snow during winter season due to the strong atmospheric river and peak flood often occurs in spring due to snowmelt when temperature rises and snow starts to melt the soil can become nearly saturated over weeks leading to clustered flood events fig 8 from this figure it is seen that the epfm provides better ensemble forecasts than the pf mcmc according to deterministic measures the ensemble mean of epfm nse 0 95 mab 0 26 shows higher skill than the pf mcmc nse 0 91 mab 0 34 and offers a better ability to forecast extreme events e g day 345 according to probabilistic measures the er95 of epfm 2 25 is closer to the ideal value 5 than the pf mcmc 16 41 the higher er95 value of pf mcmc indicates that the ensemble forecast distribution is too narrow and over confident this result can also be observed in fig 8 since the ensemble from pf mcmc fails to forecast peak events e g day 260 and 345 while the 95 predictive interval of the epfm can capture these peak events after the assimilation of a peak flood the epfm leads to better forecast as it can create a better initial condition that is closer to the real condition i e saturated soil moisture according to fig 8 a and b during the flood recession period day 220 to 240 day 350 to 380 the pf mcmc leads to under forecast while the epfm ensemble forecast accurately presents the uncertainty and the ensemble mean closely follows the observation similar results can also be observed in fig 8 c and d for a real case study in summary the epfm outperforms pf mcmc and generates more skillful streamflow forecasts to further support this claim we compared the epfm and pf mcmc for five day ahead streamflow forecasting skill during the flood season for the synthetic days 50 to 350 and the same real case study days 650 to 850 fig 9 similar to one day ahead streamflow forecasting results both deterministic and probabilistic measures demonstrated the superiority of the epfm these results are also consistent with those reported in table 4 5 conclusion a new data assimilation technique the epfm was proposed in this paper to characterize a more accurate and reliable posterior distribution for state variables of interest in data assimilation applications what distinguishes the proposed efpm model from the recently developed pf mcmc model moradkhani et al 2012 is the utilization of ga mcmc technique in the importance sampling step of the pf mcmc model the success and applicability of epfm was evaluated through both deterministic and probabilistic measures the epfm provides a comprehensive approach for both state using ga mcmc within the importance sampling step and parameter updating using mcmc within the resampling step such that the user can select and update either state parameter or both given their needs in research this study suggests using the ga mcmc technique within the particle filtering given the following features 1 it provides the possibility of generating more informative prior leading to better estimation of the posterior distribution in fact the ga mcmc expands the search space by implementing the crossover and mutation steps in the ga and subsequently the search space is refined via the mcmc technique resulting in more desirable prior distribution 2 it significantly minimizes the particle degeneracy and sample impoverishment problems that have been the main concerns in using particle filtering 3 it alleviates the need for large ensemble size which had made it traditionally a limiting factor in using widespread application of particle filters 4 in the standard pf ga algorithm which was recently emerged in the da community the usefulness of the model is more subjective in terms of ga operator i e crossover and mutation selection this problem can be significantly alleviated with the proposed ga mcmc approach embedded in the algorithm 5 using the ga mcmc a small ensemble size suffices to set up the da this is particularly a beneficial feature of this algorithm making it more attractive for large scale systems where increasing the ensemble size and model runs would be a concern 6 this approach enhances the usefulness and effectiveness of the particle filtering as demonstrated through deterministic and probabilistic measures it is noted that a lumped hydrologic model was used in this study for the proof of concept through a comparative analysis with currently the most advanced data assimilation method pf mcmc for hydrologic applications however to address the heterogeneity in forcing and landscape characteristics further examination of the epfm scalability to distributed models is desired that should follow the current study acknowledgment partial financial support for this project was provided by the national science foundation cyber innovation for sustainability science and engineering cybersees grant no ccf 1539605 supplementary materials supplementary material associated with this article can be found in the online version at doi 10 1016 j advwatres 2017 11 011 appendix supplementary materials image application 1 
900,particle filters pfs have received increasing attention by researchers from different disciplines including the hydro geosciences as an effective tool to improve model predictions in nonlinear and non gaussian dynamical systems the implication of dual state and parameter estimation using the pfs in hydrology has evolved since 2005 from the pf sir sampling importance resampling to pf mcmc markov chain monte carlo and now to the most effective and robust framework through evolutionary pf approach based on genetic algorithm ga and mcmc the so called epfm in this framework the prior distribution undergoes an evolutionary process based on the designed mutation and crossover operators of ga the merit of this approach is that the particles move to an appropriate position by using the ga optimization and then the number of effective particles is increased by means of mcmc whereby the particle degeneracy is avoided and the particle diversity is improved in this study the usefulness and effectiveness of the proposed epfm is investigated by applying the technique on a conceptual and highly nonlinear hydrologic model over four river basins located in different climate and geographical regions of the united states both synthetic and real case studies demonstrate that the epfm improves both the state and parameter estimation more effectively and reliably as compared with the pf mcmc keywords particle filters markov chain monte carlo genetic algorithm hydrologic prediction 1 introduction accurate and reliable estimation of prognostic variables such as streamflow and soil moisture has always been one of the main challenges for hydrologists although hydrologic modeling can provide estimates of these quantities the simulation results are potentially biased or erroneous given the following uncertainties 1 forcing data uncertainty due to the limitation of measurements and spatio temporal representativeness of the data 2 parameter uncertainty due to conceptualization of the model and non uniqueness of parameters 3 model structural uncertainty due to the imperfect representation of a real system 4 initial and boundary condition uncertainty therefore hydrologic predictions are better generated within a probabilistic framework providing a mechanism to estimate the uncertainties involved in all layers of hydrologic predictions moradkhani et al 2012 most often this is performed through bayesian inference bayesian methods have been well acknowledged and used in numerous efforts to estimate the uncertainties in hydrologic model predictions e g kuczera and parent 1998 marshal et al 2004 moradkhani et al 2005 dechant and moradkhani 2014 yan et al 2015 pathiraja et al 2016a pathiraja et al 2016b data assimilation da has been recognized as one of the effective methods to improve hydrologic prediction currently the most widely used da technique in the hydrologic community is the ensemble kalman filter enkf reichle et al 2002 crow and wood 2003 de lannoy et al 2007 although the successful application of the enkf and its variants has been reported in hydrologic literature this technique has some inherent features resulting in sub optimal performance these include the gaussian assumption of errors linear updating rule within the enkf and violation of water balance that limit its superiority moradkhani et al 2005 matgen et al 2010 noh et al 2011 plaza et al 2012 dechant and moradkhani 2012 yan and moradkhani 2016 given these concerns data assimilation by means of particle filter pf as a viable alternative to the enkf has garnered increasing attention in literature e g noh et al 2011 moradkhani et al 2012 montzka et al 2013 dong et al 2015 yan et al 2017 the pf approach can relax the gaussian assumption of error distributions by potentially characterizing multimodal or skewed distribution in state variables and parameters therefore it can provide a thorough representation of the posterior distribution for a given nonlinear and non gaussian system dechant and moradkhani 2012 presented a detailed performance assessment between the enkf and pf and found more robust and effective performance of the pf with respect to the enkf verified by both deterministic and probabilistic measures van leeuwen 2009 provided a comprehensive review of the pf and its applications in geophysical systems despite the success of the pf one potential limitation has been the particle degeneracy particle degeneracy occurs when most of the particles have negligible weight such that the variance of importance weights increases over time and the particles lose their ability to correctly approximate the posterior distribution in order to mitigate the particle degeneracy problem several procedures have been proposed noh et al 2011 proposed the lagged filtering approach to preserve the sample diversity andrieu et al 2010 proposed the mcmc technique within the pf to reduce weight degeneracy moradkhani et al 2012 re designed the pf mcmc by integrating the variable variance multiplier vvm leisenring and moradkhani 2012 for the appropriate and objective perturbation of observation and parameters and then conducted the combined state parameter estimation using the pf mcmc on the other hand intelligent search and optimization methods categorized as metaheuristic algorithms mas in computer science literature have also been used to mitigate the degeneracy problem this includes genetic algorithm ga higuchi 1997 kwok et al 2005 park et al 2009 evolution strategy es uosaki et al 2003 uosaki et al 2004 particle swarm optimization pso wang et al 2006 li et al 2013 ant colony optimization aco xu et al 2009 park et al 2010 zhu et al 2010 immune genetic algorithm iga han et al 2011 and inverse weed optimization iwo ahmadi et al 2012 among these attempts ga has received more attention and is known as a more effective method to combine with the pf to prevent the particle degeneracy kwok et al 2005 proposed an evolutionary pf where the genetic operators were performed on the prior particles however the procedure only included the crossover operator heris and khaloozadeh 2014 incorporated the non dominated sorting ga ii nsga ii within the pf framework to improve the estimation performance this strategy is only feasible for simple models due to large computational burden since an optimization problem should be solved in every iteration literature suggests that instead of formulating pf into a ga optimization problem incorporating only the evolutionary concept of this approach within the pf has more potential to improve its performance while making it computationally less intensive for example yin and zhu 2015 used ga operators i e crossover and mutation within the pf to modify the small weight particles and they concluded that ga could be an appropriate choice to combine with pf in order to obtain better posterior distributions more specifically in the entire hybrid ga and pf models developed by far the researchers have proposed different formulations to set up ga parameters crossover and mutation although these approaches could enhance the effectiveness of the pfs in different applications the methods appear too subjective in terms of ga parameter selection this means an inappropriate selection of these parameters may degrade the assimilation performance in addition although ga regulates the particle weights properly it may lead to sub optimal performance since it is possible that the shuffled particles after the ga operators move outside the posterior distribution to cope with these deficiencies in this study we combine mcmc with ga algorithm hereafter referred to as ga mcmc and use it within the importance sampling step of the pf mcmc model moradkhani et al 2012 the presented algorithm is named the evolutionary particle filter with mcmc epfm the developed method not only reduces the subjectivity of ga with respect to its parameter selection but also enables the user to further tackle the particle degeneracy and sample impoverishment problems to improve the hydrologic predictions the rest of the paper is organized as follows in section 2 the theory and details of the proposed epfm are described in section 3 the synthetic and real case studies are presented and the results are discussed finally the conclusion of this paper is provided in section 4 2 theory and methods 2 1 sequential bayesian estimation following previous work by moradkhani 2008 the differential equations that describe the generic nonlinear dynamic system are described as follows 1 x t f x t 1 u t θ ω t 2 y t h x t υ t where x t r n is a vector of the uncertain state variables at time t ut is the uncertain forcing data θ r d is a vector of model parameters y t r m is a vector of observation data ω t represents the model error and υ t is the measurement error in most cases ω t and υ t are assumed as white noises with mean zero and covariance qt and rt respectively furthermore the two noises ω t and υ t are assumed to be independent since moradkhani et al 2005 provided a detailed literature on the sequential bayesian filtering formalism in this paper only a concise introduction is presented based on the bayes law the posterior distribution of the state variables at time t is as follows 3 p x t y 1 t p x t y 1 t 1 y t p y t x t p x t y 1 t 1 p y t y 1 t 1 p y t x t p x t y 1 t 1 p y t x t p x t y 1 t 1 d x t 4 p x t y 1 t 1 p x t x t 1 y 1 t 1 d x t 1 p x t x t 1 p x t 1 y 1 t 1 d x t 1 where p yt xt is the likelihood for time step t p xt y 1 t 1 is the prior distribution and p yt y 1 t 1 is the normalization factor the marginal likelihood function p y 1 t can be computed as 5 p y 1 t p y 1 p y t y 1 t 1 where the normalization factor p yt y 1 t 1 is calculated as follows 6 p y t y 1 t 1 p y t x t y 1 t 1 d x t p y t x t p x t y 1 t 1 d x t the multi dimensional integration and analytical solution of 3 is only feasible for special cases such as the linear systems with gaussian noise therefore for practical reasons the posterior distribution is approximated using a set of random replicates with associated weights 2 2 particle filter markov chain monte carlo pf mcmc the pf mcmc moradkhani et al 2012 is an extension of the pf sir sampling importance resampling the application of mcmc to the pf leads to a more complete estimation of posteriors and reduces the risk of parameter impoverishment the pf mcmc consists of two steps 1 generating ensemble model state predictions and parameters and 2 updating predictions and parameters when new observations become available this leads to the posterior in 3 which is approximated as 7 p x t y 1 t i 1 n w i δ x t x t i where w i is the posterior weight of the ith particle δ is the dirac delta function and n is the number of particles the normalized weights are calculated using 8 w i w i p y t x t i θ t i i 1 n w i p y t x t i θ t i where w i is the prior particle weights and the p y t x t i θ t i can be computed from the likelihood l y t x t i θ t i for simplicity a gaussian likelihood can be used to estimate l y t x t i θ t i 9 l y t x t i θ t i 1 2 π m r t e x p 1 2 y t h x t i t r t 1 y t h x t i to obtain approximate samples from p xt y 1 t a resampling operation is necessary the sir algorithm is suggested to resample the particles with a probability greater than the uniform probability this algorithm eliminates the particles with lower weights while retaining the particles with higher weights after application of the sir algorithm all the particle weights are set to 1 n because particles with large weights are likely to be drawn multiple times during resampling this may result in a loss of diversity among particles the problem known as sample impoverishment to avoid the sample impoverishment a perturbation of the resampled parameters is recommended then a proposal distribution is formed to generate proposed parameters θ t i p 10 θ t i p θ t i ɛ t i ɛ t i n 0 s t v a r θ t i where θ t i is the parameters after sir v a r θ t i is the variance of the prior parameters at the current time step and st is a small tuning time variant parameter to reject the parameter samples θ t i p that move outside the filtering posterior distribution a metropolis acceptance ratio α is used to determine whether to accept the proposed parameters 11 α min 1 p x t i p θ t i p y 1 t p x t i θ t i y 1 t where p x t i p θ t i p y 1 t is the proposed joint probability distribution 12 p x t i p θ t i p y 1 t p y 1 t x t i p θ t i p p x t i p θ t i p y 1 t 1 p θ t i p y 1 t 1 13 x t i p f x t 1 i u t i θ t i p where x t i p is a sample from the proposal state distribution at time step t and u t i is the perturbed forcing data associated with the ith particle it is noted that we perturb the forcing data by n times prior to generating the proposal distribution therefore it is necessary to use the same perturbed forcing data u t i to calculate the proposed state x t i p unlike the variants of the kalman filters this algorithm does not adjust the state variables and therefore preserves the water balance since the optimal tuning factor st is unknown in a sequential framework it is beneficial to treat variable s t as a time variant quantity and estimate it automatically moradkhani et al 2012 modified the variable variance multiplier vvm method leisenring and moradkhani 2012 to automatically obtain the most fitting tuning factor st in 11 2 3 evolutionary pf mcmc epfm here we propose an approach relying on hybrid ga and mcmc techniques to pre process the ensemble members within the pf mcmc method moradkhani et al 2012 the developed methodology integrates the mcmc algorithm with the evolutionary concept of ga to enhance the performance of particle filtering the proposed algorithm is illustrated in fig 1 the main idea behind this method is to construct a more informative prior which leads to a more reliable posterior distribution it is important to mention that in this framework the mcmc technique is used twice once during the importance sampling step when it is combined with the ga to obtain the desirable proposal state distribution proposed ga mcmc framework which will be discussed in section 2 3 1 green dashed line shown in fig 1 and later in the resampling step for parameter updating as discussed in section 2 2 in fact what distinguishes the epfm from the pf mcmc is the utilization of ga mcmc in the importance sampling step of the pf mcmc approach here the hybrid ga mcmc technique and how it is used within the pf mcmc model are explained in detail 2 3 1 hybrid ga mcmc the ga mcmc is a new strategy that can be applied to pf to simultaneously mitigate the particle degeneration by increasing the particle diversity and enhance the accuracy and reliability of data assimilation the gas are adaptive heuristic search methods based on principles of natural evolution and genetics the ga encodes the decision variables of a problem into finite length strings of alphabets holland 1975 the strings also referred to as chromosomes are the candidate solutions to the search problem a chromosome is composed of a sequence of genes from a certain alphabet an alphabet could consist of continuous values binary digits integers symbols matrices etc the representation of chromosomes depends on how the problem is structured in the ga fig 2 demonstrates a color coding of the proposed ga mcmc process as it is seen from figs 1 and 2 we use two basic operators of ga crossover and mutation to combine particles with each other it is worth mentioning that each particle x t r n in epfm algorithm is looked upon as a chromosome in ga terminology each chromosome is comprised of number of genes or in similar term each particle is made of number of state variables to do the crossover the chromosomes should be selected from the ensemble based on darwin s evolution theory the best chromosomes should survive and produce new offspring there are many approaches to select the best chromosomes for instance roulette wheel selection tournament selection rank selection boltzman selection and some others in the current research the roulette wheel selection as a simple selection method is chosen and briefly described here since parent chromosomes are selected according to their fitness values we need to assign a fitness value for each chromosome therefore those chromosomes that have significant fitness values will be more likely to be selected as the parent chromosomes the value of weights as an appropriate indication of ensemble member quality can be directly used as the fitness value hence the particle weight is defined as the fitness value as shown in eq 14 14 f t i w t i it is necessary for the chromosomes to be arranged in the descending order of their fitness values then a roulette wheel selection should be implemented to select parent chromosomes this selection method assumes that the probability of a selection is proportional to the fitness of a chromosome in this study each chromosome is characterized by its fitness value f t i i 1 2 n thus the selection probability of the ith chromosome is given as 15 p t i f t i f t i this method is analogous to a roulette wheel in which each slice is proportional in size to the fitness value f t i of each chromosome the selection of a chromosome is equivalent to randomly choosing a point on the wheel and locating the corresponding sector in this method a line segment of length i 1 n f t i out of consecutive sectors of length f t i i 1 2 n is constructed and using a random number r 0 r i 1 n f t i the corresponding sector is identified resulting in the respective chromosome lipowski and lipowska 2012 hence the chromosomes with larger weights will have higher chance to generate new offspring chromosomes while the chromosomes with smaller weights will have less chance to do so this inspires to reduce the number of particles with small weights and find more particles with large weights this property of ga not only intensifies the diversity of population to prevent the particle degeneracy but also improves the reliability and accuracy of model estimation by providing a more realistic form of posterior distribution up to this point the proposed methodology allows us to know how to select the parent particles for the crossover operation step 2 shown in fig 2 there are numerous methods in genetic algorithm to do a crossover there are one point and two point crossovers for binary encoding situation and arithmetic and discrete crossover for float encoding situation janikow and michalewicz 1991 magalhaes mendes 2013 generally the application of genetic operators depends on the problem defined in this study the arithmetic crossover which is widely used in the context of evolutionary pf is adopted for the crossover operation park et al 2009 yin and zhu 2015 in the arithmetic crossover a pair of new particles offspring is generated by a linear combination of a pair of parent particles it is noted that the roulette wheel selection is implemented based on a crossover probability ρ c such that it specifies how many particles are to be nominated for the crossover operation step 2 shown in fig 2 the formulation of the arithmetic crossover operator can be expressed as 16 x t 1 i ξ x t 1 i 1 ξ x t 1 j 17 x t 1 j 1 ξ x t 1 i ξ x t 1 j where x t 1 i and x t 1 j are the parent particles the red and yellow particles shown in step 3 of fig 2 x t 1 i and x t 1 j are the pair of new offspring particle the black and blue particles in step 4 of fig 2 and ξ is a uniform random value in the range of 0 1 this parameter plays a pivotal role in transferring information from the parent particle to offspring particle it should be noted that if ξ 1 the crossover will not occur however if ξ 0 more information will be transferred from x t 1 j to x t 1 i x t 1 i to x t 1 j to further promote diversity of the particles a mutation strategy is designed this process alters each chromosome particle through changing its gene details of step 5 in fig 2 it is realized by the eq 18 that x t 1 k and x t 1 k are the particles before and after mutation process respectively the black and purple particles in step 5 and 6 of fig 2 the mutation operation is executed with the appropriate mutation probability ρ m for instance if ρ m 0 1 this means that 10 of the newly generated offspring crossed over particles are randomly selected to do the mutation operation step 5 shown in fig 2 it should be noted that the mutated gene is chosen randomly along the chromosomes see detail of step 5 in fig 2 18 x t 1 k x t 1 k η x t 1 k x t 1 i x t 1 j η n 0 ψ v a r x t 1 k where n 0 ψ v a r x t 1 k represents a random sample from the gaussian distribution with mean zero and variance ψ v a r x t 1 k where v a r x t 1 k is the variance of the prior states at the time t 1 and ψ is a small tuning parameter here we use gaussian mutation process in which ψ should be tuned according to different systems in this study it is preferred to use 0 01 as the value of the tuning parameter therefore the new proposal state x t 1 i p is generated through the aforementioned crossover and mutation process the number of proposal states is the same as the initial ensemble n this is highlighted in fig 2 illustrating eight particles in both the initial ensemble and the refined ensemble in the epfm the proposal states can be viewed as a new generation of offspring the ensuing task is to come up with an approach to accept or reject the proposal particles this study uses the mcmc algorithm as an effective technique to decide which offspring should survive or be eliminated the mcmc move is illustrated in fig 2 and the pdf of the proposed joint state parameters p x t i p θ t i y 1 t is estimated as 19 x t i p f x t 1 i p u t i θ t i 20 p x t i p θ t i y 1 t p y t x t i p θ t i p x t i p θ t i y 1 t 1 p θ t i y 1 t 1 where p y t x t i p θ t i is calculated based on the same likelihood function used in eq 9 to calculate the proposal state pdf p x t i p θ t i y 1 t 1 an assumption is made that the proposal states are assumed to fit marginal gaussian distributions with a mean of μ t and a variance of σ t 2 although a joint distribution would be preferred in this scenario the marginal priors are selected because the states have nonlinear relationships and thus have a joint distribution that is difficult to fit to calculate the proposal pdf based on the gaussian distribution weighted mean and variance of the filtering posterior must be calculated as follows 21 x t i f x t 1 i u t i θ t i 22 μ t w t 1 i x t i 23 σ t 2 w t 1 i x t i μ t 2 the joint pdf of the proposal and prior states are then compared via the metropolis acceptance ratio α to determine the acceptance probability in 24 24 α min 1 p x t i p θ t i y 1 t p x t i θ t i y 1 t min 1 p y 1 t x t i p θ t i p x t i p θ t i y 1 t 1 p y 1 t x t i θ t i p x t i θ t i y 1 t 1 the acceptance and rejection of the new states is shown in fig 2 this acceptance rejection process ensures that in each time step an appropriate prior state distribution is constructed leading to a better estimation of the posterior distribution this concept is the core part of this proposed epfm technique which is formulated using the ga mcmc algorithm 3 streamflow prediction results in this study synthetic and three real data experiments are performed to compare the effectiveness and robustness of the proposed epfm with those of the pf mcmc the sacramento soil moisture accounting model sac sma is used here to simulate the streamflow at four different basins the sac sma model which was first introduced by burnash 1974 is referred to as the spatially lumped continuous soil moisture model and represents each basin vertically by two soil zones an upper zone and a lower zone the upper zone models the short term storage capacity while the lower zone accounts for long term groundwater storage this model is used to generate daily streamflow from daily potential evapotranspiration pet and precipitation data model parameters are tabulated in table 1 and the model state variables are summarized in table 2 in the da setting precipitation and pet are assumed to have lognormal and normal error distributions with a relative error of 25 respectively using these values it is presumed that meteorological observations errors stemming from spatial heterogeneity inherent in these variables and sensor errors can be accounted for dechant and moradkhani 2012 also the streamflow observation errors are assumed to be normally distributed with a 15 relative error for a synthetic study the model is assumed perfect with no structural error while for a real case study the model error is assumed to follow a normal distribution with a relative error of 25 this study assumes that all errors are uncorrelated and applies them with the same magnitude in both the pf mcmc and epfm in order to provide an assessment of the epfm performance both deterministic and probabilistic measures are used in this study including the nash sutcliffe efficiency nse kling gupta efficiency kge mean absolute bias mab reliability r and 95 exceedance ratio er95 since these measures have been described by a large and sufficient body of literature only a brief definition of each is provided here nse has been widely used for calibration and evaluation of hydrological models since it was developed in 1970 this measure determines the relative magnitude of residual variance noise compared to the observed data variance information nse can range from to 1 with nse 1 as perfect fit between observation and simulation a successor of this metric called kge developed by gupta et al 2009 has also been recently used as a robust measure in analyzing the accuracy of hydrological simulations this metric measures the euclidean distance in a three dimensional space between the ideal point 1 1 1 and the pearson product moment correlation coefficient relative variability standard deviation and ratio of the average observation and simulation over the analysis period similar to nse kge varies from to 1 with kge 1 being a perfect fit between the observed and simulated values mean absolute bias mab is the magnitude of the bias for a given estimate the perfect value of this metric is zero meaning that no bias is observed between the simulated and observed data the exceedance ratio at 95 percentile er95 is an indicator by which the spread of the ensemble is evaluated dechant and moradkhani 2012 it is noted that the ideal value for er95 is 5 an er95 greater than 5 indicates the predictive distribution is too narrow while less than 5 indicates the predictive distribution is too wide the r value represents the reliability with zero as the worst and 1 as the best value this value is associated with the predictive quantile quantile plot q q plot which reflects its proximity to uniform distribution dechant and moradkhani 2012 3 1 synthetic case study we first conduct a synthetic case study to demonstrate the usefulness and applicability of the epfm for streamflow prediction with the refined prior distributions it is hypothesized that the epfm could lead to a more accurate and reliable streamflow prediction in this paper leaf river basin located in the southern mississippi was considered for a synthetic study this basin has an area of 743 square miles with the prevailing climate of humid subtropical characterized by mild temperate winters and dry summers and precipitation that is fairly well distributed throughout the year synthetic streamflow observations were generated by adding gaussian noise 15 relative error to streamflow simulations the synthetic streamflow data represent noisy measurements that are sequentially assimilated into the sac sma model the initial state variables and parameter sets were generated by latin hypercube sampling lhs method resulting in a more uniform ensemble spread over all possible parameter permutations and therefore reducing the sampling variance streamflow prediction over four years of analysis using both filters is shown in fig 3 note that this figure is reported based on an ensemble size of 100 the assimilation results are also numerically shown in this figure indicating that the epfm outperforms the pf mcmc according to both deterministic and probabilistic measures it is observed that the epfm outperforms the pf mcmc based on both deterministic and probabilistic measures these results suggest that the proposed epfm produces a more accurate expected value than the pf mcmc according to deterministic measures the probabilistic measures also show the improved performance of epfm in streamflow prediction for example the reliability r increases from 0 62 for the pf mcmc to 0 91 for the epfm indicating a more reliable ensemble prediction by the epfm to analyze the spread of the ensemble prediction the er95 is used the er95 will be 5 for an ideal distribution er95 greater than 5 suggests that the distribution is too narrow meaning that the observations are falling outside the ensemble range at 95 percentile and er95 less than 5 suggests that the distribution is too wide the er95 of pf mcmc is 27 indicating that the pf mcmc remains overconfident due to particle degeneracy however the er95 of epfm at 9 is closer to the optimal value suggesting a more accurate characterization of the uncertainty in order to further investigate the ability of proposed epfm method to estimate the posterior distribution and explore the scalability of epfm i e ability to consistently outperform pf mcmc across ensemble sizes the analysis was further performed over five different ensemble sizes of 50 100 200 500 and 1000 fig 4 depicts the results for both deterministic and probabilistic performance measures based on the five different ensemble sizes for the synthetic experiment over the six subplots in fig 4 similar trends can be observed first it is noted that with increasing the ensemble size the da performances improve for both approaches for example in the epfm the kge increases from 0 83 to 0 95 and the reliability r increases from 0 77 to 0 96 as the ensemble size increase from 50 to 1000 these results are in accordance with previous studies whereas large ensemble size leads to better estimation of the posterior and reduction of weight degeneration dechant and moradkhani 2012 yan et al 2015 secondly compared with the pf mcmc the proposed epfm produces smaller mean absolute bias and greater nse kge r values regardless of the ensemble size these results suggest that the epfm produces a more accurate expected value and a more reliable ensemble prediction than the pf mcmc at all ensemble sizes it is also noted that the er95 of pf mcmc is greater than 5 at all ensemble sizes meaning slightly overconfident prediction fig 4 summarizes the computational demand seconds of both algorithms although the epfm is more accurate than the pf mcmc the computational complexity is larger for the same ensemble size however epfm can provide the same level of accuracy in prediction for much smaller ensemble size e g 100 as compared to the pf mcmc with the ensemble size of 1000 meaning that with less computational demand it can provide comparable performance with the pf mcmc this issue further highlights that by using ga mcmc technique in the importance sampling step of the pf we can effectively improve the assimilation results without a need to increase the ensemble size therefore the developed method is suitable for assimilation of high dimensional problems where large ensemble sizes would have been needed in former versions of particle filtering this promising result is also in accordance with literature suggesting that the improvements of importance sampling in pfs might provide the potential for data assimilation application in large scale systems van leeuwen 2009 in a summary in terms of both deterministic and probabilistic measures the proposed epfm outperforms pf mcmc regardless of the ensemble size this finding comes at no surprise since the epfm is formulated in an intelligent framework where the ga can shuffle and increase diversity of the particles during the filtering process the mcmc move is then used to reject the shuffled particles that move outside the posterior distribution ensuring that shuffled particles contribute to improved performance 3 2 real case study in addition to the synthetic study three real data experiments are also performed in three basins located in different climate and geographical conditions to fully examine the performance of the proposed epfm the daily precipitation potential evapotranspiration and streamflow data for the following three river basins are acquired from the model parameter estimation experiment mopex project duan et al 2006 1 the chehalis river basin with an area of 895 square miles is the second largest basin in washington state this region has a mostly oceanic climate with wet winters autumns and springs and relatively dry summers such that it receives more than 4100 mm precipitation annually making it the wettest area in washington state average temperatures over much of this region in january range from 7 c to 9 c and in july vary from 7 c to 27 c 2 the indian creek watershed is located in the klamath national forest and drains into the klamath river in california state this watershed encompasses the area of 739 square miles and is located predominantly in california with a small northern portion extending into oregon state indian creek watershed with mean annual precipitation of 762 mm is dominated by semi arid climate conditions with summer temperatures of 24 c to 41 c while in the winter it can drop as low as 9 c 3 the carson river watershed originates from the alpine county in california with an area of nearly 3966 square miles 85 of which lies in nevada state the prevalent climate of this basin is arid and hot with sparsely distributed precipitation throughout the year this watershed has an annual precipitation of 127 mm while annual evaporation exceeds 1524 mm more hydro climatic information about the aforementioned three basins is provided in table 3 fig 5 summarizes both deterministic kge nse and mab and probabilistic reliability and er95 measures for five different ensemble sizes for the three basins also in this figure the computational demand of both algorithms is provided expectedly the use of mcmc twice in the epfm method demands more computation however as seen in both deterministic and probabilistic measures the epfm provides more accurate and reliable predictions than pf mcmc for all basins regardless of the ensemble size further support for these results is provided in fig 6 where the streamflow prediction and its uncertainty interval is shown for the chehalis river basin as an example similar to the synthetic study the epfm improves the pf mcmc performance by 7 and 6 in terms of nse and kge respectively for the probabilistic measurement the reliability is improved from 0 67 of pf mcmc to 0 90 of epfm with an increase of about 15 to further interpret the ga mcmc step within the epfm the comparisons of both epfm priors and posteriors seem necessary fig 7 presents the prior and posterior distributions at three daily time steps t 50 431 and 761 for the chehalis river basin in wa two subplots are included for each time step the first subplot a1 b1 and c1 consists of the likelihood function the prior pdf before the ga mcmc and the epfm refined prior pdf after the ga mcmc this subplot also illustrates the prior pdf for the pf ga model the model for which we used only ga algorithm without mcmc in the importance sampling step this analysis was performed to further investigate why the filtering benefits from the combination of mcmc with ga the second subplot a2 b2 and c2 compares the estimated posteriors using the priors with and without the ga mcmc step this subplot also represents the posterior distributions based on the pf ga model only in cases of a and c it is observed that in the pf prior pdfs a large percentage of particles end up far from the observations and they possess negligible weights as a result after the resampling the posteriors provide low skill predictions instead the epfm can provide more informative priors whereas the shuffled particles are drawn in the vicinity of the observations using the ga mcmc step it is noted that many more particles now end up close to the observations by implementing this framework resulting in posteriors with higher accuracy and reliability more specifically the ga mcmc provides the possibility of eliminating most of the small weight particles and replacing them with the new particles with larger weights which are effectively used in generating the posterior distribution this procedure also enhances the diversity of particles when the assimilation is implemented and consequently the degeneracy problem can be mitigated moreover this figure indicates that although using the ga in the pf ga could improve the diversity of particles it does not necessarily guarantee that the shuffled particles are properly representing the prior distribution for instance in fig 7 b1 pf ga prior pdf is even not as good as pf mcmc prior pdf indicating that single ga algorithm could not enhance the prior distribution and consequently resulted in an inaccurate posterior distribution similarly other subplots show no evidence of promising results when pf ga model is implemented the two parameters of crossover and mutation probability need to be tuned when ga is implemented the choice of these parameters depends on the specific problem and this choice may affect the efficiency of ga this issue plays an important role in many applications where ga is combined with the pf the researchers have proposed different formulations to define the aforementioned probabilities in their studies park et al 2007 han et al 2011 however those approaches could not be considered as a universal method to determine the ga parameters since they were designed for a specific application and case study hence it cannot be generalized this issue was also corroborated in the current study where different sets of ga parameters were investigated in implementing the pf ga in this paper by using ga mcmc algorithm within the importance sampling step of the pf mcmc not only can the subjective choice of ga parameters be overcome but also the effectiveness of the pf mcmc method is improved therefore the crossover and mutation probabilities can take any values that is changing these parameters does not exacerbate the results and the epfm always outperforms its counterparts i e pf ga and pf mcmc in summary the motivation of this study was to overcome the particle degeneracy and sample impoverishment problems that the earlier versions of pf have been more susceptible to the focus of the epfm is on increasing the particle diversity which is achieved through mutation and cross over steps in ga however if some particles move far away from the background states the mcmc will capture those and discard from the pool of particles 4 streamflow forecasting results data assimilation plays a very important role in both weather forecasting and hydrologic forecasting by improving model initial conditions in addition to streamflow prediction analysis the applicability and usefulness of the proposed epfm algorithm against pf mcmc algorithm for one day and five day ahead streamflow forecasting was also investigated in the current research for both synthetic and real case studies at each time step we used the posterior distributions updated by the epfm and pf mcmc to generate the streamflow forecasts table 4 summarizes the epfm and pf mcmc model performance statistics for one day and five day ahead forecasting experiments over all case studies used in this study we found that for both lead times the proposed epfm data assimilation method provided more accurate and reliable results than the pf mcmc method this result suggests the superiority of epfm to its ascendant pf mcmc and the potential for operational streamflow forecasting as seen the forecasting skills of both methods decrease from one day to five day lead time this is mainly due to the effect of initial condition on forecasting skill li et al 2009 dechant and moradkhani 2011 yuan et al 2016 since flood events are main concern for emergency managers here we further analyze this issue to this end we compared the epfm and pf mcmc for one day ahead streamflow forecasting skill during the flood season for a synthetic days 200 to 400 and for a real case study days 300 to 600 the real case study considered here is chehalis river basin in wa the majority of precipitation in this basin falls as snow during winter season due to the strong atmospheric river and peak flood often occurs in spring due to snowmelt when temperature rises and snow starts to melt the soil can become nearly saturated over weeks leading to clustered flood events fig 8 from this figure it is seen that the epfm provides better ensemble forecasts than the pf mcmc according to deterministic measures the ensemble mean of epfm nse 0 95 mab 0 26 shows higher skill than the pf mcmc nse 0 91 mab 0 34 and offers a better ability to forecast extreme events e g day 345 according to probabilistic measures the er95 of epfm 2 25 is closer to the ideal value 5 than the pf mcmc 16 41 the higher er95 value of pf mcmc indicates that the ensemble forecast distribution is too narrow and over confident this result can also be observed in fig 8 since the ensemble from pf mcmc fails to forecast peak events e g day 260 and 345 while the 95 predictive interval of the epfm can capture these peak events after the assimilation of a peak flood the epfm leads to better forecast as it can create a better initial condition that is closer to the real condition i e saturated soil moisture according to fig 8 a and b during the flood recession period day 220 to 240 day 350 to 380 the pf mcmc leads to under forecast while the epfm ensemble forecast accurately presents the uncertainty and the ensemble mean closely follows the observation similar results can also be observed in fig 8 c and d for a real case study in summary the epfm outperforms pf mcmc and generates more skillful streamflow forecasts to further support this claim we compared the epfm and pf mcmc for five day ahead streamflow forecasting skill during the flood season for the synthetic days 50 to 350 and the same real case study days 650 to 850 fig 9 similar to one day ahead streamflow forecasting results both deterministic and probabilistic measures demonstrated the superiority of the epfm these results are also consistent with those reported in table 4 5 conclusion a new data assimilation technique the epfm was proposed in this paper to characterize a more accurate and reliable posterior distribution for state variables of interest in data assimilation applications what distinguishes the proposed efpm model from the recently developed pf mcmc model moradkhani et al 2012 is the utilization of ga mcmc technique in the importance sampling step of the pf mcmc model the success and applicability of epfm was evaluated through both deterministic and probabilistic measures the epfm provides a comprehensive approach for both state using ga mcmc within the importance sampling step and parameter updating using mcmc within the resampling step such that the user can select and update either state parameter or both given their needs in research this study suggests using the ga mcmc technique within the particle filtering given the following features 1 it provides the possibility of generating more informative prior leading to better estimation of the posterior distribution in fact the ga mcmc expands the search space by implementing the crossover and mutation steps in the ga and subsequently the search space is refined via the mcmc technique resulting in more desirable prior distribution 2 it significantly minimizes the particle degeneracy and sample impoverishment problems that have been the main concerns in using particle filtering 3 it alleviates the need for large ensemble size which had made it traditionally a limiting factor in using widespread application of particle filters 4 in the standard pf ga algorithm which was recently emerged in the da community the usefulness of the model is more subjective in terms of ga operator i e crossover and mutation selection this problem can be significantly alleviated with the proposed ga mcmc approach embedded in the algorithm 5 using the ga mcmc a small ensemble size suffices to set up the da this is particularly a beneficial feature of this algorithm making it more attractive for large scale systems where increasing the ensemble size and model runs would be a concern 6 this approach enhances the usefulness and effectiveness of the particle filtering as demonstrated through deterministic and probabilistic measures it is noted that a lumped hydrologic model was used in this study for the proof of concept through a comparative analysis with currently the most advanced data assimilation method pf mcmc for hydrologic applications however to address the heterogeneity in forcing and landscape characteristics further examination of the epfm scalability to distributed models is desired that should follow the current study acknowledgment partial financial support for this project was provided by the national science foundation cyber innovation for sustainability science and engineering cybersees grant no ccf 1539605 supplementary materials supplementary material associated with this article can be found in the online version at doi 10 1016 j advwatres 2017 11 011 appendix supplementary materials image application 1 
901,a three dimensional eulerian two phase flow model for sediment transport in sheet flow conditions is presented to resolve turbulence and turbulence sediment interactions the large eddy simulation approach is adopted specifically a dynamic smagorinsky closure is used for the subgrid fluid and sediment stresses while the subgrid contribution to the drag force is included using a drift velocity model with a similar dynamic procedure the contribution of sediment stresses due to intergranular interactions is modeled by the kinetic theory of granular flow at low to intermediate sediment concentration while at high sediment concentration of enduring contact a phenomenological closure for particle pressure and frictional viscosity is used the model is validated with a comprehensive high resolution dataset of unidirectional steady sheet flow revil baudard et al 2015 journal of fluid mechanics 767 1 30 at a particle stokes number of about 10 simulation results indicate a reduced von kármán coefficient of κ 0 215 obtained from the fluid velocity profile a fluid turbulence kinetic energy budget analysis further indicates that the drag induced turbulence dissipation rate is significant in the sheet flow layer while in the dilute transport layer the pressure work plays a similar role as the buoyancy dissipation which is typically used in the single phase stratified flow formulation the present model also reproduces the sheet layer thickness and mobile bed roughness similar to measured data however the resulting mobile bed roughness is more than two times larger than that predicted by the empirical formulae further analysis suggests that through intermittent turbulent motions near the bed the resolved sediment reynolds stress plays a major role in the enhancement of mobile bed roughness our analysis on near bed intermittency also suggests that the turbulent ejection motions are highly correlated with the upward sediment suspension flux while the turbulent sweep events are mostly associated with the downward sediment deposition flux keywords large eddy simulation sediment transport sheet flow two phase flow near bed intermittency 1 introduction understanding the mechanisms driving the mobilization suspension transport and deposition of sediments is fundamental to the prediction of the earth surface evolution sheet flow represents an intense sediment transport mode in which a thick layer of concentrated sediment is mobilized above the quasi static bed however modeling sheet flow remains challenging due to the tightly coupled fluid particle and inter particle interactions covering a full range of particle concentration namely from the volumetric concentration of about 0 6 in the bed near random close packing to the dilute transport of concentration less than 10 4 the mechanisms associated with this nearly five orders of magnitude of concentration are also diverse in moderate to high concentration transport is dominated by inter particle interactions ranging from intermittent collisions to enduring contacts armanini et al 2005 berzi and fraccarollo 2015 in this sediment concentration range rheological closures are required for the contributions from both particle inertia and interstitial fluid viscosity e g jenkins and berzi 2010 boyer et al 2011 when sediment concentration decreases the transport becomes increasingly dominated by turbulent eddies while the turbulent eddies are also affected by the presence of particles a specific challenge is the vast range of cascading turbulent eddy sizes from o 10 1 to o 10 4 m and their interactions with different grain sizes from o 10 3 to o 10 6 m the conventional modeling approach for sediment transport is essentially a single phase model which splits the transport into bedload and suspended load layers due to its simplicity and numerical efficiency the single phase model has been integrated into meso large scale models e g lesser et al 2004 hu et al 2009 due to the dilute assumption in the single phase flow formulation the bedload layer cannot be resolved but must rely on semi empirical parameterizations of transport rate e g meyer peter and muller 1948 ribberink 1998 in addition a semi empirical suspension flux boundary condition has to be applied to the suspended load van rijn 1984a although the single phase based sediment transport models have clearly made progresses in predicting some aspects of sediment transport e g zedler and street 2006 liu and garcia 2008 laboratory measurements of sheet flow with the full profile of sediment transport flux revil baudard et al 2015 and net transport rate o donoghue and wright 2004 clearly indicated that these assumptions are too simple and cannot explain many observed sediment transport dynamics for example important mechanisms such as turbulent entrainment and intermittent burst events cannot be resolved e g revil baudard et al 2015 kiger and pan 2002 in addition the particle velocities are often approximated by the fluid velocity and the particle settling velocity balachandar and eaton 2010 and balachandar 2009 reviewed the applicability of such approximation and revealed that this method is only plausible when the particle stokes number the ratio of particle relaxation time to kolmogorov time scale is small 0 2 for which the particles respond to the turbulent eddies rapidly for typical sand transport in aquatic environments the relevant particle stokes number often exceeds 0 2 thus single phase based model becomes questionable even for fine sand finn and li 2016 for larger particle stokes number more sophisticated methods to model sediment transport have been developed using the euler lagrange approach in euler lagrange models the sediment particles are tracked as point particle e g drake and calantoni 2001 schmeeckle 2014 sun and xiao 2016b finn et al 2016 or with the interstitial fluid resolved fukuoka et al 2014 uhlmann 2008 the position and velocity of each particle are directly tracked using the newton s second law and individual particle collision is directly modeled in the point particle approach the fluid phase is solved as a continuum phase and it is coupled with particles through a series of averaged momentum transfer terms such as drag force buoyancy force lift force and added mass euler lagrange models are shown to be promising in modeling grain size sorting harada et al 2015 and non spherical particle shapes calantoni et al 2004 fukuoka et al 2014 sun et al 2017 schmeeckle 2014 and liu et al 2016 applied large eddy simulation to model bedload transport of coarse sand and identified the role of turbulent ejection sweep on sediment entrainment sun and xiao 2016a further carried out 3d simulation of dune evolution for coarse sand recently finn et al 2016 used a point particle method to study medium sand transport in wave boundary layer where the sediment trapping due to ripple vortexes was successfully captured in the lagrangian description of particle transport a major challenge remains to be the high computational cost as the number of particles increases though the computation technology is advancing rapidly the largest achievable number of particles in the literature was on the order of o 10 million at this moment therefore it is not practical to apply euler lagrange approach to study transport of fine to medium sand alternatively the particle phase can be treated as a continuum and a classical eulerian eulerian two phase flow approach can be used e g jenkins and hanes 1998 dong and zhang 1999 hsu et al 2004 bakhtyar et al 2009 revil baudard and chauchat 2013 cheng et al 2017 by solving the mass and momentum equations of fluid phase and sediment phase with appropriate closures for interphase momentum transfer turbulence and intergranular stresses these two phase flow models are able to resolve the entire profiles of sediment transport without the assumptions of bedload and suspended load hsu et al 2004 incorporated an empirical sediment stress closure in the enduring contact layer and adopted kinetic theory for inter granular stress in the collisional sediment transport regimes the k ϵ equations were modified to account for the turbulence sediment interactions for large particle stokes number later amoudry et al 2008 kranenburg et al 2014 and cheng et al 2017 further improved the turbulence sediment interaction parameterization and extended the turbulence closure to a wider range of particle stokes number recently new particle stress closure were adopted using phenomenological laws for dense granular flow rheology revil baudard and chauchat 2013 and it was demonstrated that granular rheology can produce similar predictions of sediment transport as other models using the kinetic theory for granular flow with the progress made in eulerian two phase modeling of sediment transport several advancements are warranted firstly nearly all these eulerian two phase sediment transport models are developed in the turbulence averaged formulation and the turbulence closures rely on eddy viscosity calculated ranging from a mixing length model to two equation models aside from their empirical treatment on turbulence sediment interaction as reported by several studies e g amoudry et al 2008 kranenburg et al 2014 cheng et al 2017 the model results are sensitive to the coefficients in the turbulence closure it is likely that the existing closures for turbulence sediment interaction in turbulence averaged sediment transport models need to be further improved to better understand the effect of sediments on modulating turbulence and conversely the mixing and transport of sediments by turbulent eddies a turbulence resolving two phase flow modeling approach is necessary for many sediment transport applications that involve sand transport at high reynolds number the stokes number is greater than unity and grain scale process is usually larger than the kolmogorov length scale hence a turbulence resolving approach based on large eddy simulation les methodology can be adopted to solve the eulerian two phase flow formulation balachandar 2009 finn and li 2016 the purpose of this study is to develop a turbulence resolving numerical modeling framework and begin to tackle the challenge of modeling turbulence sediment interactions for the full range of concentration in sediment transport recently an open source multi dimensional eulerian two phase flow model for sediment transport sedfoam cheng et al 2017 is developed using the cfd toolbox openfoam although the numerical model is created for full three dimensions 3d existing sedfoam solver has only been used for two dimensional turbulence averaged sediment transport modeling in this study we extend the sedfoam solver to a 3d large eddy simulation model in which a substantial amount of turbulent motions and turbulence sediment interactions are resolved and the effects of small eddies and sediment dispersion are modeled with subgrid closures model formulations are described in section 2 and model setup and validation for the steady unidirectional sheet flow experiment of revil baudard et al 2015 are presented in section 3 section 4 is devoted to discuss several insights of turbulence sediment interactions in sheet flow revealed by the resolved fields concluding remarks are given in section 5 2 model formulation 2 1 filtered eulerian two phase flow equations in this study we adopt the eulerian two phase flow formulation for a particulate system ding and gidaspow 1990 drew 1983 to model sediment transport cheng 2016 to better resolve turbulence sediment interactions a large eddy simulation les methodology is utilized turbulent motions eddies involve a wide range of length scales in les the large scale motions are directly resolved and the effects of the small scale motions are modeled with subgrid closures to achieve the separation of scales a filter operation is applied to the eulerian two phase flow equations similar to the previous studies using the two phase flow approach for compressible flows e g vreman et al 1995 a favre filtering concept is used i e f ϕ f ϕ f where f denotes the favre filter operation denotes the favre filtered variables and ϕ is the volumetric concentration of quantity f it shall be noted that although the favre filter operation does not commute with the partial differential operators it has been demonstrated that favre filter only makes a negligible difference to the large scale dynamics compared with the direct filtering approaches for high reynolds number flows aluie 2013 here favre filtering procedure is applied to both the fluid phase and the sediment phase considering no mass transfer between the two phases the filtered mass conservation equations for fluid phase and sediment phase can be written as 1 1 ϕ t 1 ϕ u i f x i 0 2 ϕ t ϕ u i s x i 0 where ϕ is the filtered sediment volumetric concentration u i f u i s are the filtered fluid and sediment velocities and i 1 2 3 represents streamwise x spanwise y and vertical z components respectively as a result of favre filtering the filtered continuity equations do not contain any subgrid term the filtered momentum equations for fluid and sediment phases are written as 3 ρ f 1 ϕ u i f t ρ f 1 ϕ u i f u j f x j 1 ϕ f i 1 ϕ p f x i ρ f 1 ϕ τ i j f τ i j f s g s x j ρ f 1 ϕ g i m i f s 4 ρ s ϕ u i s t ρ s ϕ u i s u j s x j ϕ f i ϕ p f x i ρ s ϕ τ i j s s g s x j p s x i τ i j s x j ρ s ϕ g i m i f s where ρf ρs are fluid and sediment densities respectively gi is the gravitational acceleration fi is the uniform external driving force and p f is the fluid pressure the particle pressure p s and particle stress τ i j s due to intergranular interactions are modeled on the basis of the kinetic theory of granular flow and phenomenological closure of contact stresses the particle stress closure is similar to cheng et al 2017 and a brief summary of the particle stress closures is given in the appendix a τ i j f and τ i j f s g s are the fluid molecular viscous stress and subgrid stress associated with the unresolved turbulent motions in analogy to the fluid phase the unresolved particle motions due to turbulence are taken into account by the subgrid stress τ i j s s g s m i f s represents the filtered inter phase momentum transfer between fluid phase and particle phase see section 2 3 the subgrid stress model and subgrid drag model will be discussed next 2 2 subgrid turbulence closures in the momentum eqs 3 and 4 the filtering of nonlinear convection term on the left hand side lhs leads to the subgrid tensor τ i j f s g s and τ i j f s g s respectively they can read as 5 1 ϕ τ i j f s g s f 1 ϕ u i f u j f 1 ϕ u i f u j f 6 ϕ τ i j s s g s f ϕ u i s u j s ϕ u i s u j s where ϕ u i f and u i s are the unfiltered sediment concentration fluid and sediment velocity respectively we further assume that the favre filter operator can be applied to the momentum flux u i f u j f and u i s u j s i e f 1 ϕ u i f u j f 1 ϕ u i f u j f and f ϕ u i s u j s ϕ u i s u j s here we will discuss the modeling of fluid subgrid stress eq 5 using a dynamic procedure in detail the residual fluid momentum flux can be modeled using a functional subgrid stress model germano et al 1991 7 τ i j f s g s u i f u j f u i f u j f 2 ν s g s f s i j f where s i j f is the resolved fluid strain rate tensor written as 8 s i j f 1 2 u i f x j u j f x i 1 3 u k f x k δ i j with δij representing the kronecker delta ν s g s f c s f δ 2 s f is the subgrid eddy viscosity with δ being the filter width which is related to the local grid cell size δ δ x δ y δ z 1 3 c s f is the smagorinsky coefficient and s f is the magnitude of the strain rate tensor s f 2 s i j f s i j f for the present sheet flow simulation the dynamic procedure originally proposed by germano et al 1991 and lilly 1992 is adopted to determine the smagorinsky coefficient c s f the dynamic smagorinsky model involves two levels of filtering and it assumes that the residual stresses at these two levels are similar consequently the smagorinsky coefficient is determined to minimize the differences the first level is the implicit filtering at the grid level and the filter size is the grid size δ by solving the filtered eulerian two phase flow equations this level of filtering is implicitly performed the second filter level is the test filter which is typically twice the grid size δ 2 δ and denotes the test filtering operation this procedure is performed explicitly by applying a box filtering operation which can be simplified to an averaging operation over the cell faces for rectangular cells in finite volume methods the residual stress due to the test filtering on the grid filtered velocities is written as 9 t i j u i f u j f u i f u j f the difference between residual stress at the test filtering level and the test filtering of residual stress at the grid level is often known as the leonard identity 10 l i j t i j τ i j f s g s u i f u j f u i f u j f if we assume a uniform smagorinsky coefficient can be used at both the grid filtering level and the test filtering level we obtain t i j 2 c s f δ 2 s f s i j f and the modeled identity denoted as l i j m can be expressed as 11 thus the smagorinsky coefficient c s f can be determined by minimizing the mean square error between lij and l i j m 12 c s f l i j l i j d l i j d l i j d where and denotes the plane averaging operator over homogeneous directions due to their similarity and consistency in the model the modeling procedure for the sediment subgrid stress see eq 4 follows the same dynamic procedure used for the fluid subgrid stress 2 3 subgrid drag model in the fluid particle system the particles are assumed to share the fluid pressure and the fluid and particle momentum equations are coupled through an inter phase momentum transfer term see eqs 3 and 4 in general the momentum interactions between the fluid phase and the particle phase include the drag force added mass force lift force maxey and riley 1983 and the effect of grain scale turbulence fluctuations on the effective momentum transfer amongst others according to the reynolds averaged two phase flow modeling study of jha and bombardelli 2010 the relative magnitude of the lift and added mass forces with respect to the drag forces were generally less than 5 and 25 for sand particles respectively therefore in a first approximation the lift force and added mass forces are neglected in this study we are aware that in a turbulence resolving approach these two forces may become important however the complexity associated with the additional closure coefficients and sub grid contributions are left for future work the filtered drag force can be written as a resolved part and subgrid part 13 m i f s ϕ β u i r β ϕ u i r i i s g s where u i r u i f u i s is the relative velocity and i i s g s is the subgrid contribution to the drag for the closure of the drag parameter β we follow ding and gidaspow 1990 by combining the model of ergun 1952 for dense sediment concentration ϕ 0 2 and the model of wen and yu 1966 for lower concentration ϕ 0 2 14 β 150 ϕ ν f ρ f 1 ϕ η d 2 1 75 ρ f u i r η d ϕ 0 2 0 75 c d ρ f u i r 1 ϕ 1 65 η d ϕ 0 2 where d is the equivalent grain diameter as proposed in chauchat 2017 a shape factor η is introduced to take account of non spherical particle shape in the drag model where η 1 for spherical particles for nonspherical particles the shape factor η is tuned to match the measured settling velocity in the experiment the drag coefficient cd is expressed as 15 c d 24 1 0 15 r e p 0 687 r e p r e p 1000 0 44 r e p 1000 in which r e p 1 ϕ u i r d e ν f is the particle reynolds number and νf is the fluid molecular viscosity it was demonstrated that the existence of mesoscale structures such as streamers and clusters can have significant effects on the overall particle dynamics o brien and syamlal 1993 these turbulent meso structures have a length scale ranging from 1 to 10 grain diameters as a result these mesostructures may not be resolved by the mesh size used in most studies unless flow around the particles is fully resolved the resolved drag force may be over predicted if the subgrid contribution of the drag force is not fully accounted for ozel et al 2013 as proposed by ozel et al 2013 the subgrid contribution due to the unresolved mesoscale structures can be modeled with a subgrid drift velocity in the drag force 16 i i s g s ϕ β u i r ϕ β u i r ϕ β k i f δ h ϕ u i r where ki is a model constant f δ was originally proposed as a filter dependent function f δ δ 2 δ 2 c f τ p u i r for fluidized bed applications with τ p ρ s β being the particle relaxation time and cf is a model constant however our preliminary numerical investigation for sheet flow indicated that this formulation significantly underestimates the sediment suspension with cf 0 thus we chose c f 0 i e a constant f δ 1 is used in eq 16 the concentration dependent function h ϕ reads as 17 h ϕ tanh ϕ c h 1 ϕ ϕ m 1 ϕ ϕ m 2 1 c h 2 ϕ ϕ m c h 3 ϕ ϕ m 2 where c h 1 0 1 c h 2 1 88 and c h 3 5 16 are suggested ozel et al 2013 and ϕm is the maximum sediment packing limit for the sediments which has been chosen to be 0 6 the significance of the function h ϕ is small when the sediment concentration is small ϕ 0 08 or close to packing limit ϕ 0 5 where turbulence plays a marginal role in the interval with intermediate sediment concentration 0 08 ϕ 0 5 where turbulence sediment interaction is expected to be most intense h ϕ reaches its minimum i e h ϕ 0 24 following the previous studies e g parmentier et al 2012 ozel et al 2013 the subgrid correlation of sediment concentration ϕ drag parameter β and relative u i r is anisotropic thus ki is evaluated separately in each direction the model constant ki is adjusted dynamically in a similar way as the dynamic smagorinsky coefficient c s f by using a test filter and plane averaging see section 2 2 18 k i d i d i d d i d d i d where d i ϕ β u i r ϕ β u i r and in the rest of this paper unless otherwise noted the overhead symbol denoting the favre filtered variables is dropped for convenience 2 4 numerical implementation the numerical implementation of the present eulerian two phase flow sediment transport models is based on the open source finite volume cfd toolbox openfoam weller 2002 specifically a multi dimensional two phase turbulence averaged model called sedfoam cheng et al 2017 is taken as the baseline and new subgrid closures subgrid stress and subgrid drag are implemented to extend its capability to 3d large eddy simulations openfoam uses the finite volume method over a collocated grid arrangement and the gauss s theorem is applied to the convection and diffusion terms to ensure a conservative form of the discretized equations the numerical discretization of the differential operators was implemented up to the second order accuracy in space and time for the temporal derivatives the second order implicit backward scheme is used to minimize numerical diffusion for the convection terms in the momentum equations a second order filteredlinear scheme implemented in openfoam is used while spurious numerical oscillations intrinsic to second order methods is minimized by introducing a small blend of upwind scheme where unphysical numerical oscillations occur for the convection terms in the mass conservation equation and granular temperature equations a bounded version the total variation diminish tvd scheme based on the sweby limiter sweby 1984 is used denoted as limitedlinear scheme in openfoam the new large eddy simulation turbulence closures and subgrid drag models see sections 2 2 and 2 3 are implemented in the openfoam toolbox to facilitate the plane averaging operations in the subgrid closures the cell ids of the same vertical height are stored in the beginning of the numerical simulation other than the subgrid closures the solution procedure is similar to the turbulence averaged version of sedfoam chauchat et al 2017 the narrow banded matrices obtained as a result of the momentum equations discretization e g eq 3 are solved using a direct solver the pressure poisson equation is constructed to ensure the mass conservation of the mixture and it is solved by using a geometric algebraic multi grid solver gamg the interested reader is referred to chauchat et al 2017 for more details on the numerical implementation 3 model validation the high resolution dataset for steady unidirectional sheet flow experiment reported by revil baudard et al 2015 is used here for model validation a fully turbulent flow of flow depth h f 0 0 17 m and a depth averaged velocity u f 0 0 52 m s see table 1 was generated above the sediment bed the sediment particles were irregularly shaped well sorted with a mean particle diameter of d 3 mm and density of ρ s 1192 kg m3 the measured mean settling velocity was w f a l l 5 59 cm s which is smaller than that calculated using the drag law assuming a spherical particle shape to be consistent with the laboratory experiment of revil baudard et al 2015 we adjusted the shape factor η 0 5 to match the measured particle settling velocity see eq 14 although our eventual goal is to apply the model for sand transport at this moment there are several advantages to validate the model using the coarse light particles reported in revil baudard et al 2015 firstly to our knowledge this is the only published sheet flow experiment that reported concurrent measurement of flow velocity sediment concentration and second order turbulence statistics which is essential for a complete model validation according to uhlmann 2008 and balachandar 2009 particles are too massive to respond to a turbulent eddy having a characteristic length scale smaller than the length scale l t p 3 2 ϵ 1 2 calculated by the particle relaxation time tp and turbulent dissipation rate ϵ in a large eddy simulation when the grid size is smaller than l it can be expected that a substantial amount of turbulent energy is resolved and the subgrid contribution to particle transport may become less important but not negligible as we will demonstrate later the peak turbulent dissipation rate in the experiment of revil baudard et al 2015 estimated from the peak turbulent production term in the tke budget is no more than 0 1 m2 s3 we expect this value is similar to other laboratory scale channel flow experiments the particle relaxation time is calculated as t p ρ s β ρ s w f a l l ρ s ρ f g and for the present coarse light particle tp 0 035 s and the resulting l 0 002 m for the computational resource that is available to us we can afford to carry out 3d simulations with grid size smaller than l in order to minimize the uncertainties in the subgrid closure on the other hand it can be easily shown that for fine and medium sand particles the particle relaxation time is at least one order of magnitude smaller and hence l is of sub millimeter scale or smaller in this case subgrid closures play a much more important role in sand transport finn and li 2016 as a first step we carry out large eddy simulations and model validation for coarse light particle reported by revil baudard et al 2015 that allow for resolving turbulent eddies down to the l scale as discussed before one of the most relevant nondimensional parameter in particle laden flow is the stokes number s t t p t η where tη is the kolmogorov time scale with an estimated peak turbulent dissipation rate of 0 1 m2 s3 the kolmogorov time scale is estimated as t η ν ϵ max 1 2 0 0032 s since the particle relaxation time is estimated as t p 0 035 s the particle stokes number for the experiment of revil baudard et al 2015 is about 11 3 1 model domain and discretization the computational domain and coordinate system are shown in fig 1 and the numerical parameters are summarized in table 2 the two phase flow system describes a steady fluid water flowing over a porous sediment bed the initial sediment bed with depth h b0 is located at the bottom of the domain and the flow above the sediment bed flow depth h f0 normal to the gravitational acceleration drives the sediment transport at the top boundary a free slip boundary condition is used for both the fluid velocity and sediment velocity while a zero gradient boundary is used for all the other quantities such as fluid pressure sediment concentration subgrid viscosity and granular temperature see table 3 at the bottom boundary of the domain a no slip boundary is used for the velocities of both phases while a zero gradient boundary is used for the other quantities it is noted that in the present eulerian two phase model the whole transport profiles from the dilute suspension dense transport and static bed are resolved and the bottom boundary of the model domain plays a minor role because it is under a thick layer of sediment bed therefore the fluid velocity particle velocity granular temperature are basically zero when they reach the bottom boundary in the experiment the channel flow is generated with a free surface while the instrumentation may also interfere with the flow close to the free surface see more details in revil baudard et al 2015 fortunately the measured data provided reynolds shear stress profile thus the location of a quasi free shear plane can be extrapolated we obtained that the flow depth location of free shear plane in the present numerical configuration should be h f 0 135 m the domain size is taken as l x 2 π h f l y π h f and bi periodic boundary conditions are applied for the streamwise x and spanwise y directions for a homogeneous turbulent flow this choice is justified if the domain length in the homogeneous directions is large enough to contain the largest turbulent eddies this requirement will be demonstrated later below the flow a layer of sediment bed of thickness h b 0 0 053 m is prescribed right above the bottom boundary considering that the flow depth increases as the sediments are eroded from the bed the initial flow depth h f0 is set to be h f 0 0 122 m slightly smaller than the target flow depth thus the total domain height is l z 0 175 m the domain is discretized into 29 229 056 grid points 512 256 223 in x y z directions with uniform grid size in streamwise and spanwise directions δ x δ y 1 65 mm nonuniform grid is applied in the vertical direction around the initial bed elevation 0 04 z 0 08 m 100 uniform grid points are used corresponding to a grid size of δ z m i n 0 4 mm above z 0 08 m δz follows a geometric sequence with a common ratio of 1 02 resulting in a maximum value of δ z m a x 2 2 mm at the top of the domain below z 0 04 m the bed is rarely mobile thus the grid size is stretched using a larger grid expansion ratio of 1 058 with a maximum value of δ z m a x 2 6 mm at the bottom of the domain a constant time step of d t 2 10 4 s is used for the numerical simulation see table 2 to ensure that the maximum courant number for fluid and sediment phases are less than 0 3 the initial conditions for the sediment concentration and velocity fields are discussed in detail in appendix b and only a brief summary is given here the initial sediment concentration within the domain is prescribed as a smooth hyperbolic tangent function in which the sediment concentration is close to the packing limit ϕ m 0 6 in the bed and gradually drops to zero above the sediment bed following de villiers 2007 streak like perturbations for both fluid and sediment velocities are added to a laminar velocity profile to expedite the growth of turbulence in the experiment the bottom frictional velocity was estimated via extrapolating the measured reynolds shear stress profile to be bed which gives a friction velocity of u 5 cm s to match the measured bottom frictional velocity the mean horizontal pressure gradient force fx is determined from a preliminary numerical simulation with coarse grid and we obtained f x 20 15 pa m in the interpretation of the model results we determine the bed location as the highest position where the sediment velocity is small enough us 1 mm s and the sediment concentration is greater than 98 of the maximum bed concentration under this flow forcing the final mean bed elevation is located at z b 0 042 m which leads to a final flow depth of h f 0 133 m this confirms that the initial condition and model domain is close to the experimental condition 3 2 model verification the statistics of turbulent flow quantities are of significant interest for model verification validation and to gain further insights in sediment transport in the literature of steady sheet flow several averaging techniques were often used particularly the following three averaging operations are used in the rest of the paper and they are define here as a plane average average of physical quantities along the two homogeneous x and y horizontal directions and it is denoted as the plane average operation is already used in the determination of the subgrid coefficients see sections 2 2 and 2 3 b time average average of physical quantities over a span of sample time after the flow reaches the statistical steady state which is denoted as t the time average requires that the span of the averaging time is sufficiently long so that two quantities separated by this time scale are uncorrelated c statistical average perform both plane averaging and time averaging of a flow quantity denoted as overline it is anticipated that the statistically averaged quantities will be close to the ensemble averaged quantities in the statistical steady state before presenting model validations several important aspects of numerical model setup need to be verified to ensure that the large eddy simulation results presented here are appropriate in this study each simulation was run for 90 s of simulation time during the simulation the temporal evolution of plane averaged sediment concentration and flow velocity are monitored we confirmed that a simulation time of 80 s is sufficeint for the flow to reach a statistical steady state hence time averaging of the last 10 s of the simulation was used between t 80 to 90 s in addition the bulk velocity is also monitored as depth averaged velocity through the entire flow depth above the sediment bed the final flow depth at the statistical steady state is h f 0 133 m and the bulk velocity is u f 0 763 m s therefore the largest eddy turnover time can be estimated as t l h f u f 0 175 s this means that the simulation was carried out for more than 500tl moreover we can estimate the streamwise flow travel time scale between two periodic boundaries which is t x l x u f 1 11 s thus the total simulation time is more than 80tx to verify the domain size is sufficiently large to apply biperiodic boundary conditions the spatial correlations of velocity fluctuations are computed using the results obtained at the end of the simulation fig 2 shows a two point autocorrelation analysis in the x and y directions at the vertical elevation z z b d 12 5 where the plane averaged sediment concentration is dilute about 1 percent see fig 4 in section 3 3 the correlation coefficient r u i x j is defined as the autocorrelation of the i component fluid velocity fluctuations u i u f v f w f in xj direction x j x y the velocity fluctuation is calculated as the difference between instantaneous velocity u i f and the statistically averaged velocity u i f namely u f u i f u i f the correlation is normalized by the mean square of velocity fluctuation u i f 2 therefore the correlation coefficient r u i x j is a function of the spatial separation δx or δy between the two points we observe that the correlation coefficient drops from 1 at δ x 0 or δ y 0 to nearly zero when the separation is half of the domain length i e δ x l x 0 5 and δ y l y 0 5 this means that the streamwise and spanwise domain lengths are sufficiently large to contain the largest eddies and the use of periodic boundary condition is justified since the lateral boundaries are sufficiently far one from the other to be considered as uncorrelated to justify the grid resolution the dimensionless turbulence kinetic energy tke spectrum for each velocity component in the streamwise and spanwise directions at the elevation z z b d 12 5 are shown in fig 3 the energy density is made dimensionless using the resolved tke k f u f 2 v f 2 w f 2 2 and the respective domain length fig 3 shows that the present large eddy simulation resolves the expected 5 3 slope both in the streamwise and in the spanwise directions thin solid lines corresponding to the inertial subrange of the kolmogorov 1962 theory the dimensional analysis of perry et al 1987 and nikora 1999 shows that the turbulent energy spectrum follows an inverse power law i e the slope of the energy spectrum is about 1 in the lower wavenumber range in wall bounded turbulent flows this feature is also captured by the present large eddy simulation see the thin dashed curve it is noted that the resolved energy decay in the inertial subrange is not wide compared with typical single phase flow this is because the presence of sediment provides several mechanisms to attenuate turbulence and they play a key role in determining small scale dissipation see section 4 1 nearly three orders of magnitude of the fluid tke cascade is resolved which confirms that the grid resolution is fine enough to resolve most of the tke 3 3 model validation and grid convergence in this section model validation is presented for three grid resolution so that grid convergence can be also evaluated the primary simulation with the highest resolution is denoted as case 0 two comparative cases with coarser grid resolutions in both streamwise and spanwise directions were carried out see table 4 compared to case 0 the horizontal grid lengths δx and δy are increased to 3 3 mm and 6 6 mm for case 1 and case 2 respectively the same initial condition of sediment concentration and velocity fields were specified for all cases and the flows were driven by the same pressure gradient force f x 20 15 pa m to verify that this pressure gradient driving force matches the hydrodynamic condition of the experiment the modeled reynolds shear stress profiles for case 0 2 are compared with the measured data in fig 4a we can see that the three model results are almost identical and they are all in good agreement with the measured data the reynolds stress profile follows a linear profile above z z b 5 d at the statistical steady state the bottom friction balances the horizontal pressure gradient force i e ρ m u 2 f x l z z b where ρ m ρ f 1 ϕ ρ s ϕ is the mixture density we confirm that the bottom frictional velocity is similar to the experimental value u 5 cm s below z z b 5 d the reynolds shear stress diminishes and drops to zero at the bed z z b the decrease of reynolds shear stress is predicted well by the numerical model and this suggests that the present les model captures the interplay between turbulent flow and sediment dynamics a point that will be discussed in depth later see section 4 2 having established that the flow forcings between the laboratory experiment and the numerical model are consistent the model is further validated against the measured data for statistically averaged streamwise velocity sediment concentration and sediment flux the statistically averaged streamwise mixture velocity profile u m 1 ϕ u f ϕ u s is shown in fig 4b the fluid and sediment velocity profiles are very close to the mixture velocity profile and their difference is on the order of cm s consistent with other laboratory observation in dilute flow muste et al 2005 hence they are not shown separately here overall the velocity profiles in case 0 and case 1 are similar and their relative differences are within 5 however a significant under prediction of velocity in case 2 is observed especially in the upper water column z z b d 6 in the near bed region 0 z z b d 6 the nearly linear velocity profile obtained in the experiment is well reproduced by all three cases between the two higher resolution cases the highest resolution run case 0 better captures the overall shape of the velocity profile in case 1 the predicted velocity profile starts to deviate from the measured data above z z b 6 d as we will discuss later in section 4 3 the sediment suspension intermittency plays a vital role in the range of 6 z z b d 15 thus the better resolved fluid and sediment fields in case 0 may contribute the better agreement with measured data we like to also point out that both case 0 and case 1 over predict the velocity above the mid depth z z b d 22 we believe that this discrepancy could be due to the difference in the top boundary condition discussed before as a result the bulk velocity from case 0 is about 0 761 m s 0 756 m s in case 1 which is slightly larger than the measured data of u f 0 71 m s a comparison of the sediment concentration profile is shown in fig 4c generally good agreements are observed for all three cases more detailed examination suggests that a slightly larger suspension of sediment in case 0 is predicted resulting in a deeper erosion into the bed about one grain diameter and an over prediction of the sediment concentration in the range of 5 z z b d 10 however in the dilute transport layer z z b d 10 concentration profile predicted by case 0 agrees much better with the measure data see the sub panel of sediment concentration in semi log scale while cases with lower resolution significantly under predicts sediment concentration while it is expected that the model all cases predicts a log linear concentration profile in dilute region similar to the measured data the slope of the log linear concentration profile is an important parameter as it is associated with sediment diffusivity or schmidt number the under prediction of such slope indicates that the sediment diffusivity is also underpredicted this point will be discussed in more details later fig 4d shows the statistically averaged streamwise sediment flux ϕ u s in case 0 by depth integration of the sediment streamwise flux ϕ u s we obtain the total transport rate as φ 8 6 10 4 m2 s while case 1 case 2 gives a slightly lower value of φ 7 9 10 4 m2 s φ 7 8 10 4 m2 s and they are all close to the measured value φ 8 0 10 4 m2 s it is evident that the peak of sediment flux occurs at intermediate sediment concentration of around 0 3 z z b d 4 rather close to the static bed meanwhile most of the sediment transport occurs within a thick layer above the static bed estimating the major sheet flow layer thickness is important to further parameterize transport rate mobile bed roughness and flow resistance e g yalin 1992 according to previous experimental studies pugh and wilson 1999 sumer et al 1996 wilson 1987 the major sheet flow layer thickness depends on both the grain size and shields parameter θ which can be generalized as 19 δ s d α θ where θ is the shields parameter as defined in section 3 and α is an empirical constant suggested to be 10 wilson 1987 or 11 8 sumer et al 1996 this empirical formula predicts a sheet layer thickness of 4 4d or 5 2d at a shields parameter of θ 0 44 for the present case in sediment transport literatures the location where sediment concentration is 8 is often defined as the top of the major sheet layer dohmen janssen et al 2001 using this definition we obtained a sheet flow layer thickness of δs 6d for all cases which agrees well with the empirical formulae by further partitioning the transport rate using z z b 6 d we obtain that the transport rate occurs within the major sheet layer as 6 0 10 4 m2 s case 0 5 8 10 4 m2 s case 1 and 5 6 10 4 m2 s case 2 which accounts for about 70 case 0 74 case 1 and 72 case 2 of the total transport rate in the remaining of the paper we name the transport layer below resp above z z b 6 d as the major sheet layer resp dilute transport layer case 2 significantly underpredicts flow velocity compared with case 0 and 1 suggesting that its resolution may not be sufficient the comparison of the statistically averaged quantities for case 0 case 1 and case 2 suggests that a good grid convergence is achieved for two higher resolution runs in the following we will focus on the highest resolution results from case 0 furthermore the comparison of the streamwise and wall normal root mean squared r m s velocity fluctuations is shown in fig 5 a overall the model results agree well with the measured data especially for streamwise component in the dilute region z z b 6 d while lower resolution cases under predict by about 30 not shown the model also captures the anisotropy of flow turbulence i e the streamwise turbulent intensity is about a factor of two stronger than the wall normal component however the model over predicts both the streamwise and wall normal velocity fluctuations close to the bed 0 z z b d 6 this overestimation of turbulent intensity may cause the large erosion depth in sediment concentration profile discussed before following the analysis adopted in revil baudard et al 2015 the mixture vertical momentum diffusivity σm above the sediment bed z zb can be estimated as 20 σ m f x l z z ρ m u f z where a balance between the reynolds shear stress and the horizontal pressure gradient force in the statistically steady state is assumed moreover the sediment diffusivity can be evaluated based on the rouse profile rouse 1939 21 σ p w f a l l ϕ ϕ z in reynolds averaged sediment transport models e g van rijn 1984b the sediment diffusivity is parameterized by the momentum diffusivity or turbulent eddy viscosity by introducing the schmidt number s c σ m σ p using eqs 20 and 21 the momentum and the sediment diffusivities can be obtained from the present simulation results and they are shown in fig 5b the turbulent eddy viscosity profile agrees well with the measured data compare solid line and circle symbol however the numerical results slightly under predict the sediment diffusivity in the dilute transport layer z z b d 8 compare dashed line with cross symbol which is consistent with the slight underestimation of suspended sediment see fig 4c the schmidt number profiles are shown in fig 5c consistent with the under prediction of the sediment diffusivity the model predicts the schmidt number of about 0 55 for z z b d 8 which is slightly larger than the measured value of 0 44 for case 1 and case 2 with lower resolution suspended sediment is under predicted more significantly and the resulting schmidt number is about 0 7 and 0 81 respectively not shown here the analysis presented here suggests that some physical mechanisms of the turbulent sediment interactions are not properly accounted for in subgrid closure particularly for coarser resolution in which subgrid closure effect is more pronounced according to previous studies of particle laden flows the added virtual mass force becomes increasingly important compared to the drag force when the specific gravity becomes smaller elghobashi and truesdell 1992 mei et al 1991 through a dimensional analysis li et al 2017 demonstrated that the relative importance of lift force to the drag force increases with the particle size for the present les of lightweight coarse particles s 1 192 d 3 mm strong vertical turbulent motions are resolved and the added mass and lift force may be non negligible it is likely that the near bed sediment ejection sweep events are under predicted due to neglecting added mass and left forces see more discussion in section 4 3 the significance of these forces should be investigated as future work however we like to also point out that both the measured data and the model results give schmidt number values lower than unity in the dilute suspended layer i e ϕ 0 08 which is consistent with van rijn 1984b s parameterization that the flow turbulence is more efficient to mix the sediment than the fluid momentum 4 discussion in particle laden flows dispersion of particles by turbulence and conversely the turbulence modulation by the presence of particles are key mechanisms that need to be fully understood and insights have been revealed by many theoretical experimental and numerical studies e g wang and maxey 1993 balachandar and eaton 2010 in the context of sediment transport turbulence sediment interactions are further complicated by a wide range of sediment concentration and their proximity to the mobile bed in this section we discuss several issues of turbulence sediment interactions with the co existence of intergranular interactions in sheet flow using the les results to motivate our investigation we examine the statistically averaged mixing length profile in fig 6 a the mixing length lm is a characteristic length scale for the momentum diffusion which can be evaluated as 22 l m f x l z z ρ m u f z the model predicts a nearly linear vertical distribution above the bed that can be fitted using the relationship l m κ z z d where κ is the von kármán constant and z d d 16 33 is the intersection of the fitted linear mixing length profile with the vertical axis in revil baudard et al 2016 zd is defined as the zero plane notice that the linear distribution is only valid in the nearly constant reynolds stress region close to the fixed bed while the elevation z z b is small compared with the water depth hf therefore the fitting is carried out in the range 5 z z b d 10 or 19 z d 24 the slope of the mixing length profile is equal to the von kármán constant κ and the best fit gives κ 0 225 for the measured data and κ 0 215 for the present numerical simulation in addition the von kármán constant can be further confirmed by the streamwise velocity profile in semi logrithmic scale fig 6b it is well established that in steady sheet flow the velocity profile in the overlapping layer between outer layer velocity profile scales with flow depth and inner layer velocity profile scales with roughness height follows the logarithmic law e g sumer et al 1996 in which the relevant local length scale is the wall distance 23 u f u 1 κ ln z z d z k s where zks is related to the bed roughness kn by z k s k n 30 the logarithmic law fits very well with the statistically averaged velocity profile from the numerical simulation solid curve in fig 6b in the range of z z d d 2 the slope of the fitted logarithmic velocity can be used to calculate the von kármán constant associated with case 0 and the same values are obtained as from the mixing length profile it is important to point out that both the modeled and measured κ are significantly smaller than the clear fluid value of 0 41 suggesting a significant damping of turbulence due to the presence of sediment is at work moreover the intersection of the fitted logarithmic velocity line with the z axis can be used to estimate the mobile bed roughness sumer et al 1996 for the model results we obtain z k s 0 48 d or k n 14 4 d which is similar to the measured value of z k s 0 33 d or k n 9 9 d as expected both the modeled and measured mobile bed roughness kn values are much larger than the roughness for fixed bed around 2d and close to the major sheet flow layer thickness see eq 19 motivated by the reduced von kármán constant κ and enhanced bed roughness kn obtained in fig 6 turbulence attenuation due to the presence of sediment or the reduction of κ is investigated using the tke budget in section 4 1 then the mobile bed roughness in sheet flow and mechanisms associated with the enhanced bed roughness are introduced section 4 2 followed by a discussion of near bed sediment suspension intermittency in sheet flows section 4 3 4 1 turbulence modulation and tke budget it is well established from laboratory observations of sediment transport that the existence of sediment mainly attenuates flow turbulence e g muste et al 2005 revil baudard et al 2015 evidence of turbulence attenuation by the suspended sediment was observed indirectly via reduced von kármán constant or mixing length or via direct measurement of turbulent fluctuations in sediment transport literatures the most well known cause for turbulence attenuation is attributed to the sediment induced stable density stratification e g winterwerp 2001 however according to the equilibrium approximation to the eulerian two phase flow equations balachandar and eaton 2010 the various turbulence modulation mechanisms can be reduced to particle induced stratification only when the particle stokes number st is much smaller than unity as mentioned before the particle stokes number in experiment of revil baudard et al 2015 is 11 this point will be confirmed again using simulation results therefore the role of sediment induced density stratification is unclear nevertheless as discussed previously our simulation results also show a reduction of von kármán constant due to the presence of sediment in the eulerian two phase flow formulation the fluid and sediment phases are coupled through inter phase momentum transfer terms mainly through the drag force therefore the role of drag forces on fluid turbulence in sheet flow and its relative importance to sediment induced density stratification can be quantified by examining the budget of resolved fluid tke according to the resolved tke spectrum see fig 3 we observe that our les simulation has resolved 2 3 orders of magnitude of the tke suggesting that the subgrid unresolved tke is of minor importance therefore we will limit our discussion on turbulence modulation to resolved fluid tke budget the balance equation for the resolved fluid tke k f u f 2 v f 2 w f 2 2 is derived from the fluid momentum equation which is written as 24 k f t u i f u j f u i f x j i ν f ν s g s f u i f x j u j f x i u i f x j ii ϕ β 1 k i h ϕ ρ f 1 ϕ u i s u i f u i f iii 1 ρ f u i f p f x i iv u j f k f x j v 1 2 u j f u i f u i f x j vi 1 1 ϕ x j 1 ϕ ν f ν s g s f u i f u i f x j u j f x i vii where the term on the lhs is the time derivative of the resolved tke the seven terms on the right hand side rhs of eq 24 are i turbulent production advection and vii viscous subgrid diffusion for convenience the last three terms namely v vi and vii are collectively named as other transport terms the pressure work term is shown individually as it is qualitatively equivalent to the buoyancy term in the stratified flow formulation we like to point out that turbulent dissipation rate ii consists of resolved dissipation rate and subgrid dissipation rate respectively with the high numerical resolution used in case 0 grid size is smaller than the averaged particle diameter the resolved dissipation rate is about twice as large as the subgrid dissipation rate this also implies that the present analysis on the resolved tke budget is meaningful as it covers most of the tke the resolved tke budget for the fluid phase is plotted in fig 7 a firstly we confirm that the turbulent production provided by the numerical simulation is in reasonably good agreement with the measurements compare symbols with solid curve in fig 7a the turbulent production is a positive source term in the fluid tke budget and as expected its magnitude is close to zero at the sediment bed turbulent production increases away from the sediment bed and reaches a peak at about z z b d 4 5 before gradually decreasing upward in the dilute transport layer z z b d 6 turbulent production is mainly balanced by total turbulent dissipation rate cross symbol the total turbulent dissipation rate reaches its peak right above the major sheet layer at about z z b d 6 and its magnitude drops rapidly when approaching the bed on the other hand close to the top of the sheet layer z z b d 6 to 12 pressure work dash dotted line and drag induced dissipation rate dashed line start to increase notably toward the bed inside the major sheet layer 1 z z b d 6 drag induced dissipation rate becomes dominant while pressure work total turbulent dissipation rate and other transport play minor but non negligible roles in balancing the turbulent production very near the bed 0 z z b d 2 turbulent production reduces to zero while the viscous subgrid diffusion and pressure work take over to balance with drag induced dissipation rate although the features of vanishing of turbulent production and increasing importance of transport terms very near the bed are similar to that in a clear fluid boundary layer kim et al 1987 we found that it is the drag induced dissipation rate that balances with the transport terms in the present two phase flow system moreover the pressure work plays a role in attenuating turbulence in most of the transport layer but it becomes positive a source term and balances with drag induced dissipation very close to the bed 0 z z b d 2 in the present two phase flow formulation the pressure work term is a more complete description encompassing the effect of buoyancy often referred in the stratified flow formulation in addition drag induced dissipation is evidently the dominant term in the concentrated region of transport therefore it is worthwhile to compare their relative contributions to the damping of turbulence in sheet flow the damping effect due to stable density stratification on the fluid turbulence can be quantified by the gradient richardson number which is defined as the ratio of turbulence attenuation caused by the density stratification to the turbulence production by using the gradient transport assumption 25 r i g ρ s ρ f 1 g ϕ z u f z 2 in stably stratified shear flows the turbulence damping effect of density stratification becomes significant if the gradient richardson number exceeds the critical value 0 25 winterwerp 2001 in fig 7b the gradient richardson number profile calculated from the simulation result dash dotted curve is compared with that calculated from the measure data cross symbols we obtain generally good agreement between these two profiles although their magnitudes are significantly smaller than the critical value of 0 25 for the sake of comparison we introduce a similar non dimensional parameter ed as the ratio of drag induced dissipation rate to turbulent production 26 e d ϕ β 1 k i h ϕ ρ f 1 ϕ u i s u i f u i f u i f u j f u i f x j likewise we introduce another non dimensional parameter pw to quantify the relative importance of pressure work 27 p w 1 ρ f u i f p f x i u i f u j f u i f x j the profiles of ed and pw are also plotted in fig 7b throughout almost the entire transport region between 2 z z b d 15 the nondimensional pressure work parameter pw is in the range of 0 1 to 0 2 in the dilute layer z z b d 10 nondimensional drag induced dissipation rate ed is much smaller than pw on the other hand in the major sheet layer 1 z z b d 6 ed becomes dominant due to vanishing turbulent production in the near bed region z z b d 2 both pw and ed diverge in this region in summary drag induced dissipation rate plays a dominating role in controlling turbulence modulation for the major transport layer in sheet flow of coarse lightweight particles it is also interesting to point out that throughout almost the entire transport layer the nondimensional pressure work pw is several times larger than the gradient richardson number rig in summary the present two phase flow model suggests that when describing sediment transport with stokes number larger than unity the use of sediment induced density stratification to represent turbulence attenuation might not be relevant 4 2 mobile bed roughness as demonstrated in fig 6b we obtain a mobile bed roughness of k n 14 4 d for the present steady sheet flow which is significantly larger than the value for clear water flow over fixed rough bed about k n 2 d the enhanced roughness for sheet flow may further affect the parameterization for flow resistance and hence the estimation of flow depth and transport capacity e g yalin 1992 here we investigate the mechanisms responsible for enhanced roughness due to the presence of a mobile bed to understand the mechanisms of the enhanced mobile bed roughness the contribution of shear stresses from the sediment phase and fluid phase are investigated in fig 8 a while the sediment concentration profile is plotted in fig 8b to signify the major sheet flow layer and dilute transport layer delimited by the circle symbol corresponding to ϕ 8 it is evident that the total shear stress follows a linear profile dashed line and a distinct pattern of shear stress contributions to the total shear stress can be found within and above the major sheet flow layer in the dilute transport layer z z b d 6 the resolved fluid reynolds shear stress is dominant circle symbol while the contribution of various sediment stresses is negligible except for the resolved sediment reynolds stress square symbol which starts to become notable below z z b d 9 or concentration above ϕ 2 in the major sheet flow layer z z b d 6 the resolved fluid reynolds stress drops rapidly while various sediment shear stresses take over as the resolved fluid reynolds shear stress begins to decrease at z z b d 6 the resolved sediment reynolds stress starts to increase more rapidly followed by an increase of sediment collisional stress dotted line moving further toward the bed the collisional contribution to the shear stress increases sharply due to higher sediment concentration and the peak location of the kinetic collisional shear stress is at about z z b d 1 56 this result is in agreement with capart and fraccarollo 2011 s experiments in which the authors observed a frictional layer thickness between 0 5d and 2d at a shields parameter of around 0 5 it is interesting to note that this location corresponds to sediment concentration of about 30 35 further toward the bed sediment concentration is very large and collisional shear stress must decay while the frictional sediment stress starts to increase sharply towards the stationary bed therefore when considering sediment transport as a mixture by adding fluid phase and sediment phase momentum equations into a mixture momentum equation the total kinetic energy is consumed by both the fluid shear stress and sediment shear stress as a result the mobile sediment particles exert extra kinetic energy dissipation due to various sediment shear stresses which leads to an enhanced roughness in sheet flow compared with a fixed rough bed for sheet flow condition many researchers proposed that the mobile bed roughness does not scale with the grain size instead it scales with the sheet layer thickness pugh and wilson 1999 this observation is consistent with our finding that particle stress is responsible for major kinetic energy dissipation as sediment concentration in the sheet layer is sufficiently high and intergranular interaction is expected to be dominant however as discussed previously the present model predict a sheet layer thickness of δs 6d see eq 19 even though this predicted sheet layer thickness agrees with the measured data and empirical formulations the mobile bed roughness obtained from the present numerical simulation k n 14 4 d remains to be more than a factor of two larger than the sheet layer thickness although there is a general consensus that the mobile bed roughness is of the same order of magnitude as the sheet layer thickness it is likely that more quantitive description also depends on sediment properties and flow unsteadiness for example sumer et al 1996 found that the ratio kn d also depends on the fall parameter which is defined as the dimensionless settling velocity wfall u dohmen janssen et al 2001 reported that for sheet flow under waves the ratio kn d is much larger for fine sand than that for medium and coarse sand importantly we further hypothesize that the significantly enhanced roughness observed here particularly regarding its value to be much larger than the sheet layer thickness may be related to near bed intermittency to be discussed next 4 3 near bed intermittency in typical sediment transport models the transport rate and entrainment are often parameterized by the excess bed shear stress e g meyer peter and muller 1948 van rijn 1984a calculated by the averaged flow velocity without explicitly considering turbulence sediment fluctuations and their interactions recent studies have shown that near bed intermittent turbulent motions are the primary triggering mechanisms of large sediment entrainment liu et al 2016 nelson et al 1995 ninto and garcia 1996 schmeeckle 2014 and they cannot be fully represented by the reynolds averaged models with the present les two phase flow model we study the effect of instantaneous turbulent motions on sediment dynamics a snapshot of the turbulent vortex structures after the flow reaches the statistical steady state are shown in fig 9 where the criteria of the second invariant q is used to identify the turbulent eddies hunt et al 1988 the second invariant q is calculated as q 1 2 ω f 2 s f 2 where ω f is the magnitude of the rotation rate tensor here we choose the critical value of q c 1000 s 2 and plot its isosurface for better visualization only a subdomain of a quarter of the horizontal plane in the vertical range of z 0 04 m to 0 09 m is shown we observe a large amount of small size turbulent structures several larger hairpin vortices can be found however they are not widespread instead significant amount of half horseshoe vortices are observed and this finding is similar to the simulation results of liu et al 2016 along with the turbulent structures sediment concentration field at the horizontal plane located at z z b 6 d is shown in fig 9 due to turbulent sediment interactions the instantaneous sediment concentration field becomes highly inhomogeneous and clusters of sediment can be observed preferential concentration in turbulent flow for inertia particles has been discussed in many studies e g wang and maxey 1993 for intermediate stokes number sediment particles are preferentially accumulated in regions of low vorticity and high strain rate q 0 as calculated in section 4 1 the particle stokes number in this case is about 10 and thus it is expected that the low sediment concentrations coincide with positive q values it is evident that the isosurface of q c 1000 s 2 preferentially accumulates at regions where the sediment concentration is low blue color while it is relatively rare to find the isosurface of q c 1000 s 2 at regions of higher sediment concentrations red color in fig 10 the time series of sediment concentration profile at the center of the domain x l x 2 and y l y 2 is presented as a 2d color coutour plot the general features of the sediment concentration evolution at other horizontal locations in the domain are statistically similar thus only the one at the center of the domain is discussed the elevation of sediment concentration contour for ϕ c 0 08 thin solid black line and the instantaneous bed level dash dotted black line are also indicated the evolution of instantaneous bed level shows a mild change with time while the isoline of ϕ c 0 08 fluctuates with much larger magnitude and at a much higher frequency as discussed in section 3 3 the dilute transport layer ϕ 0 08 contributes only a minor portion of sediment transport due to the small sediment concentration the transport layer between the contour of ϕ 0 08 and instantaneous bed level represents the major transport layer the corresponding time series of the major transport layer thickness h t 8 is shown in fig 11 a although the time average of the major transport layer thickness is 4 82d instantaneously h t 8 can vary from 2 5d to 9d the power spectrum of h t 8 can be analyzed as shown in fig 11b the power density e h t 8 is made dimensionaless by d 2 ts where t s 4 s is time duration used for the spectrum analysis it is interesting to note that peak of the power spectrum corresponds to frequencies f 1 1 0 hz f 2 2 5 hz f 3 3 75 and f 3 5 0 hz these values correspond to a timescale of variation of 1 0 0 4 0 27 and 0 2 s the latter three are on the same order of magnitude as the eddy turnover time tl 0 175 s this indicates that the fluctuation of the major sheet flow layer is closely related to the eddies motions recall that in fig 8b the resolved sediment reynolds shear stress start to become notable at about z z b d 9 which corresponds to a statistically averaged sediment concentration of about 2 the dashed line in fig 11a represents the transport layer thickness h t 2 between ϕ c 0 02 and the instantaneous bed level we observe that the time averaged value of h t 2 is 9d however instantaneously h t 2 can vary from 6d to 15d this variation of thickness is on the order of the mobile bed roughness observed for this case k n 14 d as a result the intermittent fluctuations of the sheet flow layer thickness may contribute to the enhanced roughness in sheet flows to better illustrate the relationship between sediment transport and turbulent motion a quadrant analysis is carried out the fluid velocity fluctuations are classified into four quadrants namely the outward interactions q 1 u f 0 w f 0 the ejections q 2 u f 0 w f 0 the inward interactions q 3 u f 0 w f 0 and the sweeps q 4 u f 0 w f 0 as reported by revil baudard et al 2015 the near bed intermittency of sediment concentration is mainly caused by the turbulent ejection and sweep events in this study the strength of a sweep ejection event is characterized by the non dimensional parameter r u f w f u f w f in fig 10 the contours of r 2 corresponding to ejection and sweep events are plotted as blue solid line and blue dashed line respectively qualitatively ejection events often take place near the peak elevation of the 8 concentration contour suggesting that ejection events are correlated with the occurrence of upward sediment fluxes similarly sweep events are often correlated with the trough of the 8 sediment concentration contour implying that sweep events are associated with downward sediment fluxes to make more quantitative assessment on the relationship between q 2 q 4 ejection sweep events and sediment vertical fluxes the coefficient y r z ϕc is calculated as the normalized cross correlation coefficient between r and fluctuations of the concentration isosurface elevation z ϕc at concentration level ϕc for q 2 and q 4 events respectively the standard deviation of r and z ϕc is used to normalize the cross correlation thus y r z ϕc varies from 1 to 1 if y 0 the two quantities are positively correlated while if y 0 the two quantities are negatively correlated for the isosurface of ϕ c 0 08 see fig 10 we obtain a correlation coefficient y 0 38 for ejection events suggesting that ejection events are often associated with upward sediment fluxes on the other hand the correlation coefficient is y 0 41 for sweep events implying that the downward sediment fluxes are often related to sweep events our correlation analysis is consistent with the visual observation in fig 10 furthermore the correlation coefficient can be computed for different concentration levels ϕc in the range 0 01 0 2 and conditioned by quadrants q 2 and q 4 not shown we confirmed that the cross correlation y is positive resp negative for ejection resp sweep events and its value slightly varies with the concentration ϕc the peak value y 0 42 of correlation coefficient associated with the sweep events at intermediate sediment concentrations of ϕ c 0 12 while for lower concentration ϕ c 0 01 and higher concentrations ϕ c 0 2 the correlation coefficient y becomes slightly smaller y 0 33 on the other hand the correlation coefficient associated with the ejection events is slightly larger for dilute sediment concentration y 0 4 for ϕ c 0 01 and smaller for higher sediment concentration y 0 34 for ϕ c 0 2 5 conclusion a large eddy simulation eulerian two phase flow model is developed for sediment transport and its capability is tested for turbulent sheet flow condition the effects of the unresolved turbulent motion are modeled using a dynamic smagorinsky subgrid closure germano et al 1991 lilly 1992 and the unresolved subgrid drag is modeled using a drift velocity model ozel et al 2013 the two phase flow model is validated with a comprehensive high resolution measurement of a unidirectional steady sheet flow for which profiles of streamwise and vertical flow velocities and sediment concentration are reported revil baudard et al 2015 several insights essential to turbulence sediment interactions and intergranular interactions in sheet flow condition are reported by analyzing the simulation results for statistically averaged streamwise velocity profile a reduction of the von kármán coefficient in the logarithmic layer is obtained similar to the measured data we analyzed the fluid tke budget to understand turbulence modulation due to the presence of sediment for the present problem with a particle stokes number st around 10 we identified that the drag induced damping effect dominated the turbulent modulation in the major sheet flow layer while in the dilute transport layer the pressure work plays a similar role as the stable density stratification in the single phase stratified flow the present numerical simulation also reproduces the major sheet layer thickness and mobile bed roughness similar to measured data however the mobile bed roughness is more than a factor two larger than the major sheet layer thickness to seek for an explanation we first carry out an analysis on the vertical distribution of various shear stresses in the present two phase flow formulation while it is clear that sediment collisional stress and frictional stress dominate the energy dissipation in the major sheet layer the resolved sediment reynolds shear stress is of notable magnitude above the major sheet layer with a mean sediment concentration of a few percent the intermittent motions of sediment vertical fluxes and their relationships to the turbulent sweep ejection events are studied we first demonstrated that intermittent sediment bursts is responsible for suspending notable amount of sediment up to more than 10 grain diameters above the bed and hence contribute to the resolved sediment reynolds stress consequently these near bed intermittent events may play a major role in the enhanced mobile bed roughness simulation results further suggest that the turbulent ejection motions are correlated with upward sediment fluxes while the sweep events are mostly associated with the downward sediment fluxes and this correlation holds for a wide range of sediment concentration ϕ 0 2 although the present les eulerian two pahse model is successfully validated with the steady sheet flow experiment of revil baudard et al 2015 several improvements of this model are warranted numerical experiments on lower grid resolutions with grid size δ greater than the grain size suggest that the velocity profile in the dilute transport layer is sensitive to the numerical resolution however using a high numerical resolution with grid size similar to sediment grain size may not be always attainable especially for finer grains therefore a more comprehensive subgrid closure on turbulence sediment interaction is necessary to further improve the present les two phase flow modeling approach for sediment transport meanwhile a wider range of sediment properties and flow conditions should be investigated to provide a more comprehensive understanding of natural sand transport in addition several assumptions were adopted on the fluid sediment momentum transfers such as the ignorance of added mass lift force and basset forces balachandar and eaton 2010 the relative importance of these forces compared with the drag force and the formulation of associated subgrid models should also be studied especially for various sediment properties finally the present study focuses on simulating particle turbulence interactions and their effects on sheet flow while relatively simple closures on particle stresses are adopted future modeling effort should also be extended for more complete description of particle stress in both intermediate and high particle concentration regimes e g berzi and fraccarollo 2015 acknowledgements this study was supported by national science foundation oce 1635151 oce 1537231 and office of naval research n00014 16 1 2853 j chauchat was supported by the region rhones alpes coopera project and explora pro grant and the french national programme ec2co lefe modsed numerical simulations were carried out on mills at the university of delaware supermic through support from extreme science and engineering discovery environment xsede and hpc resources from genci cines grant 2015 x2016017567 the authors would also like to acknowledge the support from the program on fluid mediated particle transport in geophysical flows at the kavli institute for theoretical physics santa barbara usa the laboratory legi is part of the labex tec 21 investissements d avenir grant agreement n anr 11 labx 0030 and labex osug 2020 investissements davenir anr10 labx56 the authors would like to acknowledge dr guillaume balarac for this suggestions on the large eddy simulation methodologies and thank dr peter traykovski for this useful comments and discussion on the near bed intermittencies appendix a particle stress model to resolve the full dynamics of sediment transport closures of intergranular stress are needed particularly in moderate to high concentration regions for moderate sediment concentration it is assumed that binary collisions dominate intergranular interactions and a closure based on the kinetic theory of granular flow is adopted for high sediment concentration ϕ 0 5 binary collisions eventually become non exist and intergranular interaction is dominated by enduring contact frictional forces among particles in this study the closures of particle pressure and particle stress both consist of a collisional kinetic component and a quasi static component hsu et al 2004 johnson and jackson 1987 a 1 p s p s c p s f a 2 τ i j s τ i j s c τ i j s f the collisional component is first discussed in the kinetic theory particle stress and particle pressure are quantified by granular temperature θ jenkins and savage 1983 and we adopted the transport equation for granular temperature suggested by ding and gidaspow 1990 a 3 3 2 ϕ ρ s θ t ϕ ρ s u j s θ x j p s c δ i j τ i j s c u i s x j x j κ s c θ x j γ s 3 β θ where the terms on the right hand side rhs are the production of granular temperature the flux of granular temperature the energy dissipation rate due to inelastic collision γs and the last term is the dissipation due to the interaction with the carrier fluid phase notice that the granular temperature equation is constructed by further neglecting the subgrid contribution to the granular temperature as we observed that the resolved granular temperature is already small in the dilute transport layer following ding and gidaspow 1990 closure of particle pressure is written as a 4 p s c ρ s ϕ 1 2 1 e ϕ g s 0 θ where e is the coefficient of restitution during collision and we take e 0 8 for sand particles in water the radial distribution function g s0 is introduced to describe the crowdiness of particle which can be calculated as carnahan and starling 1969 a 5 g s 0 2 ϕ 2 1 ϕ 3 the radial distribution function g s0 quantifies the frequency of particle collisions which is a sharp increasing function of sediment concentration ϕ the formula of carnahan and starling 1969 becomes invalid when sediment concentration becomes very large as it under predicts g s0 when the sediment concentration is approaching the close packing limit ϕm berzi and fraccarollo 2015 chialvo et al 2012 however in modeling the dense region in the present model the granular temperature reduces to nearly zero and inter granular interactions are dominated by enduring contact frictional component of the stress therefore the radial distribution function of carnahan and starling 1969 is still adopted for simplicity the particle stress is calculated as a 6 τ i j s c μ s c u i s x j u j s x i λ 2 3 μ s c u k s x k δ i j where the particle shear viscosity μsc is calculated as a function of granular temperature and radial distribution function a 7 μ s c ρ s d θ 4 5 ϕ 2 g s 0 1 e π π g s 0 1 e 3 e 1 ϕ 2 15 3 e π ϕ 6 3 e similarly the bulk viscosity is calculated as a 8 λ 4 3 ϕ 2 ρ s d g s 0 1 e θ π the κsc is the conductivity of granular temperature calculated as a 9 κ s c ρ s d θ 2 ϕ 2 g s 0 1 e π 9 π g s 0 1 e 2 2 e 1 ϕ 2 2 49 33 e 5 π ϕ 2 49 33 e the dissipation rate due to inelastic collision is calculated based on that proposed by ding and gidaspow 1990 a 10 γ s 3 1 e 2 ϕ 2 ρ s g s 0 θ 4 d θ π 1 2 u i s x i when the volumetric concentration of particles becomes close to random loose packing particles are constantly in contact with one another and particulate energy are mainly dissipated by friction between sliding particles tardos 1997 when the sediment concentration exceeds random loose packing concentration ϕf we adopt the simple model of johnson and jackson 1987 for particle pressure a 11 p s f 0 ϕ ϕ f f ϕ ϕ f m ϕ m ϕ n ϕ ϕ f where ϕ f 0 5 ϕ m 0 6 and f 0 05 m 3 and n 5 are empirical coefficients cheng et al 2017 the particle stress due to frictional contact is calculated by the model of srivastava and sundaresan 2003 a 12 τ i j s f 2 μ s f s i j s where μsf is the frictional viscosity and s i j s is the deviatoric part of strain rate tensor of sediment phase a 13 s i j s 1 2 u i s x j u j s x i 1 3 u k s x k δ i j srivastava and sundaresan 2003 combined the frictional normal stress from johnson and jackson 1987 and the frictional viscosity from schaeffer 1987 model and suggested the friction viscosity to be calculated by a 14 μ s f p s f sin θ f s s where θf 35 is the angle of repose see table 1 in sediment transport the quasi static component of particle stress plays a definite role to ensure the existence of an immobile sediment bed and a low mobility layer of enduring contact hsu et al 2004 hence the empirical coefficients presented here are calibrated to ensure that a stable sediment bed can be established below the mobile transport region appendix b numerical initial condition the initial sediment concentration is specified as a smooth vertical profile to avoid initial numerical instability b 1 ϕ z ϕ m 0 1 tanh a z b 0 z 2 where the constants ϕ m 0 0 54 and a 150 are chosen to ensure a relatively smooth transition of sediment concentration from ϕ m0 within the bed to 0 in the upper column it is found that it is practical to relax the system by setting the ϕ m0 to be lower than the maximum packing limit ϕm as the frictional stress diverges at ϕm see appendix a initially the sediment concentration in the bed will increase due to the immersed weight and the frictional stress will increase accordingly eventually the frictional pressure gradient in the bed can well balance the immersed weight of the bed for the initial condition for the velocity fields the initial velocities are set to zero within the bed z h b0 following de villiers 2007 the initial velocity profile above the bed z h b0 is specified to be a sum of laminar velocity profile and streak like perturbations in the streamwise and spanwise velocities b 2 u z u f 3 z r e τ 0 1 2 z r e τ 0 2 u f z 640 cos α y y exp λ z 2 0 5 1 0 2 ξ 1 b 3 v z u f z 400 sin α x x exp λ z 2 1 0 2 ξ 2 b 4 w z 0 where uf is the bulk velocity r e τ 0 u h f 0 ν f 6100 is the reynolds number based on the initial flow depth x y and z are coordinates in wall units x u x ν f y u y ν f and z u z h b 0 ν f ξ 1 and ξ 2 are gaussian random numbers with zero mean value and standard deviation of 1 λ 2 5 10 6 is the decay coefficient for perturbation α x π 5000 and α y π 2500 are the wavenumber for the streak waviness in the streamwise and spanwise directions respectively the streak like perturbations are beneficial for the fast growth of turbulent modes as the sinusoidal streaks induce vortex formation and further instabilities note that these coefficients are different from the values used in de villiers 2007 they are adjusted for the present high reynolds number turbulent flows so that about four wave like streaks are initialized in streamwise and spanwise directions supplementary material supplementary material associated with this article can be found in the online version at 10 1016 j advwatres 2017 11 016 appendix c supplementary materials supplementary data s1 supplementary raw research data this is open data under the cc by license http creativecommons org licenses by 4 0 supplementary data s1 
901,a three dimensional eulerian two phase flow model for sediment transport in sheet flow conditions is presented to resolve turbulence and turbulence sediment interactions the large eddy simulation approach is adopted specifically a dynamic smagorinsky closure is used for the subgrid fluid and sediment stresses while the subgrid contribution to the drag force is included using a drift velocity model with a similar dynamic procedure the contribution of sediment stresses due to intergranular interactions is modeled by the kinetic theory of granular flow at low to intermediate sediment concentration while at high sediment concentration of enduring contact a phenomenological closure for particle pressure and frictional viscosity is used the model is validated with a comprehensive high resolution dataset of unidirectional steady sheet flow revil baudard et al 2015 journal of fluid mechanics 767 1 30 at a particle stokes number of about 10 simulation results indicate a reduced von kármán coefficient of κ 0 215 obtained from the fluid velocity profile a fluid turbulence kinetic energy budget analysis further indicates that the drag induced turbulence dissipation rate is significant in the sheet flow layer while in the dilute transport layer the pressure work plays a similar role as the buoyancy dissipation which is typically used in the single phase stratified flow formulation the present model also reproduces the sheet layer thickness and mobile bed roughness similar to measured data however the resulting mobile bed roughness is more than two times larger than that predicted by the empirical formulae further analysis suggests that through intermittent turbulent motions near the bed the resolved sediment reynolds stress plays a major role in the enhancement of mobile bed roughness our analysis on near bed intermittency also suggests that the turbulent ejection motions are highly correlated with the upward sediment suspension flux while the turbulent sweep events are mostly associated with the downward sediment deposition flux keywords large eddy simulation sediment transport sheet flow two phase flow near bed intermittency 1 introduction understanding the mechanisms driving the mobilization suspension transport and deposition of sediments is fundamental to the prediction of the earth surface evolution sheet flow represents an intense sediment transport mode in which a thick layer of concentrated sediment is mobilized above the quasi static bed however modeling sheet flow remains challenging due to the tightly coupled fluid particle and inter particle interactions covering a full range of particle concentration namely from the volumetric concentration of about 0 6 in the bed near random close packing to the dilute transport of concentration less than 10 4 the mechanisms associated with this nearly five orders of magnitude of concentration are also diverse in moderate to high concentration transport is dominated by inter particle interactions ranging from intermittent collisions to enduring contacts armanini et al 2005 berzi and fraccarollo 2015 in this sediment concentration range rheological closures are required for the contributions from both particle inertia and interstitial fluid viscosity e g jenkins and berzi 2010 boyer et al 2011 when sediment concentration decreases the transport becomes increasingly dominated by turbulent eddies while the turbulent eddies are also affected by the presence of particles a specific challenge is the vast range of cascading turbulent eddy sizes from o 10 1 to o 10 4 m and their interactions with different grain sizes from o 10 3 to o 10 6 m the conventional modeling approach for sediment transport is essentially a single phase model which splits the transport into bedload and suspended load layers due to its simplicity and numerical efficiency the single phase model has been integrated into meso large scale models e g lesser et al 2004 hu et al 2009 due to the dilute assumption in the single phase flow formulation the bedload layer cannot be resolved but must rely on semi empirical parameterizations of transport rate e g meyer peter and muller 1948 ribberink 1998 in addition a semi empirical suspension flux boundary condition has to be applied to the suspended load van rijn 1984a although the single phase based sediment transport models have clearly made progresses in predicting some aspects of sediment transport e g zedler and street 2006 liu and garcia 2008 laboratory measurements of sheet flow with the full profile of sediment transport flux revil baudard et al 2015 and net transport rate o donoghue and wright 2004 clearly indicated that these assumptions are too simple and cannot explain many observed sediment transport dynamics for example important mechanisms such as turbulent entrainment and intermittent burst events cannot be resolved e g revil baudard et al 2015 kiger and pan 2002 in addition the particle velocities are often approximated by the fluid velocity and the particle settling velocity balachandar and eaton 2010 and balachandar 2009 reviewed the applicability of such approximation and revealed that this method is only plausible when the particle stokes number the ratio of particle relaxation time to kolmogorov time scale is small 0 2 for which the particles respond to the turbulent eddies rapidly for typical sand transport in aquatic environments the relevant particle stokes number often exceeds 0 2 thus single phase based model becomes questionable even for fine sand finn and li 2016 for larger particle stokes number more sophisticated methods to model sediment transport have been developed using the euler lagrange approach in euler lagrange models the sediment particles are tracked as point particle e g drake and calantoni 2001 schmeeckle 2014 sun and xiao 2016b finn et al 2016 or with the interstitial fluid resolved fukuoka et al 2014 uhlmann 2008 the position and velocity of each particle are directly tracked using the newton s second law and individual particle collision is directly modeled in the point particle approach the fluid phase is solved as a continuum phase and it is coupled with particles through a series of averaged momentum transfer terms such as drag force buoyancy force lift force and added mass euler lagrange models are shown to be promising in modeling grain size sorting harada et al 2015 and non spherical particle shapes calantoni et al 2004 fukuoka et al 2014 sun et al 2017 schmeeckle 2014 and liu et al 2016 applied large eddy simulation to model bedload transport of coarse sand and identified the role of turbulent ejection sweep on sediment entrainment sun and xiao 2016a further carried out 3d simulation of dune evolution for coarse sand recently finn et al 2016 used a point particle method to study medium sand transport in wave boundary layer where the sediment trapping due to ripple vortexes was successfully captured in the lagrangian description of particle transport a major challenge remains to be the high computational cost as the number of particles increases though the computation technology is advancing rapidly the largest achievable number of particles in the literature was on the order of o 10 million at this moment therefore it is not practical to apply euler lagrange approach to study transport of fine to medium sand alternatively the particle phase can be treated as a continuum and a classical eulerian eulerian two phase flow approach can be used e g jenkins and hanes 1998 dong and zhang 1999 hsu et al 2004 bakhtyar et al 2009 revil baudard and chauchat 2013 cheng et al 2017 by solving the mass and momentum equations of fluid phase and sediment phase with appropriate closures for interphase momentum transfer turbulence and intergranular stresses these two phase flow models are able to resolve the entire profiles of sediment transport without the assumptions of bedload and suspended load hsu et al 2004 incorporated an empirical sediment stress closure in the enduring contact layer and adopted kinetic theory for inter granular stress in the collisional sediment transport regimes the k ϵ equations were modified to account for the turbulence sediment interactions for large particle stokes number later amoudry et al 2008 kranenburg et al 2014 and cheng et al 2017 further improved the turbulence sediment interaction parameterization and extended the turbulence closure to a wider range of particle stokes number recently new particle stress closure were adopted using phenomenological laws for dense granular flow rheology revil baudard and chauchat 2013 and it was demonstrated that granular rheology can produce similar predictions of sediment transport as other models using the kinetic theory for granular flow with the progress made in eulerian two phase modeling of sediment transport several advancements are warranted firstly nearly all these eulerian two phase sediment transport models are developed in the turbulence averaged formulation and the turbulence closures rely on eddy viscosity calculated ranging from a mixing length model to two equation models aside from their empirical treatment on turbulence sediment interaction as reported by several studies e g amoudry et al 2008 kranenburg et al 2014 cheng et al 2017 the model results are sensitive to the coefficients in the turbulence closure it is likely that the existing closures for turbulence sediment interaction in turbulence averaged sediment transport models need to be further improved to better understand the effect of sediments on modulating turbulence and conversely the mixing and transport of sediments by turbulent eddies a turbulence resolving two phase flow modeling approach is necessary for many sediment transport applications that involve sand transport at high reynolds number the stokes number is greater than unity and grain scale process is usually larger than the kolmogorov length scale hence a turbulence resolving approach based on large eddy simulation les methodology can be adopted to solve the eulerian two phase flow formulation balachandar 2009 finn and li 2016 the purpose of this study is to develop a turbulence resolving numerical modeling framework and begin to tackle the challenge of modeling turbulence sediment interactions for the full range of concentration in sediment transport recently an open source multi dimensional eulerian two phase flow model for sediment transport sedfoam cheng et al 2017 is developed using the cfd toolbox openfoam although the numerical model is created for full three dimensions 3d existing sedfoam solver has only been used for two dimensional turbulence averaged sediment transport modeling in this study we extend the sedfoam solver to a 3d large eddy simulation model in which a substantial amount of turbulent motions and turbulence sediment interactions are resolved and the effects of small eddies and sediment dispersion are modeled with subgrid closures model formulations are described in section 2 and model setup and validation for the steady unidirectional sheet flow experiment of revil baudard et al 2015 are presented in section 3 section 4 is devoted to discuss several insights of turbulence sediment interactions in sheet flow revealed by the resolved fields concluding remarks are given in section 5 2 model formulation 2 1 filtered eulerian two phase flow equations in this study we adopt the eulerian two phase flow formulation for a particulate system ding and gidaspow 1990 drew 1983 to model sediment transport cheng 2016 to better resolve turbulence sediment interactions a large eddy simulation les methodology is utilized turbulent motions eddies involve a wide range of length scales in les the large scale motions are directly resolved and the effects of the small scale motions are modeled with subgrid closures to achieve the separation of scales a filter operation is applied to the eulerian two phase flow equations similar to the previous studies using the two phase flow approach for compressible flows e g vreman et al 1995 a favre filtering concept is used i e f ϕ f ϕ f where f denotes the favre filter operation denotes the favre filtered variables and ϕ is the volumetric concentration of quantity f it shall be noted that although the favre filter operation does not commute with the partial differential operators it has been demonstrated that favre filter only makes a negligible difference to the large scale dynamics compared with the direct filtering approaches for high reynolds number flows aluie 2013 here favre filtering procedure is applied to both the fluid phase and the sediment phase considering no mass transfer between the two phases the filtered mass conservation equations for fluid phase and sediment phase can be written as 1 1 ϕ t 1 ϕ u i f x i 0 2 ϕ t ϕ u i s x i 0 where ϕ is the filtered sediment volumetric concentration u i f u i s are the filtered fluid and sediment velocities and i 1 2 3 represents streamwise x spanwise y and vertical z components respectively as a result of favre filtering the filtered continuity equations do not contain any subgrid term the filtered momentum equations for fluid and sediment phases are written as 3 ρ f 1 ϕ u i f t ρ f 1 ϕ u i f u j f x j 1 ϕ f i 1 ϕ p f x i ρ f 1 ϕ τ i j f τ i j f s g s x j ρ f 1 ϕ g i m i f s 4 ρ s ϕ u i s t ρ s ϕ u i s u j s x j ϕ f i ϕ p f x i ρ s ϕ τ i j s s g s x j p s x i τ i j s x j ρ s ϕ g i m i f s where ρf ρs are fluid and sediment densities respectively gi is the gravitational acceleration fi is the uniform external driving force and p f is the fluid pressure the particle pressure p s and particle stress τ i j s due to intergranular interactions are modeled on the basis of the kinetic theory of granular flow and phenomenological closure of contact stresses the particle stress closure is similar to cheng et al 2017 and a brief summary of the particle stress closures is given in the appendix a τ i j f and τ i j f s g s are the fluid molecular viscous stress and subgrid stress associated with the unresolved turbulent motions in analogy to the fluid phase the unresolved particle motions due to turbulence are taken into account by the subgrid stress τ i j s s g s m i f s represents the filtered inter phase momentum transfer between fluid phase and particle phase see section 2 3 the subgrid stress model and subgrid drag model will be discussed next 2 2 subgrid turbulence closures in the momentum eqs 3 and 4 the filtering of nonlinear convection term on the left hand side lhs leads to the subgrid tensor τ i j f s g s and τ i j f s g s respectively they can read as 5 1 ϕ τ i j f s g s f 1 ϕ u i f u j f 1 ϕ u i f u j f 6 ϕ τ i j s s g s f ϕ u i s u j s ϕ u i s u j s where ϕ u i f and u i s are the unfiltered sediment concentration fluid and sediment velocity respectively we further assume that the favre filter operator can be applied to the momentum flux u i f u j f and u i s u j s i e f 1 ϕ u i f u j f 1 ϕ u i f u j f and f ϕ u i s u j s ϕ u i s u j s here we will discuss the modeling of fluid subgrid stress eq 5 using a dynamic procedure in detail the residual fluid momentum flux can be modeled using a functional subgrid stress model germano et al 1991 7 τ i j f s g s u i f u j f u i f u j f 2 ν s g s f s i j f where s i j f is the resolved fluid strain rate tensor written as 8 s i j f 1 2 u i f x j u j f x i 1 3 u k f x k δ i j with δij representing the kronecker delta ν s g s f c s f δ 2 s f is the subgrid eddy viscosity with δ being the filter width which is related to the local grid cell size δ δ x δ y δ z 1 3 c s f is the smagorinsky coefficient and s f is the magnitude of the strain rate tensor s f 2 s i j f s i j f for the present sheet flow simulation the dynamic procedure originally proposed by germano et al 1991 and lilly 1992 is adopted to determine the smagorinsky coefficient c s f the dynamic smagorinsky model involves two levels of filtering and it assumes that the residual stresses at these two levels are similar consequently the smagorinsky coefficient is determined to minimize the differences the first level is the implicit filtering at the grid level and the filter size is the grid size δ by solving the filtered eulerian two phase flow equations this level of filtering is implicitly performed the second filter level is the test filter which is typically twice the grid size δ 2 δ and denotes the test filtering operation this procedure is performed explicitly by applying a box filtering operation which can be simplified to an averaging operation over the cell faces for rectangular cells in finite volume methods the residual stress due to the test filtering on the grid filtered velocities is written as 9 t i j u i f u j f u i f u j f the difference between residual stress at the test filtering level and the test filtering of residual stress at the grid level is often known as the leonard identity 10 l i j t i j τ i j f s g s u i f u j f u i f u j f if we assume a uniform smagorinsky coefficient can be used at both the grid filtering level and the test filtering level we obtain t i j 2 c s f δ 2 s f s i j f and the modeled identity denoted as l i j m can be expressed as 11 thus the smagorinsky coefficient c s f can be determined by minimizing the mean square error between lij and l i j m 12 c s f l i j l i j d l i j d l i j d where and denotes the plane averaging operator over homogeneous directions due to their similarity and consistency in the model the modeling procedure for the sediment subgrid stress see eq 4 follows the same dynamic procedure used for the fluid subgrid stress 2 3 subgrid drag model in the fluid particle system the particles are assumed to share the fluid pressure and the fluid and particle momentum equations are coupled through an inter phase momentum transfer term see eqs 3 and 4 in general the momentum interactions between the fluid phase and the particle phase include the drag force added mass force lift force maxey and riley 1983 and the effect of grain scale turbulence fluctuations on the effective momentum transfer amongst others according to the reynolds averaged two phase flow modeling study of jha and bombardelli 2010 the relative magnitude of the lift and added mass forces with respect to the drag forces were generally less than 5 and 25 for sand particles respectively therefore in a first approximation the lift force and added mass forces are neglected in this study we are aware that in a turbulence resolving approach these two forces may become important however the complexity associated with the additional closure coefficients and sub grid contributions are left for future work the filtered drag force can be written as a resolved part and subgrid part 13 m i f s ϕ β u i r β ϕ u i r i i s g s where u i r u i f u i s is the relative velocity and i i s g s is the subgrid contribution to the drag for the closure of the drag parameter β we follow ding and gidaspow 1990 by combining the model of ergun 1952 for dense sediment concentration ϕ 0 2 and the model of wen and yu 1966 for lower concentration ϕ 0 2 14 β 150 ϕ ν f ρ f 1 ϕ η d 2 1 75 ρ f u i r η d ϕ 0 2 0 75 c d ρ f u i r 1 ϕ 1 65 η d ϕ 0 2 where d is the equivalent grain diameter as proposed in chauchat 2017 a shape factor η is introduced to take account of non spherical particle shape in the drag model where η 1 for spherical particles for nonspherical particles the shape factor η is tuned to match the measured settling velocity in the experiment the drag coefficient cd is expressed as 15 c d 24 1 0 15 r e p 0 687 r e p r e p 1000 0 44 r e p 1000 in which r e p 1 ϕ u i r d e ν f is the particle reynolds number and νf is the fluid molecular viscosity it was demonstrated that the existence of mesoscale structures such as streamers and clusters can have significant effects on the overall particle dynamics o brien and syamlal 1993 these turbulent meso structures have a length scale ranging from 1 to 10 grain diameters as a result these mesostructures may not be resolved by the mesh size used in most studies unless flow around the particles is fully resolved the resolved drag force may be over predicted if the subgrid contribution of the drag force is not fully accounted for ozel et al 2013 as proposed by ozel et al 2013 the subgrid contribution due to the unresolved mesoscale structures can be modeled with a subgrid drift velocity in the drag force 16 i i s g s ϕ β u i r ϕ β u i r ϕ β k i f δ h ϕ u i r where ki is a model constant f δ was originally proposed as a filter dependent function f δ δ 2 δ 2 c f τ p u i r for fluidized bed applications with τ p ρ s β being the particle relaxation time and cf is a model constant however our preliminary numerical investigation for sheet flow indicated that this formulation significantly underestimates the sediment suspension with cf 0 thus we chose c f 0 i e a constant f δ 1 is used in eq 16 the concentration dependent function h ϕ reads as 17 h ϕ tanh ϕ c h 1 ϕ ϕ m 1 ϕ ϕ m 2 1 c h 2 ϕ ϕ m c h 3 ϕ ϕ m 2 where c h 1 0 1 c h 2 1 88 and c h 3 5 16 are suggested ozel et al 2013 and ϕm is the maximum sediment packing limit for the sediments which has been chosen to be 0 6 the significance of the function h ϕ is small when the sediment concentration is small ϕ 0 08 or close to packing limit ϕ 0 5 where turbulence plays a marginal role in the interval with intermediate sediment concentration 0 08 ϕ 0 5 where turbulence sediment interaction is expected to be most intense h ϕ reaches its minimum i e h ϕ 0 24 following the previous studies e g parmentier et al 2012 ozel et al 2013 the subgrid correlation of sediment concentration ϕ drag parameter β and relative u i r is anisotropic thus ki is evaluated separately in each direction the model constant ki is adjusted dynamically in a similar way as the dynamic smagorinsky coefficient c s f by using a test filter and plane averaging see section 2 2 18 k i d i d i d d i d d i d where d i ϕ β u i r ϕ β u i r and in the rest of this paper unless otherwise noted the overhead symbol denoting the favre filtered variables is dropped for convenience 2 4 numerical implementation the numerical implementation of the present eulerian two phase flow sediment transport models is based on the open source finite volume cfd toolbox openfoam weller 2002 specifically a multi dimensional two phase turbulence averaged model called sedfoam cheng et al 2017 is taken as the baseline and new subgrid closures subgrid stress and subgrid drag are implemented to extend its capability to 3d large eddy simulations openfoam uses the finite volume method over a collocated grid arrangement and the gauss s theorem is applied to the convection and diffusion terms to ensure a conservative form of the discretized equations the numerical discretization of the differential operators was implemented up to the second order accuracy in space and time for the temporal derivatives the second order implicit backward scheme is used to minimize numerical diffusion for the convection terms in the momentum equations a second order filteredlinear scheme implemented in openfoam is used while spurious numerical oscillations intrinsic to second order methods is minimized by introducing a small blend of upwind scheme where unphysical numerical oscillations occur for the convection terms in the mass conservation equation and granular temperature equations a bounded version the total variation diminish tvd scheme based on the sweby limiter sweby 1984 is used denoted as limitedlinear scheme in openfoam the new large eddy simulation turbulence closures and subgrid drag models see sections 2 2 and 2 3 are implemented in the openfoam toolbox to facilitate the plane averaging operations in the subgrid closures the cell ids of the same vertical height are stored in the beginning of the numerical simulation other than the subgrid closures the solution procedure is similar to the turbulence averaged version of sedfoam chauchat et al 2017 the narrow banded matrices obtained as a result of the momentum equations discretization e g eq 3 are solved using a direct solver the pressure poisson equation is constructed to ensure the mass conservation of the mixture and it is solved by using a geometric algebraic multi grid solver gamg the interested reader is referred to chauchat et al 2017 for more details on the numerical implementation 3 model validation the high resolution dataset for steady unidirectional sheet flow experiment reported by revil baudard et al 2015 is used here for model validation a fully turbulent flow of flow depth h f 0 0 17 m and a depth averaged velocity u f 0 0 52 m s see table 1 was generated above the sediment bed the sediment particles were irregularly shaped well sorted with a mean particle diameter of d 3 mm and density of ρ s 1192 kg m3 the measured mean settling velocity was w f a l l 5 59 cm s which is smaller than that calculated using the drag law assuming a spherical particle shape to be consistent with the laboratory experiment of revil baudard et al 2015 we adjusted the shape factor η 0 5 to match the measured particle settling velocity see eq 14 although our eventual goal is to apply the model for sand transport at this moment there are several advantages to validate the model using the coarse light particles reported in revil baudard et al 2015 firstly to our knowledge this is the only published sheet flow experiment that reported concurrent measurement of flow velocity sediment concentration and second order turbulence statistics which is essential for a complete model validation according to uhlmann 2008 and balachandar 2009 particles are too massive to respond to a turbulent eddy having a characteristic length scale smaller than the length scale l t p 3 2 ϵ 1 2 calculated by the particle relaxation time tp and turbulent dissipation rate ϵ in a large eddy simulation when the grid size is smaller than l it can be expected that a substantial amount of turbulent energy is resolved and the subgrid contribution to particle transport may become less important but not negligible as we will demonstrate later the peak turbulent dissipation rate in the experiment of revil baudard et al 2015 estimated from the peak turbulent production term in the tke budget is no more than 0 1 m2 s3 we expect this value is similar to other laboratory scale channel flow experiments the particle relaxation time is calculated as t p ρ s β ρ s w f a l l ρ s ρ f g and for the present coarse light particle tp 0 035 s and the resulting l 0 002 m for the computational resource that is available to us we can afford to carry out 3d simulations with grid size smaller than l in order to minimize the uncertainties in the subgrid closure on the other hand it can be easily shown that for fine and medium sand particles the particle relaxation time is at least one order of magnitude smaller and hence l is of sub millimeter scale or smaller in this case subgrid closures play a much more important role in sand transport finn and li 2016 as a first step we carry out large eddy simulations and model validation for coarse light particle reported by revil baudard et al 2015 that allow for resolving turbulent eddies down to the l scale as discussed before one of the most relevant nondimensional parameter in particle laden flow is the stokes number s t t p t η where tη is the kolmogorov time scale with an estimated peak turbulent dissipation rate of 0 1 m2 s3 the kolmogorov time scale is estimated as t η ν ϵ max 1 2 0 0032 s since the particle relaxation time is estimated as t p 0 035 s the particle stokes number for the experiment of revil baudard et al 2015 is about 11 3 1 model domain and discretization the computational domain and coordinate system are shown in fig 1 and the numerical parameters are summarized in table 2 the two phase flow system describes a steady fluid water flowing over a porous sediment bed the initial sediment bed with depth h b0 is located at the bottom of the domain and the flow above the sediment bed flow depth h f0 normal to the gravitational acceleration drives the sediment transport at the top boundary a free slip boundary condition is used for both the fluid velocity and sediment velocity while a zero gradient boundary is used for all the other quantities such as fluid pressure sediment concentration subgrid viscosity and granular temperature see table 3 at the bottom boundary of the domain a no slip boundary is used for the velocities of both phases while a zero gradient boundary is used for the other quantities it is noted that in the present eulerian two phase model the whole transport profiles from the dilute suspension dense transport and static bed are resolved and the bottom boundary of the model domain plays a minor role because it is under a thick layer of sediment bed therefore the fluid velocity particle velocity granular temperature are basically zero when they reach the bottom boundary in the experiment the channel flow is generated with a free surface while the instrumentation may also interfere with the flow close to the free surface see more details in revil baudard et al 2015 fortunately the measured data provided reynolds shear stress profile thus the location of a quasi free shear plane can be extrapolated we obtained that the flow depth location of free shear plane in the present numerical configuration should be h f 0 135 m the domain size is taken as l x 2 π h f l y π h f and bi periodic boundary conditions are applied for the streamwise x and spanwise y directions for a homogeneous turbulent flow this choice is justified if the domain length in the homogeneous directions is large enough to contain the largest turbulent eddies this requirement will be demonstrated later below the flow a layer of sediment bed of thickness h b 0 0 053 m is prescribed right above the bottom boundary considering that the flow depth increases as the sediments are eroded from the bed the initial flow depth h f0 is set to be h f 0 0 122 m slightly smaller than the target flow depth thus the total domain height is l z 0 175 m the domain is discretized into 29 229 056 grid points 512 256 223 in x y z directions with uniform grid size in streamwise and spanwise directions δ x δ y 1 65 mm nonuniform grid is applied in the vertical direction around the initial bed elevation 0 04 z 0 08 m 100 uniform grid points are used corresponding to a grid size of δ z m i n 0 4 mm above z 0 08 m δz follows a geometric sequence with a common ratio of 1 02 resulting in a maximum value of δ z m a x 2 2 mm at the top of the domain below z 0 04 m the bed is rarely mobile thus the grid size is stretched using a larger grid expansion ratio of 1 058 with a maximum value of δ z m a x 2 6 mm at the bottom of the domain a constant time step of d t 2 10 4 s is used for the numerical simulation see table 2 to ensure that the maximum courant number for fluid and sediment phases are less than 0 3 the initial conditions for the sediment concentration and velocity fields are discussed in detail in appendix b and only a brief summary is given here the initial sediment concentration within the domain is prescribed as a smooth hyperbolic tangent function in which the sediment concentration is close to the packing limit ϕ m 0 6 in the bed and gradually drops to zero above the sediment bed following de villiers 2007 streak like perturbations for both fluid and sediment velocities are added to a laminar velocity profile to expedite the growth of turbulence in the experiment the bottom frictional velocity was estimated via extrapolating the measured reynolds shear stress profile to be bed which gives a friction velocity of u 5 cm s to match the measured bottom frictional velocity the mean horizontal pressure gradient force fx is determined from a preliminary numerical simulation with coarse grid and we obtained f x 20 15 pa m in the interpretation of the model results we determine the bed location as the highest position where the sediment velocity is small enough us 1 mm s and the sediment concentration is greater than 98 of the maximum bed concentration under this flow forcing the final mean bed elevation is located at z b 0 042 m which leads to a final flow depth of h f 0 133 m this confirms that the initial condition and model domain is close to the experimental condition 3 2 model verification the statistics of turbulent flow quantities are of significant interest for model verification validation and to gain further insights in sediment transport in the literature of steady sheet flow several averaging techniques were often used particularly the following three averaging operations are used in the rest of the paper and they are define here as a plane average average of physical quantities along the two homogeneous x and y horizontal directions and it is denoted as the plane average operation is already used in the determination of the subgrid coefficients see sections 2 2 and 2 3 b time average average of physical quantities over a span of sample time after the flow reaches the statistical steady state which is denoted as t the time average requires that the span of the averaging time is sufficiently long so that two quantities separated by this time scale are uncorrelated c statistical average perform both plane averaging and time averaging of a flow quantity denoted as overline it is anticipated that the statistically averaged quantities will be close to the ensemble averaged quantities in the statistical steady state before presenting model validations several important aspects of numerical model setup need to be verified to ensure that the large eddy simulation results presented here are appropriate in this study each simulation was run for 90 s of simulation time during the simulation the temporal evolution of plane averaged sediment concentration and flow velocity are monitored we confirmed that a simulation time of 80 s is sufficeint for the flow to reach a statistical steady state hence time averaging of the last 10 s of the simulation was used between t 80 to 90 s in addition the bulk velocity is also monitored as depth averaged velocity through the entire flow depth above the sediment bed the final flow depth at the statistical steady state is h f 0 133 m and the bulk velocity is u f 0 763 m s therefore the largest eddy turnover time can be estimated as t l h f u f 0 175 s this means that the simulation was carried out for more than 500tl moreover we can estimate the streamwise flow travel time scale between two periodic boundaries which is t x l x u f 1 11 s thus the total simulation time is more than 80tx to verify the domain size is sufficiently large to apply biperiodic boundary conditions the spatial correlations of velocity fluctuations are computed using the results obtained at the end of the simulation fig 2 shows a two point autocorrelation analysis in the x and y directions at the vertical elevation z z b d 12 5 where the plane averaged sediment concentration is dilute about 1 percent see fig 4 in section 3 3 the correlation coefficient r u i x j is defined as the autocorrelation of the i component fluid velocity fluctuations u i u f v f w f in xj direction x j x y the velocity fluctuation is calculated as the difference between instantaneous velocity u i f and the statistically averaged velocity u i f namely u f u i f u i f the correlation is normalized by the mean square of velocity fluctuation u i f 2 therefore the correlation coefficient r u i x j is a function of the spatial separation δx or δy between the two points we observe that the correlation coefficient drops from 1 at δ x 0 or δ y 0 to nearly zero when the separation is half of the domain length i e δ x l x 0 5 and δ y l y 0 5 this means that the streamwise and spanwise domain lengths are sufficiently large to contain the largest eddies and the use of periodic boundary condition is justified since the lateral boundaries are sufficiently far one from the other to be considered as uncorrelated to justify the grid resolution the dimensionless turbulence kinetic energy tke spectrum for each velocity component in the streamwise and spanwise directions at the elevation z z b d 12 5 are shown in fig 3 the energy density is made dimensionless using the resolved tke k f u f 2 v f 2 w f 2 2 and the respective domain length fig 3 shows that the present large eddy simulation resolves the expected 5 3 slope both in the streamwise and in the spanwise directions thin solid lines corresponding to the inertial subrange of the kolmogorov 1962 theory the dimensional analysis of perry et al 1987 and nikora 1999 shows that the turbulent energy spectrum follows an inverse power law i e the slope of the energy spectrum is about 1 in the lower wavenumber range in wall bounded turbulent flows this feature is also captured by the present large eddy simulation see the thin dashed curve it is noted that the resolved energy decay in the inertial subrange is not wide compared with typical single phase flow this is because the presence of sediment provides several mechanisms to attenuate turbulence and they play a key role in determining small scale dissipation see section 4 1 nearly three orders of magnitude of the fluid tke cascade is resolved which confirms that the grid resolution is fine enough to resolve most of the tke 3 3 model validation and grid convergence in this section model validation is presented for three grid resolution so that grid convergence can be also evaluated the primary simulation with the highest resolution is denoted as case 0 two comparative cases with coarser grid resolutions in both streamwise and spanwise directions were carried out see table 4 compared to case 0 the horizontal grid lengths δx and δy are increased to 3 3 mm and 6 6 mm for case 1 and case 2 respectively the same initial condition of sediment concentration and velocity fields were specified for all cases and the flows were driven by the same pressure gradient force f x 20 15 pa m to verify that this pressure gradient driving force matches the hydrodynamic condition of the experiment the modeled reynolds shear stress profiles for case 0 2 are compared with the measured data in fig 4a we can see that the three model results are almost identical and they are all in good agreement with the measured data the reynolds stress profile follows a linear profile above z z b 5 d at the statistical steady state the bottom friction balances the horizontal pressure gradient force i e ρ m u 2 f x l z z b where ρ m ρ f 1 ϕ ρ s ϕ is the mixture density we confirm that the bottom frictional velocity is similar to the experimental value u 5 cm s below z z b 5 d the reynolds shear stress diminishes and drops to zero at the bed z z b the decrease of reynolds shear stress is predicted well by the numerical model and this suggests that the present les model captures the interplay between turbulent flow and sediment dynamics a point that will be discussed in depth later see section 4 2 having established that the flow forcings between the laboratory experiment and the numerical model are consistent the model is further validated against the measured data for statistically averaged streamwise velocity sediment concentration and sediment flux the statistically averaged streamwise mixture velocity profile u m 1 ϕ u f ϕ u s is shown in fig 4b the fluid and sediment velocity profiles are very close to the mixture velocity profile and their difference is on the order of cm s consistent with other laboratory observation in dilute flow muste et al 2005 hence they are not shown separately here overall the velocity profiles in case 0 and case 1 are similar and their relative differences are within 5 however a significant under prediction of velocity in case 2 is observed especially in the upper water column z z b d 6 in the near bed region 0 z z b d 6 the nearly linear velocity profile obtained in the experiment is well reproduced by all three cases between the two higher resolution cases the highest resolution run case 0 better captures the overall shape of the velocity profile in case 1 the predicted velocity profile starts to deviate from the measured data above z z b 6 d as we will discuss later in section 4 3 the sediment suspension intermittency plays a vital role in the range of 6 z z b d 15 thus the better resolved fluid and sediment fields in case 0 may contribute the better agreement with measured data we like to also point out that both case 0 and case 1 over predict the velocity above the mid depth z z b d 22 we believe that this discrepancy could be due to the difference in the top boundary condition discussed before as a result the bulk velocity from case 0 is about 0 761 m s 0 756 m s in case 1 which is slightly larger than the measured data of u f 0 71 m s a comparison of the sediment concentration profile is shown in fig 4c generally good agreements are observed for all three cases more detailed examination suggests that a slightly larger suspension of sediment in case 0 is predicted resulting in a deeper erosion into the bed about one grain diameter and an over prediction of the sediment concentration in the range of 5 z z b d 10 however in the dilute transport layer z z b d 10 concentration profile predicted by case 0 agrees much better with the measure data see the sub panel of sediment concentration in semi log scale while cases with lower resolution significantly under predicts sediment concentration while it is expected that the model all cases predicts a log linear concentration profile in dilute region similar to the measured data the slope of the log linear concentration profile is an important parameter as it is associated with sediment diffusivity or schmidt number the under prediction of such slope indicates that the sediment diffusivity is also underpredicted this point will be discussed in more details later fig 4d shows the statistically averaged streamwise sediment flux ϕ u s in case 0 by depth integration of the sediment streamwise flux ϕ u s we obtain the total transport rate as φ 8 6 10 4 m2 s while case 1 case 2 gives a slightly lower value of φ 7 9 10 4 m2 s φ 7 8 10 4 m2 s and they are all close to the measured value φ 8 0 10 4 m2 s it is evident that the peak of sediment flux occurs at intermediate sediment concentration of around 0 3 z z b d 4 rather close to the static bed meanwhile most of the sediment transport occurs within a thick layer above the static bed estimating the major sheet flow layer thickness is important to further parameterize transport rate mobile bed roughness and flow resistance e g yalin 1992 according to previous experimental studies pugh and wilson 1999 sumer et al 1996 wilson 1987 the major sheet flow layer thickness depends on both the grain size and shields parameter θ which can be generalized as 19 δ s d α θ where θ is the shields parameter as defined in section 3 and α is an empirical constant suggested to be 10 wilson 1987 or 11 8 sumer et al 1996 this empirical formula predicts a sheet layer thickness of 4 4d or 5 2d at a shields parameter of θ 0 44 for the present case in sediment transport literatures the location where sediment concentration is 8 is often defined as the top of the major sheet layer dohmen janssen et al 2001 using this definition we obtained a sheet flow layer thickness of δs 6d for all cases which agrees well with the empirical formulae by further partitioning the transport rate using z z b 6 d we obtain that the transport rate occurs within the major sheet layer as 6 0 10 4 m2 s case 0 5 8 10 4 m2 s case 1 and 5 6 10 4 m2 s case 2 which accounts for about 70 case 0 74 case 1 and 72 case 2 of the total transport rate in the remaining of the paper we name the transport layer below resp above z z b 6 d as the major sheet layer resp dilute transport layer case 2 significantly underpredicts flow velocity compared with case 0 and 1 suggesting that its resolution may not be sufficient the comparison of the statistically averaged quantities for case 0 case 1 and case 2 suggests that a good grid convergence is achieved for two higher resolution runs in the following we will focus on the highest resolution results from case 0 furthermore the comparison of the streamwise and wall normal root mean squared r m s velocity fluctuations is shown in fig 5 a overall the model results agree well with the measured data especially for streamwise component in the dilute region z z b 6 d while lower resolution cases under predict by about 30 not shown the model also captures the anisotropy of flow turbulence i e the streamwise turbulent intensity is about a factor of two stronger than the wall normal component however the model over predicts both the streamwise and wall normal velocity fluctuations close to the bed 0 z z b d 6 this overestimation of turbulent intensity may cause the large erosion depth in sediment concentration profile discussed before following the analysis adopted in revil baudard et al 2015 the mixture vertical momentum diffusivity σm above the sediment bed z zb can be estimated as 20 σ m f x l z z ρ m u f z where a balance between the reynolds shear stress and the horizontal pressure gradient force in the statistically steady state is assumed moreover the sediment diffusivity can be evaluated based on the rouse profile rouse 1939 21 σ p w f a l l ϕ ϕ z in reynolds averaged sediment transport models e g van rijn 1984b the sediment diffusivity is parameterized by the momentum diffusivity or turbulent eddy viscosity by introducing the schmidt number s c σ m σ p using eqs 20 and 21 the momentum and the sediment diffusivities can be obtained from the present simulation results and they are shown in fig 5b the turbulent eddy viscosity profile agrees well with the measured data compare solid line and circle symbol however the numerical results slightly under predict the sediment diffusivity in the dilute transport layer z z b d 8 compare dashed line with cross symbol which is consistent with the slight underestimation of suspended sediment see fig 4c the schmidt number profiles are shown in fig 5c consistent with the under prediction of the sediment diffusivity the model predicts the schmidt number of about 0 55 for z z b d 8 which is slightly larger than the measured value of 0 44 for case 1 and case 2 with lower resolution suspended sediment is under predicted more significantly and the resulting schmidt number is about 0 7 and 0 81 respectively not shown here the analysis presented here suggests that some physical mechanisms of the turbulent sediment interactions are not properly accounted for in subgrid closure particularly for coarser resolution in which subgrid closure effect is more pronounced according to previous studies of particle laden flows the added virtual mass force becomes increasingly important compared to the drag force when the specific gravity becomes smaller elghobashi and truesdell 1992 mei et al 1991 through a dimensional analysis li et al 2017 demonstrated that the relative importance of lift force to the drag force increases with the particle size for the present les of lightweight coarse particles s 1 192 d 3 mm strong vertical turbulent motions are resolved and the added mass and lift force may be non negligible it is likely that the near bed sediment ejection sweep events are under predicted due to neglecting added mass and left forces see more discussion in section 4 3 the significance of these forces should be investigated as future work however we like to also point out that both the measured data and the model results give schmidt number values lower than unity in the dilute suspended layer i e ϕ 0 08 which is consistent with van rijn 1984b s parameterization that the flow turbulence is more efficient to mix the sediment than the fluid momentum 4 discussion in particle laden flows dispersion of particles by turbulence and conversely the turbulence modulation by the presence of particles are key mechanisms that need to be fully understood and insights have been revealed by many theoretical experimental and numerical studies e g wang and maxey 1993 balachandar and eaton 2010 in the context of sediment transport turbulence sediment interactions are further complicated by a wide range of sediment concentration and their proximity to the mobile bed in this section we discuss several issues of turbulence sediment interactions with the co existence of intergranular interactions in sheet flow using the les results to motivate our investigation we examine the statistically averaged mixing length profile in fig 6 a the mixing length lm is a characteristic length scale for the momentum diffusion which can be evaluated as 22 l m f x l z z ρ m u f z the model predicts a nearly linear vertical distribution above the bed that can be fitted using the relationship l m κ z z d where κ is the von kármán constant and z d d 16 33 is the intersection of the fitted linear mixing length profile with the vertical axis in revil baudard et al 2016 zd is defined as the zero plane notice that the linear distribution is only valid in the nearly constant reynolds stress region close to the fixed bed while the elevation z z b is small compared with the water depth hf therefore the fitting is carried out in the range 5 z z b d 10 or 19 z d 24 the slope of the mixing length profile is equal to the von kármán constant κ and the best fit gives κ 0 225 for the measured data and κ 0 215 for the present numerical simulation in addition the von kármán constant can be further confirmed by the streamwise velocity profile in semi logrithmic scale fig 6b it is well established that in steady sheet flow the velocity profile in the overlapping layer between outer layer velocity profile scales with flow depth and inner layer velocity profile scales with roughness height follows the logarithmic law e g sumer et al 1996 in which the relevant local length scale is the wall distance 23 u f u 1 κ ln z z d z k s where zks is related to the bed roughness kn by z k s k n 30 the logarithmic law fits very well with the statistically averaged velocity profile from the numerical simulation solid curve in fig 6b in the range of z z d d 2 the slope of the fitted logarithmic velocity can be used to calculate the von kármán constant associated with case 0 and the same values are obtained as from the mixing length profile it is important to point out that both the modeled and measured κ are significantly smaller than the clear fluid value of 0 41 suggesting a significant damping of turbulence due to the presence of sediment is at work moreover the intersection of the fitted logarithmic velocity line with the z axis can be used to estimate the mobile bed roughness sumer et al 1996 for the model results we obtain z k s 0 48 d or k n 14 4 d which is similar to the measured value of z k s 0 33 d or k n 9 9 d as expected both the modeled and measured mobile bed roughness kn values are much larger than the roughness for fixed bed around 2d and close to the major sheet flow layer thickness see eq 19 motivated by the reduced von kármán constant κ and enhanced bed roughness kn obtained in fig 6 turbulence attenuation due to the presence of sediment or the reduction of κ is investigated using the tke budget in section 4 1 then the mobile bed roughness in sheet flow and mechanisms associated with the enhanced bed roughness are introduced section 4 2 followed by a discussion of near bed sediment suspension intermittency in sheet flows section 4 3 4 1 turbulence modulation and tke budget it is well established from laboratory observations of sediment transport that the existence of sediment mainly attenuates flow turbulence e g muste et al 2005 revil baudard et al 2015 evidence of turbulence attenuation by the suspended sediment was observed indirectly via reduced von kármán constant or mixing length or via direct measurement of turbulent fluctuations in sediment transport literatures the most well known cause for turbulence attenuation is attributed to the sediment induced stable density stratification e g winterwerp 2001 however according to the equilibrium approximation to the eulerian two phase flow equations balachandar and eaton 2010 the various turbulence modulation mechanisms can be reduced to particle induced stratification only when the particle stokes number st is much smaller than unity as mentioned before the particle stokes number in experiment of revil baudard et al 2015 is 11 this point will be confirmed again using simulation results therefore the role of sediment induced density stratification is unclear nevertheless as discussed previously our simulation results also show a reduction of von kármán constant due to the presence of sediment in the eulerian two phase flow formulation the fluid and sediment phases are coupled through inter phase momentum transfer terms mainly through the drag force therefore the role of drag forces on fluid turbulence in sheet flow and its relative importance to sediment induced density stratification can be quantified by examining the budget of resolved fluid tke according to the resolved tke spectrum see fig 3 we observe that our les simulation has resolved 2 3 orders of magnitude of the tke suggesting that the subgrid unresolved tke is of minor importance therefore we will limit our discussion on turbulence modulation to resolved fluid tke budget the balance equation for the resolved fluid tke k f u f 2 v f 2 w f 2 2 is derived from the fluid momentum equation which is written as 24 k f t u i f u j f u i f x j i ν f ν s g s f u i f x j u j f x i u i f x j ii ϕ β 1 k i h ϕ ρ f 1 ϕ u i s u i f u i f iii 1 ρ f u i f p f x i iv u j f k f x j v 1 2 u j f u i f u i f x j vi 1 1 ϕ x j 1 ϕ ν f ν s g s f u i f u i f x j u j f x i vii where the term on the lhs is the time derivative of the resolved tke the seven terms on the right hand side rhs of eq 24 are i turbulent production advection and vii viscous subgrid diffusion for convenience the last three terms namely v vi and vii are collectively named as other transport terms the pressure work term is shown individually as it is qualitatively equivalent to the buoyancy term in the stratified flow formulation we like to point out that turbulent dissipation rate ii consists of resolved dissipation rate and subgrid dissipation rate respectively with the high numerical resolution used in case 0 grid size is smaller than the averaged particle diameter the resolved dissipation rate is about twice as large as the subgrid dissipation rate this also implies that the present analysis on the resolved tke budget is meaningful as it covers most of the tke the resolved tke budget for the fluid phase is plotted in fig 7 a firstly we confirm that the turbulent production provided by the numerical simulation is in reasonably good agreement with the measurements compare symbols with solid curve in fig 7a the turbulent production is a positive source term in the fluid tke budget and as expected its magnitude is close to zero at the sediment bed turbulent production increases away from the sediment bed and reaches a peak at about z z b d 4 5 before gradually decreasing upward in the dilute transport layer z z b d 6 turbulent production is mainly balanced by total turbulent dissipation rate cross symbol the total turbulent dissipation rate reaches its peak right above the major sheet layer at about z z b d 6 and its magnitude drops rapidly when approaching the bed on the other hand close to the top of the sheet layer z z b d 6 to 12 pressure work dash dotted line and drag induced dissipation rate dashed line start to increase notably toward the bed inside the major sheet layer 1 z z b d 6 drag induced dissipation rate becomes dominant while pressure work total turbulent dissipation rate and other transport play minor but non negligible roles in balancing the turbulent production very near the bed 0 z z b d 2 turbulent production reduces to zero while the viscous subgrid diffusion and pressure work take over to balance with drag induced dissipation rate although the features of vanishing of turbulent production and increasing importance of transport terms very near the bed are similar to that in a clear fluid boundary layer kim et al 1987 we found that it is the drag induced dissipation rate that balances with the transport terms in the present two phase flow system moreover the pressure work plays a role in attenuating turbulence in most of the transport layer but it becomes positive a source term and balances with drag induced dissipation very close to the bed 0 z z b d 2 in the present two phase flow formulation the pressure work term is a more complete description encompassing the effect of buoyancy often referred in the stratified flow formulation in addition drag induced dissipation is evidently the dominant term in the concentrated region of transport therefore it is worthwhile to compare their relative contributions to the damping of turbulence in sheet flow the damping effect due to stable density stratification on the fluid turbulence can be quantified by the gradient richardson number which is defined as the ratio of turbulence attenuation caused by the density stratification to the turbulence production by using the gradient transport assumption 25 r i g ρ s ρ f 1 g ϕ z u f z 2 in stably stratified shear flows the turbulence damping effect of density stratification becomes significant if the gradient richardson number exceeds the critical value 0 25 winterwerp 2001 in fig 7b the gradient richardson number profile calculated from the simulation result dash dotted curve is compared with that calculated from the measure data cross symbols we obtain generally good agreement between these two profiles although their magnitudes are significantly smaller than the critical value of 0 25 for the sake of comparison we introduce a similar non dimensional parameter ed as the ratio of drag induced dissipation rate to turbulent production 26 e d ϕ β 1 k i h ϕ ρ f 1 ϕ u i s u i f u i f u i f u j f u i f x j likewise we introduce another non dimensional parameter pw to quantify the relative importance of pressure work 27 p w 1 ρ f u i f p f x i u i f u j f u i f x j the profiles of ed and pw are also plotted in fig 7b throughout almost the entire transport region between 2 z z b d 15 the nondimensional pressure work parameter pw is in the range of 0 1 to 0 2 in the dilute layer z z b d 10 nondimensional drag induced dissipation rate ed is much smaller than pw on the other hand in the major sheet layer 1 z z b d 6 ed becomes dominant due to vanishing turbulent production in the near bed region z z b d 2 both pw and ed diverge in this region in summary drag induced dissipation rate plays a dominating role in controlling turbulence modulation for the major transport layer in sheet flow of coarse lightweight particles it is also interesting to point out that throughout almost the entire transport layer the nondimensional pressure work pw is several times larger than the gradient richardson number rig in summary the present two phase flow model suggests that when describing sediment transport with stokes number larger than unity the use of sediment induced density stratification to represent turbulence attenuation might not be relevant 4 2 mobile bed roughness as demonstrated in fig 6b we obtain a mobile bed roughness of k n 14 4 d for the present steady sheet flow which is significantly larger than the value for clear water flow over fixed rough bed about k n 2 d the enhanced roughness for sheet flow may further affect the parameterization for flow resistance and hence the estimation of flow depth and transport capacity e g yalin 1992 here we investigate the mechanisms responsible for enhanced roughness due to the presence of a mobile bed to understand the mechanisms of the enhanced mobile bed roughness the contribution of shear stresses from the sediment phase and fluid phase are investigated in fig 8 a while the sediment concentration profile is plotted in fig 8b to signify the major sheet flow layer and dilute transport layer delimited by the circle symbol corresponding to ϕ 8 it is evident that the total shear stress follows a linear profile dashed line and a distinct pattern of shear stress contributions to the total shear stress can be found within and above the major sheet flow layer in the dilute transport layer z z b d 6 the resolved fluid reynolds shear stress is dominant circle symbol while the contribution of various sediment stresses is negligible except for the resolved sediment reynolds stress square symbol which starts to become notable below z z b d 9 or concentration above ϕ 2 in the major sheet flow layer z z b d 6 the resolved fluid reynolds stress drops rapidly while various sediment shear stresses take over as the resolved fluid reynolds shear stress begins to decrease at z z b d 6 the resolved sediment reynolds stress starts to increase more rapidly followed by an increase of sediment collisional stress dotted line moving further toward the bed the collisional contribution to the shear stress increases sharply due to higher sediment concentration and the peak location of the kinetic collisional shear stress is at about z z b d 1 56 this result is in agreement with capart and fraccarollo 2011 s experiments in which the authors observed a frictional layer thickness between 0 5d and 2d at a shields parameter of around 0 5 it is interesting to note that this location corresponds to sediment concentration of about 30 35 further toward the bed sediment concentration is very large and collisional shear stress must decay while the frictional sediment stress starts to increase sharply towards the stationary bed therefore when considering sediment transport as a mixture by adding fluid phase and sediment phase momentum equations into a mixture momentum equation the total kinetic energy is consumed by both the fluid shear stress and sediment shear stress as a result the mobile sediment particles exert extra kinetic energy dissipation due to various sediment shear stresses which leads to an enhanced roughness in sheet flow compared with a fixed rough bed for sheet flow condition many researchers proposed that the mobile bed roughness does not scale with the grain size instead it scales with the sheet layer thickness pugh and wilson 1999 this observation is consistent with our finding that particle stress is responsible for major kinetic energy dissipation as sediment concentration in the sheet layer is sufficiently high and intergranular interaction is expected to be dominant however as discussed previously the present model predict a sheet layer thickness of δs 6d see eq 19 even though this predicted sheet layer thickness agrees with the measured data and empirical formulations the mobile bed roughness obtained from the present numerical simulation k n 14 4 d remains to be more than a factor of two larger than the sheet layer thickness although there is a general consensus that the mobile bed roughness is of the same order of magnitude as the sheet layer thickness it is likely that more quantitive description also depends on sediment properties and flow unsteadiness for example sumer et al 1996 found that the ratio kn d also depends on the fall parameter which is defined as the dimensionless settling velocity wfall u dohmen janssen et al 2001 reported that for sheet flow under waves the ratio kn d is much larger for fine sand than that for medium and coarse sand importantly we further hypothesize that the significantly enhanced roughness observed here particularly regarding its value to be much larger than the sheet layer thickness may be related to near bed intermittency to be discussed next 4 3 near bed intermittency in typical sediment transport models the transport rate and entrainment are often parameterized by the excess bed shear stress e g meyer peter and muller 1948 van rijn 1984a calculated by the averaged flow velocity without explicitly considering turbulence sediment fluctuations and their interactions recent studies have shown that near bed intermittent turbulent motions are the primary triggering mechanisms of large sediment entrainment liu et al 2016 nelson et al 1995 ninto and garcia 1996 schmeeckle 2014 and they cannot be fully represented by the reynolds averaged models with the present les two phase flow model we study the effect of instantaneous turbulent motions on sediment dynamics a snapshot of the turbulent vortex structures after the flow reaches the statistical steady state are shown in fig 9 where the criteria of the second invariant q is used to identify the turbulent eddies hunt et al 1988 the second invariant q is calculated as q 1 2 ω f 2 s f 2 where ω f is the magnitude of the rotation rate tensor here we choose the critical value of q c 1000 s 2 and plot its isosurface for better visualization only a subdomain of a quarter of the horizontal plane in the vertical range of z 0 04 m to 0 09 m is shown we observe a large amount of small size turbulent structures several larger hairpin vortices can be found however they are not widespread instead significant amount of half horseshoe vortices are observed and this finding is similar to the simulation results of liu et al 2016 along with the turbulent structures sediment concentration field at the horizontal plane located at z z b 6 d is shown in fig 9 due to turbulent sediment interactions the instantaneous sediment concentration field becomes highly inhomogeneous and clusters of sediment can be observed preferential concentration in turbulent flow for inertia particles has been discussed in many studies e g wang and maxey 1993 for intermediate stokes number sediment particles are preferentially accumulated in regions of low vorticity and high strain rate q 0 as calculated in section 4 1 the particle stokes number in this case is about 10 and thus it is expected that the low sediment concentrations coincide with positive q values it is evident that the isosurface of q c 1000 s 2 preferentially accumulates at regions where the sediment concentration is low blue color while it is relatively rare to find the isosurface of q c 1000 s 2 at regions of higher sediment concentrations red color in fig 10 the time series of sediment concentration profile at the center of the domain x l x 2 and y l y 2 is presented as a 2d color coutour plot the general features of the sediment concentration evolution at other horizontal locations in the domain are statistically similar thus only the one at the center of the domain is discussed the elevation of sediment concentration contour for ϕ c 0 08 thin solid black line and the instantaneous bed level dash dotted black line are also indicated the evolution of instantaneous bed level shows a mild change with time while the isoline of ϕ c 0 08 fluctuates with much larger magnitude and at a much higher frequency as discussed in section 3 3 the dilute transport layer ϕ 0 08 contributes only a minor portion of sediment transport due to the small sediment concentration the transport layer between the contour of ϕ 0 08 and instantaneous bed level represents the major transport layer the corresponding time series of the major transport layer thickness h t 8 is shown in fig 11 a although the time average of the major transport layer thickness is 4 82d instantaneously h t 8 can vary from 2 5d to 9d the power spectrum of h t 8 can be analyzed as shown in fig 11b the power density e h t 8 is made dimensionaless by d 2 ts where t s 4 s is time duration used for the spectrum analysis it is interesting to note that peak of the power spectrum corresponds to frequencies f 1 1 0 hz f 2 2 5 hz f 3 3 75 and f 3 5 0 hz these values correspond to a timescale of variation of 1 0 0 4 0 27 and 0 2 s the latter three are on the same order of magnitude as the eddy turnover time tl 0 175 s this indicates that the fluctuation of the major sheet flow layer is closely related to the eddies motions recall that in fig 8b the resolved sediment reynolds shear stress start to become notable at about z z b d 9 which corresponds to a statistically averaged sediment concentration of about 2 the dashed line in fig 11a represents the transport layer thickness h t 2 between ϕ c 0 02 and the instantaneous bed level we observe that the time averaged value of h t 2 is 9d however instantaneously h t 2 can vary from 6d to 15d this variation of thickness is on the order of the mobile bed roughness observed for this case k n 14 d as a result the intermittent fluctuations of the sheet flow layer thickness may contribute to the enhanced roughness in sheet flows to better illustrate the relationship between sediment transport and turbulent motion a quadrant analysis is carried out the fluid velocity fluctuations are classified into four quadrants namely the outward interactions q 1 u f 0 w f 0 the ejections q 2 u f 0 w f 0 the inward interactions q 3 u f 0 w f 0 and the sweeps q 4 u f 0 w f 0 as reported by revil baudard et al 2015 the near bed intermittency of sediment concentration is mainly caused by the turbulent ejection and sweep events in this study the strength of a sweep ejection event is characterized by the non dimensional parameter r u f w f u f w f in fig 10 the contours of r 2 corresponding to ejection and sweep events are plotted as blue solid line and blue dashed line respectively qualitatively ejection events often take place near the peak elevation of the 8 concentration contour suggesting that ejection events are correlated with the occurrence of upward sediment fluxes similarly sweep events are often correlated with the trough of the 8 sediment concentration contour implying that sweep events are associated with downward sediment fluxes to make more quantitative assessment on the relationship between q 2 q 4 ejection sweep events and sediment vertical fluxes the coefficient y r z ϕc is calculated as the normalized cross correlation coefficient between r and fluctuations of the concentration isosurface elevation z ϕc at concentration level ϕc for q 2 and q 4 events respectively the standard deviation of r and z ϕc is used to normalize the cross correlation thus y r z ϕc varies from 1 to 1 if y 0 the two quantities are positively correlated while if y 0 the two quantities are negatively correlated for the isosurface of ϕ c 0 08 see fig 10 we obtain a correlation coefficient y 0 38 for ejection events suggesting that ejection events are often associated with upward sediment fluxes on the other hand the correlation coefficient is y 0 41 for sweep events implying that the downward sediment fluxes are often related to sweep events our correlation analysis is consistent with the visual observation in fig 10 furthermore the correlation coefficient can be computed for different concentration levels ϕc in the range 0 01 0 2 and conditioned by quadrants q 2 and q 4 not shown we confirmed that the cross correlation y is positive resp negative for ejection resp sweep events and its value slightly varies with the concentration ϕc the peak value y 0 42 of correlation coefficient associated with the sweep events at intermediate sediment concentrations of ϕ c 0 12 while for lower concentration ϕ c 0 01 and higher concentrations ϕ c 0 2 the correlation coefficient y becomes slightly smaller y 0 33 on the other hand the correlation coefficient associated with the ejection events is slightly larger for dilute sediment concentration y 0 4 for ϕ c 0 01 and smaller for higher sediment concentration y 0 34 for ϕ c 0 2 5 conclusion a large eddy simulation eulerian two phase flow model is developed for sediment transport and its capability is tested for turbulent sheet flow condition the effects of the unresolved turbulent motion are modeled using a dynamic smagorinsky subgrid closure germano et al 1991 lilly 1992 and the unresolved subgrid drag is modeled using a drift velocity model ozel et al 2013 the two phase flow model is validated with a comprehensive high resolution measurement of a unidirectional steady sheet flow for which profiles of streamwise and vertical flow velocities and sediment concentration are reported revil baudard et al 2015 several insights essential to turbulence sediment interactions and intergranular interactions in sheet flow condition are reported by analyzing the simulation results for statistically averaged streamwise velocity profile a reduction of the von kármán coefficient in the logarithmic layer is obtained similar to the measured data we analyzed the fluid tke budget to understand turbulence modulation due to the presence of sediment for the present problem with a particle stokes number st around 10 we identified that the drag induced damping effect dominated the turbulent modulation in the major sheet flow layer while in the dilute transport layer the pressure work plays a similar role as the stable density stratification in the single phase stratified flow the present numerical simulation also reproduces the major sheet layer thickness and mobile bed roughness similar to measured data however the mobile bed roughness is more than a factor two larger than the major sheet layer thickness to seek for an explanation we first carry out an analysis on the vertical distribution of various shear stresses in the present two phase flow formulation while it is clear that sediment collisional stress and frictional stress dominate the energy dissipation in the major sheet layer the resolved sediment reynolds shear stress is of notable magnitude above the major sheet layer with a mean sediment concentration of a few percent the intermittent motions of sediment vertical fluxes and their relationships to the turbulent sweep ejection events are studied we first demonstrated that intermittent sediment bursts is responsible for suspending notable amount of sediment up to more than 10 grain diameters above the bed and hence contribute to the resolved sediment reynolds stress consequently these near bed intermittent events may play a major role in the enhanced mobile bed roughness simulation results further suggest that the turbulent ejection motions are correlated with upward sediment fluxes while the sweep events are mostly associated with the downward sediment fluxes and this correlation holds for a wide range of sediment concentration ϕ 0 2 although the present les eulerian two pahse model is successfully validated with the steady sheet flow experiment of revil baudard et al 2015 several improvements of this model are warranted numerical experiments on lower grid resolutions with grid size δ greater than the grain size suggest that the velocity profile in the dilute transport layer is sensitive to the numerical resolution however using a high numerical resolution with grid size similar to sediment grain size may not be always attainable especially for finer grains therefore a more comprehensive subgrid closure on turbulence sediment interaction is necessary to further improve the present les two phase flow modeling approach for sediment transport meanwhile a wider range of sediment properties and flow conditions should be investigated to provide a more comprehensive understanding of natural sand transport in addition several assumptions were adopted on the fluid sediment momentum transfers such as the ignorance of added mass lift force and basset forces balachandar and eaton 2010 the relative importance of these forces compared with the drag force and the formulation of associated subgrid models should also be studied especially for various sediment properties finally the present study focuses on simulating particle turbulence interactions and their effects on sheet flow while relatively simple closures on particle stresses are adopted future modeling effort should also be extended for more complete description of particle stress in both intermediate and high particle concentration regimes e g berzi and fraccarollo 2015 acknowledgements this study was supported by national science foundation oce 1635151 oce 1537231 and office of naval research n00014 16 1 2853 j chauchat was supported by the region rhones alpes coopera project and explora pro grant and the french national programme ec2co lefe modsed numerical simulations were carried out on mills at the university of delaware supermic through support from extreme science and engineering discovery environment xsede and hpc resources from genci cines grant 2015 x2016017567 the authors would also like to acknowledge the support from the program on fluid mediated particle transport in geophysical flows at the kavli institute for theoretical physics santa barbara usa the laboratory legi is part of the labex tec 21 investissements d avenir grant agreement n anr 11 labx 0030 and labex osug 2020 investissements davenir anr10 labx56 the authors would like to acknowledge dr guillaume balarac for this suggestions on the large eddy simulation methodologies and thank dr peter traykovski for this useful comments and discussion on the near bed intermittencies appendix a particle stress model to resolve the full dynamics of sediment transport closures of intergranular stress are needed particularly in moderate to high concentration regions for moderate sediment concentration it is assumed that binary collisions dominate intergranular interactions and a closure based on the kinetic theory of granular flow is adopted for high sediment concentration ϕ 0 5 binary collisions eventually become non exist and intergranular interaction is dominated by enduring contact frictional forces among particles in this study the closures of particle pressure and particle stress both consist of a collisional kinetic component and a quasi static component hsu et al 2004 johnson and jackson 1987 a 1 p s p s c p s f a 2 τ i j s τ i j s c τ i j s f the collisional component is first discussed in the kinetic theory particle stress and particle pressure are quantified by granular temperature θ jenkins and savage 1983 and we adopted the transport equation for granular temperature suggested by ding and gidaspow 1990 a 3 3 2 ϕ ρ s θ t ϕ ρ s u j s θ x j p s c δ i j τ i j s c u i s x j x j κ s c θ x j γ s 3 β θ where the terms on the right hand side rhs are the production of granular temperature the flux of granular temperature the energy dissipation rate due to inelastic collision γs and the last term is the dissipation due to the interaction with the carrier fluid phase notice that the granular temperature equation is constructed by further neglecting the subgrid contribution to the granular temperature as we observed that the resolved granular temperature is already small in the dilute transport layer following ding and gidaspow 1990 closure of particle pressure is written as a 4 p s c ρ s ϕ 1 2 1 e ϕ g s 0 θ where e is the coefficient of restitution during collision and we take e 0 8 for sand particles in water the radial distribution function g s0 is introduced to describe the crowdiness of particle which can be calculated as carnahan and starling 1969 a 5 g s 0 2 ϕ 2 1 ϕ 3 the radial distribution function g s0 quantifies the frequency of particle collisions which is a sharp increasing function of sediment concentration ϕ the formula of carnahan and starling 1969 becomes invalid when sediment concentration becomes very large as it under predicts g s0 when the sediment concentration is approaching the close packing limit ϕm berzi and fraccarollo 2015 chialvo et al 2012 however in modeling the dense region in the present model the granular temperature reduces to nearly zero and inter granular interactions are dominated by enduring contact frictional component of the stress therefore the radial distribution function of carnahan and starling 1969 is still adopted for simplicity the particle stress is calculated as a 6 τ i j s c μ s c u i s x j u j s x i λ 2 3 μ s c u k s x k δ i j where the particle shear viscosity μsc is calculated as a function of granular temperature and radial distribution function a 7 μ s c ρ s d θ 4 5 ϕ 2 g s 0 1 e π π g s 0 1 e 3 e 1 ϕ 2 15 3 e π ϕ 6 3 e similarly the bulk viscosity is calculated as a 8 λ 4 3 ϕ 2 ρ s d g s 0 1 e θ π the κsc is the conductivity of granular temperature calculated as a 9 κ s c ρ s d θ 2 ϕ 2 g s 0 1 e π 9 π g s 0 1 e 2 2 e 1 ϕ 2 2 49 33 e 5 π ϕ 2 49 33 e the dissipation rate due to inelastic collision is calculated based on that proposed by ding and gidaspow 1990 a 10 γ s 3 1 e 2 ϕ 2 ρ s g s 0 θ 4 d θ π 1 2 u i s x i when the volumetric concentration of particles becomes close to random loose packing particles are constantly in contact with one another and particulate energy are mainly dissipated by friction between sliding particles tardos 1997 when the sediment concentration exceeds random loose packing concentration ϕf we adopt the simple model of johnson and jackson 1987 for particle pressure a 11 p s f 0 ϕ ϕ f f ϕ ϕ f m ϕ m ϕ n ϕ ϕ f where ϕ f 0 5 ϕ m 0 6 and f 0 05 m 3 and n 5 are empirical coefficients cheng et al 2017 the particle stress due to frictional contact is calculated by the model of srivastava and sundaresan 2003 a 12 τ i j s f 2 μ s f s i j s where μsf is the frictional viscosity and s i j s is the deviatoric part of strain rate tensor of sediment phase a 13 s i j s 1 2 u i s x j u j s x i 1 3 u k s x k δ i j srivastava and sundaresan 2003 combined the frictional normal stress from johnson and jackson 1987 and the frictional viscosity from schaeffer 1987 model and suggested the friction viscosity to be calculated by a 14 μ s f p s f sin θ f s s where θf 35 is the angle of repose see table 1 in sediment transport the quasi static component of particle stress plays a definite role to ensure the existence of an immobile sediment bed and a low mobility layer of enduring contact hsu et al 2004 hence the empirical coefficients presented here are calibrated to ensure that a stable sediment bed can be established below the mobile transport region appendix b numerical initial condition the initial sediment concentration is specified as a smooth vertical profile to avoid initial numerical instability b 1 ϕ z ϕ m 0 1 tanh a z b 0 z 2 where the constants ϕ m 0 0 54 and a 150 are chosen to ensure a relatively smooth transition of sediment concentration from ϕ m0 within the bed to 0 in the upper column it is found that it is practical to relax the system by setting the ϕ m0 to be lower than the maximum packing limit ϕm as the frictional stress diverges at ϕm see appendix a initially the sediment concentration in the bed will increase due to the immersed weight and the frictional stress will increase accordingly eventually the frictional pressure gradient in the bed can well balance the immersed weight of the bed for the initial condition for the velocity fields the initial velocities are set to zero within the bed z h b0 following de villiers 2007 the initial velocity profile above the bed z h b0 is specified to be a sum of laminar velocity profile and streak like perturbations in the streamwise and spanwise velocities b 2 u z u f 3 z r e τ 0 1 2 z r e τ 0 2 u f z 640 cos α y y exp λ z 2 0 5 1 0 2 ξ 1 b 3 v z u f z 400 sin α x x exp λ z 2 1 0 2 ξ 2 b 4 w z 0 where uf is the bulk velocity r e τ 0 u h f 0 ν f 6100 is the reynolds number based on the initial flow depth x y and z are coordinates in wall units x u x ν f y u y ν f and z u z h b 0 ν f ξ 1 and ξ 2 are gaussian random numbers with zero mean value and standard deviation of 1 λ 2 5 10 6 is the decay coefficient for perturbation α x π 5000 and α y π 2500 are the wavenumber for the streak waviness in the streamwise and spanwise directions respectively the streak like perturbations are beneficial for the fast growth of turbulent modes as the sinusoidal streaks induce vortex formation and further instabilities note that these coefficients are different from the values used in de villiers 2007 they are adjusted for the present high reynolds number turbulent flows so that about four wave like streaks are initialized in streamwise and spanwise directions supplementary material supplementary material associated with this article can be found in the online version at 10 1016 j advwatres 2017 11 016 appendix c supplementary materials supplementary data s1 supplementary raw research data this is open data under the cc by license http creativecommons org licenses by 4 0 supplementary data s1 
902,the linkage between root zone soil moisture and groundwater is either neglected or simplified in most land surface models the fully coupled subsurface land surface model terrsysmp including variably saturated groundwater dynamics is used in this work we test and compare five data assimilation methodologies for assimilating groundwater level data via the ensemble kalman filter enkf to improve root zone soil moisture estimation with terrsysmp groundwater level data are assimilated in the form of pressure head or soil moisture set equal to porosity in the saturated zone to update state vectors in the five assimilation methodologies the state vector contains either i pressure head or ii log transformed pressure head or iii soil moisture or iv pressure head for the saturated zone only or v a combination of pressure head and soil moisture pressure head for the saturated zone and soil moisture for the unsaturated zone these methodologies are evaluated in synthetic experiments which are performed for different climate conditions soil types and plant functional types to simulate various root zone soil moisture distributions and groundwater levels the results demonstrate that enkf cannot properly handle strongly skewed pressure distributions which are caused by extreme negative pressure heads in the unsaturated zone during dry periods this problem can only be alleviated by methodology iii iv and v the last approach gives the best results and avoids unphysical updates related to strongly skewed pressure heads in the unsaturated zone if groundwater level data are assimilated by methodology iii enkf fails to update the state vector containing the soil moisture values if for almost all the realizations the observation does not bring significant new information synthetic experiments for the joint assimilation of groundwater levels and surface soil moisture support methodology v and show great potential for improving the representation of root zone soil moisture keywords root zone soil moisture integrated model data assimilation groundwater level surface soil moisture 1 introduction precise knowledge of spatial and temporal variations in root zone soil moisture content is important for agriculture weather prediction and drought monitoring lue et al 2011 however information about root zone soil moisture is usually limited due to the small volume support of point scale measurements grayson and western 1998 and the high level of uncertainty associated with land surface model predictions which is caused by uncertain input parameters and atmospheric forcings houser et al 2001 remote sensing techniques allow monitoring of surface soil moisture content over large areas but these measurements are limited to the top few centimeters of the soil and are not reliable for dense vegetation du et al 2000 in recent years some work has been carried out to improve the prediction of root zone soil moisture content using data assimilation techniques ground based surface soil moisture measurements or remotely sensed surface soil moisture data have been combined with land surface models which simulate surface and root zone soil moisture content to update soil moisture estimates for all soil layers walker et al 2002 walker et al 2003 sabater et al 2007 crow et al 2008 das et al 2008 kumar et al 2009 pipunic et al 2011 ford et al 2014 han et al 2014 de lannoy and reichle 2016 kumar et al 2009 concluded that the potential of surface soil moisture assimilation to improve root zone soil moisture characterization was higher when the surface root zone coupling was stronger walker et al 2002 showed that the soil moisture profile cannot be retrieved from near surface soil moisture measurements when the near surface and deep soil layers become decoupled such as during extreme drying events das et al 2008 found that the assimilation of remotely sensed surface soil moisture content improved soil moisture content estimation for shallow soil 0 0 50 m but hardly for the deeper soil layers 0 50 3 86 m frod et al 2014 demonstrated that the relationship between near surface 5 10 cm and root zone 25 60 cm soil moisture was generally strong pipunic et al 2011 stated that there was no clear evidence of an ability to strongly improve soil moisture content prediction for deeper layers using only surface information in summary the updating of root zone soil moisture by combining surface soil moisture from satellite and land surface model predictions via data assimilation is a way forward but often improvement is only limited groundwater effects on root zone soil moisture are either neglected or not explicitly treated in most land surface models chen and hu 2004 kollet and maxwell 2008 a shallow groundwater table is more likely to lead to surface runoff because of saturation excess and to supply water to the atmosphere at the rate demanded by the atmosphere on the other hand deeper groundwater tables generally indicate drier areas in this situation surface runoff is likely to be generated by the infiltration excess runoff mechanism the groundwater is recharged when the infiltration is enhanced and evaporation is more often limited by the available soil moisture under both conditions soil moisture content is changed through its interaction with the groundwater liang and guo 2003 miguez macho et al 2008 miguez macho and fan 2012 zhu et al 2013 recently a number of modeling platforms with varying complexity which couple groundwater land surface and atmospheric models have emerged aiming to improve the estimation of water flow in the unsaturated zone kollet and maxwell 2008 maxwell 2009 davin et al 2011 tian et al 2012 maxwell et al 2015 for example the hirham regional climate model is coupled with the mike she hydrological model larsen et al 2014 the common land model is integrated into parflow parflow clm maxwell and miller 2005 kollet and maxwell 2008 another example is the atmosphere surface subsurface terrestrial system modeling platform terrsysmp shrestha et al 2014 terrsysmp and parflow clm have been used in a number of simulation studies like the investigation of the role of topography and subsurface heterogeneity on the interactions between land surface energy fluxes and groundwater dynamics rihani et al 2010 the role of topography and subsurface heterogeneity on spatial soil moisture variability and discharge gebler et al 2017 the impact of subsurface hydrodynamics on the lower atmosphere shrestha et al 2014 the influence of crop specific physiological properties on the partitioning of land surface energy fluxes and the resulting modifications in the heat and moisture budgets of the atmospheric boundary layer sulis et al 2015 and the influence of groundwater dynamics on land surface processes net radiation latent heat flux sensible heat flux and ground heat flux kollet and maxwell 2008 information of groundwater level is available in many places and at regional and national scales as it can be continuously measured by automatic sensing devices at low cost and high accuracy like the collaborative national groundwater monitoring network ngwmn program for the united states subcommittee on ground water 2011 as mentioned above near surface soil moisture content hardly provides information about root zone soil moisture content and groundwater levels may contain valuable information about the unsaturated zone integrated models like terrsysmp are expected to be suited to explore in an optimal way the information provided by groundwater levels as well as near surface soil moisture content data using sequential data assimilation techniques like ensemble kalman filter evensen 1994 burgers et al 1998 enkf has been applied successfully in unsaturated flow and groundwater flow problems the contributions relevant in the context of this paper are the assimilation of water table depth or piezometric head chen and zhang 2006 franssen and kinzelbach 2008 li et al 2012 kurtz et al 2014 shi et al 2014 soil moisture content margulis et al 2002 reichle et al 2002 montzka et al 2013 erdal et al 2014 shi et al 2014 pasetto et al 2015 brightness temperature margulis et al 2002 crow and wood 2003 sabater et al 2007 das et al 2008 han et al 2014 and surface radiometric temperature crow et al 2008 however there are few studies focusing on the assimilation of groundwater levels to improve the characterization of the unsaturated zone zhang et al 2016 assimilated both groundwater head and soil moisture content with ensemble transform kalman filter etkf in the mike she hydrological model and emphasized the necessity of using localization when assimilating both groundwater head and soil moisture rasmussen et al 2015 also used etkf to assimilate groundwater head and stream discharge to study the relationship between the filter performance and the ensemble size shi et al 2014 2015b and shi et al 2015a employed enkf to investigate the impacts of multivariate hydrological data assimilation including groundwater levels on estimating unsaturated flow parameters with synthetic experiments and real world experiments in our study we focus on the comparison of different methodologies for the assimilation of groundwater levels in integrated hydrological models in addition the methodological developments made in this work aim for high dimensional problems and spatially heterogeneous fields of hydraulic parameters if groundwater levels are assimilated in terms of pressure head the strongly skewed and non gaussian pressure distributions for dry soils cause problems for sequential data assimilation algorithms as these algorithms perform optimally only for gaussian distributions the water retention function maps the very negative values to a very small range of water content values and is highly nonlinear therefore if enkf is applied in a standard form the nonlinear state propagation would cause large errors erdal et al 2015 discussed the importance of state transformations in enkf when dealing with strong nonlinearities for unsaturated flow modeling in this work a complication is that groundwater level data although often showing a correlation with soil moisture content are in general only weakly correlated with soil moisture content of the upper soil layers this raises the question what is the sphere of influence of groundwater level data and whether the complete unsaturated zone should be updated with groundwater level data in summary two problems to be solved for assimilating groundwater level data in integrated hydrological models are 1 strongly skewed pressure distributions under dry conditions 2 detection of the sphere of influence of groundwater level data in the context of cross compartmental data assimilation in order to test different methodologies for groundwater level and surface soil moisture assimilation in the integrated model terrsysmp a series of synthetic experiments was carried out in this work the remainder of this paper is organized as follows in section 2 we briefly review the terrsysmp model used to simulate the soil moisture and subsurface pressure distributions as well as energy exchange fluxes between the land and the atmosphere the land surface model clm and subsurface model parflow are described separately assimilation methodologies with the ensemble kalman filter enkf are also explained in this section section 3 then introduces our experiment setup and in section 4 the main findings of our assimilation study are presented discussion and conclusion are presented in sections 5 and 6 respectively 2 materials and methods in this study we perform data assimilation experiments with a coupled subsurface land surface model in order to illustrate the technical setup for these experiments section 2 1 summarizes the utilized models section 2 2 explains the implementation of data assimilation methodologies for this study and introduces our data assimilation platform 2 1 terrestrial system modelling platform terrsysmp terrsysmp shrestha et al 2014 is a modular scale consistent terrestrial system model composed of well established models for the atmosphere cosmo baldauf et al 2011 the land surface clm oleson et al 2008 and the subsurface parflow ashby and falgout 1996 jones and woodward 2001 kollet and maxwell 2008 its modularity makes it possible to run in fully coupled way cosmo clm parflow partly coupled way clm parflow clm cosmo and uncoupled way by running each model individually cosmo clm parflow in our work the partly coupled parflow clm terrsysmp configuration is used the land surface component of terrsysmp clm calculates the mass and energy balance at the land surface including evaporation from the ground surface transpiration from plants sensible and ground heat fluxes and freeze thaw processes oleson et al 2008 shrestha et al 2014 10 soil layers are internally defined with a total extend of about 3 m there is no lateral flow between grid cells because water and energy flows are calculated only in the vertical direction clm input includes atmospheric forcing data i e precipitation wind speed incoming shortwave and longwave radiation air pressure air temperature and humidity land surface data including information on plant functional type pft and adjustable parameters and physical constants clm uses soil properties like soil texture and bulk density in combination with model internal pedotransfer functions to derive soil hydraulic and thermal parameters like saturated hydraulic conductivity oleson et al 2004 oleson et al 2008 the subsurface part of terrsysmp consists of the subsurface flow model parflow in parflow a 2d distributed overland flow simulator is implemented into a 3d variably saturated groundwater flow kollet and maxwell 2006 the overland flow boundary condition helps parflow to simulate fully coupled surface and subsurface flow via a mixed form of the 3d richards equation and the kinematic wave equation parflow solves for water pressure at every time step and calculates the saturation field from which soil moisture and water table depth are determined by the pressure saturation relationship for example according the van genuchten formulation van genuchten 1980 more details about the model equations the discretization and the numerical implementation can be found in ashby and falgout 1996 jones and woodward 2001 kollet and maxwell 2006 parflow input includes soil hydraulic parameters like saturated hydraulic conductivity and residual soil water content van genuchten parameters like α and n and soil porosity data from clm and parflow are exchanged via the coupling software oasis mct valcke 2013 which is a library providing a generic interface to exchange information between standalone executable codes in memory originally clm only takes into account 1d vertical flow in the unsaturated zone with a decoupled surface routing as the upper boundary condition and a simple parameterization of groundwater table as a lower boundary condition parflow overcomes these limitations with an overland flow boundary condition and 3d variable saturated groundwater flow parflow provides clm with its calculated subsurface pressure ψ and saturation sw values for the 10 uppermost subsurface layers and in return clm provides the upper boundary condition for parflow which consists of the soil infiltration values qinf which are calculated based on the land surface fluxes of clm precipitation interception total evaporation total transpiration thus the 1d column soil moisture prediction of clm is replaced by the 3d formulation in parflow the information exchange between clm and parflow is shown schematically in fig 1 the thicknesses of the 10 uppermost subsurface layers should be consistent with those of the 10 soil layers in clm and also the porosities in both models need to be the same 2 2 data assimilation methodologies 2 2 1 ensemble kalman filter enkf in our study enkf is used in combination with the fully coupled model terrsysmp to update soil moisture content and groundwater levels the observation vector for the jth ensemble member at time step t is 1 y j t y t ɛ j t where y t is the vector with measurements at time step t εj t is a vector with perturbation terms drawn from a normal distribution n 0 σ where the standard deviation σ is equal to the expected measurement error for each ensemble member j at time step t the state vector xj t is updated by observations and model predictions 2 x j t a x j t f k t y j t hx j t f where the superscripts a and f refer to the updated state vector the analysis and the model predicted state vector respectively h is the observation operator which represents the relationship between the observation vector y and the state vector x its elements are 1 or 0 where 1 corresponds to the measurement location if measurement location and grid cell center coincide exactly the kalman gain k t is calculated by 3 k t c h t hc h t r 1 where r is the measurement error covariance matrix c is the covariance matrix which is given by 4 c 1 n 1 j 1 n x j t f x t f x j t f x t f t where n is the number of ensemble members and x t f is a vector containing ensemble mean values for the state vectors at time step t in our work the observation vector y and the state vector x include pressure and or soil moisture as mentioned above the correlations between pressure and soil moisture are needed to update pressure with soil moisture data the van genuchten model gives the relation between soil moisture and pressure 5 ψ θ s θ r θ ψ θ r 1 m 1 1 n α where θs is the saturated soil moisture content which is equal to porosity ϕ θr is residual soil moisture conten α is a measure of the first moment of the pore size density function n is an inverse measure of the second moment of the pore size density function and m 1 1 n enkf needs to be tested carefully when pressure head is the state variable in our experiment in the unsaturated zone the richard s equation is solved for pressure head and pressure head can vary between strongly negative values in case of dry soil and positive values in the aquifer and depends on the soil hydraulic parameters and climate conditions fredlund 1991 for example when the infiltration rate exceeds the saturated hydraulic conductivity ponding occurs and pressure head becomes positive at the top of the domain contrarily during a long drying period pressure head can become strongly negative because of evapotranspiration in data assimilation stochastic realizations may show a large spread of pressure head values because of varying hydraulic parameters between the realizations so that the ensemble may show a strongly skewed probability density function of pressure this strong skewness can generate unrealistically large updates in the enkf analysis step because the ensemble mean is largely influenced by few realizations with extreme values in addition enkf performs optimally for gaussian distributions but not for strongly skewed distributions erdal et al 2014 mitigated this issue by limiting the negative pressure head to 10 m but this is not an optimal solution for arid regions they also proposed two other methods to avoid very negative pressure heads in the unsaturated zone that are normal score ns transformation and pressure head to water content pwc transformation with a soil water retention function erdal et al 2015 both methods improved the state distribution as well as the assimilation results however ns transformation may distort the spatial corrections and in pwc transformation all pressure heads above or below a certain limit are mapped to the same water content we are dealing with the same issue as groundwater pressure head is assimilated to improve the pressure head distribution for the whole domain including saturated zone and unsaturated zone in our work a modular high performance data assimilation framework terrsysmp pdaf kurtz et al 2016 which couples terrsysmp and the pre existing parallel data assimilation library pdaf parallel data assimilation framework nerger and hiller 2013 is used details about technical implementation installing and running terrsysmp pdaf can be found in kurtz et al 2016 currently terrsysmp pdaf can assimilate pressure and or soil moisture data in parflow with enkf 2 2 2 assimilation methodologies in this work we are concerned with groundwater level data which can be continuously measured at high accuracy and low cost with automatic sensing devices in groundwater wells we propose two ways to introduce the groundwater level data into the model terrsysmp fig 2 illustrates how observed groundwater level is translated into observation data two different variants are possible first at locations with groundwater level measurements the pressure head in the saturated zone is calculated assuming hydrostatic conditions in the aquifer pressure head in the saturated zone is then assumed to be the observation of our assimilation system second soil moisture content in the saturated zone should be equal to porosity and then soil moisture data porosity data is assimilated based on these two observation strategies in total five methodologies are tested and compared to assimilate groundwater level data in terrsysmp to improve the predictions of root zone soil moisture 1 assimilation of pressure p here the observation is pressure head in the saturated zone at locations with groundwater level measurements the observation error of pressure head is assumed 0 01 m the state vector includes the pressure head for the whole domain including the saturated and unsaturated zones 2 assimilation of log transformed pressure data p log to reduce problems with non gaussianity in method 1 pressure head is converted into log space as log10 100 pressure to improve the enkf performance to avoid taking log of zero or negative values pressure head is subtracted from 100 m for both the measurement data and simulated values before calculating the kalman gain the observation error in log space is 4 10 5 log m which is based on the random error of 0 01 m for pressure head we calculated the standard deviation of log10 100 perturbed pressure where perturbed pressure was generated by adding a random error of 0 01 m the state vector includes the pressure head in log space for the whole domain including the saturated and unsaturated zones after the analysis step the log pressure head is back transformed 3 assimilation of soil moisture sm here the observation is soil moisture porosity in the saturated zone at locations with groundwater level measurements the observation error is 0 02 m3 m3 in enkf the state vector includes the soil moisture for the whole domain including saturated zone and unsaturated zone after the update with enkf the state vector needs to be converted to pressure head for the whole domain on the basis of the van genuchten model as pressure head is the prognostic variable in parflow 4 assimilation of pressure in saturated zone p mask this method is similar to method 1 but for each ensemble member grid cells in the unsaturated zone are not updated in the data assimilation procedure unrealistic updates for the unsaturated zone caused by extreme values in the ensemble are avoided but the disadvantage is that the groundwater level information is not directly used to update soil moisture contents in the unsaturated zone 5 assimilation of both pressure and soil moisture mix in this approach groundwater level measurements are assimilated as pressure head data with an observation error of 0 01 m however the state vector includes now two parts pressure head for the saturated zone and soil moisture for the unsaturated zone because of different parameters the ensemble members will show different vertical divisions between the unsaturated and saturated zone in the data assimilation step the definition of the state vector should be consistent for all ensemble members this implies that a given grid cell should be updated for all ensemble members in the same way either in terms of soil moisture or pressure head the distinction between the saturated and unsaturated zone is firstly defined by the deepest water table depth among the ensemble members then for grid columns with groundwater level measurements we apply a correction of this depth in case the groundwater level of the observation is higher than the one of the ensemble grid cells above this depth are updated in terms of soil moisture and grid cells below this depth in terms of pressure head after defining the state vector and defining which elements are updated in terms of pressure and which elements in terms of soil moisture the pressure head observations are assimilated via enkf in this mixed approach also joint assimilation of surface soil moisture content and groundwater level was tested where observations include both groundwater level data and surface soil moisture with measurement errors equal to 0 01 m and 0 02 m3 m3 respectively in the methodologies sm and mix after data assimilation with enkf updated soil moisture is transformed to pressure values according the van genuchten model eq 5 because pressure head is parflow s prognostic variable and soil moisture or saturation is a derived quantity which is not directly used as a state variable for the next time step kurtz et al 2016 before the transformation we check if soil moisture stays within physical boundaries between residual soil moisture content and porosity if updated soil moisture content is less than residual soil moisture we set updated soil moisture residual soil moisture 0 01 porosity and do the transformation for this updated value if updated soil moisture content is larger than porosity no transformation is performed which implies that pressure head in this grid layer is not updated and the calculated pressure value before the data assimilation step with enkf is used finally if updated soil moisture content is in the range between residual soil moisture content and porosity transformation is done directly the updated pressure head in the domain either directly updated by enkf or indirectly updated by soil moisture is used for the next time step 3 experiment setup in order to test the five assimilation methodologies with terrsysmp pdaf we define a synthetic experiment which is carried out for the land surface subsurface clm parflow components of terrsysmp a synthetic reference run provides pressure head or soil moisture observations and the ensemble needed for data assimilation experiments is generated by perturbing subsurface parameters 3 1 reference model the synthetic domain has a horizontal extension of 2000 m 2000 m and is discretized horizontally into 2 2 grid cells with a grid cell size of 1000 m 1000 m the domain has a vertical extension of 30 m which is discretized into 30 soil layers with variable thickness the uppermost 10 layers are with layer thicknesses exponentially increasing with depth these ten layers extend in total over 3 m and in these ten layers fluxes are exchanged between clm and parflow the deeper 20 subsurface layers have a constant thickness of 1 35 m the slope is 1 in both x and y directions for all grid cells hillslope thus there is topographically driven overland flow and groundwater flow in parflow soil texture meteorological conditions and pfts are homogeneous over the domain the bottom boundary and boundaries on the side are impermeable overland flow can be generated over the upper boundary kollet and maxwell 2006 where ponded water is routed along the land surface and allowed to exit the domain at the bottom of the hill in order to produce different land surface fluxes and groundwater dynamics we use two forcing datasets from a semi arid climate and a humid climate site data for the semi arid picassent site in spain are from a local meteorological station and data for the humid kennedy space center site within the merritt island in florida usa are from the ameriflux network bracho et al 2008 for each site climate data from two years are used one year is for model spin up and the other year is for the model simulation and assimilation fig 3 shows the daily accumulated precipitation average air temperature and average incoming shortwave radiation for the two climate sites three soil types with different soil hydraulic parameters are used variably saturated flow is parameterised with the van genuchten model spatially constant values of residual soil moisture content θr the saturated hydraulic conductivity ks the van genuchten parameters α and n are chosen from leij et al 1996 see table 1 to keep hydraulic consistence between clm and parflow porosity θs for both models is calculated on the basis of the sand fraction via the following pedotransfer function in clm 6 θ s 0 489 0 00126 sand three plant functional types pfts are used in the simulations namely bare soil shrubs grasland short roots and broadleaf tree deeper roots bare soil shrub and evergreen broadleaf tree are specified for simulations for the picassent site and bare soil grassland and evergreen broadleaf tree for the florida site the vegetation properties are characterized by distinct physiological pft specific parameters in clm oleson et al 2004 each configuration of the atmospheric forcings soil texture and vegetation type is initialized with a water table depth of 1 m relative to the land surface then terrsysmp is run for 100 years forced repeatedly with one year of atmospheric data for all the simulation scenarios in this work 100 years of spin up is enough to reach hydrologic and dynamic equilibrium conditions after the model has reached equilibrium the model is forced by another year of atmospheric data for the simulation and assimilation experiments 3 2 setup of data assimilation experiments two climate types three soil texture types and three pfts were combined to generate 18 terrsysmp configurations these configurations generate different land surface fluxes and groundwater dynamics for each configuration groundwater dynamics and land surface fluxes also differ between the four hillslope grid cells only one grid cell is observed the results for the highest situated grid cell and the grid cell at medium elevation are analyzed resulting in 36 2 climate types 3 soil types 3 vegetation types 2 grid cells along slope synthetic scenarios with different groundwater levels for the data assimilation experiments a synthetic reference run is created for each scenario mentioned above the synthetic runs use the true hydraulic parameters shown in table 1 and provide virtual observation data of pressure head or soil moisture as specified in section 2 2 2 although groundwater level data are assimilated we refer to them in terms of pressure head or soil moisture data below the synthetic reference simulations are run for 1 year with an hourly time step for both parflow and clm the synthetic reference runs also provide the true values of root zone soil moisture the five assimilation methodologies are evaluated by comparing the assimilation results of root zone soil moisture and their corresponding true values for the assimilation experiments an ensemble of 128 realizations of log10 ks is created for each scenario individual log10 ks realisations are generated by perturbing the reference value of log10 ks see table 1 with an additive random perturbation which is sampled from the standard normal distribution n 0 1 the spin up is done for each ensemble member individually for 100 years using the different ks values as input the ensemble is used in addition for an open loop run and data assimilation experiments observation data from the reference run groundwater level data in the form of pressure head or soil moisture are assimilated on a daily basis for the observation grid point the root zone soil moisture is updated directly or indirectly by assimilating observation data the model uncertainty from the physical parametrizations in the model is ignored in this study 4 results groundwater level assimilation experiments for all the scenarios mentioned above are performed with the five assimilation methodologies and joint assimilation experiments of groundwater level and surface soil moisture with method 5 described in section 2 2 2 some representative results are shown below to illustrate the possibility and potential of the assimilation strategies under different groundwater dynamics and compare the performance of the five methodologies to evaluate the performances of the different assimilation strategies methodologies the root mean square error rmse of soil moisture from the surface to the depth of 3 m the 10 upper soil layers which is assumed to be the root zone is calculated as 7 rms e j 1 t t 1 t θ t j sim θ t j true 2 where θ t sim is the ensemble mean soil moisture at time step t θ t true the true soil moisture at time step t j the jth soil layer and t is the number of time steps the weighted average of rmse for these 10 layers is also calculated 8 rmse j 1 10 ω j rms e j where ω j is the weight for the jth soil layer which is calculated by the depth of the jth soil layer dj 9 ω j d j j 1 10 d j the deeper layers get larger weights in correspondence to their larger vertical extend 4 1 assimilation of groundwater levels in this section only groundwater level in the form of pressure head or soil moisture is assimilated in terrsysmp pdaf the scenario name is specified by the soil type climate site plant type and grid top means the highest grid cell and medium the grid cell with medium elevation as in this work the soil moisture observation error is 0 02 m3 m3 we assume that if the rmse value of root zone soil moisture from the open loop run is less than 0 02 m3 m3 the model performs well the scatterplots in fig 4 illustrate that if the water table depth is very deep 10 m the open loop rmse values are below 0 02 m3 m3 when the water table is very deep root zone soil moisture content is low for all realizations in the open loop run so that the difference between the ensemble mean and the reference is small on the other hand in some scenarios when the water table is very shallow close to surface all realizations have very wet root zones which also result in low rmse values 0 02 m3 m3 notice that in the simulation experiments the meteorological forcings and porosity were known we analyze the results of the assimilation experiments for some scenarios with with open loop rmse values above 0 02 m3 m3 and water table depth between 5 m to 0 m where data assimilation is necessary for improving the model performance 4 1 1 aggregated results fig 5 shows the scatter plots of weighted average of rmse values for the open loops versus weighted average of rmse values for the five data assimilation methodologies for the different scenarios with open loop rmse values above 0 02 m3 m3 the 1 1 line corresponds to equal weighted averages of rmse for the open loop and data assimilation runs points which are situated below the 1 1 line indicate that assimilation of groundwater level improves soil moisture characterization compared to the open loop from the figure we can see that methodologies p mask sm and mix work well for most scenarios especially the methodology mix which results for all the scenarios in weighted average rmse values smaller than 0 02 m3 m3 after assimilation root zone soil moisture characterization is greatly improved compared to the open loop for this scenario p mask also improves the root zone soil moisture characterization for all the scenarios but the improvement is not as significant as for mix for a few scenarios the methodology sm performs bad methodology p shows the worst performance and p log works a little better than p for these last two methodologies for many scenarios soil moisture characterization is worse after assimilation more details are shown for the scenario loam florida broadleaf top fig 6 shows the ensemble spread of groundwater levels as a function of time for the open loop and the five assimilation methodologies for this scenario during the assimilation table 2 shows the corresponding rmse values of soil moisture for the upper 10 soil layers calculated by eq 7 and weighted average of rmse values calculated by eq 8 in this scenario see table 2 all the five methodologies improve the soil moisture profile estimation after assimilation compared with the open loop run the improvement for the upper layers is limited for the methodologies p and p log when compared with the other 3 methodologies and the deeper layers are greatly improved by all the methodologies from fig 6 we can see that all methodologies improve the ensemble towards the reference groundwater level and reduce the ensemble variance in particular the ensemble converges quickly to the reference in the methodologies sm and mix which produce the best results according to table 2 there are small spikes in the ensemble for the methodologies p and p log during the summer period which might affect their performances in the below section performances of the five assimilation methodologies are analyzed and compared with some representative scenarios 4 1 2 comparisons of the five assimilation methodologies the performances of the methodologies p and p log are strongly dependent on the water content distribution in the unsaturated zone under some conditions like a long dry period picassent site in spain high potential et or erroneous hydraulic parameters ks the pressure head ensemble shows a strongly skewed distribution in the unsaturated zone which cannot be properly handled by enkf when most realizations calculate a groundwater table which is higher than the reference value the methodology sm also has a problem as the variance calculated at the observation point is near zero below some representative scenarios will be shown to discuss these issues in detail we analyze now the simulation results for the spanish site which is more affected by very dry periods and therefore very dry topsoil conditions we see in fig 7 and table 3 that the simulation performance is now more strongly affected by the dry conditions which is visible in the form of high spikes in the plots for the methodologies p and p log scenario sandyloam spain baresoil medium from table 3 we can see that the methodologies p and p log improve the soil moisture estimations below the reference water table but worsen the soil moisture estimations above the reference water table the other three methodologies perform well above the groundwater table especially the methodology mix fig 7 shows that after about five months of assimilation the reference water table is below almost all the calculated water table depths in the individual realizations for methodologies p and p log related to this these methodologies perform well in the saturated zone in the unsaturated zone the bad performance of methodologies p and p log is related to the artificial large spread of the strongly skewed pressure head distribution resulting from the large updates in enkf in fig 7 there are small spikes in the ensemble for the p and p log methods during the summer period in the summer period soil profiles get drier so the pressure head near the surface becomes more negative some ensemble members with improper ks values may generate very negative pressure heads which result in strongly skewed non gaussian local probability density functions of pressure head as shown in fig 8 a and 8 c which illustrates that before the update there are a few isolated realizations with very negative pressure values while most realizations are close to the synthetic truth the update drags the other realizations to the extreme realizations and therefore to more negative pressure values see fig 8 b and 8 d some realizations are updated to high positive pressure heads these positive pressure heads make no sense in the model from a physical point of view and are the consequence of the artificial large spread of the skewed head distribution resulting from the enkf updates these unwanted positive pressure head updates result in the water table depth spikes in the methodologies p and p log in fig 7 and bad performance for the upper layers in table 3 the figure also shows that the scenario p log strongly reduces the outliers compared to the scenario p but is not able to fully eliminate them in the fig 7 the methodology p mask shows a much slower convergence than the other methodologies in the p mask methodology only the grid cells in the saturated zone are updated in the data assimilation procedure unrealistic updates for the unsaturated zone caused by extreme values in the ensemble are avoided soil moisture contents in the unsaturated zone are indirectly updated as a result of updating groundwater levels by data assimilation and the action of the model equations which leads to its slower convergence in some scenarios methodology sm also worsens the soil moisture estimations in the unsaturated zone while mix still works very well see fig 9 and table 4 for the scenario sandyloam spain shrub medium in methodology sm after two months of assimilation all realizations are above the reference water table see fig 9 b as a consequence y hx in eq 2 is very small for the soil moisture observations taken at the layer nodes as the ensemble spread is close to zero there all realizations are saturated or nearly saturated and if y hx is very small the update in eq 2 is also small therefore the data assimilation according to methodology sm is not able to correct the erroneous water table depth resulting in bad performance statistics as indicated in table 4 for the scenario loamysand spain baresoil medium in methodology sm even though the reference water table is covered by the ensemble spread during assimilation see fig 9 e sm still produces bad results in the unsaturated zone see table 4 in the model the pressure head and soil moisture are defined for the centers of layers nodes but the groundwater table can be between those layer centers see fig 2 in the methodology sm soil moisture porosity for the layer nodes below the reference water table are observations layer node i to the bottom layer node in fig 2 for this scenario in the subfig 9 e the blue line shows the layer node depth of the uppermost saturated layer node layer node i in fig 2 with observation and we can see that the layer nodes which have the observation are also saturated for all realizations which causes the same problem as the previous scenario from fig 9 and table 4 we can see that in these two scenarios methodology mix still performs very well with a fast convergence of the water table depth ensemble towards the true value and greatly reduced rmse values compared to the open loop simulation 4 2 joint assimilation of groundwater levels and surface soil moisture in section 4 1 we evaluate the five assimilation methodologies for assimilating groundwater level data in the integrated subsurface land surface model terrsysmp from the results above we find that the methodology mix works the best now we test this methodology to jointly assimilate groundwater levels and surface soil moisture joint ssm gwl with more realistic experimental settings when jointly assimilating groundwater levels and surface soil moisture the state vector of mix methodology still includes pressure for the saturated zone and soil moisture for the unsaturated zone the observation vector includes also the top surface soil moisture 0 01 m below the soil surface besides pressure head measurements in the saturated zone the soil moisture observation is taken at 0 01 m in order to mimic remote sensing observations in addition the experiments presented in this section mimic more realistic conditions vertically heterogeneous saturated hydraulic conductivity ks and van genuchten parameters α and n are used as input for the data assimilation experiments a synthetic reference run is created for each scenario the synthetic true ks value for the top surface layer is taken from table 1 saturated hydraulic conductivity decreases exponentially with soil depth niu et al 2005 10 k s z i k s z 1 e f z i z 1 i 1 10 k s z i k s z 10 i 11 20 where zi is the soil depth of the ith layer z 1 is the depth of the top surface layer ks z1 is the saturated hydraulic conductivity of the top surface layer and f is the decay factor which indicates the decrease of ks with soil depth beven 1997 f is set to 2 0 m 1 niu et al 2005 the remaining ks values for the deeper 20 layers use the same values as the 10th layer the true van genuchten parameters α and n for each soil layer are generated based on the ks values by sampling a multivariate gaussian distribution described in carsel and parrish 1988 for each soil type carsel and parrish 1988 used four kinds of transformation on ks α and n to get a multi normal distribution among the three parameters no transformation no lognormal ln log ratio sb and hyperbolic arcsine su details about the transformation for each soil type can be found in carsel and parrish 1988 this procedure takes correlations among the three parameters into account table 5 shows the covariance among the three transformed parameters for loam and loamy sand residual soil moisture content θr and porosity θs are deterministic and spatially constant see table 1 please note that ks α and n are vertically heterogeneous but with spatially homogeneous values for the different soil layers 2 5 dimensional other settings are similar as outlined in section 3 2 the synthetic true hydraulic parameters are used as model input to provide virtual measurement data of pressure head and soil moisture in the experiments only loamy sand and loam are tested in combination with the 3 pfts and 2 climate datasets for the assimilation experiments an ensemble of 128 realizations of ks α and n is created as outlined above precipitation is also perturbed in the data assimilation experiments with multiplicative noise sampled from a uniform distribution with values between 0 5 and 1 5 to further investigate the performance of joint ssm gwl we also assimilate surface soil moisture alone ssm for all the scenarios as is typical done in many land surface data assimilation studies and assimilate groundwater level alone gwl fig 10 shows the scatter plots of open loop rmse values versus gwl ssm and joint ssm gwl rmse values for all the scenarios for the upper surface layers depth 0 01 m 0 035 m and 0 075 m ssm and joint ssm gwl improve the soil moisture estimation compared with open loop run because for most cases their blue and red dots are below the 1 1 line but many green dots of gwl distribute above the 1 1 line which indicates that gwl assimilation does not significantly improve surface soil moisture estimation this is reasonable because ssm and joint ssm gwl include surface soil moisture observations to constrain the soil moisture content for the upper layers for the intermediate soil layers depth 0 135 m 0 235 m and 0 4 m joint ssm gwl still works well while the blue dots of ssm are scattered around the 1 1 line and the green dots of gwl move towards an area slightly below the 1 1 line indicating improved performance in other words the performance of ssm gets worse while gwl performs better for the deeper soil layers depth 0 65 m 1 05 m 1 65 m and 2 5 m ssm is not able to improve soil moisture characterization compared to the open loop runs while gwl and joint ssm gwl significantly improve the soil moisture characterization therefore we can conclude that assimilation of surface soil moisture alone can improve the soil moisture characterization for the upper 10 20 cm but it may even worsen the soil moisture estimation for deep layers similarly assimilation of groundwater level alone can improve soil moisture characterization for layers deeper than 40 cm but may have negative effect on soil moisture characterization for surface layers joint assimilation of surface soil moisture and groundwater level is able to integrate the advantages of the two observation types the number of dots in the figure is smaller for deeper layers because in some scenarios the water table depths are shallower than 2 5 m so that both open loop rmse values and assimilation rmse values beneath the water table are very close to zero as all ensemble members reproduce saturated conditions there fig 11 compares the soil moisture results for the reference open loop and the data assimilation scenario joint ssm gwl for loamysand spain shrub medium data assimilation results in an ensemble closer to the reference compared with the open loop run the ensemble does not suffer from variance underestimation in the top four layers soil moisture content is low with some strong fluctuations related to few precipitation events and strong potential evapotranspiration the groundwater level data are able to correct most ensemble members for the deeper soil layers 5 discussion this study compares five data assimilation methodologies for improving root zone soil moisture estimation with groundwater level data or a combination of groundwater level and surface soil moisture data these five assimilation methodologies and assimilation of groundwater level and or surface soil moisture data are evaluated and compared on the basis of 36 synthetic scenarios the problem associated with the implicit gaussian assumption in the enkf for updating pressure heads in the unsaturated zone is illustrated the results of methodologies p and p log suffer from the extreme pressure heads which drag the ensemble mean of model states far away from the truth this problem happens particularly in dry periods when some realizations with extreme ks values result in very negative pressure head values at near surface layers one important aspect in enkf is the ensemble size in our study a size of 128 realizations is used which is a small size compared to groundwater data assimilation studies however erdal et al 2015 tested a very large ensemble size of 1000 members for the problem of highly skewed distributions they pointed out that a larger ensemble size would rather increase the risk for the strong negative pressure heads since these originate from initially sampled extreme parameter values for a large ensemble it is more likely that a few extreme parameter values are sampled furthermore the extreme state values still have a major effect on the ensemble mean even though the ensemble size is larger an alternative data assimilation method which could be used here is the particle filter method which could also help to avoid extreme state values the particle filter can deal with non linear and non gaussian problems but usually requires large numbers of model evaluations to get a reliable result if one can afford the cost of using the particle filter method it is obvious that the skewed pressure distribution would not pose such a problem as in the enkf for the small synthetic examples shown here it would be possible to apply the particle filter however most real cases involve many more unknowns and a much higher dimensionality under those conditions the particle filter is expected to need an extremely large ensemble size erdal et al 2015 also discussed if the most common ad hoc methods would have any positive effect on the performance with such a strongly skewed pressure distribution they concluded that dampening the state update only has limited effect as it cannot remove the extreme realizations which still have an unwanted but dampened impact they concluded that also localization would not improve the performance significantly groundwater level information which is assimilated in the form of soil moisture data works well in most scenarios but there are some exceptions when the simulated water tables from all or most realizations are above the reference water table increasing the ensemble size could solve this specific problem but it is unclear whether this would work in all scenarios for example in real world cases with bias where the simulated water table depths are systematically closer to the surface than the observed water table depth correction is difficult in addition a larger ensemble size also implies of course increased computational costs from section 4 1 we find that the methodology mix which updates model states in terms of pressure for the saturated zone and in terms of soil moisture for the unsaturated zone outperforms the other methods and results in a very good root zone soil moisture characterization this might be related to the simple setting in our experiments where only the saturated hydraulic conductivity ks is perturbed the van genuchten parameters α and n and porosity which link the pressure head and soil moisture in the unsaturated zone are constant while this setting is relatively simple the focus is on developing assimilation methodologies with terrsysmp to improve the estimation of root zone soil moisture by assimilating groundwater levels additional experiments section 4 2 where α and n are also perturbed together with ks further verified the feasibility of the methodology mix there will be additional challenges for the application of the methodology in real world situations the van genuchten model is used to transform the soil moisture to pressure head in the unsaturated zone after enkf in real world cases the relation between soil moisture and pressure head might even not be described correctly by the van genuchten model in addition if the updated soil moisture state is porosity from unsaturated to saturated conditions no back transform is made for this grid cell the model predicted pressure head value prior to assimilation is used in this case strange pressure profiles might be generated as the states of the surrounding grid cells were either directly updated in terms of pressure saturated zone or pressure was indirectly updated via soil moisture unsaturated zone it is expected that in the model simulation for the next time step the pressure profile is partly smoothed again in our work we test joint assimilation of groundwater levels and surface soil moisture with 2 5 dimensional synthetic experiments which is proven to be a promising way to improve the estimation of root zone soil moisture surface soil moisture is easily available temporally and spatially from satellites like smap and smos groundwater level measurements are also potentially numerous and measured at low cost and high accuracy although currently large scale groundwater level data are not systematically stored in databases taking full advantage of these two types of data helps to better understand the soil water profile various studies use data assimilation techniques to combine satellite data with land surface models to update the complete soil moisture profile including the root zone generally in those works the land surface models only consider the exchange of water and energy between atmosphere land surface and vegetation but water and heat fluxes from the deeper subsurface are neglected the integrated model terrsysmp which couples the land surface and subsurface makes it possible to take advantage of the surface soil moisture information from satellite and groundwater levels from observation networks or satellite grace to improve the soil moisture estimation in the vadose zone by data assimilation we impose uncertainties with respect to soil hydraulic parameters ks α and n and model forcings precipitation in the synthetic joint assimilation study in reality these parameters and model forcings especially precipitation are always impacted by a large uncertainty especially for large scale applications it is difficult to obtain information about soil hydraulic and thermal parameter values and their statistics beyond sporadic point measurements it is important to better constrain these parameters which is possible by sequential data assimilation using joint state parameter estimation or inverse modelling in future work we will test the joint estimation of states and soil hydraulic parameters by jointly assimilating surface soil moisture and groundwater level information as this work focused on defining the best data assimilation strategy for data assimilation problems for coupled land surface subsurface models model structural errors were neglected this provides favorable conditions for our synthetic experiments model structural errors could be important in practice examples are neglecting preferential flow in the unsaturated zone or the limited number of plant functional types to cover all vegetation types it is therefore important in future work to evaluate this data assimilation methodology for real world land surface subsurface data assimilation problems nevertheless we think that the parflow clm is less affected by model structure errors than the clm stand alone model as parflow eliminates some of the deficiencies of clm for example concerning the role of groundwater 6 conclusions in this work we develop and compare five data assimilation methodologies to improve root zone soil moisture estimation using groundwater level or joint groundwater level and surface soil moisture data in an integrated model terrsysmp the performance of the five data assimilation methodologies is evaluated for 36 synthetic scenarios under different groundwater level dynamics observations of groundwater levels are assimilated in the form of pressure head data or soil moisture data in terrsysmp to update pressure heads and or soil moisture in the whole domain the results for assimilation of pressure or log transformed pressure show that it can be problematic to apply enkf which performs optimally for gaussian distributions to scenarios with strongly non gaussian state distributions caused by extreme negative pressure heads in the unsaturated zone the results from assimilation of groundwater level data in the form of soil moisture data i e for the aquifer part soil moisture content is set equal to the porosity show that this strategy often gives satisfying results but under certain conditions if nearly all ensemble members have a simulated groundwater depth closer to the surface than the measured groundwater level updating of the states in the unsaturated zone does not work the best methodology is assimilating groundwater level data to update the pressure head in the saturated zone and soil moisture content in the unsaturated zone this method alleviates the impact of skewed pressure distributions on the enkf analysis step joint assimilation of surface soil moisture and groundwater levels also shows great potential in improving the root zone soil moisture profile in synthetic scenarios with both uncertain soil hydraulic parameters and meteorological forcings it is important to extend this work to include estimation of unknown soil and aquifer hydraulic parameters and test in large scale real world applications acknowledgments the authors gratefully acknowledge the computing time granted by the jara hpc vergabegremium on the supercomputer jureca jülich supercomputing centre 2016 at forschungszentrum jülich this work was supported by tr32 patterns in soil vegetation atmosphere systems monitoring modelling and data assimilation further support was provided by the dfg forschergruppe 2131 data assimilation for improved characterization of fluxes across compartmental interfaces this work was financed by a stipend from the china government 
902,the linkage between root zone soil moisture and groundwater is either neglected or simplified in most land surface models the fully coupled subsurface land surface model terrsysmp including variably saturated groundwater dynamics is used in this work we test and compare five data assimilation methodologies for assimilating groundwater level data via the ensemble kalman filter enkf to improve root zone soil moisture estimation with terrsysmp groundwater level data are assimilated in the form of pressure head or soil moisture set equal to porosity in the saturated zone to update state vectors in the five assimilation methodologies the state vector contains either i pressure head or ii log transformed pressure head or iii soil moisture or iv pressure head for the saturated zone only or v a combination of pressure head and soil moisture pressure head for the saturated zone and soil moisture for the unsaturated zone these methodologies are evaluated in synthetic experiments which are performed for different climate conditions soil types and plant functional types to simulate various root zone soil moisture distributions and groundwater levels the results demonstrate that enkf cannot properly handle strongly skewed pressure distributions which are caused by extreme negative pressure heads in the unsaturated zone during dry periods this problem can only be alleviated by methodology iii iv and v the last approach gives the best results and avoids unphysical updates related to strongly skewed pressure heads in the unsaturated zone if groundwater level data are assimilated by methodology iii enkf fails to update the state vector containing the soil moisture values if for almost all the realizations the observation does not bring significant new information synthetic experiments for the joint assimilation of groundwater levels and surface soil moisture support methodology v and show great potential for improving the representation of root zone soil moisture keywords root zone soil moisture integrated model data assimilation groundwater level surface soil moisture 1 introduction precise knowledge of spatial and temporal variations in root zone soil moisture content is important for agriculture weather prediction and drought monitoring lue et al 2011 however information about root zone soil moisture is usually limited due to the small volume support of point scale measurements grayson and western 1998 and the high level of uncertainty associated with land surface model predictions which is caused by uncertain input parameters and atmospheric forcings houser et al 2001 remote sensing techniques allow monitoring of surface soil moisture content over large areas but these measurements are limited to the top few centimeters of the soil and are not reliable for dense vegetation du et al 2000 in recent years some work has been carried out to improve the prediction of root zone soil moisture content using data assimilation techniques ground based surface soil moisture measurements or remotely sensed surface soil moisture data have been combined with land surface models which simulate surface and root zone soil moisture content to update soil moisture estimates for all soil layers walker et al 2002 walker et al 2003 sabater et al 2007 crow et al 2008 das et al 2008 kumar et al 2009 pipunic et al 2011 ford et al 2014 han et al 2014 de lannoy and reichle 2016 kumar et al 2009 concluded that the potential of surface soil moisture assimilation to improve root zone soil moisture characterization was higher when the surface root zone coupling was stronger walker et al 2002 showed that the soil moisture profile cannot be retrieved from near surface soil moisture measurements when the near surface and deep soil layers become decoupled such as during extreme drying events das et al 2008 found that the assimilation of remotely sensed surface soil moisture content improved soil moisture content estimation for shallow soil 0 0 50 m but hardly for the deeper soil layers 0 50 3 86 m frod et al 2014 demonstrated that the relationship between near surface 5 10 cm and root zone 25 60 cm soil moisture was generally strong pipunic et al 2011 stated that there was no clear evidence of an ability to strongly improve soil moisture content prediction for deeper layers using only surface information in summary the updating of root zone soil moisture by combining surface soil moisture from satellite and land surface model predictions via data assimilation is a way forward but often improvement is only limited groundwater effects on root zone soil moisture are either neglected or not explicitly treated in most land surface models chen and hu 2004 kollet and maxwell 2008 a shallow groundwater table is more likely to lead to surface runoff because of saturation excess and to supply water to the atmosphere at the rate demanded by the atmosphere on the other hand deeper groundwater tables generally indicate drier areas in this situation surface runoff is likely to be generated by the infiltration excess runoff mechanism the groundwater is recharged when the infiltration is enhanced and evaporation is more often limited by the available soil moisture under both conditions soil moisture content is changed through its interaction with the groundwater liang and guo 2003 miguez macho et al 2008 miguez macho and fan 2012 zhu et al 2013 recently a number of modeling platforms with varying complexity which couple groundwater land surface and atmospheric models have emerged aiming to improve the estimation of water flow in the unsaturated zone kollet and maxwell 2008 maxwell 2009 davin et al 2011 tian et al 2012 maxwell et al 2015 for example the hirham regional climate model is coupled with the mike she hydrological model larsen et al 2014 the common land model is integrated into parflow parflow clm maxwell and miller 2005 kollet and maxwell 2008 another example is the atmosphere surface subsurface terrestrial system modeling platform terrsysmp shrestha et al 2014 terrsysmp and parflow clm have been used in a number of simulation studies like the investigation of the role of topography and subsurface heterogeneity on the interactions between land surface energy fluxes and groundwater dynamics rihani et al 2010 the role of topography and subsurface heterogeneity on spatial soil moisture variability and discharge gebler et al 2017 the impact of subsurface hydrodynamics on the lower atmosphere shrestha et al 2014 the influence of crop specific physiological properties on the partitioning of land surface energy fluxes and the resulting modifications in the heat and moisture budgets of the atmospheric boundary layer sulis et al 2015 and the influence of groundwater dynamics on land surface processes net radiation latent heat flux sensible heat flux and ground heat flux kollet and maxwell 2008 information of groundwater level is available in many places and at regional and national scales as it can be continuously measured by automatic sensing devices at low cost and high accuracy like the collaborative national groundwater monitoring network ngwmn program for the united states subcommittee on ground water 2011 as mentioned above near surface soil moisture content hardly provides information about root zone soil moisture content and groundwater levels may contain valuable information about the unsaturated zone integrated models like terrsysmp are expected to be suited to explore in an optimal way the information provided by groundwater levels as well as near surface soil moisture content data using sequential data assimilation techniques like ensemble kalman filter evensen 1994 burgers et al 1998 enkf has been applied successfully in unsaturated flow and groundwater flow problems the contributions relevant in the context of this paper are the assimilation of water table depth or piezometric head chen and zhang 2006 franssen and kinzelbach 2008 li et al 2012 kurtz et al 2014 shi et al 2014 soil moisture content margulis et al 2002 reichle et al 2002 montzka et al 2013 erdal et al 2014 shi et al 2014 pasetto et al 2015 brightness temperature margulis et al 2002 crow and wood 2003 sabater et al 2007 das et al 2008 han et al 2014 and surface radiometric temperature crow et al 2008 however there are few studies focusing on the assimilation of groundwater levels to improve the characterization of the unsaturated zone zhang et al 2016 assimilated both groundwater head and soil moisture content with ensemble transform kalman filter etkf in the mike she hydrological model and emphasized the necessity of using localization when assimilating both groundwater head and soil moisture rasmussen et al 2015 also used etkf to assimilate groundwater head and stream discharge to study the relationship between the filter performance and the ensemble size shi et al 2014 2015b and shi et al 2015a employed enkf to investigate the impacts of multivariate hydrological data assimilation including groundwater levels on estimating unsaturated flow parameters with synthetic experiments and real world experiments in our study we focus on the comparison of different methodologies for the assimilation of groundwater levels in integrated hydrological models in addition the methodological developments made in this work aim for high dimensional problems and spatially heterogeneous fields of hydraulic parameters if groundwater levels are assimilated in terms of pressure head the strongly skewed and non gaussian pressure distributions for dry soils cause problems for sequential data assimilation algorithms as these algorithms perform optimally only for gaussian distributions the water retention function maps the very negative values to a very small range of water content values and is highly nonlinear therefore if enkf is applied in a standard form the nonlinear state propagation would cause large errors erdal et al 2015 discussed the importance of state transformations in enkf when dealing with strong nonlinearities for unsaturated flow modeling in this work a complication is that groundwater level data although often showing a correlation with soil moisture content are in general only weakly correlated with soil moisture content of the upper soil layers this raises the question what is the sphere of influence of groundwater level data and whether the complete unsaturated zone should be updated with groundwater level data in summary two problems to be solved for assimilating groundwater level data in integrated hydrological models are 1 strongly skewed pressure distributions under dry conditions 2 detection of the sphere of influence of groundwater level data in the context of cross compartmental data assimilation in order to test different methodologies for groundwater level and surface soil moisture assimilation in the integrated model terrsysmp a series of synthetic experiments was carried out in this work the remainder of this paper is organized as follows in section 2 we briefly review the terrsysmp model used to simulate the soil moisture and subsurface pressure distributions as well as energy exchange fluxes between the land and the atmosphere the land surface model clm and subsurface model parflow are described separately assimilation methodologies with the ensemble kalman filter enkf are also explained in this section section 3 then introduces our experiment setup and in section 4 the main findings of our assimilation study are presented discussion and conclusion are presented in sections 5 and 6 respectively 2 materials and methods in this study we perform data assimilation experiments with a coupled subsurface land surface model in order to illustrate the technical setup for these experiments section 2 1 summarizes the utilized models section 2 2 explains the implementation of data assimilation methodologies for this study and introduces our data assimilation platform 2 1 terrestrial system modelling platform terrsysmp terrsysmp shrestha et al 2014 is a modular scale consistent terrestrial system model composed of well established models for the atmosphere cosmo baldauf et al 2011 the land surface clm oleson et al 2008 and the subsurface parflow ashby and falgout 1996 jones and woodward 2001 kollet and maxwell 2008 its modularity makes it possible to run in fully coupled way cosmo clm parflow partly coupled way clm parflow clm cosmo and uncoupled way by running each model individually cosmo clm parflow in our work the partly coupled parflow clm terrsysmp configuration is used the land surface component of terrsysmp clm calculates the mass and energy balance at the land surface including evaporation from the ground surface transpiration from plants sensible and ground heat fluxes and freeze thaw processes oleson et al 2008 shrestha et al 2014 10 soil layers are internally defined with a total extend of about 3 m there is no lateral flow between grid cells because water and energy flows are calculated only in the vertical direction clm input includes atmospheric forcing data i e precipitation wind speed incoming shortwave and longwave radiation air pressure air temperature and humidity land surface data including information on plant functional type pft and adjustable parameters and physical constants clm uses soil properties like soil texture and bulk density in combination with model internal pedotransfer functions to derive soil hydraulic and thermal parameters like saturated hydraulic conductivity oleson et al 2004 oleson et al 2008 the subsurface part of terrsysmp consists of the subsurface flow model parflow in parflow a 2d distributed overland flow simulator is implemented into a 3d variably saturated groundwater flow kollet and maxwell 2006 the overland flow boundary condition helps parflow to simulate fully coupled surface and subsurface flow via a mixed form of the 3d richards equation and the kinematic wave equation parflow solves for water pressure at every time step and calculates the saturation field from which soil moisture and water table depth are determined by the pressure saturation relationship for example according the van genuchten formulation van genuchten 1980 more details about the model equations the discretization and the numerical implementation can be found in ashby and falgout 1996 jones and woodward 2001 kollet and maxwell 2006 parflow input includes soil hydraulic parameters like saturated hydraulic conductivity and residual soil water content van genuchten parameters like α and n and soil porosity data from clm and parflow are exchanged via the coupling software oasis mct valcke 2013 which is a library providing a generic interface to exchange information between standalone executable codes in memory originally clm only takes into account 1d vertical flow in the unsaturated zone with a decoupled surface routing as the upper boundary condition and a simple parameterization of groundwater table as a lower boundary condition parflow overcomes these limitations with an overland flow boundary condition and 3d variable saturated groundwater flow parflow provides clm with its calculated subsurface pressure ψ and saturation sw values for the 10 uppermost subsurface layers and in return clm provides the upper boundary condition for parflow which consists of the soil infiltration values qinf which are calculated based on the land surface fluxes of clm precipitation interception total evaporation total transpiration thus the 1d column soil moisture prediction of clm is replaced by the 3d formulation in parflow the information exchange between clm and parflow is shown schematically in fig 1 the thicknesses of the 10 uppermost subsurface layers should be consistent with those of the 10 soil layers in clm and also the porosities in both models need to be the same 2 2 data assimilation methodologies 2 2 1 ensemble kalman filter enkf in our study enkf is used in combination with the fully coupled model terrsysmp to update soil moisture content and groundwater levels the observation vector for the jth ensemble member at time step t is 1 y j t y t ɛ j t where y t is the vector with measurements at time step t εj t is a vector with perturbation terms drawn from a normal distribution n 0 σ where the standard deviation σ is equal to the expected measurement error for each ensemble member j at time step t the state vector xj t is updated by observations and model predictions 2 x j t a x j t f k t y j t hx j t f where the superscripts a and f refer to the updated state vector the analysis and the model predicted state vector respectively h is the observation operator which represents the relationship between the observation vector y and the state vector x its elements are 1 or 0 where 1 corresponds to the measurement location if measurement location and grid cell center coincide exactly the kalman gain k t is calculated by 3 k t c h t hc h t r 1 where r is the measurement error covariance matrix c is the covariance matrix which is given by 4 c 1 n 1 j 1 n x j t f x t f x j t f x t f t where n is the number of ensemble members and x t f is a vector containing ensemble mean values for the state vectors at time step t in our work the observation vector y and the state vector x include pressure and or soil moisture as mentioned above the correlations between pressure and soil moisture are needed to update pressure with soil moisture data the van genuchten model gives the relation between soil moisture and pressure 5 ψ θ s θ r θ ψ θ r 1 m 1 1 n α where θs is the saturated soil moisture content which is equal to porosity ϕ θr is residual soil moisture conten α is a measure of the first moment of the pore size density function n is an inverse measure of the second moment of the pore size density function and m 1 1 n enkf needs to be tested carefully when pressure head is the state variable in our experiment in the unsaturated zone the richard s equation is solved for pressure head and pressure head can vary between strongly negative values in case of dry soil and positive values in the aquifer and depends on the soil hydraulic parameters and climate conditions fredlund 1991 for example when the infiltration rate exceeds the saturated hydraulic conductivity ponding occurs and pressure head becomes positive at the top of the domain contrarily during a long drying period pressure head can become strongly negative because of evapotranspiration in data assimilation stochastic realizations may show a large spread of pressure head values because of varying hydraulic parameters between the realizations so that the ensemble may show a strongly skewed probability density function of pressure this strong skewness can generate unrealistically large updates in the enkf analysis step because the ensemble mean is largely influenced by few realizations with extreme values in addition enkf performs optimally for gaussian distributions but not for strongly skewed distributions erdal et al 2014 mitigated this issue by limiting the negative pressure head to 10 m but this is not an optimal solution for arid regions they also proposed two other methods to avoid very negative pressure heads in the unsaturated zone that are normal score ns transformation and pressure head to water content pwc transformation with a soil water retention function erdal et al 2015 both methods improved the state distribution as well as the assimilation results however ns transformation may distort the spatial corrections and in pwc transformation all pressure heads above or below a certain limit are mapped to the same water content we are dealing with the same issue as groundwater pressure head is assimilated to improve the pressure head distribution for the whole domain including saturated zone and unsaturated zone in our work a modular high performance data assimilation framework terrsysmp pdaf kurtz et al 2016 which couples terrsysmp and the pre existing parallel data assimilation library pdaf parallel data assimilation framework nerger and hiller 2013 is used details about technical implementation installing and running terrsysmp pdaf can be found in kurtz et al 2016 currently terrsysmp pdaf can assimilate pressure and or soil moisture data in parflow with enkf 2 2 2 assimilation methodologies in this work we are concerned with groundwater level data which can be continuously measured at high accuracy and low cost with automatic sensing devices in groundwater wells we propose two ways to introduce the groundwater level data into the model terrsysmp fig 2 illustrates how observed groundwater level is translated into observation data two different variants are possible first at locations with groundwater level measurements the pressure head in the saturated zone is calculated assuming hydrostatic conditions in the aquifer pressure head in the saturated zone is then assumed to be the observation of our assimilation system second soil moisture content in the saturated zone should be equal to porosity and then soil moisture data porosity data is assimilated based on these two observation strategies in total five methodologies are tested and compared to assimilate groundwater level data in terrsysmp to improve the predictions of root zone soil moisture 1 assimilation of pressure p here the observation is pressure head in the saturated zone at locations with groundwater level measurements the observation error of pressure head is assumed 0 01 m the state vector includes the pressure head for the whole domain including the saturated and unsaturated zones 2 assimilation of log transformed pressure data p log to reduce problems with non gaussianity in method 1 pressure head is converted into log space as log10 100 pressure to improve the enkf performance to avoid taking log of zero or negative values pressure head is subtracted from 100 m for both the measurement data and simulated values before calculating the kalman gain the observation error in log space is 4 10 5 log m which is based on the random error of 0 01 m for pressure head we calculated the standard deviation of log10 100 perturbed pressure where perturbed pressure was generated by adding a random error of 0 01 m the state vector includes the pressure head in log space for the whole domain including the saturated and unsaturated zones after the analysis step the log pressure head is back transformed 3 assimilation of soil moisture sm here the observation is soil moisture porosity in the saturated zone at locations with groundwater level measurements the observation error is 0 02 m3 m3 in enkf the state vector includes the soil moisture for the whole domain including saturated zone and unsaturated zone after the update with enkf the state vector needs to be converted to pressure head for the whole domain on the basis of the van genuchten model as pressure head is the prognostic variable in parflow 4 assimilation of pressure in saturated zone p mask this method is similar to method 1 but for each ensemble member grid cells in the unsaturated zone are not updated in the data assimilation procedure unrealistic updates for the unsaturated zone caused by extreme values in the ensemble are avoided but the disadvantage is that the groundwater level information is not directly used to update soil moisture contents in the unsaturated zone 5 assimilation of both pressure and soil moisture mix in this approach groundwater level measurements are assimilated as pressure head data with an observation error of 0 01 m however the state vector includes now two parts pressure head for the saturated zone and soil moisture for the unsaturated zone because of different parameters the ensemble members will show different vertical divisions between the unsaturated and saturated zone in the data assimilation step the definition of the state vector should be consistent for all ensemble members this implies that a given grid cell should be updated for all ensemble members in the same way either in terms of soil moisture or pressure head the distinction between the saturated and unsaturated zone is firstly defined by the deepest water table depth among the ensemble members then for grid columns with groundwater level measurements we apply a correction of this depth in case the groundwater level of the observation is higher than the one of the ensemble grid cells above this depth are updated in terms of soil moisture and grid cells below this depth in terms of pressure head after defining the state vector and defining which elements are updated in terms of pressure and which elements in terms of soil moisture the pressure head observations are assimilated via enkf in this mixed approach also joint assimilation of surface soil moisture content and groundwater level was tested where observations include both groundwater level data and surface soil moisture with measurement errors equal to 0 01 m and 0 02 m3 m3 respectively in the methodologies sm and mix after data assimilation with enkf updated soil moisture is transformed to pressure values according the van genuchten model eq 5 because pressure head is parflow s prognostic variable and soil moisture or saturation is a derived quantity which is not directly used as a state variable for the next time step kurtz et al 2016 before the transformation we check if soil moisture stays within physical boundaries between residual soil moisture content and porosity if updated soil moisture content is less than residual soil moisture we set updated soil moisture residual soil moisture 0 01 porosity and do the transformation for this updated value if updated soil moisture content is larger than porosity no transformation is performed which implies that pressure head in this grid layer is not updated and the calculated pressure value before the data assimilation step with enkf is used finally if updated soil moisture content is in the range between residual soil moisture content and porosity transformation is done directly the updated pressure head in the domain either directly updated by enkf or indirectly updated by soil moisture is used for the next time step 3 experiment setup in order to test the five assimilation methodologies with terrsysmp pdaf we define a synthetic experiment which is carried out for the land surface subsurface clm parflow components of terrsysmp a synthetic reference run provides pressure head or soil moisture observations and the ensemble needed for data assimilation experiments is generated by perturbing subsurface parameters 3 1 reference model the synthetic domain has a horizontal extension of 2000 m 2000 m and is discretized horizontally into 2 2 grid cells with a grid cell size of 1000 m 1000 m the domain has a vertical extension of 30 m which is discretized into 30 soil layers with variable thickness the uppermost 10 layers are with layer thicknesses exponentially increasing with depth these ten layers extend in total over 3 m and in these ten layers fluxes are exchanged between clm and parflow the deeper 20 subsurface layers have a constant thickness of 1 35 m the slope is 1 in both x and y directions for all grid cells hillslope thus there is topographically driven overland flow and groundwater flow in parflow soil texture meteorological conditions and pfts are homogeneous over the domain the bottom boundary and boundaries on the side are impermeable overland flow can be generated over the upper boundary kollet and maxwell 2006 where ponded water is routed along the land surface and allowed to exit the domain at the bottom of the hill in order to produce different land surface fluxes and groundwater dynamics we use two forcing datasets from a semi arid climate and a humid climate site data for the semi arid picassent site in spain are from a local meteorological station and data for the humid kennedy space center site within the merritt island in florida usa are from the ameriflux network bracho et al 2008 for each site climate data from two years are used one year is for model spin up and the other year is for the model simulation and assimilation fig 3 shows the daily accumulated precipitation average air temperature and average incoming shortwave radiation for the two climate sites three soil types with different soil hydraulic parameters are used variably saturated flow is parameterised with the van genuchten model spatially constant values of residual soil moisture content θr the saturated hydraulic conductivity ks the van genuchten parameters α and n are chosen from leij et al 1996 see table 1 to keep hydraulic consistence between clm and parflow porosity θs for both models is calculated on the basis of the sand fraction via the following pedotransfer function in clm 6 θ s 0 489 0 00126 sand three plant functional types pfts are used in the simulations namely bare soil shrubs grasland short roots and broadleaf tree deeper roots bare soil shrub and evergreen broadleaf tree are specified for simulations for the picassent site and bare soil grassland and evergreen broadleaf tree for the florida site the vegetation properties are characterized by distinct physiological pft specific parameters in clm oleson et al 2004 each configuration of the atmospheric forcings soil texture and vegetation type is initialized with a water table depth of 1 m relative to the land surface then terrsysmp is run for 100 years forced repeatedly with one year of atmospheric data for all the simulation scenarios in this work 100 years of spin up is enough to reach hydrologic and dynamic equilibrium conditions after the model has reached equilibrium the model is forced by another year of atmospheric data for the simulation and assimilation experiments 3 2 setup of data assimilation experiments two climate types three soil texture types and three pfts were combined to generate 18 terrsysmp configurations these configurations generate different land surface fluxes and groundwater dynamics for each configuration groundwater dynamics and land surface fluxes also differ between the four hillslope grid cells only one grid cell is observed the results for the highest situated grid cell and the grid cell at medium elevation are analyzed resulting in 36 2 climate types 3 soil types 3 vegetation types 2 grid cells along slope synthetic scenarios with different groundwater levels for the data assimilation experiments a synthetic reference run is created for each scenario mentioned above the synthetic runs use the true hydraulic parameters shown in table 1 and provide virtual observation data of pressure head or soil moisture as specified in section 2 2 2 although groundwater level data are assimilated we refer to them in terms of pressure head or soil moisture data below the synthetic reference simulations are run for 1 year with an hourly time step for both parflow and clm the synthetic reference runs also provide the true values of root zone soil moisture the five assimilation methodologies are evaluated by comparing the assimilation results of root zone soil moisture and their corresponding true values for the assimilation experiments an ensemble of 128 realizations of log10 ks is created for each scenario individual log10 ks realisations are generated by perturbing the reference value of log10 ks see table 1 with an additive random perturbation which is sampled from the standard normal distribution n 0 1 the spin up is done for each ensemble member individually for 100 years using the different ks values as input the ensemble is used in addition for an open loop run and data assimilation experiments observation data from the reference run groundwater level data in the form of pressure head or soil moisture are assimilated on a daily basis for the observation grid point the root zone soil moisture is updated directly or indirectly by assimilating observation data the model uncertainty from the physical parametrizations in the model is ignored in this study 4 results groundwater level assimilation experiments for all the scenarios mentioned above are performed with the five assimilation methodologies and joint assimilation experiments of groundwater level and surface soil moisture with method 5 described in section 2 2 2 some representative results are shown below to illustrate the possibility and potential of the assimilation strategies under different groundwater dynamics and compare the performance of the five methodologies to evaluate the performances of the different assimilation strategies methodologies the root mean square error rmse of soil moisture from the surface to the depth of 3 m the 10 upper soil layers which is assumed to be the root zone is calculated as 7 rms e j 1 t t 1 t θ t j sim θ t j true 2 where θ t sim is the ensemble mean soil moisture at time step t θ t true the true soil moisture at time step t j the jth soil layer and t is the number of time steps the weighted average of rmse for these 10 layers is also calculated 8 rmse j 1 10 ω j rms e j where ω j is the weight for the jth soil layer which is calculated by the depth of the jth soil layer dj 9 ω j d j j 1 10 d j the deeper layers get larger weights in correspondence to their larger vertical extend 4 1 assimilation of groundwater levels in this section only groundwater level in the form of pressure head or soil moisture is assimilated in terrsysmp pdaf the scenario name is specified by the soil type climate site plant type and grid top means the highest grid cell and medium the grid cell with medium elevation as in this work the soil moisture observation error is 0 02 m3 m3 we assume that if the rmse value of root zone soil moisture from the open loop run is less than 0 02 m3 m3 the model performs well the scatterplots in fig 4 illustrate that if the water table depth is very deep 10 m the open loop rmse values are below 0 02 m3 m3 when the water table is very deep root zone soil moisture content is low for all realizations in the open loop run so that the difference between the ensemble mean and the reference is small on the other hand in some scenarios when the water table is very shallow close to surface all realizations have very wet root zones which also result in low rmse values 0 02 m3 m3 notice that in the simulation experiments the meteorological forcings and porosity were known we analyze the results of the assimilation experiments for some scenarios with with open loop rmse values above 0 02 m3 m3 and water table depth between 5 m to 0 m where data assimilation is necessary for improving the model performance 4 1 1 aggregated results fig 5 shows the scatter plots of weighted average of rmse values for the open loops versus weighted average of rmse values for the five data assimilation methodologies for the different scenarios with open loop rmse values above 0 02 m3 m3 the 1 1 line corresponds to equal weighted averages of rmse for the open loop and data assimilation runs points which are situated below the 1 1 line indicate that assimilation of groundwater level improves soil moisture characterization compared to the open loop from the figure we can see that methodologies p mask sm and mix work well for most scenarios especially the methodology mix which results for all the scenarios in weighted average rmse values smaller than 0 02 m3 m3 after assimilation root zone soil moisture characterization is greatly improved compared to the open loop for this scenario p mask also improves the root zone soil moisture characterization for all the scenarios but the improvement is not as significant as for mix for a few scenarios the methodology sm performs bad methodology p shows the worst performance and p log works a little better than p for these last two methodologies for many scenarios soil moisture characterization is worse after assimilation more details are shown for the scenario loam florida broadleaf top fig 6 shows the ensemble spread of groundwater levels as a function of time for the open loop and the five assimilation methodologies for this scenario during the assimilation table 2 shows the corresponding rmse values of soil moisture for the upper 10 soil layers calculated by eq 7 and weighted average of rmse values calculated by eq 8 in this scenario see table 2 all the five methodologies improve the soil moisture profile estimation after assimilation compared with the open loop run the improvement for the upper layers is limited for the methodologies p and p log when compared with the other 3 methodologies and the deeper layers are greatly improved by all the methodologies from fig 6 we can see that all methodologies improve the ensemble towards the reference groundwater level and reduce the ensemble variance in particular the ensemble converges quickly to the reference in the methodologies sm and mix which produce the best results according to table 2 there are small spikes in the ensemble for the methodologies p and p log during the summer period which might affect their performances in the below section performances of the five assimilation methodologies are analyzed and compared with some representative scenarios 4 1 2 comparisons of the five assimilation methodologies the performances of the methodologies p and p log are strongly dependent on the water content distribution in the unsaturated zone under some conditions like a long dry period picassent site in spain high potential et or erroneous hydraulic parameters ks the pressure head ensemble shows a strongly skewed distribution in the unsaturated zone which cannot be properly handled by enkf when most realizations calculate a groundwater table which is higher than the reference value the methodology sm also has a problem as the variance calculated at the observation point is near zero below some representative scenarios will be shown to discuss these issues in detail we analyze now the simulation results for the spanish site which is more affected by very dry periods and therefore very dry topsoil conditions we see in fig 7 and table 3 that the simulation performance is now more strongly affected by the dry conditions which is visible in the form of high spikes in the plots for the methodologies p and p log scenario sandyloam spain baresoil medium from table 3 we can see that the methodologies p and p log improve the soil moisture estimations below the reference water table but worsen the soil moisture estimations above the reference water table the other three methodologies perform well above the groundwater table especially the methodology mix fig 7 shows that after about five months of assimilation the reference water table is below almost all the calculated water table depths in the individual realizations for methodologies p and p log related to this these methodologies perform well in the saturated zone in the unsaturated zone the bad performance of methodologies p and p log is related to the artificial large spread of the strongly skewed pressure head distribution resulting from the large updates in enkf in fig 7 there are small spikes in the ensemble for the p and p log methods during the summer period in the summer period soil profiles get drier so the pressure head near the surface becomes more negative some ensemble members with improper ks values may generate very negative pressure heads which result in strongly skewed non gaussian local probability density functions of pressure head as shown in fig 8 a and 8 c which illustrates that before the update there are a few isolated realizations with very negative pressure values while most realizations are close to the synthetic truth the update drags the other realizations to the extreme realizations and therefore to more negative pressure values see fig 8 b and 8 d some realizations are updated to high positive pressure heads these positive pressure heads make no sense in the model from a physical point of view and are the consequence of the artificial large spread of the skewed head distribution resulting from the enkf updates these unwanted positive pressure head updates result in the water table depth spikes in the methodologies p and p log in fig 7 and bad performance for the upper layers in table 3 the figure also shows that the scenario p log strongly reduces the outliers compared to the scenario p but is not able to fully eliminate them in the fig 7 the methodology p mask shows a much slower convergence than the other methodologies in the p mask methodology only the grid cells in the saturated zone are updated in the data assimilation procedure unrealistic updates for the unsaturated zone caused by extreme values in the ensemble are avoided soil moisture contents in the unsaturated zone are indirectly updated as a result of updating groundwater levels by data assimilation and the action of the model equations which leads to its slower convergence in some scenarios methodology sm also worsens the soil moisture estimations in the unsaturated zone while mix still works very well see fig 9 and table 4 for the scenario sandyloam spain shrub medium in methodology sm after two months of assimilation all realizations are above the reference water table see fig 9 b as a consequence y hx in eq 2 is very small for the soil moisture observations taken at the layer nodes as the ensemble spread is close to zero there all realizations are saturated or nearly saturated and if y hx is very small the update in eq 2 is also small therefore the data assimilation according to methodology sm is not able to correct the erroneous water table depth resulting in bad performance statistics as indicated in table 4 for the scenario loamysand spain baresoil medium in methodology sm even though the reference water table is covered by the ensemble spread during assimilation see fig 9 e sm still produces bad results in the unsaturated zone see table 4 in the model the pressure head and soil moisture are defined for the centers of layers nodes but the groundwater table can be between those layer centers see fig 2 in the methodology sm soil moisture porosity for the layer nodes below the reference water table are observations layer node i to the bottom layer node in fig 2 for this scenario in the subfig 9 e the blue line shows the layer node depth of the uppermost saturated layer node layer node i in fig 2 with observation and we can see that the layer nodes which have the observation are also saturated for all realizations which causes the same problem as the previous scenario from fig 9 and table 4 we can see that in these two scenarios methodology mix still performs very well with a fast convergence of the water table depth ensemble towards the true value and greatly reduced rmse values compared to the open loop simulation 4 2 joint assimilation of groundwater levels and surface soil moisture in section 4 1 we evaluate the five assimilation methodologies for assimilating groundwater level data in the integrated subsurface land surface model terrsysmp from the results above we find that the methodology mix works the best now we test this methodology to jointly assimilate groundwater levels and surface soil moisture joint ssm gwl with more realistic experimental settings when jointly assimilating groundwater levels and surface soil moisture the state vector of mix methodology still includes pressure for the saturated zone and soil moisture for the unsaturated zone the observation vector includes also the top surface soil moisture 0 01 m below the soil surface besides pressure head measurements in the saturated zone the soil moisture observation is taken at 0 01 m in order to mimic remote sensing observations in addition the experiments presented in this section mimic more realistic conditions vertically heterogeneous saturated hydraulic conductivity ks and van genuchten parameters α and n are used as input for the data assimilation experiments a synthetic reference run is created for each scenario the synthetic true ks value for the top surface layer is taken from table 1 saturated hydraulic conductivity decreases exponentially with soil depth niu et al 2005 10 k s z i k s z 1 e f z i z 1 i 1 10 k s z i k s z 10 i 11 20 where zi is the soil depth of the ith layer z 1 is the depth of the top surface layer ks z1 is the saturated hydraulic conductivity of the top surface layer and f is the decay factor which indicates the decrease of ks with soil depth beven 1997 f is set to 2 0 m 1 niu et al 2005 the remaining ks values for the deeper 20 layers use the same values as the 10th layer the true van genuchten parameters α and n for each soil layer are generated based on the ks values by sampling a multivariate gaussian distribution described in carsel and parrish 1988 for each soil type carsel and parrish 1988 used four kinds of transformation on ks α and n to get a multi normal distribution among the three parameters no transformation no lognormal ln log ratio sb and hyperbolic arcsine su details about the transformation for each soil type can be found in carsel and parrish 1988 this procedure takes correlations among the three parameters into account table 5 shows the covariance among the three transformed parameters for loam and loamy sand residual soil moisture content θr and porosity θs are deterministic and spatially constant see table 1 please note that ks α and n are vertically heterogeneous but with spatially homogeneous values for the different soil layers 2 5 dimensional other settings are similar as outlined in section 3 2 the synthetic true hydraulic parameters are used as model input to provide virtual measurement data of pressure head and soil moisture in the experiments only loamy sand and loam are tested in combination with the 3 pfts and 2 climate datasets for the assimilation experiments an ensemble of 128 realizations of ks α and n is created as outlined above precipitation is also perturbed in the data assimilation experiments with multiplicative noise sampled from a uniform distribution with values between 0 5 and 1 5 to further investigate the performance of joint ssm gwl we also assimilate surface soil moisture alone ssm for all the scenarios as is typical done in many land surface data assimilation studies and assimilate groundwater level alone gwl fig 10 shows the scatter plots of open loop rmse values versus gwl ssm and joint ssm gwl rmse values for all the scenarios for the upper surface layers depth 0 01 m 0 035 m and 0 075 m ssm and joint ssm gwl improve the soil moisture estimation compared with open loop run because for most cases their blue and red dots are below the 1 1 line but many green dots of gwl distribute above the 1 1 line which indicates that gwl assimilation does not significantly improve surface soil moisture estimation this is reasonable because ssm and joint ssm gwl include surface soil moisture observations to constrain the soil moisture content for the upper layers for the intermediate soil layers depth 0 135 m 0 235 m and 0 4 m joint ssm gwl still works well while the blue dots of ssm are scattered around the 1 1 line and the green dots of gwl move towards an area slightly below the 1 1 line indicating improved performance in other words the performance of ssm gets worse while gwl performs better for the deeper soil layers depth 0 65 m 1 05 m 1 65 m and 2 5 m ssm is not able to improve soil moisture characterization compared to the open loop runs while gwl and joint ssm gwl significantly improve the soil moisture characterization therefore we can conclude that assimilation of surface soil moisture alone can improve the soil moisture characterization for the upper 10 20 cm but it may even worsen the soil moisture estimation for deep layers similarly assimilation of groundwater level alone can improve soil moisture characterization for layers deeper than 40 cm but may have negative effect on soil moisture characterization for surface layers joint assimilation of surface soil moisture and groundwater level is able to integrate the advantages of the two observation types the number of dots in the figure is smaller for deeper layers because in some scenarios the water table depths are shallower than 2 5 m so that both open loop rmse values and assimilation rmse values beneath the water table are very close to zero as all ensemble members reproduce saturated conditions there fig 11 compares the soil moisture results for the reference open loop and the data assimilation scenario joint ssm gwl for loamysand spain shrub medium data assimilation results in an ensemble closer to the reference compared with the open loop run the ensemble does not suffer from variance underestimation in the top four layers soil moisture content is low with some strong fluctuations related to few precipitation events and strong potential evapotranspiration the groundwater level data are able to correct most ensemble members for the deeper soil layers 5 discussion this study compares five data assimilation methodologies for improving root zone soil moisture estimation with groundwater level data or a combination of groundwater level and surface soil moisture data these five assimilation methodologies and assimilation of groundwater level and or surface soil moisture data are evaluated and compared on the basis of 36 synthetic scenarios the problem associated with the implicit gaussian assumption in the enkf for updating pressure heads in the unsaturated zone is illustrated the results of methodologies p and p log suffer from the extreme pressure heads which drag the ensemble mean of model states far away from the truth this problem happens particularly in dry periods when some realizations with extreme ks values result in very negative pressure head values at near surface layers one important aspect in enkf is the ensemble size in our study a size of 128 realizations is used which is a small size compared to groundwater data assimilation studies however erdal et al 2015 tested a very large ensemble size of 1000 members for the problem of highly skewed distributions they pointed out that a larger ensemble size would rather increase the risk for the strong negative pressure heads since these originate from initially sampled extreme parameter values for a large ensemble it is more likely that a few extreme parameter values are sampled furthermore the extreme state values still have a major effect on the ensemble mean even though the ensemble size is larger an alternative data assimilation method which could be used here is the particle filter method which could also help to avoid extreme state values the particle filter can deal with non linear and non gaussian problems but usually requires large numbers of model evaluations to get a reliable result if one can afford the cost of using the particle filter method it is obvious that the skewed pressure distribution would not pose such a problem as in the enkf for the small synthetic examples shown here it would be possible to apply the particle filter however most real cases involve many more unknowns and a much higher dimensionality under those conditions the particle filter is expected to need an extremely large ensemble size erdal et al 2015 also discussed if the most common ad hoc methods would have any positive effect on the performance with such a strongly skewed pressure distribution they concluded that dampening the state update only has limited effect as it cannot remove the extreme realizations which still have an unwanted but dampened impact they concluded that also localization would not improve the performance significantly groundwater level information which is assimilated in the form of soil moisture data works well in most scenarios but there are some exceptions when the simulated water tables from all or most realizations are above the reference water table increasing the ensemble size could solve this specific problem but it is unclear whether this would work in all scenarios for example in real world cases with bias where the simulated water table depths are systematically closer to the surface than the observed water table depth correction is difficult in addition a larger ensemble size also implies of course increased computational costs from section 4 1 we find that the methodology mix which updates model states in terms of pressure for the saturated zone and in terms of soil moisture for the unsaturated zone outperforms the other methods and results in a very good root zone soil moisture characterization this might be related to the simple setting in our experiments where only the saturated hydraulic conductivity ks is perturbed the van genuchten parameters α and n and porosity which link the pressure head and soil moisture in the unsaturated zone are constant while this setting is relatively simple the focus is on developing assimilation methodologies with terrsysmp to improve the estimation of root zone soil moisture by assimilating groundwater levels additional experiments section 4 2 where α and n are also perturbed together with ks further verified the feasibility of the methodology mix there will be additional challenges for the application of the methodology in real world situations the van genuchten model is used to transform the soil moisture to pressure head in the unsaturated zone after enkf in real world cases the relation between soil moisture and pressure head might even not be described correctly by the van genuchten model in addition if the updated soil moisture state is porosity from unsaturated to saturated conditions no back transform is made for this grid cell the model predicted pressure head value prior to assimilation is used in this case strange pressure profiles might be generated as the states of the surrounding grid cells were either directly updated in terms of pressure saturated zone or pressure was indirectly updated via soil moisture unsaturated zone it is expected that in the model simulation for the next time step the pressure profile is partly smoothed again in our work we test joint assimilation of groundwater levels and surface soil moisture with 2 5 dimensional synthetic experiments which is proven to be a promising way to improve the estimation of root zone soil moisture surface soil moisture is easily available temporally and spatially from satellites like smap and smos groundwater level measurements are also potentially numerous and measured at low cost and high accuracy although currently large scale groundwater level data are not systematically stored in databases taking full advantage of these two types of data helps to better understand the soil water profile various studies use data assimilation techniques to combine satellite data with land surface models to update the complete soil moisture profile including the root zone generally in those works the land surface models only consider the exchange of water and energy between atmosphere land surface and vegetation but water and heat fluxes from the deeper subsurface are neglected the integrated model terrsysmp which couples the land surface and subsurface makes it possible to take advantage of the surface soil moisture information from satellite and groundwater levels from observation networks or satellite grace to improve the soil moisture estimation in the vadose zone by data assimilation we impose uncertainties with respect to soil hydraulic parameters ks α and n and model forcings precipitation in the synthetic joint assimilation study in reality these parameters and model forcings especially precipitation are always impacted by a large uncertainty especially for large scale applications it is difficult to obtain information about soil hydraulic and thermal parameter values and their statistics beyond sporadic point measurements it is important to better constrain these parameters which is possible by sequential data assimilation using joint state parameter estimation or inverse modelling in future work we will test the joint estimation of states and soil hydraulic parameters by jointly assimilating surface soil moisture and groundwater level information as this work focused on defining the best data assimilation strategy for data assimilation problems for coupled land surface subsurface models model structural errors were neglected this provides favorable conditions for our synthetic experiments model structural errors could be important in practice examples are neglecting preferential flow in the unsaturated zone or the limited number of plant functional types to cover all vegetation types it is therefore important in future work to evaluate this data assimilation methodology for real world land surface subsurface data assimilation problems nevertheless we think that the parflow clm is less affected by model structure errors than the clm stand alone model as parflow eliminates some of the deficiencies of clm for example concerning the role of groundwater 6 conclusions in this work we develop and compare five data assimilation methodologies to improve root zone soil moisture estimation using groundwater level or joint groundwater level and surface soil moisture data in an integrated model terrsysmp the performance of the five data assimilation methodologies is evaluated for 36 synthetic scenarios under different groundwater level dynamics observations of groundwater levels are assimilated in the form of pressure head data or soil moisture data in terrsysmp to update pressure heads and or soil moisture in the whole domain the results for assimilation of pressure or log transformed pressure show that it can be problematic to apply enkf which performs optimally for gaussian distributions to scenarios with strongly non gaussian state distributions caused by extreme negative pressure heads in the unsaturated zone the results from assimilation of groundwater level data in the form of soil moisture data i e for the aquifer part soil moisture content is set equal to the porosity show that this strategy often gives satisfying results but under certain conditions if nearly all ensemble members have a simulated groundwater depth closer to the surface than the measured groundwater level updating of the states in the unsaturated zone does not work the best methodology is assimilating groundwater level data to update the pressure head in the saturated zone and soil moisture content in the unsaturated zone this method alleviates the impact of skewed pressure distributions on the enkf analysis step joint assimilation of surface soil moisture and groundwater levels also shows great potential in improving the root zone soil moisture profile in synthetic scenarios with both uncertain soil hydraulic parameters and meteorological forcings it is important to extend this work to include estimation of unknown soil and aquifer hydraulic parameters and test in large scale real world applications acknowledgments the authors gratefully acknowledge the computing time granted by the jara hpc vergabegremium on the supercomputer jureca jülich supercomputing centre 2016 at forschungszentrum jülich this work was supported by tr32 patterns in soil vegetation atmosphere systems monitoring modelling and data assimilation further support was provided by the dfg forschergruppe 2131 data assimilation for improved characterization of fluxes across compartmental interfaces this work was financed by a stipend from the china government 
903,in the simulation of flooding events mesh refinement is often required to capture local bathymetric features and or to detail areas of interest however if an explicit finite volume scheme is adopted the presence of small cells in the domain can restrict the allowable time step due to the stability condition thus reducing the computational efficiency with the aim of overcoming this problem the paper proposes the application of a local time stepping lts strategy to a gpu accelerated 2d shallow water numerical model able to handle non uniform structured meshes the algorithm is specifically designed to exploit the computational capability of gpus minimizing the overheads associated with the lts implementation the results of theoretical and field scale test cases show that the lts model guarantees appreciable reductions in the execution time compared to the traditional global time stepping strategy without compromising the solution accuracy keywords gpu local time stepping shallow water equations finite volume flood simulation parallel computing 1 introduction the heavy economic and human losses caused by flood events in recent years barredo 2009 demand the adoption of specific flood risk management strategies which include not only structural defense systems such as levees but also resilient policies from this point of view the numerical modeling of flood scenarios can be very helpful both for the definition of the most effective technical interventions and for the design of flood hazard and flood risk maps necessary for flood risk management planning as requested by the european council 2007 most free surface flows such as tsunamis river floods and dam break events can be simulated through the two dimensional 2d shallow water equations swe which can be discretized by different numerical methods e g alcrudo and garcia navarro 1993 horritt and bates 2002 vacondio et al 2012 costabile et al 2012 2015 among these finite volume fv schemes have the advantage of accurately describing transcritical flows and shock type discontinuities toro 1999 and can even be applied to flows over irregular bathymetries provided that the scheme is associated with a specific treatment of wetting and drying fronts e g bradford and sanders 2002 brufau et al 2004 liang and borthwick 2009 and of bed and friction slope source terms which should guarantee that the c property is satisfied e g bermudez and vazquez 1994 greenberg and leroux 1996 garcia navarro and vazquez 2000 rogers et al 2003 audusse et al 2004 aureli et al 2008 liang and marche 2009 vacondio et al 2013 the high computational cost entailed by the application of 2d swe models to field scale simulations with high resolution meshes encouraged researchers to investigate parallelization techniques on supercomputers e g sanders et al 2010 moreover the computational capability of graphics processing units gpus has been recently exploited to carry out high performance computing on personal workstations and gpu accelerated swe models are now developed by a number of authors both for structured lastra et al 2009 de la asuncion et al 2013 brodtkorb et al 2012 vacondio et al 2014 2017 ferrari et al 2017 and unstructured meshes castro et al 2011 lacasta et al 2014 petaccia et al 2016 results of these works show that significant reductions in computational time up to two orders of magnitude compared with serial codes can be achieved in many practical applications local mesh refinement is often necessary due to the presence of complex bathymetric features or specific areas of interest and some gpu enhanced models able to handle non uniform resolution have been presented in literature sætra et al 2015 vacondio et al 2017 explicit numerical schemes are often preferred to implicit ones as shown by teng et al 2017 due to their suitability for describing rapidly varying flows but also to the possibility of designing efficient parallel models exploiting modern heterogeneous computing architectures however the time step size used to update the solution with explicit schemes is computed based on the courant friedrichs lewy cfl stability condition and is generally dictated by the minimum grid size neglecting the possible high variability in wave celerity values all cells in the domain are advanced with the common minimum time step this strategy is usually referred to as global time stepping gts if only a small region is refined the other part of the domain with a coarser grid size is updated with a much smaller time step than its maximum admissible for stability thereby increasing the required computational effort in order to overcome this problem osher and sanders 1983 introduced the local time stepping lts strategy based on the idea of advancing each cell with its own time step closer to its maximum allowable lts schemes were successfully applied to swe models crossley and wright 2005 sanders 2008 kesserwani and liang 2015 dazzi et al 2016 and appeared to be beneficial for enhancing their computational performances compared to standard gts schemes without degrading the solution accuracy to the best of the authors knowledge the only attempt to apply a lts scheme in a gpu accelerated model can be attributed to sætra et al 2015 who focused on an adaptive mesh refinement amr technique this paper presents an efficient gpu implementation of a first and second order accurate fv scheme with a lts strategy which solves the 2d swes on a non uniform structured grid the present model is based on an existing gpu model vacondio et al 2014 2017 which has been thoroughly tested and validated even for the simulation of real flooding events vacondio et al 2016 compared to previous lts techniques the algorithm presented in this work has been derived and coded in such a way that it can exploit the computational capabilities of gpus the performances of the gts and lts schemes are assessed based on three theoretical and two real field test cases the paper is structured as follows section 2 briefly describes the original gts model in section 3 the new lts scheme and its gpu implementation are detailed then the model performances are evaluated through the simulation of five test cases in section 4 conclusions are drawn in section 5 2 global time stepping scheme the original gts numerical model is described in detail by vacondio et al 2014 2017 here only the most important features are briefly recalled the governing equations are the 2d swes written in integral form toro 2001 1 t a u d a c h n d c a s 0 s f d a where t is the time a is the area of the integration volume c is the volume boundary u is the vector of conserved variables h f g is the tensor of fluxes in the x and y directions n is the outward unit vector normal to c s 0 and s f are the bed and friction slope source terms respectively the modified form of the swes which guarantees that the scheme is well balanced is adopted following liang and marche 2009 2a u η u h v h f u h u 2 h 1 2 g η 2 2 η z u v h g v h u v h v 2 h 1 2 g η 2 2 η z 2b s 0 0 g η z x g η z y s f 0 g h n f 2 u u 2 v 2 h 4 3 g h n f 2 v u 2 v 2 h 4 3 in eq 2 h is the flow depth z is the bed elevation above datum and η h z is the water surface elevation above datum u and v are the velocity components along the x and y directions respectively g is the acceleration due to gravity and nf is manning s roughness coefficient an explicit fv scheme is used to discretize the equations both first order and second order accurate approximations in space and time are implemented the first order approximation exploits the following equation to update the conserved variables in time 3 u i j n 1 u i j n δ t δ x f i 1 2 j f i 1 2 j δ t δ y g i j 1 2 g i j 1 2 δ t s 0 s f subscripts i j represent the cell position while superscript n refers to the time level δx and δy are the cell dimensions in the x and y directions respectively and δt is the time step size computed according to the cfl stability condition toro 2001 4a δ t i j 1 2 c r min δ x u i j g h i j δ y v i j g h i j 4b δ t min i j δ t i j where cr is the courant number 1 notice that according to the gts scheme each cell is updated with the same time step δt equal to the minimum value in the whole domain second order accuracy in space is achieved by means of a depth positive muscl extrapolation at cell boundaries audusse et al 2004 adopting the minmod slope limiter second order accuracy in time is obtained by applying the second order runge kutta method 5 u i j n 1 u i j n 0 5 δ t d i u i j n d i u i j n 1 2 where u i j n 1 2 is obtained as 6 u i j n 1 2 u i j n δ t d i u i j n and the operator d i u i j is defined as 7 d i u i j 1 δ x f i 1 2 j f i 1 2 j 1 δ y g i j 1 2 g i j 1 2 s 0 s f fluxes are computed using the hllc approximate riemann solver toro 1999 together with the correction proposed by kurganov and petrova 2007 which avoids non physical velocity values at wet dry fronts the slope source term is discretized using a centered approximation vacondio et al 2014 while the friction source term is discretized using the implicit formulation proposed by caleffi et al 2003 the cuda c implementation of the model exploits the intrinsic parallelization of computations on gpu devices thus guaranteeing fast execution times compared to serial codes the basic work unit in cuda is the thread and many threads are organized into a block in the present model each thread corresponds to one cell used to discretize the computational domain and each block consists of k k cells in the first implementation of this model vacondio et al 2014 the domain was discretized by means of a cartesian grid information about neighboring cells blocks is easily retrieved thanks to the correspondence between the physical position of a cell block and its position on the two dimensional array where data concerning that cell block are stored in the memory recently the model has been extended to the case of a new type of non uniform structured grids called block uniform quadtree buq grids vacondio et al 2017 in this case the domain is still partitioned into blocks each containing k k cells with uniform spatial resolution within the block but cells belonging to different blocks can be characterized by a different grid size in particular starting from blocks with the maximum resolution level level 0 which contain cells with size δx 0 δxmin each level l contains cells with size δxl 2 l δxmin up to the minimum resolution level l where cells have size δxmax 2 l δxmin in this work l is assumed equal to 3 neighboring blocks can differ by one resolution level at most the idea is similar to standard quad tree partitioning liang and borthwick 2009 but each node of the quad tree mesh corresponds to a block instead of a single cell operatively the user must specify seeding points with assigned spatial resolution level as input to the simulation and the model automatically generates the multi resolution grid according to the procedure described by vacondio et al 2017 memory allocation of blocks is not pre established as in the case of cartesian grid hence some additional data must be stored in order to retrieve information about neighboring cells belonging to different blocks during the computations in particular when adjacent cells have a different spatial resolution level the natural neighboring interpolation procedure liang 2011 is employed to reconstruct the conserved variables in the neighboring cells 3 local time stepping scheme the key idea of the lts strategy is to advance each cell with a time step as close as possible to its maximum permissible for stability most papers concerning lts e g kleb et al 1992 crossley and wright 2005 krámer and józsa 2007 sanders 2008 dazzi et al 2016 assume the local time step size in each cell as an integer multiple in particular a power of two multiple of a common reference time step δt equal to the minimum in the whole domain the procedure usually starts with the application of eq 4a for each cell in order to compute its own allowable time step and of eq 4b in order to find the global minimum time step a temporal resolution level m 0 1 2 is assigned to each cell based on the condition 2 m δt δti j 2 m 1 δt the coarsest level in the domain m is also computed then the update procedure begins all cells belonging to level m will be updated via 2 m m steps of size 2 m δt for example cells with m 0 will be advanced for 2 m steps of size δt while cells with m m will only perform one step of size δtmax 2 m δt this sequence of intermediate updates for each cell leads to a synchronized solution in the whole domain then the procedure starts again the lts algorithm presented in this paper is generally based on the same idea with a major difference it is assumed that all cells belonging to a block which already share the same mesh size in the buq grid also share the same local time step while different blocks may be updated with different time steps hence similarly to the spatial resolution level l 0 1 l of each block it is possible to define the temporal resolution level m 0 1 m of each block notice that while the spatial resolution is assigned by the grid generation procedure in the pre processing stage and remains unchanged during the simulation the temporal resolution levels need to be re computed before each sequence of partial updates because flow conditions and admissible time step sizes can change throughout the domain during the simulation the choice of adopting the same time step for all cells belonging to a block may appear to be inefficient in the lts framework because in the worst case only one cell may actually need to be advanced at such a slow pace however this block based design offers a simple and well performing parallel code which fully exploits the parallel architecture of gpus it is worth recalling that the best performances on gpus are obtained when cells belonging to a block are characterized by uniform features so that special treatments are required only for cells on the border of a block and code branching is minimized ultimately this lts implementation represents a natural extension of the spatial multi resolution model and introduces limited coding efforts and overheads in the following sub sections the main aspects of the lts scheme are outlined section 3 1 deals with the definition of active blocks with the assignment of a temporal resolution level to each block and with the m level distribution regularization in section 3 2 the core updating procedure for different time resolution blocks is covered while section 3 3 describes the rollback procedure needed in cases where the cfl condition is violated finally section 3 4 reports on the expected advantages and overheads for a lts block based simulation on gpus 3 1 temporal resolution level assignment to blocks this section describes the procedures required before the asynchronous time advancement of blocks may start notice that when these operations are performed all cells are synchronized to the common time level tn the procedure for the temporal resolution assignment to each block starts with the execution of a kernel i e a cuda piece of code executed on the gpu which computes the allowable δti j in each cell and determines the minimum δtb of each block b then the cpu calculates the minimum δt in the whole domain 8 δ t mi n b δ t b and assigns an initial guess m level to each block as follows 9 m b int log 2 δ t b δ t it is worth recalling that in the original gts model a block deactivation optimization bdo was introduced in order to increase model efficiency vacondio et al 2014 thanks to this procedure only active blocks are processed during each time step a block is defined as active if it contains at least one wet cell or one cell which may become wet at the end of the current time step the kernel that computes the allowable time step also determines whether a block is wet then wettable blocks are added to the set of active blocks by cpu processing based on the simple idea that a wet dry front may propagate at most over one cell per time step hence a dry block may become wet if one of its neighboring blocks has at least one wet cell on the confining border for lts this condition is not sufficient since up to 2 m steps may be performed before re computing the set of active blocks hence all dry blocks that are neighbors to wet blocks are activated as shown in fig 1 where wet blocks and wettable blocks are active while dry blocks are not processed during the current iteration since the set of active blocks is not redefined for a few steps it is strongly recommended to impose an upper bound to m which should be limited to fulfil the condition 2 m k if the block consists of k k cells for example if 8 8 cell blocks are employed m should be limited to 3 so that 8 steps may be performed at most and that the wet dry front may not propagate over more than the 8 cells of the neighbor block obviously for wettable blocks it is impossible to compute δtb because depth and velocity are null in each cell therefore each wettable block is temporarily assigned the following m level 10 m w e t t a b l e b l o c k min n w b m n w b where nwb indicates a neighboring wet block as an example let us consider the case of block j in fig 1 its two wet neighbors are characterized by m levels equal to 1 and 2 block j will then be given mj 1 exception to this rule is the case of a wettable block with a finer spatial resolution than the one of its wet neighbors in this case its temporal resolution level is refined too as an example block k in fig 1 whose wet neighbors are coarser and are characterized by m 1 is assigned a temporal resolution level equal to 0 many authors e g crossley and wright 2005 krámer and józsa 2007 suggest that neighboring cells or blocks in this case should differ by one temporal resolution level at most in order to increase the robustness of the scheme this also helps to develop an algorithm able to guarantee mass and momentum conservation when no source terms are present at interfaces between cells with different m levels while minimizing code branching and thus maintaining sufficient efficiency on gpus for this reason after defining the set of active blocks a regularization procedure to correct the initial guess m levels is performed see algorithm 1 iterations start from blocks with the maximum temporal resolution level m 0 for each block belonging to the current level the temporal resolution of all neighboring blocks is checked and whenever the difference between the m levels of the current and neighbor block exceeds one the neighbor block is assigned a reduced m level an example of the application of this procedure is shown in fig 2 a b which depict a few blocks of a buq grid with the corresponding temporal resolution level after the initial guess m level assignment fig 2a which obviously depends on the local grid size and flow variables in this example two instances of blocks with one neighbor indicated by a black arrow whose m level must be reduced see fig 2b can be identified moreover crossley and wright 2005 analyzed how waves propagate in neighboring cells characterized by different temporal resolution levels and concluded that a buffer region needs to be defined in order to guarantee the propagation of waves travelling from maximum to minimum temporal resolution level regions in the present model the buffer region consists of a buffer block whose m level is artificially re assigned according to algorithm 2 in summary each m level region is broadened with one more line of buffer blocks the allowable time step in these blocks is in theory double than the one actually used during the computations however the local artificial temporal resolution refinement ensures the correct propagation of waves and wet dry fronts an example of the m level reassignment to buffer blocks is represented in fig 2c 3 2 lts update procedure algorithm 3 compares the gts and lts scheme pseudocodes excluding input and output procedures at the beginning of each iteration the time step is computed and active blocks are defined in the lts procedure m levels are also properly assigned as already described in section 3 1 then the update procedure may start in the gts version of the model the solution is advanced by a single time step in the whole domain and the iterations over time continue in the lts version of the scheme instead if m is the maximum temporal resolution level in the domain a series of intermediate updates which advances the solution from time level tn to time level tn δtmax where δtmax 2 m δt via 2 m steps starts let us consider the first order version of the scheme in this case muscl extrapolation is not performed and the left and right values required for computing fluxes at cell edges coincide with the conserved variables at cell centres the solution in each cell is advanced via eq 3 with the local time step characterizing the current block at each step s 0 1 2 m 1 only blocks that fulfil a specific criterion are processed all cells belonging to block b with level m are updated with a time step equal to δtb 2 m δt if s is an integer multiple of 2 m the procedure is sketched in fig 3 for a simple one dimensional 1d case with six blocks and m 2 a special treatment is necessary for cells on the border of a buffer block whose neighboring cells are characterized by a different temporal resolution level although some authors e g kesserwani and liang 2015 saetra et al 2015 adopt temporal interpolation procedures at these interfaces which in turn require corrections to guarantee mass and momentum conservation the present scheme adopts an intrinsically conservative strategy to handle these interfaces figure 4 shows two cells belonging to a buffer block with m 0 and two cells on its neighboring block with m 1 without loss of generality a 1d representation with cells characterized by the same mesh size and only two temporal resolution levels is chosen for simplicity during the first step s 0 all cells are updated and fluxes are computed based on the conserved variables values at time tn as usual during the second step s 1 however only cells belonging to the block with m 0 must be advanced in time while inner cell i 1 has both west and east neighbor values available for flux computation at time tn 1 border cell i lacks a synchronized value to the east for this reason on its eastern edge which coincides with the block border values at time tn are reused for the flux computation in this way the sum of the eastern fluxes of cell i computed at times tn and tn 1 which are then multiplied by δt and the western flux of cell i 1 at tn which is multiplied by 2δt are the same and the scheme is conservative notice that on the western edge of cell i flux computation is still performed based on values at tn 1 this strategy is possible because arrays for fluxes are not allocated in the present model and the computation of the same flux at the edge between two cells is repeated twice once for each thread for efficiency reasons as discussed by vacondio et al 2014 hence the only drawback of this procedure is the necessity of allocating an additional array in memory where conserved variables from the previous time step in cells on the block boundaries are stored and retrieved when necessary the detailed implementation neglecting gpu parallelization is reported in algorithm 4 which corresponds to lines 7 13 of algorithm 3b the second order version of the scheme is presented next in this case two sub steps are performed during each time advancement and two kernels are launched during each of these two sub steps one which executes the muscl extrapolation and one which performs flux computation and time integration finally an additional kernel sums the contributions retrieved from the two half time steps according to eq 5 in the lts implementation of the scheme each pair of sub steps of the second order scheme can be compared to two consecutive steps of the first order scheme provided that appropriate criteria are defined in order to distinguish whether blocks belonging to each m level are to be processed during the current sub step time advancement from tn to tn δtmax is schematized in algorithm 5 which substitutes lines 7 13 of algorithm 3b fig 5 depicts three blocks with different m levels m 2 specifying the step 0 1 2 3 and sub step a b at which each block is processed according to algorithm 5 fig 5a represents muscl extrapolation while figure 5b shows flux computation and time integration cells lying on the border of buffer blocks are updated asymmetrically similarly to the first order scheme at sub steps for which the synchronized neighboring values are missing muscl extrapolation is skipped in these cells on the border block edge extrapolated values at the previous sub step are reused for flux computation on the inner edge accuracy is locally reduced to first order in space e g values at the cell center are used for flux computation in order to avoid interpolations results presented in the following section show that this simplification does not significantly impair the accuracy of the solution finally a short remark about open boundary conditions bcs needs to be made for each open bc discharge rating curve and or water level the minimum allowable temporal resolution level mbc is assigned to all blocks affected by the given open bc the specific cuda kernel which assigns the prescribed quantities discharge water elevation rating curve at open bcs cells is executed only at steps for which blocks with m mbc are processed for details about bcs handling in the model the reader is referred to vacondio et al 2014 from a practical point of view this is not a relevant feature in the lts model because the number of the cells to which an open boundary conditions is assigned is usually negligible compared to the total number of cells used to discretize the domain 3 3 roll back procedure according to algorithm 3b the time step computation and the m level assignment lines 2 6 are performed before the inner lts loop lines 7 13 which advances the solution from tn to tn δtmax this means that the temporal resolution level hence the time step size is maintained fixed for a few steps however during the computations flow field conditions vary in some cases velocity and or celerity might increase and the time step can no longer fulfil the cfl condition generating instabilities for this reason a procedure to roll back simulation time is implemented in the present model an additional array needs to be allocated where conserved variables are stored at time tn after each partial time integration the cfl condition is re evaluated in each cell based on the updated values of conserved variables assuming cr 1 if this check highlights potential instabilities somewhere in the domain the lts loop is stopped and partial computations already performed are discarded then the loop begins again starting from the values stored in the rollback array and assuming a halved time step size everywhere the prevention of potential simulation crashing compensates for the slight overload induced by this procedure 3 4 efficiency of the lts algorithm on the gpu a comparison between the computational efficiency of the gts and lts schemes can be performed by evaluating the execution time ratio su defined as the ratio of the gts execution time to the lts execution time for each simulation this factor strongly depends on the test case under consideration and on the grid refinement and the maximum expected time ratio for a given test case can be theoretically estimated by analyzing the amount of computational operations associated with the two versions of the code for this analysis let us distinguish between the dt piece of code lines 2 3 in algorithm 3a for gts or 2 6 in algorithm 3b for lts and the update piece of code lines 4 5 in algorithm 3a for gts or 7 14 in algorithm 3b for lts which are substituted by algorithm 5 for the second order scheme preliminary investigations show that in the gts version of the code the dt stage takes roughly 30 of the total simulation time for the first order scheme and 15 for the second order scheme accordingly the update stage takes roughly 70 and 85 of the total execution time for the first and second order schemes respectively as regards the dt stage it can be observed that in the lts version of the scheme the time step computation and the bdo procedure are performed only once every 2 m steps while the gts scheme repeats these operations at every time step therefore the lts version is in theory 2 m times faster than the gts scheme in performing the dt stage even if the additional processing for the m level assignment see section 3 1 reduces the actual time ratio for this part of the code on the other hand the most important computational saving of the lts scheme is associated with the update stage since a subset of blocks is processed fewer times in the lts code than they would be in the gts scheme an estimate of the achievable theoretical time ratio for the update piece of code can be quantified by dividing the total number of cells blocks processed in the gts simulation by the same value in the lts simulation assuming that the update operations performed on each block require roughly the same computational time kleb et al 1992 in the example depicted in fig 3 fourteen update operations are performed instead of twenty four which results in a theoretical time ratio for the update stage only equal to 1 7 however this simple computation does not take into account the overheads associated with the lts scheme first of all a few operations must be performed inside each kernel of time advancement in order to distinguish whether a block must be processed during the current time step in addition to this managing interfaces between blocks with different temporal resolution levels requires code branching which may slow down computations on the gpu the whole roll back procedure described in section 3 3 is also added to the code all these overheads lead to an actual time ratio smaller than the theoretical one in summary the theoretically achievable time ratio can be evaluated by combining the contributions from the dt and update stages as follows 11 s u t h e o r 1 2 m t d t 1 n m 0 m n m 2 m t u p d a t e 1 where nm is the number of cells or blocks belonging to each m level n is the total number of cells or blocks in the domain and t d t and t u p d a t e are the execution times for the dt and update stages normalized to the total execution time considering the example depicted in fig 3 m 2 the theoretical time ratio is equal to 2 05 for the first order scheme assuming t d t 0 3 and t u p d a t e 0 7 and to 1 86 for the second order scheme t d t 0 15 and t u p d a t e 0 85 finally an additional aspect should be considered in the assessment of the computational efficiency of the model on gpus tasks on different blocks are executed in parallel by the different processors the hardware architecture consists of if the number of active blocks is larger than the available cores blocks need to be queued by the gpu s scheduler while when simulations with a smaller number of active blocks are performed the computational capabilities of the gpu may not be fully exploited in this latter case the increased model efficiency due to parallel computations on the gpu is somehow impaired by the cpu gpu communication overheads see vacondio et al 2017 hence the typical scalability of gpus i e the computational time normalized to the total number of cells decreases with increasing the number of processed cells and remains almost constant after reaching a threshold value which depends on the gpu type should also be taken into account when evaluating the computational efficiency of both gts and lts schemes in any case a quantitative analysis of the overheads and savings of the lts scheme is discussed for the first two test cases in section 4 4 numerical tests in this section a comparison between the performances of the original gts and the novel lts model implementation is performed based on three theoretical test cases and on two field scale practical applications all simulations were run using a k40 tesla gpu for all test cases the courant number was assumed equal to 0 8 moreover the buq grid was formed by 8 8 cell blocks hence the temporal resolution level was limited to m 3 for all simulations 4 1 vortex test case the first numerical experiment considers the steady state test case of a vortex flow with analytical solution sanders and bradford 2006 the vortex circulates clockwise on a flat frictionless bottom and its motion can be described by means of the following relations assuming that the coordinate system origin coincides with the vortex center 12a h h 0 u 0 2 4 g 1 2 r r 0 e x p 2 r r 0 e x p 2 r r 0 12b u u 0 y r 0 e x p r r 0 v u 0 x r 0 e x p r r 0 where r x 2 y 2 is the distance from the vortex center and the values attributed to the other parameters are as follows h 0 10 m u 0 1 5 m s r 0 100 m the domain is extended up to r 3000 m the maximum resolution δxmin is imposed in the area close to the vortex center approximatively up to r 500 m surrounded by a smooth transition up to the minimum resolution δxmax in the outer region of the domain a detail of the resulting multi resolution grid is shown in fig 6 which represents each 8 8 cell block as a square element the temporal resolution level for the lts simulation is dictated by the grid size for this test case thus the area characterized by the maximum temporal resolution level m 0 coincides with the finest grid size region plus the circle of neighboring blocks with halved grid size required by the numerical scheme as a buffer region to ensure correct wave propagation the same holds for the areas with coarser grid size the m level distribution is also represented in fig 6 where different colors identify blocks with homogeneous temporal resolution level sixteen simulations were run assuming four different test configurations see table 1 each performed with both models lts and gts and with both the first and the second order accurate version of the scheme minimum and maximum grid sizes were in the range 1 8 m and 8 64 m respectively the analytical solution was imposed as initial condition and the simulation was run for a physical time in the range 500 4000 s in order to ensure that the same number of update operations was performed for all test configurations as an example figs 7 and 8 compare the results of the gts and lts simulations performed with the second order accurate scheme for test configuration b fig 7 reports the contour maps of water depth and velocity magnitude u u 2 v 2 while fig 8 shows the profiles of the same variables along the y x line a quantitative comparison regarding the agreement of numerical results with the exact solution can be performed by computing the non dimensional l 2 error norms of water depth and velocity components 13 l 2 h 1 n i 1 n h i num h i exact h 0 2 where n is the number of cells in the domain and subscripts num and exact refer to the numerical and analytical solutions the same expression can be used to calculate l 2 u and l 2 v by adopting u 0 as reference value for normalization the l 2 error norms computed for the sixteen simulations are reported in table 1 as expected errors increase with the grid size for both first and second order simulations and second order error norms are at least one order of magnitude smaller than first order ones for all configurations no differences can be appreciated between lts and gts water depth error norms which remain in the range 10 5 10 6 close to machine precision for second order simulations while velocity error norms reach the order of magnitude 10 4 10 3 for the coarsest grid sizes and show small differences between lts and gts values for first order simulations lts and gts error norms practically coincide finally table 1 reports the execution time ratios for all test configurations time ratios achieved by the first order scheme reaching 1 53 are always slightly larger than the ones obtained from the second order scheme which are limited to 1 44 the best performances can be observed for tests with the finest grid size even if the time ratio is not expected to depend much on the grid size and number of cells due to the steadiness of the test case in fact the achievable theoretical time ratio is approximately the same for all configurations and is equal to 1 86 and to 1 6 for the first and second order schemes respectively a detailed analysis regarding the computational times associated with different parts of the code is reported for configuration a δxmin 1 m the theoretical time ratios for the dt and update stages are equal to 8 and 1 4 respectively fig 9 shows the execution times of the two parts of the code for first and second order schemes both gts and lts values are normalized to the total execution time of the gts second order simulation which is the longest one in the first order scheme the time ratio associated with the update piece of code is 1 26 this means that the overhead associated with lts branching in the update kernels is only 10 for this simulation on the other hand the time ratio for the dt stage is equal to 3 31 and the overhead associated with the additional operations performed for the m levels assignment is widely compensated by the fact that this piece of code is executed only once every eight steps the combination of the computational savings associated with the two parts of the code results in an actual time ratio equal to 1 53 therefore the lts overheads increase the total simulation time by roughly 18 compared to the theoretically achievable value for this test for the second order scheme due to the heavier update operations the dt piece of code is computationally less relevant than for the first order one in fact while both first and second order schemes require approximately the same execution time for the dt operations the update stage for the second order scheme takes more than twice the time required for the first order scheme this is true for both gts and lts due to the larger number of executed kernels for this reason despite similar partial time ratios the total execution time ratio for the second order scheme equal to 1 44 is slightly smaller than the value obtained from first order simulations and the lts overheads can be estimated to be only 10 of the total execution time 4 2 circular dam break the model was then tested by simulating the classical wet circular dam break problem liska and wendroff 1997 the domain 25 25 m 25 25 m is characterized by a horizontal frictionless bottom a 10 m high cylindrical water column with radius r 10 m is centered in the domain and is surrounded by 1 m deep still water due to the cylindrical symmetry of the problem a reference solution can be obtained by deriving the inhomogeneous 1d system of the swe in radial geometry toro 2001 and solving it with a very fine mesh δx 0 005 m the grid for the 2d lts and gts simulations was generated by forcing the maximum resolution δxmin 0 025 m alongside the initial discontinuity 9 m r 11 5 m and by imposing the halved mesh size 0 05 m in the regions 6 5 m r 9 m and 11 5 m r 14 5 m the automatic grid generation procedure created the remaining transitions up to the minimum resolution δxmax 0 2 m fig 10 represents a detail of the resulting mesh which consists of 0 465 106 cells simulations were run until the physical time t 1 s fig 11 compares the water depth profiles along the radial direction at t 0 4 s obtained by the lts and gts simulations with the reference solution results from both first and second order schemes agree well with the pseudo exact solution the latter showing a closer agreement as expected notice that even if the shock wave is propagating over a region with m 1 see the temporal resolution level distribution at t 0 4 s in fig 10 the lts solution accuracy is not degraded compared to gts results on the contrary dimensional l 2 error norms for water depth and velocity magnitude reported in table 2 show that the lts model provides even slightly more accurate results than gts in particular as regards the first order scheme also for this test case the execution time ratio reported in table 2 is slightly larger for the first order scheme su 1 49 than for the second order scheme su 1 39 the achievable theoretical values are equal to 1 81 and 1 56 for the first and second order schemes respectively in particular 8 for the dt stage and 1 36 for the update stage as can be inferred from fig 12 the update time ratio is equal to 1 22 for the first order simulation and to 1 28 for the second order one confirming the 6 10 overhead induced by the lts implementation within this piece of code which reduces the achieved time ratio compared to the theoretical one similarly to the previous test case the execution times for the dt operations show that the overhead induced by the procedure for m level assignment in the lts scheme is largely compensated by the computational savings due to the fewer dt executions globally the lts overhead is 10 18 for this test case too 4 3 thacker test case the lts scheme was further validated by simulating the periodic oscillation of a water volume in a frictionless paraboloidic basin thacker 1981 which involves wetting drying and non flat topographies the bottom can be described by means of the following equation 14 z z 0 x 2 y 2 l 2 1 where z 0 is the depth of the vertex and l is the radius at z 0 see fig 13 a the water volume initially at rest and paraboloidic expands and contracts periodically due to gravity the exact solution for this test case is 15 η x y t z 0 1 a 2 1 a cos ω t 1 x 2 y 2 l 2 1 a 2 1 a cos ω t 2 1 u x y t 1 1 a cos ω t 1 2 ω xa sin ω t v x y t 1 1 a cos ω t 1 2 ω ya sin ω t where ω and a are defined as 16 ω 8 g z 0 l a η 0 z 0 2 z 0 2 η 0 z 0 2 z 0 2 the test parameters are set as follows z 0 50 m η 0 10 m l 1000 m which correspond to a period of oscillation equal to 50 s the domain is extended up to r 1500 m the maximum resolution δxmin 1 m is assigned to the area subject to wetting drying 850 m r 1100 m and near the paraboloid center r 30 m while the grid generation procedure automatically creates transitions up to δxmax 8 m the final mesh fig 14 consists of 2 106 cells and the simulation time includes four complete oscillations tfinal 400 s fig 13b presents a comparison between analytical and numerical results for the second order scheme in particular slices of water surface elevation along the x axis at selected times are depicted gts and lts profiles practically coincide confirming the accuracy of the scheme even in the presence of wetting drying and bottom slope source terms dimensional l 2 error norms reported in table 3 for t 250 s show that lts results are even slightly better than gts the obtained time ratios reported in table 3 equal to 1 47 and 1 32 for first and second order simulations respectively are slightly smaller than the corresponding theoretical values estimated to be equal to 1 70 and to 1 43 due to the overheads introduced in the lts scheme as already discussed for the previous test cases the same case was also simulated with friction in order to assess the lts scheme accuracy when both source terms are present for this case no analytical solution is available therefore lts results are only compared with gts manning s roughness coefficient is set equal to 0 15 m 1 3s fig 15 reports the water surface elevation trend in time at point 0 0 0 0 for both frictionless and non frictionless simulations while oscillation amplitude and frequency remain constant in the former case in the latter case water level oscillations are dampened as expected again lts and gts results are almost overlapping in this case time ratios are equal to 1 74 and 1 65 for the first and second order simulations respectively in contrast with theoretical values of 2 11 and 1 86 4 4 parma baganza test case for field scale test cases computational savings are expected to be considerable when the lts scheme is used to simulate a large domain where only a small region is discretized with a very fine mesh this might be the case of the presence of bridge piers in a riverbed which require a high level of detail the baganza river northern italy which is crossed by a bridge 500 m upstream of its confluence in the parma river was considered for this test case the bridge is characterized by four 3 10 m round nosed piers the bathymetry is shown in fig 16 the 3 4 km long final branch of the baganza river was modelled together with the 2 km and 1 km long branches of the parma river upstream and downstream of the confluence respectively the non uniform buq grid is characterized by δxmin 0 25 m imposed only at the bridge site which gradually transitions to δxmax 2 m in the remainder of the domain as shown in the insert in fig 16 the total number of cells is equal to 0 536 106 as upstream boundary condition a discharge hydrograph is assigned at the inflow of the baganza river the initial value is 100 m3 s which is gradually increased to 300 m3 s within one hour the simulation is then prolonged for three more hours similarly a discharge hydrograph with initial and final values equal to 200 and 400 m3 s is imposed as upstream boundary condition for the parma river downstream a constant water depth is assigned initial conditions are obtained from a preliminary steady state simulation with the initial discharge values manning s roughness coefficient is assumed equal to 0 04 m 1 3s table 4 reports the execution time ratios for both first and second order simulations values of approximately 2 7 confirm the achievable reduction in execution time due to the adoption of the lts version of the model in this case the theoretical values are equal to 3 83 for the first order scheme and to 3 45 for the second order scheme for real test cases with boundary conditions and wet dry fronts code branching probably enhances the overheads associated with the lts scheme and the total overhead 22 27 is larger than the one obtained from the simulation of the theoretical test cases previously analyzed 10 18 in addition to the greater computational efficiency the choice of the lts scheme does not degrade the quality of results compared to gts the root mean square error rmse between gts and lts water surface elevations when steady state conditions were achieved was also computed 17 rmse 1 n i 1 n η i gts η i lts 2 and was observed to be almost negligible see table 4 finally as an example of results fig 17 a b show the water surface elevation and velocity maps around one of the bridge piers obtained from the second order lts simulation the adoption of a mesh with local high resolution allows predicting the local 2d flow field in detail in particular the stagnation points upstream of the pier and the recirculation region downstream can be resolved for comparison fig 17c depicts the velocity field for a simulation performed without imposing a local grid refinement in this case the pier geometry is only roughly described and the velocity field is not accurately captured this justifies the choice of a locally refined mesh when the flow field near the bridge is of interest which makes the adoption of a lts scheme particularly convenient 4 5 flooding scenario due to levee breaching finally the achievable performance improvement of the lts scheme over the gts scheme was assessed based on the simulation of the flooding scenario induced by a hypothetical levee breach on the secchia river italy the 750 km2 domain reported in fig 18 a includes a 43 km long branch of the river from modena to concordia and the floodable area on its right bank stretching to the panaro river the buq grid used for the simulations was characterized by δxmin 5 m and δxmax 40 m the maximum resolution was forced near the inflow and outflow boundary conditions and around the breach location a grid size δx 10 m was imposed along the remainder of the river the resulting grid size distribution along the river is shown in fig 18b besides the whole floodable area was discretized at the minimum resolution hence its representation is avoided in fig 18b the domain includes roughly 1 106 cells the 20 years return period discharge hydrograph with peak inflow discharge equal to 620 m3 s after 24 h was set as upstream boundary condition while a rating curve was assigned downstream far from the breach location the breach opening was generated on the right levee when the peak discharge reached that location indicated in fig 18a 28 h after the beginning of the simulation the event was then prolonged for 72 h so that 100 h of physical time were simulated manning s roughness coefficient was assumed equal to 0 05 m 1 3 s except for the residential and industrial areas where it was increased to 0 143 m 1 3 s simulations were run using both first and second order accurate schemes and results and execution times of the lts and gts models were compared fig 19 a shows an example of the temporal resolution level distribution in the blocks near the breach location 5 h after the breach opening while fig 19b represents the water depth map obtained from the second order lts simulation at the same moment it can be noticed that the maximum temporal resolution m 0 is observed along the riverbed and at the breach location where the fine grid size and the high water depth and velocity values dictate the time step for the whole domain the leveed floodplain on the left is only partially inundated with small water depths thus a smaller temporal resolution level is observed here despite the fine grid size the whole flooded area at the minimum spatial resolution is updated with the minimum temporal resolution m 3 notice that a temporal resolution level is also assigned to a few blocks outside the wet area due to the bdo procedure which activates dry blocks bounding wet blocks fig 20 describes the flooding evolution in time by depicting the water surface elevation maps at three selected times obtained from the second order lts simulation a synthetic index for comparing flood extent of different scenarios is suggested by horritt and bates 2002 18 i f n g t s n l t s n g t s n l t s where n gts and n lts indicate the number of flooded cells at a fixed time in the gts and lts simulations respectively the index coincides with the unity when the flooded areas are the same table 5 reports if for first and second order simulations at selected times all values are very close to one in particular for the first order scheme confirming the close agreement between the inundated areas for gts and lts simulations the rmse of the water surface elevations in the flooded region obtained from gts and lts simulations is also reported in table 5 for the first order scheme negligible differences can be noticed while rmse values up to a few centimeters are observed for the second order scheme after 72 h from the breach opening approximately 140 km2 are inundated a comparison between the total flooded areas for all tests is reported in table 6 showing almost equivalent flood extents for all simulations table 6 also reports information about the computational performance of the models the achieved time ratio is slightly larger for the first order scheme equal to 1 77 than for the second order scheme 1 61 the same holds for the corresponding theoretical values equal to 2 14 and to 1 74 respectively during the second order lts simulation of this test case the roll back procedure is activated only once thus not affecting the computational time significantly ratios of physical to computational time rt reported in table 6 confirm the efficiency of gpu accelerated numerical models for practical applications particularly if the lts version of the scheme is adopted 5 conclusions in this paper a local time stepping strategy was implemented in an explicit fv numerical scheme which solves the 2d swes on structured non uniform grids both first and second order accurate schemes were considered the method was developed in a cuda c code so that the computational capability of gpus could be fully exploited the model assessment was performed based on the simulation of three theoretical test cases and two field scale problems results of the numerical tests show that compared to the traditional global time stepping strategy lts reduces the total simulation times without impairing the solution accuracy execution time ratios between 1 2 and 2 8 were obtained the achievable performance improvement mainly depends on the spatial resolution level distribution and on the local flow field conditions the adoption of the lts method was observed to be especially convenient in the simulation of real field test cases characterized by a large domain discretized with a coarse mesh where a high level of refinement was required only in small selected regions e g bridge piers breach location the high values of ratios of physical to computational time achieved for the levee breach scenario support the idea of applying gpu accelerated models coupled with an efficient lts strategy to quasi real time 2d simulations of flooding events over domains of considerable extent preserving the required level of detail where necessary this prospect can become even more feasible if the rapid development of gpu capabilities and the possibility of extending the model to multi gpus are taken into account acknowledgements this work was partially supported by the ministry of education universities and research italy under the scientific independence of young researchers programme grant number rbsi14r1gp cup code d92i15000190001 the authors gratefully acknowledge the support of cineca under project pancia id hp10cigmeb of nvidia under the cuda research center program and of gncs indam the authors wish to thank the anonymous reviewers whose valuable suggestions greatly contributed to improving the paper 
903,in the simulation of flooding events mesh refinement is often required to capture local bathymetric features and or to detail areas of interest however if an explicit finite volume scheme is adopted the presence of small cells in the domain can restrict the allowable time step due to the stability condition thus reducing the computational efficiency with the aim of overcoming this problem the paper proposes the application of a local time stepping lts strategy to a gpu accelerated 2d shallow water numerical model able to handle non uniform structured meshes the algorithm is specifically designed to exploit the computational capability of gpus minimizing the overheads associated with the lts implementation the results of theoretical and field scale test cases show that the lts model guarantees appreciable reductions in the execution time compared to the traditional global time stepping strategy without compromising the solution accuracy keywords gpu local time stepping shallow water equations finite volume flood simulation parallel computing 1 introduction the heavy economic and human losses caused by flood events in recent years barredo 2009 demand the adoption of specific flood risk management strategies which include not only structural defense systems such as levees but also resilient policies from this point of view the numerical modeling of flood scenarios can be very helpful both for the definition of the most effective technical interventions and for the design of flood hazard and flood risk maps necessary for flood risk management planning as requested by the european council 2007 most free surface flows such as tsunamis river floods and dam break events can be simulated through the two dimensional 2d shallow water equations swe which can be discretized by different numerical methods e g alcrudo and garcia navarro 1993 horritt and bates 2002 vacondio et al 2012 costabile et al 2012 2015 among these finite volume fv schemes have the advantage of accurately describing transcritical flows and shock type discontinuities toro 1999 and can even be applied to flows over irregular bathymetries provided that the scheme is associated with a specific treatment of wetting and drying fronts e g bradford and sanders 2002 brufau et al 2004 liang and borthwick 2009 and of bed and friction slope source terms which should guarantee that the c property is satisfied e g bermudez and vazquez 1994 greenberg and leroux 1996 garcia navarro and vazquez 2000 rogers et al 2003 audusse et al 2004 aureli et al 2008 liang and marche 2009 vacondio et al 2013 the high computational cost entailed by the application of 2d swe models to field scale simulations with high resolution meshes encouraged researchers to investigate parallelization techniques on supercomputers e g sanders et al 2010 moreover the computational capability of graphics processing units gpus has been recently exploited to carry out high performance computing on personal workstations and gpu accelerated swe models are now developed by a number of authors both for structured lastra et al 2009 de la asuncion et al 2013 brodtkorb et al 2012 vacondio et al 2014 2017 ferrari et al 2017 and unstructured meshes castro et al 2011 lacasta et al 2014 petaccia et al 2016 results of these works show that significant reductions in computational time up to two orders of magnitude compared with serial codes can be achieved in many practical applications local mesh refinement is often necessary due to the presence of complex bathymetric features or specific areas of interest and some gpu enhanced models able to handle non uniform resolution have been presented in literature sætra et al 2015 vacondio et al 2017 explicit numerical schemes are often preferred to implicit ones as shown by teng et al 2017 due to their suitability for describing rapidly varying flows but also to the possibility of designing efficient parallel models exploiting modern heterogeneous computing architectures however the time step size used to update the solution with explicit schemes is computed based on the courant friedrichs lewy cfl stability condition and is generally dictated by the minimum grid size neglecting the possible high variability in wave celerity values all cells in the domain are advanced with the common minimum time step this strategy is usually referred to as global time stepping gts if only a small region is refined the other part of the domain with a coarser grid size is updated with a much smaller time step than its maximum admissible for stability thereby increasing the required computational effort in order to overcome this problem osher and sanders 1983 introduced the local time stepping lts strategy based on the idea of advancing each cell with its own time step closer to its maximum allowable lts schemes were successfully applied to swe models crossley and wright 2005 sanders 2008 kesserwani and liang 2015 dazzi et al 2016 and appeared to be beneficial for enhancing their computational performances compared to standard gts schemes without degrading the solution accuracy to the best of the authors knowledge the only attempt to apply a lts scheme in a gpu accelerated model can be attributed to sætra et al 2015 who focused on an adaptive mesh refinement amr technique this paper presents an efficient gpu implementation of a first and second order accurate fv scheme with a lts strategy which solves the 2d swes on a non uniform structured grid the present model is based on an existing gpu model vacondio et al 2014 2017 which has been thoroughly tested and validated even for the simulation of real flooding events vacondio et al 2016 compared to previous lts techniques the algorithm presented in this work has been derived and coded in such a way that it can exploit the computational capabilities of gpus the performances of the gts and lts schemes are assessed based on three theoretical and two real field test cases the paper is structured as follows section 2 briefly describes the original gts model in section 3 the new lts scheme and its gpu implementation are detailed then the model performances are evaluated through the simulation of five test cases in section 4 conclusions are drawn in section 5 2 global time stepping scheme the original gts numerical model is described in detail by vacondio et al 2014 2017 here only the most important features are briefly recalled the governing equations are the 2d swes written in integral form toro 2001 1 t a u d a c h n d c a s 0 s f d a where t is the time a is the area of the integration volume c is the volume boundary u is the vector of conserved variables h f g is the tensor of fluxes in the x and y directions n is the outward unit vector normal to c s 0 and s f are the bed and friction slope source terms respectively the modified form of the swes which guarantees that the scheme is well balanced is adopted following liang and marche 2009 2a u η u h v h f u h u 2 h 1 2 g η 2 2 η z u v h g v h u v h v 2 h 1 2 g η 2 2 η z 2b s 0 0 g η z x g η z y s f 0 g h n f 2 u u 2 v 2 h 4 3 g h n f 2 v u 2 v 2 h 4 3 in eq 2 h is the flow depth z is the bed elevation above datum and η h z is the water surface elevation above datum u and v are the velocity components along the x and y directions respectively g is the acceleration due to gravity and nf is manning s roughness coefficient an explicit fv scheme is used to discretize the equations both first order and second order accurate approximations in space and time are implemented the first order approximation exploits the following equation to update the conserved variables in time 3 u i j n 1 u i j n δ t δ x f i 1 2 j f i 1 2 j δ t δ y g i j 1 2 g i j 1 2 δ t s 0 s f subscripts i j represent the cell position while superscript n refers to the time level δx and δy are the cell dimensions in the x and y directions respectively and δt is the time step size computed according to the cfl stability condition toro 2001 4a δ t i j 1 2 c r min δ x u i j g h i j δ y v i j g h i j 4b δ t min i j δ t i j where cr is the courant number 1 notice that according to the gts scheme each cell is updated with the same time step δt equal to the minimum value in the whole domain second order accuracy in space is achieved by means of a depth positive muscl extrapolation at cell boundaries audusse et al 2004 adopting the minmod slope limiter second order accuracy in time is obtained by applying the second order runge kutta method 5 u i j n 1 u i j n 0 5 δ t d i u i j n d i u i j n 1 2 where u i j n 1 2 is obtained as 6 u i j n 1 2 u i j n δ t d i u i j n and the operator d i u i j is defined as 7 d i u i j 1 δ x f i 1 2 j f i 1 2 j 1 δ y g i j 1 2 g i j 1 2 s 0 s f fluxes are computed using the hllc approximate riemann solver toro 1999 together with the correction proposed by kurganov and petrova 2007 which avoids non physical velocity values at wet dry fronts the slope source term is discretized using a centered approximation vacondio et al 2014 while the friction source term is discretized using the implicit formulation proposed by caleffi et al 2003 the cuda c implementation of the model exploits the intrinsic parallelization of computations on gpu devices thus guaranteeing fast execution times compared to serial codes the basic work unit in cuda is the thread and many threads are organized into a block in the present model each thread corresponds to one cell used to discretize the computational domain and each block consists of k k cells in the first implementation of this model vacondio et al 2014 the domain was discretized by means of a cartesian grid information about neighboring cells blocks is easily retrieved thanks to the correspondence between the physical position of a cell block and its position on the two dimensional array where data concerning that cell block are stored in the memory recently the model has been extended to the case of a new type of non uniform structured grids called block uniform quadtree buq grids vacondio et al 2017 in this case the domain is still partitioned into blocks each containing k k cells with uniform spatial resolution within the block but cells belonging to different blocks can be characterized by a different grid size in particular starting from blocks with the maximum resolution level level 0 which contain cells with size δx 0 δxmin each level l contains cells with size δxl 2 l δxmin up to the minimum resolution level l where cells have size δxmax 2 l δxmin in this work l is assumed equal to 3 neighboring blocks can differ by one resolution level at most the idea is similar to standard quad tree partitioning liang and borthwick 2009 but each node of the quad tree mesh corresponds to a block instead of a single cell operatively the user must specify seeding points with assigned spatial resolution level as input to the simulation and the model automatically generates the multi resolution grid according to the procedure described by vacondio et al 2017 memory allocation of blocks is not pre established as in the case of cartesian grid hence some additional data must be stored in order to retrieve information about neighboring cells belonging to different blocks during the computations in particular when adjacent cells have a different spatial resolution level the natural neighboring interpolation procedure liang 2011 is employed to reconstruct the conserved variables in the neighboring cells 3 local time stepping scheme the key idea of the lts strategy is to advance each cell with a time step as close as possible to its maximum permissible for stability most papers concerning lts e g kleb et al 1992 crossley and wright 2005 krámer and józsa 2007 sanders 2008 dazzi et al 2016 assume the local time step size in each cell as an integer multiple in particular a power of two multiple of a common reference time step δt equal to the minimum in the whole domain the procedure usually starts with the application of eq 4a for each cell in order to compute its own allowable time step and of eq 4b in order to find the global minimum time step a temporal resolution level m 0 1 2 is assigned to each cell based on the condition 2 m δt δti j 2 m 1 δt the coarsest level in the domain m is also computed then the update procedure begins all cells belonging to level m will be updated via 2 m m steps of size 2 m δt for example cells with m 0 will be advanced for 2 m steps of size δt while cells with m m will only perform one step of size δtmax 2 m δt this sequence of intermediate updates for each cell leads to a synchronized solution in the whole domain then the procedure starts again the lts algorithm presented in this paper is generally based on the same idea with a major difference it is assumed that all cells belonging to a block which already share the same mesh size in the buq grid also share the same local time step while different blocks may be updated with different time steps hence similarly to the spatial resolution level l 0 1 l of each block it is possible to define the temporal resolution level m 0 1 m of each block notice that while the spatial resolution is assigned by the grid generation procedure in the pre processing stage and remains unchanged during the simulation the temporal resolution levels need to be re computed before each sequence of partial updates because flow conditions and admissible time step sizes can change throughout the domain during the simulation the choice of adopting the same time step for all cells belonging to a block may appear to be inefficient in the lts framework because in the worst case only one cell may actually need to be advanced at such a slow pace however this block based design offers a simple and well performing parallel code which fully exploits the parallel architecture of gpus it is worth recalling that the best performances on gpus are obtained when cells belonging to a block are characterized by uniform features so that special treatments are required only for cells on the border of a block and code branching is minimized ultimately this lts implementation represents a natural extension of the spatial multi resolution model and introduces limited coding efforts and overheads in the following sub sections the main aspects of the lts scheme are outlined section 3 1 deals with the definition of active blocks with the assignment of a temporal resolution level to each block and with the m level distribution regularization in section 3 2 the core updating procedure for different time resolution blocks is covered while section 3 3 describes the rollback procedure needed in cases where the cfl condition is violated finally section 3 4 reports on the expected advantages and overheads for a lts block based simulation on gpus 3 1 temporal resolution level assignment to blocks this section describes the procedures required before the asynchronous time advancement of blocks may start notice that when these operations are performed all cells are synchronized to the common time level tn the procedure for the temporal resolution assignment to each block starts with the execution of a kernel i e a cuda piece of code executed on the gpu which computes the allowable δti j in each cell and determines the minimum δtb of each block b then the cpu calculates the minimum δt in the whole domain 8 δ t mi n b δ t b and assigns an initial guess m level to each block as follows 9 m b int log 2 δ t b δ t it is worth recalling that in the original gts model a block deactivation optimization bdo was introduced in order to increase model efficiency vacondio et al 2014 thanks to this procedure only active blocks are processed during each time step a block is defined as active if it contains at least one wet cell or one cell which may become wet at the end of the current time step the kernel that computes the allowable time step also determines whether a block is wet then wettable blocks are added to the set of active blocks by cpu processing based on the simple idea that a wet dry front may propagate at most over one cell per time step hence a dry block may become wet if one of its neighboring blocks has at least one wet cell on the confining border for lts this condition is not sufficient since up to 2 m steps may be performed before re computing the set of active blocks hence all dry blocks that are neighbors to wet blocks are activated as shown in fig 1 where wet blocks and wettable blocks are active while dry blocks are not processed during the current iteration since the set of active blocks is not redefined for a few steps it is strongly recommended to impose an upper bound to m which should be limited to fulfil the condition 2 m k if the block consists of k k cells for example if 8 8 cell blocks are employed m should be limited to 3 so that 8 steps may be performed at most and that the wet dry front may not propagate over more than the 8 cells of the neighbor block obviously for wettable blocks it is impossible to compute δtb because depth and velocity are null in each cell therefore each wettable block is temporarily assigned the following m level 10 m w e t t a b l e b l o c k min n w b m n w b where nwb indicates a neighboring wet block as an example let us consider the case of block j in fig 1 its two wet neighbors are characterized by m levels equal to 1 and 2 block j will then be given mj 1 exception to this rule is the case of a wettable block with a finer spatial resolution than the one of its wet neighbors in this case its temporal resolution level is refined too as an example block k in fig 1 whose wet neighbors are coarser and are characterized by m 1 is assigned a temporal resolution level equal to 0 many authors e g crossley and wright 2005 krámer and józsa 2007 suggest that neighboring cells or blocks in this case should differ by one temporal resolution level at most in order to increase the robustness of the scheme this also helps to develop an algorithm able to guarantee mass and momentum conservation when no source terms are present at interfaces between cells with different m levels while minimizing code branching and thus maintaining sufficient efficiency on gpus for this reason after defining the set of active blocks a regularization procedure to correct the initial guess m levels is performed see algorithm 1 iterations start from blocks with the maximum temporal resolution level m 0 for each block belonging to the current level the temporal resolution of all neighboring blocks is checked and whenever the difference between the m levels of the current and neighbor block exceeds one the neighbor block is assigned a reduced m level an example of the application of this procedure is shown in fig 2 a b which depict a few blocks of a buq grid with the corresponding temporal resolution level after the initial guess m level assignment fig 2a which obviously depends on the local grid size and flow variables in this example two instances of blocks with one neighbor indicated by a black arrow whose m level must be reduced see fig 2b can be identified moreover crossley and wright 2005 analyzed how waves propagate in neighboring cells characterized by different temporal resolution levels and concluded that a buffer region needs to be defined in order to guarantee the propagation of waves travelling from maximum to minimum temporal resolution level regions in the present model the buffer region consists of a buffer block whose m level is artificially re assigned according to algorithm 2 in summary each m level region is broadened with one more line of buffer blocks the allowable time step in these blocks is in theory double than the one actually used during the computations however the local artificial temporal resolution refinement ensures the correct propagation of waves and wet dry fronts an example of the m level reassignment to buffer blocks is represented in fig 2c 3 2 lts update procedure algorithm 3 compares the gts and lts scheme pseudocodes excluding input and output procedures at the beginning of each iteration the time step is computed and active blocks are defined in the lts procedure m levels are also properly assigned as already described in section 3 1 then the update procedure may start in the gts version of the model the solution is advanced by a single time step in the whole domain and the iterations over time continue in the lts version of the scheme instead if m is the maximum temporal resolution level in the domain a series of intermediate updates which advances the solution from time level tn to time level tn δtmax where δtmax 2 m δt via 2 m steps starts let us consider the first order version of the scheme in this case muscl extrapolation is not performed and the left and right values required for computing fluxes at cell edges coincide with the conserved variables at cell centres the solution in each cell is advanced via eq 3 with the local time step characterizing the current block at each step s 0 1 2 m 1 only blocks that fulfil a specific criterion are processed all cells belonging to block b with level m are updated with a time step equal to δtb 2 m δt if s is an integer multiple of 2 m the procedure is sketched in fig 3 for a simple one dimensional 1d case with six blocks and m 2 a special treatment is necessary for cells on the border of a buffer block whose neighboring cells are characterized by a different temporal resolution level although some authors e g kesserwani and liang 2015 saetra et al 2015 adopt temporal interpolation procedures at these interfaces which in turn require corrections to guarantee mass and momentum conservation the present scheme adopts an intrinsically conservative strategy to handle these interfaces figure 4 shows two cells belonging to a buffer block with m 0 and two cells on its neighboring block with m 1 without loss of generality a 1d representation with cells characterized by the same mesh size and only two temporal resolution levels is chosen for simplicity during the first step s 0 all cells are updated and fluxes are computed based on the conserved variables values at time tn as usual during the second step s 1 however only cells belonging to the block with m 0 must be advanced in time while inner cell i 1 has both west and east neighbor values available for flux computation at time tn 1 border cell i lacks a synchronized value to the east for this reason on its eastern edge which coincides with the block border values at time tn are reused for the flux computation in this way the sum of the eastern fluxes of cell i computed at times tn and tn 1 which are then multiplied by δt and the western flux of cell i 1 at tn which is multiplied by 2δt are the same and the scheme is conservative notice that on the western edge of cell i flux computation is still performed based on values at tn 1 this strategy is possible because arrays for fluxes are not allocated in the present model and the computation of the same flux at the edge between two cells is repeated twice once for each thread for efficiency reasons as discussed by vacondio et al 2014 hence the only drawback of this procedure is the necessity of allocating an additional array in memory where conserved variables from the previous time step in cells on the block boundaries are stored and retrieved when necessary the detailed implementation neglecting gpu parallelization is reported in algorithm 4 which corresponds to lines 7 13 of algorithm 3b the second order version of the scheme is presented next in this case two sub steps are performed during each time advancement and two kernels are launched during each of these two sub steps one which executes the muscl extrapolation and one which performs flux computation and time integration finally an additional kernel sums the contributions retrieved from the two half time steps according to eq 5 in the lts implementation of the scheme each pair of sub steps of the second order scheme can be compared to two consecutive steps of the first order scheme provided that appropriate criteria are defined in order to distinguish whether blocks belonging to each m level are to be processed during the current sub step time advancement from tn to tn δtmax is schematized in algorithm 5 which substitutes lines 7 13 of algorithm 3b fig 5 depicts three blocks with different m levels m 2 specifying the step 0 1 2 3 and sub step a b at which each block is processed according to algorithm 5 fig 5a represents muscl extrapolation while figure 5b shows flux computation and time integration cells lying on the border of buffer blocks are updated asymmetrically similarly to the first order scheme at sub steps for which the synchronized neighboring values are missing muscl extrapolation is skipped in these cells on the border block edge extrapolated values at the previous sub step are reused for flux computation on the inner edge accuracy is locally reduced to first order in space e g values at the cell center are used for flux computation in order to avoid interpolations results presented in the following section show that this simplification does not significantly impair the accuracy of the solution finally a short remark about open boundary conditions bcs needs to be made for each open bc discharge rating curve and or water level the minimum allowable temporal resolution level mbc is assigned to all blocks affected by the given open bc the specific cuda kernel which assigns the prescribed quantities discharge water elevation rating curve at open bcs cells is executed only at steps for which blocks with m mbc are processed for details about bcs handling in the model the reader is referred to vacondio et al 2014 from a practical point of view this is not a relevant feature in the lts model because the number of the cells to which an open boundary conditions is assigned is usually negligible compared to the total number of cells used to discretize the domain 3 3 roll back procedure according to algorithm 3b the time step computation and the m level assignment lines 2 6 are performed before the inner lts loop lines 7 13 which advances the solution from tn to tn δtmax this means that the temporal resolution level hence the time step size is maintained fixed for a few steps however during the computations flow field conditions vary in some cases velocity and or celerity might increase and the time step can no longer fulfil the cfl condition generating instabilities for this reason a procedure to roll back simulation time is implemented in the present model an additional array needs to be allocated where conserved variables are stored at time tn after each partial time integration the cfl condition is re evaluated in each cell based on the updated values of conserved variables assuming cr 1 if this check highlights potential instabilities somewhere in the domain the lts loop is stopped and partial computations already performed are discarded then the loop begins again starting from the values stored in the rollback array and assuming a halved time step size everywhere the prevention of potential simulation crashing compensates for the slight overload induced by this procedure 3 4 efficiency of the lts algorithm on the gpu a comparison between the computational efficiency of the gts and lts schemes can be performed by evaluating the execution time ratio su defined as the ratio of the gts execution time to the lts execution time for each simulation this factor strongly depends on the test case under consideration and on the grid refinement and the maximum expected time ratio for a given test case can be theoretically estimated by analyzing the amount of computational operations associated with the two versions of the code for this analysis let us distinguish between the dt piece of code lines 2 3 in algorithm 3a for gts or 2 6 in algorithm 3b for lts and the update piece of code lines 4 5 in algorithm 3a for gts or 7 14 in algorithm 3b for lts which are substituted by algorithm 5 for the second order scheme preliminary investigations show that in the gts version of the code the dt stage takes roughly 30 of the total simulation time for the first order scheme and 15 for the second order scheme accordingly the update stage takes roughly 70 and 85 of the total execution time for the first and second order schemes respectively as regards the dt stage it can be observed that in the lts version of the scheme the time step computation and the bdo procedure are performed only once every 2 m steps while the gts scheme repeats these operations at every time step therefore the lts version is in theory 2 m times faster than the gts scheme in performing the dt stage even if the additional processing for the m level assignment see section 3 1 reduces the actual time ratio for this part of the code on the other hand the most important computational saving of the lts scheme is associated with the update stage since a subset of blocks is processed fewer times in the lts code than they would be in the gts scheme an estimate of the achievable theoretical time ratio for the update piece of code can be quantified by dividing the total number of cells blocks processed in the gts simulation by the same value in the lts simulation assuming that the update operations performed on each block require roughly the same computational time kleb et al 1992 in the example depicted in fig 3 fourteen update operations are performed instead of twenty four which results in a theoretical time ratio for the update stage only equal to 1 7 however this simple computation does not take into account the overheads associated with the lts scheme first of all a few operations must be performed inside each kernel of time advancement in order to distinguish whether a block must be processed during the current time step in addition to this managing interfaces between blocks with different temporal resolution levels requires code branching which may slow down computations on the gpu the whole roll back procedure described in section 3 3 is also added to the code all these overheads lead to an actual time ratio smaller than the theoretical one in summary the theoretically achievable time ratio can be evaluated by combining the contributions from the dt and update stages as follows 11 s u t h e o r 1 2 m t d t 1 n m 0 m n m 2 m t u p d a t e 1 where nm is the number of cells or blocks belonging to each m level n is the total number of cells or blocks in the domain and t d t and t u p d a t e are the execution times for the dt and update stages normalized to the total execution time considering the example depicted in fig 3 m 2 the theoretical time ratio is equal to 2 05 for the first order scheme assuming t d t 0 3 and t u p d a t e 0 7 and to 1 86 for the second order scheme t d t 0 15 and t u p d a t e 0 85 finally an additional aspect should be considered in the assessment of the computational efficiency of the model on gpus tasks on different blocks are executed in parallel by the different processors the hardware architecture consists of if the number of active blocks is larger than the available cores blocks need to be queued by the gpu s scheduler while when simulations with a smaller number of active blocks are performed the computational capabilities of the gpu may not be fully exploited in this latter case the increased model efficiency due to parallel computations on the gpu is somehow impaired by the cpu gpu communication overheads see vacondio et al 2017 hence the typical scalability of gpus i e the computational time normalized to the total number of cells decreases with increasing the number of processed cells and remains almost constant after reaching a threshold value which depends on the gpu type should also be taken into account when evaluating the computational efficiency of both gts and lts schemes in any case a quantitative analysis of the overheads and savings of the lts scheme is discussed for the first two test cases in section 4 4 numerical tests in this section a comparison between the performances of the original gts and the novel lts model implementation is performed based on three theoretical test cases and on two field scale practical applications all simulations were run using a k40 tesla gpu for all test cases the courant number was assumed equal to 0 8 moreover the buq grid was formed by 8 8 cell blocks hence the temporal resolution level was limited to m 3 for all simulations 4 1 vortex test case the first numerical experiment considers the steady state test case of a vortex flow with analytical solution sanders and bradford 2006 the vortex circulates clockwise on a flat frictionless bottom and its motion can be described by means of the following relations assuming that the coordinate system origin coincides with the vortex center 12a h h 0 u 0 2 4 g 1 2 r r 0 e x p 2 r r 0 e x p 2 r r 0 12b u u 0 y r 0 e x p r r 0 v u 0 x r 0 e x p r r 0 where r x 2 y 2 is the distance from the vortex center and the values attributed to the other parameters are as follows h 0 10 m u 0 1 5 m s r 0 100 m the domain is extended up to r 3000 m the maximum resolution δxmin is imposed in the area close to the vortex center approximatively up to r 500 m surrounded by a smooth transition up to the minimum resolution δxmax in the outer region of the domain a detail of the resulting multi resolution grid is shown in fig 6 which represents each 8 8 cell block as a square element the temporal resolution level for the lts simulation is dictated by the grid size for this test case thus the area characterized by the maximum temporal resolution level m 0 coincides with the finest grid size region plus the circle of neighboring blocks with halved grid size required by the numerical scheme as a buffer region to ensure correct wave propagation the same holds for the areas with coarser grid size the m level distribution is also represented in fig 6 where different colors identify blocks with homogeneous temporal resolution level sixteen simulations were run assuming four different test configurations see table 1 each performed with both models lts and gts and with both the first and the second order accurate version of the scheme minimum and maximum grid sizes were in the range 1 8 m and 8 64 m respectively the analytical solution was imposed as initial condition and the simulation was run for a physical time in the range 500 4000 s in order to ensure that the same number of update operations was performed for all test configurations as an example figs 7 and 8 compare the results of the gts and lts simulations performed with the second order accurate scheme for test configuration b fig 7 reports the contour maps of water depth and velocity magnitude u u 2 v 2 while fig 8 shows the profiles of the same variables along the y x line a quantitative comparison regarding the agreement of numerical results with the exact solution can be performed by computing the non dimensional l 2 error norms of water depth and velocity components 13 l 2 h 1 n i 1 n h i num h i exact h 0 2 where n is the number of cells in the domain and subscripts num and exact refer to the numerical and analytical solutions the same expression can be used to calculate l 2 u and l 2 v by adopting u 0 as reference value for normalization the l 2 error norms computed for the sixteen simulations are reported in table 1 as expected errors increase with the grid size for both first and second order simulations and second order error norms are at least one order of magnitude smaller than first order ones for all configurations no differences can be appreciated between lts and gts water depth error norms which remain in the range 10 5 10 6 close to machine precision for second order simulations while velocity error norms reach the order of magnitude 10 4 10 3 for the coarsest grid sizes and show small differences between lts and gts values for first order simulations lts and gts error norms practically coincide finally table 1 reports the execution time ratios for all test configurations time ratios achieved by the first order scheme reaching 1 53 are always slightly larger than the ones obtained from the second order scheme which are limited to 1 44 the best performances can be observed for tests with the finest grid size even if the time ratio is not expected to depend much on the grid size and number of cells due to the steadiness of the test case in fact the achievable theoretical time ratio is approximately the same for all configurations and is equal to 1 86 and to 1 6 for the first and second order schemes respectively a detailed analysis regarding the computational times associated with different parts of the code is reported for configuration a δxmin 1 m the theoretical time ratios for the dt and update stages are equal to 8 and 1 4 respectively fig 9 shows the execution times of the two parts of the code for first and second order schemes both gts and lts values are normalized to the total execution time of the gts second order simulation which is the longest one in the first order scheme the time ratio associated with the update piece of code is 1 26 this means that the overhead associated with lts branching in the update kernels is only 10 for this simulation on the other hand the time ratio for the dt stage is equal to 3 31 and the overhead associated with the additional operations performed for the m levels assignment is widely compensated by the fact that this piece of code is executed only once every eight steps the combination of the computational savings associated with the two parts of the code results in an actual time ratio equal to 1 53 therefore the lts overheads increase the total simulation time by roughly 18 compared to the theoretically achievable value for this test for the second order scheme due to the heavier update operations the dt piece of code is computationally less relevant than for the first order one in fact while both first and second order schemes require approximately the same execution time for the dt operations the update stage for the second order scheme takes more than twice the time required for the first order scheme this is true for both gts and lts due to the larger number of executed kernels for this reason despite similar partial time ratios the total execution time ratio for the second order scheme equal to 1 44 is slightly smaller than the value obtained from first order simulations and the lts overheads can be estimated to be only 10 of the total execution time 4 2 circular dam break the model was then tested by simulating the classical wet circular dam break problem liska and wendroff 1997 the domain 25 25 m 25 25 m is characterized by a horizontal frictionless bottom a 10 m high cylindrical water column with radius r 10 m is centered in the domain and is surrounded by 1 m deep still water due to the cylindrical symmetry of the problem a reference solution can be obtained by deriving the inhomogeneous 1d system of the swe in radial geometry toro 2001 and solving it with a very fine mesh δx 0 005 m the grid for the 2d lts and gts simulations was generated by forcing the maximum resolution δxmin 0 025 m alongside the initial discontinuity 9 m r 11 5 m and by imposing the halved mesh size 0 05 m in the regions 6 5 m r 9 m and 11 5 m r 14 5 m the automatic grid generation procedure created the remaining transitions up to the minimum resolution δxmax 0 2 m fig 10 represents a detail of the resulting mesh which consists of 0 465 106 cells simulations were run until the physical time t 1 s fig 11 compares the water depth profiles along the radial direction at t 0 4 s obtained by the lts and gts simulations with the reference solution results from both first and second order schemes agree well with the pseudo exact solution the latter showing a closer agreement as expected notice that even if the shock wave is propagating over a region with m 1 see the temporal resolution level distribution at t 0 4 s in fig 10 the lts solution accuracy is not degraded compared to gts results on the contrary dimensional l 2 error norms for water depth and velocity magnitude reported in table 2 show that the lts model provides even slightly more accurate results than gts in particular as regards the first order scheme also for this test case the execution time ratio reported in table 2 is slightly larger for the first order scheme su 1 49 than for the second order scheme su 1 39 the achievable theoretical values are equal to 1 81 and 1 56 for the first and second order schemes respectively in particular 8 for the dt stage and 1 36 for the update stage as can be inferred from fig 12 the update time ratio is equal to 1 22 for the first order simulation and to 1 28 for the second order one confirming the 6 10 overhead induced by the lts implementation within this piece of code which reduces the achieved time ratio compared to the theoretical one similarly to the previous test case the execution times for the dt operations show that the overhead induced by the procedure for m level assignment in the lts scheme is largely compensated by the computational savings due to the fewer dt executions globally the lts overhead is 10 18 for this test case too 4 3 thacker test case the lts scheme was further validated by simulating the periodic oscillation of a water volume in a frictionless paraboloidic basin thacker 1981 which involves wetting drying and non flat topographies the bottom can be described by means of the following equation 14 z z 0 x 2 y 2 l 2 1 where z 0 is the depth of the vertex and l is the radius at z 0 see fig 13 a the water volume initially at rest and paraboloidic expands and contracts periodically due to gravity the exact solution for this test case is 15 η x y t z 0 1 a 2 1 a cos ω t 1 x 2 y 2 l 2 1 a 2 1 a cos ω t 2 1 u x y t 1 1 a cos ω t 1 2 ω xa sin ω t v x y t 1 1 a cos ω t 1 2 ω ya sin ω t where ω and a are defined as 16 ω 8 g z 0 l a η 0 z 0 2 z 0 2 η 0 z 0 2 z 0 2 the test parameters are set as follows z 0 50 m η 0 10 m l 1000 m which correspond to a period of oscillation equal to 50 s the domain is extended up to r 1500 m the maximum resolution δxmin 1 m is assigned to the area subject to wetting drying 850 m r 1100 m and near the paraboloid center r 30 m while the grid generation procedure automatically creates transitions up to δxmax 8 m the final mesh fig 14 consists of 2 106 cells and the simulation time includes four complete oscillations tfinal 400 s fig 13b presents a comparison between analytical and numerical results for the second order scheme in particular slices of water surface elevation along the x axis at selected times are depicted gts and lts profiles practically coincide confirming the accuracy of the scheme even in the presence of wetting drying and bottom slope source terms dimensional l 2 error norms reported in table 3 for t 250 s show that lts results are even slightly better than gts the obtained time ratios reported in table 3 equal to 1 47 and 1 32 for first and second order simulations respectively are slightly smaller than the corresponding theoretical values estimated to be equal to 1 70 and to 1 43 due to the overheads introduced in the lts scheme as already discussed for the previous test cases the same case was also simulated with friction in order to assess the lts scheme accuracy when both source terms are present for this case no analytical solution is available therefore lts results are only compared with gts manning s roughness coefficient is set equal to 0 15 m 1 3s fig 15 reports the water surface elevation trend in time at point 0 0 0 0 for both frictionless and non frictionless simulations while oscillation amplitude and frequency remain constant in the former case in the latter case water level oscillations are dampened as expected again lts and gts results are almost overlapping in this case time ratios are equal to 1 74 and 1 65 for the first and second order simulations respectively in contrast with theoretical values of 2 11 and 1 86 4 4 parma baganza test case for field scale test cases computational savings are expected to be considerable when the lts scheme is used to simulate a large domain where only a small region is discretized with a very fine mesh this might be the case of the presence of bridge piers in a riverbed which require a high level of detail the baganza river northern italy which is crossed by a bridge 500 m upstream of its confluence in the parma river was considered for this test case the bridge is characterized by four 3 10 m round nosed piers the bathymetry is shown in fig 16 the 3 4 km long final branch of the baganza river was modelled together with the 2 km and 1 km long branches of the parma river upstream and downstream of the confluence respectively the non uniform buq grid is characterized by δxmin 0 25 m imposed only at the bridge site which gradually transitions to δxmax 2 m in the remainder of the domain as shown in the insert in fig 16 the total number of cells is equal to 0 536 106 as upstream boundary condition a discharge hydrograph is assigned at the inflow of the baganza river the initial value is 100 m3 s which is gradually increased to 300 m3 s within one hour the simulation is then prolonged for three more hours similarly a discharge hydrograph with initial and final values equal to 200 and 400 m3 s is imposed as upstream boundary condition for the parma river downstream a constant water depth is assigned initial conditions are obtained from a preliminary steady state simulation with the initial discharge values manning s roughness coefficient is assumed equal to 0 04 m 1 3s table 4 reports the execution time ratios for both first and second order simulations values of approximately 2 7 confirm the achievable reduction in execution time due to the adoption of the lts version of the model in this case the theoretical values are equal to 3 83 for the first order scheme and to 3 45 for the second order scheme for real test cases with boundary conditions and wet dry fronts code branching probably enhances the overheads associated with the lts scheme and the total overhead 22 27 is larger than the one obtained from the simulation of the theoretical test cases previously analyzed 10 18 in addition to the greater computational efficiency the choice of the lts scheme does not degrade the quality of results compared to gts the root mean square error rmse between gts and lts water surface elevations when steady state conditions were achieved was also computed 17 rmse 1 n i 1 n η i gts η i lts 2 and was observed to be almost negligible see table 4 finally as an example of results fig 17 a b show the water surface elevation and velocity maps around one of the bridge piers obtained from the second order lts simulation the adoption of a mesh with local high resolution allows predicting the local 2d flow field in detail in particular the stagnation points upstream of the pier and the recirculation region downstream can be resolved for comparison fig 17c depicts the velocity field for a simulation performed without imposing a local grid refinement in this case the pier geometry is only roughly described and the velocity field is not accurately captured this justifies the choice of a locally refined mesh when the flow field near the bridge is of interest which makes the adoption of a lts scheme particularly convenient 4 5 flooding scenario due to levee breaching finally the achievable performance improvement of the lts scheme over the gts scheme was assessed based on the simulation of the flooding scenario induced by a hypothetical levee breach on the secchia river italy the 750 km2 domain reported in fig 18 a includes a 43 km long branch of the river from modena to concordia and the floodable area on its right bank stretching to the panaro river the buq grid used for the simulations was characterized by δxmin 5 m and δxmax 40 m the maximum resolution was forced near the inflow and outflow boundary conditions and around the breach location a grid size δx 10 m was imposed along the remainder of the river the resulting grid size distribution along the river is shown in fig 18b besides the whole floodable area was discretized at the minimum resolution hence its representation is avoided in fig 18b the domain includes roughly 1 106 cells the 20 years return period discharge hydrograph with peak inflow discharge equal to 620 m3 s after 24 h was set as upstream boundary condition while a rating curve was assigned downstream far from the breach location the breach opening was generated on the right levee when the peak discharge reached that location indicated in fig 18a 28 h after the beginning of the simulation the event was then prolonged for 72 h so that 100 h of physical time were simulated manning s roughness coefficient was assumed equal to 0 05 m 1 3 s except for the residential and industrial areas where it was increased to 0 143 m 1 3 s simulations were run using both first and second order accurate schemes and results and execution times of the lts and gts models were compared fig 19 a shows an example of the temporal resolution level distribution in the blocks near the breach location 5 h after the breach opening while fig 19b represents the water depth map obtained from the second order lts simulation at the same moment it can be noticed that the maximum temporal resolution m 0 is observed along the riverbed and at the breach location where the fine grid size and the high water depth and velocity values dictate the time step for the whole domain the leveed floodplain on the left is only partially inundated with small water depths thus a smaller temporal resolution level is observed here despite the fine grid size the whole flooded area at the minimum spatial resolution is updated with the minimum temporal resolution m 3 notice that a temporal resolution level is also assigned to a few blocks outside the wet area due to the bdo procedure which activates dry blocks bounding wet blocks fig 20 describes the flooding evolution in time by depicting the water surface elevation maps at three selected times obtained from the second order lts simulation a synthetic index for comparing flood extent of different scenarios is suggested by horritt and bates 2002 18 i f n g t s n l t s n g t s n l t s where n gts and n lts indicate the number of flooded cells at a fixed time in the gts and lts simulations respectively the index coincides with the unity when the flooded areas are the same table 5 reports if for first and second order simulations at selected times all values are very close to one in particular for the first order scheme confirming the close agreement between the inundated areas for gts and lts simulations the rmse of the water surface elevations in the flooded region obtained from gts and lts simulations is also reported in table 5 for the first order scheme negligible differences can be noticed while rmse values up to a few centimeters are observed for the second order scheme after 72 h from the breach opening approximately 140 km2 are inundated a comparison between the total flooded areas for all tests is reported in table 6 showing almost equivalent flood extents for all simulations table 6 also reports information about the computational performance of the models the achieved time ratio is slightly larger for the first order scheme equal to 1 77 than for the second order scheme 1 61 the same holds for the corresponding theoretical values equal to 2 14 and to 1 74 respectively during the second order lts simulation of this test case the roll back procedure is activated only once thus not affecting the computational time significantly ratios of physical to computational time rt reported in table 6 confirm the efficiency of gpu accelerated numerical models for practical applications particularly if the lts version of the scheme is adopted 5 conclusions in this paper a local time stepping strategy was implemented in an explicit fv numerical scheme which solves the 2d swes on structured non uniform grids both first and second order accurate schemes were considered the method was developed in a cuda c code so that the computational capability of gpus could be fully exploited the model assessment was performed based on the simulation of three theoretical test cases and two field scale problems results of the numerical tests show that compared to the traditional global time stepping strategy lts reduces the total simulation times without impairing the solution accuracy execution time ratios between 1 2 and 2 8 were obtained the achievable performance improvement mainly depends on the spatial resolution level distribution and on the local flow field conditions the adoption of the lts method was observed to be especially convenient in the simulation of real field test cases characterized by a large domain discretized with a coarse mesh where a high level of refinement was required only in small selected regions e g bridge piers breach location the high values of ratios of physical to computational time achieved for the levee breach scenario support the idea of applying gpu accelerated models coupled with an efficient lts strategy to quasi real time 2d simulations of flooding events over domains of considerable extent preserving the required level of detail where necessary this prospect can become even more feasible if the rapid development of gpu capabilities and the possibility of extending the model to multi gpus are taken into account acknowledgements this work was partially supported by the ministry of education universities and research italy under the scientific independence of young researchers programme grant number rbsi14r1gp cup code d92i15000190001 the authors gratefully acknowledge the support of cineca under project pancia id hp10cigmeb of nvidia under the cuda research center program and of gncs indam the authors wish to thank the anonymous reviewers whose valuable suggestions greatly contributed to improving the paper 
904,hydro mechanical properties of rock fractures are core issues for many geoscience and geo engineering practices previous experimental and numerical studies have revealed that shear processes could greatly enhance the permeability of single rock fractures yet the shear effects on hydraulic properties of fractured rock masses have received little attention in most previous fracture network models single fractures are typically presumed to be formed by parallel plates and flow is presumed to obey the cubic law however related studies have suggested that the parallel plate model cannot realistically represent the surface characters of natural rock fractures and the relationship between flow rate and pressure drop will no longer be linear at sufficiently large reynolds numbers in the present study a numerical approach was established to assess the effects of shear on the hydraulic properties of 2 d discrete fracture networks dfns in both linear and nonlinear regimes dfns considering fracture surface roughness and variation of aperture in space were generated using an originally developed code dfngen numerical simulations by solving navier stokes equations were performed to simulate the fluid flow through these dfns a fracture that cuts through each model was sheared and by varying the shear and normal displacements effects of shear on equivalent permeability and nonlinear flow characteristics of dfns were estimated the results show that the critical condition of quantifying the transition from a linear flow regime to a nonlinear flow regime is 10 4 j 10 3 where j is the hydraulic gradient when the fluid flow is in a linear regime i e j 10 4 the relative deviation of equivalent permeability induced by shear δ 2 is linearly correlated with j with small variations while for fluid flow in the nonlinear regime j 10 3 δ 2 is nonlinearly correlated with j a shear process would reduce the equivalent permeability significantly in the orientation perpendicular to the sheared fracture as much as 53 86 when j 1 shear displacement d s 7 mm and normal displacement d n 1 mm by fitting the calculated results the mathematical expression for δ 2 is established to help choose proper governing equations when solving fluid flow problems in fracture networks keywords fracture network nonlinear flow shear navier stokes equations surface roughness aperture heterogeneity 1 introduction rock fractures play an important role in the hydro mechanical properties of tight rock masses e g granite and basalt due to their typically stronger permeability comparing with rock matrix koyama et al 2008a blum et al 2009 stresses acting on a rock mass can change the aperture of single fractures involved resulting in the change of macro permeability as well as the redistributions of local hydraulic pressure and flow field min and jing 2003 baghbanan and jing 2008 the discrete fracture network dfn method has achieved great successes in simulating fluid flow in fractured rock masses in which single fractures are typically presumed to be formed by parallel plates and flow is presumed to obey the cubic law where the flow rate is linearly correlated with the pressure drop bear 1972 liu et al 2016a cai et al 2017 however it is well known that the parallel plate model cannot realistically represent the surface characters of natural rock fractures and reflect their influences on fluid flow behavior and the relationship between flow rate and pressure drop is not linear at sufficiently large reynolds numbers re zimmerman et al 2004 chen et al 2015 during the past few decades great efforts have been devoted to investigations on the effects of stress on hydraulic properties of single fractures e g yeo et al 1998 esaki et al 1999 olsson and barton 2001 auradou et al 2005 auradou et al 2006 li et al 2008 indraratna et al 2015 and fracture networks e g zhang and sanderson 1996 baghbanan and jing 2008 and on the nonlinear flow characteristics of single fractures e g zimmerman et al 2004 javadi et al 2010 2014 xiong et al 2011 cherubini et al 2012 2013 zhang and nemcik 2013 zou et al 2015 wang et al 2016 fracture intersections e g kosakowski and berkowitz 1999 li et al 2016 and fracture networks e g kolditz 2001 liu et al 2016b c these studies have greatly improved the understanding on the influences of several important factors such as boundary stress fracture geometry e g aperture shape of intersections and surface roughness reynolds number fracture density and etc on the fluid flow behavior of single fractures and fracture networks for investigations via dfn modelling normal stresses confining stress are typically imposed on the model boundaries which mainly result in the closure of single fractures baghbanan and jing 2008 meanwhile past studies have revealed that the permeability change of fractures caused by shear can be several orders greater than that caused by normal stresses li et al 2008 shear displacements of rocks can either result from engineering practices such as excavations in rock masses or from natural forces such as earthquakes although the shear induced permeability change of single rock fractures has been extensively explored studies on the effects of shear on fluid flow characteristics of dfns have not if any been reported since shear can significantly enhance the permeability of fractures and thereby facilitating the transition of flow from linear to nonlinear regimes the effects of shear and the nonlinear flow behavior need to be simultaneously considered in dfns recent studies have revealed that the nonlinear flow could occur for sheared fractures when the magnitude of re is as low as 0 01 javadi et al 2014 the nonlinear flow in rock fractures is typically described using the forchheimer s law forchheimer 1901 which can be applied over the entire range of flow rate velocity including re 1 another example of typically used equations is the izbash s law izbash 1931 although it does not have physically sensible behavior at low re the critical reynolds number re c is a widely used parameter for identifying the onset of nonlinear flow in single fractures which is determined when the nonlinear term or the inertial force contributes to a certain of the total pressure drop such as 10 zimmerman et al 2014 chen et al 2015 5 wang et al 2016 and 1 liu et al 2016b however for complex dfns comprising a great amount of fractures calculation of re for each single fracture may become a time consuming and technically difficult problem instead the critical hydraulic gradient which is defined as the ratio of hydraulic head difference to the dfn model side length is utilized as a macroscopic parameter that commonly has known values in many practices on fractured rock masses gale 1984 li et al 2016 to efficiently address the shear effects and nonlinear flow behavior of dfns simultaneously in the present study a code dfngen was developed to generate 2 d dfns composed of fractures with geometries that follow well accepted mathematical expressions a single fracture that cuts through each dfn was artificially sheared by assigning different values of normal displacement and shear displacement fluid flow simulations were implemented on the original and the sheared dfns utilizing a fvm finite volume method code to investigate the influences of shear on both linear and nonlinear flow behaviors of dfns 2 coupled shear flow behavior of fractures the aperture and its evolution which are key issues to the fluid flow characteristics of fractures can be more significantly changed by a shear process than that by a normal loading unloading process due mainly to the shear induced dilation upon rough surfaces li et al 2008 koyama et al 2008b for originally well mated fractures the reduced matedness during a shear process could be another reason that alters their hydraulic properties xiong et al 2011 a mechanical aperture and a hydraulic aperture were commonly utilized to quantify the aperture variations during the coupled hydro mechanical processes the mechanical aperture is typically defined as the mean point to point distance between the two walls of a fracture and the hydraulic aperture is typically back calculated from the pressure drop flow rate relationship obtained from hydraulic tests or numerical simulations using the cubic law the hydraulic aperture should be calculated at a sufficient low re because the cubic law does not hold when the fluid flow is not in the linear regime in a shear process the mechanical aperture of a fracture bm can be estimated by li et al 2008 1 b m b 0 δ b n δ b s where b0 is the initial aperture δbn is the variation of aperture during normal loading or unloading such as closure or opening δbs is the variation of aperture during shear dilation b0 is the aperture when no stresses are applied which can be mainly obtained via two approaches 1 digitally reconstruct the surfaces of a fracture based on measured topographic data and calculate the aperture when the upper halve of the fracture can steadily sit on the lower halve 2 use the function proposed by bandis et al 1983 to approximate the normal stress normal displacement curves obtained from normal loading unloading tests and b0 is obtained when a best fit is achieved δbn is produced in the initial normal loading process prior to shear which becomes 0 if the normal stress is a constant in the following shear process i e shear tests under a constant normal load cnl boundary condition δbs is the measured normal displacement dilation during shear which is 0 when shear has not been applied and may gain minus values when a shear is just initiated as shown in fig 1 a a dfn model subjected to some newly applied stress increments i e the horizontal stress δσ x and the vertical stress δσ y may result in the deformation of fractures in both the shear and normal directions for a deformed single fracture the shear stress gives rise to an incremental shear displacement δu s and an incremental normal displacement δv s due to the shear induced dilation see fig 1 b while the normal stress yields the compression of the fracture with a total deformation of δv n see fig 1 c note that δv n includes both the deformation of the fracture and the deformation of the blocks that form the fracture the pure normal deformation of the fracture could be obtained by eliminating the deformation of the blocks with the use of elastic theories fig 1 d shows the variations of shear stress and normal displacement with u s 0 18 mm for the fractures having a jrc joint roughness coefficient barton 1973 value of 12 14 under the initial normal stresses of 1 mpa black and red curves and 2 mpa gray and blue curves respectively fig 1 e shows the fracture closure subjected to normal stresses where k n0 is the initial stiffness and v nmax is the maximum fracture closure with increasing the normal stress σ n v n increases significantly and then gently approaching the maximum value these figures present the representative behaviors of rock fractures subjected to shear which are essential for establishing some closed form solutions for hydro mechanical behaviors of dfns in the present study due to the difficulties mentioned below a simplified form of shear behavior was applied to dfns past results revealed a three stage evolution behavior of hydraulic aperture transimissivity during shear processes xiong et al 2011 1 a declining stage due to minus dilation contraction of fractures 2 a fast growth stage during which the hydraulic aperture increases approximately linearly with the shear induced dilation 3 a gentle growth stage where the hydraulic aperture continues to increase but at a much lower and decreasing rate the evolutions of the total volume of voids and the geometry and distribution of single voids during shear are responsible for such three stage behavior however this behavior has not been mathematically modeled it should be noted that for parallel plate model hydraulic aperture is essentially identical to mechanical aperture and the aperture change during shear is identical to the value of dilation normal displacement which is a simplified model to represent the behavior of natural rock fractures with rough surfaces but is still widely used due to its simplicity some attempts have been made to establish their mathematical relations for natural rock fractures as tabulated in table 1 which indicated that the surface roughness of fractures and reynolds number of flow are primary factors that contribute to the deviation between the mechanical and hydraulic apertures the normal displacement shear displacement mechanical aperture and hydraulic aperture are four independent parameters they are basically measured by independent devices methods in tests that characterize the reaction of a fracture to a coupled shear flow process among which the first three are mechanical response of a fracture to the applied normal and shear stresses and the last one is the resulting hydraulic property change by a shear the roughness parameters that have been successfully applied to assess the mechanical behavior e g jrc may not efficiently apply to the assessment of hydraulic behavior how to establish a universal roughness assessment method that could mathematically link the mechanical behavior to the hydraulic behavior remains as an open question meanwhile to consider the effect of shear on dfns highly sophisticated numerical models are needed to allow the large shear deformation of the model while computing the mechanical behavior of intact rocks and fractures simultaneously some commercial codes can realize such modeling and computation however they cannot realistically model the surface roughness of fractures and the nonlinear fluid flow behavior given these reasons in the following investigations on hydraulic properties of sheared dfns the following assumptions were adopted to simplify the modeling 1 the permeability of the intact rock is negligibly small and the fluid flow in rocks is governed by the connected fractures 2 the fluid flows from the opposite two sides of a rectangular model while the other two opposite sides that are perpendicular to the flow direction are impermeable 3 the 2d dfn model is utilized to calculate the hydraulic properties of 3d fractured rock masses due to its better computational efficiency compared with the 3d dfn model huang et al 2017 have reported that the difference between the permeability of 3d dfn model and its cut planes is within 1 order of magnitude and their values can be mathematically linked 4 the sheared fracture is formed by parallel plates that has a constant aperture other fractures can have variable apertures this assumption has to be made because the flow path of a sheared fracture with rough surfaces in a 2d model can be easily blocked by contacting asperities 5 since the sheared fracture was not generated based on a specific surface geometry its normal displacement and shear displacement during shear did not follow any laws but were assigned selected values this assumption indicates that the present investigation is a parametric study that aims to gain generic tendencies of hydraulic response of dfns to shear it would lack generality if measured displacements of sheared rock fractures are directly applied which are typically available only for small sized laboratory samples to field scale dfn models site specific investigations could be further put forward based on the presented numerical approach 3 numerical method and dfn models 3 1 governing equations for flow in fractures the flow of a viscous incompressible newtonian fluid can be simulated by solving the well known navier stokes ns equations in a vector form as follows 2 ρ u t u u p t ρ f where u is the flow velocity p is the hydraulic pressure ρ is the fluid density f is the body force t is the time and t is the shear stress tensor numerous previous studies have assumed that rock fractures can be simplified to a pair of parallel plates in which the ns equations can be simplified to the cubic law based eq 3 at a sufficiently low re e g bear 1972 witherspoon et al 1980 zimmerman and bodvarsson 1996 3 j 12 μ ρ 1 g 1 e 3 q where j is the hydraulic gradient q is the flow rate μ is the dynamic viscosity g is the gravitational acceleration and e is the hydraulic aperture of fracture eq 3 implies that j is linearly proportional to q which is true when the inertial effects of flow are negligible in a nonlinear flow regime their relation can be macroscopically characterized by a zero intercept quadratic expression the forchheimer s law as forchheimer 1901 zimmerman et al 2004 liu et al 2016b 4 j a q b q 2 where a and b are the linear and nonlinear coefficients respectively and a 12μρ 1 g 1 e 3 3 2 dfn generation and fluid flow simulation 3 d fracture network modeling has been increasingly utilized to study hydraulic characteristics of fractured rock masses due to its outstanding advances of realistically representing the geometric properties of fractures comparing to 2d models de dreuzy et al 2012 lang et al 2014 huang et al 2016 however in the previous studies for fluid flow through 3 d fracture networks the flow was typically restricted to linear regime because it is still a challenging task to model nonlinear flow through 3 d fracture networks due to enormous difficulties in establishing models considering the roughness and aperture heterogeneity of single fractures and solving the ns equations composed of a set of coupled nonlinear partial derivatives of varying orders brush and thomson 2003 javadi et al 2010 therefore in the present study as an initial attempt 2 d dfns were established and utilized to investigate the effects of shear on equivalent permeability and nonlinear flow characteristics three 2 d dfn models with a side length of 5 m were generated and then smaller dfns with a side length of 1 m were extracted from the center of these models for fluid flow simulations figs 2 a c and d f show the dfns with side lengths of 5 m and 1 m respectively these dfns figs 2 a c or d f have an identical geometric distribution of fractures differing in the surface roughness and aperture heterogeneity of fractures the dfns shown in the left column of fig 2 contain fractures with smooth parallel surfaces i e the smooth model which is a typical model for representation of fracture geometry employed in most previous studies the dfns shown in the middle column of fig 2 contain fractures with rough and mated surfaces i e the mated model which added the feature of surface roughness to the smooth model the dfns shown in the right column of fig 2 contain fractures with rough and unmated surfaces i e the unmated model which further added the variation of aperture in space to each single fracture note that a cutting though fracture located on the vertical axis exists in each dfn which is artificially sheared to investigate the hydraulic response of dfns to shear that is described in detail in the section 4 2 a dfngen code was developed to facilitate the dfn generation process as follows first a straight line was generated by connecting the starting point to the ending point of a fracture and was divided into 100 segments with 99 sub points second the 99 sub points were shifted by a distance of 0 003 l u along the direction perpendicular to the straight line to generate a zigzag line rough surface where l is the length of the straight line and u is a random number uniformly distributed in the range of 0 1 third this zigzag line was copied and was moved perpendicularly by a distance of b m to the original line these two zigzag lines form the mated walls of a rough fracture in a mated model fourth each sub point was shifted again along the direction perpendicular to the original straight line at a distance varying between b m 2 b m 2 the negative sign means that the sub point moves towards the inner space of the fracture and the positive sign means that the sub point moves towards the outer space the shifted distance was determined by multiplying b m 2 by u since u was generated randomly the aperture at different sub points was different note that the maximum shifted distance is less than b m 2 which guaranteed that the two walls of a fracture do not contact with each other to maintain a connected flow path if no shifts were applied to sub points of the original straight line then fractures that have smooth surfaces i e the smooth model can be generated in the generated dfns the value of z 2 of single fractures with rough fracture surfaces varies from 0 30 to 0 44 corresponding to the jrc values from 15 22 to 20 62 according to the correlation proposed by tse and cruden 1979 which represent considerably rough natural fracture surfaces here z 2 and jrc are typical parameters used for characterizing the roughness of 2d profiles of rock fractures barton 1973 zhao et al 2014 the number of segments or sub points of each fracture can be altered to meet the precision requirements in reconstructing the surface of fractures 100 segments were selected here as an example to represent the primary roughness of fracture surfaces the straight length of each fracture which is the length of the line formed by connecting the starting point to the ending point was presumed to follow a power law function written as davy 1993 bour and davy 1997 odling 1997 de dreuzy et al 2002 5 n l α l a l l min l max where n l is the number of fractures corresponding to fracture lengths in the range of l l dl a is the power law exponent α is the normalization factor and l min and l max are the minimum and the maximum values of fracture length respectively the orientation of fractures followed the fisher distribution min and jing 2003 min et al 2004 baghbanan and jing 2007 2008 where the deviated angle θ from the mean orientation angle of fractures was calculated by 6 θ cos 1 f c 1 ln e f c u e f c e f c where f c is the fisher constant the mean mechanical aperture of each fracture was linked with the straight length l of a fracture following a square root correlation written as vermilye and scholz 1995 olson 2003 klimczak et al 2010 7 b m α l 0 5 where α is the proportionality coefficient for the original models with a side length of 5 m α 0 0025 a 1 7 l min 1 2 m f c 8 the number density of fractures is 3 6 m 2 and the original orientations of three fracture sets are 0 60 and 120 which are typical values utilized in dfn generation e g de dreuzy et al 2002 klimczak et al 2010 liu et al 2016d these models were generated based on well accepted geometric distributions of fractures and the variation of fracture aperture in space was also considered therefore comparing with many 2 d dfn models presented in literature e g min et al 2004 baghbanan and jing 2007 klimczak et al 2010 these models with rough surfaces can more realistically represent the geometric properties of fractured rock masses all of these generation method and fracture distribution models mentioned above have been integrated into dfngen and dfns can be easily generated by inputting adequate values of parameters numerical simulations were performed based on the fvm code anasys fluent module fluent 2006 by directly solving the ns equations more details of the fvm code and the meshing techniques as well as the validation process of applying this code to dfns are described in related publications li et al 2016 liu et al 2016b c fluid flowed through the dfns from the left boundary towards the right boundary and the top and bottom boundaries were impermeable as shown in fig 3 the applied j varied in a wide range from 10 8 to 10 which covered most possible ranges of critical hydraulic gradient jc for determining the flow regime transition in dfns liu et al 2016b c note that j which is the ratio of hydraulic head difference to the dfn side length is utilized in the present study rather than re that is typically employed in the characterization of nonlinear flow through single fractures because it is a tough task to estimate the local values of re for a large number of fractures in dfns in contrast j is a macroscopic parameter and its value is commonly known in practices such as in situ hydraulic pumping tests liu et al 2016b here the dfns with side length sl 1 m were simulated and the dfns with sl 5 m were not calculated due to the limitation of computational capacity of the computer employed the scale effect of dnfs which has been extensively studied before e g min et al 2003 2004 is not investigated in the present study as an example there were 489 514 meshes in the dfn model shown in fig 2 f and the calculation took approximately 4 hours using a personal computer with an i7 core to achieve a steady state fluid flow under which the fluid rate permeability could be calculated deviations among results computed with different mesh sizes were analyzed to help select a suitable one 4 results and analysis 4 1 nonlinear flow in dfns by applying different values of j to the dfns the relation between j and q was obtained as shown in fig 4 it is observed that j has a quadratic relationship with q which can be well fitted by eq 4 indicating that the cubic law is not applicable and the value of q j would vary with the increment of j at a certain j the value of q calculated by the smooth model is larger than those calculated by the mated and unmated models because the rough fracture surface gives rise to the longer flow paths and greater frictional loss that tend to reduce the flow rate the inertial effects in the unmated model are stronger than those in the mated model due to the enhanced variation of geometry of flow paths by aperture variation in space therefore when j is large e g j 0 7 the value of q calculated by the unmated model is smaller than that calculated by the mated model these results show that the inertial effects can be significantly enhanced by geometric variations of single fractures which are in concert with many previously reported results e g zimmerman et al 2004 javadi et al 2010 2014 xiong et al 2011 zou et al 2015 according to the cubic law based eq 3 in which j is linearly correlated with q the value of q j is a constant value therefore the transition from a linear flow regime to a nonlinear flow regime should be the turning point at which the value of q j starts to change with the increasing j the variations of q j of different models against j are depicted in fig 5 when j is small i e j 10 5 q j holds constant values however when j 10 4 with the increasing j the value of q j decreases dramatically to quantitatively estimate the turning point and determine a critical hydraulic gradient jc the relative deviation rate of q j δ 1 is defined as follows 8 δ 1 q j cubic q j ns q j cubic 100 where q j cubic is the value of q j calculated by solving the cubic law and q j ns is the value of q j calculated by solving the ns equations the forchheimer number f 0 which quantifies the ratio of nonlinear to linear pressure losses was also calculated to help understand the nonlinear flow behavior defined as 9 f 0 b q 2 a q b q a fig 6 shows the relations between δ 1 and f 0 with j when j ranges from 10 8 to 10 if δ 1 10 is taken as a threshold to identify the flow regime transition it was found that δ 1 10 was satisfied when j 10 4 while δ 1 10 was confirmed when j 10 3 therefore for the studied models the critical condition for quantifying the flow regime transition is 10 4 jc 10 3 before reaching 10 2 the value of f 0 is negligible after that f 0 increases abruptly and the unmated model gains greater values than the other two models showing that the surface geometry could largely enhance the nonlinearity of fluid flow in a dfn although both δ 1 and f 0 are used to quantify the degree of nonlinearity of flow they have different physical meanings therefore they cannot be directly compared the differences in fracture surface geometries among three kinds of models do not seem to have large impacts on this critical condition this is reasonable because in a dfn when a fluid flows from one fracture to an intersected fracture the main flow direction could be changed up to nearly 180 such variation of flow direction almost happens at every intersection with varying degrees which can significantly enhance the nonlinear flow in a system comparing to this the surface roughness brings much smaller variations to the direction of flow and in most conditions only influences the flow layers close to the fracture wall with the main flow stream undisturbed as an extreme case in the smooth model the nonlinear flow only originates from the geometric variation at intersections since jc quantifies the regime transition of a whole dfn instead of a single fracture it is governed by the macroscopic geometric characters i e fracture backbone geometry rather than the microscopic geometric characters i e surface roughness of single fractures when only single fractures are concerned the surface roughness will become a governing parameter 4 2 effects of shear on equivalent permeability to simplify the modeling of shear a series of values were assigned to the normal displacement and the shear displacement to artificially shear the cutting through fracture located on the vertical axis of each model as shown in fig 7 as a result the two blocks on the left and right sides of the sheared fracture were separated by a normal distance of d n shear induced dilation and slid by a shear displacement of d s here since the fracture is artificially sheared d n and d s instead of v and u are used to represent the normal and shear displacements respectively the initial aperture of the sheared fracture was assumed to be 0 to facilitate the calculation with such a configuration these models represent a situation that a fractured rock mass is sheared along a major fracture plane with other fractures undisturbed this is frequently encountered in the field when for example a rock mass is sheared by earthquakes the large dfns 5 m in length were firstly sheared and then smaller dfns were extracted to maintain continuous boundaries numerical simulations by solving the ns equations were performed to calculate the equivalent permeability with j 10 8 10 d s 1 7 mm and d n 1 5 mm the interval for hydraulic gradient is one order and for displacement is 1 or 2 mm leading to 144 simulation cases in total and around 500 cpu hours the values of displacements were selected based on the results of previous laboratory coupled shear flow tests on single fractures li et al 2008 since the size of the sheared fracture 1 m is 5 times of the samples 20 cm used in the previous tests the normal displacement of average testing results around 1 mm was magnified by a factor of 5 leading to the maximum normal displacement of 5 mm the maximum shear displacement of 7 mm was selected to ensure that each fracture intersecting the sheared fracture has been sheared by a distance larger than its own aperture and that the residual stage of shear has been reached as a result the model can be completely sealed when d n 0 mm and d s 7 mm only horizontal flow was simulated fig 3 and only the results of the unmated model are discussed below because the other two models show almost identical response to shear enlarged views of fluid flow in the rectangular region in fig 7 b are shown in figs 8 a i when j 10 4 the fluid flow is in a linear regime and the flow vectors are distributed parabolically in the direction perpendicular to flow see figs 8 j o such parabolic streamlines were disturbed at the sheared intersection and recovered after travelling through the intersection when j increases to 10 2 some secondary vortices can be observed in the regions near the intersection as shown in figs 8 h i the flow vectors can no longer regain parabolic distributions after passing through the intersection see figs 8 n o demonstrating strong nonlinear flow characters induced by the geometric variation of the sheared fracture the combination of normal and shear displacement can significantly change the flow patterns across the sheared fracture and result in pressure dissipation at high hydraulic gradients the shear induced dislocation of fractures may generate bottle necks that may restrict fluid from flowing through the model especially when d s is large and d n is small to quantitatively investigate the effects of shear on the macroscopic hydraulic properties of dfns the relative deviation rate of equivalent permeability δ 2 is defined as follows 10 δ 2 k 0 k k 0 100 where k 0 is the equivalent permeability of the dfns with d n 0 mm and d s 0 mm and k is the equivalent permeability of the dfns with any given shear and normal displacements larger than 0 here k was back calculated according to the following equation 11 q a k μ p s l where a is the cross sectional area and sl is the dfn side length fig 9 shows the relationship between δ 2 and j for selected combinations of d n 1 2 3 5 mm and d s 1 3 5 7 mm cautions should be paid that the combinations of d n d s are unrealistic because d n is typically smaller than d s in most circumstances for natural rock fractures subject to shear here results of all combinations are presented only for the purpose of comparison when d s 1 mm δ 2 holds almost constant values at different values of j and d s suggesting that the permeability is not affected by shear at a sufficiently small shear displacement the change of δ 2 becomes obvious when d s becomes large where δ 2 decreases with the increment of d n suggesting that the shear induced permeability change may vanish at a sufficiently large d n when fluid flow is in the linear regime j 10 4 δ 2 is approximately linearly correlated with j with small variations while for fluid flow in the nonlinear regime j 10 3 δ 2 is nonlinearly correlated with j it is evident that the effects of shear on permeability change become remarkable at sufficiently large values of d s and small values of d n i e d n 1 mm and d s 7 mm because the sheared fracture under such conditions would easily form bottle necks that could dramatically reduce the effective flow paths and the equivalent permeability although not calculated it is predictable that the permeability in the orientation parallel to that of the sheared fracture will increase due to the expansion of flow path originated from the shear induced dilation analogous to the permeability rise in sheared single fractures such anisotropic hydraulic behavior needs to be quantitatively estimated in future studies 4 3 application of δ 2 the results in fig 9 show that δ 2 is correlated with d s and d n especially when j is large by fitting these data it is found that for a certain j ranging from 10 3 to 10 δ 2 can be calculated according to the following equation 12 δ 2 d d s f g d n where d f and g are coefficients with their values tabulated in table 2 here δ2 is an independent variable and d s and d n are two dependent variables the cases with j 10 3 are not considered due to the small deviations of δ2 especially when d s is less than 5 mm the variations of d n and d s and simulated and predicted results of δ2 are presented in table 3 and fig 10 the results show that eq 11 agrees well with the results of numerical simulations when j 10 3 10 d n 1 5 mm and d s 1 7 mm the correlation coefficients for all the cases are r2 0 99 indicating that eq 11 is sufficiently reliable to predict the effects of shear on the variations of equivalent permeability fig 11 shows that with the increment of j from 10 3 to 10 d and g increase from 0 0137 to 0 9183 and from 0 4441 to 0 7102 respectively whereas f decreases from 4 3096 to 2 2553 by fitting these parameters three regression functions are proposed written as 13 d 0 9223 0 9960 1 j j 0 2954 14 f 2 2476 1 0002 1 j j 0 06560 15 g 0 7134 0 9997 1 j j 0 02537 for a fractured rock masses when the shear displacement d s and normal displacement d n of a sheared fracture as well as the hydraulic gradient j applied on the two opposing boundaries are known the relative deviation rate of equivalent permeability δ 2 can be estimated by using eqs 11 14 if δ 2 is larger than a prescript critical condition i e 10 the navier stokes equations should be solved to address the nonlinear flow characterizations if δ 2 is smaller than the critical condition the hydraulic response i e permeability can be calculated using the cubic law as a first order estimation therefore the proposed eq 11 can help choose proper governing equations for fluid flow calculations in rock fractures 5 conclusions in the present study a numerical approach for establishing 2d fracture networks with improved representation of fracture surface geometries that enable the application of normal and shear displacements and the fluid flow simulation by solving navier stokes equations is presented the hydro mechanical behavior of single rock fractures during shear has been extensively investigated in previous studies however similar investigations on the effects of shear on the hydraulic properties of dfns have rarely been reported nonlinear flow behavior is emphasized here because the enhanced permeability by shear induced dilation could facilitate the transition of fluid flow from linear to nonlinear regimes which has been confirmed in coupled shear flow tests on single fractures the simulation results revealed a critical condition for quantifying the transition of flow from a linear flow regime to a nonlinear regime for the studied models as 10 4 j 10 3 where j is the hydraulic gradient comparisons among the smooth model the mated model and the unmated model showed that the surface roughness and the variation of aperture in space enhance the inertial effects of flow and thereby reducing the equivalent permeability their influences on the critical condition for flow regime transition are negligibly small because such critical condition for dfns is governed by the macroscopic geometric characters i e fracture backbone geometry rather than the microscopic geometric characters i e surface roughness of fracture networks the normal and shear displacements add further geometric variations to the original models which may facilitate the formation of bottle necks that could restrict fluid from flowing through the model especially when d s is large and d n is small the shear induced dislocation of flow paths significantly changes the flow patterns in and around the sheared intersections which will result in pressure dissipation at high hydraulic gradients the expression for predicting the relative deviation rate of equivalent permeability is proposed which can help choose proper governing equations for fluid flow in rock fracture networks as long as the associated parameters such as shear and normal displacements and hydraulic gradient are known the present study established several dfn models with one kind of fracture distribution and limited values of normal and shear displacements which is therefore merely an initial step in such explorations on the concerned topic more sophisticated models with mathematically correlated normal and shear displacements and coupled hydro mechanical model that takes into account the deformability and strength of both intact rocks and fractures need to be incorporated into the proposed numerical approach to improve the understanding on shear induced hydraulic property changes of dfns while shear can augment the permeability of dfns in the orientation parallel to the sheared fracture the permeability in the perpendicular orientation is primarily subjected to reduction which may result in a strong anisotropy in fluid flow behaviors of fractured rock masses reduction of permeability is beneficial for the projects like geometric repository of high level radioactive wastes while the opposite is expected for the projects like shale gas production using hydraulic fracturing therefore macroscopic parameters such as permeability tensor and microscopic parameters such as contact area ratio in sheared fractures need to be more thoroughly investigated using fully coupled hydro mechanical models in the future works acknowledgments this study has been partially funded by national natural science foundation of china china grant nos 51609136 51709260 national science foundation of jiangsu province china grant no bk20170276 and the zhejiang collaborative innovation center for prevention and control of mountain geological hazards china grant no pcmgh 2016 z 01 
904,hydro mechanical properties of rock fractures are core issues for many geoscience and geo engineering practices previous experimental and numerical studies have revealed that shear processes could greatly enhance the permeability of single rock fractures yet the shear effects on hydraulic properties of fractured rock masses have received little attention in most previous fracture network models single fractures are typically presumed to be formed by parallel plates and flow is presumed to obey the cubic law however related studies have suggested that the parallel plate model cannot realistically represent the surface characters of natural rock fractures and the relationship between flow rate and pressure drop will no longer be linear at sufficiently large reynolds numbers in the present study a numerical approach was established to assess the effects of shear on the hydraulic properties of 2 d discrete fracture networks dfns in both linear and nonlinear regimes dfns considering fracture surface roughness and variation of aperture in space were generated using an originally developed code dfngen numerical simulations by solving navier stokes equations were performed to simulate the fluid flow through these dfns a fracture that cuts through each model was sheared and by varying the shear and normal displacements effects of shear on equivalent permeability and nonlinear flow characteristics of dfns were estimated the results show that the critical condition of quantifying the transition from a linear flow regime to a nonlinear flow regime is 10 4 j 10 3 where j is the hydraulic gradient when the fluid flow is in a linear regime i e j 10 4 the relative deviation of equivalent permeability induced by shear δ 2 is linearly correlated with j with small variations while for fluid flow in the nonlinear regime j 10 3 δ 2 is nonlinearly correlated with j a shear process would reduce the equivalent permeability significantly in the orientation perpendicular to the sheared fracture as much as 53 86 when j 1 shear displacement d s 7 mm and normal displacement d n 1 mm by fitting the calculated results the mathematical expression for δ 2 is established to help choose proper governing equations when solving fluid flow problems in fracture networks keywords fracture network nonlinear flow shear navier stokes equations surface roughness aperture heterogeneity 1 introduction rock fractures play an important role in the hydro mechanical properties of tight rock masses e g granite and basalt due to their typically stronger permeability comparing with rock matrix koyama et al 2008a blum et al 2009 stresses acting on a rock mass can change the aperture of single fractures involved resulting in the change of macro permeability as well as the redistributions of local hydraulic pressure and flow field min and jing 2003 baghbanan and jing 2008 the discrete fracture network dfn method has achieved great successes in simulating fluid flow in fractured rock masses in which single fractures are typically presumed to be formed by parallel plates and flow is presumed to obey the cubic law where the flow rate is linearly correlated with the pressure drop bear 1972 liu et al 2016a cai et al 2017 however it is well known that the parallel plate model cannot realistically represent the surface characters of natural rock fractures and reflect their influences on fluid flow behavior and the relationship between flow rate and pressure drop is not linear at sufficiently large reynolds numbers re zimmerman et al 2004 chen et al 2015 during the past few decades great efforts have been devoted to investigations on the effects of stress on hydraulic properties of single fractures e g yeo et al 1998 esaki et al 1999 olsson and barton 2001 auradou et al 2005 auradou et al 2006 li et al 2008 indraratna et al 2015 and fracture networks e g zhang and sanderson 1996 baghbanan and jing 2008 and on the nonlinear flow characteristics of single fractures e g zimmerman et al 2004 javadi et al 2010 2014 xiong et al 2011 cherubini et al 2012 2013 zhang and nemcik 2013 zou et al 2015 wang et al 2016 fracture intersections e g kosakowski and berkowitz 1999 li et al 2016 and fracture networks e g kolditz 2001 liu et al 2016b c these studies have greatly improved the understanding on the influences of several important factors such as boundary stress fracture geometry e g aperture shape of intersections and surface roughness reynolds number fracture density and etc on the fluid flow behavior of single fractures and fracture networks for investigations via dfn modelling normal stresses confining stress are typically imposed on the model boundaries which mainly result in the closure of single fractures baghbanan and jing 2008 meanwhile past studies have revealed that the permeability change of fractures caused by shear can be several orders greater than that caused by normal stresses li et al 2008 shear displacements of rocks can either result from engineering practices such as excavations in rock masses or from natural forces such as earthquakes although the shear induced permeability change of single rock fractures has been extensively explored studies on the effects of shear on fluid flow characteristics of dfns have not if any been reported since shear can significantly enhance the permeability of fractures and thereby facilitating the transition of flow from linear to nonlinear regimes the effects of shear and the nonlinear flow behavior need to be simultaneously considered in dfns recent studies have revealed that the nonlinear flow could occur for sheared fractures when the magnitude of re is as low as 0 01 javadi et al 2014 the nonlinear flow in rock fractures is typically described using the forchheimer s law forchheimer 1901 which can be applied over the entire range of flow rate velocity including re 1 another example of typically used equations is the izbash s law izbash 1931 although it does not have physically sensible behavior at low re the critical reynolds number re c is a widely used parameter for identifying the onset of nonlinear flow in single fractures which is determined when the nonlinear term or the inertial force contributes to a certain of the total pressure drop such as 10 zimmerman et al 2014 chen et al 2015 5 wang et al 2016 and 1 liu et al 2016b however for complex dfns comprising a great amount of fractures calculation of re for each single fracture may become a time consuming and technically difficult problem instead the critical hydraulic gradient which is defined as the ratio of hydraulic head difference to the dfn model side length is utilized as a macroscopic parameter that commonly has known values in many practices on fractured rock masses gale 1984 li et al 2016 to efficiently address the shear effects and nonlinear flow behavior of dfns simultaneously in the present study a code dfngen was developed to generate 2 d dfns composed of fractures with geometries that follow well accepted mathematical expressions a single fracture that cuts through each dfn was artificially sheared by assigning different values of normal displacement and shear displacement fluid flow simulations were implemented on the original and the sheared dfns utilizing a fvm finite volume method code to investigate the influences of shear on both linear and nonlinear flow behaviors of dfns 2 coupled shear flow behavior of fractures the aperture and its evolution which are key issues to the fluid flow characteristics of fractures can be more significantly changed by a shear process than that by a normal loading unloading process due mainly to the shear induced dilation upon rough surfaces li et al 2008 koyama et al 2008b for originally well mated fractures the reduced matedness during a shear process could be another reason that alters their hydraulic properties xiong et al 2011 a mechanical aperture and a hydraulic aperture were commonly utilized to quantify the aperture variations during the coupled hydro mechanical processes the mechanical aperture is typically defined as the mean point to point distance between the two walls of a fracture and the hydraulic aperture is typically back calculated from the pressure drop flow rate relationship obtained from hydraulic tests or numerical simulations using the cubic law the hydraulic aperture should be calculated at a sufficient low re because the cubic law does not hold when the fluid flow is not in the linear regime in a shear process the mechanical aperture of a fracture bm can be estimated by li et al 2008 1 b m b 0 δ b n δ b s where b0 is the initial aperture δbn is the variation of aperture during normal loading or unloading such as closure or opening δbs is the variation of aperture during shear dilation b0 is the aperture when no stresses are applied which can be mainly obtained via two approaches 1 digitally reconstruct the surfaces of a fracture based on measured topographic data and calculate the aperture when the upper halve of the fracture can steadily sit on the lower halve 2 use the function proposed by bandis et al 1983 to approximate the normal stress normal displacement curves obtained from normal loading unloading tests and b0 is obtained when a best fit is achieved δbn is produced in the initial normal loading process prior to shear which becomes 0 if the normal stress is a constant in the following shear process i e shear tests under a constant normal load cnl boundary condition δbs is the measured normal displacement dilation during shear which is 0 when shear has not been applied and may gain minus values when a shear is just initiated as shown in fig 1 a a dfn model subjected to some newly applied stress increments i e the horizontal stress δσ x and the vertical stress δσ y may result in the deformation of fractures in both the shear and normal directions for a deformed single fracture the shear stress gives rise to an incremental shear displacement δu s and an incremental normal displacement δv s due to the shear induced dilation see fig 1 b while the normal stress yields the compression of the fracture with a total deformation of δv n see fig 1 c note that δv n includes both the deformation of the fracture and the deformation of the blocks that form the fracture the pure normal deformation of the fracture could be obtained by eliminating the deformation of the blocks with the use of elastic theories fig 1 d shows the variations of shear stress and normal displacement with u s 0 18 mm for the fractures having a jrc joint roughness coefficient barton 1973 value of 12 14 under the initial normal stresses of 1 mpa black and red curves and 2 mpa gray and blue curves respectively fig 1 e shows the fracture closure subjected to normal stresses where k n0 is the initial stiffness and v nmax is the maximum fracture closure with increasing the normal stress σ n v n increases significantly and then gently approaching the maximum value these figures present the representative behaviors of rock fractures subjected to shear which are essential for establishing some closed form solutions for hydro mechanical behaviors of dfns in the present study due to the difficulties mentioned below a simplified form of shear behavior was applied to dfns past results revealed a three stage evolution behavior of hydraulic aperture transimissivity during shear processes xiong et al 2011 1 a declining stage due to minus dilation contraction of fractures 2 a fast growth stage during which the hydraulic aperture increases approximately linearly with the shear induced dilation 3 a gentle growth stage where the hydraulic aperture continues to increase but at a much lower and decreasing rate the evolutions of the total volume of voids and the geometry and distribution of single voids during shear are responsible for such three stage behavior however this behavior has not been mathematically modeled it should be noted that for parallel plate model hydraulic aperture is essentially identical to mechanical aperture and the aperture change during shear is identical to the value of dilation normal displacement which is a simplified model to represent the behavior of natural rock fractures with rough surfaces but is still widely used due to its simplicity some attempts have been made to establish their mathematical relations for natural rock fractures as tabulated in table 1 which indicated that the surface roughness of fractures and reynolds number of flow are primary factors that contribute to the deviation between the mechanical and hydraulic apertures the normal displacement shear displacement mechanical aperture and hydraulic aperture are four independent parameters they are basically measured by independent devices methods in tests that characterize the reaction of a fracture to a coupled shear flow process among which the first three are mechanical response of a fracture to the applied normal and shear stresses and the last one is the resulting hydraulic property change by a shear the roughness parameters that have been successfully applied to assess the mechanical behavior e g jrc may not efficiently apply to the assessment of hydraulic behavior how to establish a universal roughness assessment method that could mathematically link the mechanical behavior to the hydraulic behavior remains as an open question meanwhile to consider the effect of shear on dfns highly sophisticated numerical models are needed to allow the large shear deformation of the model while computing the mechanical behavior of intact rocks and fractures simultaneously some commercial codes can realize such modeling and computation however they cannot realistically model the surface roughness of fractures and the nonlinear fluid flow behavior given these reasons in the following investigations on hydraulic properties of sheared dfns the following assumptions were adopted to simplify the modeling 1 the permeability of the intact rock is negligibly small and the fluid flow in rocks is governed by the connected fractures 2 the fluid flows from the opposite two sides of a rectangular model while the other two opposite sides that are perpendicular to the flow direction are impermeable 3 the 2d dfn model is utilized to calculate the hydraulic properties of 3d fractured rock masses due to its better computational efficiency compared with the 3d dfn model huang et al 2017 have reported that the difference between the permeability of 3d dfn model and its cut planes is within 1 order of magnitude and their values can be mathematically linked 4 the sheared fracture is formed by parallel plates that has a constant aperture other fractures can have variable apertures this assumption has to be made because the flow path of a sheared fracture with rough surfaces in a 2d model can be easily blocked by contacting asperities 5 since the sheared fracture was not generated based on a specific surface geometry its normal displacement and shear displacement during shear did not follow any laws but were assigned selected values this assumption indicates that the present investigation is a parametric study that aims to gain generic tendencies of hydraulic response of dfns to shear it would lack generality if measured displacements of sheared rock fractures are directly applied which are typically available only for small sized laboratory samples to field scale dfn models site specific investigations could be further put forward based on the presented numerical approach 3 numerical method and dfn models 3 1 governing equations for flow in fractures the flow of a viscous incompressible newtonian fluid can be simulated by solving the well known navier stokes ns equations in a vector form as follows 2 ρ u t u u p t ρ f where u is the flow velocity p is the hydraulic pressure ρ is the fluid density f is the body force t is the time and t is the shear stress tensor numerous previous studies have assumed that rock fractures can be simplified to a pair of parallel plates in which the ns equations can be simplified to the cubic law based eq 3 at a sufficiently low re e g bear 1972 witherspoon et al 1980 zimmerman and bodvarsson 1996 3 j 12 μ ρ 1 g 1 e 3 q where j is the hydraulic gradient q is the flow rate μ is the dynamic viscosity g is the gravitational acceleration and e is the hydraulic aperture of fracture eq 3 implies that j is linearly proportional to q which is true when the inertial effects of flow are negligible in a nonlinear flow regime their relation can be macroscopically characterized by a zero intercept quadratic expression the forchheimer s law as forchheimer 1901 zimmerman et al 2004 liu et al 2016b 4 j a q b q 2 where a and b are the linear and nonlinear coefficients respectively and a 12μρ 1 g 1 e 3 3 2 dfn generation and fluid flow simulation 3 d fracture network modeling has been increasingly utilized to study hydraulic characteristics of fractured rock masses due to its outstanding advances of realistically representing the geometric properties of fractures comparing to 2d models de dreuzy et al 2012 lang et al 2014 huang et al 2016 however in the previous studies for fluid flow through 3 d fracture networks the flow was typically restricted to linear regime because it is still a challenging task to model nonlinear flow through 3 d fracture networks due to enormous difficulties in establishing models considering the roughness and aperture heterogeneity of single fractures and solving the ns equations composed of a set of coupled nonlinear partial derivatives of varying orders brush and thomson 2003 javadi et al 2010 therefore in the present study as an initial attempt 2 d dfns were established and utilized to investigate the effects of shear on equivalent permeability and nonlinear flow characteristics three 2 d dfn models with a side length of 5 m were generated and then smaller dfns with a side length of 1 m were extracted from the center of these models for fluid flow simulations figs 2 a c and d f show the dfns with side lengths of 5 m and 1 m respectively these dfns figs 2 a c or d f have an identical geometric distribution of fractures differing in the surface roughness and aperture heterogeneity of fractures the dfns shown in the left column of fig 2 contain fractures with smooth parallel surfaces i e the smooth model which is a typical model for representation of fracture geometry employed in most previous studies the dfns shown in the middle column of fig 2 contain fractures with rough and mated surfaces i e the mated model which added the feature of surface roughness to the smooth model the dfns shown in the right column of fig 2 contain fractures with rough and unmated surfaces i e the unmated model which further added the variation of aperture in space to each single fracture note that a cutting though fracture located on the vertical axis exists in each dfn which is artificially sheared to investigate the hydraulic response of dfns to shear that is described in detail in the section 4 2 a dfngen code was developed to facilitate the dfn generation process as follows first a straight line was generated by connecting the starting point to the ending point of a fracture and was divided into 100 segments with 99 sub points second the 99 sub points were shifted by a distance of 0 003 l u along the direction perpendicular to the straight line to generate a zigzag line rough surface where l is the length of the straight line and u is a random number uniformly distributed in the range of 0 1 third this zigzag line was copied and was moved perpendicularly by a distance of b m to the original line these two zigzag lines form the mated walls of a rough fracture in a mated model fourth each sub point was shifted again along the direction perpendicular to the original straight line at a distance varying between b m 2 b m 2 the negative sign means that the sub point moves towards the inner space of the fracture and the positive sign means that the sub point moves towards the outer space the shifted distance was determined by multiplying b m 2 by u since u was generated randomly the aperture at different sub points was different note that the maximum shifted distance is less than b m 2 which guaranteed that the two walls of a fracture do not contact with each other to maintain a connected flow path if no shifts were applied to sub points of the original straight line then fractures that have smooth surfaces i e the smooth model can be generated in the generated dfns the value of z 2 of single fractures with rough fracture surfaces varies from 0 30 to 0 44 corresponding to the jrc values from 15 22 to 20 62 according to the correlation proposed by tse and cruden 1979 which represent considerably rough natural fracture surfaces here z 2 and jrc are typical parameters used for characterizing the roughness of 2d profiles of rock fractures barton 1973 zhao et al 2014 the number of segments or sub points of each fracture can be altered to meet the precision requirements in reconstructing the surface of fractures 100 segments were selected here as an example to represent the primary roughness of fracture surfaces the straight length of each fracture which is the length of the line formed by connecting the starting point to the ending point was presumed to follow a power law function written as davy 1993 bour and davy 1997 odling 1997 de dreuzy et al 2002 5 n l α l a l l min l max where n l is the number of fractures corresponding to fracture lengths in the range of l l dl a is the power law exponent α is the normalization factor and l min and l max are the minimum and the maximum values of fracture length respectively the orientation of fractures followed the fisher distribution min and jing 2003 min et al 2004 baghbanan and jing 2007 2008 where the deviated angle θ from the mean orientation angle of fractures was calculated by 6 θ cos 1 f c 1 ln e f c u e f c e f c where f c is the fisher constant the mean mechanical aperture of each fracture was linked with the straight length l of a fracture following a square root correlation written as vermilye and scholz 1995 olson 2003 klimczak et al 2010 7 b m α l 0 5 where α is the proportionality coefficient for the original models with a side length of 5 m α 0 0025 a 1 7 l min 1 2 m f c 8 the number density of fractures is 3 6 m 2 and the original orientations of three fracture sets are 0 60 and 120 which are typical values utilized in dfn generation e g de dreuzy et al 2002 klimczak et al 2010 liu et al 2016d these models were generated based on well accepted geometric distributions of fractures and the variation of fracture aperture in space was also considered therefore comparing with many 2 d dfn models presented in literature e g min et al 2004 baghbanan and jing 2007 klimczak et al 2010 these models with rough surfaces can more realistically represent the geometric properties of fractured rock masses all of these generation method and fracture distribution models mentioned above have been integrated into dfngen and dfns can be easily generated by inputting adequate values of parameters numerical simulations were performed based on the fvm code anasys fluent module fluent 2006 by directly solving the ns equations more details of the fvm code and the meshing techniques as well as the validation process of applying this code to dfns are described in related publications li et al 2016 liu et al 2016b c fluid flowed through the dfns from the left boundary towards the right boundary and the top and bottom boundaries were impermeable as shown in fig 3 the applied j varied in a wide range from 10 8 to 10 which covered most possible ranges of critical hydraulic gradient jc for determining the flow regime transition in dfns liu et al 2016b c note that j which is the ratio of hydraulic head difference to the dfn side length is utilized in the present study rather than re that is typically employed in the characterization of nonlinear flow through single fractures because it is a tough task to estimate the local values of re for a large number of fractures in dfns in contrast j is a macroscopic parameter and its value is commonly known in practices such as in situ hydraulic pumping tests liu et al 2016b here the dfns with side length sl 1 m were simulated and the dfns with sl 5 m were not calculated due to the limitation of computational capacity of the computer employed the scale effect of dnfs which has been extensively studied before e g min et al 2003 2004 is not investigated in the present study as an example there were 489 514 meshes in the dfn model shown in fig 2 f and the calculation took approximately 4 hours using a personal computer with an i7 core to achieve a steady state fluid flow under which the fluid rate permeability could be calculated deviations among results computed with different mesh sizes were analyzed to help select a suitable one 4 results and analysis 4 1 nonlinear flow in dfns by applying different values of j to the dfns the relation between j and q was obtained as shown in fig 4 it is observed that j has a quadratic relationship with q which can be well fitted by eq 4 indicating that the cubic law is not applicable and the value of q j would vary with the increment of j at a certain j the value of q calculated by the smooth model is larger than those calculated by the mated and unmated models because the rough fracture surface gives rise to the longer flow paths and greater frictional loss that tend to reduce the flow rate the inertial effects in the unmated model are stronger than those in the mated model due to the enhanced variation of geometry of flow paths by aperture variation in space therefore when j is large e g j 0 7 the value of q calculated by the unmated model is smaller than that calculated by the mated model these results show that the inertial effects can be significantly enhanced by geometric variations of single fractures which are in concert with many previously reported results e g zimmerman et al 2004 javadi et al 2010 2014 xiong et al 2011 zou et al 2015 according to the cubic law based eq 3 in which j is linearly correlated with q the value of q j is a constant value therefore the transition from a linear flow regime to a nonlinear flow regime should be the turning point at which the value of q j starts to change with the increasing j the variations of q j of different models against j are depicted in fig 5 when j is small i e j 10 5 q j holds constant values however when j 10 4 with the increasing j the value of q j decreases dramatically to quantitatively estimate the turning point and determine a critical hydraulic gradient jc the relative deviation rate of q j δ 1 is defined as follows 8 δ 1 q j cubic q j ns q j cubic 100 where q j cubic is the value of q j calculated by solving the cubic law and q j ns is the value of q j calculated by solving the ns equations the forchheimer number f 0 which quantifies the ratio of nonlinear to linear pressure losses was also calculated to help understand the nonlinear flow behavior defined as 9 f 0 b q 2 a q b q a fig 6 shows the relations between δ 1 and f 0 with j when j ranges from 10 8 to 10 if δ 1 10 is taken as a threshold to identify the flow regime transition it was found that δ 1 10 was satisfied when j 10 4 while δ 1 10 was confirmed when j 10 3 therefore for the studied models the critical condition for quantifying the flow regime transition is 10 4 jc 10 3 before reaching 10 2 the value of f 0 is negligible after that f 0 increases abruptly and the unmated model gains greater values than the other two models showing that the surface geometry could largely enhance the nonlinearity of fluid flow in a dfn although both δ 1 and f 0 are used to quantify the degree of nonlinearity of flow they have different physical meanings therefore they cannot be directly compared the differences in fracture surface geometries among three kinds of models do not seem to have large impacts on this critical condition this is reasonable because in a dfn when a fluid flows from one fracture to an intersected fracture the main flow direction could be changed up to nearly 180 such variation of flow direction almost happens at every intersection with varying degrees which can significantly enhance the nonlinear flow in a system comparing to this the surface roughness brings much smaller variations to the direction of flow and in most conditions only influences the flow layers close to the fracture wall with the main flow stream undisturbed as an extreme case in the smooth model the nonlinear flow only originates from the geometric variation at intersections since jc quantifies the regime transition of a whole dfn instead of a single fracture it is governed by the macroscopic geometric characters i e fracture backbone geometry rather than the microscopic geometric characters i e surface roughness of single fractures when only single fractures are concerned the surface roughness will become a governing parameter 4 2 effects of shear on equivalent permeability to simplify the modeling of shear a series of values were assigned to the normal displacement and the shear displacement to artificially shear the cutting through fracture located on the vertical axis of each model as shown in fig 7 as a result the two blocks on the left and right sides of the sheared fracture were separated by a normal distance of d n shear induced dilation and slid by a shear displacement of d s here since the fracture is artificially sheared d n and d s instead of v and u are used to represent the normal and shear displacements respectively the initial aperture of the sheared fracture was assumed to be 0 to facilitate the calculation with such a configuration these models represent a situation that a fractured rock mass is sheared along a major fracture plane with other fractures undisturbed this is frequently encountered in the field when for example a rock mass is sheared by earthquakes the large dfns 5 m in length were firstly sheared and then smaller dfns were extracted to maintain continuous boundaries numerical simulations by solving the ns equations were performed to calculate the equivalent permeability with j 10 8 10 d s 1 7 mm and d n 1 5 mm the interval for hydraulic gradient is one order and for displacement is 1 or 2 mm leading to 144 simulation cases in total and around 500 cpu hours the values of displacements were selected based on the results of previous laboratory coupled shear flow tests on single fractures li et al 2008 since the size of the sheared fracture 1 m is 5 times of the samples 20 cm used in the previous tests the normal displacement of average testing results around 1 mm was magnified by a factor of 5 leading to the maximum normal displacement of 5 mm the maximum shear displacement of 7 mm was selected to ensure that each fracture intersecting the sheared fracture has been sheared by a distance larger than its own aperture and that the residual stage of shear has been reached as a result the model can be completely sealed when d n 0 mm and d s 7 mm only horizontal flow was simulated fig 3 and only the results of the unmated model are discussed below because the other two models show almost identical response to shear enlarged views of fluid flow in the rectangular region in fig 7 b are shown in figs 8 a i when j 10 4 the fluid flow is in a linear regime and the flow vectors are distributed parabolically in the direction perpendicular to flow see figs 8 j o such parabolic streamlines were disturbed at the sheared intersection and recovered after travelling through the intersection when j increases to 10 2 some secondary vortices can be observed in the regions near the intersection as shown in figs 8 h i the flow vectors can no longer regain parabolic distributions after passing through the intersection see figs 8 n o demonstrating strong nonlinear flow characters induced by the geometric variation of the sheared fracture the combination of normal and shear displacement can significantly change the flow patterns across the sheared fracture and result in pressure dissipation at high hydraulic gradients the shear induced dislocation of fractures may generate bottle necks that may restrict fluid from flowing through the model especially when d s is large and d n is small to quantitatively investigate the effects of shear on the macroscopic hydraulic properties of dfns the relative deviation rate of equivalent permeability δ 2 is defined as follows 10 δ 2 k 0 k k 0 100 where k 0 is the equivalent permeability of the dfns with d n 0 mm and d s 0 mm and k is the equivalent permeability of the dfns with any given shear and normal displacements larger than 0 here k was back calculated according to the following equation 11 q a k μ p s l where a is the cross sectional area and sl is the dfn side length fig 9 shows the relationship between δ 2 and j for selected combinations of d n 1 2 3 5 mm and d s 1 3 5 7 mm cautions should be paid that the combinations of d n d s are unrealistic because d n is typically smaller than d s in most circumstances for natural rock fractures subject to shear here results of all combinations are presented only for the purpose of comparison when d s 1 mm δ 2 holds almost constant values at different values of j and d s suggesting that the permeability is not affected by shear at a sufficiently small shear displacement the change of δ 2 becomes obvious when d s becomes large where δ 2 decreases with the increment of d n suggesting that the shear induced permeability change may vanish at a sufficiently large d n when fluid flow is in the linear regime j 10 4 δ 2 is approximately linearly correlated with j with small variations while for fluid flow in the nonlinear regime j 10 3 δ 2 is nonlinearly correlated with j it is evident that the effects of shear on permeability change become remarkable at sufficiently large values of d s and small values of d n i e d n 1 mm and d s 7 mm because the sheared fracture under such conditions would easily form bottle necks that could dramatically reduce the effective flow paths and the equivalent permeability although not calculated it is predictable that the permeability in the orientation parallel to that of the sheared fracture will increase due to the expansion of flow path originated from the shear induced dilation analogous to the permeability rise in sheared single fractures such anisotropic hydraulic behavior needs to be quantitatively estimated in future studies 4 3 application of δ 2 the results in fig 9 show that δ 2 is correlated with d s and d n especially when j is large by fitting these data it is found that for a certain j ranging from 10 3 to 10 δ 2 can be calculated according to the following equation 12 δ 2 d d s f g d n where d f and g are coefficients with their values tabulated in table 2 here δ2 is an independent variable and d s and d n are two dependent variables the cases with j 10 3 are not considered due to the small deviations of δ2 especially when d s is less than 5 mm the variations of d n and d s and simulated and predicted results of δ2 are presented in table 3 and fig 10 the results show that eq 11 agrees well with the results of numerical simulations when j 10 3 10 d n 1 5 mm and d s 1 7 mm the correlation coefficients for all the cases are r2 0 99 indicating that eq 11 is sufficiently reliable to predict the effects of shear on the variations of equivalent permeability fig 11 shows that with the increment of j from 10 3 to 10 d and g increase from 0 0137 to 0 9183 and from 0 4441 to 0 7102 respectively whereas f decreases from 4 3096 to 2 2553 by fitting these parameters three regression functions are proposed written as 13 d 0 9223 0 9960 1 j j 0 2954 14 f 2 2476 1 0002 1 j j 0 06560 15 g 0 7134 0 9997 1 j j 0 02537 for a fractured rock masses when the shear displacement d s and normal displacement d n of a sheared fracture as well as the hydraulic gradient j applied on the two opposing boundaries are known the relative deviation rate of equivalent permeability δ 2 can be estimated by using eqs 11 14 if δ 2 is larger than a prescript critical condition i e 10 the navier stokes equations should be solved to address the nonlinear flow characterizations if δ 2 is smaller than the critical condition the hydraulic response i e permeability can be calculated using the cubic law as a first order estimation therefore the proposed eq 11 can help choose proper governing equations for fluid flow calculations in rock fractures 5 conclusions in the present study a numerical approach for establishing 2d fracture networks with improved representation of fracture surface geometries that enable the application of normal and shear displacements and the fluid flow simulation by solving navier stokes equations is presented the hydro mechanical behavior of single rock fractures during shear has been extensively investigated in previous studies however similar investigations on the effects of shear on the hydraulic properties of dfns have rarely been reported nonlinear flow behavior is emphasized here because the enhanced permeability by shear induced dilation could facilitate the transition of fluid flow from linear to nonlinear regimes which has been confirmed in coupled shear flow tests on single fractures the simulation results revealed a critical condition for quantifying the transition of flow from a linear flow regime to a nonlinear regime for the studied models as 10 4 j 10 3 where j is the hydraulic gradient comparisons among the smooth model the mated model and the unmated model showed that the surface roughness and the variation of aperture in space enhance the inertial effects of flow and thereby reducing the equivalent permeability their influences on the critical condition for flow regime transition are negligibly small because such critical condition for dfns is governed by the macroscopic geometric characters i e fracture backbone geometry rather than the microscopic geometric characters i e surface roughness of fracture networks the normal and shear displacements add further geometric variations to the original models which may facilitate the formation of bottle necks that could restrict fluid from flowing through the model especially when d s is large and d n is small the shear induced dislocation of flow paths significantly changes the flow patterns in and around the sheared intersections which will result in pressure dissipation at high hydraulic gradients the expression for predicting the relative deviation rate of equivalent permeability is proposed which can help choose proper governing equations for fluid flow in rock fracture networks as long as the associated parameters such as shear and normal displacements and hydraulic gradient are known the present study established several dfn models with one kind of fracture distribution and limited values of normal and shear displacements which is therefore merely an initial step in such explorations on the concerned topic more sophisticated models with mathematically correlated normal and shear displacements and coupled hydro mechanical model that takes into account the deformability and strength of both intact rocks and fractures need to be incorporated into the proposed numerical approach to improve the understanding on shear induced hydraulic property changes of dfns while shear can augment the permeability of dfns in the orientation parallel to the sheared fracture the permeability in the perpendicular orientation is primarily subjected to reduction which may result in a strong anisotropy in fluid flow behaviors of fractured rock masses reduction of permeability is beneficial for the projects like geometric repository of high level radioactive wastes while the opposite is expected for the projects like shale gas production using hydraulic fracturing therefore macroscopic parameters such as permeability tensor and microscopic parameters such as contact area ratio in sheared fractures need to be more thoroughly investigated using fully coupled hydro mechanical models in the future works acknowledgments this study has been partially funded by national natural science foundation of china china grant nos 51609136 51709260 national science foundation of jiangsu province china grant no bk20170276 and the zhejiang collaborative innovation center for prevention and control of mountain geological hazards china grant no pcmgh 2016 z 01 
