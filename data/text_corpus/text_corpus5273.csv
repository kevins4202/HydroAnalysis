index,text
26365,agricultural land management often involves trade offs between ecosystem services es and disservices eds balancing these trade offs to achieve low impact production of agricultural commodities requires rigorous approaches for quantifying and optimizing es and eds reconciling biophysical constraints and different management objectives in this study we demonstrate a high resolution spatially explicit analysis of es and eds trade offs for irrigated corn production systems in the south platte river basin colorado usa as a case study the analysis integrated a biogeochemical model daycent with optimization algorithms to assess trade offs between multiple es and eds indicators including net primary production soil organic carbon water use nitrogen leaching and greenhouse gas emissions our results show a large fraction of total potential system productivity up to 21 mg ha 1 year 1 can be realized at minimal ecosystem impacts through careful land management decisions our analysis also explores how different land management objectives imply different landscape configurations keywords ecosystem services environmental impact assessment ecosystem modeling trade off analysis multi objective optimization 1 introduction although the provision of food and fiber has always been the primary objective of agricultural production agricultural ecosystems can also be managed for other benefits such as climate mitigation water quality improvement and biodiversity conservation the collection of these benefits are referred as ecosystem services es ma 2005 agricultural ecosystems are affected by a variety of human activities involving land use decisions and specific land management practices the negative impacts of humans on these ecosystems can directly reduce productivity e g reduced soil fertility and loss of habitat for biodiversity conservation or impose detrimental off site effects on other ecosystems and human society such as ground water pollution from nutrient leaching pesticide poisoning of non target species and increased greenhouse gas ghg emissions these negative impacts are known as ecosystem disservices eds zhang et al 2007 due to resource limitations e g land water nutrients technology and labor in agricultural production there are often trade offs between and among ecosystem services disservices hereafter referred as es eds trade offs for instance increasing food and fiber production tends to come with higher ghg emissions and nitrogen leaching power 2010 the questions of interest are what is the magnitude of the trade off i e how much change in one es or eds would lead to change in other es and or eds and how do we optimize es eds trade offs for the most efficient agricultural production answering these questions is a context dependent exercise that necessitates quantifying agricultural es and eds and their spatial temporal dynamics at different scales and levels de groot et al 2010 and integrating those results with optimization procedures for trade off analyses earlier studies on es assessments used land cover types as indicators to infer potential values of es and eds in different landscapes maynard et al 2010 kershner et al 2011 schneiders et al 2012 bagstad et al 2013 subsequently es eds trade offs were examined using scenario analysis which facilitates the investigation of drivers of change and the impacts of certain land use and land management options on es and eds indicators under specifically defined scenarios volk 2013 the use of land cover types as es indicators while useful for the rapid and cost effective analysis of aggregated watersheds and natural landscapes falls short of describing the underlying ecosystem processes the temporal and spatial dynamics of the es and eds provision and the changes in es and eds as a function of varying external factors such as management decisions policy and market price villa et al 2014 therefore it does not allow the finer measurements of the es eds trade off nor support the integration of optimization procedures quantifying es and eds is a difficult task that requires thorough understanding of fundamental physical and biological processes within the ecosystem in practice such understanding is often challenged by limited incomplete and or costly field measurements furthermore ecosystem responses to external disturbances are highly heterogeneous due to variability in soils climate land use history and other site specific attributes making interpolation between existing field trials with statistical models very difficult more recent studies overcome these issues by using process based models coupled with geographic information system gis to quantify es and eds associated with variations in crop rotation schemes and management practices in a spatially explicit manner lautenbach et al 2013 kragt and robertson 2014 balbi et al 2015 a process based model is the mathematical representation of the underlying processes that characterize the functioning of well delimited biological systems buck sorlin 2013 models such as daycent del grosso et al 2000 dndc li et al 1992 and apsim keating et al 2003 can capture the finer scale influence of site specific weather conditions soil properties crop types cropping practices and land use history that determine the provision of es and eds nguyen et al 2017 although this approach requires calibration validation setup and implementation of complex dynamic models as well as in field expertise to carry out the analysis it provides more insights into the fundamental physical and biological processes that determine es and eds allowing a continuous feedback between decision making and the corresponding changes in different es and eds at multiple spatial scales the use of process based modeling approaches for es and eds quantification coupled with optimization procedures simulation optimization for trade off analyses can make decision making in natural resource management more effective efficient and defensible volk 2013 process based models can be employed for exploratory quantification of es and eds to investigate the potential production of an agricultural landscape based on a set of well defined scenarios the results can then be optimized with mathematical algorithms like the non dominated sorting genetic algorithm nsga ii lautenbach et al 2013 simulated annealing chan et al 2006 or goal programming aldea et al 2014 es eds trade offs are often presented via simulated pareto frontiers also called production possibility frontiers which define the set of solutions that maximize es while minimizing eds given finite available resources i e biophysical constraints decision makers can then decide on the optimal solutions on the pareto frontier that meet their specific management objectives although this simulation optimization approach has been adopted in previous ecosystem service studies we found that most studies focused on the aggregation of es eds trade offs at regional national or sub global levels to inform strategic policy making e g lautenbach et al 2013 lester et al 2013 kragt and robertson 2014 balbi et al 2015 ewing and runck 2015 king et al 2015 kennedy et al 2016 other spatially explicit studies zeroed in on the tactical optimizations of biofuel supply chain and or biofuel crop production systems at coarse resolutions such as land resource unit level 5 square mile hexagons yu et al 2014 hydrological response units hru 204 ha lautenbach et al 2013 county level tittmann et al 2010 watershed and sub basin level parish et al 2012 only few studies could quantify es eds at finer spatial resolutions such as field level zhang et al 2010 besides these studies often considered a limited number between 1 and 4 of es eds objectives to ease the optimization and visualization thus there is a lack of higher dimensional trade off analyses at more local scales i e finer spatial temporal and management resolutions to inform the direct decision makers of agricultural ecosystems e g farmers ranchers and forest landowners on how they could manage their farms the principle decision unit in the agricultural landscape for optimal es and eds provision as implied by zhang et al 2007 and power 2010 when it comes to es eds trade offs in spatially heterogeneous ecosystems like agriculture the devil is really in the details our study aims at demonstrating the linkages among different components including the field scale and detailed quantification of management induced es changes the landowner s management preferences and the multiobjective optimizations for rigorous trade off analyses of multiple es and eds in agricultural ecosystems we present this research with a case study on high resolution quantification of es eds tradeoffs and optimization of fertilizer and irrigation decisions for irrigated corn production in the south platte river basin colorado united states a biogeochemical model daycent was employed for exploratory quantification of five es and eds including biomass production soil carbon storage water provision water quality and climate regulation at the field scale 1 ha the model simulations considered the effects of site specific factors such as soil properties weather data and historical dated back to the 1880s land use and current management practices on the es and eds quantification the simulated outputs of ed and eds were linked with a non dominated sorting algorithm to construct pareto frontiers quantifying the best possible basin scale outcomes we then used linear programming to identify the optimum fertilizer and irrigation rates for each land unit in the basin based on different predefined land management objectives 2 case study and method 2 1 study site our study focused on the irrigated corn growing area in the south platte river basin located within north central colorado usa fig 1 the region is among the most productive irrigated agricultural areas in the state with a majority of fine loamy soils average growing season evapotranspiration of irritated corn crop of 65 cm reported by the colorado agricultural meteorological network http www coagmet com average growing season precipitation of 31 cm and average minimum and maximum growing season temperature of 11 4 c and 24 7 c respectively mesinger et al 2006 the total area of the study region is 116 959 ha comprising 33 of all irrigated area in the basin cdss 2010 the south platte river basin is facing many issues such as water pollution from excessive fertilizer run off and large scale diversion of limited water resources away from irrigated agriculture dry up in order to meet future municipal and industrial m i needs water conservation board 2010 the south platte agricultural area ranked first in nitrate contamination and second in phosphorus contamination among the 20 major rivers in the us strange et al 1999 this is due to the basin s low capacity of contaminant dilution which is 10 times below national average level mueller et al 1995 and the lack of riparian vegetation to filter irrigation return flows and feedlot run off loomis et al 2000 according to the colorado s statewide water supply initiative 2010 report water conservation board 2010 under medium economic development assumptions the population of the south platte basin is projected to grow from 3 5 million people in 2008 to 6 0 million people by 2050 this would result in a 136 million cubic meter gap in water supply for m i uses and will likely trigger a permanent dry up of 73 000 to 108 000 ha of irrigated farmland in the basin water conservation board 2010 the large scale dry up of irrigated agriculture land will likely cause significant negative economic social and environmental impacts to the basin and to the whole state these issues necessitate integrated assessments and better landscape designs to improve the efficiency of resource allocation and minimize detrimental impacts on the environment while meeting future demands 2 2 daycent model simulation 2 2 1 daycent model daycent parton et al 1998 is a daily time step process based model that represents biogeochemical flows of carbon nitrogen and water it simulates the dynamics of many ecosystem processes including changes in soil organic matter soil water nutrient cycling and trace gas fluxes del grosso et al 2002 plant growth net primary productivity or npp is simulated as a function of nutrient availability soil water and temperature shading vegetation type and plant phenology metherell et al 1993 the resulting carbon is then partitioned to different plant components e g roots vs shoots based on plant type phenology soil water content and nutrient availability soil organic matter som dynamics are simulated for surface litter pools and the top soil layer 0 20 cm som is divided into two litter pools structural and metabolic and three mineral associated organic matter pools active slow and passive with different potential decomposition rates microbially mediated decomposition of litter and som are represented as functions of substrate availability substrate quality lignin content c n ratio soil texture temperature water availability and tillage intensity daycent s water flow submodel simulates the daily flow of water through the plant canopy litter and soil layers soil water content is simulated for each soil layer throughout the soil profile depth parton et al 1998 daycent also simulates water loss through sublimation of snowpack interception of rainfall by surface litter and the plant canopy water runoff from infiltration excess water leaching from the bottom of the soil profile or deep storage soil evaporation and plant transpiration the soil nitrogen submodel includes processes such as n addition atmospheric deposition volatilization leaching plant uptake and n mineralization and immobilization daycent calculates n leaching as a function of soil nitrate inorganic n leaching and active soil som pool decomposition organic n leaching soil texture and the amount of water moving through the soil profile the trace gas submodel of daycent simulates soil nox and n2o gas emissions from nitrification and denitrification as well as n2 emissions from denitrification methane ch4 efflux from soil are also modeled by daycent the model s primary inputs are daily maximum and minimum air temperature and precipitation soil texture for each horizon in the soil profile soil rooting depth land cover use data e g vegetation types land use history and management practices e g irrigation tillage fertilization the nominal spatial scale for daycent simulations is one square meter though model results are typically scaled up across larger areas ha sized for which model inputs are assumed as uniform del grosso et al 2008 the daycent model has been used previously for the simulation of agricultural system productivity soil carbon and trace gas emissions accounting in a wide variety of us agricultural systems for example the model has been calibrated and validated across all principal us cropping systems in the process of producing the annual inventory of u s greenhouse gas emissions and sinks us epa 2014 for this study region in particular zhang 2016 reported a coefficient of determination r2 of 0 89 and a root mean squared error rmse of 1 343 kg ha 1 when comparing the 15 year average daycent s simulated corn yield with those reported by the national agricultural statistics service nass 2 2 2 spatial input databases an overview of our methodological approach is presented in fig 2 several spatial databases were used to create input data for daycent soil and climate data were obtained from the natural resource conservation service soil survey geographic database ssurgo nrcs usda 2014 and the north american regional reanalysis database narr mesinger et al 2006 respectively for each climate grid cell a table of daily precipitation maximum temperature and minimum temperature is constructed in a format described in easter et al 2005 soil texture rock fraction and ph for different soil profile layers of the dominant soil component for each map unit were taken directly from the ssurgo database the bulk density field capacity wilting point and saturated hydraulic conductivity were computed using the saxton equations saxton et al 1986 irrigated land cover data of the south platte river basin were obtained from the gis database of colorado s decision support systems cdss 2010 these three gis layers soil climate and land use were then intersected and any small slivers 1 ha were merged into their longest shared edge neighbors the intersection created 16 651 polygons of various sizes for irrigated corn in the south platte river basin including 1263 unique combinations of soil type and weather data daycent input strata each unique stratum was simulated with daycent and the results were then linked back to their associated landscape polygons for further calculation and spatial visualization to facilitate similar landscape studies we created an arcgis add in toolbox to automate the geospatial data processing for daycent landscape simulation 2 2 3 simulation of baseline conditions before conducting experimental simulations for each stratum we initialized daycent using pre settlement and historical agricultural land use assumptions at the major land resource area mlra scale as described in ogle et al 2010 this is the same procedure used in the inventory of u s greenhouse gas emissions and sinks us epa 2014 and the comet farm system http cometfarm nrel colostate edu the model initialization included two stages an initial spin up followed by a simulation of historical land use the spin up simulation defines steady state i e equilibrium soil carbon stocks assuming little or no disturbance from human activities prior to the conversion to agriculture grassland was chosen as the native vegetation and was simulated with daycent for 5000 years subsequently the model run was extended from this equilibrium condition with historical land use assumptions for each mlra there were five mlras within the study region with different areal coverage percentage including g67b 84 2 h72 15 3 g67a 0 3 and e49 0 17 and e48a 0 03 based on data compiled by ogle et al 2010 a two year rotation between bare fallow and winter wheat was assigned to g67a g67b e49 and e48a for the period from 1881 to 1979 where plow out of the native grassland was set to occur in 1880 as for h72 a four year rotation of corn corn spring small grain winter wheat was assigned from 1881 to 1920 and a three year rotation of corn spring small grain alfalfa was assigned from 1921 to 1979 typical management practices for each crop over time were compiled from various sources e g usda ers 1997 usda nass 2004 iback and adams 1967 smalley and engle 1942 to complete the model initialization we extended the historical baseline conditions from 1980 to 2015 with a business as usual scenario bau for irrigated corn production in colorado the bau reflects a system state under current management practice and is used to evaluate the magnitude and direction of change due to subsequent changes in management practices corn in northern colorado is usually planted between april 28 and may 20 and harvested between october 8 and november 13 nass 2010 the average bau management practices reported by the economic research service s agricultural resource management survey tailored reports ers arms 2010 for colorado s irrigated corn crop were 170 kg of nitrogen per hectare 34 cm of irrigated water per growing season 75 residue removal and conventional tillage 2 2 4 management practice scenarios to examine the impacts of n fertilization and irrigation on the provision of different agricultural es and eds we ran our forward simulations with n fertilizer and irrigation rates set to 20 40 60 100 120 140 and 160 of the bau level 68 102 136 170 204 238 and 272 kg of n per ha and 13 6 20 4 27 2 34 0 40 8 47 6 and 54 4 cm water year 1 respectively the permutation of these fertilizer and irrigation rates yielded 49 management practice scenarios we simulated these 49 management practice scenarios for each polygon for a 30 year period extending from 2016 to 2046 the percentage of stover removal was kept constant at the baseline level 75 and only conventional tillage was considered we did not consider the effects of changes in co2 concentration and climate on crop production and water use efficiency the combination of 1263 daycent input strata and 49 management practice scenarios yielded 61 887 daycent simulations these simulations were executed in parallel on an 18 node 216 processor cluster computing system at colorado state university s natural resources ecology laboratory 2 2 5 ecosystem service and disservice indicators daycent simulation outputs were used to estimate multiple es and eds indicators for each simulation soil organic carbon soc and net primary productivity npp were chosen as es indicators representing ecosystem carbon storage and biomass production services respectively water use nitrogen n leaching and greenhouse gas emissions ghg were chosen as eds indicators representing water provision water quality and climate regulation disservices since soc dynamics are a long term effect of management practices soil properties and weather conditions we reported the net change in soc in mg of carbon per hectare at the end of the simulation period to capture the cumulative effects of changing management practices due to the large seasonal variations other indicators were reported as flow variables averaged over the entire analysis including mg of carbon dioxide equivalent co2e per hectare per year for ghg kg of nitrogen per hectare per year for n leaching mg of dry weight biomass per hectare per year for npp assuming 43 5 carbon content gesch et al 2010 and cm of water per year for water use the values of soc and npp indicators were taken as direct daycent outputs while soil ghg emission estimates included modeled outputs from daycent as well as additional models e g for indirect n2o emissions from the u s department of agriculture entity scale greenhouse gas inventory guidelines eve et al 2014 annual net co2 emission was assumed to come mainly from soil microbial respiration of soc and was computed by taking the difference of soc levels between two consecutive years annual n2o emission comprised of direct and indirect n2o emissions the direct n2o emission was reported by daycent in term of n2o efflux while the indirect n2o emission was computed from nox efflux volatilized nh3 and nitrogen leaching with the emission factors ef of 0 01 0 01 and 0 075 respectively annual n2o and ch4 were converted into co2 equivalents by using 100 year global warming potential values gwp100 of 298 and 25 respectively ipcc 2006 n leaching was calculated from the amount of inorganic and organic n leached out of the soil profile since we did not model the effect of terrain slope and stream water flow the n leaching values only reflected the potential amounts of nitrate that can enter the ground water the ecosystem water use over a growing season was calculated as the total amount of water used to sustain all ecosystem processes including evapotranspiration the water utilized by the crop to produce biomass and other processes such as sublimation the vaporized water from snow and ice interception by the crop canopy run off and deep drainage to simplify the indicator comparisons between different management scenarios we computed the area weighted average value of each indicator using the following equation 1 v i j 1 16 651 a j v j a where v i area weighted average value of an es or eds indicator i j the j th polygon within the landscape a j the area of the j th polygon within the landscape ha v j the value of es or eds indicator i of the j th polygon within the landscape a the total area of the study region a 116 959 ha 2 3 calculation of pareto frontiers and optimization of landscape designs to examine the magnitude of trade offs between npp and other es and eds indicators in the study region we used a simple non dominated sorting algorithm to identify the pareto frontier for each pair of npp and es or eds indicators see supplementary material s1 these pareto frontiers illustrate macro trade offs between npp and other indicators assuming that management practice scenarios were uniformly applied to the landscape uniform management in reality it is of greater importance to identify and visualize the optimum landscape design under variable management i e the optimum management practice scenario for each polygon within the landscape corresponding to a specific management objective to do this we used a linear optimization algorithm that seeks to maximize the following objective function 2 maximize i n c i x i where x i the normalized value of es or eds indicator i x i x i p i m i n p i m a x p i m i n x i the value of es or eds indicator i p i min and p i max the population minimum and maximum of es or eds indicator i respectively c i the objective function coefficient a k a weighting factor corresponding to x i and n total number of es and eds indicators considered in the optimization n 5 since all the es and eds indicators were expressed in different units and scales we first normalized the indicators to a uniform scale 0 1 and used the normalized values in the objective function for a fair comparison hwang et al 1980 the optimization algorithm was applied to every polygon in the landscape to identify the optimum fertilizer and irrigation rates for each individual polygon seven management objective scenarios representing four different hypothetical management objectives by the landowner were chosen for the optimization table 1 for each scenario a set of objective function coefficients was assigned to the es and eds indicators we limited the coefficients to integer numbers that ranged from 2 to 2 in our analysis the coefficient absolute values indicated the importance of one indicator relative to the others in the objective function eq 2 and the sign of the coefficients reflected the indicator s direction of change that we wanted to optimize for example in scenario 2a increasing npp and decreasing water use expressed in the normalized unit term were set to be twice as important as increasing or decreasing other indicators except for scenario 1 where we equally considered all es and eds indicators coefficients 1 other management objective scenarios were biomass centric i e higher positive npp coefficients as biomass production is the primary provisioning goal of the study region in addition scenarios 2 3 and 4 focused on reducing eds coefficients 2 including water use n leaching and ghg respectively among these management objective scenarios the a scenarios put additional constraints on the remaining es and eds indicators while the b scenarios excluded them from the optimization table 1 we further compared the eds footprints between different management objectives the eds footprint is defined as the amount of eds indicator produced per unit of npp the eds footprints for a polygon under a specific management objective were calculated using the following equation 3 f i j k x i j k n p p j k where f i j k footprint of eds i for polygon j under management objective scenario k x i j k the value of es or eds indicator i for polygon j under management objective scenario k npp j k the value of npp indicator for polygon j under management objective scenario k the optimization results were mapped for spatial visualization using arcgis software esri 2014 we performed descriptive statistical analysis e g distribution central tendency and dispersion to explore es and eds indicators data see supplementary material s2 and one way anova analysis to test the effects of fertilizer and irrigation on each indicator for better presentation of the results see supplementary material s3 the simulation processes data analyses and map generation routines were automated using the python programming language https www python org our derived model was named the agricultural ecosystem service optimization ag ecosopt for the sake of future studies and development 3 results 3 1 effects of fertilizer and irrigation on es and eds indicators there were various combined effects of fertilizer and irrigation on soc npp n leaching and ghg emission fig 3 the rate of change of an indicator at a specific irrigation rate is defined as the difference in indicator values between the two fertilizer levels rise run higher fertilizer rates resulted in higher soc npp n leaching and ghg emission as the fertilizer rate increased soc and npp responded with a decreasing rate of change until a maximum value was reached asymptotically while n leaching and ghg emission rose linearly or with an increasing rate of change irrigation rate was proportional with the steepness of the slopes of the indicator s response lines curves the effects of irrigation on increasing soc npp and n leaching diminished at the irrigation rates above 47 6 cm year 1 we also found that reducing irrigation rates amplified net ghg emission within any specific fertilizer level below 238 kg ha 1 this is due to the increasing contribution of net co2 flux from soil c stock change in the total ghg emission see supplementary material s4 since we did not find statistically significant impacts of fertilizer application rate on total water use as determined by one way anova f 6 27502 0 00 p 1 0 see supplementary material s3 we only reported total water use against the irrigation rate averaged across all fertilizer levels fig 4 while water use was linearly correlated with irrigation different water use components responded differently to increasing irrigation rate in particular the evapotranspiration increased with a decreasing rate and plateaued at the irrigation rate of 34 cm year 1 whereas the water use from other processes rose slowly at lower irrigation rates and soared as soon as the evapotranspiration leveled off 3 2 trade offs between npp and other indicators the trade off between the es indicators i e npp and soc was a win win situation as increasing npp led to a linear increase in soc whereas those between npp and eds indicators were win loss situations since higher npp also came with higher eds fig 5 the uniform management pareto frontiers for npp and other eds revealed convex patterns this means that the marginal improvements of npp at the higher end will come with exponential penalties on environmental performance while trade offs between water use and npp were almost linear frontiers were increasingly convex for n leaching and ghg emissions all trade off frontiers showed a marked increase in eds to achieve npp greater than 21 mg ha 1 year 1 corresponding to average n fertilizer and irrigation rates above 136 kg ha 1 and 40 8 cm year 1 respectively this indicates that a relatively large fraction 90 of total potential system productivity can be realized while incurring only a relatively small fraction of total potential disservices through proper management practice decisions our results also showed the influence of fertilizer and irrigation in achieving certain optimal solutions i e scenarios that lie on the pareto frontiers the trade offs between npp and other indicators were affected by fertilizer limitation fl and irrigation limitation il which are states on a pareto frontier at which no further improvement of the optimum npp can be made without adding more fertilizer and irrigation respectively in the n leaching and ghg optimization cases optimal solutions with npp below this level were achieved by modulating irrigation rates while leaving fertilizer application at a minimum level additional npp gains beyond this point required increasing n fertilizer application causing a sharp increase in n leaching and n2o driven ghg emissions inappropriate applications of fertilizer and irrigation could lead to sub optimality in es eds trade offs i e scenarios that do not lie on the pareto frontiers this sub optimality was due to either excessive application of irrigation at the fl states fl sub optimality or excessive application of fertilizer at the il states il sub optimality fl sub optimality reduced npp as irrigation increased while il sub optimality improved npp with enormous costs on other es and eds with increasing fertilizer although we found no significant il sub optimality in the trade offs between npp and soc fig 5a and between npp and water use fig 5b the addition of fertilizer at the il states demonstrated a waste in resource investments 3 3 trade offs between management objectives fig 6 presents the es eds trade offs for the optimization of the management practices under seven management objective scenarios defined in table 1 in general we observed slight modulations of es and eds around the uniform management pareto frontiers except for the objective 2b unconstrained optimization of npp and water use where high sub optimality in n leaching and ghg emissions occurred for this scenario 2b our model allowed higher fertilizer application table 2 because there was no constraint placed on either reducing n leaching or ghgs our results also showed that allowing variable management during optimization obviously improved landscape performance in terms of n leaching as compared to the uniform management pareto frontiers i e objectives 3a 3b and 4a in fig 6c 3 4 landscape design of management practices and the spatial distribution of eds footprints the optimum spatial designs of fertilizer and irrigation rates and the spatial distribution of eds footprints of irrigated corn polygons in the south platte river basin were visualized for the management objective scenarios with constraints 1 2a 3a and 4a figs 7 and 8 those for management objective scenarios 2b 3b and 4b can be obtained from supplementary material s5 the distributions of optimal fertilizer and irrigation rates were heterogeneous throughout the landscape with many visible clusters of high and low values while the optimum rates of irrigation varied widely from 13 6 to 47 6 cm year 1 those of fertilizer only varied below the bau fertilizer level of 170 kg ha 1 the eds footprint values also varied greatly within the study region depending on the management objective and the eds indicator we observed some clusters of high eds footprint values in the areas north of denver and around the town of fort morgan 4 discussion pressure from the population and industrial expansion in our study region will result in higher demands for food biomass and water the question is how much of a viable productive agriculture is possible with declining water supply and what are the environmental impacts to answer this we compared the area weighted landscape sum of es and eds indicators and the corresponding management practices under the bau scenario table 3 with those from the landscape optimization of the seven management objective scenarios fig 9 we found that ecosystem water use can be reduced by 1 9 8 6 relative to the bau scenario level through lowering irrigation by 3 4 17 3 respectively objective 1 2a and 2b however these reductions came with either declines in soc and npp objective 1 and 2a due to reduced n fertilization or increases in ghgs and n leaching objective 2b due to increased n fertilization this indicated that it was not possible to improve both water use and environmental performance at the current and higher corn production level by simply adjusting fertilizer and irrigation application rates however our results implied that these management practices can be modulated to attain higher food and biomass provision at the lowest costs on ecosystems for the study region one advantage of the spatially explicit approaches for es and eds quantification is the ability to visualize the fine scale details and the hidden spatial patterns of landscapes deangelis and yurek 2017 detailed maps such as figs 7 and 8 have great utility for research and policy applications since they can be used in combination with other spatial layers for more complex quantitative spatial analyses and or to identify potential hotspots i e significant spatial clusters of high values of ecosystem threats for specific policy foci while our study involved a simple optimization problem in a geographically limited corn production landscape the approach and the resulting model would be equally applicable for more complex problems such as those with many constraints broader scopes or multiple stakeholders more constraints can be formulated to reflect biophysical barriers of the landscape policy regulation of environmental impacts and regional or local targets demands for ecosystem service production the system boundary can be expanded to include the optimization of multiple land cover scenarios or the supply chain optimization of multiple agricultural products the potential for the incorporation of economic optimization in this type of analysis allows not only the identification of the economically optimal solutions but also a better representation of the farmer s perspective and influences of the market economy lautenbach et al 2013 the involvement of stakeholders in the analysis would help to identify the most preferred problem solutions miettinen et al 2008 for instance our optimization results could be used as the pareto optimal starting points to show to the decision maker the decision maker would then be expected to express preference for es eds trade offs the preference information would be used to adjust the coefficients of the objective function for a new optimization simulation this process could be iterated until the most desirable solution was attained our analysis might also have practical implications for direct decision makers like farmers if results of such analysis were made available to farmers they could adjust their management decisions on their own farms accordingly for the most efficient production farmers could also report es eds trade offs in their farm planning and budgeting we believe that making the information more available and accessible to stakeholders would be the best way to propagate the es and eds concepts boost their communication and facilitate the sprouting of es and eds market mechanisms paustian et al 2009 despite the potential advantages of the approach we do realize some drawbacks in the methodology including but not limited to the expertise challenge and computational burden of the simulation the discretization of management practice variables the ambiguousness in the use of the objective function coefficients and the curse of dimensionality using process based models like daycent for detailed quantification of es and eds requires some level of model literacy and is computationally expensive for large area simulations in our case the analysis necessitated much effort for initial model setup and simulation time took several hours on a cluster of 216 high performance computer cores the infrastructure requirements for this type of analysis could impede the adoption of this approach for similar or more complex studies a possible solution to this disadvantage is the integration of the methodology into infrastructure ready tools or platforms to enable the flexible replications of es and eds analyses furthermore in order to alleviate the computational burden advanced meta modeling techniques such as support vector machine and neural network can be applied to create cost effective simplified surrogates of the detailed process based models wu et al 2016 based on these solutions further efforts are being made to prepare the infrastructure for our model ag ecosopt the analysis and optimization of es and eds based on fixed intervals of the continuous fertilizer and irrigation variables i e discretization might not guarantee actual optimal solutions since the solutions were forced to some discretized options however it significantly reduced the simulation time and supported the quality control of the analysis our method also used the normalized values of es and eds in the objective function with arbitrary objective function coefficients although this approach was good enough for the scope of our study finding the optimal landscape designs for pre defined sets of objective function coefficients based on discretized values of fertilizer and irrigation the unclear relationships between the coefficients and actual es and eds outcomes are a challenge for more complex analyses for example one might need to conduct a brute force search through all possible combinations of objective function coefficients to find the specific set that results in a certain level of es eds trade offs potential solutions for this problem are to involve the decision maker in interactive multi objective optimization wierzbicki et al 2000 to integrate more sophisticated algorithms that are capable of coefficients optimization such as levenberg marquardt backpropagation levenberg 1944 marquardt 1963 or to utilize different optimization solvers that allow the consideration of multiple objective functions with constraints and bounds configuration such as nsga ii deb 2011 last but not least the employment of this approach for more complex studies could be impeded with increasing dimensions of the analyses such as the addition of management practice types e g tillage and residue harvest considered levels for each management practice es and eds management objectives different land use cover types and land use cover change scenarios therefore a careful consideration of the potential trade offs between the scope of analysis and modeling capacity is always recommended for this type of study 5 conclusion we demonstrated a robust analysis of ecosystem services es and disservices eds trade offs for the irrigated corn growing area in south platte river basin of colorado the analysis integrated a high resolution quantification of es and eds associated with different fertilization and irrigation applications through the use of a process based biogeochemical model daycent the calculation of pareto frontiers for the trade offs among multiple es and eds considering uniform management practices and the identification of optimum fertilizer and irrigation rates of each polygon in the landscape based on different hypothetical management preferences we assessed five es and eds indicators including net primary production soil organic carbon water use nitrogen leaching and greenhouse gas emissions our results showed the influence of management practices and site specific factors on the provision of the es and eds and various trade offs among es and eds management practices eds footprints and management objectives inappropriate applications of fertilizer and irrigation could lead to sub optimal trade offs of es and eds although the analysis of es and eds trade offs is case specific we believe that our approach can be adopted for other locations as well as for more complex studies for decision support in sustainable agriculture further research is encouraged to improve this approach and to prepare the infrastructure for its widespread use in public and private sector decision making acknowledgements the work was supported by a usda nifa project grant 2011 67009 30083 decision support tool for integrated biofuel greenhouse gas emission footprints the fulbright vietnam scholarship the multidisciplinary approaches to sustainable bioenergy phd program funded by the united states national science foundation hue university of agriculture and forestry vietnam huaf and shell technology center houston the authors thank colleagues at the natural resource ecology laboratory nrel and at biodomain shell technology center houston for model training and data sources appendix a supplementary data the following are the supplementary data related to this article online data online data supplementary material supplementary material appendix a supplementary data supplementary data related to this article can be found at https doi org 10 1016 j envsoft 2018 06 006 
26365,agricultural land management often involves trade offs between ecosystem services es and disservices eds balancing these trade offs to achieve low impact production of agricultural commodities requires rigorous approaches for quantifying and optimizing es and eds reconciling biophysical constraints and different management objectives in this study we demonstrate a high resolution spatially explicit analysis of es and eds trade offs for irrigated corn production systems in the south platte river basin colorado usa as a case study the analysis integrated a biogeochemical model daycent with optimization algorithms to assess trade offs between multiple es and eds indicators including net primary production soil organic carbon water use nitrogen leaching and greenhouse gas emissions our results show a large fraction of total potential system productivity up to 21 mg ha 1 year 1 can be realized at minimal ecosystem impacts through careful land management decisions our analysis also explores how different land management objectives imply different landscape configurations keywords ecosystem services environmental impact assessment ecosystem modeling trade off analysis multi objective optimization 1 introduction although the provision of food and fiber has always been the primary objective of agricultural production agricultural ecosystems can also be managed for other benefits such as climate mitigation water quality improvement and biodiversity conservation the collection of these benefits are referred as ecosystem services es ma 2005 agricultural ecosystems are affected by a variety of human activities involving land use decisions and specific land management practices the negative impacts of humans on these ecosystems can directly reduce productivity e g reduced soil fertility and loss of habitat for biodiversity conservation or impose detrimental off site effects on other ecosystems and human society such as ground water pollution from nutrient leaching pesticide poisoning of non target species and increased greenhouse gas ghg emissions these negative impacts are known as ecosystem disservices eds zhang et al 2007 due to resource limitations e g land water nutrients technology and labor in agricultural production there are often trade offs between and among ecosystem services disservices hereafter referred as es eds trade offs for instance increasing food and fiber production tends to come with higher ghg emissions and nitrogen leaching power 2010 the questions of interest are what is the magnitude of the trade off i e how much change in one es or eds would lead to change in other es and or eds and how do we optimize es eds trade offs for the most efficient agricultural production answering these questions is a context dependent exercise that necessitates quantifying agricultural es and eds and their spatial temporal dynamics at different scales and levels de groot et al 2010 and integrating those results with optimization procedures for trade off analyses earlier studies on es assessments used land cover types as indicators to infer potential values of es and eds in different landscapes maynard et al 2010 kershner et al 2011 schneiders et al 2012 bagstad et al 2013 subsequently es eds trade offs were examined using scenario analysis which facilitates the investigation of drivers of change and the impacts of certain land use and land management options on es and eds indicators under specifically defined scenarios volk 2013 the use of land cover types as es indicators while useful for the rapid and cost effective analysis of aggregated watersheds and natural landscapes falls short of describing the underlying ecosystem processes the temporal and spatial dynamics of the es and eds provision and the changes in es and eds as a function of varying external factors such as management decisions policy and market price villa et al 2014 therefore it does not allow the finer measurements of the es eds trade off nor support the integration of optimization procedures quantifying es and eds is a difficult task that requires thorough understanding of fundamental physical and biological processes within the ecosystem in practice such understanding is often challenged by limited incomplete and or costly field measurements furthermore ecosystem responses to external disturbances are highly heterogeneous due to variability in soils climate land use history and other site specific attributes making interpolation between existing field trials with statistical models very difficult more recent studies overcome these issues by using process based models coupled with geographic information system gis to quantify es and eds associated with variations in crop rotation schemes and management practices in a spatially explicit manner lautenbach et al 2013 kragt and robertson 2014 balbi et al 2015 a process based model is the mathematical representation of the underlying processes that characterize the functioning of well delimited biological systems buck sorlin 2013 models such as daycent del grosso et al 2000 dndc li et al 1992 and apsim keating et al 2003 can capture the finer scale influence of site specific weather conditions soil properties crop types cropping practices and land use history that determine the provision of es and eds nguyen et al 2017 although this approach requires calibration validation setup and implementation of complex dynamic models as well as in field expertise to carry out the analysis it provides more insights into the fundamental physical and biological processes that determine es and eds allowing a continuous feedback between decision making and the corresponding changes in different es and eds at multiple spatial scales the use of process based modeling approaches for es and eds quantification coupled with optimization procedures simulation optimization for trade off analyses can make decision making in natural resource management more effective efficient and defensible volk 2013 process based models can be employed for exploratory quantification of es and eds to investigate the potential production of an agricultural landscape based on a set of well defined scenarios the results can then be optimized with mathematical algorithms like the non dominated sorting genetic algorithm nsga ii lautenbach et al 2013 simulated annealing chan et al 2006 or goal programming aldea et al 2014 es eds trade offs are often presented via simulated pareto frontiers also called production possibility frontiers which define the set of solutions that maximize es while minimizing eds given finite available resources i e biophysical constraints decision makers can then decide on the optimal solutions on the pareto frontier that meet their specific management objectives although this simulation optimization approach has been adopted in previous ecosystem service studies we found that most studies focused on the aggregation of es eds trade offs at regional national or sub global levels to inform strategic policy making e g lautenbach et al 2013 lester et al 2013 kragt and robertson 2014 balbi et al 2015 ewing and runck 2015 king et al 2015 kennedy et al 2016 other spatially explicit studies zeroed in on the tactical optimizations of biofuel supply chain and or biofuel crop production systems at coarse resolutions such as land resource unit level 5 square mile hexagons yu et al 2014 hydrological response units hru 204 ha lautenbach et al 2013 county level tittmann et al 2010 watershed and sub basin level parish et al 2012 only few studies could quantify es eds at finer spatial resolutions such as field level zhang et al 2010 besides these studies often considered a limited number between 1 and 4 of es eds objectives to ease the optimization and visualization thus there is a lack of higher dimensional trade off analyses at more local scales i e finer spatial temporal and management resolutions to inform the direct decision makers of agricultural ecosystems e g farmers ranchers and forest landowners on how they could manage their farms the principle decision unit in the agricultural landscape for optimal es and eds provision as implied by zhang et al 2007 and power 2010 when it comes to es eds trade offs in spatially heterogeneous ecosystems like agriculture the devil is really in the details our study aims at demonstrating the linkages among different components including the field scale and detailed quantification of management induced es changes the landowner s management preferences and the multiobjective optimizations for rigorous trade off analyses of multiple es and eds in agricultural ecosystems we present this research with a case study on high resolution quantification of es eds tradeoffs and optimization of fertilizer and irrigation decisions for irrigated corn production in the south platte river basin colorado united states a biogeochemical model daycent was employed for exploratory quantification of five es and eds including biomass production soil carbon storage water provision water quality and climate regulation at the field scale 1 ha the model simulations considered the effects of site specific factors such as soil properties weather data and historical dated back to the 1880s land use and current management practices on the es and eds quantification the simulated outputs of ed and eds were linked with a non dominated sorting algorithm to construct pareto frontiers quantifying the best possible basin scale outcomes we then used linear programming to identify the optimum fertilizer and irrigation rates for each land unit in the basin based on different predefined land management objectives 2 case study and method 2 1 study site our study focused on the irrigated corn growing area in the south platte river basin located within north central colorado usa fig 1 the region is among the most productive irrigated agricultural areas in the state with a majority of fine loamy soils average growing season evapotranspiration of irritated corn crop of 65 cm reported by the colorado agricultural meteorological network http www coagmet com average growing season precipitation of 31 cm and average minimum and maximum growing season temperature of 11 4 c and 24 7 c respectively mesinger et al 2006 the total area of the study region is 116 959 ha comprising 33 of all irrigated area in the basin cdss 2010 the south platte river basin is facing many issues such as water pollution from excessive fertilizer run off and large scale diversion of limited water resources away from irrigated agriculture dry up in order to meet future municipal and industrial m i needs water conservation board 2010 the south platte agricultural area ranked first in nitrate contamination and second in phosphorus contamination among the 20 major rivers in the us strange et al 1999 this is due to the basin s low capacity of contaminant dilution which is 10 times below national average level mueller et al 1995 and the lack of riparian vegetation to filter irrigation return flows and feedlot run off loomis et al 2000 according to the colorado s statewide water supply initiative 2010 report water conservation board 2010 under medium economic development assumptions the population of the south platte basin is projected to grow from 3 5 million people in 2008 to 6 0 million people by 2050 this would result in a 136 million cubic meter gap in water supply for m i uses and will likely trigger a permanent dry up of 73 000 to 108 000 ha of irrigated farmland in the basin water conservation board 2010 the large scale dry up of irrigated agriculture land will likely cause significant negative economic social and environmental impacts to the basin and to the whole state these issues necessitate integrated assessments and better landscape designs to improve the efficiency of resource allocation and minimize detrimental impacts on the environment while meeting future demands 2 2 daycent model simulation 2 2 1 daycent model daycent parton et al 1998 is a daily time step process based model that represents biogeochemical flows of carbon nitrogen and water it simulates the dynamics of many ecosystem processes including changes in soil organic matter soil water nutrient cycling and trace gas fluxes del grosso et al 2002 plant growth net primary productivity or npp is simulated as a function of nutrient availability soil water and temperature shading vegetation type and plant phenology metherell et al 1993 the resulting carbon is then partitioned to different plant components e g roots vs shoots based on plant type phenology soil water content and nutrient availability soil organic matter som dynamics are simulated for surface litter pools and the top soil layer 0 20 cm som is divided into two litter pools structural and metabolic and three mineral associated organic matter pools active slow and passive with different potential decomposition rates microbially mediated decomposition of litter and som are represented as functions of substrate availability substrate quality lignin content c n ratio soil texture temperature water availability and tillage intensity daycent s water flow submodel simulates the daily flow of water through the plant canopy litter and soil layers soil water content is simulated for each soil layer throughout the soil profile depth parton et al 1998 daycent also simulates water loss through sublimation of snowpack interception of rainfall by surface litter and the plant canopy water runoff from infiltration excess water leaching from the bottom of the soil profile or deep storage soil evaporation and plant transpiration the soil nitrogen submodel includes processes such as n addition atmospheric deposition volatilization leaching plant uptake and n mineralization and immobilization daycent calculates n leaching as a function of soil nitrate inorganic n leaching and active soil som pool decomposition organic n leaching soil texture and the amount of water moving through the soil profile the trace gas submodel of daycent simulates soil nox and n2o gas emissions from nitrification and denitrification as well as n2 emissions from denitrification methane ch4 efflux from soil are also modeled by daycent the model s primary inputs are daily maximum and minimum air temperature and precipitation soil texture for each horizon in the soil profile soil rooting depth land cover use data e g vegetation types land use history and management practices e g irrigation tillage fertilization the nominal spatial scale for daycent simulations is one square meter though model results are typically scaled up across larger areas ha sized for which model inputs are assumed as uniform del grosso et al 2008 the daycent model has been used previously for the simulation of agricultural system productivity soil carbon and trace gas emissions accounting in a wide variety of us agricultural systems for example the model has been calibrated and validated across all principal us cropping systems in the process of producing the annual inventory of u s greenhouse gas emissions and sinks us epa 2014 for this study region in particular zhang 2016 reported a coefficient of determination r2 of 0 89 and a root mean squared error rmse of 1 343 kg ha 1 when comparing the 15 year average daycent s simulated corn yield with those reported by the national agricultural statistics service nass 2 2 2 spatial input databases an overview of our methodological approach is presented in fig 2 several spatial databases were used to create input data for daycent soil and climate data were obtained from the natural resource conservation service soil survey geographic database ssurgo nrcs usda 2014 and the north american regional reanalysis database narr mesinger et al 2006 respectively for each climate grid cell a table of daily precipitation maximum temperature and minimum temperature is constructed in a format described in easter et al 2005 soil texture rock fraction and ph for different soil profile layers of the dominant soil component for each map unit were taken directly from the ssurgo database the bulk density field capacity wilting point and saturated hydraulic conductivity were computed using the saxton equations saxton et al 1986 irrigated land cover data of the south platte river basin were obtained from the gis database of colorado s decision support systems cdss 2010 these three gis layers soil climate and land use were then intersected and any small slivers 1 ha were merged into their longest shared edge neighbors the intersection created 16 651 polygons of various sizes for irrigated corn in the south platte river basin including 1263 unique combinations of soil type and weather data daycent input strata each unique stratum was simulated with daycent and the results were then linked back to their associated landscape polygons for further calculation and spatial visualization to facilitate similar landscape studies we created an arcgis add in toolbox to automate the geospatial data processing for daycent landscape simulation 2 2 3 simulation of baseline conditions before conducting experimental simulations for each stratum we initialized daycent using pre settlement and historical agricultural land use assumptions at the major land resource area mlra scale as described in ogle et al 2010 this is the same procedure used in the inventory of u s greenhouse gas emissions and sinks us epa 2014 and the comet farm system http cometfarm nrel colostate edu the model initialization included two stages an initial spin up followed by a simulation of historical land use the spin up simulation defines steady state i e equilibrium soil carbon stocks assuming little or no disturbance from human activities prior to the conversion to agriculture grassland was chosen as the native vegetation and was simulated with daycent for 5000 years subsequently the model run was extended from this equilibrium condition with historical land use assumptions for each mlra there were five mlras within the study region with different areal coverage percentage including g67b 84 2 h72 15 3 g67a 0 3 and e49 0 17 and e48a 0 03 based on data compiled by ogle et al 2010 a two year rotation between bare fallow and winter wheat was assigned to g67a g67b e49 and e48a for the period from 1881 to 1979 where plow out of the native grassland was set to occur in 1880 as for h72 a four year rotation of corn corn spring small grain winter wheat was assigned from 1881 to 1920 and a three year rotation of corn spring small grain alfalfa was assigned from 1921 to 1979 typical management practices for each crop over time were compiled from various sources e g usda ers 1997 usda nass 2004 iback and adams 1967 smalley and engle 1942 to complete the model initialization we extended the historical baseline conditions from 1980 to 2015 with a business as usual scenario bau for irrigated corn production in colorado the bau reflects a system state under current management practice and is used to evaluate the magnitude and direction of change due to subsequent changes in management practices corn in northern colorado is usually planted between april 28 and may 20 and harvested between october 8 and november 13 nass 2010 the average bau management practices reported by the economic research service s agricultural resource management survey tailored reports ers arms 2010 for colorado s irrigated corn crop were 170 kg of nitrogen per hectare 34 cm of irrigated water per growing season 75 residue removal and conventional tillage 2 2 4 management practice scenarios to examine the impacts of n fertilization and irrigation on the provision of different agricultural es and eds we ran our forward simulations with n fertilizer and irrigation rates set to 20 40 60 100 120 140 and 160 of the bau level 68 102 136 170 204 238 and 272 kg of n per ha and 13 6 20 4 27 2 34 0 40 8 47 6 and 54 4 cm water year 1 respectively the permutation of these fertilizer and irrigation rates yielded 49 management practice scenarios we simulated these 49 management practice scenarios for each polygon for a 30 year period extending from 2016 to 2046 the percentage of stover removal was kept constant at the baseline level 75 and only conventional tillage was considered we did not consider the effects of changes in co2 concentration and climate on crop production and water use efficiency the combination of 1263 daycent input strata and 49 management practice scenarios yielded 61 887 daycent simulations these simulations were executed in parallel on an 18 node 216 processor cluster computing system at colorado state university s natural resources ecology laboratory 2 2 5 ecosystem service and disservice indicators daycent simulation outputs were used to estimate multiple es and eds indicators for each simulation soil organic carbon soc and net primary productivity npp were chosen as es indicators representing ecosystem carbon storage and biomass production services respectively water use nitrogen n leaching and greenhouse gas emissions ghg were chosen as eds indicators representing water provision water quality and climate regulation disservices since soc dynamics are a long term effect of management practices soil properties and weather conditions we reported the net change in soc in mg of carbon per hectare at the end of the simulation period to capture the cumulative effects of changing management practices due to the large seasonal variations other indicators were reported as flow variables averaged over the entire analysis including mg of carbon dioxide equivalent co2e per hectare per year for ghg kg of nitrogen per hectare per year for n leaching mg of dry weight biomass per hectare per year for npp assuming 43 5 carbon content gesch et al 2010 and cm of water per year for water use the values of soc and npp indicators were taken as direct daycent outputs while soil ghg emission estimates included modeled outputs from daycent as well as additional models e g for indirect n2o emissions from the u s department of agriculture entity scale greenhouse gas inventory guidelines eve et al 2014 annual net co2 emission was assumed to come mainly from soil microbial respiration of soc and was computed by taking the difference of soc levels between two consecutive years annual n2o emission comprised of direct and indirect n2o emissions the direct n2o emission was reported by daycent in term of n2o efflux while the indirect n2o emission was computed from nox efflux volatilized nh3 and nitrogen leaching with the emission factors ef of 0 01 0 01 and 0 075 respectively annual n2o and ch4 were converted into co2 equivalents by using 100 year global warming potential values gwp100 of 298 and 25 respectively ipcc 2006 n leaching was calculated from the amount of inorganic and organic n leached out of the soil profile since we did not model the effect of terrain slope and stream water flow the n leaching values only reflected the potential amounts of nitrate that can enter the ground water the ecosystem water use over a growing season was calculated as the total amount of water used to sustain all ecosystem processes including evapotranspiration the water utilized by the crop to produce biomass and other processes such as sublimation the vaporized water from snow and ice interception by the crop canopy run off and deep drainage to simplify the indicator comparisons between different management scenarios we computed the area weighted average value of each indicator using the following equation 1 v i j 1 16 651 a j v j a where v i area weighted average value of an es or eds indicator i j the j th polygon within the landscape a j the area of the j th polygon within the landscape ha v j the value of es or eds indicator i of the j th polygon within the landscape a the total area of the study region a 116 959 ha 2 3 calculation of pareto frontiers and optimization of landscape designs to examine the magnitude of trade offs between npp and other es and eds indicators in the study region we used a simple non dominated sorting algorithm to identify the pareto frontier for each pair of npp and es or eds indicators see supplementary material s1 these pareto frontiers illustrate macro trade offs between npp and other indicators assuming that management practice scenarios were uniformly applied to the landscape uniform management in reality it is of greater importance to identify and visualize the optimum landscape design under variable management i e the optimum management practice scenario for each polygon within the landscape corresponding to a specific management objective to do this we used a linear optimization algorithm that seeks to maximize the following objective function 2 maximize i n c i x i where x i the normalized value of es or eds indicator i x i x i p i m i n p i m a x p i m i n x i the value of es or eds indicator i p i min and p i max the population minimum and maximum of es or eds indicator i respectively c i the objective function coefficient a k a weighting factor corresponding to x i and n total number of es and eds indicators considered in the optimization n 5 since all the es and eds indicators were expressed in different units and scales we first normalized the indicators to a uniform scale 0 1 and used the normalized values in the objective function for a fair comparison hwang et al 1980 the optimization algorithm was applied to every polygon in the landscape to identify the optimum fertilizer and irrigation rates for each individual polygon seven management objective scenarios representing four different hypothetical management objectives by the landowner were chosen for the optimization table 1 for each scenario a set of objective function coefficients was assigned to the es and eds indicators we limited the coefficients to integer numbers that ranged from 2 to 2 in our analysis the coefficient absolute values indicated the importance of one indicator relative to the others in the objective function eq 2 and the sign of the coefficients reflected the indicator s direction of change that we wanted to optimize for example in scenario 2a increasing npp and decreasing water use expressed in the normalized unit term were set to be twice as important as increasing or decreasing other indicators except for scenario 1 where we equally considered all es and eds indicators coefficients 1 other management objective scenarios were biomass centric i e higher positive npp coefficients as biomass production is the primary provisioning goal of the study region in addition scenarios 2 3 and 4 focused on reducing eds coefficients 2 including water use n leaching and ghg respectively among these management objective scenarios the a scenarios put additional constraints on the remaining es and eds indicators while the b scenarios excluded them from the optimization table 1 we further compared the eds footprints between different management objectives the eds footprint is defined as the amount of eds indicator produced per unit of npp the eds footprints for a polygon under a specific management objective were calculated using the following equation 3 f i j k x i j k n p p j k where f i j k footprint of eds i for polygon j under management objective scenario k x i j k the value of es or eds indicator i for polygon j under management objective scenario k npp j k the value of npp indicator for polygon j under management objective scenario k the optimization results were mapped for spatial visualization using arcgis software esri 2014 we performed descriptive statistical analysis e g distribution central tendency and dispersion to explore es and eds indicators data see supplementary material s2 and one way anova analysis to test the effects of fertilizer and irrigation on each indicator for better presentation of the results see supplementary material s3 the simulation processes data analyses and map generation routines were automated using the python programming language https www python org our derived model was named the agricultural ecosystem service optimization ag ecosopt for the sake of future studies and development 3 results 3 1 effects of fertilizer and irrigation on es and eds indicators there were various combined effects of fertilizer and irrigation on soc npp n leaching and ghg emission fig 3 the rate of change of an indicator at a specific irrigation rate is defined as the difference in indicator values between the two fertilizer levels rise run higher fertilizer rates resulted in higher soc npp n leaching and ghg emission as the fertilizer rate increased soc and npp responded with a decreasing rate of change until a maximum value was reached asymptotically while n leaching and ghg emission rose linearly or with an increasing rate of change irrigation rate was proportional with the steepness of the slopes of the indicator s response lines curves the effects of irrigation on increasing soc npp and n leaching diminished at the irrigation rates above 47 6 cm year 1 we also found that reducing irrigation rates amplified net ghg emission within any specific fertilizer level below 238 kg ha 1 this is due to the increasing contribution of net co2 flux from soil c stock change in the total ghg emission see supplementary material s4 since we did not find statistically significant impacts of fertilizer application rate on total water use as determined by one way anova f 6 27502 0 00 p 1 0 see supplementary material s3 we only reported total water use against the irrigation rate averaged across all fertilizer levels fig 4 while water use was linearly correlated with irrigation different water use components responded differently to increasing irrigation rate in particular the evapotranspiration increased with a decreasing rate and plateaued at the irrigation rate of 34 cm year 1 whereas the water use from other processes rose slowly at lower irrigation rates and soared as soon as the evapotranspiration leveled off 3 2 trade offs between npp and other indicators the trade off between the es indicators i e npp and soc was a win win situation as increasing npp led to a linear increase in soc whereas those between npp and eds indicators were win loss situations since higher npp also came with higher eds fig 5 the uniform management pareto frontiers for npp and other eds revealed convex patterns this means that the marginal improvements of npp at the higher end will come with exponential penalties on environmental performance while trade offs between water use and npp were almost linear frontiers were increasingly convex for n leaching and ghg emissions all trade off frontiers showed a marked increase in eds to achieve npp greater than 21 mg ha 1 year 1 corresponding to average n fertilizer and irrigation rates above 136 kg ha 1 and 40 8 cm year 1 respectively this indicates that a relatively large fraction 90 of total potential system productivity can be realized while incurring only a relatively small fraction of total potential disservices through proper management practice decisions our results also showed the influence of fertilizer and irrigation in achieving certain optimal solutions i e scenarios that lie on the pareto frontiers the trade offs between npp and other indicators were affected by fertilizer limitation fl and irrigation limitation il which are states on a pareto frontier at which no further improvement of the optimum npp can be made without adding more fertilizer and irrigation respectively in the n leaching and ghg optimization cases optimal solutions with npp below this level were achieved by modulating irrigation rates while leaving fertilizer application at a minimum level additional npp gains beyond this point required increasing n fertilizer application causing a sharp increase in n leaching and n2o driven ghg emissions inappropriate applications of fertilizer and irrigation could lead to sub optimality in es eds trade offs i e scenarios that do not lie on the pareto frontiers this sub optimality was due to either excessive application of irrigation at the fl states fl sub optimality or excessive application of fertilizer at the il states il sub optimality fl sub optimality reduced npp as irrigation increased while il sub optimality improved npp with enormous costs on other es and eds with increasing fertilizer although we found no significant il sub optimality in the trade offs between npp and soc fig 5a and between npp and water use fig 5b the addition of fertilizer at the il states demonstrated a waste in resource investments 3 3 trade offs between management objectives fig 6 presents the es eds trade offs for the optimization of the management practices under seven management objective scenarios defined in table 1 in general we observed slight modulations of es and eds around the uniform management pareto frontiers except for the objective 2b unconstrained optimization of npp and water use where high sub optimality in n leaching and ghg emissions occurred for this scenario 2b our model allowed higher fertilizer application table 2 because there was no constraint placed on either reducing n leaching or ghgs our results also showed that allowing variable management during optimization obviously improved landscape performance in terms of n leaching as compared to the uniform management pareto frontiers i e objectives 3a 3b and 4a in fig 6c 3 4 landscape design of management practices and the spatial distribution of eds footprints the optimum spatial designs of fertilizer and irrigation rates and the spatial distribution of eds footprints of irrigated corn polygons in the south platte river basin were visualized for the management objective scenarios with constraints 1 2a 3a and 4a figs 7 and 8 those for management objective scenarios 2b 3b and 4b can be obtained from supplementary material s5 the distributions of optimal fertilizer and irrigation rates were heterogeneous throughout the landscape with many visible clusters of high and low values while the optimum rates of irrigation varied widely from 13 6 to 47 6 cm year 1 those of fertilizer only varied below the bau fertilizer level of 170 kg ha 1 the eds footprint values also varied greatly within the study region depending on the management objective and the eds indicator we observed some clusters of high eds footprint values in the areas north of denver and around the town of fort morgan 4 discussion pressure from the population and industrial expansion in our study region will result in higher demands for food biomass and water the question is how much of a viable productive agriculture is possible with declining water supply and what are the environmental impacts to answer this we compared the area weighted landscape sum of es and eds indicators and the corresponding management practices under the bau scenario table 3 with those from the landscape optimization of the seven management objective scenarios fig 9 we found that ecosystem water use can be reduced by 1 9 8 6 relative to the bau scenario level through lowering irrigation by 3 4 17 3 respectively objective 1 2a and 2b however these reductions came with either declines in soc and npp objective 1 and 2a due to reduced n fertilization or increases in ghgs and n leaching objective 2b due to increased n fertilization this indicated that it was not possible to improve both water use and environmental performance at the current and higher corn production level by simply adjusting fertilizer and irrigation application rates however our results implied that these management practices can be modulated to attain higher food and biomass provision at the lowest costs on ecosystems for the study region one advantage of the spatially explicit approaches for es and eds quantification is the ability to visualize the fine scale details and the hidden spatial patterns of landscapes deangelis and yurek 2017 detailed maps such as figs 7 and 8 have great utility for research and policy applications since they can be used in combination with other spatial layers for more complex quantitative spatial analyses and or to identify potential hotspots i e significant spatial clusters of high values of ecosystem threats for specific policy foci while our study involved a simple optimization problem in a geographically limited corn production landscape the approach and the resulting model would be equally applicable for more complex problems such as those with many constraints broader scopes or multiple stakeholders more constraints can be formulated to reflect biophysical barriers of the landscape policy regulation of environmental impacts and regional or local targets demands for ecosystem service production the system boundary can be expanded to include the optimization of multiple land cover scenarios or the supply chain optimization of multiple agricultural products the potential for the incorporation of economic optimization in this type of analysis allows not only the identification of the economically optimal solutions but also a better representation of the farmer s perspective and influences of the market economy lautenbach et al 2013 the involvement of stakeholders in the analysis would help to identify the most preferred problem solutions miettinen et al 2008 for instance our optimization results could be used as the pareto optimal starting points to show to the decision maker the decision maker would then be expected to express preference for es eds trade offs the preference information would be used to adjust the coefficients of the objective function for a new optimization simulation this process could be iterated until the most desirable solution was attained our analysis might also have practical implications for direct decision makers like farmers if results of such analysis were made available to farmers they could adjust their management decisions on their own farms accordingly for the most efficient production farmers could also report es eds trade offs in their farm planning and budgeting we believe that making the information more available and accessible to stakeholders would be the best way to propagate the es and eds concepts boost their communication and facilitate the sprouting of es and eds market mechanisms paustian et al 2009 despite the potential advantages of the approach we do realize some drawbacks in the methodology including but not limited to the expertise challenge and computational burden of the simulation the discretization of management practice variables the ambiguousness in the use of the objective function coefficients and the curse of dimensionality using process based models like daycent for detailed quantification of es and eds requires some level of model literacy and is computationally expensive for large area simulations in our case the analysis necessitated much effort for initial model setup and simulation time took several hours on a cluster of 216 high performance computer cores the infrastructure requirements for this type of analysis could impede the adoption of this approach for similar or more complex studies a possible solution to this disadvantage is the integration of the methodology into infrastructure ready tools or platforms to enable the flexible replications of es and eds analyses furthermore in order to alleviate the computational burden advanced meta modeling techniques such as support vector machine and neural network can be applied to create cost effective simplified surrogates of the detailed process based models wu et al 2016 based on these solutions further efforts are being made to prepare the infrastructure for our model ag ecosopt the analysis and optimization of es and eds based on fixed intervals of the continuous fertilizer and irrigation variables i e discretization might not guarantee actual optimal solutions since the solutions were forced to some discretized options however it significantly reduced the simulation time and supported the quality control of the analysis our method also used the normalized values of es and eds in the objective function with arbitrary objective function coefficients although this approach was good enough for the scope of our study finding the optimal landscape designs for pre defined sets of objective function coefficients based on discretized values of fertilizer and irrigation the unclear relationships between the coefficients and actual es and eds outcomes are a challenge for more complex analyses for example one might need to conduct a brute force search through all possible combinations of objective function coefficients to find the specific set that results in a certain level of es eds trade offs potential solutions for this problem are to involve the decision maker in interactive multi objective optimization wierzbicki et al 2000 to integrate more sophisticated algorithms that are capable of coefficients optimization such as levenberg marquardt backpropagation levenberg 1944 marquardt 1963 or to utilize different optimization solvers that allow the consideration of multiple objective functions with constraints and bounds configuration such as nsga ii deb 2011 last but not least the employment of this approach for more complex studies could be impeded with increasing dimensions of the analyses such as the addition of management practice types e g tillage and residue harvest considered levels for each management practice es and eds management objectives different land use cover types and land use cover change scenarios therefore a careful consideration of the potential trade offs between the scope of analysis and modeling capacity is always recommended for this type of study 5 conclusion we demonstrated a robust analysis of ecosystem services es and disservices eds trade offs for the irrigated corn growing area in south platte river basin of colorado the analysis integrated a high resolution quantification of es and eds associated with different fertilization and irrigation applications through the use of a process based biogeochemical model daycent the calculation of pareto frontiers for the trade offs among multiple es and eds considering uniform management practices and the identification of optimum fertilizer and irrigation rates of each polygon in the landscape based on different hypothetical management preferences we assessed five es and eds indicators including net primary production soil organic carbon water use nitrogen leaching and greenhouse gas emissions our results showed the influence of management practices and site specific factors on the provision of the es and eds and various trade offs among es and eds management practices eds footprints and management objectives inappropriate applications of fertilizer and irrigation could lead to sub optimal trade offs of es and eds although the analysis of es and eds trade offs is case specific we believe that our approach can be adopted for other locations as well as for more complex studies for decision support in sustainable agriculture further research is encouraged to improve this approach and to prepare the infrastructure for its widespread use in public and private sector decision making acknowledgements the work was supported by a usda nifa project grant 2011 67009 30083 decision support tool for integrated biofuel greenhouse gas emission footprints the fulbright vietnam scholarship the multidisciplinary approaches to sustainable bioenergy phd program funded by the united states national science foundation hue university of agriculture and forestry vietnam huaf and shell technology center houston the authors thank colleagues at the natural resource ecology laboratory nrel and at biodomain shell technology center houston for model training and data sources appendix a supplementary data the following are the supplementary data related to this article online data online data supplementary material supplementary material appendix a supplementary data supplementary data related to this article can be found at https doi org 10 1016 j envsoft 2018 06 006 
26366,flood inundation models are increasingly used for a wide variety of river and coastal management applications nevertheless the computational effort to run these models remains a substantial constraint on their application in this study four developments to the lisflood fp 2d flood inundation model have been documented that 1 refine the parallelisation of the model 2 reduce the computational burden of dry cells 3 reduce the data movements between cpu and ram and 4 vectorise the core numerical solver the value of each of these developments in terms of compute time and parallel efficiency was tested on 12 test cases for realistic test cases improvements in single core performance of between 4 2x and 8 4x were achieved which when combined with parallelisation on 16 cores resulted in computation times 34 60x shorter than previous lisflood fp models on one core results were compared to a sample of commercial models for context keywords flood inundation modelling hpc lisflood fp parallelisation vectorisation 1 introduction predictions of flood hazard from two dimensional flood inundation models form an essential component of flood risk management strategies in many countries de moel et al 2009 the use of these models has increased substantially over the last 20 years due in part to an increase in the availability of precise and accurate digital terrain models dtm datasets with sub meter resolution are becoming increasingly available in urban areas where fine resolution is needed to capture the complex flow pathways around urban structures schubert and sanders 2012 or resolve small scale flow connectivity neal et al 2011 yu and lane 2006 this need for high resolution inundation simulation results in a situation where computational resource becomes one of the main factors affecting simulation accuracy in practical applications two dimensional inundation models are also increasingly used at large scale for modelling of globally significant wetland systems de paiva et al 2013 yamazaki et al 2011 providing continental overviews of flood hazard alfieri et al 2014 dottori et al 2016 sampson et al 2015 vousdoukas et al 2016 or as the surface water flow component of landscape evolution modelling systems adams et al 2017b barkwith et al 2015 coulthard et al 2013 for these applications the size of the domain and the requirement to characterise model uncertainty through monte carlo simulation also creates significant computational cost where thousands of simulations can be necessary to explore the models parameter space a substantial body of work has been undertaken to address these issues with solutions falling into two categories 1 developments to the governing equations that improve the numerical schemes and 2 parallelisation of the code for application on multiple computational cores developments to the governing equations are wide ranging but often include simplification of the physical process representation such as the removal of inertia terms from the shallow water equations bates et al 2010 de almeida et al 2012 dottori et al 2016 or the omission of any floodplain dynamics gouldby et al 2008 winsemius et al 2013 the limitation of such an approach is that as the models become simpler the range of applications where they are applicable and simulation accuracy typically reduces vousdoukas et al 2016 by contrast parallelisation does not change the model simulation and typically involves implementation of the model over multiple processors via message passing neal et al 2010 sanders et al 2010 threading on shared memory central processing units judi et al 2011 leandro et al 2014 neal et al 2009a petaccia et al 2016 or by offloading work onto graphical processing units gpus kalyanapu et al 2011 lamb et al 2009 petaccia et al 2016 vacondio et al 2017 however as technology continually develops it becomes periodically necessary to revisit the optimisation of these numerical schemes in order to benefit from the enhanced capabilities of new hardware it is also necessary to understand the potential benefits of undertaking code development work and if perceived improvements to the code are realised across a wide range of realistic test scenarios this paper revisits the parallelisation of a numerically efficient two dimensional flood inundation model lisflood fp on multicore 86 cpu processors to investigate what changes to the code structure are most beneficial in order to utilise recent developments in cpu architecture in addition to substantial refinements to the parallelisation of the model we document the impact of vectorising the numerical scheme adapting how the code processes the model domain such that only wet cells are evaluated and writing the code to allow for better memory management by the compiler the performance of the model was evaluated using a range of test cases that are representative of typical inundation modelling applications since a substantial number of flood inundation modelling codes exist we hope that this short paper will provide useful information for researchers and practitioners developing their own model 2 model description the lisflood fp code was used as the hydraulic model in this study but is typical of a wide range of similar schemes the model solves the shallow water equations without the convective acceleration term on a staggered cartesian grid using an explicit finite difference method numerically this involves calculating the flow between cells given the mass in each cell momentum equation eq 1 and the change in mass in each cell given the flows between cells continuity equation eq 2 these equations including their derivation are reported in detail elsewhere bates et al 2010 de almeida et al 2012 and are therefore only briefly outlined here the momentum equation is described by 1 q i 1 2 t δ t q i 1 2 t g a f l o w t δ t s i 1 2 t 1 g δ t n 2 q i 1 2 t r f l o w t 4 3 a f l o w t where q i 1 2 t δ t is the flow rate in between two cells i and i 1 that will apply from time t to t δt a f l o w t is the area of flow between cells r f l o w t is the hydraulic radius s i 1 2 t is the water surface slope between cells n is manning s roughness coefficient and g is acceleration due to gravity for each cell the momentum equation is implemented at all four interfaces with its neighbours before applying the continuity equation to the cell 2 v i j t δ t v i j t δ t q i 1 2 j t δ t q i 1 2 j t δ t q i j 1 2 t δ t q i j 1 2 t δ t where v is the cell volume from which water surface elevation is easily computed while i and j index the cartesian grid the model also includes subroutines to simulate rainfall routing of flows over steep surfaces sampson et al 2013 1d river channels neal et al 2012a evaporation from open water and some hydraulic structures bates et al 2016 details of these are available in the lisflood fp user manual bates et al 2016 in practical terms the calculation stencil for the momentum equation never exceeds the two neighbouring cells while the continuity equation stencil requires only the four adjacent flow estimates plus any source terms e g rainfall evaporation runoff therefore the domain can be easily decomposed and run on separate cores making the scheme simple to parallelise as demonstrated by previous studies neal et al 2010 the left hand side of fig 1 describes the sequence of operations used by lisflood fp after it was parallelised and presented by neal et al 2009a for the purpose of this paper this will be called original lisflood fp and represents the basic architecture of all lisflood fp versions between neal et al 2009a and this paper this code architecture is a logical way of solving the governing equations and we would imagine can be widely adopted after reading the necessary input data and parameters from disk this version of the model simulates the hydrodynamics using five functions that each loop across the model domain fig 1a undertaking the following numerical operations 1 calculate eq 1 in the x direction between all cells 2 calculate eq 1 in the y direction for all cells 3 implement a variant of eq 1 along all model boundary cell edges 4 add any source terms to the cells and 5 implement eq 2 for all cells each loop is easily parallelised as shown in the pseudo c code in fig 2 which is applicable to most explicit hydrodynamic models unfortunately the layout of this code has a number of potentially significant limitations the significant of which we will investigate in the results section that may compromise computational efficiency and which can be summarised as 1 parallel loop structure each loop requires the creation of new threads that increase the overhead associated with parallelisation 2 wet and dry cells a loop will access data for each cell regardless of whether that cell is wet or not e g on a dry domain data will be repeatedly accessed but no computation undertaken 3 data access the loops repeatedly access the same dem and parameter data from memory meaning data must repeatedly be moved from ram to the processor 4 vectorisation the work within the loop is undertaken on a cell by cell basis and thus does not take advantage of potential vectorisation available on the processor 2 1 optimisation the four issues above were addressed by making the following changes to the code with the new structure summarised by the flow diagram in fig 1b 2 1 1 parallel loop structure in the optimised code threads were created at the start of the simulation rather than for each parallel for loop the change is illustrated by the pseudo code in fig 3 setting up the threads in this way is reasonably straightforward however unlike the situation where each loop is parallelised all sections of the code that do not run in parallel need to be identified threads process rows of data in the model domain with a nowait instruction used to let the compiler know that a thread can begin processing another row without waiting on other threads to finish we assessed the parallel performance of the model by comparing the original and optimised version of the model across a range of test cases using 1 to 16 threads 2 1 2 wet and dry cells a simple tracking of the wet edge during an inundation simulation was implemented which allows the numerical scheme to be active over a smaller portion of the model domain this is by no means a new idea and the idea of tracking only wet cells has been around for some time for example the original jflow scheme bradbrook 2006 maintains a look up list of wet and newly wet cells despite using a raster grid while the ceasar model adapts its active calculations in time i e by missing out periods of minimal dynamics coulthard et al 2013 for each row of the model domain the cells are indexed from left to right in ascending order when the simulation starts the wet cells with the lowest and highest index i start i end are identified in each row with i start set greater than i end when the row is dry these indices are then expanded to align in memory when necessary e g i start might be reduced to fall on a memory block boundary and used to define which cells in the row are considered by the numerical scheme when a cell wets or dries a check is made to see if the indices need to be changed and a check is also made for any source terms in the domain the test cases in the results section will be used to assess the overhead of this scheme and its expected benefits for realistic simulation cases one limitation of this simple approach occurs when dry cells are located between wet cells on a row there is potentially an additional speedup to be gained over our approach by not visiting these cells which would be done by expanding the algorithm to track multiple wet and dry edges per row we have not assessed when such additional complexity could yield a faster simulation 2 1 3 data access for most hydrodynamic models the numerical effort required to calculate flow and update cell volume is such that the program will become inefficient if the computer has to do a lot of work moving data around in memory gibson 2015 leandro et al 2014 by far the most significant change to the code was to rearrange the data access such that fewer movements of data between ram and core cache are required this is not something that the developer specifically controls but requires the code to be written in a structure that the compiler can more easily optimise the most significant change to the structure made here was to combine the calculations of flow in x and y rather than have this arranged in two independent functions see fig 1 box a such that each cell is visited only once during the momentum calculation the same applies for the continuity equation with respect to source terms such as evaporation and precipitation furthermore the original version of the code stores data for each variable e g elevation depth as continuous blocks for the whole model domain in the optimised version the end of each row is padded such that the start of each rows data is 64 bit aligned which allows the threads easier and quicker access to these data than is the case where rows can start anywhere in memory these blocks are also numa aligned meaning the data are stored on ram closest to the cpu where the computation will occur reducing the need to move data between cpu sockets on the server 2 1 4 vectorisation the final code development step was to vectorise the momentum and continuity equations for each row using advanced vector extension avx as with improving how the code accesses data from ram this is not explicitly controlled by the developer instead we rewrote the core computational component of the solver such that the compiler was able to implement the vectorisation the code snippet in appendix a describes the implementation of the momentum equation between two cells in a manner that can be vectorised on an intel chip by the intel compiler note the hint to the compiler pragma simd indicating that it should be possible to vectorise the numerical scheme we also tested two ways of implementing the most numerically intensive component of the computation where the hydraulic radius r is raised to the power of 4 3 by comparing the use of a generic c power function pow r 4 0 3 0 against multiplying r by itself four times and taking the cubed root of that cbrt r r r r 3 test cases hydraulic models are used for a wide variety of applications meaning the performance of the modelling program needs to be robust across a representative range of test cases in particular computational performance is often found to change with the number of cells and the distribution of wet cells within the model domain neal et al 2009b therefore the new code was assessed using 12 models developed during previous research projects these models are listed in table 1 along within basic information on the processes simulated by each model and their size the models also have different grid resolutions time steps and number of simulation time steps which we include in table 1 for completeness the models also have different boundary conditions ranging from no boundary conditions 2d dry and 2d wet tests to rainfall inputs into every cell pluvial test most test cases have point inflow boundaries carlisle ea test 5 glasgow inner niger delta and severn with some having additional edge boundaries to allow flow to leave the domain carlisle inner niger delta severn new york has a time varying water surface boundary the total number of boundary cells is reported in table 1 the main aim of the test case selection was to characterise the performance of the two dimensional floodplain solver hence most of the test cases are models of differing sizes that only use this solver however a key reason for using a cpu over a gpu architecture is the flexibility to add physical processes as additional modules thus some models include additional physical processes modules column in table 1 detailed descriptions of each model will not be provided here but can be found in the referenced sources where appropriate however to aid the discussion of the results the digital elevation models fig 4 maximum simulated depths fig 5 and percent of domain flooded over time fig 6 are presented for the non synthetic test cases e g those with realistic topography and inundation patterns the synthetic tests cases are not plotted because they all use a dem of zero elevation everywhere and have a constant percentage of the domain flooded 4 results to assess the performance of the new code the 12 test cases were run on a dedicated node of the university of bristol supercomputer bluecrystal which has 16 2 6 ghz intel e5 2670 sandybridge cores with 4gb core of ram therefore simulations were run on up to 16 real cores with cores left idle when less than 16 threads were created for the 16 core simulations each model was run three times with the shortest simulation time presented here other simulations were run just once due to the longer simulation times on fewer cores in this paper simulation time represents only the computation time needed to undertake the simulation and excludes the reading and writing of results at the start and end of the simulation as these depend on the supercomputer file store which is shared by other users the intel c compiler version 13 1 for linux was used throughout table 2 records the compute times for the 12 test cases for the original lisflood fp model and optimised version on one and 16 cores the global flood model simulation required the longest simulation time of up to 275 k second 76 h while the dry test case could take as little as 0 3 s parallel speedups for the two versions of the code are also shown omp speedup along with a comparison between code on 1 and 16 cores before considering the implications of these results we will deal with three caveats relating to results highlighted in italic and red for the original model the pluvial simulation which is the only model to include the runoff routing scheme of sampson et al 2013 obtained the worst speedup of 3 2x this was unsurprising given that the routing component of this model was not implemented in parallel in the original code and means that the 15 7 times speedup between the 16 core optimised and original model is largely attributable to this improvement the other two highlighted models are the new york and global flood model test cases for these models single core original lisflood fp model simulation time has been estimated from the two core and eight core original lisflood fp simulations respectively this was necessary due to the long compute times needed by these models exceeding the supercomputer time limits this means that the parallel speedups for the original model are likely to have been overestimated in these cases as would the improvement in simulation time between the original and optimised codes results from the optimised model and comparison between the respective 16 core simulation times are unaffected the synthetic dry test case had the greatest speedup between the optimised and original code with the optimised code executing two orders of magnitude faster this was expected due to the implementation of wet edge tracking in the optimised version of the code parallel speedups for this test case with the optimised model are the lowest of any test case 3 5 x due to the lack of work required by each thread the synthetic all wet test case had the greatest parallel speedup for both the original and optimised codes 13 4 10 1 x respectively speedup between the original and optimised code was also relative high 6 7 10 1 x both these results were expected because the omp threads will all have an equal amount of work to undertake the vectorisation will be most efficient when all cells are wet and the computational work verses data accessed by the cpu will be maximised e g you need to undertake the computationally expensive flow calculation for every cell interface in the domain although the synthetic test cases are interesting they are not representative of most real applications and therefore the majority of model simulations run by scientists and practitioners the remaining simulations on actual dems show parallel speedups of between 4 2 x and 8 2 x with large wet test cases such as new york tending to parallelise more efficiently than small domains such as glasgow and dry domains such as ea test 5 see max extents in fig 5 and percentage wet statistics in fig 6 that larger domains tend to improve parallel efficiency has been well reported in the literature leandro et al 2014 neal et al 2009a the presence of the 1d channel model generally reduces the parallel speedup interestingly the speedup via code optimisation was greater than the speedup due to parallelisation in over half of the test cases highlighted in table 2 in bold and of similar order in the others therefore we find that optimising the code layout to allow for vectorisation implementing a wet dry edge tracking approach and minimising the data movements during computation are as beneficial in terms of runtime as parallelisation on the processors used here it is difficult to explicitly quantify the benefits of each improvement we made to the code because there is likely to be strong interaction effects between the various changes for example the combined effect of reducing the data movement and implementing vectorisation will not be the sum of the two efforts in isolation it was also not possible to implement the vectorisation without significantly changing the structure of the code and data from the original model nevertheless we were able to disable a number of the optimisation steps to establish an indicative measure of how valuable these were in reducing the compute time the results of disabling the vectorisation wet edge tracking and numa alignment a type of memory access optimisation are summarised in fig 7 along with a comparison with the original model simulation times all these results use floating point precision and 16 cores for the synthetic dry domain the wet edge tracking is confirmed as the major contributor to the improvement in performance disabling this feature increases the optimised model simulation time by 124 times indicated by the number on top of the bars for the 2d model the vectorisation has essentially no effect as expected due to the lack of any work to perform therefore improvements to the code structure account for the remaining speedup over the original model totalling 539 x for the wet 2d simulation disabling the vectorisation increases the simulation time by 2 8 x since the single instruction multiple data vectorisation available on sandy bridge cores can accommodate up to four floating point numbers in parallel this speedup is a substantial portion of the theoretical maximum 4 x for real test cases removing the vectorisation increased computation time between 1 8 x severn and 3 5 x carlisle with three models having values below 2 x inner niger delta severn pluvial test the inner niger delta and severn domains are characterised by a relatively small and fragmented pattern of flooding connected by 1d channels figs 5 and 6 while the runoff routing model in the pluvial test cases is not vectorised these factors potentially explain the limited improvement brought about via vectorisation for these tests however as no test took longer when the vectorisation was enabled we conclude from these tests that the vectorisation will be universally beneficial in terms of compute time disabling the wet edge tracking resulted in a slowdown of between essentially nothing for carlisle and 1 86 x for the seasonally dry inner niger delta as with vectorisation the wet edge tracking was sufficiently cheap that no simulation showed a noticeable slowdown when it was implemented disabling numa alignment has no substantial effect on any test case although this might become more significant on machines with more cores it was impossible to isolate many of the reductions in data transfer between code functions that we made and these structural improvements to the code as outlined by fig 1 are thought to yield most of the speedup from the original to optimised model not accounted for by vectorisation and wet edge tracking e g in the region of 2 4 x for the real test cases for completeness we also include the model speedups by number of cores in fig 8 to examine the parallel efficiency of the code parallel efficiency varied strongly with the distribution of wet cells in the domain the completely wet models remain near 100 efficient in terms of speedup until 8 cores are in use however the dry test shows essentially no speedup after 4 cores due to the lack of parallel computation relative to serial overheads except for the dry test cases parallel efficiency remained similar between 4 and 16 cores suggesting the parallelisation is scaling well 5 comment on results relative to previous studies benchmarking of hydrodynamic models from a computational performance perspective is a difficult task because the test case can have a significant influence on the relative model performance the solvers used and their accuracies vary substantially and codes are rarely compared on identical hardware meaning relative performance might vary by hardware and compiler nevertheless it is worth placing the results here in a wider context the benchmarking exercise by néelz and pender 2010 was one of the most comprehensive efforts to benchmark the main commercial and research focused two dimensional hydrodynamic modelling codes they report simulation times for several models for ea test 5 and the hardware used for the simulation although the néelz and pender study is a few years old it did report results for the original lisflood fp model which allows a comparison to be made simulation times of 9 168 s were reported by néelz and pender for ea test 5 and are reproduced in table 3 the original lisflood fp model required 28 2 s using 8 cores and was competitive on simulation time but not especially quick for this particular case note that the relative position of the models varied from test to test in néelz and pender 2010 in this study the same simulation on an identical number of cores was 1 3 x faster at 21 5 s for the original model reducing to 2 8 s 10 x for the optimised model on eight cores although this comparison is rather limited for the reasons outlined above it confirms that our optimisation efforts are relevant and substantive within the wider flood inundation modelling context 6 conclusions for 2d hydrodynamic simulations our code developments yielded between 4 2 and 8 4 x speedups when the model was run on the same number of cores while the speedups from a single core implementation of the original lisflood fp to 16 core simulations on the new code was between 34 and 60 x under idealised conditions where the whole domain was wet code speedup was up to 111 x interestingly roughly the same improvement in numerical efficiency was achieved through code development as was achieved through parallelisation on a 16 core cpu in relation to the four development areas identified in this paper the following conclusions can be drawn from this work 6 1 parallel loop structure for shared memory parallelisation using openmp the original version of lisflood fp yielded speedups of 5 13 x on 16 cores by simply implementing for loops in parallel by restructuring the parallelisation to create threads at the start of the simulation rather than within each loop it was possible to maintain this parallel efficiency despite other developments to the code reducing the work done by the rest of the model by a similar margin this change is likely to be applicable to many models where processes are often written as separate sub routines with their own loops and parallel loop definitions the disadvantage of the restructuring was that care had to be taken to identify areas of the code that could not run efficiently in parallel for example structures or that must run sequentially for example calculations of momentum and continuity however these cases could all be handled with barrier and omp single commands that force all threads to complete before moving on and force a single thread implementation respectively 6 2 wet and dry edge tracking implementing a simple wet edge tracking approach yielded substantial improvements in compute time for most realistic tests cases while the overhead of tracking the edge was sufficiently low that this was not observed in the simulation where the whole domain was wet implementing such a scheme where something more advanced doesn t already exist would therefore be expected to yield a 0 2 x speedup for most test cases and substantially more for very dry domains 6 3 data access improving memory access by the code was believed to be a substantial limiting factor on numerical efficiency in the tests conducted here it is difficult to isolate how the restructuring of the code improved compute time however given the overall model performance statistics and the performance of the models when other optimisations were disabled it is likely that these developments account for a halving of the compute time over the original simulation time 2 x speedup aligning data access and improving the data movement during simulations was also an intrinsic component of the vectorisation process and we suspect this will not have been so successful without this initial reorganisation of the code structure the most expensive component of the momentum equation is raising the hydraulic radius to the power of 4 3 we tested the use of a specific cubed root function over the more general power function and found a small improvement in model performance on some test cases 6 4 vectorisation the benefits of vectorising the solver will vary depending on the cpu however for an simd register that supports a vector length of four floating point number speedups was as high as 3 5 x when avx was enabled furthermore enabling vectorisation did not increase computation time for any of our tests cases even when the flood inundation was quite fragmented e g inner niger delta overall this paper has documented several model development steps that can yield quite substantial improvements in model performance on standard computer hardware these improvements were more substantial than the reduction in computation time of 3 5 x between a full shallow water model and the local inertia implementation when both were implemented in the lisflood fp code neal et al 2012b we hope that other developers and researchers will find the steps we have taken a useful when considering their own model development plans there are several numerical schemes that directly use the numerical approach adopted here e g adams et al 2017a b coulthard et al 2013 courty et al 2017 dottori et al 2016 however all explicit hydrodynamic model on regular grids and many process based models in geosciences have a small stencil of neighbours where information cannot travel more than one cell in any time step these could therefore all be optimised with the methods outlined here 7 software and data availability the lisflood fp software is developed by the university of bristol a freeware version of the code for non commercial use can be downloaded from the universities website http www bristol ac uk geography research hydrology models lisflood downloads the lead developers are dr jeffrey neal corresponding author and prof paul bates at the school of geographical sciences university of bristol bs8 1ss uk lisflood fp is written in c and can be complied for windows and linux version 6 of the code used in this study requires a processor hardware with avx capability or newer the code is not open source however we give access to the lisflood fp code repository to numerous research collaborators researchers interested in accessing the code are encouraged to email the lead authors or access the open source version of the code associated with the paper by hoch et al 2017 the test cases from this paper that use open data are available on the mendeley link acknowledgements toby dunne was supported by the nerc impact accelerator account at the university of bristol while chris sampson and andy smith were supported by nerc via ne m007766 1 appendix a image image appendix a code snippets describing an implementation of the lisflood fp momentum equation that can be vectorised using advanced vector extension where i is the cell index h is water depth z is bed elevation hflow cross sectional depth of flow area is the cross sectional flow area dh is the difference in water surface elevation between adjacent cells row dy and row dx are the cell widths in x and y directions delta time is the model time step g is acceleration due to gravity g friction sq x grid is the friction squared and qx old grid is the discharge from the previous time step 
26366,flood inundation models are increasingly used for a wide variety of river and coastal management applications nevertheless the computational effort to run these models remains a substantial constraint on their application in this study four developments to the lisflood fp 2d flood inundation model have been documented that 1 refine the parallelisation of the model 2 reduce the computational burden of dry cells 3 reduce the data movements between cpu and ram and 4 vectorise the core numerical solver the value of each of these developments in terms of compute time and parallel efficiency was tested on 12 test cases for realistic test cases improvements in single core performance of between 4 2x and 8 4x were achieved which when combined with parallelisation on 16 cores resulted in computation times 34 60x shorter than previous lisflood fp models on one core results were compared to a sample of commercial models for context keywords flood inundation modelling hpc lisflood fp parallelisation vectorisation 1 introduction predictions of flood hazard from two dimensional flood inundation models form an essential component of flood risk management strategies in many countries de moel et al 2009 the use of these models has increased substantially over the last 20 years due in part to an increase in the availability of precise and accurate digital terrain models dtm datasets with sub meter resolution are becoming increasingly available in urban areas where fine resolution is needed to capture the complex flow pathways around urban structures schubert and sanders 2012 or resolve small scale flow connectivity neal et al 2011 yu and lane 2006 this need for high resolution inundation simulation results in a situation where computational resource becomes one of the main factors affecting simulation accuracy in practical applications two dimensional inundation models are also increasingly used at large scale for modelling of globally significant wetland systems de paiva et al 2013 yamazaki et al 2011 providing continental overviews of flood hazard alfieri et al 2014 dottori et al 2016 sampson et al 2015 vousdoukas et al 2016 or as the surface water flow component of landscape evolution modelling systems adams et al 2017b barkwith et al 2015 coulthard et al 2013 for these applications the size of the domain and the requirement to characterise model uncertainty through monte carlo simulation also creates significant computational cost where thousands of simulations can be necessary to explore the models parameter space a substantial body of work has been undertaken to address these issues with solutions falling into two categories 1 developments to the governing equations that improve the numerical schemes and 2 parallelisation of the code for application on multiple computational cores developments to the governing equations are wide ranging but often include simplification of the physical process representation such as the removal of inertia terms from the shallow water equations bates et al 2010 de almeida et al 2012 dottori et al 2016 or the omission of any floodplain dynamics gouldby et al 2008 winsemius et al 2013 the limitation of such an approach is that as the models become simpler the range of applications where they are applicable and simulation accuracy typically reduces vousdoukas et al 2016 by contrast parallelisation does not change the model simulation and typically involves implementation of the model over multiple processors via message passing neal et al 2010 sanders et al 2010 threading on shared memory central processing units judi et al 2011 leandro et al 2014 neal et al 2009a petaccia et al 2016 or by offloading work onto graphical processing units gpus kalyanapu et al 2011 lamb et al 2009 petaccia et al 2016 vacondio et al 2017 however as technology continually develops it becomes periodically necessary to revisit the optimisation of these numerical schemes in order to benefit from the enhanced capabilities of new hardware it is also necessary to understand the potential benefits of undertaking code development work and if perceived improvements to the code are realised across a wide range of realistic test scenarios this paper revisits the parallelisation of a numerically efficient two dimensional flood inundation model lisflood fp on multicore 86 cpu processors to investigate what changes to the code structure are most beneficial in order to utilise recent developments in cpu architecture in addition to substantial refinements to the parallelisation of the model we document the impact of vectorising the numerical scheme adapting how the code processes the model domain such that only wet cells are evaluated and writing the code to allow for better memory management by the compiler the performance of the model was evaluated using a range of test cases that are representative of typical inundation modelling applications since a substantial number of flood inundation modelling codes exist we hope that this short paper will provide useful information for researchers and practitioners developing their own model 2 model description the lisflood fp code was used as the hydraulic model in this study but is typical of a wide range of similar schemes the model solves the shallow water equations without the convective acceleration term on a staggered cartesian grid using an explicit finite difference method numerically this involves calculating the flow between cells given the mass in each cell momentum equation eq 1 and the change in mass in each cell given the flows between cells continuity equation eq 2 these equations including their derivation are reported in detail elsewhere bates et al 2010 de almeida et al 2012 and are therefore only briefly outlined here the momentum equation is described by 1 q i 1 2 t δ t q i 1 2 t g a f l o w t δ t s i 1 2 t 1 g δ t n 2 q i 1 2 t r f l o w t 4 3 a f l o w t where q i 1 2 t δ t is the flow rate in between two cells i and i 1 that will apply from time t to t δt a f l o w t is the area of flow between cells r f l o w t is the hydraulic radius s i 1 2 t is the water surface slope between cells n is manning s roughness coefficient and g is acceleration due to gravity for each cell the momentum equation is implemented at all four interfaces with its neighbours before applying the continuity equation to the cell 2 v i j t δ t v i j t δ t q i 1 2 j t δ t q i 1 2 j t δ t q i j 1 2 t δ t q i j 1 2 t δ t where v is the cell volume from which water surface elevation is easily computed while i and j index the cartesian grid the model also includes subroutines to simulate rainfall routing of flows over steep surfaces sampson et al 2013 1d river channels neal et al 2012a evaporation from open water and some hydraulic structures bates et al 2016 details of these are available in the lisflood fp user manual bates et al 2016 in practical terms the calculation stencil for the momentum equation never exceeds the two neighbouring cells while the continuity equation stencil requires only the four adjacent flow estimates plus any source terms e g rainfall evaporation runoff therefore the domain can be easily decomposed and run on separate cores making the scheme simple to parallelise as demonstrated by previous studies neal et al 2010 the left hand side of fig 1 describes the sequence of operations used by lisflood fp after it was parallelised and presented by neal et al 2009a for the purpose of this paper this will be called original lisflood fp and represents the basic architecture of all lisflood fp versions between neal et al 2009a and this paper this code architecture is a logical way of solving the governing equations and we would imagine can be widely adopted after reading the necessary input data and parameters from disk this version of the model simulates the hydrodynamics using five functions that each loop across the model domain fig 1a undertaking the following numerical operations 1 calculate eq 1 in the x direction between all cells 2 calculate eq 1 in the y direction for all cells 3 implement a variant of eq 1 along all model boundary cell edges 4 add any source terms to the cells and 5 implement eq 2 for all cells each loop is easily parallelised as shown in the pseudo c code in fig 2 which is applicable to most explicit hydrodynamic models unfortunately the layout of this code has a number of potentially significant limitations the significant of which we will investigate in the results section that may compromise computational efficiency and which can be summarised as 1 parallel loop structure each loop requires the creation of new threads that increase the overhead associated with parallelisation 2 wet and dry cells a loop will access data for each cell regardless of whether that cell is wet or not e g on a dry domain data will be repeatedly accessed but no computation undertaken 3 data access the loops repeatedly access the same dem and parameter data from memory meaning data must repeatedly be moved from ram to the processor 4 vectorisation the work within the loop is undertaken on a cell by cell basis and thus does not take advantage of potential vectorisation available on the processor 2 1 optimisation the four issues above were addressed by making the following changes to the code with the new structure summarised by the flow diagram in fig 1b 2 1 1 parallel loop structure in the optimised code threads were created at the start of the simulation rather than for each parallel for loop the change is illustrated by the pseudo code in fig 3 setting up the threads in this way is reasonably straightforward however unlike the situation where each loop is parallelised all sections of the code that do not run in parallel need to be identified threads process rows of data in the model domain with a nowait instruction used to let the compiler know that a thread can begin processing another row without waiting on other threads to finish we assessed the parallel performance of the model by comparing the original and optimised version of the model across a range of test cases using 1 to 16 threads 2 1 2 wet and dry cells a simple tracking of the wet edge during an inundation simulation was implemented which allows the numerical scheme to be active over a smaller portion of the model domain this is by no means a new idea and the idea of tracking only wet cells has been around for some time for example the original jflow scheme bradbrook 2006 maintains a look up list of wet and newly wet cells despite using a raster grid while the ceasar model adapts its active calculations in time i e by missing out periods of minimal dynamics coulthard et al 2013 for each row of the model domain the cells are indexed from left to right in ascending order when the simulation starts the wet cells with the lowest and highest index i start i end are identified in each row with i start set greater than i end when the row is dry these indices are then expanded to align in memory when necessary e g i start might be reduced to fall on a memory block boundary and used to define which cells in the row are considered by the numerical scheme when a cell wets or dries a check is made to see if the indices need to be changed and a check is also made for any source terms in the domain the test cases in the results section will be used to assess the overhead of this scheme and its expected benefits for realistic simulation cases one limitation of this simple approach occurs when dry cells are located between wet cells on a row there is potentially an additional speedup to be gained over our approach by not visiting these cells which would be done by expanding the algorithm to track multiple wet and dry edges per row we have not assessed when such additional complexity could yield a faster simulation 2 1 3 data access for most hydrodynamic models the numerical effort required to calculate flow and update cell volume is such that the program will become inefficient if the computer has to do a lot of work moving data around in memory gibson 2015 leandro et al 2014 by far the most significant change to the code was to rearrange the data access such that fewer movements of data between ram and core cache are required this is not something that the developer specifically controls but requires the code to be written in a structure that the compiler can more easily optimise the most significant change to the structure made here was to combine the calculations of flow in x and y rather than have this arranged in two independent functions see fig 1 box a such that each cell is visited only once during the momentum calculation the same applies for the continuity equation with respect to source terms such as evaporation and precipitation furthermore the original version of the code stores data for each variable e g elevation depth as continuous blocks for the whole model domain in the optimised version the end of each row is padded such that the start of each rows data is 64 bit aligned which allows the threads easier and quicker access to these data than is the case where rows can start anywhere in memory these blocks are also numa aligned meaning the data are stored on ram closest to the cpu where the computation will occur reducing the need to move data between cpu sockets on the server 2 1 4 vectorisation the final code development step was to vectorise the momentum and continuity equations for each row using advanced vector extension avx as with improving how the code accesses data from ram this is not explicitly controlled by the developer instead we rewrote the core computational component of the solver such that the compiler was able to implement the vectorisation the code snippet in appendix a describes the implementation of the momentum equation between two cells in a manner that can be vectorised on an intel chip by the intel compiler note the hint to the compiler pragma simd indicating that it should be possible to vectorise the numerical scheme we also tested two ways of implementing the most numerically intensive component of the computation where the hydraulic radius r is raised to the power of 4 3 by comparing the use of a generic c power function pow r 4 0 3 0 against multiplying r by itself four times and taking the cubed root of that cbrt r r r r 3 test cases hydraulic models are used for a wide variety of applications meaning the performance of the modelling program needs to be robust across a representative range of test cases in particular computational performance is often found to change with the number of cells and the distribution of wet cells within the model domain neal et al 2009b therefore the new code was assessed using 12 models developed during previous research projects these models are listed in table 1 along within basic information on the processes simulated by each model and their size the models also have different grid resolutions time steps and number of simulation time steps which we include in table 1 for completeness the models also have different boundary conditions ranging from no boundary conditions 2d dry and 2d wet tests to rainfall inputs into every cell pluvial test most test cases have point inflow boundaries carlisle ea test 5 glasgow inner niger delta and severn with some having additional edge boundaries to allow flow to leave the domain carlisle inner niger delta severn new york has a time varying water surface boundary the total number of boundary cells is reported in table 1 the main aim of the test case selection was to characterise the performance of the two dimensional floodplain solver hence most of the test cases are models of differing sizes that only use this solver however a key reason for using a cpu over a gpu architecture is the flexibility to add physical processes as additional modules thus some models include additional physical processes modules column in table 1 detailed descriptions of each model will not be provided here but can be found in the referenced sources where appropriate however to aid the discussion of the results the digital elevation models fig 4 maximum simulated depths fig 5 and percent of domain flooded over time fig 6 are presented for the non synthetic test cases e g those with realistic topography and inundation patterns the synthetic tests cases are not plotted because they all use a dem of zero elevation everywhere and have a constant percentage of the domain flooded 4 results to assess the performance of the new code the 12 test cases were run on a dedicated node of the university of bristol supercomputer bluecrystal which has 16 2 6 ghz intel e5 2670 sandybridge cores with 4gb core of ram therefore simulations were run on up to 16 real cores with cores left idle when less than 16 threads were created for the 16 core simulations each model was run three times with the shortest simulation time presented here other simulations were run just once due to the longer simulation times on fewer cores in this paper simulation time represents only the computation time needed to undertake the simulation and excludes the reading and writing of results at the start and end of the simulation as these depend on the supercomputer file store which is shared by other users the intel c compiler version 13 1 for linux was used throughout table 2 records the compute times for the 12 test cases for the original lisflood fp model and optimised version on one and 16 cores the global flood model simulation required the longest simulation time of up to 275 k second 76 h while the dry test case could take as little as 0 3 s parallel speedups for the two versions of the code are also shown omp speedup along with a comparison between code on 1 and 16 cores before considering the implications of these results we will deal with three caveats relating to results highlighted in italic and red for the original model the pluvial simulation which is the only model to include the runoff routing scheme of sampson et al 2013 obtained the worst speedup of 3 2x this was unsurprising given that the routing component of this model was not implemented in parallel in the original code and means that the 15 7 times speedup between the 16 core optimised and original model is largely attributable to this improvement the other two highlighted models are the new york and global flood model test cases for these models single core original lisflood fp model simulation time has been estimated from the two core and eight core original lisflood fp simulations respectively this was necessary due to the long compute times needed by these models exceeding the supercomputer time limits this means that the parallel speedups for the original model are likely to have been overestimated in these cases as would the improvement in simulation time between the original and optimised codes results from the optimised model and comparison between the respective 16 core simulation times are unaffected the synthetic dry test case had the greatest speedup between the optimised and original code with the optimised code executing two orders of magnitude faster this was expected due to the implementation of wet edge tracking in the optimised version of the code parallel speedups for this test case with the optimised model are the lowest of any test case 3 5 x due to the lack of work required by each thread the synthetic all wet test case had the greatest parallel speedup for both the original and optimised codes 13 4 10 1 x respectively speedup between the original and optimised code was also relative high 6 7 10 1 x both these results were expected because the omp threads will all have an equal amount of work to undertake the vectorisation will be most efficient when all cells are wet and the computational work verses data accessed by the cpu will be maximised e g you need to undertake the computationally expensive flow calculation for every cell interface in the domain although the synthetic test cases are interesting they are not representative of most real applications and therefore the majority of model simulations run by scientists and practitioners the remaining simulations on actual dems show parallel speedups of between 4 2 x and 8 2 x with large wet test cases such as new york tending to parallelise more efficiently than small domains such as glasgow and dry domains such as ea test 5 see max extents in fig 5 and percentage wet statistics in fig 6 that larger domains tend to improve parallel efficiency has been well reported in the literature leandro et al 2014 neal et al 2009a the presence of the 1d channel model generally reduces the parallel speedup interestingly the speedup via code optimisation was greater than the speedup due to parallelisation in over half of the test cases highlighted in table 2 in bold and of similar order in the others therefore we find that optimising the code layout to allow for vectorisation implementing a wet dry edge tracking approach and minimising the data movements during computation are as beneficial in terms of runtime as parallelisation on the processors used here it is difficult to explicitly quantify the benefits of each improvement we made to the code because there is likely to be strong interaction effects between the various changes for example the combined effect of reducing the data movement and implementing vectorisation will not be the sum of the two efforts in isolation it was also not possible to implement the vectorisation without significantly changing the structure of the code and data from the original model nevertheless we were able to disable a number of the optimisation steps to establish an indicative measure of how valuable these were in reducing the compute time the results of disabling the vectorisation wet edge tracking and numa alignment a type of memory access optimisation are summarised in fig 7 along with a comparison with the original model simulation times all these results use floating point precision and 16 cores for the synthetic dry domain the wet edge tracking is confirmed as the major contributor to the improvement in performance disabling this feature increases the optimised model simulation time by 124 times indicated by the number on top of the bars for the 2d model the vectorisation has essentially no effect as expected due to the lack of any work to perform therefore improvements to the code structure account for the remaining speedup over the original model totalling 539 x for the wet 2d simulation disabling the vectorisation increases the simulation time by 2 8 x since the single instruction multiple data vectorisation available on sandy bridge cores can accommodate up to four floating point numbers in parallel this speedup is a substantial portion of the theoretical maximum 4 x for real test cases removing the vectorisation increased computation time between 1 8 x severn and 3 5 x carlisle with three models having values below 2 x inner niger delta severn pluvial test the inner niger delta and severn domains are characterised by a relatively small and fragmented pattern of flooding connected by 1d channels figs 5 and 6 while the runoff routing model in the pluvial test cases is not vectorised these factors potentially explain the limited improvement brought about via vectorisation for these tests however as no test took longer when the vectorisation was enabled we conclude from these tests that the vectorisation will be universally beneficial in terms of compute time disabling the wet edge tracking resulted in a slowdown of between essentially nothing for carlisle and 1 86 x for the seasonally dry inner niger delta as with vectorisation the wet edge tracking was sufficiently cheap that no simulation showed a noticeable slowdown when it was implemented disabling numa alignment has no substantial effect on any test case although this might become more significant on machines with more cores it was impossible to isolate many of the reductions in data transfer between code functions that we made and these structural improvements to the code as outlined by fig 1 are thought to yield most of the speedup from the original to optimised model not accounted for by vectorisation and wet edge tracking e g in the region of 2 4 x for the real test cases for completeness we also include the model speedups by number of cores in fig 8 to examine the parallel efficiency of the code parallel efficiency varied strongly with the distribution of wet cells in the domain the completely wet models remain near 100 efficient in terms of speedup until 8 cores are in use however the dry test shows essentially no speedup after 4 cores due to the lack of parallel computation relative to serial overheads except for the dry test cases parallel efficiency remained similar between 4 and 16 cores suggesting the parallelisation is scaling well 5 comment on results relative to previous studies benchmarking of hydrodynamic models from a computational performance perspective is a difficult task because the test case can have a significant influence on the relative model performance the solvers used and their accuracies vary substantially and codes are rarely compared on identical hardware meaning relative performance might vary by hardware and compiler nevertheless it is worth placing the results here in a wider context the benchmarking exercise by néelz and pender 2010 was one of the most comprehensive efforts to benchmark the main commercial and research focused two dimensional hydrodynamic modelling codes they report simulation times for several models for ea test 5 and the hardware used for the simulation although the néelz and pender study is a few years old it did report results for the original lisflood fp model which allows a comparison to be made simulation times of 9 168 s were reported by néelz and pender for ea test 5 and are reproduced in table 3 the original lisflood fp model required 28 2 s using 8 cores and was competitive on simulation time but not especially quick for this particular case note that the relative position of the models varied from test to test in néelz and pender 2010 in this study the same simulation on an identical number of cores was 1 3 x faster at 21 5 s for the original model reducing to 2 8 s 10 x for the optimised model on eight cores although this comparison is rather limited for the reasons outlined above it confirms that our optimisation efforts are relevant and substantive within the wider flood inundation modelling context 6 conclusions for 2d hydrodynamic simulations our code developments yielded between 4 2 and 8 4 x speedups when the model was run on the same number of cores while the speedups from a single core implementation of the original lisflood fp to 16 core simulations on the new code was between 34 and 60 x under idealised conditions where the whole domain was wet code speedup was up to 111 x interestingly roughly the same improvement in numerical efficiency was achieved through code development as was achieved through parallelisation on a 16 core cpu in relation to the four development areas identified in this paper the following conclusions can be drawn from this work 6 1 parallel loop structure for shared memory parallelisation using openmp the original version of lisflood fp yielded speedups of 5 13 x on 16 cores by simply implementing for loops in parallel by restructuring the parallelisation to create threads at the start of the simulation rather than within each loop it was possible to maintain this parallel efficiency despite other developments to the code reducing the work done by the rest of the model by a similar margin this change is likely to be applicable to many models where processes are often written as separate sub routines with their own loops and parallel loop definitions the disadvantage of the restructuring was that care had to be taken to identify areas of the code that could not run efficiently in parallel for example structures or that must run sequentially for example calculations of momentum and continuity however these cases could all be handled with barrier and omp single commands that force all threads to complete before moving on and force a single thread implementation respectively 6 2 wet and dry edge tracking implementing a simple wet edge tracking approach yielded substantial improvements in compute time for most realistic tests cases while the overhead of tracking the edge was sufficiently low that this was not observed in the simulation where the whole domain was wet implementing such a scheme where something more advanced doesn t already exist would therefore be expected to yield a 0 2 x speedup for most test cases and substantially more for very dry domains 6 3 data access improving memory access by the code was believed to be a substantial limiting factor on numerical efficiency in the tests conducted here it is difficult to isolate how the restructuring of the code improved compute time however given the overall model performance statistics and the performance of the models when other optimisations were disabled it is likely that these developments account for a halving of the compute time over the original simulation time 2 x speedup aligning data access and improving the data movement during simulations was also an intrinsic component of the vectorisation process and we suspect this will not have been so successful without this initial reorganisation of the code structure the most expensive component of the momentum equation is raising the hydraulic radius to the power of 4 3 we tested the use of a specific cubed root function over the more general power function and found a small improvement in model performance on some test cases 6 4 vectorisation the benefits of vectorising the solver will vary depending on the cpu however for an simd register that supports a vector length of four floating point number speedups was as high as 3 5 x when avx was enabled furthermore enabling vectorisation did not increase computation time for any of our tests cases even when the flood inundation was quite fragmented e g inner niger delta overall this paper has documented several model development steps that can yield quite substantial improvements in model performance on standard computer hardware these improvements were more substantial than the reduction in computation time of 3 5 x between a full shallow water model and the local inertia implementation when both were implemented in the lisflood fp code neal et al 2012b we hope that other developers and researchers will find the steps we have taken a useful when considering their own model development plans there are several numerical schemes that directly use the numerical approach adopted here e g adams et al 2017a b coulthard et al 2013 courty et al 2017 dottori et al 2016 however all explicit hydrodynamic model on regular grids and many process based models in geosciences have a small stencil of neighbours where information cannot travel more than one cell in any time step these could therefore all be optimised with the methods outlined here 7 software and data availability the lisflood fp software is developed by the university of bristol a freeware version of the code for non commercial use can be downloaded from the universities website http www bristol ac uk geography research hydrology models lisflood downloads the lead developers are dr jeffrey neal corresponding author and prof paul bates at the school of geographical sciences university of bristol bs8 1ss uk lisflood fp is written in c and can be complied for windows and linux version 6 of the code used in this study requires a processor hardware with avx capability or newer the code is not open source however we give access to the lisflood fp code repository to numerous research collaborators researchers interested in accessing the code are encouraged to email the lead authors or access the open source version of the code associated with the paper by hoch et al 2017 the test cases from this paper that use open data are available on the mendeley link acknowledgements toby dunne was supported by the nerc impact accelerator account at the university of bristol while chris sampson and andy smith were supported by nerc via ne m007766 1 appendix a image image appendix a code snippets describing an implementation of the lisflood fp momentum equation that can be vectorised using advanced vector extension where i is the cell index h is water depth z is bed elevation hflow cross sectional depth of flow area is the cross sectional flow area dh is the difference in water surface elevation between adjacent cells row dy and row dx are the cell widths in x and y directions delta time is the model time step g is acceleration due to gravity g friction sq x grid is the friction squared and qx old grid is the discharge from the previous time step 
26367,software applications for life cycle assessment of greenhouse gas ghg emissions have become popular over the last decade their objective is to provide insight into how ghg emissions could be reduced in the sectors defined by the unfccc however boundaries between these sectors are not closed and current tools are not designed to represent this complexity or to assess the numerous sources of uncertainties in this paper we present cat v1 0 software developed for managed forests in the lulucf sector but whose emission life cycle is linked to that of other sectors while the structure of the software follows ipcc guidelines it also contains additional features such as an embedded monte carlo error propagation technique and a user friendly flux manager that allows for complex cradle to grave representations of the wood transformation industry the flexibility of the software is illustrated through two case studies in northeastern france keywords lulucf sector wood transformation industry cradle to grave life cycle assessment uncertainty assessment monte carlo simulation software name cat v1 0 developer mathieu fortin umr silva inra ul agroparistech 54000 nancy france e mail mathieu fortin re gmail com available since 2017 hardware required no specific requirement software required java 8 availability fully available at https sourceforge net projects lerfobforesttools files cat website https sourceforge net p lerfobforesttools wiki cat license lgpl v3 cost free program language java program size 7 1 mb 1 introduction since the adoption of the united nations framework convention on climate change unfccc in 1992 the accounting of anthropogenic greenhouse gas ghg emissions has become an international issue every year the members of the unfccc that were recognized as industrialized countries in 1992 better known as annex i countries have to report their national ghg emissions in different sectors unfccc 2014 i energy ii industrial processes and product use iii agriculture iv land use land use change and forestry lulucf and v waste since 1995 the international panel on climate change ipcc has published guidelines that establish the methodological basis for carbon accounting in all the aforementioned sectors however their implementation is not straightforward and can be hindered by numerous uncertainties risks of carbon leakage and double counting fortin et al 2012 dong et al 2013 affecting interpretations even for experts in this field shvidenko et al 2010 jonas et al 2010 one reason is due to the fact that the boundaries between the five sectors are not closed forming complex emission life cycles that are hard to represent and even harder to evaluate in the lulucf sector for example the cost efficiency and carbon balance heavily depend on the correct but complex integration between multiple lines of wood extraction transportation transformation and production and the fossil co2 emissions that can be linked to the industrial processes sector the production of biomass clearly interacts with the energy sector management of the harvested wood products after their useful lifetime belongs to the waste sector all these interactions are hard to evaluate and decision makers therefore need software implemented carbon accounting tools to obtain an overall view of the big picture this increasing popularity of carbon accounting tools is not limited to the lulucf sector however regardless of the sector most of these tools show major limitations as to how emission life cycles can be realistically and easily represented and to how the uncertainties associated with these complex life cycles can be assessed with respect to uncertainty assessment theory on the topic shows that the more complex and integrated the life cycle is with cascading use and open multi loops operating at different scales the greater the need for sophisticated error propagation methods will be groen et al 2014 among the 15 tools reviewed by gentil et al 2010 in the waste sector uncertainty assessment was not even selected as a primary criterion of software evaluation mirroring the fact that uncertainty assessment was applied in only 4 of the case studies laurent et al 2014 similarly whittaker et al 2013 found that only one out of 11 carbon accounting tools used in the uk agricultural sector implemented some features for uncertainty assessment in the lulucf sector where carbon accounting tools are even more abundant brunet navarro et al 2016 reported 41 tools with only 40 of them integrating uncertainty assessment features of any sort regarding the realism of the life cycles represented most carbon accounting tools are designed to provide the carbon balance over a regional supply chain or with a predefined generic structure representing the basic supply chain compartments including extraction transport production lines product use and end of life users can change the interactions between the components of the emission life cycle in only a limited number of carbon accounting tools however even in these few cases the changes need to be hard coded or implemented in a spreadsheet like in the co2fix model schelhaas et al 2004 or the cot module of remsoft cameron et al 2013 all of this inevitably hinders the capacity of users to obtain a reliable uncertainty assessment and to avoid carbon leakages and double counting when dealing with these complex emission life cycles it also distorts the big picture so that the cost and benefits of detailed disruptive technologies management and policies that could help reduce ghg emissions through a better integration of the different sectors cannot be evaluated in this paper we present cat v1 0 a software platform that mainly applies to managed forests in the lulucf sector but also considers interactions with other sectors the tool offers a user friendly interface that makes it possible to represent complex emission life cycles inherent to managed forests moreover the assessment of the carbon balance is supported by built in monte carlo error propagation methods cat was initially developed as part of a larger forestry modeling initiative within the open source capsis platform computer aided projections of strategies in silviculture see dufour kowalski et al 2012 but its implementation has been improved to create a standalone application this paper is structured as follows we first describe the architecture of cat with its different carbon pools as well as how uncertainty assessment and scenario comparisons can be carried out a second section presents the java implementation of the software and the user interface that enables the design and comparison of complex emission life cycles we then showcase the modularity of cat in a third section based on two case studies in lorraine the largest forested region of france changes in the harvesting production lines diversity of products and end of life processes are suggested and debated at a regional level to support the cost effective transition from fossil fuel to renewable energy as well as the use of grow and bury strategies to improve the carbon balance of the region we finally discuss the interest of cat for future software in combination with previous initiatives and future challenges the source code of cat the compiled application to reproduce the results of the case studies and the basic technical services can be found at https sourceforge net p lerfobforesttools wiki cat 2 cat architecture cat follows the ipcc guidelines for national ghg inventories in the lulucf and waste sectors ipcc 2006a b 2014 its system boundaries follow that of the production approach as defined in ipcc 2006a ch 12 the basic implementation of cat offers the default methods shown in the 2006 version of the ipcc guidelines which are referred to as tier 1 methods however it also provides the flexibility for higher tier implementation basically cat recognizes the following carbon pools i aboveground and belowground living biomass ii dead organic matter dom and iii harvested wood products hwp either in use or deposited at solid waste disposal sites swds soil organic carbon is assumed to be constant as in tier 1 ipcc 2006a p 4 23 moreover litter and dead wood are not differentiated in the dom carbon pool although it is possible to specify different parameters for each one in addition to the ipcc standards cat can provide estimates i of cumulative material and energy substitution ii of cumulative fossil fuel carbon emissions during the life cycle of the different hwp iii of the accumulation of non degradable hwp at swds and iv of cumulative methane ch4 emissions caused by the degradation of hwp at swds material and energy substitution represents the ghg emissions avoided when a hwp replaces an alternative product sathre and o connor 2010 the substitution is estimated by comparing the life cycles of the two products and as such it already takes the fossil fuel emissions during their entire life cycles into account the replacement of alternative products by hwp also implies the comparison of two scenarios in which the substitution is estimated from the difference in production to avoid double counting cat provides the fossil fuel emissions only when visualizing a single carbon balance simulation and the substitution only when comparing the carbon balance of two scenarios like many carbon accounting tools in the lulucf sector see e g schelhaas et al 2004 kurz et al 2009 cat does not run any growth simulations it retrieves growth projections from existing models or yield tables a growth projection can be defined as a succession of growth steps that leads to a series of predicted characteristics at different times t cat retrieves the trees at each time t and deals with them in a different way depending on their status which can be alive dead harvested or windfall fig 1 this list of statuses is not extensive but is adapted to the context of european forests where windstorms represent the main disturbance schelhaas et al 2003 gardiner et al 2010 living trees are simply converted into carbon as shown in section 2 1 trees with other statuses dead harvested or windfall are handled by a flux manager which is the major component of the cat interface it is extensively described in section 3 3 2 1 living biomass the basic implementation of cat converts living trees into carbon using the tier 1 methodology which is based on the commercial volume ipcc 2006a p 2 12 2 1 c ˆ t s i v t s i bd s bef ag s bef bg s cf s where c ˆ t s i is the estimate of the carbon content of tree i of species s at time t mg of c v t s i is the commercial volume of this tree m3 bd s is a species specific basic density mg m 3 of dry biomass bef ag s is a species specific aboveground biomass expansion factor bef bg s is a species specific belowground biomass expansion factor and cf s is a species specific carbon fraction the ipcc guidelines ipcc 2006a ch 4 provide default values for these factors at least for groups of species when species specific values are not available the carbon stock in the living biomass at a particular date is estimated by summing the carbon content of all the trees at time t i e s i c ˆ t s i equation 2 1 can easily be adapted to provide the stocks in the aboveground and belowground compartments 2 2a c ˆ ag t s i v t s i bd s bef ag s cf s 2 2b c ˆ bg t s i v t s i bd s bef ag s 1 bef bg s cf s in practice only the basic densities are species specific in the ipcc tier 1 method whereas the biomass expansion factors and carbon fractions are given by groups of species i e broadleaved and coniferous species ipcc 2006a ch 4 these default basic densities biomass expansion factors and carbon fractions are already available in the user interface as well as those used for the tier 2 method in france for national reporting see citepa 2015 p 1101 cat also allows the user to define his own factors like in tier 2 through a simple dialogue box these country or context specific values can be saved in a file for future use tier 3 methods can be implemented using java interfaces as explained in section 3 1 2 2 dead organic matter windfall and dead trees as well as residues from harvested trees feed the dom carbon pool when they are retrieved from the growth simulation they are converted into carbon using eq 2 1 depending on the size cat recognizes three types of debris fine woody debris commercial sized woody debris and coarse woody debris the first is actually the difference between the aboveground carbon content and the carbon content in the commercial part of the tree commercial sized woody debris is the carbon content of the commercial biomass of the dead or windfall trees coarse woody debris is composed of the carbon in the stumps and roots by default the fine woody debris is left on the forest floor and becomes part of the litter likewise coarse woody debris and a proportion of commercial sized debris are also left on site to integrate the dead wood compartment fig 1 the dom carbon pool is calculated as the sum of these two compartments the stock in each compartment is assumed to follow a first order decay rate marland and marland 2003 which is actually an exponential distribution see lawless 2003 p 17 2 3 pr t t e t λ where t is the lifetime t is the time year and λ is the average lifetime the probability that a particular unit of carbon is still in the litter or the dead wood compartment at time t 1 is conditional on the probability that it was already there at t 0 this probability can be defined as 2 4 pr t t 1 t t 0 e t 1 λ e t 0 λ e δ t λ where δ t t 1 t 0 when dealing with a large number of carbon units the probability shown in eq 2 4 can be treated as a proportion the balance of probability i e 1 e δ t λ is actually the amount that is released into the atmosphere the ipcc guidelines are based on the half life t 1 2 and not on the average lifetime see ipcc 2006a ch 12 while the average lifetime is the expectation of the lifetime the half life is the time until half the matter remains i e pr t t 1 2 with a first order decay rate the half life is easily derived from the average lifetime as follows 2 5 t 1 2 l n 2 λ from eq 2 5 it can be deduced that the half life is always smaller than the average lifetime in cat the user can specify the lifetime either in terms of average lifetime or half life the latter being the default mode the ipcc guidelines also use the decay rate k in their terminology with an exponential distribution like in eq 2 3 the decay rate is actually the multiplicative inverse of the average lifetime i e k 1 λ zell et al 2009 conducted a meta analysis of decay rates for coarse woody debris in temperate forests they estimated a decay rate of k 0 031 or equivalently an average lifetime of 32 years the decay rates of branches and twigs are much larger for beech they ranged from 0 149 to 0 220 which implies average lifetimes of between 4 5 and 6 7 years boddy and swift 1984 2 3 harvested wood products hwp compared to the living dead or windfall trees harvested trees are processed differently cat provides a user friendly interface that allows users to represent the hwp supply chain of interest this interface is largely described in section 3 3 regardless of the complexity the implementation of any supply chain can be described as follows first harvested trees are transformed into logs using a bucking module to avoid any leakage the difference between the carbon contained in the logs and the aboveground carbon is considered as fine woody debris whereas the carbon in the stump and the roots becomes coarse woody debris see section 2 2 secondly these different logs are sent to production lines which define the different hwp as well as their associated half lives or average lifetimes substitution factors and the fossil fuel emissions associated with their life cycles the amount of products still in use is updated over time using a first order decay rate see eq 2 4 the part of the hwp that is no longer in use at a particular time t can be either recycled or deposited at the swds thirdly the carbon in disposed hwp is managed according to the ipcc guidelines for the waste sector ipcc 2006b ch 3 the degradable organic carbon fraction doc f which is the part of the carbon that is actually subject to decomposition and the half life or average lifetime of this degradable fraction can be specified by default cat assumes a doc f of 0 4 which is slightly lower than the ipcc tier 1 value of 0 5 ipcc 2006b p 3 13 actually the ipcc tier 1 value of 0 5 has been found to overestimate the actual degradable fraction by some authors barlaz 2006 ximenes et al 2008 the default decay rate of the doc f is assumed to be that of a temperate climate with wet conditions k 0 03 see ipcc 2006b p 3 17 which leads to an average lifetime of 33 years the sum of the carbon in the hwp in use and what remains of the degradable part at the swds is considered as the hwp carbon pool given the anaerobic conditions a part of the carbon that decomposes at swds is partly released in the atmosphere in a ch4 form which has a higher global warming potential in anaerobic conditions cat assumes that 50 of the carbon emitted from the swds is methane which corresponds to the tier 1 method ipcc 2006b p 3 15 depending on the facilities of the swds these methane emissions can be reduced for instance if the swds is managed under semi aerobic conditions the methane emissions are reduced by half ipcc 2006b p 3 14 by default cat assumes that the swds belongs to the managed semi aerobic category however the user can choose another type among the five suggested in the ipcc 2006b p 3 14 guidelines the non degradable part of carbon that accumulates at a swds is assumed to be permanently sequestered scholz and hasse 2008 the fossil fuel emissions associated with the hwp life cycle the methane emissions and the substitution are also considered in cat the cumulative sequestration or emissions related to these can be calculated over the projection length 2 4 uncertainty assessment a major issue in carbon accounting remains the many sources of uncertainty green et al 2006 ståhl et al 2014 valade et al 2017 the growth simulation is uncertain even before estimating the carbon balance the errors in the parameter estimates and the errors in the model inputs which stem from the sampling and measurement errors count among the sources of uncertainty in growth models kangas 1999 the estimation of the carbon balance implies additional sources of uncertainty on top of those of the growth simulation in order to assess their impact cat implements uncertainty assessment features for five categories of parameters i biomass expansion factors ii basic wood densities iii carbon fractions iv half lives or average lifetimes and v substitution factors different methods exist for propagating errors and assessing prediction uncertainty across complex emission life cycles groen et al 2014 analytical methods are mainly based on taylor series e g gertner 1990 gertner et al 1995 however when the emission life cycle under study is too complex these taylor series based methods are difficult if not impossible to implement an alternative is the well known monte carlo technique which is extensively described in rubinstein and kroese 2008 it consists of drawing random deviates from the distribution of the parameter estimates and computing a realized value of the phenomenon of interest repeating this process a great number of times provides many realizations of the phenomenon and simulates its natural variability growth simulations based on the monte carlo technique are often referred to as stochastic simulations whereas those based on the mean only are known as deterministic simulations vanclay 1994 p 7 cat is designed to deal with growth simulations whether they are stochastic or deterministic in both cases the application allows the user to specify a distribution which can be gaussian or uniform as well as an error margin for each one of the five aforementioned categories of parameters involved in the estimation of the carbon balance the error margin is actually proportional to the current value of the parameter an error margin of 10 for a uniform distribution implies that the current parameter is multiplied by a random deviate between 0 9 and 1 1 for gaussian deviates the error margins are set at a probability level of 0 95 if the growth simulation was deterministic the user can decide how many realizations are to be performed in this uncertainty assessment in the case of stochastic growth simulations the number of realizations is that of the growth simulation for each realization random deviates are drawn to account for species specific errors in the aforementioned parameters and the carbon balance is estimated accordingly when all the realized carbon balances have been calculated cat computes non parametric confidence limits based on the percentile rank method see efron and tibshirani 1993 p 170 and the probability level set by the user the deviates are stored in memory enabling consistent comparisons between different scenarios as shown in section 2 5 when running the first uncertainty assessment the deviates as well as the realizations for which they apply are recorded when running subsequent uncertainty assessments those deviates are reused in the same realization this way the first realization of each monte carlo simulation is based on the same random deviates it is also possible to enable or disable the variability for some parameters for instance it can be decided to disable the variability of the carbon fractions while enabling those of the basic wood densities and half lives the ipcc guidelines provide some reference values for uncertainty assessment which are based on the literature these values as well as their references in the literature are shown in table 1 these are the default uncertainty levels implemented in cat the values can be changed within a given range using the cat interface 2 5 scenario comparisons cat allows the comparison of scenarios in two different manners depending on the type of forest management the first comparison mode is available for any type of management and is based on the comparison of punctual estimates for example if two scenarios have been simulated from 2017 to 2050 it is then possible to compute the differences between the estimated stocks at the end date of both simulations furthermore the carbon stock estimates can be compared within the same scenario for a given scenario it can be of interest to quantify any increase or decrease in the different carbon pools between 2017 and 2050 if the growth simulations are stochastic in both scenarios and if they are based on the same number of realizations then the realizations are paired and the comparison is made on the differences observed in each pair of realizations this procedure is basically the same as a paired t test which has a greater statistical power than a non paired test zimmerman 1997 when the comparison is run under this idea of paired t test the result becomes a set of realized differences the mean of these realized differences is the estimated difference confidence interval limits are calculated using the percentile rank method see efron and tibshirani 1993 p 170 as well when the number of realizations is different across the simulations the comparison is based on the difference between the means of the simulations as in a non paired t test compared to the paired mode this comparison only provides a mean difference and its variance confidence intervals are then based on a gaussian distribution the second comparison mode is available only if the growth simulation took place in an even aged stand in both scenarios in such a context the carbon balance can be calculated on an infinite sequence as if the same management strategy was repeated over time a comparison of the carbon balances in infinite sequence is then made available to the user such comparisons have been performed in different pure even aged stands with the objective of identifying the most carbon friendly forest management strategies liski et al 2001 kaipainen et al 2004 vallet 2005 vallet et al 2009 fortin et al 2012 2014 3 software implementation 3 1 interfaces for compatibility and extended use cat was coded in java using the library known as lerfob foresttools which has dependencies on other java libraries repicea for the monte carlo technique and user interface features and jfreechart for graphical display cat is distributed as a standalone application but it can also be called from another java application or even from another language in its current version cat can be used for carbon life cycle assessment potentially using any sort of existing forest model whose outputs are compatible with cat s inputs the application assumes that the growth projections that cat relies on can be described through two classes of objects a stand class and a tree class as a standalone application cat allows the import of yield tables or growth simulations from comma separated files csv calling cat from another java application is made possible if the two aforementioned classes implement the two interfaces catcompatiblestand and catcompatibletree available in the lerfob foresttools library for example some growth models of the capsis platform dufour kowalski et al 2012 already implement these two interfaces so that cat is available as a tool within the platform a customized implementation with the go model lousteau et al 2012 which was implemented in python has also been made possible through the py4j library www py4j org further information can be found at the cat website the two above mentioned interfaces ensure that the growth simulations can provide all the basic information for tier 1 methods additional interfaces are optional for the tree class and provide methods for biomass and carbon estimation if they are implemented cat may rely on these methods instead of using the ipcc tier 1 approach these additional interfaces are listed in table 2 3 2 user interface running a carbon balance simulation in cat consists of three steps 1 import the growth simulation which can be done automatically if cat is part of another software such as the capsis platform 2 set the biomass parameters and define the flux configuration that handles dead harvested and windfall trees 3 run the simulation and obtain the results the user interface has all the components to go through these three steps fig 2 the file menu contains the options to import yield tables or projections from growth models in the upper part of the interface two combo boxes entitled biomass parameters and flux manager dom hwp provide access to the biomass conversion factors and the flux configuration respectively in both cases some presets are available however the user can choose to customize the biomass parameters as well as the flux configuration by choosing the customized option in the combo box clicking on the folder icon on the right hand side of the combo box display windows to specify the new biomass parameters or create new flux configurations as described in section 3 3 once the biomass parameters and the flux configuration have been set the simulation is run by clicking on the button in the top left corner 12c of the interface or by choosing the option calculate carbon balance in the actions menu the results of the simulation are displayed in the panel on the right in different tabs the user can choose which graph to display using a combo box just under the tab label among others users can visualize the evolution of the carbon stocks in the different pools the annual flows per log grade and the annual flows per harvested wood product the panel on the left contains check boxes which enable or disable the carbon pools to be displayed in the panel on the right right clicking on the tab label of a particular simulation in the panel on the right causes a pop up menu to appear this pop up menu contains different options to either delete this tab all the tabs or only the other tabs more importantly it also contains options for exporting the result or comparing the simulations the menu bar in the upper part of the interface offers additional features under the options menu the units can be switched from c to co2 eq the probability level of the confidence intervals which is set to 0 95 by default can be changed as well the global warming potential factors used by default are those of the fifth assessment reports on climate change ipcc 2013 p 714 however the user can decide to use those of the second or fourth assessment reports if more relevant the settings of the uncertainty assessment can also be changed through this menu 3 3 flux manager the flux manager is the central element of cat since it manages all the carbon that is not in the living biomass access to the flux manager is granted by choosing the customized option in the combo box on the right side in the upper panel of the interface fig 2 in the upper part of the window the user can choose a bucking module which actually defines the different log grades fig 3 the basic implementation consists of a splitting of incoming volumes into particle and sawing log grades other implementations are possible in which more log grades are available the flux manager window contains a tool bar on the left hand side the top three icons represent industrial dead organic matter and swds processors respectively the flux configuration is in the main panel the user can create as many processors as needed by simply dragging and dropping any of the three icons from the tool bar to the main panel double clicking on newly created processors gives access to a dialogue in which the half life and other features can be set processors can be linked by clicking on the fifth and sixth icons in the tool bar the solid line represents an instantaneous flux between two processors at the creation of the hwp while the dashed link indicates a flux after the useful lifetime the magnitude of any instantaneous flux can be set by double clicking on the small button at mid range along the solid line a flux is expressed as a percentage of the outcome of an upstream processor by default there is a series of processors on the left hand side of the main panel that correspond to the different types of woody debris and of log grades that can be obtained from the bucking module these processors on the left are the entry points of the flux configuration all of them must be linked to either an industrial a swds or a dom processor the flux manager is meant to detect two types of inconsistencies the first case occurs when the sum of the flux from a particular processor is different from 100 the fluxes are then displayed in red as a warning to the user a second type of inconsistency is the occurrence of endless loops if a downstream processor feeds an upstream processor along the same stream then the biomass can loop endlessly between these two processors in such a case all the fluxes that are part of the endless loop are displayed in orange to help users correct this situation a carbon balance simulation cannot be run if inconsistencies of these two types are present in the flux configuration an example of a simple flux configuration is given in fig 3 the two dom processors account for different half lives in the litter and in the dead wood the five processors with blue borders represent those that produce hwp they are referred to as paper and paperboard wood based panels sawnwood energy wood and incineration the treatment of these different products after their useful lifetime is different since there is no dashed link coming out of the energy wood and incineration processors the products are assumed to oxidize as for the paper and paperboard the wood based panels and the sawnwood they are sent to the recycling facility after their useful lifetime where 60 is recycled into energy wood 30 is incinerated and 10 is sent to the swds the user can reach a higher level of complexity by setting additional industrial dom and swds processors basically all the processes involved in the life cycle of hwp such as industrial processing transport and distribution maintenance recycling and disposal can be created moreover different wood use alternatives such as the harvesting of fine or coarse woody debris for energy production riffell et al 2011 francois et al 2014 or wood burial scholz and hasse 2008 can be easily designed by creating new fluxes an example of a more complex flux configuration is shown in fig 4 we illustrate how this higher level of complexity can be handled through two case studies in section 4 4 application examples in this section we present two application examples the first one aims at testing alternative scenarios for increasing the availability of biomass in the context of transition to renewable sources of energy the second example is inspired by the work of scholz and hasse 2008 who suggested that carbon emissions could be partially compensated for by burying large amounts of wood in both case studies we exemplify the strengths of cat by coupling it with a multi species forest growth model called mathilde this coupling was made possible by integrating cat into the capsis platform dufour kowalski et al 2012 which hosts the aforementioned model using the growth model and cat we explored alternative scenarios applied to the lorraine region in northeastern france 4 1 biomass availability in lorraine france in europe biomass is considered as an important source of energy that could partly replace fossil fuels and thereby decrease ghg emissions nord larsen and talbot 2004 sikkema et al 2011 repo et al 2011 among renewable energies solid biomass is actually the most important source for heating and a significant source for electricity production eea 2016 since 2009 france has adopted an aggressive policy of conversion to renewable energy which promotes the installation of biomass boilers in industrial and public buildings energy wood has been traditionally considered as carbon neutral because it was assumed that it would not cause any net emissions since the amount of co2 released into the atmosphere during the combustion should be taken up by the remaining growing forests sikkema et al 2011 repo et al 2011 carbon neutrality appears as a scale and usage dependent concept that is no longer universally accepted because it can lead to a flawed assessment of the carbon footprint of biomass combustion at a given temporal scale johnson 2009 increasing the production of energy wood may lead to conflicts between future needs in energy wood pulp other wood products and ecological services from forests lecocq et al 2011 caurla et al 2013 in oak and beech stands which are typical of northern france increasing the production of energy wood through an increased harvest intensity usually leads to a worse carbon balance vallet 2005 fortin et al 2012 2014 if the forest management remains the same then industry may have to adapt its supply chain to produce more wood pellets out of wood chips and wood logs which are currently used to produce paper and panel boards part of the fine woody debris that is currently left on the forest floor after harvesting could also be processed into wood pellets francois et al 2014 although some impacts on biodiversity should be expected riffell et al 2011 we tested such alternative scenarios for the lorraine region in northeastern france using the data from the french national forest inventory nfi and the coupling between the mathilde model and cat more information about the model and the dataset can be found in the supplementary materials sm1 and sm2 in this case study we present projections of the carbon balance in oak beech and hornbeam stands in lorraine for the period 2015 2100 to do this we first made growth projections using the mathilde model we then used cat to compare the carbon balance of the business as usual scenario hereafter referred to as bau which represented the baseline for the french forest sector with three alternative scenarios of wood use for the purpose of increasing the availability of energy wood in the first alternative scenario s1 we reoriented 80 of the small sawlogs and 10 of the large sawlogs which are mainly used as lumberwood to the production of wood pellets in the second alternative scenario s2 the forest industry was assumed to extract an additional 40 of the fine woody debris i e 50 of the total woody debris immediately after harvesting to produce energy wood in the third alternative scenario s3 the forest sector was assumed to combine the first and second alternative scenarios building on the available sources of information for the industrial sector in northern france normandin 1990 paquet and deroubaix 2003 agreste 2012 2015 lenglet 2015 and previous studies on the carbon balance in the lorraine region vallet 2005 fortin et al 2012 we designed a flux configuration for the bau scenario using the flux manager in cat this flux configuration is shown in fig 4 more details can be found in the supplementary material sm3 the alternative scenarios were simply adapted from this flux configuration by adding or changing some fluxes the files containing the bau and the three alternative flux configurations are available online at https sourceforge net projects lerfobforesttools files cat lorraine they can be visualized using the cat application we simulated the evolution of the carbon stocks in the pools over the 2015 2100 period for the four scenarios we then compared the alternative scenarios to the baseline in terms of predicted stocks and cumulative fluxes at the end of the projection we also used the built in features of cat to assess the impact of simulated uncertainty in the five categories of parameters for the sake of the example the uncertainty levels were set to their default values as shown in table 1 4 1 1 evolution of carbon stocks and emissions under the bau scenario using cat we managed to represent the current wood supply chain of the lorraine region and to simulate its carbon balance until 2100 based on the 2015 nfi data the expected stocks in the carbon pools of the living biomass lb dead organic matter dom and harvested wood products hwp as well as of the cumulative fossil fuel carbon emissions from hwp life cycles the accumulation of non degradable hwp in swds and the cumulative methane emissions from swds are shown in fig 5 on average the carbon stock in the lb remained stable at around 440 mg ha 1 of co2 eq until 2050 and then steadily decreased to 360 mg ha 1 of co2 eq until 2100 the hwp carbon pool showed an asymptotic pattern after the first five year interval of the simulation and remained approximately constant with an average stock of 36 mg ha 1 of co2 eq the carbon stock in dom slowly but steadily increased until it reached a plateau at 84 mg ha 1 of co2 eq in 2070 it must be stressed that these predicted trends for the dom and hwp carbon pools were partially biased by the fact that in the absence of data the initial conditions for the simulation in 2015 were artificially set to zero in these pools the impact of the simulated uncertainty in the parameters specified in table 1 was obvious for the lb and the dom fig 5b for all carbon pools these additional uncertainties resulted in wider confidence intervals throughout the simulation period most emissions came from the life cycle of the hwp and not from the ch4 emissions from the swds fig 5c and d actually the accumulation of non degradable carbon at the swds was small and partly compensated for by ch4 emissions within this time frame considering the uncertainties in the parameters specified in table 1 resulted in slightly wider confidence intervals the sequestration in the lb can be estimated as the difference between the carbon stocks in 2015 and 2100 calculating the realization wise estimates of the difference made it possible to obtain a confidence interval of the sequestration shown in fig 6 the simulated uncertainty in the biomass expansion factors the basic wood densities and the carbon fractions did not have much impact on the width of the confidence interval fig 6b the sequestration in the dom and hwp between 2015 and 2100 could be estimated but would necessarily be biased since the 2015 stocks were artificially set to 0 as we already mentioned 4 1 2 impacts of alternative scenarios on carbon stocks and emissions the differences between the alternative scenarios and the bau scenario in 2100 are shown in fig 7 the first alternative scenario which consisted of reorienting part of the lumber wood from sawmills to energy wood was not without effect on the carbon balance fig 7a and b this strategy implied a reduction of carbon sequestered in the hwp since a greater proportion of short lived hwp was produced fewer long lived hwp also meant less cumulated material substitution over time throughout the supply chain less cumulated sequestration of non degradable carbon at the swds and less cumulated ch4 emissions from the swds accounting for the uncertainties in the parameters of table 1 resulted in wider confidence intervals fig 7b despite the great deal of uncertainty around the estimated differences it was unlikely that the alternative scenario would result in a better carbon balance than the bau scenario in comparison the second alternative scenario which prescribed the use of an additional 40 of the fine woody debris immediately after harvesting to produce wood pellets fig 7c and d exhibited much larger values the gain in substitution was estimated at 42 mg ha 1 of co2 eq in 2100 when compared to the bau scenario it largely compensated for the reduction of the carbon stocks in the dom 8 mg ha 1 of co2 eq the increase in carbon stock in the hwp pool was modest 3 mg ha 1 of co2 eq the swds emissions and non degradable products remained unchanged because the same quantity of long lived hwp was produced in both scenarios factoring uncertainties did not change the interpretation of the results even though the uncertainty in cat parameters resulted in wider confidence intervals fig 7d this second alternative scenario would outperform the bau scenario in terms of carbon balance for the third alternative scenario fig 7e and f which merged the two previous alternative scenarios cat predicted that the effect should simply add up by broadly speaking producing the same picture as in the previous scenario this was essentially due to the fact that the first alternative scenario had a relatively low impact on the different pools we did not show the comparison of the stocks in the living biomass since the three alternative scenarios had no effect on this pool 4 1 3 impacts of alternative scenarios on harvested wood products beyond the carbon balance cat also provides estimates of the quantity of hwp by class the comparison of these estimated quantities between the alternative scenarios and the bau scenario are shown in fig 8 the impact of reorienting part of the current extraction from sawmills to wood pellets fig 8a caused an increase in the production of energy wood at the expense of other classes of hwp except for barrels the gain in energy wood was rather modest considering an area of 433 000 ha and a simulation period of 85 years this alternative scenario would increase the supply of energy wood by around 18 500 mg yr 1 of dry biomass which was far from the current governmental objectives in france colin et al 2009 accounting for the uncertainties in the parameters of table 1 also resulted in wider confidence intervals but this did not change the interpretation of the results in comparison the second alternative scenario which prescribed the use of an additional 40 of the fine woody debris immediately after harvesting to produce wood pellets significantly increased the energy wood production 45 mg ha 1 of dry biomass fig 8c and d over the study area and the simulation period such a quantity of energy wood meant a supply increase of 230 000 mg yr 1 of dry biomass which was definitely more in accordance with the current governmental objectives colin et al 2009 obviously none of the other hwp categories i e building furniture packages paper barrels was impacted by this scenario for the third alternative scenario fig 8e and f which merged the two previous alternative scenarios cat also predicts that the effect should simply add up for the same reasons previously mentioned for the results presented in fig 7e and f 4 2 how good would it be to bury wood for lorraine france the strategy of growing harvesting and burying wood in swds as an option for creating a durable carbon pool scholz and hasse 2008 is based on the idea that the buried form would quickly decarbonate the atmosphere or at least balance our emissions from fossil fuel activities the secondary long term circular economic idea is that the buried form could eventually be reused in hundreds or thousands of years from now to produce future resources of carbon compounds that could be utilized once the current fossil resources have been depleted scholz and hasse 2008 even though this strategy could support an incremental change in the area not yet covered by forests it has many ecological cultural and industrial drawbacks however beyond these considerations this suggestion has also been attacked on its supposedly positive carbon balance for instance the grow and bury strategy was said to ignore the material and energy substitution induced by the use of harvested wood products koehl and fruehwald 2009 it was also criticized for overlooking the impact of methane emissions produced by wood decomposition in the swds nevertheless it is now accepted that a consensus on the issue should be made based on clear evaluations of its carbon balance merit in different conditions koehl and fruehwald 2009 to approach this question in cat we kept the same setup as the one described in the previous example for the lorraine region based on the bau scenario we created another alternative scenario where 50 of the volume that was supposed to be transferred to the production lines was instead directly buried in the swds the comparison with the baseline is shown in fig sm2 in the supplementary material cat unequivocally predicted that the non degraded fraction of carbon that accumulates in the swds should be 120 mg ha 1 larger than in the bau scenario by 2100 the hwp pool would also have more carbon 20 mg ha 1 which can be explained by the fact that the half life of doc f in the swds is much greater than the half lives of short lived hwp in the bau scenario however this huge potential should be compensated for by the combined impact of the methane emitted from the buried wood in the swds and the reduction in the global substitution throughout the life cycle which was estimated at more than 200 mg ha 1 considering uncertainties on parameter types from table 1 does not change this interpretation fig sm2b the evaluation of this grow and bury strategy for the lorraine case is therefore expected to be inefficient for achieving the 2100 carbon targets 5 discussion 5 1 advancing the handling of complex life cycles in carbon accounting tools boundaries between the emission sectors defined by the unfccc are not hermetic forming complex cross sectoral life cycles current carbon accounting tools are not designed to represent this growing complexity or to evaluate the impact of numerous sources of uncertainty as such they are exposed to leakage and double counting fortin et al 2012 dong et al 2013 affecting interpretations even for experts in this field shvidenko et al 2010 jonas et al 2010 mckone et al 2011 cat turns the representation and analysis of life cycles along complex structures such as the wood supply chain into an exploratory exercise that minimizes cumbersome coding tasks and complex uncertainty assessments while maximizing users time spent at comparing testing and evaluating a wider range of relevant scenarios and hypotheses to the best of our knowledge no other software used in the lulufc brunet navarro et al 2017 agriculture whittaker et al 2013 and waste sectors laurent et al 2014 has this capacity with the case studies in the lorraine region we showed that cat provides options to easily create and compare different scenarios of life cycles which can differ in terms of wood extraction production lines use and end of life of many wood products cat s flux manager makes it possible to compare scenarios of different levels of complexity for instance the flux configuration of the bau scenario fig 4 could be compared to a much simpler scenario based on the configuration shown in fig 3 non trivial or contentious hypotheses like growing burying can be easily evaluated and compared in terms of carbon balance and industrial merit this application could potentially be used to support regional decision making and future consensus at a higher scale for such a complex multidimensional issue cat has been designed in such a way that the outcomes such as those in fig 5 and the structure follow the ipcc guidelines in the lulucf and the waste sectors ipcc 2006a b 2014 we also showcased how the tool can provide detailed information on the carbon contributions of particular emission loops cascading chains of fluxes and compartments e g specific factories to the overall carbon balance within the context of complex supply and recycling chains in our case studies we chose to aggregate all the sawmills energy factories land fill sites and recycling brokers from the lorraine region into single processors however the user could perform much more exhaustive analyses and use cat to obtain a more realistic representation of the different components of the life cycle and their contributions to the regional carbon balance there is no limit to the resolution of the life cycle at this time it must be stressed that cat does not provide any default values for the processor features which means that the user must provide for the biomass of the functional unit and the co2 emissions per functional unit for all the intermediate processors half lives and substitution factors must also be provided for final processors which produce hwp consequently high resolution life cycles imply a greater number of values that must be provided in the flux manager even though large life cycle databases exist such as ecoinvent wernet et al 2016 it may be a challenge to retrieve all the values needed to fully cover a high resolution life cycle it could be argued that some parts of the life cycles will eventually become obsolete and that there is no need to take them into account this could be the case of swds which belongs to the waste sector as a matter of fact many countries have banned the disposal of wood waste in swds the european union has already set ambitious targets for the reduction of disposed biodegradable waste such as wood eu 2008 even though the proportion of wood waste sent to swds is expected to decrease mantau et al 2010 it is unlikely that it will be null on the short term for instance the proportion of wood waste disposed in swds in 2015 was estimated at 19 in france guinard et al 2015 unless a major breakthrough occurs in waste management the waste sector will remain part of the life cycles although its importance will decrease over time even if cat represents an advance in the representation of complex carbon emission life cycles and in the implementation of uncertainty assessment there is still room for several improvements in order to make it a more viable option for instance the current cat version assumes that the industrial sector does not change over time however demand in wood products and the transformation sector both change over time and their dynamics inevitably interact with forest management and economic constraints to handle this dynamic national regional economic models of forest supply chains would need to be linked to cat for the lorraine region cat would for example benefit from integrating economic models of forest supply chains like the one developed by lecocq et al 2011 and caurla et al 2013 such economic models would help assess the impact of economic incentives subsidies taxes etc on the demand in forest products as a non linear function of price and income they would also explicitly consider the competition for resources as well as industrial capacities however this kind of implementation might require adaptations of both cat and the economic model in the 2006 ipcc guidelines ipcc 2006a ch 12 four different approaches for hwp accounting were suggested the stock change the atmospheric flow the production and the simple decay approaches there was no preference for any of these four approaches in its current form cat is based on the production approach it allows for exported products that can be represented in the flux manager however it cannot account for imported wood products according to decision 2 cmp 7 unfccc 2012 carbon in imported hwp should not be included in the national reporting of ghg emissions by the importing country however this decision could be amended in the future a substantial improvement in cat would consist of giving users the opportunity to change the boundaries of the emission life cycle by opening it to import like in the stock change and atmospheric flow approaches brunet navarro et al 2017 reported that it is rarely integrated into carbon accounting tools the ipcc guidelines suggest working under the assumption of constant organic soil carbon when the land use does not change and no data on this pool are available which is the case with cat ipcc 2006a p 4 23 by doing so we assume that forest management has no impact on the carbon in the soil this seems to be the case for traditional management practices but not for those that involve intensive biomass harvesting achat et al 2015 taking the changes in organic carbon stock into account requires models in the next version of cat we intend to implement the yasso soil carbon model liski et al 2005 tuomi et al 2011 whose structure could be connected to the flux manager processors representing dead organic matter in the flux manager would then be connected to the soil carbon model cat does not implement any sort of initialization for hwp and dom actually the stocks in hwp and dom at the beginning of the simulation are usually unknown and they are artificially set to 0 see fig 5a and b the ipcc guidelines suggest a procedure that consists of simulating the stocks in the hwp since 1900 relying on fao data and many assumptions ipcc 2006a section 12 2 this procedure has not been implemented in cat for two reasons first the procedure applies at the national level while the simulations in cat are not necessarily run at that scale secondly the uncertainty associated with these initial estimates of hwp and dom is not straightforward because this model initialization is missing comparisons between point estimates of the same scenario are biased for hwp and dom however the comparisons between an alternative scenario and a baseline scenario such as those shown in fig 7 are valid under the assumption of steady state more specifically if we assume that the hwp and dom pools are initially at a steady state then their stocks should be approximately those observed at the end of the projection in the baseline scenario to be valid the assumption requires projections that are long enough to reach the steady state and harvesting rates that do not change over time this is precisely the case in our baseline scenario actually the hwp and dom pools approximately stabilized after 2050 and 2070 respectively fig 5a and b the steady state assumption is one of the three strategies identified by brunet navarro et al 2016 for simulating initial hwp and it has also been used when it comes to estimating the carbon stocks in the soil and the litter e g liski et al 2001 2005 vallet 2005 given that the initial stocks in the hwp and dom pools as estimated under the steady state assumption are the same in both the alternative and the baseline scenarios their contribution simply cancels out when calculating the difference between the scenarios even though the steady state is not achieved it can reasonably be assumed that the contributions of these missing hwp in terms of carbon storage will be the same in both scenarios again they would simply cancel each other out in the calculation of the difference between the scenarios kurz 2010 lemprière et al 2013 this latter assumption is valid if the hwp have the same half lives in both scenarios which is precisely the case in this study even though these factors change in the alternative scenarios it could be rightfully argued that these changes only apply to new hwp and not to the hwp initially in use there is actually only one special case where the contributions of initial hwp do not cancel out in the difference which is when the recycling rate changes in the alternative scenario an increased recycling or reuse rate after the useful lifetime of long lived hwp could be implemented in a short period of time and there is no reason why initial hwp should be treated differently than newly produced hwp in this regard this was not the context of our case studies but the potential bias clearly deserved to be raised following the ipcc guidelines ipcc 2014 cat uses a first order decay rate for the lifetimes of dom hwp and doc f the first order decay process is largely described in marland and marland 2003 it assumes that the proportion of the matter that is discarded remains constant over time since the largest amount is observed at the beginning of the lifetime it implies that the greatest quantity of discarded matter occurs at the beginning of the lifetime as well an assumption that probably does not hold for long lived products marland et al 2010 a distributed decay model relying on other distributions has been suggested by marland et al 2010 their idea consists of treating the hwp not as a single pool but instead as a series of distinct products since the decay rates are dependent upon the time since they were produced in its current version cat partly implements this distributed decay model the products are not merged into a single pool instead the quantity of a particular product is independently actualized over the projection length using a first order decay and its user specified half life the quantities are then aggregated in order to provide an estimate of the co2 stock in the hwp pool however cat does not allow the users to change the half lives according to the production dates the option of using other decay functions such as weibull logistic or gamma is also missing in cat in order to implement the distributed decay model of marland et al 2010 we intend to implement these in the next version of cat these new decay functions coupled with the transition between two flux configurations could account for changes in the lifetime distribution of a particular hwp class over time 5 2 advancing uncertainty assessment in carbon accounting tools there are multiple sources of uncertainty when simulating the carbon balance at the regional level the current version of cat takes the sources of uncertainty related to the model into account since they come from the model itself the tool is fully dependent on the model implementation on this issue partial stochastic implementation would probably lead to underestimating the width of the confidence intervals and cat has no control over that simulating the errors in some parameters of cat increases the uncertainty associated with the predictions of some pools the confidence limits of the living biomass were greatly impacted by these additional sources of uncertainty as shown in fig 5a and b the degree of imprecision is not so high that nothing can be concluded here in fact the estimates of 2100 are correlated to those of 2015 because it can be assumed that these errors remain constant all along a particular realization or that if they change over time the variability is much smaller than the variability between the realizations an overestimated carbon stock in 2100 is likely to be overestimated in 2015 and vice versa when subtracting to obtain a realization wise difference the overestimation or underestimation of the value of 2100 is partly compensated for by that of the value in 2015 ståhl et al 2014 obtained similar results using static models to determine the change in biomass between two inventories in sweden and finland while the confidence limits are approximately 100 and 150 mg ha 1co2 eq around the mean estimates in 2015 and 2100 respectively fig 5b the confidence limits of the difference are 120 mg ha 1co2 eq around the mean estimate while cat takes these constant errors in the parameters across the carbon balance simulations into consideration the errors in the growth model components are completely out of reach and depend on the implementation the mathilde growth model was implemented so that for two growth projections the random errors in the parameter estimates and the disturbance occurrence are consistent across the projections and therefore the comparison between different forest management scenarios should be consistent in cat there is no guarantee that other models have this kind of implementation by default there are several sources of uncertainty that remain to be implemented in cat in the current version there is no uncertainty about the carbon emissions from the hwp life cycles although we did not find any assessment of the variability of these emission factors it can be reasonably assumed that they are quite variable we did not consider the uncertainty associated with fluxes between the different processors in the flux configuration in this case the implementation is probably more complicated since the sum of the outgoing fluxes from a particular processor has to be 100 as a consequence the random deviates from these outgoing fluxes are necessarily negatively correlated the multinomial distribution could provide these negatively correlated deviates mccullagh and nelder 1989 p 164 but this remains to be tested the uncertainty that stems from the sampling is not considered in cat either estimators that are both model and design based can take both sources of uncertainty into account some of these have been developed in contexts of carbon accounting in managed forests e g ståhl et al 2011 mcroberts et al 2016 but they are either incompatible with the monte carlo method or they tend to overestimate the variance a recent estimator based on parametric bootstrap fortin et al 2018 might prove compatible with built in cat uncertainty features while being unbiased in similar studies there seems to be a major confusion about the scale at which the uncertainty applies the parameters in table 1 are actually species or species group specific mean estimates like any statistical model a residual error exists which is the difference between the observed value and the mean predicted value at the most basic level which would be the tree level in this case the uncertainty arises from the error in the estimate of the mean as well as from the residual error however when scaling up to the landscape level the tree level residual errors compensate for each other so that their contribution to the uncertainty at the landscape level decreases and the error in the estimate of the mean usually becomes the major source of uncertainty apart from the sampling related variability breidenbach et al 2014 fortin et al 2016a b unfortunately the scales at which the uncertainty reported in most studies applies are often heterogeneous for instance njana et al 2016 reported an error margin at α 0 95 of less than 2 on the mean predicted basic wood density for three species whereas the tree level confidence envelope showed a variability of 33 we assumed an error margin of 20 around the basic wood density for the lorraine region as reported in table 1 and as did green et al 2006 for ireland under such an assumption the basic wood density of oak could be lower than that of fir while this can be true for two stands it cannot be true at the scale of a region or a country the same rationale applies for the carbon fractions and the biomass expansion factors for this reason the confidence intervals around the living biomass shown in fig 4 are probably too wide appropriate error margins for the regional and national levels remain to be defined but they will certainly be smaller than those reported in table 1 additional guidelines in this regard are needed valade et al 2017 carried out a literature review on the substitution factors of construction products and energy wood and found a large range of substitution coefficients as reported in table 1 they obtained an error margin of about 50 for energy wood however the uncertainty associated with construction products was larger they also found a log normal distribution for construction products whereas the substitution factors of energy wood were approximately gaussian although the log normal distribution could easily be implemented in cat simulating uncertainty using different error margins for the substitution factors of different hwp classes remains to be implemented for increasingly more complex life cycles and uncertainty constraints it is expected that monte carlo sampling may not necessarily be the most appropriate tool groen et al 2014 other uncertainty techniques like latin hypercube and quasi monte carlo sampling have been found to provide more accuracy and faster convergence in determining the sample mean than monte carlo sampling for this reason it would be interesting to use cat as a testing ground to compare these techniques and provide more targeted recommendations in the context of complex unfccc life cycles in any case cat users can already test and produce risk analyses under uncertainties in order to support potential adaptation plans operational tactical strategic or contingency that could reduce the carbon footprint across the entire forestry supply chain 5 3 how cat compares to other tools from the lulucf sector in the lulucf sector and particularly in the forestry sub sector brunet navarro et al 2016 listed 41 carbon accounting tools and highlighted 13 criteria seven structural 1 1 1 bucking allocation 2 fossil fuel substitution 3 industrial processes 4 carbon pools 5 product removal 6 recycling 7 versatility of allocation parameters and six end user 2 2 1 graphical user interface gui 2 the open sourcing of the code 3 user training opportunities 4 technical support service 5 user community and 6 regular model updates criteria that should be met to ensure the highest structural achievement and end user satisfaction none of the 41 tools mentioned in the review could achieve a perfect score for the 13 criteria even though some of them were part of the most advanced a group as per brunet navarro et al 2016 a former version of cat was listed in this a group under the name of capsis because it was an extension of the platform and not yet a standalone application when brunet navarro et al 2016 carried out their study the current version of cat counts among the most complete carbon accounting tools satisfying 12 of the 13 criteria even with the cot module from remsoft hennigar et al 2008 cameron et al 2013 the only criterion that was not met was that of versatility of allocation parameters in its current version cat considers that the flux configuration is constant over time it turns out that new policies are gradually implemented and things do not change drastically in one day a more realistic representation would be one where the flux configuration is initially set to the business as usual scenario and gradually changes to an alternative configuration over one or two decades we intend to implement this new feature in cat by allowing the user to define two flux configurations an initial configuration representing the current context and a target configuration with a time interval over which the implementation is gradual during the time required for the full implementation the biomass would be split with a linearly increasing proportion of the biomass being sent to the target configuration this remains to be developed and the current software structure makes it possible without major changes 5 4 toward a universal life cycle assessment tool for all the sectors in the last two decades the development and use of carbon accounting tools has gained in popularity in most emission sectors brunet navarro et al 2016 whittaker et al 2013 gentil et al 2010 laurent et al 2014 with various degrees of development and emphasis on complexity and uncertainties even if our focus was the lulucf sector the way cat deals with the representation of the cross sectoral life cycles is an interesting advance compared to carbon accounting tools used in other sectors whittaker et al 2013 gentil et al 2010 laurent et al 2014 policy makers base their decisions not only on carbon balance but also on other competing criteria which therefore need to be taken into account in models and software for instance in the lulucf sector the availability of certain wood products the supply of other essential chemical wood elements e g n p k used as indicators of forest health and of economically viable extractable wood compounds with pharmaceutical nutraceutic naturopathic cosmetic or industrial properties will need to be monitored and their life cycle evaluated the methodological difficulty here would be to simultaneously account for these different yet inextricably linked life cycles that also spread across various sectors this is the case for carbon as well as the extraction transformation and recycling of biochemical compounds whose life cycles include parts of the supply chain network in the forestry agriculture industrial and waste sectors all of this leads to a difficult representation of the structure of life cycles as well as of difficult trade offs between competing stakeholders involved in the carbon timber energy conservation recycling and extractable compound industries it will affect decision making when changing supply chains and life cycles will be about managing preferences between multiple criteria cat would need to evolve to become a more universal tool in order to evaluate the carbon balance for any type of sector and chemical compound to achieve this cat would require more than just applying simple optimization algorithms as has been done in remsofttm cot cameron et al 2013 cat capabilities would need to offer forest stakeholders smart interactibility between i the step of collectively defining the constraints and opportunities of changing their production lines and the way various hwp and compounds of interest can transit along them i e the constraint space and ii the step of expressing their competing preferences for different outcomes in terms of carbon wood products and biodiversity a k a the space of criteria cat has been designed with this type of interactivity in mind and could benefit from recent advances in interactive multi objective optimization algorithms from operations research see e g the interactive reference point multi objective method developed by dujardin et al 2015 altogether even with all its limitations this first version of cat demonstrates a new way to explore and evaluate options as well as to make decisions in order to adapt the supply chain of forest wood products to the new reality of the 2050 cop21 climate change targets and to the growing need to conserve the forest s ecological capital and to support industrial innovation 6 conclusions according to the recent international negotiation around climate change and with the cop21 paris agreement unfccc 2015 in particular unfccc members will have to report ghg emissions in managed forests and harvested wood products for the next commitment periods carbon accounting tools can prove to be valuable options for the assessment of ghg emission life cycles and for the support of structural decisions that can help decarbonize the unfccc sectors as the boundaries between the different sectors are opened forming more and more complex cross sectoral ghg emission life cycles existing carbon accounting tools and software become less adapted for representing this growing complexity and assessing the impact of numerous uncertainties risks of carbon leakage and double counting on the global carbon balance this paper showcases the new cat v1 0 software that counts among the most complete carbon accounting tools capable of supporting decisions to adapt the cross sectoral supply chains to the new social ecological reality and innovation needs cat particularly represents a unique and major advance in the way users can easily create modify compare and analyze scenarios of complex and uncertain ghg emission life cycles these scenarios can differ in terms of their level of uncertainties and network complexity when dealing with many lines of wood extraction production and transport use and transformation into energy and waste management even with its current limitations and focus on the forestry sector cat still represents a milestone for future more universal life cycle assessment tools that can deal with complex uncertainties complex supply chain structures and trade offs between a diversity of fluxes that need to be managed simultaneously between the unfccc sectors e g ghg emissions biomass of raw material biomolecular compounds of economic interest multiple wastes energy and economic fluxes acknowledgements many projects contributed to the development of cat the authors are thankful to the following funding agencies france forêt for a two year contract on carbon accounting agence de l environnement et de la maîtrise de l énergie ademe 1260c0055 through the reactiff i gesfor project the french national research agency anr 12 agro 0007 through the forwind project and the office national des forêts onf through the modelfor2 convention the authors are thankful to sylvain caurla umr lef and two anonymous reviewers for their constructive comments on a preliminary version of this paper special thanks are due to estelle vial fcba and jessica françois université de lorraine for their advice regarding the user interface of cat m f developed cat and r m developed the mathilde model j b p and m f designed the case studies j b p ran the simulations using cat and performed the analysis all authors contributed to the writing of the paper abbreviations bau business as usual capsis computer aided projections of strategies in sylviculture ce carbon emissions doc f degradable organic carbon fraction dom dead organic matter fwd fine wood debris ghg greenhouse gas hwp harvested wood products ipcc intergovernmental panel on climate change lb living biomass lulucf land use land use change and forestry nfi national forest inventory swds solid waste disposal site unfccc united nations framework convention on climate change appendix a supplementary data the following are the supplementary data related to this article data profile data profile supp mat supp mat appendix a supplementary data supplementary data related to this article can be found at https doi org 10 1016 j envsoft 2018 06 005 
26367,software applications for life cycle assessment of greenhouse gas ghg emissions have become popular over the last decade their objective is to provide insight into how ghg emissions could be reduced in the sectors defined by the unfccc however boundaries between these sectors are not closed and current tools are not designed to represent this complexity or to assess the numerous sources of uncertainties in this paper we present cat v1 0 software developed for managed forests in the lulucf sector but whose emission life cycle is linked to that of other sectors while the structure of the software follows ipcc guidelines it also contains additional features such as an embedded monte carlo error propagation technique and a user friendly flux manager that allows for complex cradle to grave representations of the wood transformation industry the flexibility of the software is illustrated through two case studies in northeastern france keywords lulucf sector wood transformation industry cradle to grave life cycle assessment uncertainty assessment monte carlo simulation software name cat v1 0 developer mathieu fortin umr silva inra ul agroparistech 54000 nancy france e mail mathieu fortin re gmail com available since 2017 hardware required no specific requirement software required java 8 availability fully available at https sourceforge net projects lerfobforesttools files cat website https sourceforge net p lerfobforesttools wiki cat license lgpl v3 cost free program language java program size 7 1 mb 1 introduction since the adoption of the united nations framework convention on climate change unfccc in 1992 the accounting of anthropogenic greenhouse gas ghg emissions has become an international issue every year the members of the unfccc that were recognized as industrialized countries in 1992 better known as annex i countries have to report their national ghg emissions in different sectors unfccc 2014 i energy ii industrial processes and product use iii agriculture iv land use land use change and forestry lulucf and v waste since 1995 the international panel on climate change ipcc has published guidelines that establish the methodological basis for carbon accounting in all the aforementioned sectors however their implementation is not straightforward and can be hindered by numerous uncertainties risks of carbon leakage and double counting fortin et al 2012 dong et al 2013 affecting interpretations even for experts in this field shvidenko et al 2010 jonas et al 2010 one reason is due to the fact that the boundaries between the five sectors are not closed forming complex emission life cycles that are hard to represent and even harder to evaluate in the lulucf sector for example the cost efficiency and carbon balance heavily depend on the correct but complex integration between multiple lines of wood extraction transportation transformation and production and the fossil co2 emissions that can be linked to the industrial processes sector the production of biomass clearly interacts with the energy sector management of the harvested wood products after their useful lifetime belongs to the waste sector all these interactions are hard to evaluate and decision makers therefore need software implemented carbon accounting tools to obtain an overall view of the big picture this increasing popularity of carbon accounting tools is not limited to the lulucf sector however regardless of the sector most of these tools show major limitations as to how emission life cycles can be realistically and easily represented and to how the uncertainties associated with these complex life cycles can be assessed with respect to uncertainty assessment theory on the topic shows that the more complex and integrated the life cycle is with cascading use and open multi loops operating at different scales the greater the need for sophisticated error propagation methods will be groen et al 2014 among the 15 tools reviewed by gentil et al 2010 in the waste sector uncertainty assessment was not even selected as a primary criterion of software evaluation mirroring the fact that uncertainty assessment was applied in only 4 of the case studies laurent et al 2014 similarly whittaker et al 2013 found that only one out of 11 carbon accounting tools used in the uk agricultural sector implemented some features for uncertainty assessment in the lulucf sector where carbon accounting tools are even more abundant brunet navarro et al 2016 reported 41 tools with only 40 of them integrating uncertainty assessment features of any sort regarding the realism of the life cycles represented most carbon accounting tools are designed to provide the carbon balance over a regional supply chain or with a predefined generic structure representing the basic supply chain compartments including extraction transport production lines product use and end of life users can change the interactions between the components of the emission life cycle in only a limited number of carbon accounting tools however even in these few cases the changes need to be hard coded or implemented in a spreadsheet like in the co2fix model schelhaas et al 2004 or the cot module of remsoft cameron et al 2013 all of this inevitably hinders the capacity of users to obtain a reliable uncertainty assessment and to avoid carbon leakages and double counting when dealing with these complex emission life cycles it also distorts the big picture so that the cost and benefits of detailed disruptive technologies management and policies that could help reduce ghg emissions through a better integration of the different sectors cannot be evaluated in this paper we present cat v1 0 a software platform that mainly applies to managed forests in the lulucf sector but also considers interactions with other sectors the tool offers a user friendly interface that makes it possible to represent complex emission life cycles inherent to managed forests moreover the assessment of the carbon balance is supported by built in monte carlo error propagation methods cat was initially developed as part of a larger forestry modeling initiative within the open source capsis platform computer aided projections of strategies in silviculture see dufour kowalski et al 2012 but its implementation has been improved to create a standalone application this paper is structured as follows we first describe the architecture of cat with its different carbon pools as well as how uncertainty assessment and scenario comparisons can be carried out a second section presents the java implementation of the software and the user interface that enables the design and comparison of complex emission life cycles we then showcase the modularity of cat in a third section based on two case studies in lorraine the largest forested region of france changes in the harvesting production lines diversity of products and end of life processes are suggested and debated at a regional level to support the cost effective transition from fossil fuel to renewable energy as well as the use of grow and bury strategies to improve the carbon balance of the region we finally discuss the interest of cat for future software in combination with previous initiatives and future challenges the source code of cat the compiled application to reproduce the results of the case studies and the basic technical services can be found at https sourceforge net p lerfobforesttools wiki cat 2 cat architecture cat follows the ipcc guidelines for national ghg inventories in the lulucf and waste sectors ipcc 2006a b 2014 its system boundaries follow that of the production approach as defined in ipcc 2006a ch 12 the basic implementation of cat offers the default methods shown in the 2006 version of the ipcc guidelines which are referred to as tier 1 methods however it also provides the flexibility for higher tier implementation basically cat recognizes the following carbon pools i aboveground and belowground living biomass ii dead organic matter dom and iii harvested wood products hwp either in use or deposited at solid waste disposal sites swds soil organic carbon is assumed to be constant as in tier 1 ipcc 2006a p 4 23 moreover litter and dead wood are not differentiated in the dom carbon pool although it is possible to specify different parameters for each one in addition to the ipcc standards cat can provide estimates i of cumulative material and energy substitution ii of cumulative fossil fuel carbon emissions during the life cycle of the different hwp iii of the accumulation of non degradable hwp at swds and iv of cumulative methane ch4 emissions caused by the degradation of hwp at swds material and energy substitution represents the ghg emissions avoided when a hwp replaces an alternative product sathre and o connor 2010 the substitution is estimated by comparing the life cycles of the two products and as such it already takes the fossil fuel emissions during their entire life cycles into account the replacement of alternative products by hwp also implies the comparison of two scenarios in which the substitution is estimated from the difference in production to avoid double counting cat provides the fossil fuel emissions only when visualizing a single carbon balance simulation and the substitution only when comparing the carbon balance of two scenarios like many carbon accounting tools in the lulucf sector see e g schelhaas et al 2004 kurz et al 2009 cat does not run any growth simulations it retrieves growth projections from existing models or yield tables a growth projection can be defined as a succession of growth steps that leads to a series of predicted characteristics at different times t cat retrieves the trees at each time t and deals with them in a different way depending on their status which can be alive dead harvested or windfall fig 1 this list of statuses is not extensive but is adapted to the context of european forests where windstorms represent the main disturbance schelhaas et al 2003 gardiner et al 2010 living trees are simply converted into carbon as shown in section 2 1 trees with other statuses dead harvested or windfall are handled by a flux manager which is the major component of the cat interface it is extensively described in section 3 3 2 1 living biomass the basic implementation of cat converts living trees into carbon using the tier 1 methodology which is based on the commercial volume ipcc 2006a p 2 12 2 1 c ˆ t s i v t s i bd s bef ag s bef bg s cf s where c ˆ t s i is the estimate of the carbon content of tree i of species s at time t mg of c v t s i is the commercial volume of this tree m3 bd s is a species specific basic density mg m 3 of dry biomass bef ag s is a species specific aboveground biomass expansion factor bef bg s is a species specific belowground biomass expansion factor and cf s is a species specific carbon fraction the ipcc guidelines ipcc 2006a ch 4 provide default values for these factors at least for groups of species when species specific values are not available the carbon stock in the living biomass at a particular date is estimated by summing the carbon content of all the trees at time t i e s i c ˆ t s i equation 2 1 can easily be adapted to provide the stocks in the aboveground and belowground compartments 2 2a c ˆ ag t s i v t s i bd s bef ag s cf s 2 2b c ˆ bg t s i v t s i bd s bef ag s 1 bef bg s cf s in practice only the basic densities are species specific in the ipcc tier 1 method whereas the biomass expansion factors and carbon fractions are given by groups of species i e broadleaved and coniferous species ipcc 2006a ch 4 these default basic densities biomass expansion factors and carbon fractions are already available in the user interface as well as those used for the tier 2 method in france for national reporting see citepa 2015 p 1101 cat also allows the user to define his own factors like in tier 2 through a simple dialogue box these country or context specific values can be saved in a file for future use tier 3 methods can be implemented using java interfaces as explained in section 3 1 2 2 dead organic matter windfall and dead trees as well as residues from harvested trees feed the dom carbon pool when they are retrieved from the growth simulation they are converted into carbon using eq 2 1 depending on the size cat recognizes three types of debris fine woody debris commercial sized woody debris and coarse woody debris the first is actually the difference between the aboveground carbon content and the carbon content in the commercial part of the tree commercial sized woody debris is the carbon content of the commercial biomass of the dead or windfall trees coarse woody debris is composed of the carbon in the stumps and roots by default the fine woody debris is left on the forest floor and becomes part of the litter likewise coarse woody debris and a proportion of commercial sized debris are also left on site to integrate the dead wood compartment fig 1 the dom carbon pool is calculated as the sum of these two compartments the stock in each compartment is assumed to follow a first order decay rate marland and marland 2003 which is actually an exponential distribution see lawless 2003 p 17 2 3 pr t t e t λ where t is the lifetime t is the time year and λ is the average lifetime the probability that a particular unit of carbon is still in the litter or the dead wood compartment at time t 1 is conditional on the probability that it was already there at t 0 this probability can be defined as 2 4 pr t t 1 t t 0 e t 1 λ e t 0 λ e δ t λ where δ t t 1 t 0 when dealing with a large number of carbon units the probability shown in eq 2 4 can be treated as a proportion the balance of probability i e 1 e δ t λ is actually the amount that is released into the atmosphere the ipcc guidelines are based on the half life t 1 2 and not on the average lifetime see ipcc 2006a ch 12 while the average lifetime is the expectation of the lifetime the half life is the time until half the matter remains i e pr t t 1 2 with a first order decay rate the half life is easily derived from the average lifetime as follows 2 5 t 1 2 l n 2 λ from eq 2 5 it can be deduced that the half life is always smaller than the average lifetime in cat the user can specify the lifetime either in terms of average lifetime or half life the latter being the default mode the ipcc guidelines also use the decay rate k in their terminology with an exponential distribution like in eq 2 3 the decay rate is actually the multiplicative inverse of the average lifetime i e k 1 λ zell et al 2009 conducted a meta analysis of decay rates for coarse woody debris in temperate forests they estimated a decay rate of k 0 031 or equivalently an average lifetime of 32 years the decay rates of branches and twigs are much larger for beech they ranged from 0 149 to 0 220 which implies average lifetimes of between 4 5 and 6 7 years boddy and swift 1984 2 3 harvested wood products hwp compared to the living dead or windfall trees harvested trees are processed differently cat provides a user friendly interface that allows users to represent the hwp supply chain of interest this interface is largely described in section 3 3 regardless of the complexity the implementation of any supply chain can be described as follows first harvested trees are transformed into logs using a bucking module to avoid any leakage the difference between the carbon contained in the logs and the aboveground carbon is considered as fine woody debris whereas the carbon in the stump and the roots becomes coarse woody debris see section 2 2 secondly these different logs are sent to production lines which define the different hwp as well as their associated half lives or average lifetimes substitution factors and the fossil fuel emissions associated with their life cycles the amount of products still in use is updated over time using a first order decay rate see eq 2 4 the part of the hwp that is no longer in use at a particular time t can be either recycled or deposited at the swds thirdly the carbon in disposed hwp is managed according to the ipcc guidelines for the waste sector ipcc 2006b ch 3 the degradable organic carbon fraction doc f which is the part of the carbon that is actually subject to decomposition and the half life or average lifetime of this degradable fraction can be specified by default cat assumes a doc f of 0 4 which is slightly lower than the ipcc tier 1 value of 0 5 ipcc 2006b p 3 13 actually the ipcc tier 1 value of 0 5 has been found to overestimate the actual degradable fraction by some authors barlaz 2006 ximenes et al 2008 the default decay rate of the doc f is assumed to be that of a temperate climate with wet conditions k 0 03 see ipcc 2006b p 3 17 which leads to an average lifetime of 33 years the sum of the carbon in the hwp in use and what remains of the degradable part at the swds is considered as the hwp carbon pool given the anaerobic conditions a part of the carbon that decomposes at swds is partly released in the atmosphere in a ch4 form which has a higher global warming potential in anaerobic conditions cat assumes that 50 of the carbon emitted from the swds is methane which corresponds to the tier 1 method ipcc 2006b p 3 15 depending on the facilities of the swds these methane emissions can be reduced for instance if the swds is managed under semi aerobic conditions the methane emissions are reduced by half ipcc 2006b p 3 14 by default cat assumes that the swds belongs to the managed semi aerobic category however the user can choose another type among the five suggested in the ipcc 2006b p 3 14 guidelines the non degradable part of carbon that accumulates at a swds is assumed to be permanently sequestered scholz and hasse 2008 the fossil fuel emissions associated with the hwp life cycle the methane emissions and the substitution are also considered in cat the cumulative sequestration or emissions related to these can be calculated over the projection length 2 4 uncertainty assessment a major issue in carbon accounting remains the many sources of uncertainty green et al 2006 ståhl et al 2014 valade et al 2017 the growth simulation is uncertain even before estimating the carbon balance the errors in the parameter estimates and the errors in the model inputs which stem from the sampling and measurement errors count among the sources of uncertainty in growth models kangas 1999 the estimation of the carbon balance implies additional sources of uncertainty on top of those of the growth simulation in order to assess their impact cat implements uncertainty assessment features for five categories of parameters i biomass expansion factors ii basic wood densities iii carbon fractions iv half lives or average lifetimes and v substitution factors different methods exist for propagating errors and assessing prediction uncertainty across complex emission life cycles groen et al 2014 analytical methods are mainly based on taylor series e g gertner 1990 gertner et al 1995 however when the emission life cycle under study is too complex these taylor series based methods are difficult if not impossible to implement an alternative is the well known monte carlo technique which is extensively described in rubinstein and kroese 2008 it consists of drawing random deviates from the distribution of the parameter estimates and computing a realized value of the phenomenon of interest repeating this process a great number of times provides many realizations of the phenomenon and simulates its natural variability growth simulations based on the monte carlo technique are often referred to as stochastic simulations whereas those based on the mean only are known as deterministic simulations vanclay 1994 p 7 cat is designed to deal with growth simulations whether they are stochastic or deterministic in both cases the application allows the user to specify a distribution which can be gaussian or uniform as well as an error margin for each one of the five aforementioned categories of parameters involved in the estimation of the carbon balance the error margin is actually proportional to the current value of the parameter an error margin of 10 for a uniform distribution implies that the current parameter is multiplied by a random deviate between 0 9 and 1 1 for gaussian deviates the error margins are set at a probability level of 0 95 if the growth simulation was deterministic the user can decide how many realizations are to be performed in this uncertainty assessment in the case of stochastic growth simulations the number of realizations is that of the growth simulation for each realization random deviates are drawn to account for species specific errors in the aforementioned parameters and the carbon balance is estimated accordingly when all the realized carbon balances have been calculated cat computes non parametric confidence limits based on the percentile rank method see efron and tibshirani 1993 p 170 and the probability level set by the user the deviates are stored in memory enabling consistent comparisons between different scenarios as shown in section 2 5 when running the first uncertainty assessment the deviates as well as the realizations for which they apply are recorded when running subsequent uncertainty assessments those deviates are reused in the same realization this way the first realization of each monte carlo simulation is based on the same random deviates it is also possible to enable or disable the variability for some parameters for instance it can be decided to disable the variability of the carbon fractions while enabling those of the basic wood densities and half lives the ipcc guidelines provide some reference values for uncertainty assessment which are based on the literature these values as well as their references in the literature are shown in table 1 these are the default uncertainty levels implemented in cat the values can be changed within a given range using the cat interface 2 5 scenario comparisons cat allows the comparison of scenarios in two different manners depending on the type of forest management the first comparison mode is available for any type of management and is based on the comparison of punctual estimates for example if two scenarios have been simulated from 2017 to 2050 it is then possible to compute the differences between the estimated stocks at the end date of both simulations furthermore the carbon stock estimates can be compared within the same scenario for a given scenario it can be of interest to quantify any increase or decrease in the different carbon pools between 2017 and 2050 if the growth simulations are stochastic in both scenarios and if they are based on the same number of realizations then the realizations are paired and the comparison is made on the differences observed in each pair of realizations this procedure is basically the same as a paired t test which has a greater statistical power than a non paired test zimmerman 1997 when the comparison is run under this idea of paired t test the result becomes a set of realized differences the mean of these realized differences is the estimated difference confidence interval limits are calculated using the percentile rank method see efron and tibshirani 1993 p 170 as well when the number of realizations is different across the simulations the comparison is based on the difference between the means of the simulations as in a non paired t test compared to the paired mode this comparison only provides a mean difference and its variance confidence intervals are then based on a gaussian distribution the second comparison mode is available only if the growth simulation took place in an even aged stand in both scenarios in such a context the carbon balance can be calculated on an infinite sequence as if the same management strategy was repeated over time a comparison of the carbon balances in infinite sequence is then made available to the user such comparisons have been performed in different pure even aged stands with the objective of identifying the most carbon friendly forest management strategies liski et al 2001 kaipainen et al 2004 vallet 2005 vallet et al 2009 fortin et al 2012 2014 3 software implementation 3 1 interfaces for compatibility and extended use cat was coded in java using the library known as lerfob foresttools which has dependencies on other java libraries repicea for the monte carlo technique and user interface features and jfreechart for graphical display cat is distributed as a standalone application but it can also be called from another java application or even from another language in its current version cat can be used for carbon life cycle assessment potentially using any sort of existing forest model whose outputs are compatible with cat s inputs the application assumes that the growth projections that cat relies on can be described through two classes of objects a stand class and a tree class as a standalone application cat allows the import of yield tables or growth simulations from comma separated files csv calling cat from another java application is made possible if the two aforementioned classes implement the two interfaces catcompatiblestand and catcompatibletree available in the lerfob foresttools library for example some growth models of the capsis platform dufour kowalski et al 2012 already implement these two interfaces so that cat is available as a tool within the platform a customized implementation with the go model lousteau et al 2012 which was implemented in python has also been made possible through the py4j library www py4j org further information can be found at the cat website the two above mentioned interfaces ensure that the growth simulations can provide all the basic information for tier 1 methods additional interfaces are optional for the tree class and provide methods for biomass and carbon estimation if they are implemented cat may rely on these methods instead of using the ipcc tier 1 approach these additional interfaces are listed in table 2 3 2 user interface running a carbon balance simulation in cat consists of three steps 1 import the growth simulation which can be done automatically if cat is part of another software such as the capsis platform 2 set the biomass parameters and define the flux configuration that handles dead harvested and windfall trees 3 run the simulation and obtain the results the user interface has all the components to go through these three steps fig 2 the file menu contains the options to import yield tables or projections from growth models in the upper part of the interface two combo boxes entitled biomass parameters and flux manager dom hwp provide access to the biomass conversion factors and the flux configuration respectively in both cases some presets are available however the user can choose to customize the biomass parameters as well as the flux configuration by choosing the customized option in the combo box clicking on the folder icon on the right hand side of the combo box display windows to specify the new biomass parameters or create new flux configurations as described in section 3 3 once the biomass parameters and the flux configuration have been set the simulation is run by clicking on the button in the top left corner 12c of the interface or by choosing the option calculate carbon balance in the actions menu the results of the simulation are displayed in the panel on the right in different tabs the user can choose which graph to display using a combo box just under the tab label among others users can visualize the evolution of the carbon stocks in the different pools the annual flows per log grade and the annual flows per harvested wood product the panel on the left contains check boxes which enable or disable the carbon pools to be displayed in the panel on the right right clicking on the tab label of a particular simulation in the panel on the right causes a pop up menu to appear this pop up menu contains different options to either delete this tab all the tabs or only the other tabs more importantly it also contains options for exporting the result or comparing the simulations the menu bar in the upper part of the interface offers additional features under the options menu the units can be switched from c to co2 eq the probability level of the confidence intervals which is set to 0 95 by default can be changed as well the global warming potential factors used by default are those of the fifth assessment reports on climate change ipcc 2013 p 714 however the user can decide to use those of the second or fourth assessment reports if more relevant the settings of the uncertainty assessment can also be changed through this menu 3 3 flux manager the flux manager is the central element of cat since it manages all the carbon that is not in the living biomass access to the flux manager is granted by choosing the customized option in the combo box on the right side in the upper panel of the interface fig 2 in the upper part of the window the user can choose a bucking module which actually defines the different log grades fig 3 the basic implementation consists of a splitting of incoming volumes into particle and sawing log grades other implementations are possible in which more log grades are available the flux manager window contains a tool bar on the left hand side the top three icons represent industrial dead organic matter and swds processors respectively the flux configuration is in the main panel the user can create as many processors as needed by simply dragging and dropping any of the three icons from the tool bar to the main panel double clicking on newly created processors gives access to a dialogue in which the half life and other features can be set processors can be linked by clicking on the fifth and sixth icons in the tool bar the solid line represents an instantaneous flux between two processors at the creation of the hwp while the dashed link indicates a flux after the useful lifetime the magnitude of any instantaneous flux can be set by double clicking on the small button at mid range along the solid line a flux is expressed as a percentage of the outcome of an upstream processor by default there is a series of processors on the left hand side of the main panel that correspond to the different types of woody debris and of log grades that can be obtained from the bucking module these processors on the left are the entry points of the flux configuration all of them must be linked to either an industrial a swds or a dom processor the flux manager is meant to detect two types of inconsistencies the first case occurs when the sum of the flux from a particular processor is different from 100 the fluxes are then displayed in red as a warning to the user a second type of inconsistency is the occurrence of endless loops if a downstream processor feeds an upstream processor along the same stream then the biomass can loop endlessly between these two processors in such a case all the fluxes that are part of the endless loop are displayed in orange to help users correct this situation a carbon balance simulation cannot be run if inconsistencies of these two types are present in the flux configuration an example of a simple flux configuration is given in fig 3 the two dom processors account for different half lives in the litter and in the dead wood the five processors with blue borders represent those that produce hwp they are referred to as paper and paperboard wood based panels sawnwood energy wood and incineration the treatment of these different products after their useful lifetime is different since there is no dashed link coming out of the energy wood and incineration processors the products are assumed to oxidize as for the paper and paperboard the wood based panels and the sawnwood they are sent to the recycling facility after their useful lifetime where 60 is recycled into energy wood 30 is incinerated and 10 is sent to the swds the user can reach a higher level of complexity by setting additional industrial dom and swds processors basically all the processes involved in the life cycle of hwp such as industrial processing transport and distribution maintenance recycling and disposal can be created moreover different wood use alternatives such as the harvesting of fine or coarse woody debris for energy production riffell et al 2011 francois et al 2014 or wood burial scholz and hasse 2008 can be easily designed by creating new fluxes an example of a more complex flux configuration is shown in fig 4 we illustrate how this higher level of complexity can be handled through two case studies in section 4 4 application examples in this section we present two application examples the first one aims at testing alternative scenarios for increasing the availability of biomass in the context of transition to renewable sources of energy the second example is inspired by the work of scholz and hasse 2008 who suggested that carbon emissions could be partially compensated for by burying large amounts of wood in both case studies we exemplify the strengths of cat by coupling it with a multi species forest growth model called mathilde this coupling was made possible by integrating cat into the capsis platform dufour kowalski et al 2012 which hosts the aforementioned model using the growth model and cat we explored alternative scenarios applied to the lorraine region in northeastern france 4 1 biomass availability in lorraine france in europe biomass is considered as an important source of energy that could partly replace fossil fuels and thereby decrease ghg emissions nord larsen and talbot 2004 sikkema et al 2011 repo et al 2011 among renewable energies solid biomass is actually the most important source for heating and a significant source for electricity production eea 2016 since 2009 france has adopted an aggressive policy of conversion to renewable energy which promotes the installation of biomass boilers in industrial and public buildings energy wood has been traditionally considered as carbon neutral because it was assumed that it would not cause any net emissions since the amount of co2 released into the atmosphere during the combustion should be taken up by the remaining growing forests sikkema et al 2011 repo et al 2011 carbon neutrality appears as a scale and usage dependent concept that is no longer universally accepted because it can lead to a flawed assessment of the carbon footprint of biomass combustion at a given temporal scale johnson 2009 increasing the production of energy wood may lead to conflicts between future needs in energy wood pulp other wood products and ecological services from forests lecocq et al 2011 caurla et al 2013 in oak and beech stands which are typical of northern france increasing the production of energy wood through an increased harvest intensity usually leads to a worse carbon balance vallet 2005 fortin et al 2012 2014 if the forest management remains the same then industry may have to adapt its supply chain to produce more wood pellets out of wood chips and wood logs which are currently used to produce paper and panel boards part of the fine woody debris that is currently left on the forest floor after harvesting could also be processed into wood pellets francois et al 2014 although some impacts on biodiversity should be expected riffell et al 2011 we tested such alternative scenarios for the lorraine region in northeastern france using the data from the french national forest inventory nfi and the coupling between the mathilde model and cat more information about the model and the dataset can be found in the supplementary materials sm1 and sm2 in this case study we present projections of the carbon balance in oak beech and hornbeam stands in lorraine for the period 2015 2100 to do this we first made growth projections using the mathilde model we then used cat to compare the carbon balance of the business as usual scenario hereafter referred to as bau which represented the baseline for the french forest sector with three alternative scenarios of wood use for the purpose of increasing the availability of energy wood in the first alternative scenario s1 we reoriented 80 of the small sawlogs and 10 of the large sawlogs which are mainly used as lumberwood to the production of wood pellets in the second alternative scenario s2 the forest industry was assumed to extract an additional 40 of the fine woody debris i e 50 of the total woody debris immediately after harvesting to produce energy wood in the third alternative scenario s3 the forest sector was assumed to combine the first and second alternative scenarios building on the available sources of information for the industrial sector in northern france normandin 1990 paquet and deroubaix 2003 agreste 2012 2015 lenglet 2015 and previous studies on the carbon balance in the lorraine region vallet 2005 fortin et al 2012 we designed a flux configuration for the bau scenario using the flux manager in cat this flux configuration is shown in fig 4 more details can be found in the supplementary material sm3 the alternative scenarios were simply adapted from this flux configuration by adding or changing some fluxes the files containing the bau and the three alternative flux configurations are available online at https sourceforge net projects lerfobforesttools files cat lorraine they can be visualized using the cat application we simulated the evolution of the carbon stocks in the pools over the 2015 2100 period for the four scenarios we then compared the alternative scenarios to the baseline in terms of predicted stocks and cumulative fluxes at the end of the projection we also used the built in features of cat to assess the impact of simulated uncertainty in the five categories of parameters for the sake of the example the uncertainty levels were set to their default values as shown in table 1 4 1 1 evolution of carbon stocks and emissions under the bau scenario using cat we managed to represent the current wood supply chain of the lorraine region and to simulate its carbon balance until 2100 based on the 2015 nfi data the expected stocks in the carbon pools of the living biomass lb dead organic matter dom and harvested wood products hwp as well as of the cumulative fossil fuel carbon emissions from hwp life cycles the accumulation of non degradable hwp in swds and the cumulative methane emissions from swds are shown in fig 5 on average the carbon stock in the lb remained stable at around 440 mg ha 1 of co2 eq until 2050 and then steadily decreased to 360 mg ha 1 of co2 eq until 2100 the hwp carbon pool showed an asymptotic pattern after the first five year interval of the simulation and remained approximately constant with an average stock of 36 mg ha 1 of co2 eq the carbon stock in dom slowly but steadily increased until it reached a plateau at 84 mg ha 1 of co2 eq in 2070 it must be stressed that these predicted trends for the dom and hwp carbon pools were partially biased by the fact that in the absence of data the initial conditions for the simulation in 2015 were artificially set to zero in these pools the impact of the simulated uncertainty in the parameters specified in table 1 was obvious for the lb and the dom fig 5b for all carbon pools these additional uncertainties resulted in wider confidence intervals throughout the simulation period most emissions came from the life cycle of the hwp and not from the ch4 emissions from the swds fig 5c and d actually the accumulation of non degradable carbon at the swds was small and partly compensated for by ch4 emissions within this time frame considering the uncertainties in the parameters specified in table 1 resulted in slightly wider confidence intervals the sequestration in the lb can be estimated as the difference between the carbon stocks in 2015 and 2100 calculating the realization wise estimates of the difference made it possible to obtain a confidence interval of the sequestration shown in fig 6 the simulated uncertainty in the biomass expansion factors the basic wood densities and the carbon fractions did not have much impact on the width of the confidence interval fig 6b the sequestration in the dom and hwp between 2015 and 2100 could be estimated but would necessarily be biased since the 2015 stocks were artificially set to 0 as we already mentioned 4 1 2 impacts of alternative scenarios on carbon stocks and emissions the differences between the alternative scenarios and the bau scenario in 2100 are shown in fig 7 the first alternative scenario which consisted of reorienting part of the lumber wood from sawmills to energy wood was not without effect on the carbon balance fig 7a and b this strategy implied a reduction of carbon sequestered in the hwp since a greater proportion of short lived hwp was produced fewer long lived hwp also meant less cumulated material substitution over time throughout the supply chain less cumulated sequestration of non degradable carbon at the swds and less cumulated ch4 emissions from the swds accounting for the uncertainties in the parameters of table 1 resulted in wider confidence intervals fig 7b despite the great deal of uncertainty around the estimated differences it was unlikely that the alternative scenario would result in a better carbon balance than the bau scenario in comparison the second alternative scenario which prescribed the use of an additional 40 of the fine woody debris immediately after harvesting to produce wood pellets fig 7c and d exhibited much larger values the gain in substitution was estimated at 42 mg ha 1 of co2 eq in 2100 when compared to the bau scenario it largely compensated for the reduction of the carbon stocks in the dom 8 mg ha 1 of co2 eq the increase in carbon stock in the hwp pool was modest 3 mg ha 1 of co2 eq the swds emissions and non degradable products remained unchanged because the same quantity of long lived hwp was produced in both scenarios factoring uncertainties did not change the interpretation of the results even though the uncertainty in cat parameters resulted in wider confidence intervals fig 7d this second alternative scenario would outperform the bau scenario in terms of carbon balance for the third alternative scenario fig 7e and f which merged the two previous alternative scenarios cat predicted that the effect should simply add up by broadly speaking producing the same picture as in the previous scenario this was essentially due to the fact that the first alternative scenario had a relatively low impact on the different pools we did not show the comparison of the stocks in the living biomass since the three alternative scenarios had no effect on this pool 4 1 3 impacts of alternative scenarios on harvested wood products beyond the carbon balance cat also provides estimates of the quantity of hwp by class the comparison of these estimated quantities between the alternative scenarios and the bau scenario are shown in fig 8 the impact of reorienting part of the current extraction from sawmills to wood pellets fig 8a caused an increase in the production of energy wood at the expense of other classes of hwp except for barrels the gain in energy wood was rather modest considering an area of 433 000 ha and a simulation period of 85 years this alternative scenario would increase the supply of energy wood by around 18 500 mg yr 1 of dry biomass which was far from the current governmental objectives in france colin et al 2009 accounting for the uncertainties in the parameters of table 1 also resulted in wider confidence intervals but this did not change the interpretation of the results in comparison the second alternative scenario which prescribed the use of an additional 40 of the fine woody debris immediately after harvesting to produce wood pellets significantly increased the energy wood production 45 mg ha 1 of dry biomass fig 8c and d over the study area and the simulation period such a quantity of energy wood meant a supply increase of 230 000 mg yr 1 of dry biomass which was definitely more in accordance with the current governmental objectives colin et al 2009 obviously none of the other hwp categories i e building furniture packages paper barrels was impacted by this scenario for the third alternative scenario fig 8e and f which merged the two previous alternative scenarios cat also predicts that the effect should simply add up for the same reasons previously mentioned for the results presented in fig 7e and f 4 2 how good would it be to bury wood for lorraine france the strategy of growing harvesting and burying wood in swds as an option for creating a durable carbon pool scholz and hasse 2008 is based on the idea that the buried form would quickly decarbonate the atmosphere or at least balance our emissions from fossil fuel activities the secondary long term circular economic idea is that the buried form could eventually be reused in hundreds or thousands of years from now to produce future resources of carbon compounds that could be utilized once the current fossil resources have been depleted scholz and hasse 2008 even though this strategy could support an incremental change in the area not yet covered by forests it has many ecological cultural and industrial drawbacks however beyond these considerations this suggestion has also been attacked on its supposedly positive carbon balance for instance the grow and bury strategy was said to ignore the material and energy substitution induced by the use of harvested wood products koehl and fruehwald 2009 it was also criticized for overlooking the impact of methane emissions produced by wood decomposition in the swds nevertheless it is now accepted that a consensus on the issue should be made based on clear evaluations of its carbon balance merit in different conditions koehl and fruehwald 2009 to approach this question in cat we kept the same setup as the one described in the previous example for the lorraine region based on the bau scenario we created another alternative scenario where 50 of the volume that was supposed to be transferred to the production lines was instead directly buried in the swds the comparison with the baseline is shown in fig sm2 in the supplementary material cat unequivocally predicted that the non degraded fraction of carbon that accumulates in the swds should be 120 mg ha 1 larger than in the bau scenario by 2100 the hwp pool would also have more carbon 20 mg ha 1 which can be explained by the fact that the half life of doc f in the swds is much greater than the half lives of short lived hwp in the bau scenario however this huge potential should be compensated for by the combined impact of the methane emitted from the buried wood in the swds and the reduction in the global substitution throughout the life cycle which was estimated at more than 200 mg ha 1 considering uncertainties on parameter types from table 1 does not change this interpretation fig sm2b the evaluation of this grow and bury strategy for the lorraine case is therefore expected to be inefficient for achieving the 2100 carbon targets 5 discussion 5 1 advancing the handling of complex life cycles in carbon accounting tools boundaries between the emission sectors defined by the unfccc are not hermetic forming complex cross sectoral life cycles current carbon accounting tools are not designed to represent this growing complexity or to evaluate the impact of numerous sources of uncertainty as such they are exposed to leakage and double counting fortin et al 2012 dong et al 2013 affecting interpretations even for experts in this field shvidenko et al 2010 jonas et al 2010 mckone et al 2011 cat turns the representation and analysis of life cycles along complex structures such as the wood supply chain into an exploratory exercise that minimizes cumbersome coding tasks and complex uncertainty assessments while maximizing users time spent at comparing testing and evaluating a wider range of relevant scenarios and hypotheses to the best of our knowledge no other software used in the lulufc brunet navarro et al 2017 agriculture whittaker et al 2013 and waste sectors laurent et al 2014 has this capacity with the case studies in the lorraine region we showed that cat provides options to easily create and compare different scenarios of life cycles which can differ in terms of wood extraction production lines use and end of life of many wood products cat s flux manager makes it possible to compare scenarios of different levels of complexity for instance the flux configuration of the bau scenario fig 4 could be compared to a much simpler scenario based on the configuration shown in fig 3 non trivial or contentious hypotheses like growing burying can be easily evaluated and compared in terms of carbon balance and industrial merit this application could potentially be used to support regional decision making and future consensus at a higher scale for such a complex multidimensional issue cat has been designed in such a way that the outcomes such as those in fig 5 and the structure follow the ipcc guidelines in the lulucf and the waste sectors ipcc 2006a b 2014 we also showcased how the tool can provide detailed information on the carbon contributions of particular emission loops cascading chains of fluxes and compartments e g specific factories to the overall carbon balance within the context of complex supply and recycling chains in our case studies we chose to aggregate all the sawmills energy factories land fill sites and recycling brokers from the lorraine region into single processors however the user could perform much more exhaustive analyses and use cat to obtain a more realistic representation of the different components of the life cycle and their contributions to the regional carbon balance there is no limit to the resolution of the life cycle at this time it must be stressed that cat does not provide any default values for the processor features which means that the user must provide for the biomass of the functional unit and the co2 emissions per functional unit for all the intermediate processors half lives and substitution factors must also be provided for final processors which produce hwp consequently high resolution life cycles imply a greater number of values that must be provided in the flux manager even though large life cycle databases exist such as ecoinvent wernet et al 2016 it may be a challenge to retrieve all the values needed to fully cover a high resolution life cycle it could be argued that some parts of the life cycles will eventually become obsolete and that there is no need to take them into account this could be the case of swds which belongs to the waste sector as a matter of fact many countries have banned the disposal of wood waste in swds the european union has already set ambitious targets for the reduction of disposed biodegradable waste such as wood eu 2008 even though the proportion of wood waste sent to swds is expected to decrease mantau et al 2010 it is unlikely that it will be null on the short term for instance the proportion of wood waste disposed in swds in 2015 was estimated at 19 in france guinard et al 2015 unless a major breakthrough occurs in waste management the waste sector will remain part of the life cycles although its importance will decrease over time even if cat represents an advance in the representation of complex carbon emission life cycles and in the implementation of uncertainty assessment there is still room for several improvements in order to make it a more viable option for instance the current cat version assumes that the industrial sector does not change over time however demand in wood products and the transformation sector both change over time and their dynamics inevitably interact with forest management and economic constraints to handle this dynamic national regional economic models of forest supply chains would need to be linked to cat for the lorraine region cat would for example benefit from integrating economic models of forest supply chains like the one developed by lecocq et al 2011 and caurla et al 2013 such economic models would help assess the impact of economic incentives subsidies taxes etc on the demand in forest products as a non linear function of price and income they would also explicitly consider the competition for resources as well as industrial capacities however this kind of implementation might require adaptations of both cat and the economic model in the 2006 ipcc guidelines ipcc 2006a ch 12 four different approaches for hwp accounting were suggested the stock change the atmospheric flow the production and the simple decay approaches there was no preference for any of these four approaches in its current form cat is based on the production approach it allows for exported products that can be represented in the flux manager however it cannot account for imported wood products according to decision 2 cmp 7 unfccc 2012 carbon in imported hwp should not be included in the national reporting of ghg emissions by the importing country however this decision could be amended in the future a substantial improvement in cat would consist of giving users the opportunity to change the boundaries of the emission life cycle by opening it to import like in the stock change and atmospheric flow approaches brunet navarro et al 2017 reported that it is rarely integrated into carbon accounting tools the ipcc guidelines suggest working under the assumption of constant organic soil carbon when the land use does not change and no data on this pool are available which is the case with cat ipcc 2006a p 4 23 by doing so we assume that forest management has no impact on the carbon in the soil this seems to be the case for traditional management practices but not for those that involve intensive biomass harvesting achat et al 2015 taking the changes in organic carbon stock into account requires models in the next version of cat we intend to implement the yasso soil carbon model liski et al 2005 tuomi et al 2011 whose structure could be connected to the flux manager processors representing dead organic matter in the flux manager would then be connected to the soil carbon model cat does not implement any sort of initialization for hwp and dom actually the stocks in hwp and dom at the beginning of the simulation are usually unknown and they are artificially set to 0 see fig 5a and b the ipcc guidelines suggest a procedure that consists of simulating the stocks in the hwp since 1900 relying on fao data and many assumptions ipcc 2006a section 12 2 this procedure has not been implemented in cat for two reasons first the procedure applies at the national level while the simulations in cat are not necessarily run at that scale secondly the uncertainty associated with these initial estimates of hwp and dom is not straightforward because this model initialization is missing comparisons between point estimates of the same scenario are biased for hwp and dom however the comparisons between an alternative scenario and a baseline scenario such as those shown in fig 7 are valid under the assumption of steady state more specifically if we assume that the hwp and dom pools are initially at a steady state then their stocks should be approximately those observed at the end of the projection in the baseline scenario to be valid the assumption requires projections that are long enough to reach the steady state and harvesting rates that do not change over time this is precisely the case in our baseline scenario actually the hwp and dom pools approximately stabilized after 2050 and 2070 respectively fig 5a and b the steady state assumption is one of the three strategies identified by brunet navarro et al 2016 for simulating initial hwp and it has also been used when it comes to estimating the carbon stocks in the soil and the litter e g liski et al 2001 2005 vallet 2005 given that the initial stocks in the hwp and dom pools as estimated under the steady state assumption are the same in both the alternative and the baseline scenarios their contribution simply cancels out when calculating the difference between the scenarios even though the steady state is not achieved it can reasonably be assumed that the contributions of these missing hwp in terms of carbon storage will be the same in both scenarios again they would simply cancel each other out in the calculation of the difference between the scenarios kurz 2010 lemprière et al 2013 this latter assumption is valid if the hwp have the same half lives in both scenarios which is precisely the case in this study even though these factors change in the alternative scenarios it could be rightfully argued that these changes only apply to new hwp and not to the hwp initially in use there is actually only one special case where the contributions of initial hwp do not cancel out in the difference which is when the recycling rate changes in the alternative scenario an increased recycling or reuse rate after the useful lifetime of long lived hwp could be implemented in a short period of time and there is no reason why initial hwp should be treated differently than newly produced hwp in this regard this was not the context of our case studies but the potential bias clearly deserved to be raised following the ipcc guidelines ipcc 2014 cat uses a first order decay rate for the lifetimes of dom hwp and doc f the first order decay process is largely described in marland and marland 2003 it assumes that the proportion of the matter that is discarded remains constant over time since the largest amount is observed at the beginning of the lifetime it implies that the greatest quantity of discarded matter occurs at the beginning of the lifetime as well an assumption that probably does not hold for long lived products marland et al 2010 a distributed decay model relying on other distributions has been suggested by marland et al 2010 their idea consists of treating the hwp not as a single pool but instead as a series of distinct products since the decay rates are dependent upon the time since they were produced in its current version cat partly implements this distributed decay model the products are not merged into a single pool instead the quantity of a particular product is independently actualized over the projection length using a first order decay and its user specified half life the quantities are then aggregated in order to provide an estimate of the co2 stock in the hwp pool however cat does not allow the users to change the half lives according to the production dates the option of using other decay functions such as weibull logistic or gamma is also missing in cat in order to implement the distributed decay model of marland et al 2010 we intend to implement these in the next version of cat these new decay functions coupled with the transition between two flux configurations could account for changes in the lifetime distribution of a particular hwp class over time 5 2 advancing uncertainty assessment in carbon accounting tools there are multiple sources of uncertainty when simulating the carbon balance at the regional level the current version of cat takes the sources of uncertainty related to the model into account since they come from the model itself the tool is fully dependent on the model implementation on this issue partial stochastic implementation would probably lead to underestimating the width of the confidence intervals and cat has no control over that simulating the errors in some parameters of cat increases the uncertainty associated with the predictions of some pools the confidence limits of the living biomass were greatly impacted by these additional sources of uncertainty as shown in fig 5a and b the degree of imprecision is not so high that nothing can be concluded here in fact the estimates of 2100 are correlated to those of 2015 because it can be assumed that these errors remain constant all along a particular realization or that if they change over time the variability is much smaller than the variability between the realizations an overestimated carbon stock in 2100 is likely to be overestimated in 2015 and vice versa when subtracting to obtain a realization wise difference the overestimation or underestimation of the value of 2100 is partly compensated for by that of the value in 2015 ståhl et al 2014 obtained similar results using static models to determine the change in biomass between two inventories in sweden and finland while the confidence limits are approximately 100 and 150 mg ha 1co2 eq around the mean estimates in 2015 and 2100 respectively fig 5b the confidence limits of the difference are 120 mg ha 1co2 eq around the mean estimate while cat takes these constant errors in the parameters across the carbon balance simulations into consideration the errors in the growth model components are completely out of reach and depend on the implementation the mathilde growth model was implemented so that for two growth projections the random errors in the parameter estimates and the disturbance occurrence are consistent across the projections and therefore the comparison between different forest management scenarios should be consistent in cat there is no guarantee that other models have this kind of implementation by default there are several sources of uncertainty that remain to be implemented in cat in the current version there is no uncertainty about the carbon emissions from the hwp life cycles although we did not find any assessment of the variability of these emission factors it can be reasonably assumed that they are quite variable we did not consider the uncertainty associated with fluxes between the different processors in the flux configuration in this case the implementation is probably more complicated since the sum of the outgoing fluxes from a particular processor has to be 100 as a consequence the random deviates from these outgoing fluxes are necessarily negatively correlated the multinomial distribution could provide these negatively correlated deviates mccullagh and nelder 1989 p 164 but this remains to be tested the uncertainty that stems from the sampling is not considered in cat either estimators that are both model and design based can take both sources of uncertainty into account some of these have been developed in contexts of carbon accounting in managed forests e g ståhl et al 2011 mcroberts et al 2016 but they are either incompatible with the monte carlo method or they tend to overestimate the variance a recent estimator based on parametric bootstrap fortin et al 2018 might prove compatible with built in cat uncertainty features while being unbiased in similar studies there seems to be a major confusion about the scale at which the uncertainty applies the parameters in table 1 are actually species or species group specific mean estimates like any statistical model a residual error exists which is the difference between the observed value and the mean predicted value at the most basic level which would be the tree level in this case the uncertainty arises from the error in the estimate of the mean as well as from the residual error however when scaling up to the landscape level the tree level residual errors compensate for each other so that their contribution to the uncertainty at the landscape level decreases and the error in the estimate of the mean usually becomes the major source of uncertainty apart from the sampling related variability breidenbach et al 2014 fortin et al 2016a b unfortunately the scales at which the uncertainty reported in most studies applies are often heterogeneous for instance njana et al 2016 reported an error margin at α 0 95 of less than 2 on the mean predicted basic wood density for three species whereas the tree level confidence envelope showed a variability of 33 we assumed an error margin of 20 around the basic wood density for the lorraine region as reported in table 1 and as did green et al 2006 for ireland under such an assumption the basic wood density of oak could be lower than that of fir while this can be true for two stands it cannot be true at the scale of a region or a country the same rationale applies for the carbon fractions and the biomass expansion factors for this reason the confidence intervals around the living biomass shown in fig 4 are probably too wide appropriate error margins for the regional and national levels remain to be defined but they will certainly be smaller than those reported in table 1 additional guidelines in this regard are needed valade et al 2017 carried out a literature review on the substitution factors of construction products and energy wood and found a large range of substitution coefficients as reported in table 1 they obtained an error margin of about 50 for energy wood however the uncertainty associated with construction products was larger they also found a log normal distribution for construction products whereas the substitution factors of energy wood were approximately gaussian although the log normal distribution could easily be implemented in cat simulating uncertainty using different error margins for the substitution factors of different hwp classes remains to be implemented for increasingly more complex life cycles and uncertainty constraints it is expected that monte carlo sampling may not necessarily be the most appropriate tool groen et al 2014 other uncertainty techniques like latin hypercube and quasi monte carlo sampling have been found to provide more accuracy and faster convergence in determining the sample mean than monte carlo sampling for this reason it would be interesting to use cat as a testing ground to compare these techniques and provide more targeted recommendations in the context of complex unfccc life cycles in any case cat users can already test and produce risk analyses under uncertainties in order to support potential adaptation plans operational tactical strategic or contingency that could reduce the carbon footprint across the entire forestry supply chain 5 3 how cat compares to other tools from the lulucf sector in the lulucf sector and particularly in the forestry sub sector brunet navarro et al 2016 listed 41 carbon accounting tools and highlighted 13 criteria seven structural 1 1 1 bucking allocation 2 fossil fuel substitution 3 industrial processes 4 carbon pools 5 product removal 6 recycling 7 versatility of allocation parameters and six end user 2 2 1 graphical user interface gui 2 the open sourcing of the code 3 user training opportunities 4 technical support service 5 user community and 6 regular model updates criteria that should be met to ensure the highest structural achievement and end user satisfaction none of the 41 tools mentioned in the review could achieve a perfect score for the 13 criteria even though some of them were part of the most advanced a group as per brunet navarro et al 2016 a former version of cat was listed in this a group under the name of capsis because it was an extension of the platform and not yet a standalone application when brunet navarro et al 2016 carried out their study the current version of cat counts among the most complete carbon accounting tools satisfying 12 of the 13 criteria even with the cot module from remsoft hennigar et al 2008 cameron et al 2013 the only criterion that was not met was that of versatility of allocation parameters in its current version cat considers that the flux configuration is constant over time it turns out that new policies are gradually implemented and things do not change drastically in one day a more realistic representation would be one where the flux configuration is initially set to the business as usual scenario and gradually changes to an alternative configuration over one or two decades we intend to implement this new feature in cat by allowing the user to define two flux configurations an initial configuration representing the current context and a target configuration with a time interval over which the implementation is gradual during the time required for the full implementation the biomass would be split with a linearly increasing proportion of the biomass being sent to the target configuration this remains to be developed and the current software structure makes it possible without major changes 5 4 toward a universal life cycle assessment tool for all the sectors in the last two decades the development and use of carbon accounting tools has gained in popularity in most emission sectors brunet navarro et al 2016 whittaker et al 2013 gentil et al 2010 laurent et al 2014 with various degrees of development and emphasis on complexity and uncertainties even if our focus was the lulucf sector the way cat deals with the representation of the cross sectoral life cycles is an interesting advance compared to carbon accounting tools used in other sectors whittaker et al 2013 gentil et al 2010 laurent et al 2014 policy makers base their decisions not only on carbon balance but also on other competing criteria which therefore need to be taken into account in models and software for instance in the lulucf sector the availability of certain wood products the supply of other essential chemical wood elements e g n p k used as indicators of forest health and of economically viable extractable wood compounds with pharmaceutical nutraceutic naturopathic cosmetic or industrial properties will need to be monitored and their life cycle evaluated the methodological difficulty here would be to simultaneously account for these different yet inextricably linked life cycles that also spread across various sectors this is the case for carbon as well as the extraction transformation and recycling of biochemical compounds whose life cycles include parts of the supply chain network in the forestry agriculture industrial and waste sectors all of this leads to a difficult representation of the structure of life cycles as well as of difficult trade offs between competing stakeholders involved in the carbon timber energy conservation recycling and extractable compound industries it will affect decision making when changing supply chains and life cycles will be about managing preferences between multiple criteria cat would need to evolve to become a more universal tool in order to evaluate the carbon balance for any type of sector and chemical compound to achieve this cat would require more than just applying simple optimization algorithms as has been done in remsofttm cot cameron et al 2013 cat capabilities would need to offer forest stakeholders smart interactibility between i the step of collectively defining the constraints and opportunities of changing their production lines and the way various hwp and compounds of interest can transit along them i e the constraint space and ii the step of expressing their competing preferences for different outcomes in terms of carbon wood products and biodiversity a k a the space of criteria cat has been designed with this type of interactivity in mind and could benefit from recent advances in interactive multi objective optimization algorithms from operations research see e g the interactive reference point multi objective method developed by dujardin et al 2015 altogether even with all its limitations this first version of cat demonstrates a new way to explore and evaluate options as well as to make decisions in order to adapt the supply chain of forest wood products to the new reality of the 2050 cop21 climate change targets and to the growing need to conserve the forest s ecological capital and to support industrial innovation 6 conclusions according to the recent international negotiation around climate change and with the cop21 paris agreement unfccc 2015 in particular unfccc members will have to report ghg emissions in managed forests and harvested wood products for the next commitment periods carbon accounting tools can prove to be valuable options for the assessment of ghg emission life cycles and for the support of structural decisions that can help decarbonize the unfccc sectors as the boundaries between the different sectors are opened forming more and more complex cross sectoral ghg emission life cycles existing carbon accounting tools and software become less adapted for representing this growing complexity and assessing the impact of numerous uncertainties risks of carbon leakage and double counting on the global carbon balance this paper showcases the new cat v1 0 software that counts among the most complete carbon accounting tools capable of supporting decisions to adapt the cross sectoral supply chains to the new social ecological reality and innovation needs cat particularly represents a unique and major advance in the way users can easily create modify compare and analyze scenarios of complex and uncertain ghg emission life cycles these scenarios can differ in terms of their level of uncertainties and network complexity when dealing with many lines of wood extraction production and transport use and transformation into energy and waste management even with its current limitations and focus on the forestry sector cat still represents a milestone for future more universal life cycle assessment tools that can deal with complex uncertainties complex supply chain structures and trade offs between a diversity of fluxes that need to be managed simultaneously between the unfccc sectors e g ghg emissions biomass of raw material biomolecular compounds of economic interest multiple wastes energy and economic fluxes acknowledgements many projects contributed to the development of cat the authors are thankful to the following funding agencies france forêt for a two year contract on carbon accounting agence de l environnement et de la maîtrise de l énergie ademe 1260c0055 through the reactiff i gesfor project the french national research agency anr 12 agro 0007 through the forwind project and the office national des forêts onf through the modelfor2 convention the authors are thankful to sylvain caurla umr lef and two anonymous reviewers for their constructive comments on a preliminary version of this paper special thanks are due to estelle vial fcba and jessica françois université de lorraine for their advice regarding the user interface of cat m f developed cat and r m developed the mathilde model j b p and m f designed the case studies j b p ran the simulations using cat and performed the analysis all authors contributed to the writing of the paper abbreviations bau business as usual capsis computer aided projections of strategies in sylviculture ce carbon emissions doc f degradable organic carbon fraction dom dead organic matter fwd fine wood debris ghg greenhouse gas hwp harvested wood products ipcc intergovernmental panel on climate change lb living biomass lulucf land use land use change and forestry nfi national forest inventory swds solid waste disposal site unfccc united nations framework convention on climate change appendix a supplementary data the following are the supplementary data related to this article data profile data profile supp mat supp mat appendix a supplementary data supplementary data related to this article can be found at https doi org 10 1016 j envsoft 2018 06 005 
26368,high quality data is of crucial importance for model development it provides a model input and is a prerequisite for model calibration and validation data reconciliation is often a very time consuming task so even when on line data is available the option is often chosen to synthetically generate data losing a lot of information contained in the available data this contribution showcases a python package that allows a streamlined work flow and provides possibilities for data analysis validation and gap filling with as main goals to use as much of the data as possible and to fill gaps in the data with a known reliability this provides a means towards more data use and a more sound calibration and validation while significantly reducing time spent on data reconciliation the package is published and made openly available on github this avoids multiple implementations while being accessible to the community for suggested improvements keywords data analysis data imputation online data software availability name of the software package wwdata developer chaïm de mulder biomath research unit faculty of bioscience engineering ghent university coupure links 653 9000 gent chaim demulder ugent be software required python additional packages required numpy scipy pandas matplotlib additional packages advised jupyter seaborn package size 53 9 mb current version v0 1 0 october 2017 availability available on github and pypi org license gnu gplv3 0 reference de mulder 2017 1 introduction wastewater treatment plants wwtps more recently called water and resource recovery facilities wrrfs have been improving the quality of human life for a long time and have enhanced the protection of nature and its resources worldwide the overall capital and operational costs associated with this treatment are high and efforts of different kinds are undertaken to lower them rodriguez garcia et al 2011 dürrenmatt and gujer 2012 one of these efforts resides in the field of modeling a mathematical representation of a wwtp indeed offers the possibility to test different scenarios for their ability to optimize operation and bring down costs while maintaining a satisfying effluent quality one of the most important but also most time consuming parts in the modeling process is the acquisition and reconciliation of high quality and quantity data hauduc et al 2009 without data no model giving reliable predictions can be built to describe a specific installation rieger et al 2012 the type of data that is needed depends on the modeling goal if a relatively simple steady state model is envisioned discrete data e g daily or hourly averages obtained with offline measurements will likely suffice however in a lot of other cases the intention is to capture the real dynamic behavior of an installation this implies that high frequency data in the order of minutes is necessary usually involving sensors that monitor the process online and send their signals to a supervisory control and data acquisition scada or equivalent system rieger et al 2012 this data containing a lot of information on the relevant dynamics is then used as model input as well as for calibration and validation cierkens et al 2012 although efforts are increasing to gather growing amounts of this type of high frequency data the appropriate analysis and reconciliation of it before use is key in avoiding data graveyards where unused data is virtually piling up in the process of obtaining a usable high frequency dataset some challenges can be distinguished firstly the availability of sufficient high frequency data is not a given in most cases operators do not have sensors for all necessary parameters installed in all necessary locations the obvious but not always easy option is to install sensors where needed preferably in the most optimal way villez et al 2016 and depending on the modeling goal secondly in the case where high frequency datasets are indeed available they are often incomplete noisy or scattered due to unavoidable circumstances linked to the hostile environment the sensors are placed in issues include sensor failure sensor drift or the necessity to calibrate sensors rieger et al 2010 this creates the problem of how to select what part of the data is useful and what part needs to be discarded often referred to as data filtering or data selection this issue of data selection in the context of waste water has already been widely discussed in the scientific literature yielding an array of possibilities to detect whether a data point is a trustworthy one or rather an anomaly of some kind some common methodologies include detection of constant or otherwise abnormal signals mass balancing visual assessment statistical tests principal component analysis pca and expert knowledge alferes and vanrolleghem 2014 rangeti et al 2015 alferes et al 2017 this data filtering results in a dataset with depending on the reason of the filtering small or large gaps in between validated data points in some cases the impact of these gaps on the modeling process is limited during calibration or validation for example there is always the option to compare the validated data points with the generated model output and simply ignore the discarded ones missing or faulty data becomes more of a problem if the purpose of the data is to be used as model input since in this case even a small number of missing or unreliable data points can already pose a problem with regards to prediction capability and stability of the model this problem was also the premise of a review by martin and vanrolleghem 2014 who investigated possible solutions for an engineer confronted with an incomplete dataset their literature search showed that several methods exist to generate influent datasets and profiles most of these are either based on a limited amount of data available or on extra knowledge available with regard to the catchment area e g amount of rainfall number of households industry types combined with a mathematical approach devisscher et al 2006 langergraber et al 2008 mannina et al 2011 gernaey et al 2011 such constructed datasets are in the majority of these cases then used throughout the rest of the modeling exercise however little attention has been paid to the situation where on line high frequency influent measurements are indeed available for some periods but are missing for others in that case only making use of a constructed dataset no matter how scientifically sound will cause the loss of information by omitting the actual data points that are available as stated by flores alsina et al 2014 and in rieger et al 2012 influent generators are promising tools not only to generate complete datasets but also to fill gaps due to missing data in influent profiles other options to fill gaps in datasets also known as imputation are also extensively mentioned in the literature but less extensively executed and compared proposed techniques vary from a simple imputation based on the dataset mean over the use of interpolation and relationships between measured parameters to curve fitting the use of smoothing artificial and recurrent neural networks ann rnn and hybrid evolutionary algorithms hea smits and baggelaar 2010 starrett et al 2010 kandasamy et al 2013 rangeti et al 2015 che et al 2016 especially ann rnn approaches have received attention in the literature although reported to have a lower reliability in case the amount of missing data exceeds 10 aitkenhead and coull 2013 kandasamy et al 2013 are to the authors knowledge the only ones actually comparing the filling techniques they describe based on the amount of gaps their algorithms were able to fill although a lot of work has clearly been done with regards to data selection and gap filling be it in the context of waste water or not all these efforts seem to remain unlinked to one another the entire process of going from a raw data set to a usable high frequency input data set is covered in literature but never in its entirety the possibility to go through this process in one single environment and with a transparently described work flow would in that sense allow for a more concise and repeatable data treatment that keeps all knowledge on a dataset in one place the code presented in this paper was obtained in the context of the smile project invoked by waterboard de dommel the netherlands since the waterboard has a multitude of sensors available in the influent and bioreactors of its largest wastewater treatment plant located in eindhoven the netherlands the modeling context of this paper is very comparable to the one described earlier in this introduction high frequency data is available but data series from the scada system are often incomplete noisy or scattered as is often the case a large amount of time in modeling of full scale wwtps goes to the analysis of this available data up to 28 of the whole effort hauduc et al 2009 to construct one or more usable influent datasets this turns the continuous calibration and validation of the available wwtp model for the installation in eindhoven into a task where a lot of time is spent on repeated actions for every new year long dataset this paper describes a python package that standardizes the data selection and gap filling of wwtp data and makes it reproducible in addition it allows to compare the reliability of the applied gap filling methodologies this enhances the easy production of full datasets of good quality in a significantly smaller amount of time than before which will in the end lead to an increased use of data both in a modelling and in a broader context 2 methods 2 1 data collection and pretreatment raw data for code testing was obtained from the wwtp of eindhoven the netherlands the plant receives wastewater from three major sewer systems riool zuid eindhoven stad and nuenen son the availability of influent measurements relevant for the model of the installation is shown in table 1 along with the measurement method after acquisition the data is stored in a database system ihistorian ge boston usa querying was performed by the waterboard after which the data was delivered as mat files i e the proprietary matlab format the relevant data series were extracted from these mat files and saved in csv files before applying any data analysis data was re sampled at a frequency of 5 min 2 2 the wwdata package the code for data selection and gap filling was written as a python python software foundation oregon usa package named wwdata de mulder 2017 the package available on github under a gnu gpl 3 0 i e open source license is under development but the authors agree that its functionalities are currently sufficiently elaborate for other researchers and practitioners to already make use of it before going into detail on the reasoning behind the package and the code itself it is important to mention that the wwdata package relies on several other python packages these include scipy numpy pandas and matplotlib see respectively scipy org numpy org pandas pydata org and matplotlib org as well as the jupyter notebook environment jupyter org during the development of the code a jupyter notebook v4 4 1 was continuously used for testing although the use of the package should be possible without the use of such a notebook this was not tested for and the use of a jupyter notebook along with the wwdata package is highly recommended because it offers a powerful environment for visualization and for creating transparency in the work flow for the sake of clarity the reader should be aware of the fact that the original premise of the wwdata package is to provide a package for data analysis in a much broader sense than only the data considered here i e online high frequency data also discrete data and sensor data obtained in lab experiments are within the scope of the package fig a1 in the appendix shows the structure of the whole package the labsensorbased and labexperimbased subclasses are out of scope of this paper the code and results described in the remainder of this paper concern the hydrodata superclass and the onlinesensorbased subclass the further description of the package given below follows the usual work flow when going from raw data to model input data reading and exploration data filtering and data reconciliation and or filling fig 1 features the more technical side of this work flow as it is used in the wwdata package the start of the work flow is to create a pandas dataframe containing the available data and make that into an onlinesensorbased class object further referred to as osb object so that all wwdata functionalities can be applied to it 2 2 1 data formatting data is usually delivered in a variety of ways and formats since the pandas package already offers an array of possibilities to read different data files and execute conversions no additional implementations were needed for this purpose for some functionalities the relevant pandas function was piped to be easily accessed from the osb object extra functionalities with regards to data formatting are limited to some small convenient features see appendix a 1 1 2 2 2 data exploration also in the context of data exploration a lot of functionalities are already present in the pandas and matplotlib packages respectively with regards to the calculation of data characteristics and visualization again some convenient functions were piped and integrated within the package see appendix a 1 2 2 2 3 data filtering one of the most important underlying principles of the wwdata package is to never delete data from a dataset but rather to tag it the calling of a filter function on the data therefore creates a pandas dataframe obj meta valid see also fig 1 parallel to the one containing the original data but now containing a tag indicating whether a data point is validated or not original vs filtered calling an additional filter function on the same data can either update the obj meta valid dataframe with additional filtered tags where appropriate or just tag the data points filtered by that specific function depending on user preference since all created dataframes are easily accessible the user can always override tags by adding or changing them manually the filter functions currently implemented are largely based on previous research alferes et al 2017 and include the filtering of nan values constant signals noise and outliers a more detailed description of the functionalities can be found in the appendix a 1 3 the implemented functionalities require several arguments indicating user preferences 1 the definition of what the user considers to be a constant signal noise outlier depending on his her experience with the data at hand examples of this will be given by means of a showcase 2 an argument stating what part of the data the filtering needs to be executed on i e the whole dataset or a specified subset 2 2 4 data filling once the non reliable data points are tagged they can be replaced in several ways two additional datasets are created parallel to the original one obj filled and obj meta filled respectively containing the combination of original and filled data points and tags indicating what filling function was used to replace a certain data point see also fig 1 the provided functions can replace data points by 1 interpolation linear or other 2 linear correlation with another measured value 3 using a calculated daily profile 4 using the data from the day before and 5 using modeled data more details about these methods can be found in the appendix a 1 4 at this point i e after filling in the gaps in the data there are now 4 parallel pandas dataframes see also fig 1 one containing the original data obj data one containing tags indicating whether a data point is validated or not obj meta valid one with tags stating what filling algorithm was used to fill a certain data point obj meta filled and a last one containing the dataset with a combination of original and filled data points obj filled it is this last one that will contain the final result to be used in model simulations 2 2 5 reliability of the filling algorithms in order to get an idea on the reliability of the proposed algorithms to fill in gaps in the data some additional functions are implemented the reliability testing procedure is as follows 1 gaps are randomly created in a specified part of the data the user can define the amount and maximum size of these gaps e g to allow for testing how good an algorithm is at specifically filling small or large gaps 2 the gaps are filled using a filling function whose arguments are specified by the user 3 the filled data points are compared with the value of the original data points and the average percentage difference between them is used as a measure to check how reliable the filling function is under the specified circumstances 4 since gap creation is done randomly steps 1 to 3 are iterated a number of user defined times and the average percentage deviation is given as a user output and saved it is important to execute these checks on a subset of the data that is largely reliable or at least does not contain large gaps if this is not the case step 3 will not produce the expected result in the sense that filled data will be compared with data that was not valid in the first place therefore giving no indication on the reliability of the gap filling algorithm for a proper understanding of the figures shown further on it is worth mentioning that for this reliability checking no visualization is currently implemented all graphs showing time series data relate to the filtering and filling of the example dataset and are not directly related to the checking of reliabilities 2 3 showcase specifics for the sake of reproducibility table 2 lists the function arguments as they were applied to obtain the results in section 3 i e for the case of may 2015 data from the wwtp in eindhoven if no arguments are mentioned in table 2 either the default arguments implemented at the time of publication were used or the function was applied to the month may in case of the arange argument the reasonings behind the function arguments and the function application in general can be listed as follows i iv filtering v viii filling ix xii reliability testing i rain events are defined as the datapoints in the total influent flow that exceed the 95 percentile calculated on the complete dataset ii a constant signal is one that for two subsequent data points differs less than 2 mgcod l or 0 05 mgn l in value for respectively codt and ammonium data iii noise is defined here as a signal that changes from its minimal value to half of its maximal value in a matter of seconds for example for codt a signal that changes from 0 to 1000 mg l in 10 s is too steep to be realistic this means that for a timespan of 5 min the frequency of the described data the slope between two consecutive data points can be a maximum of 1 1000 mg l 10 s 60 s 1 min 5 min 30000 mg l before the last of the two data points is removed this is still a very mild reasoning to use for filtering but since this is a function argument the user is always at liberty to change it iv outliers are in the described case defined as points that deviate more than 80 from the hourly moving average i e 12 data points v the maximum range for which interpolation can be used is 12 data points i e 1 h of data if more data is missing no interpolation is used vi the influent model used is the one described by langeveld et al 2017 it was made in the specific context of the eindhoven wwtp and predicts influent concentrations of a o ammonium and cod based on the influent flow to the installation the model was calibrated on influent data from the year 2015 and the outputs were used to fill gaps in the dataset at hand vii the average day used to fill gaps was calculated based on influent data from april 14th until may 1st 2015 which was a period with no peaks or other deviations from the expected dry weather pattern viii the correlation used to showcase the filling based on correlation is the one between codt and tss this was an arbitrary choice that was decided upon before the start of the procedure and might later not appear to be the best possible correlation basically any correlation can be customly used by the user instead ix with regards to the reliability testing the test data range argument was equal to april 1st to may 31st 2015 while the arange argument the data range for which the specified filling function was actually used was the range from april 15th to may 1st a test data range larger than the range of application of the filling function is necessary because for some filling algorithms e g based on correlation data outside of the data range being filled is needed e g to calculate a correlation x the reliability of the interpolation filling was tested only for smaller gaps in the data by creating 250 gaps with a maximum size of 12 data points i e 1 h of data yielding on average 1500 missing data points on the two week dataset of 4609 data points xi the other filling functions were tested for larger gaps 3 gaps with a maximum size of 1152 data points i e 4 days straight of missing data and yielding on average 1728 missing data points to test the function using data from the day before to fill the gap fill missing daybefore the maximum size of gaps to fill had to be adjusted to 0 12 days to avoid the situation where not all artificially created gaps are filled 3 gaps of maximum 4 days means that the filling function needs to be applicable up to 12 days of missing data xii the filling functions were tested for reliability in the context in which they were used in the current showcase the fill missing interpolation function was checked for both ammonium and codt data the fill missing model function was tested on ammonium data the other filling functions were tested for the codt data 3 results to demonstrate the wwdata package data from the wwtp in eindhoven as described in sections 2 1 and 2 3 has been used because visualization is both an important tool during data analysis and a crucial part of the wwdata package the reader will find that the results are mostly described by means of graphs placed in the relevant context additionally it is to be kept in mind that the purpose of these results is merely to showcase the possibilities of the presented package the reader will notice that some algorithms are not suitable for use in the presented case as will be shown by the reliability testing these algorithms have nevertheless been used to provide a concise showcase example 3 1 data exploration fig 2 shows the raw codt and ammonium data for may 2015 as well as the total influent flow during that period all taken from the eindhoven stad sewer influent typical features of online acquired data seem to be present noise outliers and periods of sensor failure also notice the large amount of whitespace in the ammonium data especially during the last two days of may this is because nan values i e missing data points cannot be plotted and therefore leave holes in the graph example output plots of the data exploration functions are shown in fig a2 the graphs in fig a2 are shown in exactly the same way as they are produced by the functions in the wwdata package 3 2 data filtering fig 3 shows the result of different filter functions applied to the ammonium data along with the user output given by the respective functions in addition to the plotted results 4601 nan values were also filtered these could not be plotted because they are nan values given that the filtering based on constant signal detection by far tags the most data points 1100 vs 1 vs 0 for respectively constant signal noise and outliers the aggregated result of the filtering yields a figure similar to the top graph in fig 3 the filtering result for codt data is excluded for the sake of brevity but can be inferred from fig 4 see further the number of filtered codt data points by each algorithm separately is 2016 2352 3 and 0 for filtering of respectively nan values constant signal values filtering of outliers based on the moving average and filtering of noise based on the slope between data points after applying the different filtering functions a total of 33 and 51 of original datapoints were retained for respectively ammonium and codt data it is noteworthy that within some large gaps there are still data points that remain untagged by any of the filter algorithms as mentioned in section 2 2 4 the wwdata package allows the user to replace all values not just the filtered ones within a certain time range which will be showcased in the following sections 3 3 reliability of the filling methods because this paper describes a showcase the filling algorithms to be used on the filtered data were established upfront to assure that all available algorithms could be represented for the ammonium data only interpolation and the available influent model are used to fill gaps in the codt data interpolation filling with the daily average filling with values from the previous day and filling based on correlation are used for each of these combinations of data and filling algorithm the reliability was calculated and results are shown in table 3 using the influent model to fill gaps in the data yields by far the highest reliability lowest percentage deviation all other filling methods have a relatively low reliability in the tested case as stated before these filling methods will nevertheless still be used in what follows for the mere sake of showcasing their use and the resulting visualization 3 4 data filling once the reliability of the filling algorithms has been determined they can be used to fill gaps in the data created by the filtering procedures 3 4 1 filling gaps by interpolating a first option when filling gaps in a dataset is to make use of interpolation of any preferred form the results for the codt and ammonium datasets when applying linear interpolation to fill gaps smaller than 1 h of data are shown in fig 4 clearly a large part of both datasets is filled making use of the assigned tags and the pandas groupby functionality this can be quantified 14 and 40 of total data points for respectively codt and ammonium data are filled by interpolation corresponding to 42 and 78 of filtered data points next to the linear interpolation option the pandas interpolate function used in the wwdata package offers other options as well some of these pchip akima krogh and barycentric have been tested shortly on a dataset different than the one showcased here some gave satisfying results e g pchip others did not for the sake of scope these options are not elaborated on here 3 4 2 filling gaps with modeled data the ammonium data is used to showcase the possibility of filling gaps with modeled values applying this type of filling to all gaps still left in the ammonium dataset at this point results in fig 5 it is noteworthy that for the large gap at the end of the dataset the option is chosen to replace all data points after may 26 so also the ones that were not tagged as filtered see figs 3 and 4 the first large gap occurring during dry weather shows a profile comparable to that of other dry weather days near the end of the dataset larger dips in modeled values are present and are due to the increased influent flow and thus likely rainy weather in that period with regards to the filling of smaller gaps it can be noticed that in some cases e g during the rain event on may 6th values are introduced that do not seem to be following the trend of the data nicely hereby also possibly introducing some sharp increase or decrease in the data 3 4 3 filling gaps with the average daily profile the first large gap in the codt data is filled based on the average daily profile calculated based on two weeks of data obtained in april 2015 fig 6 illustrates that the range of this daily average is correct although it is hard to verify the dynamics based only on the may data gap filling is stopped right before the rain event on may 19th 3 4 4 filling gaps based on data from the previous day the last large gap in the codt data is filled making use of the data on the day preceding the gap this is clearly visible on the right hand side of fig 6 in contrast with the filling based on modeled values fig 5 the impact of the increased influent flow near the end of may is not visible in the data of the day before the influent flow increases 3 4 5 filling gaps based on correlation the assumed correlation between codt and tss data is used to fill in the remaining gaps in the data as can be seen from fig a2 this correlation is in this case not very strong so also here the result serves more as a showcase than as an actual optimal result 3 4 6 comparison of distributions in order to get an idea of how the filtering and filling procedures change the data plots are made of the distributions of original filtered and filled data fig 7 the constant sensor signals visible in fig 2 can clearly be found in the distribution plots as well where they are represented by peaks in the distribution these are of course removed during the filtering while going from original to filtered to filled data the distributions do not seem to narrow down a lot and also the shape remains relatively similar after filling more data points are present 3 5 dataset size and calculation time as a last result a short mention is made of the dataset sizes the package was tested with so far and the calculation times usually encountered development of the package was done using datasets with an order of magnitude of 10e4 data points but the application on datasets with 10e5 data points one year of data at 5 min frequency was also tried yielding acceptable results the power of visualization is in such cases limited due to very dense plots the longest calculation times are those of the filling functions although calculation times for separate functions are never higher than a couple of minutes 4 discussion 4 1 data filtering from fig 3 and from the amount of data points tagged by the filter algorithms it seems that no outliers or noise are present in the considered dataset this is of course very unlikely and is due to the chosen filter function arguments as mentioned in section 2 3 these arguments are in the presented case very mild and exclude only the points that with near certainty are not valid the fact that this can easily be adjusted is considered one of the features providing the package with the necessary flexibility to handle a wide range of data apart from this remark the applied filter functions work well and data points that are not valid are correctly tagged the visualization option offers the possibility to quickly check the effect of each separate filter function and adjust arguments where necessary 4 2 reliability of the filling methods the absolute reliability values shown in table 3 seem to indicate that in the presented situation gap filling using modeled values would be the preferred option for the ammonium data while gap filling based on interpolation and on the average daily profile would be the way to go for respectively smaller and larger gaps in codt data two remarks are relevant in light of this result 1 although the differences in reliability seem to be linked to the measured parameter the authors can state that based on experience with other similar datasets this is not the case the reliability of the tested filling algorithms will not differ much between measured variables e g codt and nh4 obtained in the same context 2 the statement made at the beginning of this paragraph cannot be seen as conclusive the authors at this point will not give general guidelines about what filling algorithms to use for what data because they feel experience with testing the package on different kinds of data is still too limited to allow this in general uncertainties calculated as described in section 2 2 5 need to be approached with caution their calculation is based on the comparison between imputed and original data points these original data points are however also subject to uncertainty e g measurement error and cannot blindly be assumed to be exact the estimated uncertainties shown in table 3 thus need to be added to the uncertainty already inherent to the original data nonetheless the availability of an uncertainty estimate is very valuable especially with regards to the further modeling process knowledge of uncertainties in the model input is of great importance as it allows modelers to get a better idea about the uncertainties of the model output as well rieger et al 2010 4 3 data filling in general all filling algorithms seem to do what they were designed for in a satisfying way regardless of how reliable the filling method is gaps in the data are filled to yield a usable model input the large fraction of ammonium data that is filled by linear interpolation along with the large visual difference with the graph of the original data suggests that a lot of nan values are present for only a short period of time max 1 h in a row and are therefore filled by linear interpolation especially in such a case interpolation provides a very simple means to fill gaps in a dataset while avoiding sharp changes in subsequent data points possibly occurring when using other filling algorithms imagine for example replacing every other data point with modeled values that have the same dynamic trend as the original data but a small deviation this would induce high frequency fluctuations in the data like a saw pattern the reason why some other interpolation methods that were shortly tested did not do so well is most likely because piping between the function calls in the wwdata package and pandas scipy needs to be further optimized the comparison of the distributions of the original and filtered datasets proves that the filtering procedure improves the data in the sense that outliers or artifacts are removed and the distributions move more towards a normal or lognormal one which is representative of most wastewater data tchobanoglous et al 2004 additional filling only adds data points in this case with a known reliability without altering the distribution too much all the above does not necessarily mean that the filtering and or filling applied is the most appropriate one but it does provide a means for quickly double checking whether the procedures are introducing unwanted or unreliable information although all filling methods seem to work clearly not every method can be applied independent of the case be it due to practical issues or to logical reasoning consider the following four examples 1 in the case of eindhoven the sensor for codt cods and tss is the same so the measurements of all three parameters regularly fail at the same time although codt and cods often show a high correlation a large gap in codt data cannot be replaced making use of a correlation with cods data if that is not available either redundancy of the installed sensors can offer a solution in such a case villez et al 2016 2 following up on the use of correlations to fill gaps it is clear that the strength of the correlation plays a large role fig a2 and the results mentioned in section 3 1 indicate that in the presented case the correlation is in fact not sufficiently strong r2 0 3 to use it for the sake of showcasing that specific filling function this was still done here 3 if filling of data obtained during wet weather and or an increased influent flow is performed with an average daily profile or with data from the previous dry day it is likely to introduce a large error simply because those filling options do not make sense average or dry weather profiles are often very different from wet weather profiles for a wide range of measurements making use of load data instead of concentration data can in some cases avoid this problem as it is sometimes less susceptible to variations due to wet weather 4 some filling methods are better applied for small gaps than for large ones and vice versa in section 3 4 2 for example it is noticed that when smaller gaps are filled with modeled values the imputed data does not always follow the exact trend of the original data hereby introducing a sharp increase or decrease in the data as described in the first paragraph of this section to fill gaps of medium size i e between 1 h and 1 day the additional interpolation methods that were shortly mentioned e g pchip seem to provide an interesting alternative since they are developed to better follow trends in the data the maximum size of the gap that can be filled by these interpolation methods can be extended in short the wwdata package does not guarantee a good data cleaning and gap filling without the necessary user knowledge to apply its functionalities this can be very well considered as one of the strong suits of the package the lack of a generally applicable approach provides the user who knows his her data best with the flexibility to fine tune his her data filtering and or filling in the way s he sees fit 4 4 advantages and scientific relevance of the wwdata package although up to this point this contribution focused on the description of the wwdata package and its use in an example case this section will highlight the specific advantages and the scientific novelty of it 4 4 1 time gain one of the main goals of the wwdata package is to obtain a time gain when going from a raw dataset to one that can be used as model input especially for repeated actions this time gain is noticeably present a user familiar with the package can receive a completely new dataset explore that data visualize it and get a feel for it in one day determination of the appropriate filter parameters and checking the reliability of the available filling options is estimated to take a second day whereas a third and final day is sufficient to execute the filling and present a usable dataset the reason this is possible is the fact that easy to use functions are directly combined with visualization when a filtering procedure does not yield the expected result the user will immediately see this in one of the produced graphs be able to change the filter parameters and fastly asses the new result a traditional way of analyzing and preparing a dataset for modeling would easily take a month or more underpinning the relevance of this contribution for both academics and practitioners looking to make more use of the data available to them in an era of big data fast data analysis and reconciliation will help to avoid the buildup of data graveyards at utilities and companies see also section 4 4 3 furthermore a modeling effort is typically an iterative process requiring several cycles of calibration model updates and validation for some of these iterations new input data and thus a new data analysis filtering and filling is necessary in that case the package flexibility allows to very easily identify where parameters or filling methods might need to be changed as compared to the previous dataset s based on the authors experience a time reduction in the order of magnitude of weeks to even months is possible this way 4 4 2 work flow the work flow presented in this paper see also fig 1 and especially the fact that it is integrated within one single environment is another advantage there is no need for data scientists to adjust data in one software read it with another do some operations and write it out again only to use it in yet another software in addition to this the use of jupyter notebooks also increases efficiency transparency and user flexibility although the authors are aware that jupyter notebooks are not commonly applied yet and that many practitioners like to stick to their well known data processing environment often excel we would still like to make a case for their use the notebook user can execute the earlier mentioned repeated actions fast while staying in control and having in line plotting functionalities available to dig deeper into the data when needed or wanted moreover the jupyter notebook enviroment does allow the integration and use of other programming languages such as r and matlab offering the opportunity to combine the wwdata package with already available in house code thanks to the fact that a notebook can be considered a document with actual information on the why of certain actions it can also be seen as a memory or even an actual report serving very well when one wants to recheck how s he ended up with a specific dataset in addition to being very transparent when sharing procedures between different people 4 4 3 increased data use the increased use of original data throughout the undertaken modeling effort is likely the outcome of the presented research that is most scientifically relevant in essence the package allows an increased amount of real information to be fed to a model which will provide better options for a sound calibration and validation in the end leading to a model with a higher predictive power and hence a more solid basis for decision making additionally the reliability testing allows to make an informed decision on the applied filling algorithm making sure that the information added to the data while filling gaps has the best quality possible the same testing also allows to have an idea about what this quality actually is i e what the uncertainty of the model input is 4 4 4 semi automation with regards to the level of automation the wwdata package provides one could argue that more advanced automation is still possible and could provide an even larger time gain although this is acknowledged by the authors we are of the opinion that it is important and will in the end pay off to get a good feeling of ones data by handling it in a semi automated way as described here an additional effect of such an approach together with the visualization capabilities of the package is an improved communication with possible project utility partners an effect that in a current climate of increased interest in data use is not to be underestimated people at waterboard de dommel highly appreciated getting an insight into how their data was analyzed and what process led to the datasets that are used as input in the wwtp model 4 4 5 extendability finally because all data and all added tags are at any point in time accessible by the user s he can easily extend the current implementation with own data analysis code or algorithms for example data validation as presented in this contribution is solely based on the data itself in reality data validation is also linked to the sensors that were used and the environment and context of the measurement process bertrand krajewski et al 2003 the availability of the code allows the user to also take into account these factors hereby further encouraging them to increase their data use 5 conclusions and perspectives in conclusion the wwdata package can be considered a useful flexible and accessible tool to streamline data analysis and gap filling of relatively large and high frequency datasets in order to yield model input datasets ensure quality and reproducibility of this analysis and gap filling as a consequence of the structured integrated approach and the possibility to test for reliability save time while executing the analysis and gap filling preserve as much information as possible in the available datasets by not deleting but tagging data points improve communication on data analysis with possible collaboration partners some of the topics that are currently taken into consideration for further development see also the issues on the github repository github com ugentbiomath wwdata are the implementation of additional and more advanced filtering and filling algorithms based on literature findings bertrand krajewski et al 2003 alferes et al 2017 absent data is not necessarily faulty data the fact that data is absent e g because above or below detection limit provides information as well ways to use this information will be looked into collaboration on further development is warmly welcomed the reader is encouraged to create an issue on github com ugentbiomath wwdata to give suggestions acknowledgements waterboard de dommel supported the research described in this paper and provided all the data used for which the authors are very grateful stijn van hoey is gratefully acknowledged for his support in the initiation of this work the tips with regards to object oriented programming in python and his advice on documenting and publishing python packages the authors would also like to thank the reviewers of this paper for their relevant remarks and ideas for future development of the package appendix a 1 wwdata functionalities this section non exhaustively lists the several functionalities available within the wwdata package as well as some information on their implementation and use for more detailed information the reader is referred to the online package documentation ugentbiomath github io wwdata docs a 1 1 data formatting functionalities the possibility to indicate that the index of the onlinesensorbased object is containing time data this is also important for further operations on the data in case index values are missing a function is implemented to insert them assuming equidistant data conversion of absolute time values i e date and hour to relative time values some models for example cannot handle date values as an input and need relative time values i e float or integer values the possibility to add units data types or tags to the class object upon creating it so that the user can always recheck this information while handling the data a 1 2 data exploration functionalities the checking for a linear correlation between data series yielding the linear regression coefficients and the r2 value of the correlation the computation of an average daily profile as well as the standard deviation and upper and lower quantiles around it this average profile can be calculated based on the whole dataset or on a subset of it depending on user preference the calculation of daily averages including the standard deviation on the average the tagging of rain events based on what the user defines to be a rain event this is currently a simple tagging of values higher than a certain value or percentile this tagging will become important further on when filling gaps in the data section 2 2 4 the above mentioned functions all include the option to plot their impact see examples in appendix fig a2 a 1 3 data filtering functionalities filtering not a number nan values these are detected and a tag is added in obj meta valid filtering values that are physically not possible values above or below a user defined upper or lower bound receive a tag in obj meta valid filtering values that are part of a constant signal indicating sensor failure subsequent data points having the same value or a value within a certain user defined bound within which a signal is assumed to be constant are tagged in obj meta valid filtering values based on the difference between two consecutive points if a value for example changes from a very low to a very high value in a short amount of time it is likely that it is a data point prone to noise i e a data point that does not represent the true value of the measured parameter due to unwanted and usually unknown interferences during signal processing this is checked based on the slope between two consecutive values where the last value of the two is tagged if this slope value is too high filtering values based on the difference with a smooth representation of the dataset for example a data point with a deviation of more than 90 from the moving average calculated with a certain window can be considered an outlier i e a data point that due to its large deviation from other data points can not be considered statistically relevant within the dataset such data points are also tagged in obj meta valid also here there is the option to plot the result of a filtering function see figures in section 3 2 next to the possibility to plot all filter functions give a user output stating what the initial amount of data points was and how many of those were filtered based on that filter function when choosing the plotting option user output also indicates the total percentage of data points left after filtering a 1 4 data filling functionalities interpolation linear or other as long as the consecutive amount of missing data points is limited limited being defined by the user see also showcase description in section 2 3 and table 2 interpolation can be a easy and sound way to fill gaps although in the rest of this contribution linear interpolation is used the implementation makes use of the pandas series interpolate function so any interpolation algorithm as implemented there should can be used linear correlation with another measured value see also section a 1 2 when for example cod data points are missing during a certain time period a successful measurement of tss along with an established correlation between the two can be used to replace some of the missing data using a calculated daily profile see also section a 1 2 if the diurnal pattern of the data shows a low enough variability this pattern can be used to fill some larger gaps in a dataset using the data from the day before if weather and or influent flow dynamics stay stable during a certain period it is sound to assume that conditions during two consecutive days are similar and the data from the one day can be used to fill in data from the next where data is missing the maximum length of the period to apply this algorithm for is a user defined function argument the optimal value depending on the specific data set the reliability testing as described and discussed in respectively sections 2 2 5 and 4 2 offers a tool to determine this optimum using modeled data as discussed in section 1 influent models or generators are well described in literature and often give good results the output from these models generators can be used to fill large gaps in a dataset once again function arguments include the possibility to plot results in this case yielding a plot of the filled dataset where the data points filled by a certain function have a different color see plots in section 3 4 in case the onlinesensorbased object has the tag wwtp assigned to it every filling function also gives a user warning when replacing data obtained during a rain event as defined before see section a 1 2 this is to make the user aware of the fact that the filling algorithm might not work as expected when replacing data obtained during a rain event additionally the user has the option to replace either data points tagged as filtered or all data points within a certain range a2 figures fig a 1 simple representation of the wwdata package structure the hydrodata superclass contains three subclasses each with their own functions specifically aimed at a certain type of data the onlinesensorbased class is currently the most developed one fig a 1 fig a 2 example output of the data exploration functions in the wwdata package top the tagging of rain weather flows middle daily average and standard deviation on the average of ammonium measurements bottom left the average daily profile of ammonium including the upper and lower 90 quantile lines bottom right scatter plot and linear correlation between codt and total suspended solids tss in the influent of eindhoven stad units are m3 h for the flow and respectively mg l and mgn l for cod tss and ammonium additional output of the get correlation function includes correlation coefficients and the r2 value in the presented case i e linear correlation between codt and tss data from the eindhoven stad sewer influent these are 0 35 slope 747 intercept and 0 3 r2 fig a 2 
26368,high quality data is of crucial importance for model development it provides a model input and is a prerequisite for model calibration and validation data reconciliation is often a very time consuming task so even when on line data is available the option is often chosen to synthetically generate data losing a lot of information contained in the available data this contribution showcases a python package that allows a streamlined work flow and provides possibilities for data analysis validation and gap filling with as main goals to use as much of the data as possible and to fill gaps in the data with a known reliability this provides a means towards more data use and a more sound calibration and validation while significantly reducing time spent on data reconciliation the package is published and made openly available on github this avoids multiple implementations while being accessible to the community for suggested improvements keywords data analysis data imputation online data software availability name of the software package wwdata developer chaïm de mulder biomath research unit faculty of bioscience engineering ghent university coupure links 653 9000 gent chaim demulder ugent be software required python additional packages required numpy scipy pandas matplotlib additional packages advised jupyter seaborn package size 53 9 mb current version v0 1 0 october 2017 availability available on github and pypi org license gnu gplv3 0 reference de mulder 2017 1 introduction wastewater treatment plants wwtps more recently called water and resource recovery facilities wrrfs have been improving the quality of human life for a long time and have enhanced the protection of nature and its resources worldwide the overall capital and operational costs associated with this treatment are high and efforts of different kinds are undertaken to lower them rodriguez garcia et al 2011 dürrenmatt and gujer 2012 one of these efforts resides in the field of modeling a mathematical representation of a wwtp indeed offers the possibility to test different scenarios for their ability to optimize operation and bring down costs while maintaining a satisfying effluent quality one of the most important but also most time consuming parts in the modeling process is the acquisition and reconciliation of high quality and quantity data hauduc et al 2009 without data no model giving reliable predictions can be built to describe a specific installation rieger et al 2012 the type of data that is needed depends on the modeling goal if a relatively simple steady state model is envisioned discrete data e g daily or hourly averages obtained with offline measurements will likely suffice however in a lot of other cases the intention is to capture the real dynamic behavior of an installation this implies that high frequency data in the order of minutes is necessary usually involving sensors that monitor the process online and send their signals to a supervisory control and data acquisition scada or equivalent system rieger et al 2012 this data containing a lot of information on the relevant dynamics is then used as model input as well as for calibration and validation cierkens et al 2012 although efforts are increasing to gather growing amounts of this type of high frequency data the appropriate analysis and reconciliation of it before use is key in avoiding data graveyards where unused data is virtually piling up in the process of obtaining a usable high frequency dataset some challenges can be distinguished firstly the availability of sufficient high frequency data is not a given in most cases operators do not have sensors for all necessary parameters installed in all necessary locations the obvious but not always easy option is to install sensors where needed preferably in the most optimal way villez et al 2016 and depending on the modeling goal secondly in the case where high frequency datasets are indeed available they are often incomplete noisy or scattered due to unavoidable circumstances linked to the hostile environment the sensors are placed in issues include sensor failure sensor drift or the necessity to calibrate sensors rieger et al 2010 this creates the problem of how to select what part of the data is useful and what part needs to be discarded often referred to as data filtering or data selection this issue of data selection in the context of waste water has already been widely discussed in the scientific literature yielding an array of possibilities to detect whether a data point is a trustworthy one or rather an anomaly of some kind some common methodologies include detection of constant or otherwise abnormal signals mass balancing visual assessment statistical tests principal component analysis pca and expert knowledge alferes and vanrolleghem 2014 rangeti et al 2015 alferes et al 2017 this data filtering results in a dataset with depending on the reason of the filtering small or large gaps in between validated data points in some cases the impact of these gaps on the modeling process is limited during calibration or validation for example there is always the option to compare the validated data points with the generated model output and simply ignore the discarded ones missing or faulty data becomes more of a problem if the purpose of the data is to be used as model input since in this case even a small number of missing or unreliable data points can already pose a problem with regards to prediction capability and stability of the model this problem was also the premise of a review by martin and vanrolleghem 2014 who investigated possible solutions for an engineer confronted with an incomplete dataset their literature search showed that several methods exist to generate influent datasets and profiles most of these are either based on a limited amount of data available or on extra knowledge available with regard to the catchment area e g amount of rainfall number of households industry types combined with a mathematical approach devisscher et al 2006 langergraber et al 2008 mannina et al 2011 gernaey et al 2011 such constructed datasets are in the majority of these cases then used throughout the rest of the modeling exercise however little attention has been paid to the situation where on line high frequency influent measurements are indeed available for some periods but are missing for others in that case only making use of a constructed dataset no matter how scientifically sound will cause the loss of information by omitting the actual data points that are available as stated by flores alsina et al 2014 and in rieger et al 2012 influent generators are promising tools not only to generate complete datasets but also to fill gaps due to missing data in influent profiles other options to fill gaps in datasets also known as imputation are also extensively mentioned in the literature but less extensively executed and compared proposed techniques vary from a simple imputation based on the dataset mean over the use of interpolation and relationships between measured parameters to curve fitting the use of smoothing artificial and recurrent neural networks ann rnn and hybrid evolutionary algorithms hea smits and baggelaar 2010 starrett et al 2010 kandasamy et al 2013 rangeti et al 2015 che et al 2016 especially ann rnn approaches have received attention in the literature although reported to have a lower reliability in case the amount of missing data exceeds 10 aitkenhead and coull 2013 kandasamy et al 2013 are to the authors knowledge the only ones actually comparing the filling techniques they describe based on the amount of gaps their algorithms were able to fill although a lot of work has clearly been done with regards to data selection and gap filling be it in the context of waste water or not all these efforts seem to remain unlinked to one another the entire process of going from a raw data set to a usable high frequency input data set is covered in literature but never in its entirety the possibility to go through this process in one single environment and with a transparently described work flow would in that sense allow for a more concise and repeatable data treatment that keeps all knowledge on a dataset in one place the code presented in this paper was obtained in the context of the smile project invoked by waterboard de dommel the netherlands since the waterboard has a multitude of sensors available in the influent and bioreactors of its largest wastewater treatment plant located in eindhoven the netherlands the modeling context of this paper is very comparable to the one described earlier in this introduction high frequency data is available but data series from the scada system are often incomplete noisy or scattered as is often the case a large amount of time in modeling of full scale wwtps goes to the analysis of this available data up to 28 of the whole effort hauduc et al 2009 to construct one or more usable influent datasets this turns the continuous calibration and validation of the available wwtp model for the installation in eindhoven into a task where a lot of time is spent on repeated actions for every new year long dataset this paper describes a python package that standardizes the data selection and gap filling of wwtp data and makes it reproducible in addition it allows to compare the reliability of the applied gap filling methodologies this enhances the easy production of full datasets of good quality in a significantly smaller amount of time than before which will in the end lead to an increased use of data both in a modelling and in a broader context 2 methods 2 1 data collection and pretreatment raw data for code testing was obtained from the wwtp of eindhoven the netherlands the plant receives wastewater from three major sewer systems riool zuid eindhoven stad and nuenen son the availability of influent measurements relevant for the model of the installation is shown in table 1 along with the measurement method after acquisition the data is stored in a database system ihistorian ge boston usa querying was performed by the waterboard after which the data was delivered as mat files i e the proprietary matlab format the relevant data series were extracted from these mat files and saved in csv files before applying any data analysis data was re sampled at a frequency of 5 min 2 2 the wwdata package the code for data selection and gap filling was written as a python python software foundation oregon usa package named wwdata de mulder 2017 the package available on github under a gnu gpl 3 0 i e open source license is under development but the authors agree that its functionalities are currently sufficiently elaborate for other researchers and practitioners to already make use of it before going into detail on the reasoning behind the package and the code itself it is important to mention that the wwdata package relies on several other python packages these include scipy numpy pandas and matplotlib see respectively scipy org numpy org pandas pydata org and matplotlib org as well as the jupyter notebook environment jupyter org during the development of the code a jupyter notebook v4 4 1 was continuously used for testing although the use of the package should be possible without the use of such a notebook this was not tested for and the use of a jupyter notebook along with the wwdata package is highly recommended because it offers a powerful environment for visualization and for creating transparency in the work flow for the sake of clarity the reader should be aware of the fact that the original premise of the wwdata package is to provide a package for data analysis in a much broader sense than only the data considered here i e online high frequency data also discrete data and sensor data obtained in lab experiments are within the scope of the package fig a1 in the appendix shows the structure of the whole package the labsensorbased and labexperimbased subclasses are out of scope of this paper the code and results described in the remainder of this paper concern the hydrodata superclass and the onlinesensorbased subclass the further description of the package given below follows the usual work flow when going from raw data to model input data reading and exploration data filtering and data reconciliation and or filling fig 1 features the more technical side of this work flow as it is used in the wwdata package the start of the work flow is to create a pandas dataframe containing the available data and make that into an onlinesensorbased class object further referred to as osb object so that all wwdata functionalities can be applied to it 2 2 1 data formatting data is usually delivered in a variety of ways and formats since the pandas package already offers an array of possibilities to read different data files and execute conversions no additional implementations were needed for this purpose for some functionalities the relevant pandas function was piped to be easily accessed from the osb object extra functionalities with regards to data formatting are limited to some small convenient features see appendix a 1 1 2 2 2 data exploration also in the context of data exploration a lot of functionalities are already present in the pandas and matplotlib packages respectively with regards to the calculation of data characteristics and visualization again some convenient functions were piped and integrated within the package see appendix a 1 2 2 2 3 data filtering one of the most important underlying principles of the wwdata package is to never delete data from a dataset but rather to tag it the calling of a filter function on the data therefore creates a pandas dataframe obj meta valid see also fig 1 parallel to the one containing the original data but now containing a tag indicating whether a data point is validated or not original vs filtered calling an additional filter function on the same data can either update the obj meta valid dataframe with additional filtered tags where appropriate or just tag the data points filtered by that specific function depending on user preference since all created dataframes are easily accessible the user can always override tags by adding or changing them manually the filter functions currently implemented are largely based on previous research alferes et al 2017 and include the filtering of nan values constant signals noise and outliers a more detailed description of the functionalities can be found in the appendix a 1 3 the implemented functionalities require several arguments indicating user preferences 1 the definition of what the user considers to be a constant signal noise outlier depending on his her experience with the data at hand examples of this will be given by means of a showcase 2 an argument stating what part of the data the filtering needs to be executed on i e the whole dataset or a specified subset 2 2 4 data filling once the non reliable data points are tagged they can be replaced in several ways two additional datasets are created parallel to the original one obj filled and obj meta filled respectively containing the combination of original and filled data points and tags indicating what filling function was used to replace a certain data point see also fig 1 the provided functions can replace data points by 1 interpolation linear or other 2 linear correlation with another measured value 3 using a calculated daily profile 4 using the data from the day before and 5 using modeled data more details about these methods can be found in the appendix a 1 4 at this point i e after filling in the gaps in the data there are now 4 parallel pandas dataframes see also fig 1 one containing the original data obj data one containing tags indicating whether a data point is validated or not obj meta valid one with tags stating what filling algorithm was used to fill a certain data point obj meta filled and a last one containing the dataset with a combination of original and filled data points obj filled it is this last one that will contain the final result to be used in model simulations 2 2 5 reliability of the filling algorithms in order to get an idea on the reliability of the proposed algorithms to fill in gaps in the data some additional functions are implemented the reliability testing procedure is as follows 1 gaps are randomly created in a specified part of the data the user can define the amount and maximum size of these gaps e g to allow for testing how good an algorithm is at specifically filling small or large gaps 2 the gaps are filled using a filling function whose arguments are specified by the user 3 the filled data points are compared with the value of the original data points and the average percentage difference between them is used as a measure to check how reliable the filling function is under the specified circumstances 4 since gap creation is done randomly steps 1 to 3 are iterated a number of user defined times and the average percentage deviation is given as a user output and saved it is important to execute these checks on a subset of the data that is largely reliable or at least does not contain large gaps if this is not the case step 3 will not produce the expected result in the sense that filled data will be compared with data that was not valid in the first place therefore giving no indication on the reliability of the gap filling algorithm for a proper understanding of the figures shown further on it is worth mentioning that for this reliability checking no visualization is currently implemented all graphs showing time series data relate to the filtering and filling of the example dataset and are not directly related to the checking of reliabilities 2 3 showcase specifics for the sake of reproducibility table 2 lists the function arguments as they were applied to obtain the results in section 3 i e for the case of may 2015 data from the wwtp in eindhoven if no arguments are mentioned in table 2 either the default arguments implemented at the time of publication were used or the function was applied to the month may in case of the arange argument the reasonings behind the function arguments and the function application in general can be listed as follows i iv filtering v viii filling ix xii reliability testing i rain events are defined as the datapoints in the total influent flow that exceed the 95 percentile calculated on the complete dataset ii a constant signal is one that for two subsequent data points differs less than 2 mgcod l or 0 05 mgn l in value for respectively codt and ammonium data iii noise is defined here as a signal that changes from its minimal value to half of its maximal value in a matter of seconds for example for codt a signal that changes from 0 to 1000 mg l in 10 s is too steep to be realistic this means that for a timespan of 5 min the frequency of the described data the slope between two consecutive data points can be a maximum of 1 1000 mg l 10 s 60 s 1 min 5 min 30000 mg l before the last of the two data points is removed this is still a very mild reasoning to use for filtering but since this is a function argument the user is always at liberty to change it iv outliers are in the described case defined as points that deviate more than 80 from the hourly moving average i e 12 data points v the maximum range for which interpolation can be used is 12 data points i e 1 h of data if more data is missing no interpolation is used vi the influent model used is the one described by langeveld et al 2017 it was made in the specific context of the eindhoven wwtp and predicts influent concentrations of a o ammonium and cod based on the influent flow to the installation the model was calibrated on influent data from the year 2015 and the outputs were used to fill gaps in the dataset at hand vii the average day used to fill gaps was calculated based on influent data from april 14th until may 1st 2015 which was a period with no peaks or other deviations from the expected dry weather pattern viii the correlation used to showcase the filling based on correlation is the one between codt and tss this was an arbitrary choice that was decided upon before the start of the procedure and might later not appear to be the best possible correlation basically any correlation can be customly used by the user instead ix with regards to the reliability testing the test data range argument was equal to april 1st to may 31st 2015 while the arange argument the data range for which the specified filling function was actually used was the range from april 15th to may 1st a test data range larger than the range of application of the filling function is necessary because for some filling algorithms e g based on correlation data outside of the data range being filled is needed e g to calculate a correlation x the reliability of the interpolation filling was tested only for smaller gaps in the data by creating 250 gaps with a maximum size of 12 data points i e 1 h of data yielding on average 1500 missing data points on the two week dataset of 4609 data points xi the other filling functions were tested for larger gaps 3 gaps with a maximum size of 1152 data points i e 4 days straight of missing data and yielding on average 1728 missing data points to test the function using data from the day before to fill the gap fill missing daybefore the maximum size of gaps to fill had to be adjusted to 0 12 days to avoid the situation where not all artificially created gaps are filled 3 gaps of maximum 4 days means that the filling function needs to be applicable up to 12 days of missing data xii the filling functions were tested for reliability in the context in which they were used in the current showcase the fill missing interpolation function was checked for both ammonium and codt data the fill missing model function was tested on ammonium data the other filling functions were tested for the codt data 3 results to demonstrate the wwdata package data from the wwtp in eindhoven as described in sections 2 1 and 2 3 has been used because visualization is both an important tool during data analysis and a crucial part of the wwdata package the reader will find that the results are mostly described by means of graphs placed in the relevant context additionally it is to be kept in mind that the purpose of these results is merely to showcase the possibilities of the presented package the reader will notice that some algorithms are not suitable for use in the presented case as will be shown by the reliability testing these algorithms have nevertheless been used to provide a concise showcase example 3 1 data exploration fig 2 shows the raw codt and ammonium data for may 2015 as well as the total influent flow during that period all taken from the eindhoven stad sewer influent typical features of online acquired data seem to be present noise outliers and periods of sensor failure also notice the large amount of whitespace in the ammonium data especially during the last two days of may this is because nan values i e missing data points cannot be plotted and therefore leave holes in the graph example output plots of the data exploration functions are shown in fig a2 the graphs in fig a2 are shown in exactly the same way as they are produced by the functions in the wwdata package 3 2 data filtering fig 3 shows the result of different filter functions applied to the ammonium data along with the user output given by the respective functions in addition to the plotted results 4601 nan values were also filtered these could not be plotted because they are nan values given that the filtering based on constant signal detection by far tags the most data points 1100 vs 1 vs 0 for respectively constant signal noise and outliers the aggregated result of the filtering yields a figure similar to the top graph in fig 3 the filtering result for codt data is excluded for the sake of brevity but can be inferred from fig 4 see further the number of filtered codt data points by each algorithm separately is 2016 2352 3 and 0 for filtering of respectively nan values constant signal values filtering of outliers based on the moving average and filtering of noise based on the slope between data points after applying the different filtering functions a total of 33 and 51 of original datapoints were retained for respectively ammonium and codt data it is noteworthy that within some large gaps there are still data points that remain untagged by any of the filter algorithms as mentioned in section 2 2 4 the wwdata package allows the user to replace all values not just the filtered ones within a certain time range which will be showcased in the following sections 3 3 reliability of the filling methods because this paper describes a showcase the filling algorithms to be used on the filtered data were established upfront to assure that all available algorithms could be represented for the ammonium data only interpolation and the available influent model are used to fill gaps in the codt data interpolation filling with the daily average filling with values from the previous day and filling based on correlation are used for each of these combinations of data and filling algorithm the reliability was calculated and results are shown in table 3 using the influent model to fill gaps in the data yields by far the highest reliability lowest percentage deviation all other filling methods have a relatively low reliability in the tested case as stated before these filling methods will nevertheless still be used in what follows for the mere sake of showcasing their use and the resulting visualization 3 4 data filling once the reliability of the filling algorithms has been determined they can be used to fill gaps in the data created by the filtering procedures 3 4 1 filling gaps by interpolating a first option when filling gaps in a dataset is to make use of interpolation of any preferred form the results for the codt and ammonium datasets when applying linear interpolation to fill gaps smaller than 1 h of data are shown in fig 4 clearly a large part of both datasets is filled making use of the assigned tags and the pandas groupby functionality this can be quantified 14 and 40 of total data points for respectively codt and ammonium data are filled by interpolation corresponding to 42 and 78 of filtered data points next to the linear interpolation option the pandas interpolate function used in the wwdata package offers other options as well some of these pchip akima krogh and barycentric have been tested shortly on a dataset different than the one showcased here some gave satisfying results e g pchip others did not for the sake of scope these options are not elaborated on here 3 4 2 filling gaps with modeled data the ammonium data is used to showcase the possibility of filling gaps with modeled values applying this type of filling to all gaps still left in the ammonium dataset at this point results in fig 5 it is noteworthy that for the large gap at the end of the dataset the option is chosen to replace all data points after may 26 so also the ones that were not tagged as filtered see figs 3 and 4 the first large gap occurring during dry weather shows a profile comparable to that of other dry weather days near the end of the dataset larger dips in modeled values are present and are due to the increased influent flow and thus likely rainy weather in that period with regards to the filling of smaller gaps it can be noticed that in some cases e g during the rain event on may 6th values are introduced that do not seem to be following the trend of the data nicely hereby also possibly introducing some sharp increase or decrease in the data 3 4 3 filling gaps with the average daily profile the first large gap in the codt data is filled based on the average daily profile calculated based on two weeks of data obtained in april 2015 fig 6 illustrates that the range of this daily average is correct although it is hard to verify the dynamics based only on the may data gap filling is stopped right before the rain event on may 19th 3 4 4 filling gaps based on data from the previous day the last large gap in the codt data is filled making use of the data on the day preceding the gap this is clearly visible on the right hand side of fig 6 in contrast with the filling based on modeled values fig 5 the impact of the increased influent flow near the end of may is not visible in the data of the day before the influent flow increases 3 4 5 filling gaps based on correlation the assumed correlation between codt and tss data is used to fill in the remaining gaps in the data as can be seen from fig a2 this correlation is in this case not very strong so also here the result serves more as a showcase than as an actual optimal result 3 4 6 comparison of distributions in order to get an idea of how the filtering and filling procedures change the data plots are made of the distributions of original filtered and filled data fig 7 the constant sensor signals visible in fig 2 can clearly be found in the distribution plots as well where they are represented by peaks in the distribution these are of course removed during the filtering while going from original to filtered to filled data the distributions do not seem to narrow down a lot and also the shape remains relatively similar after filling more data points are present 3 5 dataset size and calculation time as a last result a short mention is made of the dataset sizes the package was tested with so far and the calculation times usually encountered development of the package was done using datasets with an order of magnitude of 10e4 data points but the application on datasets with 10e5 data points one year of data at 5 min frequency was also tried yielding acceptable results the power of visualization is in such cases limited due to very dense plots the longest calculation times are those of the filling functions although calculation times for separate functions are never higher than a couple of minutes 4 discussion 4 1 data filtering from fig 3 and from the amount of data points tagged by the filter algorithms it seems that no outliers or noise are present in the considered dataset this is of course very unlikely and is due to the chosen filter function arguments as mentioned in section 2 3 these arguments are in the presented case very mild and exclude only the points that with near certainty are not valid the fact that this can easily be adjusted is considered one of the features providing the package with the necessary flexibility to handle a wide range of data apart from this remark the applied filter functions work well and data points that are not valid are correctly tagged the visualization option offers the possibility to quickly check the effect of each separate filter function and adjust arguments where necessary 4 2 reliability of the filling methods the absolute reliability values shown in table 3 seem to indicate that in the presented situation gap filling using modeled values would be the preferred option for the ammonium data while gap filling based on interpolation and on the average daily profile would be the way to go for respectively smaller and larger gaps in codt data two remarks are relevant in light of this result 1 although the differences in reliability seem to be linked to the measured parameter the authors can state that based on experience with other similar datasets this is not the case the reliability of the tested filling algorithms will not differ much between measured variables e g codt and nh4 obtained in the same context 2 the statement made at the beginning of this paragraph cannot be seen as conclusive the authors at this point will not give general guidelines about what filling algorithms to use for what data because they feel experience with testing the package on different kinds of data is still too limited to allow this in general uncertainties calculated as described in section 2 2 5 need to be approached with caution their calculation is based on the comparison between imputed and original data points these original data points are however also subject to uncertainty e g measurement error and cannot blindly be assumed to be exact the estimated uncertainties shown in table 3 thus need to be added to the uncertainty already inherent to the original data nonetheless the availability of an uncertainty estimate is very valuable especially with regards to the further modeling process knowledge of uncertainties in the model input is of great importance as it allows modelers to get a better idea about the uncertainties of the model output as well rieger et al 2010 4 3 data filling in general all filling algorithms seem to do what they were designed for in a satisfying way regardless of how reliable the filling method is gaps in the data are filled to yield a usable model input the large fraction of ammonium data that is filled by linear interpolation along with the large visual difference with the graph of the original data suggests that a lot of nan values are present for only a short period of time max 1 h in a row and are therefore filled by linear interpolation especially in such a case interpolation provides a very simple means to fill gaps in a dataset while avoiding sharp changes in subsequent data points possibly occurring when using other filling algorithms imagine for example replacing every other data point with modeled values that have the same dynamic trend as the original data but a small deviation this would induce high frequency fluctuations in the data like a saw pattern the reason why some other interpolation methods that were shortly tested did not do so well is most likely because piping between the function calls in the wwdata package and pandas scipy needs to be further optimized the comparison of the distributions of the original and filtered datasets proves that the filtering procedure improves the data in the sense that outliers or artifacts are removed and the distributions move more towards a normal or lognormal one which is representative of most wastewater data tchobanoglous et al 2004 additional filling only adds data points in this case with a known reliability without altering the distribution too much all the above does not necessarily mean that the filtering and or filling applied is the most appropriate one but it does provide a means for quickly double checking whether the procedures are introducing unwanted or unreliable information although all filling methods seem to work clearly not every method can be applied independent of the case be it due to practical issues or to logical reasoning consider the following four examples 1 in the case of eindhoven the sensor for codt cods and tss is the same so the measurements of all three parameters regularly fail at the same time although codt and cods often show a high correlation a large gap in codt data cannot be replaced making use of a correlation with cods data if that is not available either redundancy of the installed sensors can offer a solution in such a case villez et al 2016 2 following up on the use of correlations to fill gaps it is clear that the strength of the correlation plays a large role fig a2 and the results mentioned in section 3 1 indicate that in the presented case the correlation is in fact not sufficiently strong r2 0 3 to use it for the sake of showcasing that specific filling function this was still done here 3 if filling of data obtained during wet weather and or an increased influent flow is performed with an average daily profile or with data from the previous dry day it is likely to introduce a large error simply because those filling options do not make sense average or dry weather profiles are often very different from wet weather profiles for a wide range of measurements making use of load data instead of concentration data can in some cases avoid this problem as it is sometimes less susceptible to variations due to wet weather 4 some filling methods are better applied for small gaps than for large ones and vice versa in section 3 4 2 for example it is noticed that when smaller gaps are filled with modeled values the imputed data does not always follow the exact trend of the original data hereby introducing a sharp increase or decrease in the data as described in the first paragraph of this section to fill gaps of medium size i e between 1 h and 1 day the additional interpolation methods that were shortly mentioned e g pchip seem to provide an interesting alternative since they are developed to better follow trends in the data the maximum size of the gap that can be filled by these interpolation methods can be extended in short the wwdata package does not guarantee a good data cleaning and gap filling without the necessary user knowledge to apply its functionalities this can be very well considered as one of the strong suits of the package the lack of a generally applicable approach provides the user who knows his her data best with the flexibility to fine tune his her data filtering and or filling in the way s he sees fit 4 4 advantages and scientific relevance of the wwdata package although up to this point this contribution focused on the description of the wwdata package and its use in an example case this section will highlight the specific advantages and the scientific novelty of it 4 4 1 time gain one of the main goals of the wwdata package is to obtain a time gain when going from a raw dataset to one that can be used as model input especially for repeated actions this time gain is noticeably present a user familiar with the package can receive a completely new dataset explore that data visualize it and get a feel for it in one day determination of the appropriate filter parameters and checking the reliability of the available filling options is estimated to take a second day whereas a third and final day is sufficient to execute the filling and present a usable dataset the reason this is possible is the fact that easy to use functions are directly combined with visualization when a filtering procedure does not yield the expected result the user will immediately see this in one of the produced graphs be able to change the filter parameters and fastly asses the new result a traditional way of analyzing and preparing a dataset for modeling would easily take a month or more underpinning the relevance of this contribution for both academics and practitioners looking to make more use of the data available to them in an era of big data fast data analysis and reconciliation will help to avoid the buildup of data graveyards at utilities and companies see also section 4 4 3 furthermore a modeling effort is typically an iterative process requiring several cycles of calibration model updates and validation for some of these iterations new input data and thus a new data analysis filtering and filling is necessary in that case the package flexibility allows to very easily identify where parameters or filling methods might need to be changed as compared to the previous dataset s based on the authors experience a time reduction in the order of magnitude of weeks to even months is possible this way 4 4 2 work flow the work flow presented in this paper see also fig 1 and especially the fact that it is integrated within one single environment is another advantage there is no need for data scientists to adjust data in one software read it with another do some operations and write it out again only to use it in yet another software in addition to this the use of jupyter notebooks also increases efficiency transparency and user flexibility although the authors are aware that jupyter notebooks are not commonly applied yet and that many practitioners like to stick to their well known data processing environment often excel we would still like to make a case for their use the notebook user can execute the earlier mentioned repeated actions fast while staying in control and having in line plotting functionalities available to dig deeper into the data when needed or wanted moreover the jupyter notebook enviroment does allow the integration and use of other programming languages such as r and matlab offering the opportunity to combine the wwdata package with already available in house code thanks to the fact that a notebook can be considered a document with actual information on the why of certain actions it can also be seen as a memory or even an actual report serving very well when one wants to recheck how s he ended up with a specific dataset in addition to being very transparent when sharing procedures between different people 4 4 3 increased data use the increased use of original data throughout the undertaken modeling effort is likely the outcome of the presented research that is most scientifically relevant in essence the package allows an increased amount of real information to be fed to a model which will provide better options for a sound calibration and validation in the end leading to a model with a higher predictive power and hence a more solid basis for decision making additionally the reliability testing allows to make an informed decision on the applied filling algorithm making sure that the information added to the data while filling gaps has the best quality possible the same testing also allows to have an idea about what this quality actually is i e what the uncertainty of the model input is 4 4 4 semi automation with regards to the level of automation the wwdata package provides one could argue that more advanced automation is still possible and could provide an even larger time gain although this is acknowledged by the authors we are of the opinion that it is important and will in the end pay off to get a good feeling of ones data by handling it in a semi automated way as described here an additional effect of such an approach together with the visualization capabilities of the package is an improved communication with possible project utility partners an effect that in a current climate of increased interest in data use is not to be underestimated people at waterboard de dommel highly appreciated getting an insight into how their data was analyzed and what process led to the datasets that are used as input in the wwtp model 4 4 5 extendability finally because all data and all added tags are at any point in time accessible by the user s he can easily extend the current implementation with own data analysis code or algorithms for example data validation as presented in this contribution is solely based on the data itself in reality data validation is also linked to the sensors that were used and the environment and context of the measurement process bertrand krajewski et al 2003 the availability of the code allows the user to also take into account these factors hereby further encouraging them to increase their data use 5 conclusions and perspectives in conclusion the wwdata package can be considered a useful flexible and accessible tool to streamline data analysis and gap filling of relatively large and high frequency datasets in order to yield model input datasets ensure quality and reproducibility of this analysis and gap filling as a consequence of the structured integrated approach and the possibility to test for reliability save time while executing the analysis and gap filling preserve as much information as possible in the available datasets by not deleting but tagging data points improve communication on data analysis with possible collaboration partners some of the topics that are currently taken into consideration for further development see also the issues on the github repository github com ugentbiomath wwdata are the implementation of additional and more advanced filtering and filling algorithms based on literature findings bertrand krajewski et al 2003 alferes et al 2017 absent data is not necessarily faulty data the fact that data is absent e g because above or below detection limit provides information as well ways to use this information will be looked into collaboration on further development is warmly welcomed the reader is encouraged to create an issue on github com ugentbiomath wwdata to give suggestions acknowledgements waterboard de dommel supported the research described in this paper and provided all the data used for which the authors are very grateful stijn van hoey is gratefully acknowledged for his support in the initiation of this work the tips with regards to object oriented programming in python and his advice on documenting and publishing python packages the authors would also like to thank the reviewers of this paper for their relevant remarks and ideas for future development of the package appendix a 1 wwdata functionalities this section non exhaustively lists the several functionalities available within the wwdata package as well as some information on their implementation and use for more detailed information the reader is referred to the online package documentation ugentbiomath github io wwdata docs a 1 1 data formatting functionalities the possibility to indicate that the index of the onlinesensorbased object is containing time data this is also important for further operations on the data in case index values are missing a function is implemented to insert them assuming equidistant data conversion of absolute time values i e date and hour to relative time values some models for example cannot handle date values as an input and need relative time values i e float or integer values the possibility to add units data types or tags to the class object upon creating it so that the user can always recheck this information while handling the data a 1 2 data exploration functionalities the checking for a linear correlation between data series yielding the linear regression coefficients and the r2 value of the correlation the computation of an average daily profile as well as the standard deviation and upper and lower quantiles around it this average profile can be calculated based on the whole dataset or on a subset of it depending on user preference the calculation of daily averages including the standard deviation on the average the tagging of rain events based on what the user defines to be a rain event this is currently a simple tagging of values higher than a certain value or percentile this tagging will become important further on when filling gaps in the data section 2 2 4 the above mentioned functions all include the option to plot their impact see examples in appendix fig a2 a 1 3 data filtering functionalities filtering not a number nan values these are detected and a tag is added in obj meta valid filtering values that are physically not possible values above or below a user defined upper or lower bound receive a tag in obj meta valid filtering values that are part of a constant signal indicating sensor failure subsequent data points having the same value or a value within a certain user defined bound within which a signal is assumed to be constant are tagged in obj meta valid filtering values based on the difference between two consecutive points if a value for example changes from a very low to a very high value in a short amount of time it is likely that it is a data point prone to noise i e a data point that does not represent the true value of the measured parameter due to unwanted and usually unknown interferences during signal processing this is checked based on the slope between two consecutive values where the last value of the two is tagged if this slope value is too high filtering values based on the difference with a smooth representation of the dataset for example a data point with a deviation of more than 90 from the moving average calculated with a certain window can be considered an outlier i e a data point that due to its large deviation from other data points can not be considered statistically relevant within the dataset such data points are also tagged in obj meta valid also here there is the option to plot the result of a filtering function see figures in section 3 2 next to the possibility to plot all filter functions give a user output stating what the initial amount of data points was and how many of those were filtered based on that filter function when choosing the plotting option user output also indicates the total percentage of data points left after filtering a 1 4 data filling functionalities interpolation linear or other as long as the consecutive amount of missing data points is limited limited being defined by the user see also showcase description in section 2 3 and table 2 interpolation can be a easy and sound way to fill gaps although in the rest of this contribution linear interpolation is used the implementation makes use of the pandas series interpolate function so any interpolation algorithm as implemented there should can be used linear correlation with another measured value see also section a 1 2 when for example cod data points are missing during a certain time period a successful measurement of tss along with an established correlation between the two can be used to replace some of the missing data using a calculated daily profile see also section a 1 2 if the diurnal pattern of the data shows a low enough variability this pattern can be used to fill some larger gaps in a dataset using the data from the day before if weather and or influent flow dynamics stay stable during a certain period it is sound to assume that conditions during two consecutive days are similar and the data from the one day can be used to fill in data from the next where data is missing the maximum length of the period to apply this algorithm for is a user defined function argument the optimal value depending on the specific data set the reliability testing as described and discussed in respectively sections 2 2 5 and 4 2 offers a tool to determine this optimum using modeled data as discussed in section 1 influent models or generators are well described in literature and often give good results the output from these models generators can be used to fill large gaps in a dataset once again function arguments include the possibility to plot results in this case yielding a plot of the filled dataset where the data points filled by a certain function have a different color see plots in section 3 4 in case the onlinesensorbased object has the tag wwtp assigned to it every filling function also gives a user warning when replacing data obtained during a rain event as defined before see section a 1 2 this is to make the user aware of the fact that the filling algorithm might not work as expected when replacing data obtained during a rain event additionally the user has the option to replace either data points tagged as filtered or all data points within a certain range a2 figures fig a 1 simple representation of the wwdata package structure the hydrodata superclass contains three subclasses each with their own functions specifically aimed at a certain type of data the onlinesensorbased class is currently the most developed one fig a 1 fig a 2 example output of the data exploration functions in the wwdata package top the tagging of rain weather flows middle daily average and standard deviation on the average of ammonium measurements bottom left the average daily profile of ammonium including the upper and lower 90 quantile lines bottom right scatter plot and linear correlation between codt and total suspended solids tss in the influent of eindhoven stad units are m3 h for the flow and respectively mg l and mgn l for cod tss and ammonium additional output of the get correlation function includes correlation coefficients and the r2 value in the presented case i e linear correlation between codt and tss data from the eindhoven stad sewer influent these are 0 35 slope 747 intercept and 0 3 r2 fig a 2 
26369,integrating advanced simulation techniques and data analysis tools in a freeware geographic information system gis provides a valuable contribution to the management of conjunctive use of groundwater the world s largest freshwater resource and surface water to this aim we describe here the freewat free and open source software tools for water resource management platform freewat is a free and open source qgis integrated interface for planning and management of water resources with specific attention to groundwater the freewat platform couples the power of gis geo processing and post processing tools in spatial data analysis with that of process based simulation models the freewat environment allows storage of large spatial datasets data management and visualization and running of several distributed modelling codes mainly belonging to the modflow family it simulates hydrologic and transport processes and provides a database framework and visualization capabilities for hydrochemical analysis examples of real case study applications are provided keywords freewat qgis modflow free and open source software groundwater management ict 1 introduction groundwater is the world s largest freshwater resource trenberth et al 2006 life sustaining at global scale supplying water to people irrigated agriculture industry energy production and maintaining ecosystems as such groundwater exploitation groundwater sustainability and management groundwater depletion wada et al 2010 siebert et al 2010 groundwater quality deterioration menció et al 2016 chabukdhara et al 2017 werner et al 2013 and conjunctive use of ground and surface water li et al 2016 singh 2014 constitute a critical issue worldwide foster et al 2000 gleeson et al 2012 singh 2014 and need to be carefully addressed to manage all these issues spatial databases for the description of groundwater bodies characteristics including i e surface and subsurface geology information aquifer hydrodynamics and hydrodispersive data as a result of direct or indirect site investigations surface water groundwater relationships are available schwarz and alexander 1995 refsgaard et al 2010 di luzio et al 2017 regione toscana 2017 supsi 2017 and extensive monitoring networks are in operation in many areas of the world as required by groundwater related legislation crc 2004 eu 2000 2006 california department of water resources 2016a 2016b moreover authorities in view of improving the management of groundwater abstractions are increasingly building spatial database where well characteristics and discharge are stored such piece of information starts to be available also as open data and standard formats e g schwarz and alexander 1995 regione toscana 2017 aca 2000 as several hydrologic and hydrochemical variables are being monitored and both satellite and ground based observation data are collected there is the opportunity to take advantage of this large mass of data and information to develop dynamically growing and efficient groundwater management plans while the use of semi quantitative or analytical approaches is widespread this type of approach alone does not take advantage of all the information that might be derived by the newly collected data thus making inconsistent the large economic effort done in data collection and archiving and potentially leading to unsuccessful groundwater management geographic information systems giss have been applied to support environmental modelling and being able to store manage analyse and visualize large temporal spatial and non spatial datasets they are the most efficient tools to deal with geometric and alphanumerical data this makes gis a perfect candidate for advancing and facilitating the use of tools to manage large set of data and complex modelling environments kresic and mikszewski 2012 traditionally gis has been used in groundwater studies for producing groundwater head contour or contaminant plume spread maps making use of gis integrated interpolation methods or to perform groundwater vulnerability analysis i e using the drastic method neh et al 2015 shrestha et al 2017 on the other hand in the last 15 years several authors have been integrating basic tools for facilitating groundwater management in gis environment with increasing production since 2010 maidment 2002 developed a dedicated data model for water resource gogu et al 2001 martin et al 2005 strassberg et al 2005 de dreuzy et al 2006 chesnaux et al 2011 and strassberg et al 2011 focused the data model on groundwater related applications as further examples akbar et al 2011 presented a gis based modelling system called arcprzm 3 for spatial modelling of pesticide leaching potential from soil towards groundwater rios et al 2013 programmed a gis based software to simulate groundwater nitrate load from septic systems to surface water bodies ajami et al 2012 describe the ripgis net a gis tool for riparian groundwater evapotranspiration in modflow toews and gusyev 2013 describe a gis tool to delineate groundwater capture zone velasco et al 2014 developed quimet a gis based hydrogeochemical analysis tools criollo et al 2016 developed an integrated gis based tool for aquifer test analysis however all these efforts are sparse and non coordinated and almost all of them are developed within commercial not open gis software among the available icts information and communication technologies physically based and distributed groundwater numerical models coupling ground and surface water and unsaturated zone processes and incorporating climate land use morphological hydrological and hydrogeological data may represent comprehensive and dynamic tools to target water resource management issues refsgaard et al 2010 cao et al 2013 singh 2014 these tools allow simulating the distribution of the water resource in space and time taking into account anthropogenic stresses and providing readily usable information to decision makers pullar and springer 2000 they may support the development of highly informative representations of hydrological systems by i combining all the available spatial and non spatial data in a single framework ii allowing update and improvement as new data are gathered iii providing information in space and time to water managers iv offering relevant predictive functions thus allowing evaluation on how a hydrological system might behave under different scenarios of natural and anthropogenic constraints anderson et al 2015 discuss in detail the potential applications of such tools while singh 2014 presents a review on the use of numerical groundwater models for managing the groundwater resource examples of applications to fulfill water regulation requirements may be found in vázquez suñé et al 2006 shepley et al 2012 moran 2016 modelers may take advantage of integrating advanced hydrological modelling codes within a gis environment thus reducing model setup and analysis time and avoiding data isolation data integrity problems and broken data flows between model implementation and pre and post processing steps alcaraz et al 2017 bhatt et al 2008 2014 pullar and springer 2000 since 2000 researchers have been devoted to design the integration of modelling codes within a gis environment alcaraz et al 2017 bhatt et al 2014 carrera hernandez and gaskin 2006 crestaz et al 2012 dile et al 2016 rossetto et al 2013 strassberg et al 2005 wang et al 2016 lei et al 2011 the coupling strategy between the hydrological model and the gis framework is a core issue in the integration of the two components three different approaches are presented in the literature brimicombe 2003 goodchild 1992 nyerges 1991 fig 1 i loose coupling ii close tight coupling iii embedding the simplest approach is the loose coupling fig 1a which treats the two components independently and allows interaction through manually enabled file exchange only in the close tight coupling strategy fig 1b gis and hydrological model engines work separately but the first provides the interface where data are pre processed run and then visualized as such direct communication between the two components occurs during program execution when the gis integrated graphical user interface gui allows to generate input text files which are then read by the program executable for running and producing output files full integration at programming language level is required in the third approach also called seamless integration fig 1c where new models using gis data format are embedded as full component of the host gis application pullar and springer 2000 wang et al 2016 nowadays gis is a well consolidated technology among water authorities utilities and consultant companies giss are commonly used to process data for input into groundwater models or post process results as these models require large spatial and temporal datasets wang et al 2016 while the use of modelling techniques continuously spreads loose coupling has been the traditional way of dialogue between giss and models e g visual modflow guiguer and franz 1996 groundwater vistas rumbaugh and rumbaugh 2011 modelmuse winston 2009 also close tight coupling solutions are available e g feflow crestaz et al 2012 modflow shapiro et al 1997 modflow analyst aquaveo 2012 sid grid rossetto et al 2013 the most used gis software for developing close tight coupling is esri gis software esri 2011 followed by argus one argus holdings ltd 1995 qgis qgis development team 2009 and mapwindow gis ames et al 2008 grass gis grass development team 2017 and gvsig anguix and díaz 2008 gvsig association 2010 were used in one coupling experience each most of the solutions are developed using free and open source gis software e g bhatt et al 2008 2014 carrera hernandez and gaskin 2006 to our knowledge only one example of seamless coupling exists the bgs gis groundwater wang et al 2016 embedded in esri proprietary software several modelling codes are open source and freely available e g modflow it must be noted that the openness of a code is increasingly a relevant factor in scientific analysis as it constitutes a guarantee for reproducibility and reliability of the analysis performed ince et al 2012 hanson et al 2011 and fast deployment of the code dile et al 2016 codes neither open nor free among them the well known mike she hughes and liu 2008 and feflow diersch 2009 restrict the usage only to those able to buy such software i e high income countries dile et al 2016 the cost of the software may then constitute a barrier to the use of advanced ict tools for groundwater management as for modelling codes commercial giss besides the licensing costs often bring concerns about proprietary data structures rigidity in data models and platform dependence bhatt et al 2008 2014 hence producing open source and freely available gis integrated software tools based at least on a close tight coupling approach may contribute to enhance groundwater management capabilities from a technical point of view dile et al 2016 rossetto et al 2013 2015a de filippis et al 2017a wang et al 2016 finally this may also support water policies implementation i e the eu water framework directive eu 2000 and the groundwater directive eu 2006 in this paper we aim to present the architecture and capabilities of freewat an open source and free environment developed within the qgis gis desktop where several tools and modelling codes for groundwater management and conjunctive use of ground and surface water are integrated freewat is conceived so that data coming from groundwater bodies characterization and their relationships with surface water bodies and human activities and monitoring networks may be stored analyzed with dedicated tools processed by means of simulation models and finally results evaluated and visualized in the unique qgis environment tools integrated in freewat and their relevance are presented along with their application to selected case studies links to the source code to a reference manual and to six user manuals and thirteen tutorials with related datasets are provided as additional material the objective of producing the freewat software is to enlarge the capabilities of authorities and companies in managing the groundwater resource by using up to date robust well documented and reliable software without entailing the need of costly licensing 2 freewat architecture freewat was developed using open source and public domain codes within the framework of the horizon 2020 freewat project free and open source software tools for water resource management rossetto et al 2015a de filippis et al 2017a foglia et al 2018 freewat development evolved from the sid grid platform rossetto et al 2013 which integrated simulation codes within the open source and free gis gvsig anguix and díaz 2008 gvsig association 2010 the freewat code is released with a gnu general public gpl version 2 license and it is accessible through the main project portal www freewat eu the official qgis repository of experimental plugins and also through the gitlab repository https gitlab com freewat the freewat software fig 2 is built on i the gis qgis qgis development team 2009 ii a spatialite relational database management system rdbms which is an sqlite database engine with spatial functions added spatialite development team 2011 for spatial data management and sharing iii dedicated tools for pre processing of field data the akvagis for hydrochemical and hydrogeological analysis and the observation analysis tool for time series analysis iv several existing process based simulation models belonging to the modflow family harbaugh 2005 except for the crop growth module which belongs to the epic apex family of codes gassman et al 2005 williams et al 1989 for simulating crop water uptake and crop yield for the simulation of hydrological processes with particular reference to groundwater flow advective dispersive solute transport and density dependent flow v dedicated tools for post processing of model results the adoption of a spatialite rdbms is convenient for model sharing a spatialite database is a file where all the model information is stored and it can be easily shared among users as such freewat architecture is designed as a modular ensemble of three main classes of tools a raw data analysis and pre processing tools b simulation tools and c tools for post processing integration of such pillars is performed via python programming language www python org with extensive use of the python flopy library flopy 2016 bakker et al 2016 2017 for writing inputs and post processing the majority of simulation codes details about tools integrated in the freewat platform and how they are connected are provided in fig 3 the pre processing tools for the analysis interpretation and visualization of hydrochemical and hydrogeological data are included in the akvagis module serrano et al 2017 and in the observation analysis tool oat cannata and neumann 2017 module that focuses on advanced time series analysis on the other hand freewat integrates in qgis a whole set of simulation codes including among the others hydrological simulation codes and in particular codes for groundwater management and conjunctive use of ground and surface water modflow 2005 harbaugh 2005 modflow nwt niswonger et al 2011 modflow owhm hanson et al 2014a codes for simulating advective dispersive transport in aquifers mt3dms zheng and wang 1999 and in the unsaturated zone mt3d usgs bedekar et al 2016 including density dependent flow seawat langevin et al 2007 the occurrence of basic chemical reactions can also be accounted specifically the following processes can be handled equilibrium controlled linear or nonlinear sorption non equilibrium rate limited sorption and first order reaction representing radioactive decay and biodegradation one code to perform sensitivity analysis and model calibration ucode 2014 poeter et al 2014 one code for crop growth modelling cgm gassman et al 2005 williams et al 1989 tools for general gis operations to prepare input data and post processing functionalities for model data output freewat is developed as a qgis plugin so that once installed and activated it appears as a drop down menu in the qgis toolbar fig 4 such drop down menu consists of several sub menus each of them dedicated to a specific module process including pre and post processing modules tools for model implementation and supplementary tools for managing gis and spatialite layers a similar approach has been used in dile et al 2016 for the qswat plugin the freewat plugin comes with a set of manuals volume 0 the reference manual borsi et al 2017 provides details about the plugin characteristics and development and modules six user manuals explain how to use the different modules and tools integrated in the plugin finally a set of thirteen tutorials drive the user to the use of such tools and modules the following sections describe the components of the freewat platform and some applications to case studies developed within the h2020 freewat project 2 1 gis interface and pre processing tools qgis is an open source gis gui supported on linux unix mac osx windows and android qfield and licensed under the gnu general public license v3 0 gpl 3 0 the software is mostly written in c 67 and it supports python language trough python bindings pyqgis that enable the creation of plugins the command execution in a python console integrated in qgis and the creation of standalone scripts or custom applications based on qgis api during the last years qgis has become a worldwide used geographic free and open source software qgis github repository https github com qgis qgis capabilities for freewat purposes are well described in bhatt et al 2014 since 2013 a new stable version of qgis is released every 4 months and a long term release ltr claiming a stronger reliability of the algorithms and the whole software infrastructure is released every year the last ltr is qgis las palmas 2 18 in view of the migration to python3 qt5 support hereinafter an overview of the pre processing tools integrated in freewat is provided 2 1 1 the akvagis module akvagis serrano et al 2017 is a pre processing module integrated in freewat to allow water agencies stakeholders public authorities and professionals of the water sector to address among the others the following issues a identifying the main processes influencing the chemical composition of groundwater and the corresponding spatial and temporal distribution b evaluating groundwater quality and the achievement of good chemical status based on thresholds issued e g by the water framework directive wfd eu 2000 c managing and integrating a large amount of time and space dependent data e g hydrogeological hydrochemical etc d homogenizing and harmonizing large sets of data collected from diverse sources gathered with different techniques and formats supported by ogs open geospatial consortium and inspire eu 2007 to be easily shared across different operating systems e performing a comprehensive analysis of the available data for generating input files for hydrogeological models time series and surfaces of hydrogeological units akvagis tools may be divided in two groups tools for hydrochemical analysis and tools for hydrogeological analysis the entry point for using both these sets of tools is a dedicated relational spatialite database the observed hydrogeological parameters the collected hydrochemical samples and their physical chemical or microbiological measurements are related to the spatial points stored in a points table these points can be wells piezometers springs or any other specific point from water bodies where measurements have been collected e g swallow holes rivers lakes sea etc additional information such as other hydrogeological parameters responsible parties and project information among the others can be stored for a quick data management without losing information all the akvagis tables and their fields are described in detail in the freewat user manual volume 4 serrano et al 2017 three sub menus are specifically dedicated to the hydrochemical and the hydrogeological data exploitation 1 the database management tools are devoted to create a new akvagis database or open or close an existing one the hydrochemical and the hydrogeological spatio temporal data have to be previously stored in the akvagis database when the hydrochemical and the hydrogeological spatio temporal data are stored in an akvagis database they are ready for representation or analysis using the next sub modules see fig 5 2 the hydrochemical analysis tools allow to improve the harmonization integration standardization visualization and interpretation of hydrochemical data these tools include different instruments that cover a wide range of methodologies for querying interpreting and comparing groundwater quality data they are conceived in order to facilitate the pre processing analysis for being used in the definition of conceptual groundwater models for instance hydrochemical analysis is useful to ensure flow paths to control interactions between different water bodies e g ground and surface water interactions or to characterize water rock interactions to perform these kind of analysis and others related to physical and chemical characteristics of water some of the tools developed allow to perform ionic balance calculations chemical time series analysis correlation of chemical parameters and calculation of various common hydrochemical diagrams salinity schöller berkalof piper stiff among the others all these diagrams are created managed and customized with the chemplotlib library hunter 2007 that given its versatility can be used independently and applied to reproduce other diagrams and plots furthermore the user may generate maps of the spatial distributions of parameters stiff diagram maps and thematic maps for parameters according to pre set thresholds following a given regulation e g the wfd 3 the hydrogeological analysis tools allow to manage visualize and interpret hydrogeological data the user has the possibility to 1 query the hydrogeological measurements e g piezometric head wells abstractions etc performed in wells piezometers springs etc and stored in the akvagis database 2 create thematic maps e g piezometric maps based on selected points time intervals and parameters 3 calculate some general statistics such as the minimum maximum or average value for each selected hydrogeological parameter 4 query the depth or the thickness of the identified hydrogeological units for further processing such parameters with qgis interpolation tools creating hydrogeological surfaces these surfaces can be used as input hydrogeological layers in a groundwater numerical model the advantages of using the akvagis tool relies in having a dedicated free and open source database which is shared among the facility planners relevant water authorities and the environmental protection agency allowing each of these entities to perform analysis on the monitored data this way authorities and agencies have the chance not only to comment on reports but to work on the raw data 2 1 2 observation analysis tool oat oat cannata and neumann 2017 is a pre processing tool integrated in freewat for processing time series observations to be used in deriving model input data and supporting the calibration process oat is inspired to tsproc time series processing westenbroek et al 2012 software which allows time series processing using a script language oat is similar to tsproc in its final aim but differs in its design and implementation requirements in order to make some new processing capabilities available details are provided below and to attain compatibility with commonly applied programming languages and with the standards in the field of sensors observation data management and formatting the library design follows existing standards and can be considered a simplified version of the sensor observation service bröring et al 2012 objects the oat library implements two main classes the oat sensor class designed to handle time series data and metadata and the oat method class which is designed to represent a processing method the general structure and implemented use of the oat library in freewat is presented in fig 6 each oat sensor object is characterized by a single time series represented by a data section consisting in a time series and a location metadata section time series are managed thanks to the pandas library mckinney 2011 every oat sensor object can be stored in a spatialite database and re loaded back in python as oat sensor with its own data and metadata the metadata section includes name description location latitude longitude elevation unit of measurement observed property coordinate system time zone frequency weight statistic and data availability time interval the data section contains time data name and quality index as well as a tag marking whether or not an individual observation in the series is going to be used sensor data can be retrieved from the istsos istituto scienze della terra sensor observation service server cannata and antonovic 2010 or from local files or databases in the freewat gis environment for further use additionally model results can be imported as oat sensor for further time series analysis the oat method objects are based on tsproc processing capabilities with the addition of new freewat specific processes examples of methods are resampling for calculating a new time series with a given frequency comparison of different time series filling to fill a time series which contains gaps in data statistics for calculating some basic statistics for a time series the result of a method is generally a new oat sensor so that processes can be concatenated and the final resulting time series can be saved in the freewat model database or exported the library is integrated in the freewat platform by specific gui designed following the qgis specifications the interface allows non software programming users to take advantage of the library features and manage temporal data in the modelling environment four oat specific interfaces have been implemented to create and manage metadata of a time series and to process and compare its data in fig 7 the manage sensor and compare sensor frames are presented to illustrate the gui look 2 2 the modelling framework freewat includes a suite of modelling codes for performing groundwater flow and related processes simulation and analysis of groundwater management and conjunctive use of ground and surface water the modelling framework is based on the popular 3d finite difference code for groundwater flow modflow and related codes by integrating primarily the modflow 2005 harbaugh 2005 and modflow owhm one water hydrologic flow model hanson et al 2014a versions modflow is a physically based spatially distributed code developed by the usgs which simulates groundwater flow dynamics in the saturated and unsaturated zones both in confined and unconfined aquifers with constant or variable thickness and transmissivity values in steady state or transient conditions the modflow source code written in fortran is open well documented freely available on the web at https water usgs gov ogw modflow and it has become a global standard for groundwater modelling applications e g davison and lerner 2000 ebraheem et al 2004 faunt et al 2009 hanson et al 2015 phillips et al 2015 2 2 1 integrated simulation codes table 1 lists codes and modules currently available through freewat to simulate different processes these codes are widely used through commercial or free dedicated guis both for professional and academic applications refer to the cited references for a comprehensive description of the implemented codes in freewat the application of modflow 2005 for simulating groundwater flow in porous media including ground and surface water relation and the vertical flow through the unsaturated zone is conceived through the implementation of several modflow packages harbaugh 2005 to represent flow associated with external stresses such as wells areal recharge evapotranspiration drains and rivers as boundary conditions and sink source terms furthermore the following must be noted modflow nwt executable is needed in freewat if a groundwater flow model has to be linked to a solute transport model in the vadose zone this because mt3d usgs needs a specific ascii file so far generated only by modflow nwt modflow owhm executable is needed to run a farm process fmp scenario in freewat modflow owhm is used to deal with the conjunctive use of ground and surface water for water management issues to this purpose modflow owhm is complemented by the fmp for setting up and running water management scenarios in modflow owhm the volumetric water budget calculated by modflow 2005 for the modelled hydrologic system is further complemented by water budgets calculated by the fmp module for specific sub regions of the model called farms these farms are called water units in freewat and consist in areal units defined by sets of grid cells requiring water for irrigated agriculture natural vegetation and i e other anthropic activities in urban areas the major scope of simulations performed via modflow owhm and fmp is to provide an effective representation of conjunctive use of ground and surface water resources to meet the required water demand in freewat fmp results visualization consists of plots showing how the components of water demand and supply change over time modflow owhm and fmp have been applied to rural environments in california for water use management purposes as described in faunt et al 2009 hanson et al 2014b hanson et al 2015 and phillips et al 2015 in freewat the fmp is further coupled with a module dedicated to crop growth the crop growth module cgm aiming at estimating crop water uptake and crop yield at harvest based on hydrology solar radiation and temperature information the cgm is based on the epic apex family of models gassman et al 2005 williams et al 1989 the cgm is run sequentially after the fmp and all over the growing season of the crop from seeding to harvest crop yield at harvest is calculated as a function of the above ground biomass the cgm compares the potential crop yield and the actual crop yield i e taking into account the amount of water taken through root uptake for plant transpiration in freewat any groundwater model may be coupled with one or more solute transport models aiming at simulating multi species advective dispersive transport both in unsaturated and saturated zone the reference code integrated for simulating solute transport in the saturated zone is mt3dms zheng and wang 1999 which has a comprehensive set of options and capabilities for simulating changes in concentrations of miscible contaminants in groundwater considering advection dispersion diffusion and some basic chemical reactions with various types of boundary conditions and external sinks or sources simulation of heat transport is also possible by treating temperature as a species and defining diffusive coefficient and other parameters in a coherent way see e g hecht méndez et al 2010 alberti et al 2012 simulation of viscosity and density dependent flow may be performed in freewat by applying seawat langevin et al 2007 a coupled version of modflow and mt3dms designed to simulate 3d variable density viscosity groundwater flow and multi species transport such capabilities are particularly relevant to approach studies on seawater intrusion processes where density variations of water due to salinity effects are crucial solute transport in the vadose zone can be simulated through two different approaches use of mt3d usgs bedekar et al 2016 with new transport modelling capabilities including simulation of solute transport in the unsaturated zone unsaturated solute balance usb borsi et al 2017 module which estimates the concentration at the water table of a contaminant released at the ground surface according to the infiltration rate through the vadose zone as calculated by the modflow uzf package niswonger et al 2006 such concentration at the water table can be then considered as a constant concentration term for mt3dms to simulate solute transport in groundwater sensitivity analysis calibration and uncertainty evaluation methods are crucial to practical applications of complex hydrological models important characteristics cannot be estimated accurately and or completely enough to fully define model input values many reviews and discussions are available in the literature to demonstrate the importance of properly performing sensitivity analysis calibration and uncertainty evaluation in order to increase model reliability and transparency when dealing with environmental models hill and tiedeman 2007 bennett et al 2013 doherty 2015 these critical steps are also necessary to explore the relations between different types of data and the processes represented in a model including the comparison of different models and model results when used by stakeholders and policy makers to support decisions for water resources management in freewat inclusion of inverse modelling capabilities is performed by ucode 2014 poeter et al 2014 ucode 2014 can use local perturbation methods for sensitivity analysis and non linear least squared regression through a modified gauss newton method for model calibration compared to global methods this approach is characterized by relatively frugal computational requirements and is well suited for complex models of natural systems which assume long execution time foglia et al 2007 2013 la vigna et al 2016 in ucode 2014 model parameters are estimated automatically by examining model results after performing model runs with different parameter values in hopes of improving how well the model represents the system of concern goodness of such representation is accomplished by comparing model results to field measurements sensitivity of model parameters can be determined prior to perform any parameter estimation to avoid estimating insensitive parameters and thus reducing execution time as mentioned above the calibration and sensitivity analysis module can be directly connected to the oat module 2 2 2 modelling workflow modflow requires text input files with a specific file structure close tight coupling between the qgis software and the simulation codes integrated in freewat is achieved via four different file formats gis layer a typical gis vector or raster input dataset without any explicit reference to the model space and time discretization e g a point polyline or polygon layer containing the geometric component only model layer ml a subsurface model vector layer defining the finite difference grid where 3d geometric features such as land surface elevation or layer top and bottom elevations hydrodynamic parameters hydraulic conductivity and storage parameters and basic parameters are written at each cell of the grid a model may consist in several mls and their number is based on the hydrostratigraphy defined by the modeller model data objects mdos spatial temporal and finite difference grid data needed to generate inputs for the simulation codes integrated in freewat an mdo is created from a geographical input a point line or polygon gis layer a temporal input derived from a timetable and a finite difference grid an mdo brings information about the spatial coordinate system as in a gis layer but also the spatial finite difference grid reference system and the time discretization of the simulated processes model file text file generated from an mdo and required to run the simulation the modelling workflow within freewat is accomplished within qgis and it is based on the sequence a data pre processing b model implementation c model run d post processing a general modelling workflow envisages the following steps here presented for a groundwater flow model for the sake of simplicity fig 8 a data pre processing data pre processing mostly consists in collecting storing and editing available geographic and non geographic information by performing gis vector or raster operations to prepare data for the next step model implementation all the gis vector and raster data need to be created selecting a specific coordinate reference system crs which will be used coherently during all the phases of model implementation the activities to be performed are related for example to define top and bottom surfaces of hydrostratigraphic units to create shapefiles for the drainage network from digital elevation model dem etc this step can also take advantage of the akvagis and oat capabilities b model implementation define model setup and translate the previously created data files gis layers tables in mls mdos database tables this takes into account the space and time discretization of the hydrological model initially a new hydrological model and the related spatialite geodatabase file sqlite are created within a specific working folder where model files and results are stored in the following steps of the workflow i e when generating model text files when running the model and when importing model results fig 9 model sharing is thus made straightforward through sharing the sqlite file moreover the user can also easily interface with the mls mdos stored within the spatial database through specific qgis plugins e g the db manager during model creation time and map units the setup of the first stress period and the model crs must be defined as well after this step model discretization has to be defined regarding both space discretization in the horizontal model grid and vertical mls planes and time discretization geometry and hydrodynamic properties are then assigned to each ml for this task the user may use spatial gis functions e g spatial joins selection tools for using raster and vector layers to assign properties values at each grid cell two dedicated algorithms the copy from vector layer and copy from raster layer tools are developed in freewat to copy properties values from raster vector layers to grid cells through automatically performing spatial intersection between raster pixels or point line polygon shape files and the model grid in order to get required modflow text files to define boundary and initial conditions and source sink terms mdos have to be created i e well mdo river mdo etc this completes the phase of model implementation and lays the basis for model run c model run this phase consists in translating by means of the python flopy library flopy 2016 bakker et al 2016 2017 all the mls and mdos implemented and stored within the spatialite database in model files i e text files these are then input to the modflow executable for running the groundwater flow model in the run model window fig 9 the user activates packages and inputs solver parameters including solver convergence criteria for which initial default values are set de filippis et al 2017b the run model window contains four tabs 1 the groundwater flow tab allows to run modflow and simulate groundwater dynamics this step is mandatory and needs to be performed before using any other tabs 2 the solute transport tab allows to run mt3dms mt3d usgs or seawat using the output from step 1 3 the owhm farm process tab allows to run the fmp schmid et al 2006 schmid and hanson 2009 the simulation will start with the conditions set on step 1 and rerun modflow owhm based on the fmp inputs 4 the model calibration tab allows to run ucode 2014 for sensitivity analysis and or parameter estimation using models resulting from completion of steps 1 2 or 3 all the steps related to model implementation can be repeated for setting up one or more solute transport models or an fmp model linked to the modflow model mandatorily implemented and run the same holds true also for calibration with ucode 2014 at each of these tabs the run button triggers the writing process of model files specific to the process which is going to be simulated the executable of the needed simulation code is retrieved according to the path defined by the user through the prg locations table and the simulation is performed d post processing once a simulation has successfully terminated the user can display results with freewat post processing tools that take advantage of all the visualization tools available through qgis freewat embeds a dedicated set of sub menus for visualizing results obtained from specific simulation codes these are available through the post processing menu of freewat fig 5 and include generate raster files with distribution of the simulated hydraulic head solute concentration for each model layer at specific time steps within a selected stress period these can be handled into gis using geographical algorithms e g contouring to perform the desired analysis visualize cross sections for contaminant plume spreads in the vertical plane fig 10a visualize volumetric model budget at specific time steps and stress periods by means of bar charts save a cvs file with the water budget of user defined sub regions after the application of the zone budget visualize a scatter plot for estimating model fit by comparing simulated to observed values for the hydraulic head at certain locations fig 10b generate graphs to evaluate sensitivity indexes estimate parameters and model fit after running ucode 2014 generate water budget plots after an fmp simulation generate pathlines after running modpath results visualization goes through reading of binary output files generated after model run and this is accomplished in freewat by using the python flopy library flopy 2016 bakker et al 2016 2017 3 case studies the tools integrated in freewat were tested at 16 case studies in eu and non eu countries for dealing with a number of water related issues cannata et al 2017 dadaser celik and celik 2017 de filippis et al 2017d 2017e freewat 2017a 2017b grodzynskyi and svidzinska 2017 kopač and vremec 2017 panteleit et al 2017 perdikaki et al 2017 positano and nannucci 2017 thirteen synthetic applications were also designed for tutorials an overview of the locations of the above mentioned real world case studies is provided in fig 11 descriptions of freewat application at some of these case studies are reported below 3 1 design of a managed aquifer recharge facility at suvereto central italy within the life rewat project www liferewat eu freewat was applied to support the design of a managed aquifer recharge mar facility at suvereto tuscany region central italy rossetto et al 2018 the mar plant aims at restoring the overexploited coastal aquifer of the cornia plain for irrigation drinking and industrial purposes the present italian regulation for permitting a mar plant requires a one year monitoring of groundwater and surface water level and chemical water quality dm 100 2016 ministero dell ambiente 2016 in this context the akvagis tools were used to input store and analyse all the monitored data data were then used to build piper schöller berkalof fig 12 a and b and other plots for assessing the main groundwater chemical characteristics their variability during the year and relationships with surface water chemistry and geothermal groundwater chemistry from adjoining groundwater bodies in this example the piper and schöeller berkalof diagrams show that samples of ground and surface water belong to the bicarbonate calcium facies while limited chemical variations can be observed at some points 3 2 ground and surface water interaction at the lugano lake case study the freewat platform was applied at the lugano lake case study cannata et al 2017 a transboundary water body shared between switzerland and italy in order to assess interactions between the lugano lake and the aquifers connected to this surface water body a modflow model was built using the lak package merritt and konikow 2000 in the development of the lugano lake case study oat was used to facilitate the automatic import in qgis of all the observations of the canton ticino hydro meteorological monitoring system and the related hydrogeological database gespos supsi 2017 measurements of temperature river stage and precipitation were harvested with the oat capability to connect with an istsos web service and create for the user defined time period and resolution new oat sensors and related time series in freewat groundwater level time series were generated by importing csv files fig 8 statistical analysis of the time series was conducted to better understand the system recharge and to estimate evapotranspiration to set appropriate boundary conditions additionally the created oat sensor was used to set observations for calibration oat can speed up the model creation and calibration phases in a gis integrated environment finally its capability to record time series metadata facilitates model sharing of sensor types and observed properties 3 3 managing the induced riverbank filtration mar scheme at sant alessio plain lucca italy groundwater modelling capabilities were also tested to a case study developed for demonstrating the effectiveness of managing the induced riverbank filtration mar scheme at the sant alessio plain in central italy rossetto et al 2015b in this case study the modelling framework was used to define the well head protection area for a well field consisting of 12 vertical wells the overall abstraction is about 0 5 m3 s set along the serchio riverbank for drinking purposes fig 13 the freewat platform was applied to estimate induced increased infiltration rates in the aquifer caused by large groundwater pumping and building of a river weir to rise the river head using modflow 2005 and wellhead protection areas by means of isochrones using modpath and gis tools de filippis et al 2017d 3 4 analysis of contamination caused by diffuse pollution lucca italy the sant alessio case study was also used to demonstrate the simulation of solute transport in freewat using mt3dms de filippis et al 2017d in the case study we evaluated the dilution effects of better quality surface water recharging the aquifer with reference to aquifer nitrate contamination due to agricultural activities and untreated wastewater discharged in areas located north east of the study area fig 14 3 5 conjunctive use of ground and surface water in rural water management a synthetic problem was designed to show the application of fmp in freewat as a tool to address water management issues in urban and rural areas this example is inspired to a hypothetical case study presented in schmid et al 2006 and used in the simulating water management in agricultural catchments tutorial produced within the h2020 freewat project fig 15 shows the water budget computed for an irrigated water unit the following terms are shown i precipitation as specified in input by the user ii evaporation and transpiration from groundwater as calculated by the fmp water supply is guaranteed by means of groundwater pumping well pumping as specified in input by the user periods of water deficit i e demand exceeding supply can be easily identified when the external water delivery term is not null this means that supply from all the other inflow components is not sufficient to meet the water demand of the irrigated area it occurs i e between day 100 and 200 when precipitation rates are low evapotranspiration rates increase and user defined maximum pumping rates i e legally constrained are reached in such cases alternative sources of water supply external water delivery other than groundwater should be taken into account and the possibility to conjunctively use ground and surface water could be tested in an fmp scenario by connecting the water unit to a network of surface channels delivering pipelines 3 6 sensitivity analysis and calibration for modelling groundwater management in the stampriet area among the case studies developed within the h2020 freewat project the stampriet transboundary aquifer system the stas represents an important resource of freshwater shared among namibia botswana and south africa and mainly exploited for irrigation purposes the main objective of this case study was to provide a tool able to provide a shared knowledge of the stas aquifer system in order to foster a cooperation among the three governments for a sustainable management of groundwater resources freewat 2017b ucode 2014 was extensively used for evaluating and calibrating a modflow model developed for the stas aquifer among the available sensitivity indicators the composite scaled sensitivity css indicates the information content of all the available observations for the estimation of each parameter poeter et al 2014 fig 16 presents a plot of the css evaluated for the following parameters inflow rates specified along the northern boundary of the active domain cfr fig 17 and simulated through sets of pumping wells parameters wellx y where x refers to the model layer and y identifies a set of pumping wells to which a specific rate was assigned hydraulic conductivity values assigned at specific zones of the active domain parameters hk x y where x refers to the model layer and y identifies a specific zone of the active domain distributed recharge flux assigned at specific zones of the active domain parameters rchy where y identifies a specific zone of the active domain css values were analysed along with parameter correlation coefficients pcc to evaluate which parameters were to be included in the calibration process strong positive correlation i e pcc 0 95 occurred between parameters well1 2 and well1 3 only fig 17 shows a spatially distributed representation of the residuals observed minus simulated values calculated after calibration of the modflow model red dots identify areas where residuals are negative and the model over estimates the observed values vice versa blue dots provide information on areas where the model under estimates the observations 4 limitations and further development freewat development and maintenance has been addressed so far according to suggestions from horizon 2020 freewat project partners in relation to the application to their specific case studies moreover about 1100 individuals from the academic world water authorities water utilities and geoenvironmental companies among the others were trained during extensive dissemination activities throughout 60 dedicated courses in about 50 countries until the end of september 2017 these allowed gathering a huge mass of information on code malfunctioning which was then fixed code improvements and suggestions for further code development as per the modflow suite at present not all the modflow packages and processes are implemented in the freewat platform for instance the following stress packages are not available flow and head boundary fhb reservoir res and stream str similarly the conduit flow process cfp shoemaker et al 2008 for the simulation of turbulent groundwater flow conditions in dual porosity aquifers is not presently supported in freewat only one solver is implemented the preconditioned conjugate gradient pcg package additional limitations exist in using mnw2 lak uzf and sfr2 packages arising from a selection of some options in the code which cannot be managed through the freewat interface de filippis et al 2017b these choices were made in order to allow an easier and faster application of the software even if limiting its full capabilities nevertheless advanced users may directly modify modflow model files to cope with such limitations additionally the advanced user may implement packages even if not supported in freewat producing the related text file independently of the freewat interface and running the model by directly using the modelling code executable mt3d usgs can be applied only to address unsaturated zone transport uzt other specific packages included in mt3d usgs are not yet supported e g cts contaminant treatment system package lkt lake transport package sft streamflow transport package bedekar et al 2016 the fmp version currently implemented within the platform has two main limitations spatial distribution of water units cannot vary throughout the simulation and crop rotation is not allowed further assumptions have been included and these result in adopting some default options in order to ease the code usability de filippis et al 2017c also in such case the advanced user can modify model files and run modflow owhm independently of freewat the authors wish to mention among the others as relevant suggestions for code capabilities improvement the need to integrate a method for grid refinement only at selected model areas i e the local grid refinement capability mehl and hill 2005 the need for integrating more modflow solver packages as well as the implementation of stochastic simulations methods and of modules for improving simulation of mass exchange between ground and surface water some modflow versions are missing i e modflow usg modflow 6 and their integration is considered for further development tools to cope with some of these limitations are currently under development 5 conclusions groundwater is a critical resource for people and ecosystems tools are needed for efficient data management so that more technically sound and community supported decisions may be made in this view development and diffusion of robust open source and free software constitutes a cornerstone to enhance groundwater management thus empowering as much as possible technical units in water authorities academia and private companies also in communities countries with limited resources in recent years several efforts have addressed these issues by developing tools in gis and coupling gis and numerical hydrological models often commercial software was used to this scope but the cost of such software is usually prohibitive in many areas of the world the freewat platform aims at targeting these goals by having developed within the qgis gis environment and adopting spatialite as a dbms dedicated tools for processing of large ground and surface water related datasets and using a close tight coupling strategy integration of several hydrological simulation codes mostly from the modflow family by using the freewat plugin the user can archive pre process and analyse data related to groundwater bodies characterization and their relationships with surface water bodies and human activities and monitoring networks build a set of models groundwater flow models solute transport models inversion models and post process results in a unique qgis environment tools developed and integrated in freewat and their relevance are presented along with their application to selected case studies models can be run at different scales from small contaminated site to large watershed the latter is demonstrated in freewat by the case study on the stampriet transboundary aquifer shared among namibia botswana and south africa the freewat plugin is freely available through the freewat project website www freewat eu the gitlab repository https gitlab com freewat and the qgis plugin repository along with one reference manual six user manuals and a set of thirteen tutorials with related datasets dealing with different groundwater management issues i e contamination issues managed aquifer recharge rural water management seawater intrusion calibration of groundwater models etc all this material aims to disseminate not only freewat use as a standard software but also to increase capacity on ict use for managing groundwater quantity and quality by providing lectures tutorials still open and free starting from basic theory to applications we also aim to increase capacity in numerical modelling in order to foster full understanding of the concepts and limitations of the methods and mindful applications at present freewat version 1 0 2 is available since march 2018 finally the objective of producing the freewat software is that of enlarging the capabilities of authorities and companies to manage groundwater resources by using up to date robust well documented and reliable software whose documentation is accessible and modifiable without entailing the need of costly licensing maintenance and diffusion of this experience will strongly rely on building a large community of users and developers this community may help in identifying potential bugs to be fixed and providing suggestions for further development in this view the large capacity building activities performed so far and their outcomes constitute in the authors opinion sufficient guarantee for its continuation software availability software name freewat v 1 0 2 development team iacopo borsi head developer iacopo borsi tea group com massimiliano cannata rotman criollo laura foglia giovanna de filippis matteo ghetta steffen mehl vincent mora vincent picavet rudy rossetto enric vázquez suñé violeta velasco mansilla year first available 2017 software required qgis and the up to date version of the freewat plugin availability software and documentation can be downloaded from the freewat website through the download area to access the download area free of charge registration is requested for statistical purposes only by filling the form at http www freewat eu download information the freewat plugin can also be downloaded through the official qgis repository of experimental plugins the freewat code can also be accessed through the gitlab repository https gitlab com freewat license freewat is released under a gnu general public license version 2 june 1991 https www gnu org licenses old licenses gpl 2 0 en html cost free program language python program size about 70 mb referred to the freewat plugin only acknowledgements this paper is presented within the framework of the h2020 freewat project which received funding from the european union s horizon 2020 research and innovation programme grant agreement n 642224 additional freewat development received funding from the following projects porting of sid grid under qgis was performed through funds provided by regione toscana to scuola superiore s anna project evoluzione del sistema open source sid grid di elaborazione dei dati geografici vettoriali e raster per il porting negli ambienti qgis e spatialite in uso presso la regione toscana cig za50e4058a saturated zone solute transport simulation capability was developed within the eu fp7 env 2013 water inno demo marsol marsol project received funding from the european union s seventh framework programme for research technological development and demonstration under grant agreement n 619120 www marsol eu this paper content reflects only the authors views and the european union is not liable for any use that may be made of the information contained therein the authors are grateful to mary c hill for her in depth review of the manuscript which greatly helped in improving this paper and to an anonymous reviewer appendix a supplementary data the following is the supplementary data related to this article data profile data profile appendix a supplementary data supplementary data related to this article can be found at https doi org 10 1016 j envsoft 2018 06 007 
26369,integrating advanced simulation techniques and data analysis tools in a freeware geographic information system gis provides a valuable contribution to the management of conjunctive use of groundwater the world s largest freshwater resource and surface water to this aim we describe here the freewat free and open source software tools for water resource management platform freewat is a free and open source qgis integrated interface for planning and management of water resources with specific attention to groundwater the freewat platform couples the power of gis geo processing and post processing tools in spatial data analysis with that of process based simulation models the freewat environment allows storage of large spatial datasets data management and visualization and running of several distributed modelling codes mainly belonging to the modflow family it simulates hydrologic and transport processes and provides a database framework and visualization capabilities for hydrochemical analysis examples of real case study applications are provided keywords freewat qgis modflow free and open source software groundwater management ict 1 introduction groundwater is the world s largest freshwater resource trenberth et al 2006 life sustaining at global scale supplying water to people irrigated agriculture industry energy production and maintaining ecosystems as such groundwater exploitation groundwater sustainability and management groundwater depletion wada et al 2010 siebert et al 2010 groundwater quality deterioration menció et al 2016 chabukdhara et al 2017 werner et al 2013 and conjunctive use of ground and surface water li et al 2016 singh 2014 constitute a critical issue worldwide foster et al 2000 gleeson et al 2012 singh 2014 and need to be carefully addressed to manage all these issues spatial databases for the description of groundwater bodies characteristics including i e surface and subsurface geology information aquifer hydrodynamics and hydrodispersive data as a result of direct or indirect site investigations surface water groundwater relationships are available schwarz and alexander 1995 refsgaard et al 2010 di luzio et al 2017 regione toscana 2017 supsi 2017 and extensive monitoring networks are in operation in many areas of the world as required by groundwater related legislation crc 2004 eu 2000 2006 california department of water resources 2016a 2016b moreover authorities in view of improving the management of groundwater abstractions are increasingly building spatial database where well characteristics and discharge are stored such piece of information starts to be available also as open data and standard formats e g schwarz and alexander 1995 regione toscana 2017 aca 2000 as several hydrologic and hydrochemical variables are being monitored and both satellite and ground based observation data are collected there is the opportunity to take advantage of this large mass of data and information to develop dynamically growing and efficient groundwater management plans while the use of semi quantitative or analytical approaches is widespread this type of approach alone does not take advantage of all the information that might be derived by the newly collected data thus making inconsistent the large economic effort done in data collection and archiving and potentially leading to unsuccessful groundwater management geographic information systems giss have been applied to support environmental modelling and being able to store manage analyse and visualize large temporal spatial and non spatial datasets they are the most efficient tools to deal with geometric and alphanumerical data this makes gis a perfect candidate for advancing and facilitating the use of tools to manage large set of data and complex modelling environments kresic and mikszewski 2012 traditionally gis has been used in groundwater studies for producing groundwater head contour or contaminant plume spread maps making use of gis integrated interpolation methods or to perform groundwater vulnerability analysis i e using the drastic method neh et al 2015 shrestha et al 2017 on the other hand in the last 15 years several authors have been integrating basic tools for facilitating groundwater management in gis environment with increasing production since 2010 maidment 2002 developed a dedicated data model for water resource gogu et al 2001 martin et al 2005 strassberg et al 2005 de dreuzy et al 2006 chesnaux et al 2011 and strassberg et al 2011 focused the data model on groundwater related applications as further examples akbar et al 2011 presented a gis based modelling system called arcprzm 3 for spatial modelling of pesticide leaching potential from soil towards groundwater rios et al 2013 programmed a gis based software to simulate groundwater nitrate load from septic systems to surface water bodies ajami et al 2012 describe the ripgis net a gis tool for riparian groundwater evapotranspiration in modflow toews and gusyev 2013 describe a gis tool to delineate groundwater capture zone velasco et al 2014 developed quimet a gis based hydrogeochemical analysis tools criollo et al 2016 developed an integrated gis based tool for aquifer test analysis however all these efforts are sparse and non coordinated and almost all of them are developed within commercial not open gis software among the available icts information and communication technologies physically based and distributed groundwater numerical models coupling ground and surface water and unsaturated zone processes and incorporating climate land use morphological hydrological and hydrogeological data may represent comprehensive and dynamic tools to target water resource management issues refsgaard et al 2010 cao et al 2013 singh 2014 these tools allow simulating the distribution of the water resource in space and time taking into account anthropogenic stresses and providing readily usable information to decision makers pullar and springer 2000 they may support the development of highly informative representations of hydrological systems by i combining all the available spatial and non spatial data in a single framework ii allowing update and improvement as new data are gathered iii providing information in space and time to water managers iv offering relevant predictive functions thus allowing evaluation on how a hydrological system might behave under different scenarios of natural and anthropogenic constraints anderson et al 2015 discuss in detail the potential applications of such tools while singh 2014 presents a review on the use of numerical groundwater models for managing the groundwater resource examples of applications to fulfill water regulation requirements may be found in vázquez suñé et al 2006 shepley et al 2012 moran 2016 modelers may take advantage of integrating advanced hydrological modelling codes within a gis environment thus reducing model setup and analysis time and avoiding data isolation data integrity problems and broken data flows between model implementation and pre and post processing steps alcaraz et al 2017 bhatt et al 2008 2014 pullar and springer 2000 since 2000 researchers have been devoted to design the integration of modelling codes within a gis environment alcaraz et al 2017 bhatt et al 2014 carrera hernandez and gaskin 2006 crestaz et al 2012 dile et al 2016 rossetto et al 2013 strassberg et al 2005 wang et al 2016 lei et al 2011 the coupling strategy between the hydrological model and the gis framework is a core issue in the integration of the two components three different approaches are presented in the literature brimicombe 2003 goodchild 1992 nyerges 1991 fig 1 i loose coupling ii close tight coupling iii embedding the simplest approach is the loose coupling fig 1a which treats the two components independently and allows interaction through manually enabled file exchange only in the close tight coupling strategy fig 1b gis and hydrological model engines work separately but the first provides the interface where data are pre processed run and then visualized as such direct communication between the two components occurs during program execution when the gis integrated graphical user interface gui allows to generate input text files which are then read by the program executable for running and producing output files full integration at programming language level is required in the third approach also called seamless integration fig 1c where new models using gis data format are embedded as full component of the host gis application pullar and springer 2000 wang et al 2016 nowadays gis is a well consolidated technology among water authorities utilities and consultant companies giss are commonly used to process data for input into groundwater models or post process results as these models require large spatial and temporal datasets wang et al 2016 while the use of modelling techniques continuously spreads loose coupling has been the traditional way of dialogue between giss and models e g visual modflow guiguer and franz 1996 groundwater vistas rumbaugh and rumbaugh 2011 modelmuse winston 2009 also close tight coupling solutions are available e g feflow crestaz et al 2012 modflow shapiro et al 1997 modflow analyst aquaveo 2012 sid grid rossetto et al 2013 the most used gis software for developing close tight coupling is esri gis software esri 2011 followed by argus one argus holdings ltd 1995 qgis qgis development team 2009 and mapwindow gis ames et al 2008 grass gis grass development team 2017 and gvsig anguix and díaz 2008 gvsig association 2010 were used in one coupling experience each most of the solutions are developed using free and open source gis software e g bhatt et al 2008 2014 carrera hernandez and gaskin 2006 to our knowledge only one example of seamless coupling exists the bgs gis groundwater wang et al 2016 embedded in esri proprietary software several modelling codes are open source and freely available e g modflow it must be noted that the openness of a code is increasingly a relevant factor in scientific analysis as it constitutes a guarantee for reproducibility and reliability of the analysis performed ince et al 2012 hanson et al 2011 and fast deployment of the code dile et al 2016 codes neither open nor free among them the well known mike she hughes and liu 2008 and feflow diersch 2009 restrict the usage only to those able to buy such software i e high income countries dile et al 2016 the cost of the software may then constitute a barrier to the use of advanced ict tools for groundwater management as for modelling codes commercial giss besides the licensing costs often bring concerns about proprietary data structures rigidity in data models and platform dependence bhatt et al 2008 2014 hence producing open source and freely available gis integrated software tools based at least on a close tight coupling approach may contribute to enhance groundwater management capabilities from a technical point of view dile et al 2016 rossetto et al 2013 2015a de filippis et al 2017a wang et al 2016 finally this may also support water policies implementation i e the eu water framework directive eu 2000 and the groundwater directive eu 2006 in this paper we aim to present the architecture and capabilities of freewat an open source and free environment developed within the qgis gis desktop where several tools and modelling codes for groundwater management and conjunctive use of ground and surface water are integrated freewat is conceived so that data coming from groundwater bodies characterization and their relationships with surface water bodies and human activities and monitoring networks may be stored analyzed with dedicated tools processed by means of simulation models and finally results evaluated and visualized in the unique qgis environment tools integrated in freewat and their relevance are presented along with their application to selected case studies links to the source code to a reference manual and to six user manuals and thirteen tutorials with related datasets are provided as additional material the objective of producing the freewat software is to enlarge the capabilities of authorities and companies in managing the groundwater resource by using up to date robust well documented and reliable software without entailing the need of costly licensing 2 freewat architecture freewat was developed using open source and public domain codes within the framework of the horizon 2020 freewat project free and open source software tools for water resource management rossetto et al 2015a de filippis et al 2017a foglia et al 2018 freewat development evolved from the sid grid platform rossetto et al 2013 which integrated simulation codes within the open source and free gis gvsig anguix and díaz 2008 gvsig association 2010 the freewat code is released with a gnu general public gpl version 2 license and it is accessible through the main project portal www freewat eu the official qgis repository of experimental plugins and also through the gitlab repository https gitlab com freewat the freewat software fig 2 is built on i the gis qgis qgis development team 2009 ii a spatialite relational database management system rdbms which is an sqlite database engine with spatial functions added spatialite development team 2011 for spatial data management and sharing iii dedicated tools for pre processing of field data the akvagis for hydrochemical and hydrogeological analysis and the observation analysis tool for time series analysis iv several existing process based simulation models belonging to the modflow family harbaugh 2005 except for the crop growth module which belongs to the epic apex family of codes gassman et al 2005 williams et al 1989 for simulating crop water uptake and crop yield for the simulation of hydrological processes with particular reference to groundwater flow advective dispersive solute transport and density dependent flow v dedicated tools for post processing of model results the adoption of a spatialite rdbms is convenient for model sharing a spatialite database is a file where all the model information is stored and it can be easily shared among users as such freewat architecture is designed as a modular ensemble of three main classes of tools a raw data analysis and pre processing tools b simulation tools and c tools for post processing integration of such pillars is performed via python programming language www python org with extensive use of the python flopy library flopy 2016 bakker et al 2016 2017 for writing inputs and post processing the majority of simulation codes details about tools integrated in the freewat platform and how they are connected are provided in fig 3 the pre processing tools for the analysis interpretation and visualization of hydrochemical and hydrogeological data are included in the akvagis module serrano et al 2017 and in the observation analysis tool oat cannata and neumann 2017 module that focuses on advanced time series analysis on the other hand freewat integrates in qgis a whole set of simulation codes including among the others hydrological simulation codes and in particular codes for groundwater management and conjunctive use of ground and surface water modflow 2005 harbaugh 2005 modflow nwt niswonger et al 2011 modflow owhm hanson et al 2014a codes for simulating advective dispersive transport in aquifers mt3dms zheng and wang 1999 and in the unsaturated zone mt3d usgs bedekar et al 2016 including density dependent flow seawat langevin et al 2007 the occurrence of basic chemical reactions can also be accounted specifically the following processes can be handled equilibrium controlled linear or nonlinear sorption non equilibrium rate limited sorption and first order reaction representing radioactive decay and biodegradation one code to perform sensitivity analysis and model calibration ucode 2014 poeter et al 2014 one code for crop growth modelling cgm gassman et al 2005 williams et al 1989 tools for general gis operations to prepare input data and post processing functionalities for model data output freewat is developed as a qgis plugin so that once installed and activated it appears as a drop down menu in the qgis toolbar fig 4 such drop down menu consists of several sub menus each of them dedicated to a specific module process including pre and post processing modules tools for model implementation and supplementary tools for managing gis and spatialite layers a similar approach has been used in dile et al 2016 for the qswat plugin the freewat plugin comes with a set of manuals volume 0 the reference manual borsi et al 2017 provides details about the plugin characteristics and development and modules six user manuals explain how to use the different modules and tools integrated in the plugin finally a set of thirteen tutorials drive the user to the use of such tools and modules the following sections describe the components of the freewat platform and some applications to case studies developed within the h2020 freewat project 2 1 gis interface and pre processing tools qgis is an open source gis gui supported on linux unix mac osx windows and android qfield and licensed under the gnu general public license v3 0 gpl 3 0 the software is mostly written in c 67 and it supports python language trough python bindings pyqgis that enable the creation of plugins the command execution in a python console integrated in qgis and the creation of standalone scripts or custom applications based on qgis api during the last years qgis has become a worldwide used geographic free and open source software qgis github repository https github com qgis qgis capabilities for freewat purposes are well described in bhatt et al 2014 since 2013 a new stable version of qgis is released every 4 months and a long term release ltr claiming a stronger reliability of the algorithms and the whole software infrastructure is released every year the last ltr is qgis las palmas 2 18 in view of the migration to python3 qt5 support hereinafter an overview of the pre processing tools integrated in freewat is provided 2 1 1 the akvagis module akvagis serrano et al 2017 is a pre processing module integrated in freewat to allow water agencies stakeholders public authorities and professionals of the water sector to address among the others the following issues a identifying the main processes influencing the chemical composition of groundwater and the corresponding spatial and temporal distribution b evaluating groundwater quality and the achievement of good chemical status based on thresholds issued e g by the water framework directive wfd eu 2000 c managing and integrating a large amount of time and space dependent data e g hydrogeological hydrochemical etc d homogenizing and harmonizing large sets of data collected from diverse sources gathered with different techniques and formats supported by ogs open geospatial consortium and inspire eu 2007 to be easily shared across different operating systems e performing a comprehensive analysis of the available data for generating input files for hydrogeological models time series and surfaces of hydrogeological units akvagis tools may be divided in two groups tools for hydrochemical analysis and tools for hydrogeological analysis the entry point for using both these sets of tools is a dedicated relational spatialite database the observed hydrogeological parameters the collected hydrochemical samples and their physical chemical or microbiological measurements are related to the spatial points stored in a points table these points can be wells piezometers springs or any other specific point from water bodies where measurements have been collected e g swallow holes rivers lakes sea etc additional information such as other hydrogeological parameters responsible parties and project information among the others can be stored for a quick data management without losing information all the akvagis tables and their fields are described in detail in the freewat user manual volume 4 serrano et al 2017 three sub menus are specifically dedicated to the hydrochemical and the hydrogeological data exploitation 1 the database management tools are devoted to create a new akvagis database or open or close an existing one the hydrochemical and the hydrogeological spatio temporal data have to be previously stored in the akvagis database when the hydrochemical and the hydrogeological spatio temporal data are stored in an akvagis database they are ready for representation or analysis using the next sub modules see fig 5 2 the hydrochemical analysis tools allow to improve the harmonization integration standardization visualization and interpretation of hydrochemical data these tools include different instruments that cover a wide range of methodologies for querying interpreting and comparing groundwater quality data they are conceived in order to facilitate the pre processing analysis for being used in the definition of conceptual groundwater models for instance hydrochemical analysis is useful to ensure flow paths to control interactions between different water bodies e g ground and surface water interactions or to characterize water rock interactions to perform these kind of analysis and others related to physical and chemical characteristics of water some of the tools developed allow to perform ionic balance calculations chemical time series analysis correlation of chemical parameters and calculation of various common hydrochemical diagrams salinity schöller berkalof piper stiff among the others all these diagrams are created managed and customized with the chemplotlib library hunter 2007 that given its versatility can be used independently and applied to reproduce other diagrams and plots furthermore the user may generate maps of the spatial distributions of parameters stiff diagram maps and thematic maps for parameters according to pre set thresholds following a given regulation e g the wfd 3 the hydrogeological analysis tools allow to manage visualize and interpret hydrogeological data the user has the possibility to 1 query the hydrogeological measurements e g piezometric head wells abstractions etc performed in wells piezometers springs etc and stored in the akvagis database 2 create thematic maps e g piezometric maps based on selected points time intervals and parameters 3 calculate some general statistics such as the minimum maximum or average value for each selected hydrogeological parameter 4 query the depth or the thickness of the identified hydrogeological units for further processing such parameters with qgis interpolation tools creating hydrogeological surfaces these surfaces can be used as input hydrogeological layers in a groundwater numerical model the advantages of using the akvagis tool relies in having a dedicated free and open source database which is shared among the facility planners relevant water authorities and the environmental protection agency allowing each of these entities to perform analysis on the monitored data this way authorities and agencies have the chance not only to comment on reports but to work on the raw data 2 1 2 observation analysis tool oat oat cannata and neumann 2017 is a pre processing tool integrated in freewat for processing time series observations to be used in deriving model input data and supporting the calibration process oat is inspired to tsproc time series processing westenbroek et al 2012 software which allows time series processing using a script language oat is similar to tsproc in its final aim but differs in its design and implementation requirements in order to make some new processing capabilities available details are provided below and to attain compatibility with commonly applied programming languages and with the standards in the field of sensors observation data management and formatting the library design follows existing standards and can be considered a simplified version of the sensor observation service bröring et al 2012 objects the oat library implements two main classes the oat sensor class designed to handle time series data and metadata and the oat method class which is designed to represent a processing method the general structure and implemented use of the oat library in freewat is presented in fig 6 each oat sensor object is characterized by a single time series represented by a data section consisting in a time series and a location metadata section time series are managed thanks to the pandas library mckinney 2011 every oat sensor object can be stored in a spatialite database and re loaded back in python as oat sensor with its own data and metadata the metadata section includes name description location latitude longitude elevation unit of measurement observed property coordinate system time zone frequency weight statistic and data availability time interval the data section contains time data name and quality index as well as a tag marking whether or not an individual observation in the series is going to be used sensor data can be retrieved from the istsos istituto scienze della terra sensor observation service server cannata and antonovic 2010 or from local files or databases in the freewat gis environment for further use additionally model results can be imported as oat sensor for further time series analysis the oat method objects are based on tsproc processing capabilities with the addition of new freewat specific processes examples of methods are resampling for calculating a new time series with a given frequency comparison of different time series filling to fill a time series which contains gaps in data statistics for calculating some basic statistics for a time series the result of a method is generally a new oat sensor so that processes can be concatenated and the final resulting time series can be saved in the freewat model database or exported the library is integrated in the freewat platform by specific gui designed following the qgis specifications the interface allows non software programming users to take advantage of the library features and manage temporal data in the modelling environment four oat specific interfaces have been implemented to create and manage metadata of a time series and to process and compare its data in fig 7 the manage sensor and compare sensor frames are presented to illustrate the gui look 2 2 the modelling framework freewat includes a suite of modelling codes for performing groundwater flow and related processes simulation and analysis of groundwater management and conjunctive use of ground and surface water the modelling framework is based on the popular 3d finite difference code for groundwater flow modflow and related codes by integrating primarily the modflow 2005 harbaugh 2005 and modflow owhm one water hydrologic flow model hanson et al 2014a versions modflow is a physically based spatially distributed code developed by the usgs which simulates groundwater flow dynamics in the saturated and unsaturated zones both in confined and unconfined aquifers with constant or variable thickness and transmissivity values in steady state or transient conditions the modflow source code written in fortran is open well documented freely available on the web at https water usgs gov ogw modflow and it has become a global standard for groundwater modelling applications e g davison and lerner 2000 ebraheem et al 2004 faunt et al 2009 hanson et al 2015 phillips et al 2015 2 2 1 integrated simulation codes table 1 lists codes and modules currently available through freewat to simulate different processes these codes are widely used through commercial or free dedicated guis both for professional and academic applications refer to the cited references for a comprehensive description of the implemented codes in freewat the application of modflow 2005 for simulating groundwater flow in porous media including ground and surface water relation and the vertical flow through the unsaturated zone is conceived through the implementation of several modflow packages harbaugh 2005 to represent flow associated with external stresses such as wells areal recharge evapotranspiration drains and rivers as boundary conditions and sink source terms furthermore the following must be noted modflow nwt executable is needed in freewat if a groundwater flow model has to be linked to a solute transport model in the vadose zone this because mt3d usgs needs a specific ascii file so far generated only by modflow nwt modflow owhm executable is needed to run a farm process fmp scenario in freewat modflow owhm is used to deal with the conjunctive use of ground and surface water for water management issues to this purpose modflow owhm is complemented by the fmp for setting up and running water management scenarios in modflow owhm the volumetric water budget calculated by modflow 2005 for the modelled hydrologic system is further complemented by water budgets calculated by the fmp module for specific sub regions of the model called farms these farms are called water units in freewat and consist in areal units defined by sets of grid cells requiring water for irrigated agriculture natural vegetation and i e other anthropic activities in urban areas the major scope of simulations performed via modflow owhm and fmp is to provide an effective representation of conjunctive use of ground and surface water resources to meet the required water demand in freewat fmp results visualization consists of plots showing how the components of water demand and supply change over time modflow owhm and fmp have been applied to rural environments in california for water use management purposes as described in faunt et al 2009 hanson et al 2014b hanson et al 2015 and phillips et al 2015 in freewat the fmp is further coupled with a module dedicated to crop growth the crop growth module cgm aiming at estimating crop water uptake and crop yield at harvest based on hydrology solar radiation and temperature information the cgm is based on the epic apex family of models gassman et al 2005 williams et al 1989 the cgm is run sequentially after the fmp and all over the growing season of the crop from seeding to harvest crop yield at harvest is calculated as a function of the above ground biomass the cgm compares the potential crop yield and the actual crop yield i e taking into account the amount of water taken through root uptake for plant transpiration in freewat any groundwater model may be coupled with one or more solute transport models aiming at simulating multi species advective dispersive transport both in unsaturated and saturated zone the reference code integrated for simulating solute transport in the saturated zone is mt3dms zheng and wang 1999 which has a comprehensive set of options and capabilities for simulating changes in concentrations of miscible contaminants in groundwater considering advection dispersion diffusion and some basic chemical reactions with various types of boundary conditions and external sinks or sources simulation of heat transport is also possible by treating temperature as a species and defining diffusive coefficient and other parameters in a coherent way see e g hecht méndez et al 2010 alberti et al 2012 simulation of viscosity and density dependent flow may be performed in freewat by applying seawat langevin et al 2007 a coupled version of modflow and mt3dms designed to simulate 3d variable density viscosity groundwater flow and multi species transport such capabilities are particularly relevant to approach studies on seawater intrusion processes where density variations of water due to salinity effects are crucial solute transport in the vadose zone can be simulated through two different approaches use of mt3d usgs bedekar et al 2016 with new transport modelling capabilities including simulation of solute transport in the unsaturated zone unsaturated solute balance usb borsi et al 2017 module which estimates the concentration at the water table of a contaminant released at the ground surface according to the infiltration rate through the vadose zone as calculated by the modflow uzf package niswonger et al 2006 such concentration at the water table can be then considered as a constant concentration term for mt3dms to simulate solute transport in groundwater sensitivity analysis calibration and uncertainty evaluation methods are crucial to practical applications of complex hydrological models important characteristics cannot be estimated accurately and or completely enough to fully define model input values many reviews and discussions are available in the literature to demonstrate the importance of properly performing sensitivity analysis calibration and uncertainty evaluation in order to increase model reliability and transparency when dealing with environmental models hill and tiedeman 2007 bennett et al 2013 doherty 2015 these critical steps are also necessary to explore the relations between different types of data and the processes represented in a model including the comparison of different models and model results when used by stakeholders and policy makers to support decisions for water resources management in freewat inclusion of inverse modelling capabilities is performed by ucode 2014 poeter et al 2014 ucode 2014 can use local perturbation methods for sensitivity analysis and non linear least squared regression through a modified gauss newton method for model calibration compared to global methods this approach is characterized by relatively frugal computational requirements and is well suited for complex models of natural systems which assume long execution time foglia et al 2007 2013 la vigna et al 2016 in ucode 2014 model parameters are estimated automatically by examining model results after performing model runs with different parameter values in hopes of improving how well the model represents the system of concern goodness of such representation is accomplished by comparing model results to field measurements sensitivity of model parameters can be determined prior to perform any parameter estimation to avoid estimating insensitive parameters and thus reducing execution time as mentioned above the calibration and sensitivity analysis module can be directly connected to the oat module 2 2 2 modelling workflow modflow requires text input files with a specific file structure close tight coupling between the qgis software and the simulation codes integrated in freewat is achieved via four different file formats gis layer a typical gis vector or raster input dataset without any explicit reference to the model space and time discretization e g a point polyline or polygon layer containing the geometric component only model layer ml a subsurface model vector layer defining the finite difference grid where 3d geometric features such as land surface elevation or layer top and bottom elevations hydrodynamic parameters hydraulic conductivity and storage parameters and basic parameters are written at each cell of the grid a model may consist in several mls and their number is based on the hydrostratigraphy defined by the modeller model data objects mdos spatial temporal and finite difference grid data needed to generate inputs for the simulation codes integrated in freewat an mdo is created from a geographical input a point line or polygon gis layer a temporal input derived from a timetable and a finite difference grid an mdo brings information about the spatial coordinate system as in a gis layer but also the spatial finite difference grid reference system and the time discretization of the simulated processes model file text file generated from an mdo and required to run the simulation the modelling workflow within freewat is accomplished within qgis and it is based on the sequence a data pre processing b model implementation c model run d post processing a general modelling workflow envisages the following steps here presented for a groundwater flow model for the sake of simplicity fig 8 a data pre processing data pre processing mostly consists in collecting storing and editing available geographic and non geographic information by performing gis vector or raster operations to prepare data for the next step model implementation all the gis vector and raster data need to be created selecting a specific coordinate reference system crs which will be used coherently during all the phases of model implementation the activities to be performed are related for example to define top and bottom surfaces of hydrostratigraphic units to create shapefiles for the drainage network from digital elevation model dem etc this step can also take advantage of the akvagis and oat capabilities b model implementation define model setup and translate the previously created data files gis layers tables in mls mdos database tables this takes into account the space and time discretization of the hydrological model initially a new hydrological model and the related spatialite geodatabase file sqlite are created within a specific working folder where model files and results are stored in the following steps of the workflow i e when generating model text files when running the model and when importing model results fig 9 model sharing is thus made straightforward through sharing the sqlite file moreover the user can also easily interface with the mls mdos stored within the spatial database through specific qgis plugins e g the db manager during model creation time and map units the setup of the first stress period and the model crs must be defined as well after this step model discretization has to be defined regarding both space discretization in the horizontal model grid and vertical mls planes and time discretization geometry and hydrodynamic properties are then assigned to each ml for this task the user may use spatial gis functions e g spatial joins selection tools for using raster and vector layers to assign properties values at each grid cell two dedicated algorithms the copy from vector layer and copy from raster layer tools are developed in freewat to copy properties values from raster vector layers to grid cells through automatically performing spatial intersection between raster pixels or point line polygon shape files and the model grid in order to get required modflow text files to define boundary and initial conditions and source sink terms mdos have to be created i e well mdo river mdo etc this completes the phase of model implementation and lays the basis for model run c model run this phase consists in translating by means of the python flopy library flopy 2016 bakker et al 2016 2017 all the mls and mdos implemented and stored within the spatialite database in model files i e text files these are then input to the modflow executable for running the groundwater flow model in the run model window fig 9 the user activates packages and inputs solver parameters including solver convergence criteria for which initial default values are set de filippis et al 2017b the run model window contains four tabs 1 the groundwater flow tab allows to run modflow and simulate groundwater dynamics this step is mandatory and needs to be performed before using any other tabs 2 the solute transport tab allows to run mt3dms mt3d usgs or seawat using the output from step 1 3 the owhm farm process tab allows to run the fmp schmid et al 2006 schmid and hanson 2009 the simulation will start with the conditions set on step 1 and rerun modflow owhm based on the fmp inputs 4 the model calibration tab allows to run ucode 2014 for sensitivity analysis and or parameter estimation using models resulting from completion of steps 1 2 or 3 all the steps related to model implementation can be repeated for setting up one or more solute transport models or an fmp model linked to the modflow model mandatorily implemented and run the same holds true also for calibration with ucode 2014 at each of these tabs the run button triggers the writing process of model files specific to the process which is going to be simulated the executable of the needed simulation code is retrieved according to the path defined by the user through the prg locations table and the simulation is performed d post processing once a simulation has successfully terminated the user can display results with freewat post processing tools that take advantage of all the visualization tools available through qgis freewat embeds a dedicated set of sub menus for visualizing results obtained from specific simulation codes these are available through the post processing menu of freewat fig 5 and include generate raster files with distribution of the simulated hydraulic head solute concentration for each model layer at specific time steps within a selected stress period these can be handled into gis using geographical algorithms e g contouring to perform the desired analysis visualize cross sections for contaminant plume spreads in the vertical plane fig 10a visualize volumetric model budget at specific time steps and stress periods by means of bar charts save a cvs file with the water budget of user defined sub regions after the application of the zone budget visualize a scatter plot for estimating model fit by comparing simulated to observed values for the hydraulic head at certain locations fig 10b generate graphs to evaluate sensitivity indexes estimate parameters and model fit after running ucode 2014 generate water budget plots after an fmp simulation generate pathlines after running modpath results visualization goes through reading of binary output files generated after model run and this is accomplished in freewat by using the python flopy library flopy 2016 bakker et al 2016 2017 3 case studies the tools integrated in freewat were tested at 16 case studies in eu and non eu countries for dealing with a number of water related issues cannata et al 2017 dadaser celik and celik 2017 de filippis et al 2017d 2017e freewat 2017a 2017b grodzynskyi and svidzinska 2017 kopač and vremec 2017 panteleit et al 2017 perdikaki et al 2017 positano and nannucci 2017 thirteen synthetic applications were also designed for tutorials an overview of the locations of the above mentioned real world case studies is provided in fig 11 descriptions of freewat application at some of these case studies are reported below 3 1 design of a managed aquifer recharge facility at suvereto central italy within the life rewat project www liferewat eu freewat was applied to support the design of a managed aquifer recharge mar facility at suvereto tuscany region central italy rossetto et al 2018 the mar plant aims at restoring the overexploited coastal aquifer of the cornia plain for irrigation drinking and industrial purposes the present italian regulation for permitting a mar plant requires a one year monitoring of groundwater and surface water level and chemical water quality dm 100 2016 ministero dell ambiente 2016 in this context the akvagis tools were used to input store and analyse all the monitored data data were then used to build piper schöller berkalof fig 12 a and b and other plots for assessing the main groundwater chemical characteristics their variability during the year and relationships with surface water chemistry and geothermal groundwater chemistry from adjoining groundwater bodies in this example the piper and schöeller berkalof diagrams show that samples of ground and surface water belong to the bicarbonate calcium facies while limited chemical variations can be observed at some points 3 2 ground and surface water interaction at the lugano lake case study the freewat platform was applied at the lugano lake case study cannata et al 2017 a transboundary water body shared between switzerland and italy in order to assess interactions between the lugano lake and the aquifers connected to this surface water body a modflow model was built using the lak package merritt and konikow 2000 in the development of the lugano lake case study oat was used to facilitate the automatic import in qgis of all the observations of the canton ticino hydro meteorological monitoring system and the related hydrogeological database gespos supsi 2017 measurements of temperature river stage and precipitation were harvested with the oat capability to connect with an istsos web service and create for the user defined time period and resolution new oat sensors and related time series in freewat groundwater level time series were generated by importing csv files fig 8 statistical analysis of the time series was conducted to better understand the system recharge and to estimate evapotranspiration to set appropriate boundary conditions additionally the created oat sensor was used to set observations for calibration oat can speed up the model creation and calibration phases in a gis integrated environment finally its capability to record time series metadata facilitates model sharing of sensor types and observed properties 3 3 managing the induced riverbank filtration mar scheme at sant alessio plain lucca italy groundwater modelling capabilities were also tested to a case study developed for demonstrating the effectiveness of managing the induced riverbank filtration mar scheme at the sant alessio plain in central italy rossetto et al 2015b in this case study the modelling framework was used to define the well head protection area for a well field consisting of 12 vertical wells the overall abstraction is about 0 5 m3 s set along the serchio riverbank for drinking purposes fig 13 the freewat platform was applied to estimate induced increased infiltration rates in the aquifer caused by large groundwater pumping and building of a river weir to rise the river head using modflow 2005 and wellhead protection areas by means of isochrones using modpath and gis tools de filippis et al 2017d 3 4 analysis of contamination caused by diffuse pollution lucca italy the sant alessio case study was also used to demonstrate the simulation of solute transport in freewat using mt3dms de filippis et al 2017d in the case study we evaluated the dilution effects of better quality surface water recharging the aquifer with reference to aquifer nitrate contamination due to agricultural activities and untreated wastewater discharged in areas located north east of the study area fig 14 3 5 conjunctive use of ground and surface water in rural water management a synthetic problem was designed to show the application of fmp in freewat as a tool to address water management issues in urban and rural areas this example is inspired to a hypothetical case study presented in schmid et al 2006 and used in the simulating water management in agricultural catchments tutorial produced within the h2020 freewat project fig 15 shows the water budget computed for an irrigated water unit the following terms are shown i precipitation as specified in input by the user ii evaporation and transpiration from groundwater as calculated by the fmp water supply is guaranteed by means of groundwater pumping well pumping as specified in input by the user periods of water deficit i e demand exceeding supply can be easily identified when the external water delivery term is not null this means that supply from all the other inflow components is not sufficient to meet the water demand of the irrigated area it occurs i e between day 100 and 200 when precipitation rates are low evapotranspiration rates increase and user defined maximum pumping rates i e legally constrained are reached in such cases alternative sources of water supply external water delivery other than groundwater should be taken into account and the possibility to conjunctively use ground and surface water could be tested in an fmp scenario by connecting the water unit to a network of surface channels delivering pipelines 3 6 sensitivity analysis and calibration for modelling groundwater management in the stampriet area among the case studies developed within the h2020 freewat project the stampriet transboundary aquifer system the stas represents an important resource of freshwater shared among namibia botswana and south africa and mainly exploited for irrigation purposes the main objective of this case study was to provide a tool able to provide a shared knowledge of the stas aquifer system in order to foster a cooperation among the three governments for a sustainable management of groundwater resources freewat 2017b ucode 2014 was extensively used for evaluating and calibrating a modflow model developed for the stas aquifer among the available sensitivity indicators the composite scaled sensitivity css indicates the information content of all the available observations for the estimation of each parameter poeter et al 2014 fig 16 presents a plot of the css evaluated for the following parameters inflow rates specified along the northern boundary of the active domain cfr fig 17 and simulated through sets of pumping wells parameters wellx y where x refers to the model layer and y identifies a set of pumping wells to which a specific rate was assigned hydraulic conductivity values assigned at specific zones of the active domain parameters hk x y where x refers to the model layer and y identifies a specific zone of the active domain distributed recharge flux assigned at specific zones of the active domain parameters rchy where y identifies a specific zone of the active domain css values were analysed along with parameter correlation coefficients pcc to evaluate which parameters were to be included in the calibration process strong positive correlation i e pcc 0 95 occurred between parameters well1 2 and well1 3 only fig 17 shows a spatially distributed representation of the residuals observed minus simulated values calculated after calibration of the modflow model red dots identify areas where residuals are negative and the model over estimates the observed values vice versa blue dots provide information on areas where the model under estimates the observations 4 limitations and further development freewat development and maintenance has been addressed so far according to suggestions from horizon 2020 freewat project partners in relation to the application to their specific case studies moreover about 1100 individuals from the academic world water authorities water utilities and geoenvironmental companies among the others were trained during extensive dissemination activities throughout 60 dedicated courses in about 50 countries until the end of september 2017 these allowed gathering a huge mass of information on code malfunctioning which was then fixed code improvements and suggestions for further code development as per the modflow suite at present not all the modflow packages and processes are implemented in the freewat platform for instance the following stress packages are not available flow and head boundary fhb reservoir res and stream str similarly the conduit flow process cfp shoemaker et al 2008 for the simulation of turbulent groundwater flow conditions in dual porosity aquifers is not presently supported in freewat only one solver is implemented the preconditioned conjugate gradient pcg package additional limitations exist in using mnw2 lak uzf and sfr2 packages arising from a selection of some options in the code which cannot be managed through the freewat interface de filippis et al 2017b these choices were made in order to allow an easier and faster application of the software even if limiting its full capabilities nevertheless advanced users may directly modify modflow model files to cope with such limitations additionally the advanced user may implement packages even if not supported in freewat producing the related text file independently of the freewat interface and running the model by directly using the modelling code executable mt3d usgs can be applied only to address unsaturated zone transport uzt other specific packages included in mt3d usgs are not yet supported e g cts contaminant treatment system package lkt lake transport package sft streamflow transport package bedekar et al 2016 the fmp version currently implemented within the platform has two main limitations spatial distribution of water units cannot vary throughout the simulation and crop rotation is not allowed further assumptions have been included and these result in adopting some default options in order to ease the code usability de filippis et al 2017c also in such case the advanced user can modify model files and run modflow owhm independently of freewat the authors wish to mention among the others as relevant suggestions for code capabilities improvement the need to integrate a method for grid refinement only at selected model areas i e the local grid refinement capability mehl and hill 2005 the need for integrating more modflow solver packages as well as the implementation of stochastic simulations methods and of modules for improving simulation of mass exchange between ground and surface water some modflow versions are missing i e modflow usg modflow 6 and their integration is considered for further development tools to cope with some of these limitations are currently under development 5 conclusions groundwater is a critical resource for people and ecosystems tools are needed for efficient data management so that more technically sound and community supported decisions may be made in this view development and diffusion of robust open source and free software constitutes a cornerstone to enhance groundwater management thus empowering as much as possible technical units in water authorities academia and private companies also in communities countries with limited resources in recent years several efforts have addressed these issues by developing tools in gis and coupling gis and numerical hydrological models often commercial software was used to this scope but the cost of such software is usually prohibitive in many areas of the world the freewat platform aims at targeting these goals by having developed within the qgis gis environment and adopting spatialite as a dbms dedicated tools for processing of large ground and surface water related datasets and using a close tight coupling strategy integration of several hydrological simulation codes mostly from the modflow family by using the freewat plugin the user can archive pre process and analyse data related to groundwater bodies characterization and their relationships with surface water bodies and human activities and monitoring networks build a set of models groundwater flow models solute transport models inversion models and post process results in a unique qgis environment tools developed and integrated in freewat and their relevance are presented along with their application to selected case studies models can be run at different scales from small contaminated site to large watershed the latter is demonstrated in freewat by the case study on the stampriet transboundary aquifer shared among namibia botswana and south africa the freewat plugin is freely available through the freewat project website www freewat eu the gitlab repository https gitlab com freewat and the qgis plugin repository along with one reference manual six user manuals and a set of thirteen tutorials with related datasets dealing with different groundwater management issues i e contamination issues managed aquifer recharge rural water management seawater intrusion calibration of groundwater models etc all this material aims to disseminate not only freewat use as a standard software but also to increase capacity on ict use for managing groundwater quantity and quality by providing lectures tutorials still open and free starting from basic theory to applications we also aim to increase capacity in numerical modelling in order to foster full understanding of the concepts and limitations of the methods and mindful applications at present freewat version 1 0 2 is available since march 2018 finally the objective of producing the freewat software is that of enlarging the capabilities of authorities and companies to manage groundwater resources by using up to date robust well documented and reliable software whose documentation is accessible and modifiable without entailing the need of costly licensing maintenance and diffusion of this experience will strongly rely on building a large community of users and developers this community may help in identifying potential bugs to be fixed and providing suggestions for further development in this view the large capacity building activities performed so far and their outcomes constitute in the authors opinion sufficient guarantee for its continuation software availability software name freewat v 1 0 2 development team iacopo borsi head developer iacopo borsi tea group com massimiliano cannata rotman criollo laura foglia giovanna de filippis matteo ghetta steffen mehl vincent mora vincent picavet rudy rossetto enric vázquez suñé violeta velasco mansilla year first available 2017 software required qgis and the up to date version of the freewat plugin availability software and documentation can be downloaded from the freewat website through the download area to access the download area free of charge registration is requested for statistical purposes only by filling the form at http www freewat eu download information the freewat plugin can also be downloaded through the official qgis repository of experimental plugins the freewat code can also be accessed through the gitlab repository https gitlab com freewat license freewat is released under a gnu general public license version 2 june 1991 https www gnu org licenses old licenses gpl 2 0 en html cost free program language python program size about 70 mb referred to the freewat plugin only acknowledgements this paper is presented within the framework of the h2020 freewat project which received funding from the european union s horizon 2020 research and innovation programme grant agreement n 642224 additional freewat development received funding from the following projects porting of sid grid under qgis was performed through funds provided by regione toscana to scuola superiore s anna project evoluzione del sistema open source sid grid di elaborazione dei dati geografici vettoriali e raster per il porting negli ambienti qgis e spatialite in uso presso la regione toscana cig za50e4058a saturated zone solute transport simulation capability was developed within the eu fp7 env 2013 water inno demo marsol marsol project received funding from the european union s seventh framework programme for research technological development and demonstration under grant agreement n 619120 www marsol eu this paper content reflects only the authors views and the european union is not liable for any use that may be made of the information contained therein the authors are grateful to mary c hill for her in depth review of the manuscript which greatly helped in improving this paper and to an anonymous reviewer appendix a supplementary data the following is the supplementary data related to this article data profile data profile appendix a supplementary data supplementary data related to this article can be found at https doi org 10 1016 j envsoft 2018 06 007 
