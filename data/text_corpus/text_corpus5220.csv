index,text
26100,the transient storage model tsm for the analysis of pollutant mixing in rivers has been hampered by parameter uncertainties due to the equifinality problem the generalized uncertainty estimation method which was frequently used to quantify the parameter uncertainty of tsm has been criticized because this method uses informal likelihood which can cause overestimation of the uncertainty thus in this study we suggest a bayesian inference method using a segment mixture sm likelihood which is a formal likelihood based on the mixture distribution of the segmented breakthrough curve the uncertainty estimation was conducted using three synthetic data and the real tracer test data achieved from uvas creek in the usa the results show that the sm likelihood estimated uncertainties of tsm appropriately by correctly representing the error distribution of the tsm and identifying the behavioral parameters keywords transient storage model parameter uncertainty estimation bayesian inference formal likelihood glue breakthrough curve segmentation 1 introduction in river mixing studies the one dimensional transient storage model tsm has been frequently used to simulate behaviors of solute transport since it was proposed by hays in 1966 hays 1966 fischer et al 1979 nordin and troutman 1980 bencala and walters 1983 bencala 1984 seo and maxwell 1992 runkel and chapra 1993 cheong and seo 2003 cheong et al 2007 camacho and gonzález 2008 kelleher et al 2013 ward et al 2017 the reason for the popularity of the tsm is that it can accurately reproduce the observed breakthrough curve btc which is characterized by its long tail via explaining river mixing processes with three important mixing mechanisms advection shear dispersion and storage effect however apart from its successful representation it has been pointed out that the parameters of tsm are highly uncertain to estimate which would lead to difficulty in characterizing river mixing processes and the application of tsm for prediction harvey and bencala 1993 harvey et al 1996 wagner and harvey 1997 wagener et al 2002 kelleher et al 2013 ward et al 2017 in tsm there are four parameters that represent different river mixing mechanisms such as advection shear dispersion and storage effect as shown in fig 1 these parameters should be estimated indirectly since their values are hard to observe physically in natural rivers wagener and gupta 2005 kelleher et al 2013 in general the parameter combination is determined using inverse modeling in which a parameter combination producing the best fit result with the observed btc is selected as the optimal parameter combination in other words the parameter combination is usually estimated based on an optimization technique with a certain objective function which can be formulated with various goodness of fit measures dennis et al 1981 wagner and gorelick 1986 runkel and broshears 1991 however in this parameter estimation process more than one parameter combination can generally show similarly good levels of fitting to the observed btc which has been referred to as the equifinality problem beven and binley 1992 kelleher et al 2013 it is known that this phenomenon results in difficulty in identifying a unique parameter combination of tsm and consequently results in the parameter uncertainty in particular the parameters standing for the storage effect have been known for their higher uncertainties than the other parameters related to the advection and shear dispersion mechanisms in the free flowing water zone wagner and harvey 1997 wagener et al 2002 kelleher et al 2013 when estimating parameters of tsm with the inverse modeling the parameter uncertainty has usually been estimated in local space of the optimized value by calculating the confidence interval of parameter based on the assumption that the parameter estimate is normally distributed donaldson and tryon 1990 runkel 1998 ward et al 2017 however this method of assessing uncertainty can be extremely incorrect for a highly nonlinear model even though its application is relatively easy donaldson and tryon 1990 furthermore this method cannot report generalized information on uncertainty across the whole parameter space ward et al 2017 in order to overcome this locality problem wagener et al 2002 camacho and gonzález 2008 and ward et al 2017 estimated the parameter uncertainty of tsm using the generalized likelihood uncertainty estimation glue framework which has been widely applied in the field of rainfall runoff modeling since beven and binley proposed the glue in 1992 in principle the glue was developed based on the bayesian inference in which a posterior distribution of parameter is achieved by combining the prior knowledge of the parameter distribution and the likelihood of describing the newly observed information however the glue has been regarded as a pseudo bayesian inference because it uses the informal likelihood based on some goodness of fit measure rather than the formal likelihood based on classical probability distribution freni and mannina 2012 use of the informal likelihood in glue in the estimation of parameter uncertainty has recently been warned against by several previous studies of mantovan and todini 2006 stedinger et al 2008 and clark et al 2011 the previous studies suggested that the formal likelihood based on the probability distribution representing the error structures should be used in their rainfall runoff modeling examples since the glue with the informal likelihood would overestimate the parameter uncertainty for this reason the recent applications have often employed the formal likelihood rather than the informal likelihood to estimate the parameter uncertainty of various hydrologic models such as the rainfall runoff model mcmillan and clark 2009 schaefli et al 2007 vrugt et al 2009 schoups and vrugt 2010 smith et al 2010 water distribution model hutton et al 2014 and tmdl model hantush and chaudhary 2014 chaudhary and hantush 2017 the aforementioned criticisms have brought about the reasonable doubt that the parameter uncertainty in tsm can also be unsoundly estimated with informal likelihood in the previous studies wagener et al 2002 camacho and gonzález 2008 ward et al 2017 even though the informal likelihood is relatively simple to implement in this context there has been a need to apply the formal likelihood for parameter uncertainty estimation of tsm however as of yet the use of formal likelihood has never been employed in the study of tsm meanwhile the formal likelihood should be carefully formulated to appropriately represent the error structure of a given model and to prevent the biased uncertainty estimation of parameters stedinger et al 2008 schoups and vrugt 2010 in other words an ill formulated formal likelihood might result in a worse estimation of uncertainty than the informal likelihood yang et al 2008 freni and mannina 2012 the previous studies on rainfall runoff modeling proposed different types of the formal likelihood that can capture key statistical characteristics of error structures such as normality autocorrelation and heteroscedasticity for example schaefli et al 2007 and smith et al 2010 used the formal likelihood function of mixture distributions to handle the varying variance of the residual errors in high and low discharge observations separately this heteroscedasticity condition that appeared in the discharge curve can be frequently found in the observed btc of pollutant studies in natural streams the btc usually consists of segments of rising limb peak falling limb and tail as shown in fig 2 a among them the tail part of btc tends to show distinct error structure from the other segments of btc due to its long extension with low concentration as shown in fig 2 b therefore capturing these characteristics of btcs is important in formulating the formal likelihood of tsm this study therefore proposed a bayesian framework with the formal likelihood that can capture the distinct statistical characteristics of the btcs the btc segmentation was a particular interest of this study in which the heteroscedasticity in residual errors as well as the biasedness and autocorrelation were effectively reflected by adopting the likelihood function based on the finite mixture distribution for segmented btc this study also aimed to demonstrate the superiority of the suggested formal likelihood by comparing it with the informal likelihood that has been applied in the uncertainty estimation of tsm 2 materials and methods 2 1 transient storage model tsm was proposed to represent the observed btc that cannot be reproduced properly by the conventional one dimensional advection dispersion 1d ad model of taylor 1954 the observed btc is usually more skewed than the curve simulated by the 1d ad model this is because the 1d ad model simulates the behavior of solutes without considering the storage effect that exists along the river channel as shown in fig 1 the storage zone is a relatively immobile zone compared to the main free flowing water zone where the flow is constantly moving the storage zone is usually made due to irregular channel boundaries such as pools vegetation hydraulic structures and gravel beds when solutes pass by in the river channel some portion of solutes is captured by the storage zone and slowly released back into the main free flowing water zone after the main cloud has already moved downstream which leads to the long tailed and the skewed btc this storage effect was taken into account in tsm by adding the equation of the storage zone to the 1d ad model seo and maxwell 1992 cheong and seo 2003 in tsm the river channel is divided into two conceptual areas the main free flowing water zone and the storage zone as shown in fig 1 b in the main free flowing water zone advection and shear dispersion are the dominant transport mechanisms while the storage zone represents the portion of the stream that contributes to the storage of pollutants runkel 1998 tsm consists of two governing equations that model the different mixing processes in each zone based on the assumptions of steady uniform flow conservative solute and completely mixed storage zone the governing equations can be established as follows bencala and walters 1983 runkel and broshears 1991 runkel 1998 1 c t u f c x x k f c x α c s c 2 c s t α a f a s c c s where c u f k f and a f are the solute concentration mean flow velocity longitudinal dispersion coefficient and area of the main free flowing water zone respectively c s and a s are the solute concentration and area of the storage zone respectively and α is the mass exchange rate between the main free flowing water zone and the storage zone in the tsm governing equations as aforementioned k f a f a s and α are usually treated as the parameters that should be determined by the inverse modeling using the observed btc as explained earlier among the four parameters k f and a f stand for the shear dispersion and advection mechanisms respectively while a s and α represent the storage effect in some cases u f in eq 1 is also treated as a model parameter to be estimated however in this study u f was considered as an input variable that could be measured through field study seo et al 2016 also although the governing equations of the tsm above can be extended to include additional mechanisms such as lateral flow sorption desorption and chemical reaction this study took account of only the main mixing mechanisms such as advection shear dispersion and storage effect which results in four parameters to be estimated in this study the numerical model of the tsm based on eqs 1 and 2 was constructed in matlab code using the finite difference method and the crank nicolson method to approximate the spatial and temporal derivatives respectively runkel 1998 2 2 uncertainty estimation 2 2 1 bayesian inference framework the bayesian inference is a useful framework for obtaining a probability distribution of a model parameter that can be quantified to a degree of uncertainty in the bayesian inference a posterior probability distribution of the model parameter is updated by multiplying a prior distribution of the model parameter and a likelihood as shown in the following equation 3 f θ c c f c θ f θ where θ θ 1 θ d is the parameter vector i e in this study θ k f a f a s α and d 4 c c 1 c t is the vector of observed concentrations in the main free flowing water zone at times t 1 t f θ c is the posterior probability density function for the model parameter vector θ given a set of observed concentration in the main free flowing water zone c c is a normalization constant f c θ is the likelihood function to represent the probability of the observed concentration vector c for a given set of model parameters θ and f θ is the prior probability density function for the model parameter vector θ since the likelihood plays an important role in obtaining the posterior distribution using eq 3 the selection of an appropriate likelihood is critical in uncertainty estimation smith et al 2010 as aforementioned types of likelihood can be classified into formal and informal likelihoods depending on which type of likelihood is used the uncertainty estimation method based on the bayesian inference can be classified into the formal bayesian inference and the glue in the glue the informal likelihood is formulated based on a goodness of fit measure such as root mean square error rmse residual sum of squares rss and nash sutcliffe efficiency nse beven and binley 1992 2014 stedinger et al 2008 in the previous application of the glue for the uncertainty estimation of the tsm wagener et al 2002 camacho and gonzález 2008 ward et al 2017 the rmse based informal likelihood has been widely used as follows 4 f c θ t 1 t c t c t 2 t n 2 where c t and c t are the observed and simulated concentration in the main free flowing water zone at time t respectively t is the number of observed concentration data and n is a shaping factor that is used to put more weight to the parameters that give a better goodness of fit the formal likelihood can be rigorously formulated based on the probabilistic structure of the errors which are the differences between simulated and observed concentrations when we assume that the errors are normally distributed unbiased independent and homoscedastic the formal likelihood can be given as follows 5 f c θ 1 2 π σ ε 2 exp t 1 t ε t 2 2 σ ε 2 1 2 π σ ε 2 exp t 1 t c t c t 2 2 σ ε 2 where ε t is the error between the simulated and observed concentrations in the main free flowing water zone at time t i e ε t c t c t and σ ε 2 is the variance of the errors however in many cases including tsm the assumption of unbiasedness independence and homoscedasticity of the errors can be easily violated in order to solve the problem of biasedness and autocorrelation the first order autoregressive model ar 1 can be adopted as follows sorooshian and dracup 1980 hantush and chaudhary 2014 6 ε t μ ρ ε t 1 μ ω t ω t n 0 σ ω 2 where ρ is the first order autocorrelation coefficient of errors μ is the mean of errors ω t is the residual error at time t which is normally and independently distributed with zero mean and variance σ ω 2 by combining eqs 5 and 6 the formal likelihood reflecting the biasedness and autocorrelation of errors can be derived as follows 7 f c θ 1 2 π σ ω 2 exp t 1 t ε t μ ρ ε t 1 μ 2 2 σ ω 2 in order to rectify the heteroscedasticity of the model residual errors data transformation such as the box cox transformation has been employed in previous studies stedinger et al 2008 vrugt et al 2009 hantush and chaudhary 2014 however this solution can be problematic since the parameter uncertainty can be overestimated by the data transformation schaefli et al 2007 freni and mannina 2012 another solution is to employ the finite mixture distribution which expresses a complex distribution with the weighted sum of normal distributions of different means and variances schaefli et al 2007 smith et al 2010 in this study since the residual error corresponding to the tail segment of btc tended to show distinct variance from that corresponding to the body segment i e the rising limb the peak and the falling limb of btc the segment mixture likelihood hereafter named the sm likelihood in which two different distributions corresponding to the body and the tail segments of btc were combined was suggested as follows 8 f c θ 2 π σ ω 1 2 t 1 2 2 π σ ω 2 2 t 2 2 exp t s 1 e 1 ε t μ 1 ρ 1 ε t 1 μ 1 2 2 σ ω 1 2 t s 2 e 2 ε t μ 2 ρ 2 ε t 1 μ 2 2 2 σ ω 2 2 where subscripts j 1 and 2 stand for the body and tail segments of btc respectively t j is the duration of the segment j s j and e j are the starting and ending times of the segment j i e s 1 1 e 1 t 1 s 2 t 1 1 and e 2 t μ j and ρ j are the mean and first order autocorrelation coefficient of errors of the segment j respectively and σ ω j 2 is the variance of the residual errors of the segment j 2 2 2 parameter uncertainty estimation in order to perform the parameter uncertainty estimation of tsm based on the bayesian inference as shown in eq 3 tsm should be implemented with a large number of the parameter combinations that were sampled by the sampling technique since eq 3 cannot be implemented in an analytical way this study employed two sampling techniques according to the type of likelihood when applying the formal likelihood a differential evolution adaptive metropolis dream sampling technique was used to sample the parameter combinations based on the multi chain markov chain monte carlo mcmc simulation algorithm vrugt et al 2009 meanwhile in the case of informal likelihood a latin hypercube sampling technique lhs was used in lhs parameters were sampled from the equally divided partitions of cumulative probability distributions of the parameters mckay et al 1979 the flow charts in fig 3 explain the simplified procedures of the parameter uncertainty estimation using the dream and the lhs in dream the first step was to determine the prior distributions of the parameters and then the initial parameter combinations θ k 1 m m 1 m were sampled for m chains from the prior distributions after simulating the tsm with the initial parameter combinations the likelihoods of the initial parameter combinations were calculated based on the sm likelihood eq 8 afterwards the evolution of chains began by generating the candidate parameter combinations θ c a n d m via the evolutionary algorithm and by determining to accept the candidate parameter combinations based on the metropolis acceptance probability p a c c the metropolis acceptance probability could be calculated as a ratio of the likelihoods of the previous and the present parameter combinations if the metropolis acceptance probability was larger than the random number drawn from the uniform distribution on the interval 0 1 then the candidate parameter combination was accepted as the k th parameter combination of the m th chain i e θ k m θ c a n d m otherwise the previous parameter combination was added to the chain as the k th parameter combination i e θ k m θ k 1 m this evolution process was performed iteratively until the convergence condition was met i e the gelman rubin convergence diagnostic r ˆ was less than 1 2 the parameter combinations that were achieved after the convergence were used to construct the posterior distribution of parameters in addition to what is described here detailed explanation of the dream can be found in the studies of vrugt et al 2009 and vrugt 2016 when using the lhs in the case of informal likelihoods the prior distributions of the parameters were determined and then n parameter combinations were sampled all at once from the prior distributions after the likelihood for each parameter combination was calculated based on the informal likelihood eq 4 a cutoff value was introduced to distinguish behavioral parameter combinations from non behavioral parameter combinations in which a behavioral parameter combination is defined as a parameter combination that provides a simulation series that is well fitted to the observation with a high degree of accuracy non behavioral parameter combinations were discarded and only the behavioral parameter combinations were used to obtain the posterior distributions of parameters though the parameters of the error distribution such as μ j σ ω j 2 and ρ j in eq 8 should be estimated by using the sampling technique as in the model parameters this study assumed that these values were close to the maximum likelihood estimates mle this study employed the following equations suggested by chaudhary and hantush 2017 in order to reduce the dimensionality of the parameters to be sampled 9 ρ ˆ j t s j e j ε t 1 ε t 1 ε t ε t t s j e j ε t 1 ε t 1 2 10 μ ˆ j 1 t j 1 ρ j t s j e j ε t ρ j ε t 1 11 σ ˆ ω j 2 1 t j t s j e j ε t μ ˆ j ρ ˆ j ε t 1 μ ˆ j 2 where ε t 1 is the sample mean of the one lagged errors from t s j 1 to t e j 1 ε t is the sample mean of the present errors from t s j to t e j and the hat operator denotes the estimator of the corresponding variable 2 2 3 prediction uncertainty estimation in bayesian inference the posterior distribution of predicted concentrations can be derived after obtaining the posterior distribution of parameters by the following equation zellner 1971 12 f p c θ f p θ c f θ c d θ where p p 1 p t is the vector of the predicted concentrations in the main free flowing water zone at times t 1 t f p c is the posterior probability density function for the predicted concentration given c and f p θ c is the posterior probability density function for the predicted concentration given c and θ when using the sm likelihood to estimate the prediction uncertainty based on eq 12 the predicted concentrations were calculated by adding error series ε ε 1 ε t to the simulated concentrations c c 1 c t in order to represent that the uncertainty of predicted concentrations contained both parameter and model uncertainties explicitly stedinger et al 2008 vrugt et al 2009 herein the error at the time t was generated from the assumed error structure of the sm likelihood and added to the simulated concentration at the time t as follows 13 p t c t ε t ε t n μ 1 σ ω 1 2 1 ρ 1 2 when 1 t e 1 n μ 2 σ ω 2 2 1 ρ 2 2 when s 2 t t however in the case of informal likelihood the simulated concentration became the predicted concentration i e p t c t since it was assumed that the model uncertainty was implicitly included in the parameter uncertainty estimated by the informal likelihood so that the model errors were not considered explicitly 3 case study 3 1 synthetic tracer test data three synthetic data were generated to see whether the suggested sm likelihood eq 8 can spot the true parameters within the 95 confidence intervals of parameters regardless of the level of parameter uncertainty thus three synthetic data were devised according to the damkohler number dal that represents the relationship between the rate of storage effect and the rate of advection as follows 14 d a l α 1 a f a s l u f where l is the length of the targeted reach wagner and harvey 1997 showed that parameters were estimated with the smallest uncertainty when dal was close to one while the parameter uncertainty increased when dal was much higher or much lower than one as either the advection or storage effect became to prevail in detail when dal was less than 0 01 the coefficients of variance covs of a s and α became more than 100 times larger compared to the situation when dal was one due to the dominance of advection meanwhile when dal was larger than 10 covs of a s and α increased by more than 10 times due to the dominance of storage effect three types of synthetic data referred to as syn1 syn2 and syn3 were generated corresponding to dal of 1 0 01 and 10 respectively as shown in fig 4 in this study the procedure to generate synthetic btc data could be largely divided into two steps first btcs were generated by tsm using the assumed input variables and parameters given in table 1 these values were determined within the reasonable parameter ranges according to the previous studies to satisfy the specific dal values the numerical parameters δ x and δ t which represent the length of spatial intervals consisting of the target reach and the time step respectively were also selected to satisfy the numerical stability condition based on the courant number ward et al 2017 in the second step the errors were generated by sampling from the assumed error distributions that had the parameters given in table 1 and added to generated btcs to mimic the characteristics of the real data 3 2 real tracer test data the real tracer test data used in this study was achieved in uvas creek which is a small stream located in santa clara county california usa zand et al 1976 avanzino et al 1984 the uvas creek tracer test data was selected for application of the proposed method because it has been a benchmark data in the study of river mixing since bencala and walters 1983 used it for tsm application ward et al 2017 also employed the uvas creek data for uncertainty estimation of tsm using the glue as shown in fig 5 the tracer test was conducted in a 635 m experimental reach of uvas creek on september 26 1972 avanzino et al 1984 the study reach consists of pool and riffle sequences and the riverbed is composed of bed materials greater than 4 mm in diameter during the tracer test the average daily streamflow was relatively constant at 0 0125 m3 s as a tracer chloride was injected at a constant rate for 3 h at the upstream of the study reach before the injection the background concentration of chloride was measured as 3 7 mg l chloride concentrations were monitored at five sections of uv s1 uv s2 uv s3 uv s4 and uv s5 which were located at 38 105 281 433 and 619 m downstream from the injection point respectively as shown in fig 5 a water samples were collected to measure chloride concentration from 30 min before the injection to 27 h after the injection fig 5 b shows the measured chloride concentration curves at the five sections for uncertainty estimation the data achieved in the uv s1 and uv s5 sections were used as an upstream boundary condition and downstream observation data respectively the modeling scheme was chosen the same as the study of ward et al 2017 4 results 4 1 application to synthetic data the sm likelihood based on the mixture distribution eq 8 was applied for uncertainty estimation of the synthesized data herein the likelihood was calculated on logarithmic scale the prior distributions of parameters were assumed as uniform distributions and the parameter ranges are given in table 2 the dream was applied to sample the parameter combinations the numbers of sampling required to satisfy the convergence condition were 3 000 6 500 and 20 800 for syn1 syn2 and syn3 cases respectively after the convergence the respective 10 000 parameter combinations for the three cases were sampled to obtain the posterior distributions the statistics of parameter posterior distributions were calculated as listed in table 3 the calculated covs in table 3 demonstrate that the parameter uncertainty changed according to the dal which was consistent with the study of wagner and harvey 1997 when dal changed from 1 to 0 01 the covs of a s and α increased by almost seven and sixty times respectively and the cov of a f decreased by almost eight times these results indicate that the uncertainty of parameters related to the storage effect significantly increased while the advection related parameter became less uncertain when the advection became dominant meanwhile when dal increased from 1 to 10 the cov of a f increased six times due to the dominance of the storage effect the uncertainty of the other parameters especially α also increased greatly since the mass exchange rate increased to the level of equilibrium condition in which the storage mechanism can be explained by the longitudinal dispersion leading to difficulties in identifying the parameters harvey et al 1996 wagner and harvey 1997 in order to compare the true and the expected values of the posterior distributions the percent errors were calculated as shown in table 3 in the case of syn1 the percent errors of four parameters were less than 4 which showed that the expected values were estimated similarly to the true values of parameters the case of syn2 demonstrated that the percent errors of a s and α significantly increased while those values of a f and k f were less than 4 meanwhile in the case of syn3 the percent errors of all parameters increased in general and in particular the expected values of α deviated substantially from the true values the trend of the percent errors was in accord with that of the parameter uncertainty in which the highly uncertain parameters showed large percent errors however despite the discrepancy between the true and estimated values of parameters in syn2 and syn3 it was shown that the 95 confidence interval of parameters in all cases included the true values within the ranges this indicated that the 95 confidence interval estimated by using the sm likelihood could capture the true parameter value well regardless of the level of parameter uncertainty in addition the uncertainty estimation was validated by calculating the coverage rate which could be calculated as the percentage of the observations that are included within the 95 prediction interval yadav et al 2007 smith and marshall 2010 herein the ideal value of the coverage rate corresponded to the value of the desired interval 95 smith et al 2010 the coverage rates were calculated as 93 3 93 5 and 94 2 for syn1 syn2 and syn3 respectively as shown in table 3 the calculated coverage rates could approximate the ideal values which suggests that the sm likelihood can perform the uncertainty estimation of tsm well when applying it to the various synthetic data having different levels of uncertainty 4 2 application to the real tracer test in uvas creek in application to the uvas creek tracer test data besides the sm likelihood four types of rmse based informal likelihood were also applied among them two rmse based informal likelihoods were applied in particular to compare the results of parameter uncertainty estimation of the sm likelihood with those of the rmse based likelihoods that were employed in the previous study of ward et al 2017 rmse 1 1 which had the cutoff value of the top 1 and the shaping factor of 1 and rmse 10 1 which had the cutoff value of the top 10 and the shaping factor of 1 besides two additional rmse based informal likelihoods were applied to investigate whether the capacity of the informal likelihood could be improved by controlling the shaping factor of the rmse based informal likelihood these informal likelihoods rmse 1 6 and rmse 10 6 had the same shaping factors of 6 but had the cutoff values of the top 1 and 10 respectively the prior distributions of parameters were assumed as uniform distributions spanning the parameter ranges as shown in table 2 the parameter ranges used in this study were determined based on the previous study of ward et al 2017 from the dotty plots of parameters and the corresponding rmse that were estimated by ward et al 2017 we could narrow down the ranges of parameters by discarding the non behavioral parameter ranges where the distribution of scatter points appears flat when dream sampling technique was employed to use the sm likelihood the convergence was fulfilled when the number of sampled parameters reached 48 000 additional parameter combinations of 10 000 were sampled after the convergence was met the number of sampling necessary for the convergence was larger in this case than the case of synthetic data because there could be more uncertain sources in the data of real tracer test than in the synthetic data sets meanwhile lhs sampling technique was used to sample the 50 000 parameters in order to apply the rmse based informal likelihoods 4 2 1 estimation of parameter uncertainty by applying the suggested sm likelihoods and the four rmse based informal likelihoods we obtained the cumulative distributions of the four parameters of tsm from the cumulative distributions the statistics of the four parameters were calculated and plotted in boxplots as shown in fig 6 in the boxplots the center vertical line in the box is the median of the data the left and right of the box are the 25th and 75th percentiles respectively and the ends of the whiskers are the 2 5th and 97 5th percentiles so that the range between whiskers shows the 95 confidence interval this figure shows that the uncertainties of the tsm parameters estimated by the sm likelihood were smaller than those estimated by any type of the informal likelihoods which was also reported in the previous studies of hydrological modeling stedinger et al 2008 vrugt et al 2009 in the cases of informal likelihoods as the cutoff values changed from the top 1 to the top 10 the parameter uncertainty increased as the 95 confidence interval got wider these results happened because the number of parameters that were treated as the behavioral parameters increased which resulted in wider confidence intervals on the other hand when the shaping factor increased from 1 to 6 the parameters with smaller likelihood became neglected in constructing the confidence intervals by emphasizing the parameters with the larger likelihoods which led to the decrease of the 95 confidence intervals of the parameters as with these results when using the informal likelihood the parameter uncertainty can be estimated differently depending on the cutoff values and the shaping factors that are usually determined subjectively another notable result of the parameter uncertainty is shown in the statistical summary listed in table 4 among the four parameters the least uncertain parameter was found to be a f which showed the smallest cov regardless of the type of likelihood in the meantime two parameters of the storage effect a s and α were revealed to be the most uncertain parameters of which the cov showed different values ranging from 0 267 to 2 138 depending upon the type of likelihood among the five likelihoods the sm likelihood produced the smallest covs for a s and α the uncertainty of k f was found to show cov ranging from 0 248 to 1 530 table 4 also lists the dal calculated using the expected values of the parameters in which dal calculated by sm likelihood was less than unity while dal of all rmse based informal likelihood ranged from 1 11 to 10 93 this result indicates that the sm likelihood characterized the mixing processes in the uvas creek as the advection dominated mechanism whereas the rmse based informal likelihood interpreted the mixing process as a storage effect dominated process these results demonstrate that the selection of likelihood type was significantly important in characterizing the mixing characteristics of natural rivers based on parameter uncertainty estimation in particular when using the informal likelihoods subjective judgments which were taken to determine the form of the informal likelihood such as cutoff value and shaping factor could result in the incorrect representation of the mixing characteristics of natural rivers 4 2 2 validation based on model prediction uncertainty in this study to analyze the model prediction uncertainties by the sm and rmse based informal likelihoods the 95 prediction interval of each likelihood was estimated herein in addition to coverage rates the mean widths of the 95 prediction intervals were also calculated the mean width was obtained by averaging the width of the 95 prediction interval over time which can stand for the size of the estimated uncertainty of the prediction results table 5 summarizes the estimated coverage rates and mean widths the uncertainty of prediction results generally followed the trend of the parameter uncertainty according to the type of likelihood among the informal likelihoods rmse 1 1 rmse 10 1 and rmse 10 6 which showed larger parameter uncertainty than the sm likelihood also presented wider 95 prediction intervals than the sm likelihood on the other hand the mean width of rmse 1 6 was similar to that of the sm likelihood despite the larger parameter uncertainty this was because the predicted concentration of sm likelihood was estimated by explicitly adding the model error to the simulated concentration while that of rmse 1 6 was not as shown in eq 13 when comparing the four informal likelihoods with each other the mean width increased as the cutoff value and the shaping factor varied from the top 1 to the top 10 and from 6 to 1 respectively which reflected the trend of the parameter uncertainty in general it can be thought that the wider prediction interval leads to the increase in coverage rate however the four informal likelihoods yielded lower coverage rates than the sm likelihood despite their wider or similar mean widths the coverage rates of the four informal likelihoods were much less than the ideal coverage rate of 95 in particular the rmse 10 1 showed the largest mean width but the lowest coverage rate given that the mean widths were large but the coverage rates were low in the informal likelihoods it seems that the informal likelihoods estimated the posterior distributions of the parameters incorrectly in deviated directions from the spaces where the true behavioral parameters existed when applying the sm likelihood the coverage rate increased to 92 5 which was close to 95 although it yielded the narrower 95 prediction interval than the informal likelihoods this result meant that the sm likelihood could result in more appropriate uncertainty estimation of the parameter and the prediction because the 95 prediction interval of the sm likelihood covered the observation well and did so tightly without overestimation 5 discussion the validation in the previous section showed that the sm likelihood yielded the better uncertainty estimation of tsm compared to the rmse based informal likelihood the reasons for the successful application of the sm likelihood can be summarized as 1 appropriate assumptions of the error distribution 2 better reflection of the information in the entire segments of btc and 3 sound judgment on the behavioral parameters the sm likelihood could result in successful uncertainty estimation thanks to the proper assumptions of the error distribution the accurate assumptions of the error distribution are important for robust uncertainty estimation since false assumptions can lead to erroneous uncertainty estimation stedinger et al 2008 smith et al 2010 fig 7 shows the error diagnostics for two formal likelihoods representing different characteristics of the error in which the ns not segmented likelihood of eq 7 assumed the homoscedastic residual error distribution along the whole btc while the sm likelihood of eq 8 segmented the btc and assumed the heteroscedasticity of the residual error distribution between the body and tail segments fig 7 a and b show the error diagnostics for the ns likelihood herein quantile quantile qq plots were used to check whether the residual errors follow the assumed distribution in the case of ns likelihood the variance of residual errors as shown in fig 7 a fluctuated quite substantially for different values of concentration of the btc which implied that the residual errors did not follow the assumptions of homoscedasticity the qq plot in fig 7 b also shows that the residual errors did not satisfy the assumed distribution either by not following the linear trend of the dotted line however the results of sm likelihood revealed that the heteroscedasticity was successfully rectified by dividing the btc into the body and tail segments as shown in fig 7 c and e not only that the assumption of the distribution of the residual errors became more satisfied by showing the more linear trend of the qq plots as shown in fig 7 d and f the error diagnostics showed that it was rational to divide the btc into the body and tail segment and to use the different distribution of the errors for each segment as the sm likelihood adopted table 5 demonstrates that the sm likelihood generated higher coverage rate in the model prediction compared to the rmse based informal likelihood by reflecting the information of the entire segments of btc this table shows that the coverage rates of sm likelihood for both body and tail segments were higher than those of rmse based informal likelihoods even though it was seen that the 95 prediction intervals of both likelihoods missed a lot of observations in the tail segment in particular the prediction intervals estimated by the rmse based informal likelihoods missed the larger number of the observations in the tail segment the failures of the rmse based informal likelihoods in explaining the observation of the tail segment can be attributed to the property of rmse in which it tended to focus on fitting the higher concentration in the peak of btc while ignoring the information contained in the low concentration in the tail segment kelleher et al 2013 therefore the rmse based informal likelihoods could not explain well the observation in the tail segment on the other hand the sm likelihood could capture more observations in the tail segment than the rmse based informal likelihood which implied that the sm likelihood was better able to reflect the information in the tail segment the better performance of the sm likelihood compared to the rmse based informal likelihood was manifested by estimating the parameter uncertainty using the tail truncated btc data which corresponded to the data of only the body segment as shown in fig 5 b fig 8 shows the boxplots of the posterior distributions of the parameters that were estimated by assuming that the tail of btc was truncated during the tracer test in this case the parameter uncertainty estimation was performed using the two likelihoods the sm and the rmse 10 1 likelihood and the results were compared with the results of two likelihoods using the original whole btc which are shown in fig 6 as a result when the parameter uncertainty was estimated by the sm likelihood there were significant changes in the medians and the 95 confidence intervals of the storage zone effect related parameters a s and α however when using the rmse 10 1 likelihood the medians and the 95 confidence intervals were not varying regardless of the existence of the data of the tail segment this insensitivity of a s and α to the absence of the data in the tail segment when using the rmse 10 1 was unreasonable since the data in the tail segment contained significant information about the storage effect compared to the body segment of the btc wagner and harvey 1997 wagener et al 2002 and kelleher et al 2013 have demonstrated that the parameters related to the storage effect a s and α were the most sensitive to the data of the tail segment via the sensitivity analysis thus this result demonstrates that the use of the sm likelihood estimates the parameter uncertainty accurately by properly reflecting the information contained in the tail segment of btc the robust uncertainty estimation of the sm likelihood could also be attributed to the ability to distinguish the behavioral parameters from the non behavioral parameters on the other hand the rmse based informal likelihood misjudged the non behavioral parameters as behavioral parameters due to the predetermined cutoff value as indicated by the wider confidence intervals of parameters and larger mean widths compared to the sm likelihood this argument can be corroborated by the interaction plots shown in fig 9 fig 9 a shows the interaction of the parameters that were included in the 95 confidence intervals estimated by the sm likelihood in these plots the clear correlation relationships appear in the relationships between k f a f and α in particular k f and α had a negative correlation since these two parameters are in a relationship of trade off interaction this result clearly demonstrates that the mixing in natural rivers happens by two causes the shear flow and storage zone effect which in turn results in a trade off between the dispersion due to the shear flow and mixing due to the storage zone effect meanwhile fig 9 b presents the interaction relationships between the parameters that were included in the 95 confidence intervals when applying the rmse 10 1 unlike the case of sm likelihood no clear relationships were shown this is because the non behavioral parameters were selected as the behavioral parameters which obscured the obvious interaction relationships among the parameters 6 conclusion in this study the bayesian inference with formal likelihood was applied to estimate the parameter uncertainty of tsm and the result was compared with that of informal likelihood this study proposed the segment mixture sm likelihood based on the finite mixture distribution that consists of two error distributions that correspond to the body and tail segments of btc respectively the proposed sm likelihood was tested by applying it to the synthetic tracer test data and the real stream data of the uvas creek and by comparing the uncertainty estimation results with the results of the informal likelihood the results when applying the sm likelihood to the three synthetic data showed that the sm likelihood could capture the true parameters within the estimated 95 confidence intervals regardless of the level of parameter uncertainty the properness of the sm likelihood was also validated by calculating the coverage rates of 95 prediction intervals the coverage rates of all synthetic data were calculated as close to the ideal value of 95 similarly in the case of uvas creek the sm likelihood yielded 95 prediction interval which strictly and well covered the observation with the coverage rate of about 92 5 therefore the sm likelihood was found to perform appropriately for the parameter and prediction uncertainty of tsm in contrast the rmse based informal likelihoods estimated the parameter uncertainty incorrectly when compared to the sm likelihood all of the rmse based informal likelihoods gave much fewer coverage rates than the 95 although these likelihoods yielded larger mean widths than the sm likelihood the successful uncertainty estimation of sm likelihood compared to the rmse based likelihoods could be attributed to three properties of the sm likelihood first the sm likelihood represented the error characteristics of btc for tsm appropriately as shown in the error diagnostic tests in particular the sm likelihood could represent the heteroscedasticity of the residual errors by segmenting the btc into the body and tail parts second the sm likelihood explained the observation along the btc well by combining comprehensive information from the whole segments of btc such as the body and tail segments on the other hand the rmse based informal likelihoods could not employ the information of the tail segments which is useful in estimating the storage effect related parameters when comparing the parameter uncertainties estimated by the sm and rmse based likelihoods using the tail truncated btc the 95 confidence intervals of the parameters estimated by the rmse based likelihood were insensitive to the nonexistence of the data in the tail segment while the sm likelihood was not also the sm likelihood was successful in distinguishing the behavioral parameter and non behavioral parameter and showed the clear correlations between the parameters in the interaction plots meanwhile the rmse based informal likelihood tended to misclassify the non behavioral parameters as behavioral parameters which could lead to the incorrect parameter and prediction uncertainty estimation when it comes to the uncertainty estimation of tsm it is important to choose appropriate likelihood since the selection of inappropriate likelihood can give rise to incorrect interpretation of the river mixing characterization and degradation of the accurate prediction of the pollutants mixing in that sense the sm likelihood that was suggested in this study can be a good option for the likelihood in uncertainty estimation of the tsm software data availability the tsm developed in matlab is available from the link http ehlab snu ac kr and the matlab toolbox of dream vrugt 2016 is available from the link https faculty sites uci edu jasper software the tracer data of uvas creek was taken from avanzino et al 1984 and the synthetic data can be downloaded from the link http ehlab snu ac kr acknowledgments this research is supported by chemical accident response r d program 2018001960001 of korea ministry of environment moe and the bk21 plus research program of the national research foundation of korea this research work was conducted at the institute of engineering research and institute of construction and environmental engineering in seoul national university seoul korea 
26100,the transient storage model tsm for the analysis of pollutant mixing in rivers has been hampered by parameter uncertainties due to the equifinality problem the generalized uncertainty estimation method which was frequently used to quantify the parameter uncertainty of tsm has been criticized because this method uses informal likelihood which can cause overestimation of the uncertainty thus in this study we suggest a bayesian inference method using a segment mixture sm likelihood which is a formal likelihood based on the mixture distribution of the segmented breakthrough curve the uncertainty estimation was conducted using three synthetic data and the real tracer test data achieved from uvas creek in the usa the results show that the sm likelihood estimated uncertainties of tsm appropriately by correctly representing the error distribution of the tsm and identifying the behavioral parameters keywords transient storage model parameter uncertainty estimation bayesian inference formal likelihood glue breakthrough curve segmentation 1 introduction in river mixing studies the one dimensional transient storage model tsm has been frequently used to simulate behaviors of solute transport since it was proposed by hays in 1966 hays 1966 fischer et al 1979 nordin and troutman 1980 bencala and walters 1983 bencala 1984 seo and maxwell 1992 runkel and chapra 1993 cheong and seo 2003 cheong et al 2007 camacho and gonzález 2008 kelleher et al 2013 ward et al 2017 the reason for the popularity of the tsm is that it can accurately reproduce the observed breakthrough curve btc which is characterized by its long tail via explaining river mixing processes with three important mixing mechanisms advection shear dispersion and storage effect however apart from its successful representation it has been pointed out that the parameters of tsm are highly uncertain to estimate which would lead to difficulty in characterizing river mixing processes and the application of tsm for prediction harvey and bencala 1993 harvey et al 1996 wagner and harvey 1997 wagener et al 2002 kelleher et al 2013 ward et al 2017 in tsm there are four parameters that represent different river mixing mechanisms such as advection shear dispersion and storage effect as shown in fig 1 these parameters should be estimated indirectly since their values are hard to observe physically in natural rivers wagener and gupta 2005 kelleher et al 2013 in general the parameter combination is determined using inverse modeling in which a parameter combination producing the best fit result with the observed btc is selected as the optimal parameter combination in other words the parameter combination is usually estimated based on an optimization technique with a certain objective function which can be formulated with various goodness of fit measures dennis et al 1981 wagner and gorelick 1986 runkel and broshears 1991 however in this parameter estimation process more than one parameter combination can generally show similarly good levels of fitting to the observed btc which has been referred to as the equifinality problem beven and binley 1992 kelleher et al 2013 it is known that this phenomenon results in difficulty in identifying a unique parameter combination of tsm and consequently results in the parameter uncertainty in particular the parameters standing for the storage effect have been known for their higher uncertainties than the other parameters related to the advection and shear dispersion mechanisms in the free flowing water zone wagner and harvey 1997 wagener et al 2002 kelleher et al 2013 when estimating parameters of tsm with the inverse modeling the parameter uncertainty has usually been estimated in local space of the optimized value by calculating the confidence interval of parameter based on the assumption that the parameter estimate is normally distributed donaldson and tryon 1990 runkel 1998 ward et al 2017 however this method of assessing uncertainty can be extremely incorrect for a highly nonlinear model even though its application is relatively easy donaldson and tryon 1990 furthermore this method cannot report generalized information on uncertainty across the whole parameter space ward et al 2017 in order to overcome this locality problem wagener et al 2002 camacho and gonzález 2008 and ward et al 2017 estimated the parameter uncertainty of tsm using the generalized likelihood uncertainty estimation glue framework which has been widely applied in the field of rainfall runoff modeling since beven and binley proposed the glue in 1992 in principle the glue was developed based on the bayesian inference in which a posterior distribution of parameter is achieved by combining the prior knowledge of the parameter distribution and the likelihood of describing the newly observed information however the glue has been regarded as a pseudo bayesian inference because it uses the informal likelihood based on some goodness of fit measure rather than the formal likelihood based on classical probability distribution freni and mannina 2012 use of the informal likelihood in glue in the estimation of parameter uncertainty has recently been warned against by several previous studies of mantovan and todini 2006 stedinger et al 2008 and clark et al 2011 the previous studies suggested that the formal likelihood based on the probability distribution representing the error structures should be used in their rainfall runoff modeling examples since the glue with the informal likelihood would overestimate the parameter uncertainty for this reason the recent applications have often employed the formal likelihood rather than the informal likelihood to estimate the parameter uncertainty of various hydrologic models such as the rainfall runoff model mcmillan and clark 2009 schaefli et al 2007 vrugt et al 2009 schoups and vrugt 2010 smith et al 2010 water distribution model hutton et al 2014 and tmdl model hantush and chaudhary 2014 chaudhary and hantush 2017 the aforementioned criticisms have brought about the reasonable doubt that the parameter uncertainty in tsm can also be unsoundly estimated with informal likelihood in the previous studies wagener et al 2002 camacho and gonzález 2008 ward et al 2017 even though the informal likelihood is relatively simple to implement in this context there has been a need to apply the formal likelihood for parameter uncertainty estimation of tsm however as of yet the use of formal likelihood has never been employed in the study of tsm meanwhile the formal likelihood should be carefully formulated to appropriately represent the error structure of a given model and to prevent the biased uncertainty estimation of parameters stedinger et al 2008 schoups and vrugt 2010 in other words an ill formulated formal likelihood might result in a worse estimation of uncertainty than the informal likelihood yang et al 2008 freni and mannina 2012 the previous studies on rainfall runoff modeling proposed different types of the formal likelihood that can capture key statistical characteristics of error structures such as normality autocorrelation and heteroscedasticity for example schaefli et al 2007 and smith et al 2010 used the formal likelihood function of mixture distributions to handle the varying variance of the residual errors in high and low discharge observations separately this heteroscedasticity condition that appeared in the discharge curve can be frequently found in the observed btc of pollutant studies in natural streams the btc usually consists of segments of rising limb peak falling limb and tail as shown in fig 2 a among them the tail part of btc tends to show distinct error structure from the other segments of btc due to its long extension with low concentration as shown in fig 2 b therefore capturing these characteristics of btcs is important in formulating the formal likelihood of tsm this study therefore proposed a bayesian framework with the formal likelihood that can capture the distinct statistical characteristics of the btcs the btc segmentation was a particular interest of this study in which the heteroscedasticity in residual errors as well as the biasedness and autocorrelation were effectively reflected by adopting the likelihood function based on the finite mixture distribution for segmented btc this study also aimed to demonstrate the superiority of the suggested formal likelihood by comparing it with the informal likelihood that has been applied in the uncertainty estimation of tsm 2 materials and methods 2 1 transient storage model tsm was proposed to represent the observed btc that cannot be reproduced properly by the conventional one dimensional advection dispersion 1d ad model of taylor 1954 the observed btc is usually more skewed than the curve simulated by the 1d ad model this is because the 1d ad model simulates the behavior of solutes without considering the storage effect that exists along the river channel as shown in fig 1 the storage zone is a relatively immobile zone compared to the main free flowing water zone where the flow is constantly moving the storage zone is usually made due to irregular channel boundaries such as pools vegetation hydraulic structures and gravel beds when solutes pass by in the river channel some portion of solutes is captured by the storage zone and slowly released back into the main free flowing water zone after the main cloud has already moved downstream which leads to the long tailed and the skewed btc this storage effect was taken into account in tsm by adding the equation of the storage zone to the 1d ad model seo and maxwell 1992 cheong and seo 2003 in tsm the river channel is divided into two conceptual areas the main free flowing water zone and the storage zone as shown in fig 1 b in the main free flowing water zone advection and shear dispersion are the dominant transport mechanisms while the storage zone represents the portion of the stream that contributes to the storage of pollutants runkel 1998 tsm consists of two governing equations that model the different mixing processes in each zone based on the assumptions of steady uniform flow conservative solute and completely mixed storage zone the governing equations can be established as follows bencala and walters 1983 runkel and broshears 1991 runkel 1998 1 c t u f c x x k f c x α c s c 2 c s t α a f a s c c s where c u f k f and a f are the solute concentration mean flow velocity longitudinal dispersion coefficient and area of the main free flowing water zone respectively c s and a s are the solute concentration and area of the storage zone respectively and α is the mass exchange rate between the main free flowing water zone and the storage zone in the tsm governing equations as aforementioned k f a f a s and α are usually treated as the parameters that should be determined by the inverse modeling using the observed btc as explained earlier among the four parameters k f and a f stand for the shear dispersion and advection mechanisms respectively while a s and α represent the storage effect in some cases u f in eq 1 is also treated as a model parameter to be estimated however in this study u f was considered as an input variable that could be measured through field study seo et al 2016 also although the governing equations of the tsm above can be extended to include additional mechanisms such as lateral flow sorption desorption and chemical reaction this study took account of only the main mixing mechanisms such as advection shear dispersion and storage effect which results in four parameters to be estimated in this study the numerical model of the tsm based on eqs 1 and 2 was constructed in matlab code using the finite difference method and the crank nicolson method to approximate the spatial and temporal derivatives respectively runkel 1998 2 2 uncertainty estimation 2 2 1 bayesian inference framework the bayesian inference is a useful framework for obtaining a probability distribution of a model parameter that can be quantified to a degree of uncertainty in the bayesian inference a posterior probability distribution of the model parameter is updated by multiplying a prior distribution of the model parameter and a likelihood as shown in the following equation 3 f θ c c f c θ f θ where θ θ 1 θ d is the parameter vector i e in this study θ k f a f a s α and d 4 c c 1 c t is the vector of observed concentrations in the main free flowing water zone at times t 1 t f θ c is the posterior probability density function for the model parameter vector θ given a set of observed concentration in the main free flowing water zone c c is a normalization constant f c θ is the likelihood function to represent the probability of the observed concentration vector c for a given set of model parameters θ and f θ is the prior probability density function for the model parameter vector θ since the likelihood plays an important role in obtaining the posterior distribution using eq 3 the selection of an appropriate likelihood is critical in uncertainty estimation smith et al 2010 as aforementioned types of likelihood can be classified into formal and informal likelihoods depending on which type of likelihood is used the uncertainty estimation method based on the bayesian inference can be classified into the formal bayesian inference and the glue in the glue the informal likelihood is formulated based on a goodness of fit measure such as root mean square error rmse residual sum of squares rss and nash sutcliffe efficiency nse beven and binley 1992 2014 stedinger et al 2008 in the previous application of the glue for the uncertainty estimation of the tsm wagener et al 2002 camacho and gonzález 2008 ward et al 2017 the rmse based informal likelihood has been widely used as follows 4 f c θ t 1 t c t c t 2 t n 2 where c t and c t are the observed and simulated concentration in the main free flowing water zone at time t respectively t is the number of observed concentration data and n is a shaping factor that is used to put more weight to the parameters that give a better goodness of fit the formal likelihood can be rigorously formulated based on the probabilistic structure of the errors which are the differences between simulated and observed concentrations when we assume that the errors are normally distributed unbiased independent and homoscedastic the formal likelihood can be given as follows 5 f c θ 1 2 π σ ε 2 exp t 1 t ε t 2 2 σ ε 2 1 2 π σ ε 2 exp t 1 t c t c t 2 2 σ ε 2 where ε t is the error between the simulated and observed concentrations in the main free flowing water zone at time t i e ε t c t c t and σ ε 2 is the variance of the errors however in many cases including tsm the assumption of unbiasedness independence and homoscedasticity of the errors can be easily violated in order to solve the problem of biasedness and autocorrelation the first order autoregressive model ar 1 can be adopted as follows sorooshian and dracup 1980 hantush and chaudhary 2014 6 ε t μ ρ ε t 1 μ ω t ω t n 0 σ ω 2 where ρ is the first order autocorrelation coefficient of errors μ is the mean of errors ω t is the residual error at time t which is normally and independently distributed with zero mean and variance σ ω 2 by combining eqs 5 and 6 the formal likelihood reflecting the biasedness and autocorrelation of errors can be derived as follows 7 f c θ 1 2 π σ ω 2 exp t 1 t ε t μ ρ ε t 1 μ 2 2 σ ω 2 in order to rectify the heteroscedasticity of the model residual errors data transformation such as the box cox transformation has been employed in previous studies stedinger et al 2008 vrugt et al 2009 hantush and chaudhary 2014 however this solution can be problematic since the parameter uncertainty can be overestimated by the data transformation schaefli et al 2007 freni and mannina 2012 another solution is to employ the finite mixture distribution which expresses a complex distribution with the weighted sum of normal distributions of different means and variances schaefli et al 2007 smith et al 2010 in this study since the residual error corresponding to the tail segment of btc tended to show distinct variance from that corresponding to the body segment i e the rising limb the peak and the falling limb of btc the segment mixture likelihood hereafter named the sm likelihood in which two different distributions corresponding to the body and the tail segments of btc were combined was suggested as follows 8 f c θ 2 π σ ω 1 2 t 1 2 2 π σ ω 2 2 t 2 2 exp t s 1 e 1 ε t μ 1 ρ 1 ε t 1 μ 1 2 2 σ ω 1 2 t s 2 e 2 ε t μ 2 ρ 2 ε t 1 μ 2 2 2 σ ω 2 2 where subscripts j 1 and 2 stand for the body and tail segments of btc respectively t j is the duration of the segment j s j and e j are the starting and ending times of the segment j i e s 1 1 e 1 t 1 s 2 t 1 1 and e 2 t μ j and ρ j are the mean and first order autocorrelation coefficient of errors of the segment j respectively and σ ω j 2 is the variance of the residual errors of the segment j 2 2 2 parameter uncertainty estimation in order to perform the parameter uncertainty estimation of tsm based on the bayesian inference as shown in eq 3 tsm should be implemented with a large number of the parameter combinations that were sampled by the sampling technique since eq 3 cannot be implemented in an analytical way this study employed two sampling techniques according to the type of likelihood when applying the formal likelihood a differential evolution adaptive metropolis dream sampling technique was used to sample the parameter combinations based on the multi chain markov chain monte carlo mcmc simulation algorithm vrugt et al 2009 meanwhile in the case of informal likelihood a latin hypercube sampling technique lhs was used in lhs parameters were sampled from the equally divided partitions of cumulative probability distributions of the parameters mckay et al 1979 the flow charts in fig 3 explain the simplified procedures of the parameter uncertainty estimation using the dream and the lhs in dream the first step was to determine the prior distributions of the parameters and then the initial parameter combinations θ k 1 m m 1 m were sampled for m chains from the prior distributions after simulating the tsm with the initial parameter combinations the likelihoods of the initial parameter combinations were calculated based on the sm likelihood eq 8 afterwards the evolution of chains began by generating the candidate parameter combinations θ c a n d m via the evolutionary algorithm and by determining to accept the candidate parameter combinations based on the metropolis acceptance probability p a c c the metropolis acceptance probability could be calculated as a ratio of the likelihoods of the previous and the present parameter combinations if the metropolis acceptance probability was larger than the random number drawn from the uniform distribution on the interval 0 1 then the candidate parameter combination was accepted as the k th parameter combination of the m th chain i e θ k m θ c a n d m otherwise the previous parameter combination was added to the chain as the k th parameter combination i e θ k m θ k 1 m this evolution process was performed iteratively until the convergence condition was met i e the gelman rubin convergence diagnostic r ˆ was less than 1 2 the parameter combinations that were achieved after the convergence were used to construct the posterior distribution of parameters in addition to what is described here detailed explanation of the dream can be found in the studies of vrugt et al 2009 and vrugt 2016 when using the lhs in the case of informal likelihoods the prior distributions of the parameters were determined and then n parameter combinations were sampled all at once from the prior distributions after the likelihood for each parameter combination was calculated based on the informal likelihood eq 4 a cutoff value was introduced to distinguish behavioral parameter combinations from non behavioral parameter combinations in which a behavioral parameter combination is defined as a parameter combination that provides a simulation series that is well fitted to the observation with a high degree of accuracy non behavioral parameter combinations were discarded and only the behavioral parameter combinations were used to obtain the posterior distributions of parameters though the parameters of the error distribution such as μ j σ ω j 2 and ρ j in eq 8 should be estimated by using the sampling technique as in the model parameters this study assumed that these values were close to the maximum likelihood estimates mle this study employed the following equations suggested by chaudhary and hantush 2017 in order to reduce the dimensionality of the parameters to be sampled 9 ρ ˆ j t s j e j ε t 1 ε t 1 ε t ε t t s j e j ε t 1 ε t 1 2 10 μ ˆ j 1 t j 1 ρ j t s j e j ε t ρ j ε t 1 11 σ ˆ ω j 2 1 t j t s j e j ε t μ ˆ j ρ ˆ j ε t 1 μ ˆ j 2 where ε t 1 is the sample mean of the one lagged errors from t s j 1 to t e j 1 ε t is the sample mean of the present errors from t s j to t e j and the hat operator denotes the estimator of the corresponding variable 2 2 3 prediction uncertainty estimation in bayesian inference the posterior distribution of predicted concentrations can be derived after obtaining the posterior distribution of parameters by the following equation zellner 1971 12 f p c θ f p θ c f θ c d θ where p p 1 p t is the vector of the predicted concentrations in the main free flowing water zone at times t 1 t f p c is the posterior probability density function for the predicted concentration given c and f p θ c is the posterior probability density function for the predicted concentration given c and θ when using the sm likelihood to estimate the prediction uncertainty based on eq 12 the predicted concentrations were calculated by adding error series ε ε 1 ε t to the simulated concentrations c c 1 c t in order to represent that the uncertainty of predicted concentrations contained both parameter and model uncertainties explicitly stedinger et al 2008 vrugt et al 2009 herein the error at the time t was generated from the assumed error structure of the sm likelihood and added to the simulated concentration at the time t as follows 13 p t c t ε t ε t n μ 1 σ ω 1 2 1 ρ 1 2 when 1 t e 1 n μ 2 σ ω 2 2 1 ρ 2 2 when s 2 t t however in the case of informal likelihood the simulated concentration became the predicted concentration i e p t c t since it was assumed that the model uncertainty was implicitly included in the parameter uncertainty estimated by the informal likelihood so that the model errors were not considered explicitly 3 case study 3 1 synthetic tracer test data three synthetic data were generated to see whether the suggested sm likelihood eq 8 can spot the true parameters within the 95 confidence intervals of parameters regardless of the level of parameter uncertainty thus three synthetic data were devised according to the damkohler number dal that represents the relationship between the rate of storage effect and the rate of advection as follows 14 d a l α 1 a f a s l u f where l is the length of the targeted reach wagner and harvey 1997 showed that parameters were estimated with the smallest uncertainty when dal was close to one while the parameter uncertainty increased when dal was much higher or much lower than one as either the advection or storage effect became to prevail in detail when dal was less than 0 01 the coefficients of variance covs of a s and α became more than 100 times larger compared to the situation when dal was one due to the dominance of advection meanwhile when dal was larger than 10 covs of a s and α increased by more than 10 times due to the dominance of storage effect three types of synthetic data referred to as syn1 syn2 and syn3 were generated corresponding to dal of 1 0 01 and 10 respectively as shown in fig 4 in this study the procedure to generate synthetic btc data could be largely divided into two steps first btcs were generated by tsm using the assumed input variables and parameters given in table 1 these values were determined within the reasonable parameter ranges according to the previous studies to satisfy the specific dal values the numerical parameters δ x and δ t which represent the length of spatial intervals consisting of the target reach and the time step respectively were also selected to satisfy the numerical stability condition based on the courant number ward et al 2017 in the second step the errors were generated by sampling from the assumed error distributions that had the parameters given in table 1 and added to generated btcs to mimic the characteristics of the real data 3 2 real tracer test data the real tracer test data used in this study was achieved in uvas creek which is a small stream located in santa clara county california usa zand et al 1976 avanzino et al 1984 the uvas creek tracer test data was selected for application of the proposed method because it has been a benchmark data in the study of river mixing since bencala and walters 1983 used it for tsm application ward et al 2017 also employed the uvas creek data for uncertainty estimation of tsm using the glue as shown in fig 5 the tracer test was conducted in a 635 m experimental reach of uvas creek on september 26 1972 avanzino et al 1984 the study reach consists of pool and riffle sequences and the riverbed is composed of bed materials greater than 4 mm in diameter during the tracer test the average daily streamflow was relatively constant at 0 0125 m3 s as a tracer chloride was injected at a constant rate for 3 h at the upstream of the study reach before the injection the background concentration of chloride was measured as 3 7 mg l chloride concentrations were monitored at five sections of uv s1 uv s2 uv s3 uv s4 and uv s5 which were located at 38 105 281 433 and 619 m downstream from the injection point respectively as shown in fig 5 a water samples were collected to measure chloride concentration from 30 min before the injection to 27 h after the injection fig 5 b shows the measured chloride concentration curves at the five sections for uncertainty estimation the data achieved in the uv s1 and uv s5 sections were used as an upstream boundary condition and downstream observation data respectively the modeling scheme was chosen the same as the study of ward et al 2017 4 results 4 1 application to synthetic data the sm likelihood based on the mixture distribution eq 8 was applied for uncertainty estimation of the synthesized data herein the likelihood was calculated on logarithmic scale the prior distributions of parameters were assumed as uniform distributions and the parameter ranges are given in table 2 the dream was applied to sample the parameter combinations the numbers of sampling required to satisfy the convergence condition were 3 000 6 500 and 20 800 for syn1 syn2 and syn3 cases respectively after the convergence the respective 10 000 parameter combinations for the three cases were sampled to obtain the posterior distributions the statistics of parameter posterior distributions were calculated as listed in table 3 the calculated covs in table 3 demonstrate that the parameter uncertainty changed according to the dal which was consistent with the study of wagner and harvey 1997 when dal changed from 1 to 0 01 the covs of a s and α increased by almost seven and sixty times respectively and the cov of a f decreased by almost eight times these results indicate that the uncertainty of parameters related to the storage effect significantly increased while the advection related parameter became less uncertain when the advection became dominant meanwhile when dal increased from 1 to 10 the cov of a f increased six times due to the dominance of the storage effect the uncertainty of the other parameters especially α also increased greatly since the mass exchange rate increased to the level of equilibrium condition in which the storage mechanism can be explained by the longitudinal dispersion leading to difficulties in identifying the parameters harvey et al 1996 wagner and harvey 1997 in order to compare the true and the expected values of the posterior distributions the percent errors were calculated as shown in table 3 in the case of syn1 the percent errors of four parameters were less than 4 which showed that the expected values were estimated similarly to the true values of parameters the case of syn2 demonstrated that the percent errors of a s and α significantly increased while those values of a f and k f were less than 4 meanwhile in the case of syn3 the percent errors of all parameters increased in general and in particular the expected values of α deviated substantially from the true values the trend of the percent errors was in accord with that of the parameter uncertainty in which the highly uncertain parameters showed large percent errors however despite the discrepancy between the true and estimated values of parameters in syn2 and syn3 it was shown that the 95 confidence interval of parameters in all cases included the true values within the ranges this indicated that the 95 confidence interval estimated by using the sm likelihood could capture the true parameter value well regardless of the level of parameter uncertainty in addition the uncertainty estimation was validated by calculating the coverage rate which could be calculated as the percentage of the observations that are included within the 95 prediction interval yadav et al 2007 smith and marshall 2010 herein the ideal value of the coverage rate corresponded to the value of the desired interval 95 smith et al 2010 the coverage rates were calculated as 93 3 93 5 and 94 2 for syn1 syn2 and syn3 respectively as shown in table 3 the calculated coverage rates could approximate the ideal values which suggests that the sm likelihood can perform the uncertainty estimation of tsm well when applying it to the various synthetic data having different levels of uncertainty 4 2 application to the real tracer test in uvas creek in application to the uvas creek tracer test data besides the sm likelihood four types of rmse based informal likelihood were also applied among them two rmse based informal likelihoods were applied in particular to compare the results of parameter uncertainty estimation of the sm likelihood with those of the rmse based likelihoods that were employed in the previous study of ward et al 2017 rmse 1 1 which had the cutoff value of the top 1 and the shaping factor of 1 and rmse 10 1 which had the cutoff value of the top 10 and the shaping factor of 1 besides two additional rmse based informal likelihoods were applied to investigate whether the capacity of the informal likelihood could be improved by controlling the shaping factor of the rmse based informal likelihood these informal likelihoods rmse 1 6 and rmse 10 6 had the same shaping factors of 6 but had the cutoff values of the top 1 and 10 respectively the prior distributions of parameters were assumed as uniform distributions spanning the parameter ranges as shown in table 2 the parameter ranges used in this study were determined based on the previous study of ward et al 2017 from the dotty plots of parameters and the corresponding rmse that were estimated by ward et al 2017 we could narrow down the ranges of parameters by discarding the non behavioral parameter ranges where the distribution of scatter points appears flat when dream sampling technique was employed to use the sm likelihood the convergence was fulfilled when the number of sampled parameters reached 48 000 additional parameter combinations of 10 000 were sampled after the convergence was met the number of sampling necessary for the convergence was larger in this case than the case of synthetic data because there could be more uncertain sources in the data of real tracer test than in the synthetic data sets meanwhile lhs sampling technique was used to sample the 50 000 parameters in order to apply the rmse based informal likelihoods 4 2 1 estimation of parameter uncertainty by applying the suggested sm likelihoods and the four rmse based informal likelihoods we obtained the cumulative distributions of the four parameters of tsm from the cumulative distributions the statistics of the four parameters were calculated and plotted in boxplots as shown in fig 6 in the boxplots the center vertical line in the box is the median of the data the left and right of the box are the 25th and 75th percentiles respectively and the ends of the whiskers are the 2 5th and 97 5th percentiles so that the range between whiskers shows the 95 confidence interval this figure shows that the uncertainties of the tsm parameters estimated by the sm likelihood were smaller than those estimated by any type of the informal likelihoods which was also reported in the previous studies of hydrological modeling stedinger et al 2008 vrugt et al 2009 in the cases of informal likelihoods as the cutoff values changed from the top 1 to the top 10 the parameter uncertainty increased as the 95 confidence interval got wider these results happened because the number of parameters that were treated as the behavioral parameters increased which resulted in wider confidence intervals on the other hand when the shaping factor increased from 1 to 6 the parameters with smaller likelihood became neglected in constructing the confidence intervals by emphasizing the parameters with the larger likelihoods which led to the decrease of the 95 confidence intervals of the parameters as with these results when using the informal likelihood the parameter uncertainty can be estimated differently depending on the cutoff values and the shaping factors that are usually determined subjectively another notable result of the parameter uncertainty is shown in the statistical summary listed in table 4 among the four parameters the least uncertain parameter was found to be a f which showed the smallest cov regardless of the type of likelihood in the meantime two parameters of the storage effect a s and α were revealed to be the most uncertain parameters of which the cov showed different values ranging from 0 267 to 2 138 depending upon the type of likelihood among the five likelihoods the sm likelihood produced the smallest covs for a s and α the uncertainty of k f was found to show cov ranging from 0 248 to 1 530 table 4 also lists the dal calculated using the expected values of the parameters in which dal calculated by sm likelihood was less than unity while dal of all rmse based informal likelihood ranged from 1 11 to 10 93 this result indicates that the sm likelihood characterized the mixing processes in the uvas creek as the advection dominated mechanism whereas the rmse based informal likelihood interpreted the mixing process as a storage effect dominated process these results demonstrate that the selection of likelihood type was significantly important in characterizing the mixing characteristics of natural rivers based on parameter uncertainty estimation in particular when using the informal likelihoods subjective judgments which were taken to determine the form of the informal likelihood such as cutoff value and shaping factor could result in the incorrect representation of the mixing characteristics of natural rivers 4 2 2 validation based on model prediction uncertainty in this study to analyze the model prediction uncertainties by the sm and rmse based informal likelihoods the 95 prediction interval of each likelihood was estimated herein in addition to coverage rates the mean widths of the 95 prediction intervals were also calculated the mean width was obtained by averaging the width of the 95 prediction interval over time which can stand for the size of the estimated uncertainty of the prediction results table 5 summarizes the estimated coverage rates and mean widths the uncertainty of prediction results generally followed the trend of the parameter uncertainty according to the type of likelihood among the informal likelihoods rmse 1 1 rmse 10 1 and rmse 10 6 which showed larger parameter uncertainty than the sm likelihood also presented wider 95 prediction intervals than the sm likelihood on the other hand the mean width of rmse 1 6 was similar to that of the sm likelihood despite the larger parameter uncertainty this was because the predicted concentration of sm likelihood was estimated by explicitly adding the model error to the simulated concentration while that of rmse 1 6 was not as shown in eq 13 when comparing the four informal likelihoods with each other the mean width increased as the cutoff value and the shaping factor varied from the top 1 to the top 10 and from 6 to 1 respectively which reflected the trend of the parameter uncertainty in general it can be thought that the wider prediction interval leads to the increase in coverage rate however the four informal likelihoods yielded lower coverage rates than the sm likelihood despite their wider or similar mean widths the coverage rates of the four informal likelihoods were much less than the ideal coverage rate of 95 in particular the rmse 10 1 showed the largest mean width but the lowest coverage rate given that the mean widths were large but the coverage rates were low in the informal likelihoods it seems that the informal likelihoods estimated the posterior distributions of the parameters incorrectly in deviated directions from the spaces where the true behavioral parameters existed when applying the sm likelihood the coverage rate increased to 92 5 which was close to 95 although it yielded the narrower 95 prediction interval than the informal likelihoods this result meant that the sm likelihood could result in more appropriate uncertainty estimation of the parameter and the prediction because the 95 prediction interval of the sm likelihood covered the observation well and did so tightly without overestimation 5 discussion the validation in the previous section showed that the sm likelihood yielded the better uncertainty estimation of tsm compared to the rmse based informal likelihood the reasons for the successful application of the sm likelihood can be summarized as 1 appropriate assumptions of the error distribution 2 better reflection of the information in the entire segments of btc and 3 sound judgment on the behavioral parameters the sm likelihood could result in successful uncertainty estimation thanks to the proper assumptions of the error distribution the accurate assumptions of the error distribution are important for robust uncertainty estimation since false assumptions can lead to erroneous uncertainty estimation stedinger et al 2008 smith et al 2010 fig 7 shows the error diagnostics for two formal likelihoods representing different characteristics of the error in which the ns not segmented likelihood of eq 7 assumed the homoscedastic residual error distribution along the whole btc while the sm likelihood of eq 8 segmented the btc and assumed the heteroscedasticity of the residual error distribution between the body and tail segments fig 7 a and b show the error diagnostics for the ns likelihood herein quantile quantile qq plots were used to check whether the residual errors follow the assumed distribution in the case of ns likelihood the variance of residual errors as shown in fig 7 a fluctuated quite substantially for different values of concentration of the btc which implied that the residual errors did not follow the assumptions of homoscedasticity the qq plot in fig 7 b also shows that the residual errors did not satisfy the assumed distribution either by not following the linear trend of the dotted line however the results of sm likelihood revealed that the heteroscedasticity was successfully rectified by dividing the btc into the body and tail segments as shown in fig 7 c and e not only that the assumption of the distribution of the residual errors became more satisfied by showing the more linear trend of the qq plots as shown in fig 7 d and f the error diagnostics showed that it was rational to divide the btc into the body and tail segment and to use the different distribution of the errors for each segment as the sm likelihood adopted table 5 demonstrates that the sm likelihood generated higher coverage rate in the model prediction compared to the rmse based informal likelihood by reflecting the information of the entire segments of btc this table shows that the coverage rates of sm likelihood for both body and tail segments were higher than those of rmse based informal likelihoods even though it was seen that the 95 prediction intervals of both likelihoods missed a lot of observations in the tail segment in particular the prediction intervals estimated by the rmse based informal likelihoods missed the larger number of the observations in the tail segment the failures of the rmse based informal likelihoods in explaining the observation of the tail segment can be attributed to the property of rmse in which it tended to focus on fitting the higher concentration in the peak of btc while ignoring the information contained in the low concentration in the tail segment kelleher et al 2013 therefore the rmse based informal likelihoods could not explain well the observation in the tail segment on the other hand the sm likelihood could capture more observations in the tail segment than the rmse based informal likelihood which implied that the sm likelihood was better able to reflect the information in the tail segment the better performance of the sm likelihood compared to the rmse based informal likelihood was manifested by estimating the parameter uncertainty using the tail truncated btc data which corresponded to the data of only the body segment as shown in fig 5 b fig 8 shows the boxplots of the posterior distributions of the parameters that were estimated by assuming that the tail of btc was truncated during the tracer test in this case the parameter uncertainty estimation was performed using the two likelihoods the sm and the rmse 10 1 likelihood and the results were compared with the results of two likelihoods using the original whole btc which are shown in fig 6 as a result when the parameter uncertainty was estimated by the sm likelihood there were significant changes in the medians and the 95 confidence intervals of the storage zone effect related parameters a s and α however when using the rmse 10 1 likelihood the medians and the 95 confidence intervals were not varying regardless of the existence of the data of the tail segment this insensitivity of a s and α to the absence of the data in the tail segment when using the rmse 10 1 was unreasonable since the data in the tail segment contained significant information about the storage effect compared to the body segment of the btc wagner and harvey 1997 wagener et al 2002 and kelleher et al 2013 have demonstrated that the parameters related to the storage effect a s and α were the most sensitive to the data of the tail segment via the sensitivity analysis thus this result demonstrates that the use of the sm likelihood estimates the parameter uncertainty accurately by properly reflecting the information contained in the tail segment of btc the robust uncertainty estimation of the sm likelihood could also be attributed to the ability to distinguish the behavioral parameters from the non behavioral parameters on the other hand the rmse based informal likelihood misjudged the non behavioral parameters as behavioral parameters due to the predetermined cutoff value as indicated by the wider confidence intervals of parameters and larger mean widths compared to the sm likelihood this argument can be corroborated by the interaction plots shown in fig 9 fig 9 a shows the interaction of the parameters that were included in the 95 confidence intervals estimated by the sm likelihood in these plots the clear correlation relationships appear in the relationships between k f a f and α in particular k f and α had a negative correlation since these two parameters are in a relationship of trade off interaction this result clearly demonstrates that the mixing in natural rivers happens by two causes the shear flow and storage zone effect which in turn results in a trade off between the dispersion due to the shear flow and mixing due to the storage zone effect meanwhile fig 9 b presents the interaction relationships between the parameters that were included in the 95 confidence intervals when applying the rmse 10 1 unlike the case of sm likelihood no clear relationships were shown this is because the non behavioral parameters were selected as the behavioral parameters which obscured the obvious interaction relationships among the parameters 6 conclusion in this study the bayesian inference with formal likelihood was applied to estimate the parameter uncertainty of tsm and the result was compared with that of informal likelihood this study proposed the segment mixture sm likelihood based on the finite mixture distribution that consists of two error distributions that correspond to the body and tail segments of btc respectively the proposed sm likelihood was tested by applying it to the synthetic tracer test data and the real stream data of the uvas creek and by comparing the uncertainty estimation results with the results of the informal likelihood the results when applying the sm likelihood to the three synthetic data showed that the sm likelihood could capture the true parameters within the estimated 95 confidence intervals regardless of the level of parameter uncertainty the properness of the sm likelihood was also validated by calculating the coverage rates of 95 prediction intervals the coverage rates of all synthetic data were calculated as close to the ideal value of 95 similarly in the case of uvas creek the sm likelihood yielded 95 prediction interval which strictly and well covered the observation with the coverage rate of about 92 5 therefore the sm likelihood was found to perform appropriately for the parameter and prediction uncertainty of tsm in contrast the rmse based informal likelihoods estimated the parameter uncertainty incorrectly when compared to the sm likelihood all of the rmse based informal likelihoods gave much fewer coverage rates than the 95 although these likelihoods yielded larger mean widths than the sm likelihood the successful uncertainty estimation of sm likelihood compared to the rmse based likelihoods could be attributed to three properties of the sm likelihood first the sm likelihood represented the error characteristics of btc for tsm appropriately as shown in the error diagnostic tests in particular the sm likelihood could represent the heteroscedasticity of the residual errors by segmenting the btc into the body and tail parts second the sm likelihood explained the observation along the btc well by combining comprehensive information from the whole segments of btc such as the body and tail segments on the other hand the rmse based informal likelihoods could not employ the information of the tail segments which is useful in estimating the storage effect related parameters when comparing the parameter uncertainties estimated by the sm and rmse based likelihoods using the tail truncated btc the 95 confidence intervals of the parameters estimated by the rmse based likelihood were insensitive to the nonexistence of the data in the tail segment while the sm likelihood was not also the sm likelihood was successful in distinguishing the behavioral parameter and non behavioral parameter and showed the clear correlations between the parameters in the interaction plots meanwhile the rmse based informal likelihood tended to misclassify the non behavioral parameters as behavioral parameters which could lead to the incorrect parameter and prediction uncertainty estimation when it comes to the uncertainty estimation of tsm it is important to choose appropriate likelihood since the selection of inappropriate likelihood can give rise to incorrect interpretation of the river mixing characterization and degradation of the accurate prediction of the pollutants mixing in that sense the sm likelihood that was suggested in this study can be a good option for the likelihood in uncertainty estimation of the tsm software data availability the tsm developed in matlab is available from the link http ehlab snu ac kr and the matlab toolbox of dream vrugt 2016 is available from the link https faculty sites uci edu jasper software the tracer data of uvas creek was taken from avanzino et al 1984 and the synthetic data can be downloaded from the link http ehlab snu ac kr acknowledgments this research is supported by chemical accident response r d program 2018001960001 of korea ministry of environment moe and the bk21 plus research program of the national research foundation of korea this research work was conducted at the institute of engineering research and institute of construction and environmental engineering in seoul national university seoul korea 
26101,estimating of daily hospital admissions due to air pollution is a leading issue in environmental science to better understand this problem it is essential to improve the applied methodologies the use of generalized linear models glm is well known however they may be improved using different methods to coefficients estimation and to consider seasonality alternative methodologies rarely applied in such topic are artificial neural networks ann efficient to solve non linear problems and ensembles which combine various models outputs this research aims to apply 10 distinct ann and 4 ensemble to estimate hospital admissions for respiratory diseases caused by particulate matter and meteorological variables of campinas and são paulo cities brazil in addition a new proposal of glm was introduced considering coefficients calculation via particle swarm optimization and seasonality via normalization procedure ann and ensembles use showed significant improvements and may allow studies into areas with flawing database developing countries reality keywords artificial neural networks ensemble method generalized linear model particle swarm optimization hospital admissions pm10 1 introduction the air pollution impact on human health is a topic discussed worldwide the world health organization who reports that more than 6 million premature deaths are related to the pollution who 2017 to better understand the association between air pollution and adverse health effects it is essential to improve the applied methodologies air pollution impact on human health is commonly performed using statistical regression techniques especially the generalized linear models glm tadano et al 2012 vanos et al 2014 polezer et al 2018 pan et al 2019 kim and lee 2019 glm usually use maximum likelihood estimators of regression coefficients dobson and barnett 2008a mccullagh and nelder 1989 the default option of statistical softwares such as splus sas stata and r an alternative for regression coefficients estimation may consist on instead of maximum likelihood estimators bio inspired metaheuristics as the particle swarm optimization pso which presents a search capability that may achieve better accuracy in the coefficients tuning than traditional methods matsuzaki 2017 another improvement may be achieved for seasonality trends switching the use of smoothing splines with monthly deseasonalization by normalization technique a simpler method with potential of transforming the variables into stationary matsuzaki 2017 siqueira et al 2014 siqueira et al 2018 the use of statistical regression techniques to assess adverse health effects of air pollution requires at least two years of consistent and robust databases tadano et al 2012 a minimum of missing data is needed otherwise the model may not capture a relationship between the inputs independent variables and the output dependent variable due to seasonal tendencies of the series tadano et al 2012 polezer et al 2018 turning this kind of studies a true challenge in developing countries that usually have incomplete databases an alternative methodology consists on artificial neural networks ann widely used in many different types of predictive modeling including air pollution mapping mishra et al 2015 franceschi et al 2018 feng et al 2019 cabaneros et al 2019 cabaneros calautit and hughes cabaneros et al 2019 reported a review about researches aiming to forecast air pollution using ann from 2001 to february 2019 resulting in 139 peer reviewed articles however the application of ann to estimate potential air pollution impacts on population health is less used polezer et al 2018 wang et al 2008 kassomenos et al 2011 tadano et al 2016 sundaram et al 2016 also the frequently used ann consists of multilayer perceptron mlp feedforward ann as the mlp are nonlinear methodologies that are capable of approximate any nonlinear continuous function if it is limited differentiable and present inputs defined in a compact space with arbitrary precision in other words it means that this kind of method can map a set of inputs in an output using a nonlinear way being known as universal approximators which present elevated generalization capability this is exactly the case of the problem we addressed in addition recurrent ann are general cases of the feedforward ones in essence ensembles are extensions of the ann because they only combine the outputs of the previous methods in epidemiological studies the generalized linear models frequently do not achieve satisfactory performance due to the inherent process of free coefficients adjustment coman et al 2008 lauret et al 2016 chelani et al 2002 in recent times the possibility of using ensembles has gained much attention due to their accuracy and efficiency because they combine the output of multiple single models responses trainable ensembles are those in which the combiner is for example an adjustable model like a neural network while a non trainable method consists of a statistical measure of the inputs as the average de mattos neto et al 2014 firmino et al 1016 with this in mind this research proposes the application of ann neural based ensembles and the aforementioned new version of the glm to estimate the impact of air quality on population health the addressed ann were multilayer perceptron mlp radial basis function network rbf extreme learning machines elm echo state networks esn jordan and elman networks the goal was to verify the contribution of these approaches to estimate the number of hospital admissions due to air pollution exposure the case studies consider particulate matter with aerodynamic diameter less than or equal to 10 μ m pm10 and meteorological variables influence on hospital admissions for respiratory diseases at two important brazilian cities campinas and são paulo cities 2 generalized linear model generalized linear models glm are an extension of the classical linear model used for continuous problems this method describes the relationship between one or more prediction variables independent explanatory or covariable x x 1 x p for a wide variety of non distributed responses neuhaus and mcculloch 2011 a key point on glm use is that many of the considerations in its construction are the same for the standard linear regression models since both have many characteristics in common matsuzaki 2017 the glm presents three specifications the random component or the distribution of the response the systematic component the function of the prediction variables and the connection between the other components neuhaus and mcculloch 2011 therefore glm assumes that the response refers to the expected value of the target variable achievable through a linear predictor it is supposed that the model manipulates a combination of inputs and can use polynomial functions or splines which is a smoothing that approximates functions behavior that have local and sudden changes tadano et al 2012 the poisson regression is a specific approach of the glm which further explains that the response variable must follow the same distribution and the data must have equal dispersion neuhaus and mcculloch 2011 according to matsuzaki 2017 in the poisson regression model the response variable describes the number of times that an event occurs in a finite observation space also the function given by equation 1 is the inverse of the function log α called the link function 1 α e β 0 β 1 x 1 β n x n in which α is the predicted value β 0 plays the role of the offset and β 1 β n are the free coefficients that are multiplied by the inputs vector x x 1 x n 2 1 new glm the free coefficients of glm with poisson regression are commonly estimated using the maximum likelihood method paula 2004 dobson and barnett 2008b however it is possible to tune the model using bio inspired metaheuristics like the particle swarm optimization pso the pso was inspired in the intelligent collective behavior of animals groups such as school of fishes and flocks of birds this technique was introduced by eberhart and kennedy 1995 the method works with a group of agents moving through the search space trying to identify the optimal points of the function to be optimized each agent is an independent particle or a candidate solution and during the search process it works together with other agents forming a population in this work each particle is composed by the glm free parameters during the displacement the particles move towards the best position they have reached pbest and in the direction of the best position achieved by some of its neighbors gbest the particle i 1 p has a position vector x with d 1 d dimensions which are changed by the speed operator v these values are updated as shown in equations 2 and 3 2 v i d k 1 ω v i d k c 1 r a n d p b e s t i d x i d k c 2 r a n d g b e s t d x i d k 3 x i d k 1 x i d k v i d k 1 in which v i d k is the velocity of the individual in the iteration k v d m i n v i d k v d m a x ω is the inertia weight factor c 1 e c 2 are the acceleration constants r a n d uniformly generated random numbers between 0 and 1 and x i d k is the current position of the individual in iteration k besides the glm tuning another important remark is to consider the data seasonality according to tadano et al tadano et al 2012 it is an important long term trend that appears in this kind of study as the meteorological variables and concentration of pollutants vary during the year to adjust these components the spline function is often used because it provides a better approximation than simple polynomial interpolations markatos 1985 however it is possible to replace the spline procedure by the deseasonalization method siqueira et al 2014 siqueira et al 2018 it makes the series stationary by means of the application of equation 4 siqueira and luna 2019 4 z i x i μ m σ m being μ m the mean and σ m the standard deviation of each month the new series z i is approximately stationary with zero mean and unit standard deviation 3 artificial neural networks in this study we addressed feedforward networks multilayer perceptron mlp and radial basis function networks rbf recurrent architectures elman and jordan networks and the unorganized machines extreme learning machines and the echo state networks siqueira et al 2014 polezer et al 2018 haykin et al 2009 3 1 multilayer perceptron the multilayer perceptron mlp is considered one of the most important architectures of ann this method presents a set of information processors units known as artificial neurons which form the input layer one or more hidden layers and an output layer as shown in fig 1 haykin et al 2009 the information flows from the input layer passes through the intermediate layers and closes in the output layer generating an output response in a mlp the disjoint layers are connected whereas neurons of the same layer do not communicate the training or the adjustment of their weights is performed in two phases the first one is the forward propagation in which the signals from a training set sample are inserted as inputs of the network being propagated layer by layer in the second phase the errors are propagated in a recursive manner and the weights are adjusted through some adjustment rule haykin et al 2009 siqueira et al 2014 the most commonly used method to tune the mlp is the steepest descent algorithm with the derivatives calculated via the backpropagation method 3 2 radial basis function networks radial basis function networks rbf present their origin in the execution of exact interpolation of a set of data points in a multidimensional space powell 1987 these ann have a simpler structure and a faster training process compared to mlp it is composed of only one intermediate layer in which the neurons present activation functions different from the output layer neurons in this case the activation function inputs are the euclidean distances between the input information and the weight vectors fig 2 shows the rbf architecture in a rbf each neuron in the hidden layer uses a nonlinear radial basis function as activation function this layer performs a nonlinear transformation of the input in this sense the output layer acts as a linear combiner that maps the nonlinearity to a new space the bias of the output layer neurons can be modeled by an additional signal in the previous one which has a constant activation function du and swamy 2014 the rbf training consists of two steps the first one is associated to the adjustment of the weights of the intermediate layer neurons and uses unsupervised learning methods directly related to radial basis functions the second step is the adjustment of the output layer weights which uses for example the generalized delta rule or the backpropagation method similar to the mlp 3 3 neural network of elman and jordan recurrent neural networks rnn are those endowed with feedback loops of information in these cases some output of hidden or output layers are reinserted in some previous one this process provides to the network a kind of internal memory which can be useful to problems with time dependency haykin et al 2009 the elman neural network enn is a recurrent model which presents additional inputs in the hidden layer forming the context layer fig 3 describes the elman network with four layers input hidden intermediate context and output layers ren et al 2018 the context layer stores the output values of the hidden layer of the previous time also excepting the context layer the model is similar to a feedforward network in fig 3 the inputs are represented by u k the outputs are y k the entries for i th neuron of the hidden layer are denoted as v k i the output of the i th hidden layer is given by x k i and the output of the j th context is x k c j the standard method to training an enn is called the elman backpropagation algorithm ebp ren et al 2018 in which the time is treated as an explicit part of the input it allows time to be represented by the effect it has on the process which gives dynamic properties to the system which respond to temporal sequences elman 1990 in short the network becomes endowed with memory when the backpropagation is applied the dependence of x c k to the weights must be taken into account the algorithm that deals with this dependency is the dynamic backpropagation algorithm kuan 1989 local rnn stores the past state of the networks and use it as part of the entry in the next iteration they can be divided into two categories a network with external time delay feedback and a network with internal delay feedback the first case is called jordan neural network jnn ren et al 2018 the main difference in terms of the structure between the enn and jnn is shown in fig 4 while the in an enn the recurrent information comes from the intermediate layer to the input in the jnn the recurrence is from the output layer to the input haykin et al 2009 3 4 unorganized machines unorganized machines um are architectures of artificial neural networks in which the weights of the intermediate layer are not adjusted and are randomly generated because of this their training process are simple and computationally efficient the term unorganized machines was proposed by boccato et al 2011 which evokes alan turing s work on intelligent machine behavior turing in this work we addressed the extreme learning machines and the echo state networks 3 4 1 extreme learning machines according to huang huang et al 2006 the extreme learning machines elm are similar to mlp networks with a single intermediate layer however in this case the weights of the neurons are chosen in a random and independent way the adjustment process is performed only on the output layer which is a linear combiner therefore the training process has a linear computational cost and uses a supervised approach fig 5 shows the structure of a generic elm with its layers here it is represented only one output the input vector is u t u 1 u 2 u t k 1 t its components passed to the hidden layer w h r n h k which contains the random generated weights the output signal of this layer is according to equation 5 5 x t h f h w h u t b in which b is the bias of each neuron and f h is the activation function of the intermediate neurons such activations are linearly combined to produce the network output y t using equation 6 6 y t w o u t x t h being w o u t the matrix of the output layer weights the training process consists of finding the best weights in the output layer the moore penrose operator is an interesting option to solve the task 7 w o u t x h t x h 1 x h t d where x h ε r t s n k is the matrix containing the exit of the intermediate layer t s is the number of samples of the training x h t x h 1 x h t is a pseudo inverse of x h and d is the vector containing the desired output in huang et al 2006 the authors suggest that the generalization capability of the network can be increased by inserting a weighting term c to equation 7 as in 8 8 w o u t i c x h t x h 1 x h t d to determine the value of c the authors indicate to test the 52 possible values of the vector λ 26 25 24 25 so that c 2 λ thus after training a validation set must be used to this term c the authors gave the name regularization coefficient 3 4 2 echo state networks echo state networks esn were proposed by jaeger 2001 they are recurrent structures because they have feedback loops between the neurons which differs it from the elm this feature allows the creation of an intrinsic memory which may be beneficial to problems involving temporal relation between samples the training process is similar to that of the elm which must find the coefficients of a linear combiner based on a least squares problem with reference signal fig 6 shows the generic structure of an esn in which the input vector is u n u n u n 1 u n k 1 t which is transmitted from the input layer w i n to the dynamic reservoir intermediate layer by means of non linear combinations the theoretical element that guarantees the presence of memory in the esn is known as the echo state property jaeger 2001 under specific conditions in the reservoir w the recent history of the inputs governs the internal dynamics of this layer neurons the network weights can thus be previously defined jaeger 2001 yildiz et al 2012 compared to the other rnn the training process of esn is simpler and faster there are practically no problems related to the instability and manipulation of cost functions siqueira et al 2014 as in the elm the task is to find the coefficients of a linear combiner using the moore penrose inverse operator and the regularization coefficient we highlight that the design of the reservoir plays an important role in the development of the esn in this work we addressed the proposals from jaeger 2001 and from ozturk et al 2007 following the steps described in siqueira et al 2014 4 ensembles the literature has shown that the use of ensembles brings performance enhancements to the single models de mattos neto et al 2014 firmino et al 1016 perrone and cooper 1992 according to perrone and cooper 1992 ensembles combine the outputs of various models which were pretuned in the beginning the application of this technique was given by the weighted average calculation of each methods outputs the approach is known as basic ensemble method bem and is given by 9 9 f b e m 1 n i 1 n f i where n is the total number of single models and f i is the output of these models however in recent times several works have presented the possibility of using a neural network as the combinator de mattos neto et al 2014 firmino et al 1016 the idea can be exemplified as follows consider that there are three different ann to estimate a given variable the neural based ensemble will use the point to point predictions of each network and will use them as input then the combinatorial network must be trained in the same traditional way this process tends to lead to better solutions in this work we will use the mlp and rbf networks to perform this task because they are the most important feedforward ann models and other studies addressed them in correlated problems de mattos neto et al 2014 firmino et al 1016 5 case study the case study consists of estimating the number of hospital admissions due to respiratory diseases caused by the exposure to particulate matter pm10 to campinas and são paulo cities brazil according to data from the brazilian institute of geography and statistics ibge campinas had 1 194 094 inhabitants in 2017 being the third most populous municipality in the state of são paulo it occupies an area of 797 6 km2 of which 238 3 km2 are in the urban perimeter and the remaining 559 3 km2 is the rural area de campinas 2014 it presents a tropical climate of high altitude with hot and rainy summers the temperatures are in the range from 13 c to 29 c being rarely less than 9 c or greater than 33 c spark 2018 são paulo city is the capital of the state of são paulo and the main business and commercial financial center of south america it is the most populous city in brazil with 12 176 866 inhabitants according to ibge ibge 2019 it is also the most influential in the global scenario being considered in 2016 the 11th most globalized city on the planet it is 464 years old and has a total area of 1521 11 km2 being 968 32 km2 of urban areas the information about hospital admissions for respiratory diseases were obtained from the brazilian national health system at http www2 datasus gov br datasus index php area 02 datasus 2010 the database from campinas used for the models adjustment and estimation has a total of 732 samples comprising the period from january 1st 2007 to december 31st 2008 during that period 11 121 hospitalizations were detected being 2 the smaller number and 32 the greater to a single day as shown in fig 7 são paulo dataset has 1068 samples in which the number of hospitalizations lies from january 1st 2014 to december 31st 2016 totaling 159 683 occurrences the highest daily number of hospitalizations was 409 as can be observed in fig 8 on the other hand there were days with no attendance regarding respiratory diseases probably due to a matter of database record system or the fact the available data is only from public health system miss considering data from health insurances and private units the pollution concentration pm10 and meteorological data ambient temperature and relative humidity are from air quality monitoring stations administered by environmental company of são paulo state cetesb at https cetesb sp gov br ar qualar cetesb 2010 due to lack of resources são paulo government had only one air quality monitoring station aqms at campinas city nowadays they have one more station that has pm10 data but only since 2016 cetesb 2018 pm10 and meteorological dataset consists of daily averages from the cerqueira cesar station held by cetesb to the studied period only four out of twelve stations had pm10 available data even more only one station has less than 100 days lack due to the ligh linear correlation between data from these four stations we chose data from cerqueira cesar station with only 29 days lack besides that this study considered as an independent input the day of the week and whether the day of hospitalization is a holiday or not these two variables were used because they follow a pattern in epidemiological studies tadano et al 2016 since the number of hospitalizations decreases on weekends and holidays polezer et al 2018 in this way the entries of the models covered contain the following fields date date referring to the day of measurement pm10 concentration of particulate matter with aerodynamic diameter less than or equal to 10 μ m temperature recorded average temperature humidity recorded air relative humidity day day of the week from sunday 1 to saturday 7 holiday whether the day is a holiday 1 or not 0 number of hospitalizations number of hospital admissions due to respiratory problems according to the international classification of diseases icd 10 j00 to j99 in both cases the samples were divided into three sets training containing the first 70 of the data validation with the following 15 and the test set with the remaining 15 that percentage was tested being reasonable by the amount of available data 5 1 results and analyzes to perform an extensive and significant analysis of the models mapping capability the following neural network architectures were elaborated mlp rbf elman network jordan network elm esn jaeger esn ozturk also we perform the models that have the regularization coefficient in their structure elm rc esn jaeger rc esn ozturk rc in addition the ensembles were implemented considering the linear average median mlp and rbf as combinators finally we used the traditional glm model with coefficients calculated by the maximum likelihood estimator and long term trend smoothing with splines and the new proposal optimized by the pso and using the deseasonalization procedure the parameters of the pso were chosen based on previous empirical tests and they are in table 1 all models were developed using the java programming language with the object oriented paradigm under netbeans ide the exception was the traditional glm which was performed using the r software in this case in order to adjust seasonal and long term trends we used the spline function toolbox from the r with value equals to 16 to both cities value based on previous studies in this field the ann were elaborated using only one intermediate layer in the training the amount of artificial neurons was selected in the range from 5 to 200 with increments of 5 the weight adjustment was performed in the training set the cross validation procedure was applied in view to increase the generalization capability of the networks haykin et al 2009 except to the elm and esn without rc the training stop condition was a precision of 10 6 with a maximum of 2000 iterations it is important to mention that all neural networks had the linear identity function in the output neurons as activation function and the hyperbolic tangent in the hidden layer except rbf in the last case the k means algorithm was applied to cluster the centers in addition the weights were generated randomly in the interval 1 1 as well as the data were normalized in the same range then we achieved the output responses the data were renormalized to allow an analysis of the error in the original domain haykin et al 2009 the error metrics used in this study were the mean square error mse the mean absolute error mae and the mean absolute percentage error mape which formulations are given by 10 11 12 10 m s e 1 n s n s t 1 d t y t 2 11 m a e 1 n s n s t 1 d t y t 12 m a p e 1 n s n s t 1 d t y t d t being d t the observed number of hospitalizations and y t the output of the networks in cases in which the metrics indicate distinct models as the best that with the lowest mse was assumed as the best one siqueira et al 2014 all the results estimate the number of hospital admissions for up to seven days after exposure to pm10 as suggests the literature polezer et al 2018 tadano et al 2016 li et al 2015 because air pollution may affect population s health some days after the exposure in this case we applied the models to eight situations lag 0 to lag 7 meaning lag 0 relates the concentration of pm10 from today with the health outcomes of the same day lag 1 relates the concentration of pm10 from today with the health outcomes of tomorrow lag 2 relates the concentration of pm10 from today with the health outcomes two days ahead and son on the general performances of campinas and são paulo are presented in tables 3 and 3 respectively to neural models the number of neurons in their hidden layer n n is also presented the acronym rc represents regularization coefficient and the ensembles are nominated as follows ensemble 1 average ensemble 2 median ensemble 3 mlp and ensemble 4 rbf to all neural models and the glm pso 30 simulations were performed tables 2 and 3 present the best of these 30 cases regarding the mse tzanis et al 2019 alimissis et al 2018 ding et al 2016 finally the friedman test was applied to verify if the results were significantly different in all cases the achieved p values were very close to zero indicating that a change in the model leads to distinct results to campinas among the results obtained to the estimation three days after exposure lag 3 ensemble 3 with 175 neurons was the method with the lowest overall errors considering the 3 aforementioned metrics on the other hand the estimate with worse result was achieved by the traditional glm comparing traditional glm with glm pso the results showed pso improved the performance due to the better capability to perform global and local search simultaneously in the sene of minimizing the output error fig 9 shows the comparison between actual number of hospital admissions blue circles and the results of the test set with the best predictor the ensemble 3 method with 175 neurons to lag 3 red square in campinas we need to highlight that countless factors may affect population s health besides air quality such as people s lifestyle genetics age and so on with that in mind and the poorness of data quality in developing countries like brazil we may assume that the models gave a good estimate of air quality impact on population health following we presented table 3 containing the errors to são paulo estimates the best performance found was again favored to the ensemble using the mlp with 40 neurons to all lags among the obtained results the estimate performed to lag 7 achieved the lowest errors again the superiority of the neural based ensembles in comparison to the linear cases was highlighted fig 10 shows the comparison between the number of actual hospitalizations blue circle and the estimate performed to lag 7 by the ensemble based on the mlp red square as a final analysis it is clear that the comparison among all models was largely favorable to the ensemble combined by mlp regarding the mse the reasons that can lead to this are diverse we can firstly point the insertion of a large variety of input methods such as ann proposals and glm the diversity is important to form accurate outputs as pointed by matos neto et al de mattos neto et al 2014 this diversity is a preponderant factor to a good performance of ensembles however it is important to discuss the use of the metrics addressed to analyse the comparative performance willmott and matsuura 2005 advocate that the most relevant in assessing average model performances is the mae instead of the mse considering this approach in the case of são paulo the best result achieved is related to the elm cr and still lag 7 being the ensemble 3 the second best to campinas the best results for all metrics were achieved by ensemble 3 it is evident that the use of ann is more adequate to the studied cases in comparison to the linear models this is important because the specialized literature of air pollution epidemiology is still very biased towards the use of classical glm especially the implementation in the r software in this sense it is clear that there is still room for improvement of these methods comparing each approach to create the ann the rbf achieved the best overall performances to the feedforward models to the recurrent ones the jordan networks were the best to campinas and the esn from ozturk et al to são paulo this difference between scenarios is a strong indicative that the problem is complex to be solved and the more models available the better may be the analysis through the comparative analysis considering the ensembles it is noticeable that the trainable methods which use ann as combiners achieved the best performances in comparison to the non trainable approaches average and median the main reason is the high capability of the neural models for mapping the data in a nonlinear manner and generalizing the results as shown in tables 2 and 3 the non trainable ensembles can be even worse than the single models in some scenarios therefore it seems to be clear that the ann are more suitable to be used in this case the glm pso proposal introduced in this study still needs to be evaluated in other scenarios as well as more optimization algorithms should be tested in addition the spline methodology presents a degree of complexity often not understood by the researchers therefore replace the spline by a simple deseasonalization process is a factor that favors the understanding of the method and can also lead to better performance the general analysis shows that the glm pso is a competitive candidate in this area as for the number of lags ideal to each case it is difficult to assert an exact value if all models are considered since for each one there was a discrepancy in the best results this is not a surprising factor and it shows how complex the problem is the same observation applies to the number of neurons in the middle layer of neural networks 6 conclusion the general analysis revealed some important points first the glm pso won the classical proposal to são paulo however it is worth noting that the general results strongly indicate that linear models can not achieve lower errors than nonlinear performances second in this same scenario the esn from ozturk et al was able to overcome all the classic recurring methodologies and the most important result is that even with a high dispersion mlp based neural ensemble was the best in both cases the new approach of glm and the use of ann and ensembles showed significant improvements to classical glm this research opens the horizon to the methodologies that may be applied to assess health adverse effects of air pollution as it is a complex problem to be solved expanding for example possibilities of researches at areas with small amount of data and improving predictions we need to highlight that the present research showed the proposed methodologies has succeeded on estimating air quality impact on population health even with the assumption that measured air quality is representative of the whole city and the citizens exposure then we recommend that the models should be tested to more representative data to assure their performance the main difficulty was to capture peaks of hospitalizations that must be considered with more caution in future researches however the results of ann and ensembles were better than those of glm there are an important difference between scenarios which is a strong indicative that the problem is complex to be solved in this sense the more models available the better may be the analysis we understand that the best action to improve air quality is reducing emissions however it is not possible in a short term then due to the good performance of ann and ensembles they could be used to estimate hospital admissions and alert government and hospitals for possible increases of hospitalizations due to high air pollution episodes or adverse meteorological conditions also developing countries like brazil has a lack of information about air quality due to financial resources making it difficult sometimes to estimate air quality impacts on human health using statistical regressions then universal approximators like ann may allow studies of air quality health impacts to be done as it can find relationship between variables even with small databases as a future perspectives to improve and spread the available methodologies used to assess health air pollution impacts it is proposed to use other optimization algorithms for glm such as genetic differential or immuno inspired evolution in the case of neural networks it is worth discussing the use of other ensemble combiners such as elm itself using other weather variables besides temperature and relative humidity other types of pollutants other than pm10 may also be alternatives deep learning techniques depending on the number of data available can also be applied to the problem acknowledgment the authors would like to thank conselho nacional de desenvolvimento científico e tecnológico for the financial support under process number 405580 2018 5 appendix a supplementary data the following is the supplementary data to this article multimedia component 1 multimedia component 1 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2019 104567 
26101,estimating of daily hospital admissions due to air pollution is a leading issue in environmental science to better understand this problem it is essential to improve the applied methodologies the use of generalized linear models glm is well known however they may be improved using different methods to coefficients estimation and to consider seasonality alternative methodologies rarely applied in such topic are artificial neural networks ann efficient to solve non linear problems and ensembles which combine various models outputs this research aims to apply 10 distinct ann and 4 ensemble to estimate hospital admissions for respiratory diseases caused by particulate matter and meteorological variables of campinas and são paulo cities brazil in addition a new proposal of glm was introduced considering coefficients calculation via particle swarm optimization and seasonality via normalization procedure ann and ensembles use showed significant improvements and may allow studies into areas with flawing database developing countries reality keywords artificial neural networks ensemble method generalized linear model particle swarm optimization hospital admissions pm10 1 introduction the air pollution impact on human health is a topic discussed worldwide the world health organization who reports that more than 6 million premature deaths are related to the pollution who 2017 to better understand the association between air pollution and adverse health effects it is essential to improve the applied methodologies air pollution impact on human health is commonly performed using statistical regression techniques especially the generalized linear models glm tadano et al 2012 vanos et al 2014 polezer et al 2018 pan et al 2019 kim and lee 2019 glm usually use maximum likelihood estimators of regression coefficients dobson and barnett 2008a mccullagh and nelder 1989 the default option of statistical softwares such as splus sas stata and r an alternative for regression coefficients estimation may consist on instead of maximum likelihood estimators bio inspired metaheuristics as the particle swarm optimization pso which presents a search capability that may achieve better accuracy in the coefficients tuning than traditional methods matsuzaki 2017 another improvement may be achieved for seasonality trends switching the use of smoothing splines with monthly deseasonalization by normalization technique a simpler method with potential of transforming the variables into stationary matsuzaki 2017 siqueira et al 2014 siqueira et al 2018 the use of statistical regression techniques to assess adverse health effects of air pollution requires at least two years of consistent and robust databases tadano et al 2012 a minimum of missing data is needed otherwise the model may not capture a relationship between the inputs independent variables and the output dependent variable due to seasonal tendencies of the series tadano et al 2012 polezer et al 2018 turning this kind of studies a true challenge in developing countries that usually have incomplete databases an alternative methodology consists on artificial neural networks ann widely used in many different types of predictive modeling including air pollution mapping mishra et al 2015 franceschi et al 2018 feng et al 2019 cabaneros et al 2019 cabaneros calautit and hughes cabaneros et al 2019 reported a review about researches aiming to forecast air pollution using ann from 2001 to february 2019 resulting in 139 peer reviewed articles however the application of ann to estimate potential air pollution impacts on population health is less used polezer et al 2018 wang et al 2008 kassomenos et al 2011 tadano et al 2016 sundaram et al 2016 also the frequently used ann consists of multilayer perceptron mlp feedforward ann as the mlp are nonlinear methodologies that are capable of approximate any nonlinear continuous function if it is limited differentiable and present inputs defined in a compact space with arbitrary precision in other words it means that this kind of method can map a set of inputs in an output using a nonlinear way being known as universal approximators which present elevated generalization capability this is exactly the case of the problem we addressed in addition recurrent ann are general cases of the feedforward ones in essence ensembles are extensions of the ann because they only combine the outputs of the previous methods in epidemiological studies the generalized linear models frequently do not achieve satisfactory performance due to the inherent process of free coefficients adjustment coman et al 2008 lauret et al 2016 chelani et al 2002 in recent times the possibility of using ensembles has gained much attention due to their accuracy and efficiency because they combine the output of multiple single models responses trainable ensembles are those in which the combiner is for example an adjustable model like a neural network while a non trainable method consists of a statistical measure of the inputs as the average de mattos neto et al 2014 firmino et al 1016 with this in mind this research proposes the application of ann neural based ensembles and the aforementioned new version of the glm to estimate the impact of air quality on population health the addressed ann were multilayer perceptron mlp radial basis function network rbf extreme learning machines elm echo state networks esn jordan and elman networks the goal was to verify the contribution of these approaches to estimate the number of hospital admissions due to air pollution exposure the case studies consider particulate matter with aerodynamic diameter less than or equal to 10 μ m pm10 and meteorological variables influence on hospital admissions for respiratory diseases at two important brazilian cities campinas and são paulo cities 2 generalized linear model generalized linear models glm are an extension of the classical linear model used for continuous problems this method describes the relationship between one or more prediction variables independent explanatory or covariable x x 1 x p for a wide variety of non distributed responses neuhaus and mcculloch 2011 a key point on glm use is that many of the considerations in its construction are the same for the standard linear regression models since both have many characteristics in common matsuzaki 2017 the glm presents three specifications the random component or the distribution of the response the systematic component the function of the prediction variables and the connection between the other components neuhaus and mcculloch 2011 therefore glm assumes that the response refers to the expected value of the target variable achievable through a linear predictor it is supposed that the model manipulates a combination of inputs and can use polynomial functions or splines which is a smoothing that approximates functions behavior that have local and sudden changes tadano et al 2012 the poisson regression is a specific approach of the glm which further explains that the response variable must follow the same distribution and the data must have equal dispersion neuhaus and mcculloch 2011 according to matsuzaki 2017 in the poisson regression model the response variable describes the number of times that an event occurs in a finite observation space also the function given by equation 1 is the inverse of the function log α called the link function 1 α e β 0 β 1 x 1 β n x n in which α is the predicted value β 0 plays the role of the offset and β 1 β n are the free coefficients that are multiplied by the inputs vector x x 1 x n 2 1 new glm the free coefficients of glm with poisson regression are commonly estimated using the maximum likelihood method paula 2004 dobson and barnett 2008b however it is possible to tune the model using bio inspired metaheuristics like the particle swarm optimization pso the pso was inspired in the intelligent collective behavior of animals groups such as school of fishes and flocks of birds this technique was introduced by eberhart and kennedy 1995 the method works with a group of agents moving through the search space trying to identify the optimal points of the function to be optimized each agent is an independent particle or a candidate solution and during the search process it works together with other agents forming a population in this work each particle is composed by the glm free parameters during the displacement the particles move towards the best position they have reached pbest and in the direction of the best position achieved by some of its neighbors gbest the particle i 1 p has a position vector x with d 1 d dimensions which are changed by the speed operator v these values are updated as shown in equations 2 and 3 2 v i d k 1 ω v i d k c 1 r a n d p b e s t i d x i d k c 2 r a n d g b e s t d x i d k 3 x i d k 1 x i d k v i d k 1 in which v i d k is the velocity of the individual in the iteration k v d m i n v i d k v d m a x ω is the inertia weight factor c 1 e c 2 are the acceleration constants r a n d uniformly generated random numbers between 0 and 1 and x i d k is the current position of the individual in iteration k besides the glm tuning another important remark is to consider the data seasonality according to tadano et al tadano et al 2012 it is an important long term trend that appears in this kind of study as the meteorological variables and concentration of pollutants vary during the year to adjust these components the spline function is often used because it provides a better approximation than simple polynomial interpolations markatos 1985 however it is possible to replace the spline procedure by the deseasonalization method siqueira et al 2014 siqueira et al 2018 it makes the series stationary by means of the application of equation 4 siqueira and luna 2019 4 z i x i μ m σ m being μ m the mean and σ m the standard deviation of each month the new series z i is approximately stationary with zero mean and unit standard deviation 3 artificial neural networks in this study we addressed feedforward networks multilayer perceptron mlp and radial basis function networks rbf recurrent architectures elman and jordan networks and the unorganized machines extreme learning machines and the echo state networks siqueira et al 2014 polezer et al 2018 haykin et al 2009 3 1 multilayer perceptron the multilayer perceptron mlp is considered one of the most important architectures of ann this method presents a set of information processors units known as artificial neurons which form the input layer one or more hidden layers and an output layer as shown in fig 1 haykin et al 2009 the information flows from the input layer passes through the intermediate layers and closes in the output layer generating an output response in a mlp the disjoint layers are connected whereas neurons of the same layer do not communicate the training or the adjustment of their weights is performed in two phases the first one is the forward propagation in which the signals from a training set sample are inserted as inputs of the network being propagated layer by layer in the second phase the errors are propagated in a recursive manner and the weights are adjusted through some adjustment rule haykin et al 2009 siqueira et al 2014 the most commonly used method to tune the mlp is the steepest descent algorithm with the derivatives calculated via the backpropagation method 3 2 radial basis function networks radial basis function networks rbf present their origin in the execution of exact interpolation of a set of data points in a multidimensional space powell 1987 these ann have a simpler structure and a faster training process compared to mlp it is composed of only one intermediate layer in which the neurons present activation functions different from the output layer neurons in this case the activation function inputs are the euclidean distances between the input information and the weight vectors fig 2 shows the rbf architecture in a rbf each neuron in the hidden layer uses a nonlinear radial basis function as activation function this layer performs a nonlinear transformation of the input in this sense the output layer acts as a linear combiner that maps the nonlinearity to a new space the bias of the output layer neurons can be modeled by an additional signal in the previous one which has a constant activation function du and swamy 2014 the rbf training consists of two steps the first one is associated to the adjustment of the weights of the intermediate layer neurons and uses unsupervised learning methods directly related to radial basis functions the second step is the adjustment of the output layer weights which uses for example the generalized delta rule or the backpropagation method similar to the mlp 3 3 neural network of elman and jordan recurrent neural networks rnn are those endowed with feedback loops of information in these cases some output of hidden or output layers are reinserted in some previous one this process provides to the network a kind of internal memory which can be useful to problems with time dependency haykin et al 2009 the elman neural network enn is a recurrent model which presents additional inputs in the hidden layer forming the context layer fig 3 describes the elman network with four layers input hidden intermediate context and output layers ren et al 2018 the context layer stores the output values of the hidden layer of the previous time also excepting the context layer the model is similar to a feedforward network in fig 3 the inputs are represented by u k the outputs are y k the entries for i th neuron of the hidden layer are denoted as v k i the output of the i th hidden layer is given by x k i and the output of the j th context is x k c j the standard method to training an enn is called the elman backpropagation algorithm ebp ren et al 2018 in which the time is treated as an explicit part of the input it allows time to be represented by the effect it has on the process which gives dynamic properties to the system which respond to temporal sequences elman 1990 in short the network becomes endowed with memory when the backpropagation is applied the dependence of x c k to the weights must be taken into account the algorithm that deals with this dependency is the dynamic backpropagation algorithm kuan 1989 local rnn stores the past state of the networks and use it as part of the entry in the next iteration they can be divided into two categories a network with external time delay feedback and a network with internal delay feedback the first case is called jordan neural network jnn ren et al 2018 the main difference in terms of the structure between the enn and jnn is shown in fig 4 while the in an enn the recurrent information comes from the intermediate layer to the input in the jnn the recurrence is from the output layer to the input haykin et al 2009 3 4 unorganized machines unorganized machines um are architectures of artificial neural networks in which the weights of the intermediate layer are not adjusted and are randomly generated because of this their training process are simple and computationally efficient the term unorganized machines was proposed by boccato et al 2011 which evokes alan turing s work on intelligent machine behavior turing in this work we addressed the extreme learning machines and the echo state networks 3 4 1 extreme learning machines according to huang huang et al 2006 the extreme learning machines elm are similar to mlp networks with a single intermediate layer however in this case the weights of the neurons are chosen in a random and independent way the adjustment process is performed only on the output layer which is a linear combiner therefore the training process has a linear computational cost and uses a supervised approach fig 5 shows the structure of a generic elm with its layers here it is represented only one output the input vector is u t u 1 u 2 u t k 1 t its components passed to the hidden layer w h r n h k which contains the random generated weights the output signal of this layer is according to equation 5 5 x t h f h w h u t b in which b is the bias of each neuron and f h is the activation function of the intermediate neurons such activations are linearly combined to produce the network output y t using equation 6 6 y t w o u t x t h being w o u t the matrix of the output layer weights the training process consists of finding the best weights in the output layer the moore penrose operator is an interesting option to solve the task 7 w o u t x h t x h 1 x h t d where x h ε r t s n k is the matrix containing the exit of the intermediate layer t s is the number of samples of the training x h t x h 1 x h t is a pseudo inverse of x h and d is the vector containing the desired output in huang et al 2006 the authors suggest that the generalization capability of the network can be increased by inserting a weighting term c to equation 7 as in 8 8 w o u t i c x h t x h 1 x h t d to determine the value of c the authors indicate to test the 52 possible values of the vector λ 26 25 24 25 so that c 2 λ thus after training a validation set must be used to this term c the authors gave the name regularization coefficient 3 4 2 echo state networks echo state networks esn were proposed by jaeger 2001 they are recurrent structures because they have feedback loops between the neurons which differs it from the elm this feature allows the creation of an intrinsic memory which may be beneficial to problems involving temporal relation between samples the training process is similar to that of the elm which must find the coefficients of a linear combiner based on a least squares problem with reference signal fig 6 shows the generic structure of an esn in which the input vector is u n u n u n 1 u n k 1 t which is transmitted from the input layer w i n to the dynamic reservoir intermediate layer by means of non linear combinations the theoretical element that guarantees the presence of memory in the esn is known as the echo state property jaeger 2001 under specific conditions in the reservoir w the recent history of the inputs governs the internal dynamics of this layer neurons the network weights can thus be previously defined jaeger 2001 yildiz et al 2012 compared to the other rnn the training process of esn is simpler and faster there are practically no problems related to the instability and manipulation of cost functions siqueira et al 2014 as in the elm the task is to find the coefficients of a linear combiner using the moore penrose inverse operator and the regularization coefficient we highlight that the design of the reservoir plays an important role in the development of the esn in this work we addressed the proposals from jaeger 2001 and from ozturk et al 2007 following the steps described in siqueira et al 2014 4 ensembles the literature has shown that the use of ensembles brings performance enhancements to the single models de mattos neto et al 2014 firmino et al 1016 perrone and cooper 1992 according to perrone and cooper 1992 ensembles combine the outputs of various models which were pretuned in the beginning the application of this technique was given by the weighted average calculation of each methods outputs the approach is known as basic ensemble method bem and is given by 9 9 f b e m 1 n i 1 n f i where n is the total number of single models and f i is the output of these models however in recent times several works have presented the possibility of using a neural network as the combinator de mattos neto et al 2014 firmino et al 1016 the idea can be exemplified as follows consider that there are three different ann to estimate a given variable the neural based ensemble will use the point to point predictions of each network and will use them as input then the combinatorial network must be trained in the same traditional way this process tends to lead to better solutions in this work we will use the mlp and rbf networks to perform this task because they are the most important feedforward ann models and other studies addressed them in correlated problems de mattos neto et al 2014 firmino et al 1016 5 case study the case study consists of estimating the number of hospital admissions due to respiratory diseases caused by the exposure to particulate matter pm10 to campinas and são paulo cities brazil according to data from the brazilian institute of geography and statistics ibge campinas had 1 194 094 inhabitants in 2017 being the third most populous municipality in the state of são paulo it occupies an area of 797 6 km2 of which 238 3 km2 are in the urban perimeter and the remaining 559 3 km2 is the rural area de campinas 2014 it presents a tropical climate of high altitude with hot and rainy summers the temperatures are in the range from 13 c to 29 c being rarely less than 9 c or greater than 33 c spark 2018 são paulo city is the capital of the state of são paulo and the main business and commercial financial center of south america it is the most populous city in brazil with 12 176 866 inhabitants according to ibge ibge 2019 it is also the most influential in the global scenario being considered in 2016 the 11th most globalized city on the planet it is 464 years old and has a total area of 1521 11 km2 being 968 32 km2 of urban areas the information about hospital admissions for respiratory diseases were obtained from the brazilian national health system at http www2 datasus gov br datasus index php area 02 datasus 2010 the database from campinas used for the models adjustment and estimation has a total of 732 samples comprising the period from january 1st 2007 to december 31st 2008 during that period 11 121 hospitalizations were detected being 2 the smaller number and 32 the greater to a single day as shown in fig 7 são paulo dataset has 1068 samples in which the number of hospitalizations lies from january 1st 2014 to december 31st 2016 totaling 159 683 occurrences the highest daily number of hospitalizations was 409 as can be observed in fig 8 on the other hand there were days with no attendance regarding respiratory diseases probably due to a matter of database record system or the fact the available data is only from public health system miss considering data from health insurances and private units the pollution concentration pm10 and meteorological data ambient temperature and relative humidity are from air quality monitoring stations administered by environmental company of são paulo state cetesb at https cetesb sp gov br ar qualar cetesb 2010 due to lack of resources são paulo government had only one air quality monitoring station aqms at campinas city nowadays they have one more station that has pm10 data but only since 2016 cetesb 2018 pm10 and meteorological dataset consists of daily averages from the cerqueira cesar station held by cetesb to the studied period only four out of twelve stations had pm10 available data even more only one station has less than 100 days lack due to the ligh linear correlation between data from these four stations we chose data from cerqueira cesar station with only 29 days lack besides that this study considered as an independent input the day of the week and whether the day of hospitalization is a holiday or not these two variables were used because they follow a pattern in epidemiological studies tadano et al 2016 since the number of hospitalizations decreases on weekends and holidays polezer et al 2018 in this way the entries of the models covered contain the following fields date date referring to the day of measurement pm10 concentration of particulate matter with aerodynamic diameter less than or equal to 10 μ m temperature recorded average temperature humidity recorded air relative humidity day day of the week from sunday 1 to saturday 7 holiday whether the day is a holiday 1 or not 0 number of hospitalizations number of hospital admissions due to respiratory problems according to the international classification of diseases icd 10 j00 to j99 in both cases the samples were divided into three sets training containing the first 70 of the data validation with the following 15 and the test set with the remaining 15 that percentage was tested being reasonable by the amount of available data 5 1 results and analyzes to perform an extensive and significant analysis of the models mapping capability the following neural network architectures were elaborated mlp rbf elman network jordan network elm esn jaeger esn ozturk also we perform the models that have the regularization coefficient in their structure elm rc esn jaeger rc esn ozturk rc in addition the ensembles were implemented considering the linear average median mlp and rbf as combinators finally we used the traditional glm model with coefficients calculated by the maximum likelihood estimator and long term trend smoothing with splines and the new proposal optimized by the pso and using the deseasonalization procedure the parameters of the pso were chosen based on previous empirical tests and they are in table 1 all models were developed using the java programming language with the object oriented paradigm under netbeans ide the exception was the traditional glm which was performed using the r software in this case in order to adjust seasonal and long term trends we used the spline function toolbox from the r with value equals to 16 to both cities value based on previous studies in this field the ann were elaborated using only one intermediate layer in the training the amount of artificial neurons was selected in the range from 5 to 200 with increments of 5 the weight adjustment was performed in the training set the cross validation procedure was applied in view to increase the generalization capability of the networks haykin et al 2009 except to the elm and esn without rc the training stop condition was a precision of 10 6 with a maximum of 2000 iterations it is important to mention that all neural networks had the linear identity function in the output neurons as activation function and the hyperbolic tangent in the hidden layer except rbf in the last case the k means algorithm was applied to cluster the centers in addition the weights were generated randomly in the interval 1 1 as well as the data were normalized in the same range then we achieved the output responses the data were renormalized to allow an analysis of the error in the original domain haykin et al 2009 the error metrics used in this study were the mean square error mse the mean absolute error mae and the mean absolute percentage error mape which formulations are given by 10 11 12 10 m s e 1 n s n s t 1 d t y t 2 11 m a e 1 n s n s t 1 d t y t 12 m a p e 1 n s n s t 1 d t y t d t being d t the observed number of hospitalizations and y t the output of the networks in cases in which the metrics indicate distinct models as the best that with the lowest mse was assumed as the best one siqueira et al 2014 all the results estimate the number of hospital admissions for up to seven days after exposure to pm10 as suggests the literature polezer et al 2018 tadano et al 2016 li et al 2015 because air pollution may affect population s health some days after the exposure in this case we applied the models to eight situations lag 0 to lag 7 meaning lag 0 relates the concentration of pm10 from today with the health outcomes of the same day lag 1 relates the concentration of pm10 from today with the health outcomes of tomorrow lag 2 relates the concentration of pm10 from today with the health outcomes two days ahead and son on the general performances of campinas and são paulo are presented in tables 3 and 3 respectively to neural models the number of neurons in their hidden layer n n is also presented the acronym rc represents regularization coefficient and the ensembles are nominated as follows ensemble 1 average ensemble 2 median ensemble 3 mlp and ensemble 4 rbf to all neural models and the glm pso 30 simulations were performed tables 2 and 3 present the best of these 30 cases regarding the mse tzanis et al 2019 alimissis et al 2018 ding et al 2016 finally the friedman test was applied to verify if the results were significantly different in all cases the achieved p values were very close to zero indicating that a change in the model leads to distinct results to campinas among the results obtained to the estimation three days after exposure lag 3 ensemble 3 with 175 neurons was the method with the lowest overall errors considering the 3 aforementioned metrics on the other hand the estimate with worse result was achieved by the traditional glm comparing traditional glm with glm pso the results showed pso improved the performance due to the better capability to perform global and local search simultaneously in the sene of minimizing the output error fig 9 shows the comparison between actual number of hospital admissions blue circles and the results of the test set with the best predictor the ensemble 3 method with 175 neurons to lag 3 red square in campinas we need to highlight that countless factors may affect population s health besides air quality such as people s lifestyle genetics age and so on with that in mind and the poorness of data quality in developing countries like brazil we may assume that the models gave a good estimate of air quality impact on population health following we presented table 3 containing the errors to são paulo estimates the best performance found was again favored to the ensemble using the mlp with 40 neurons to all lags among the obtained results the estimate performed to lag 7 achieved the lowest errors again the superiority of the neural based ensembles in comparison to the linear cases was highlighted fig 10 shows the comparison between the number of actual hospitalizations blue circle and the estimate performed to lag 7 by the ensemble based on the mlp red square as a final analysis it is clear that the comparison among all models was largely favorable to the ensemble combined by mlp regarding the mse the reasons that can lead to this are diverse we can firstly point the insertion of a large variety of input methods such as ann proposals and glm the diversity is important to form accurate outputs as pointed by matos neto et al de mattos neto et al 2014 this diversity is a preponderant factor to a good performance of ensembles however it is important to discuss the use of the metrics addressed to analyse the comparative performance willmott and matsuura 2005 advocate that the most relevant in assessing average model performances is the mae instead of the mse considering this approach in the case of são paulo the best result achieved is related to the elm cr and still lag 7 being the ensemble 3 the second best to campinas the best results for all metrics were achieved by ensemble 3 it is evident that the use of ann is more adequate to the studied cases in comparison to the linear models this is important because the specialized literature of air pollution epidemiology is still very biased towards the use of classical glm especially the implementation in the r software in this sense it is clear that there is still room for improvement of these methods comparing each approach to create the ann the rbf achieved the best overall performances to the feedforward models to the recurrent ones the jordan networks were the best to campinas and the esn from ozturk et al to são paulo this difference between scenarios is a strong indicative that the problem is complex to be solved and the more models available the better may be the analysis through the comparative analysis considering the ensembles it is noticeable that the trainable methods which use ann as combiners achieved the best performances in comparison to the non trainable approaches average and median the main reason is the high capability of the neural models for mapping the data in a nonlinear manner and generalizing the results as shown in tables 2 and 3 the non trainable ensembles can be even worse than the single models in some scenarios therefore it seems to be clear that the ann are more suitable to be used in this case the glm pso proposal introduced in this study still needs to be evaluated in other scenarios as well as more optimization algorithms should be tested in addition the spline methodology presents a degree of complexity often not understood by the researchers therefore replace the spline by a simple deseasonalization process is a factor that favors the understanding of the method and can also lead to better performance the general analysis shows that the glm pso is a competitive candidate in this area as for the number of lags ideal to each case it is difficult to assert an exact value if all models are considered since for each one there was a discrepancy in the best results this is not a surprising factor and it shows how complex the problem is the same observation applies to the number of neurons in the middle layer of neural networks 6 conclusion the general analysis revealed some important points first the glm pso won the classical proposal to são paulo however it is worth noting that the general results strongly indicate that linear models can not achieve lower errors than nonlinear performances second in this same scenario the esn from ozturk et al was able to overcome all the classic recurring methodologies and the most important result is that even with a high dispersion mlp based neural ensemble was the best in both cases the new approach of glm and the use of ann and ensembles showed significant improvements to classical glm this research opens the horizon to the methodologies that may be applied to assess health adverse effects of air pollution as it is a complex problem to be solved expanding for example possibilities of researches at areas with small amount of data and improving predictions we need to highlight that the present research showed the proposed methodologies has succeeded on estimating air quality impact on population health even with the assumption that measured air quality is representative of the whole city and the citizens exposure then we recommend that the models should be tested to more representative data to assure their performance the main difficulty was to capture peaks of hospitalizations that must be considered with more caution in future researches however the results of ann and ensembles were better than those of glm there are an important difference between scenarios which is a strong indicative that the problem is complex to be solved in this sense the more models available the better may be the analysis we understand that the best action to improve air quality is reducing emissions however it is not possible in a short term then due to the good performance of ann and ensembles they could be used to estimate hospital admissions and alert government and hospitals for possible increases of hospitalizations due to high air pollution episodes or adverse meteorological conditions also developing countries like brazil has a lack of information about air quality due to financial resources making it difficult sometimes to estimate air quality impacts on human health using statistical regressions then universal approximators like ann may allow studies of air quality health impacts to be done as it can find relationship between variables even with small databases as a future perspectives to improve and spread the available methodologies used to assess health air pollution impacts it is proposed to use other optimization algorithms for glm such as genetic differential or immuno inspired evolution in the case of neural networks it is worth discussing the use of other ensemble combiners such as elm itself using other weather variables besides temperature and relative humidity other types of pollutants other than pm10 may also be alternatives deep learning techniques depending on the number of data available can also be applied to the problem acknowledgment the authors would like to thank conselho nacional de desenvolvimento científico e tecnológico for the financial support under process number 405580 2018 5 appendix a supplementary data the following is the supplementary data to this article multimedia component 1 multimedia component 1 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2019 104567 
26102,nonpoint source nps pollution is a common problem faced in many places around the world on site evaluation of nps pollution is helpful in deciding how serious the pollution is and where a controlling practice can potentially be implemented with the best effectiveness in this study a web gis interface for the agricultural policy environmental extender apex model named geoapexol was developed the interface serves as a tool to automatically setup the apex model for a watershed or field and conduct a rapid evaluation of the nps pollution a background database including dem landuse soil climate and agricultural management was prepared for automating the process of model setup one important feature of the interface is that nps pollution can be evaluated for a field boundary considering contribution from upstream areas which may be located outside the field boundary and is generally ignored by traditional nps evaluation methods at the field scale keywords geoapexol apex model field and watershed simulation open source software web gis 1 introduction the agricultural policy environmental extender apex model willams et al 2000 was developed in the early 1990s to simulate the impacts of management and land cover changes on landscape processes gassman et al 2010 it was developed initially by combining the environmental policy impact climate epic model izaurralde et al 2006 with a routing module making the model applicable for both single farms as was the epic model or the small watershed scale containing several fields or subareas willams et al 2000 since its development the apex model has continuously evolved 8 key versions and has attained worldwide application ford et al 2016 luo and wang 2019 zhang et al 2018 making it a key tool in solving problems related to nonpoint source nps pollution such as in evaluating the effectiveness of conservation practices francesconi et al 2014 smith et al 2015 tuppad et al 2010 the simulation of macropore phosphorus loss ford et al 2016 and evaluating evapotranspiration for dryland cropping systems tadesse et al 2018 the applicability and accuracy of the apex model have been validated in numerous studies especially by the widely applications in the national assessment of conservation practices on water erosion and sediment loss u s demartment of agriculture 2017 in addition sensitivity analysis of the model s results to parameters wang et al 2006 coupling with auto calibration wang et al 2014 and parameter estimation techniques moriasi et al 2016 all contribute to make apex a strong candidate model for problems involving simulation of landscape processes in addition to scientific representation and simulation of important processes having a user friendly interface for model setup is another necessary component of a popular model currently there are four user interfaces available for facilitating setup of the apex model arcapex winapex i apex and apexonline arcapex tuppad et al 2009 is an arcgis extension for processing spatial input layers for topography digital elevation model dem soil and land cover type and tabulates input data land management soil properties climate data etc into the format required for apex runs winapex is a stand alone windows pc based interface for apex that supports a pplication of the model for a whole farm or small watershed and for optimizing crop management in order to maximize production and profit the i apex tool was developed for the purpose of applying the model at regional scales which requires thousands of model runs neither winapex nor i apex require users to have arcgis software which is required when using arcapex the apexonline interface was recently developed to perform quick evaluations of various land management choices feng et al 2019 users of this interface only need to specify basic properties of their field such as area location soil management etc the interface incorporates soil climate and management databases which are used to prepare corresponding input files for the apex model in this paper we present a new web gis interface for conducting apex runs at the whole farm level or watershed scale called geoapexol the major features of this interface include 1 incorporation of an online map enabling the identification of simulation sites 2 two methods of model setup including watershed and field simulation 3 complete database of the continental us for dem land use soil climate and management enabling automation of model setup processes without tedious data collection and preparation and 4 incorporation of web geographic information system gis technologies for visualizing and processing necessary gis layers into input files required as well as rasterizing results simulated by the apex model with these features the development of this interface was intended to provide a user friendly tool with only a few buttons for setting up the apex model and rapidly obtaining onsite evaluation of nps pollution status for simulation sites of interest 2 structure of the geoapexol interface 2 1 overview of the interface fig 1 shows a screenshot of the geoapexol interface to use the interface users first need to zoom in to a location of interest either through the search box by typing a us zip code or text in the format of city name state abbreviation and press the go button or by the zoom in and out button on the map then they can choose to conduct a watershed or a field simulation a watershed simulation starts with generating a stream network stream network button followed by outlet selection set outlet button and watershed delineation get watersheds button a field simulation begins with drawing a boundary for a field of interest draw a field boundary button followed by delineating all watersheds covered by the field boundary get watersheds button the map provides three background views for navigation a topography view a satellite view and a street view the best view for drawing a field boundary is to choose the satellite view option once users click the stream network button the interface will close the choice of field simulation by disabling the draw a field boundary and get watersheds button for conducting a field simulation after the apex model is setup for either a watershed simulation as shown in fig 2 or a field simulation the run apex model button will appear users can select up to 4 land cover scenarios to run the model and obtain results the simulation results for runoff mm soil erosion ton ha total nitrogen kg ha and total phosphorus kg ha of each subarea of the watershed under all user specified land cover scenarios will be displayed in two forms as shown in fig 2 the average annual values for the four variables of each subarea are rasterized and displayed in the map in addition the exact values are displayed in a table below the button area 2 2 the techniques and database behind the interface the web gis interface includes three major components the front end webpage background database and data processing modules users actions include entering text into the interface to specify the location through the search box clicking the buttons for setting up the apex model and viewing the output tables and maps these are realized with html for webpage construction javascript for user operations like button click and displaying gis layers and php for sending user inputs to the server calling server side functions and receiving server operation status and data these languages are executed through an apache webserver version 2 4 https apache org on an ubuntu server operation system version 1804 www ubuntu com the background database includes spatial layers for the continental us location soil climate and management spatial layers include dem 30 m resolution by u s geological survey and cropland data layer cdl generated by the national agricultural statistics service nass https nassgeodata gmu edu cropscape other databases are stored in an sql postgresql database as described in detail in feng et al 2019 the data processing modules include functions for processing gis layers using the gdal library for generating stream networks and watershed boundaries by calling taudem functions and for processing input output files of the apex model with some python scripts the background display maps for location search are provided by arcgis rest services https services arcgisonline com arcgis rest services the general steps for performing runs of the apex model include 1 obtaining a watershed which will be divided into several subareas with unique combinations of land management soil slope and climate properties 2 writing information on subarea properties to text files in the format required by the apex model and 3 executing the apex model to perform runs when a user zooms to a desired simulation site and decides to conduct a watershed simulation by clicking the stream network button the interface first clips a dem layer at the extent of the map view a map here refers to the map object of the openlayers ol library version 4 6 5 https openlayers org the dem is then used to generate a stream network with the taudem functions listed in fig 3 the next step is to determine the location of the watershed outlet by clicking on the map near a streamline by clicking the set outlet button the map object will generate a point feature and send it to the server as a json file then the user needs to click the get watershed button invoking the execution of another set of taudem functions taking the outlet json file and the output layers generated by the stream network generation step the raster layers for the streamline and watershed are displayed with the mapserver package version 7 2 https mapserver org at this point the gis layers for a watershed with several subareas are prepared for setting up apex model simulations if a user chooses to conduct a field simulation by clicking the draw a field boundary button the map object will generate a polygon feature of the boundary drawn and send it to the server as a json file then by clicking the get watersheds button the interface first creates a buffer polygon of 0 025 around the field polygon and clips a dem layer for the buffer polygon the watershed delineation for a field simulation is different from that for a watershed simulation in that an outlet is not specified without an outlet the taudem library will delineate subareas covering the whole extent of the dem and they are not grouped into a single watershed each subarea has one streamline and they are assigned with the same unique id as well as the ids of their upstream and downstream subareas based on the information the subareas covered by the field boundary are selected and grouped into several watersheds fig 4 after obtaining the watersheds either for the watershed or field simulation the interface will clip the soil and land use layers nass to the extent of the watershed s then the major combinations of soil land use and slope the combination with the largest area in a subarea are identified this information combined with values of climate and management is then written into input files for the apex model finally the executable file of the apex model version 1501 is called and the output files are generated the simulation results are then extracted processed and displayed in the interface 3 summary geoapexol is a web gis interface for quick on site evaluations of the nps pollution status for a watershed or field with comparison of the status under different land cover types currently based on nass cdl 2016 all converted to fallow perennial grass or trees the scripts apply several popular techniques to setup a web gis interface and are highly modular facilitating maintenance in addition we provide comments to assist customizing sharing and expansion of the interface by interested users such as including more control on agricultural management options or evaluation of the effects of best management practices to control nps pollution a detailed introduction on how to redeploy this system is provided in the supporting information the main feature of this interface is the inclusion of both watershed and field simulations compared to setting up the apex model for a watershed simulation the field simulation enables the combination of field and watershed boundaries in evaluating nps pollution this is important for the following reasons usually farms are managed by field boundary instead of watershed boundary nps pollution evaluation conducted at the field scale normally only consider what is happening within the field boundary such evaluation ignores the contribution of pollutants from the upstream area as shown in fig 3 especially when the upstream area is large compared to the field area not considering upstream contributions can introduce huge uncertainties into the evaluation results and may lead to improper and even incorrect decisions in addition the visualization of the results at subarea levels helps identify the location contributing the greatest nps pollution this information can be used in deciding where control practices can be implemented with better effects software and data availability name of software geoapexol developers qingyu feng software required internet browser tested on firefox chrome edge safari and internet explorer programming language php html javascript python and c url of the web page http horizon nserl purdue edu geoapexol source code available https github com qingyufeng geoapexol background database available at https dataverse harvard edu dataset xhtml persistentid doi 3a10 7910 2fdvn 2f2cjw0m first available april 2019 declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgement the authors would like to thank james frankenberger at the usda ars national soil erosion research laboratory for help in designing the framework and debugging the scripts throughout the interface development process the work was supported by the farm service agency united states department of agriculture solicitation number 12315118r0009 we also would like to appreciate the efforts by dr feng pan on testing and providing valuable feedbacks on improving this system appendix a supplementary data the following is the supplementary data to this article multimedia component 1 multimedia component 1 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2019 104569 
26102,nonpoint source nps pollution is a common problem faced in many places around the world on site evaluation of nps pollution is helpful in deciding how serious the pollution is and where a controlling practice can potentially be implemented with the best effectiveness in this study a web gis interface for the agricultural policy environmental extender apex model named geoapexol was developed the interface serves as a tool to automatically setup the apex model for a watershed or field and conduct a rapid evaluation of the nps pollution a background database including dem landuse soil climate and agricultural management was prepared for automating the process of model setup one important feature of the interface is that nps pollution can be evaluated for a field boundary considering contribution from upstream areas which may be located outside the field boundary and is generally ignored by traditional nps evaluation methods at the field scale keywords geoapexol apex model field and watershed simulation open source software web gis 1 introduction the agricultural policy environmental extender apex model willams et al 2000 was developed in the early 1990s to simulate the impacts of management and land cover changes on landscape processes gassman et al 2010 it was developed initially by combining the environmental policy impact climate epic model izaurralde et al 2006 with a routing module making the model applicable for both single farms as was the epic model or the small watershed scale containing several fields or subareas willams et al 2000 since its development the apex model has continuously evolved 8 key versions and has attained worldwide application ford et al 2016 luo and wang 2019 zhang et al 2018 making it a key tool in solving problems related to nonpoint source nps pollution such as in evaluating the effectiveness of conservation practices francesconi et al 2014 smith et al 2015 tuppad et al 2010 the simulation of macropore phosphorus loss ford et al 2016 and evaluating evapotranspiration for dryland cropping systems tadesse et al 2018 the applicability and accuracy of the apex model have been validated in numerous studies especially by the widely applications in the national assessment of conservation practices on water erosion and sediment loss u s demartment of agriculture 2017 in addition sensitivity analysis of the model s results to parameters wang et al 2006 coupling with auto calibration wang et al 2014 and parameter estimation techniques moriasi et al 2016 all contribute to make apex a strong candidate model for problems involving simulation of landscape processes in addition to scientific representation and simulation of important processes having a user friendly interface for model setup is another necessary component of a popular model currently there are four user interfaces available for facilitating setup of the apex model arcapex winapex i apex and apexonline arcapex tuppad et al 2009 is an arcgis extension for processing spatial input layers for topography digital elevation model dem soil and land cover type and tabulates input data land management soil properties climate data etc into the format required for apex runs winapex is a stand alone windows pc based interface for apex that supports a pplication of the model for a whole farm or small watershed and for optimizing crop management in order to maximize production and profit the i apex tool was developed for the purpose of applying the model at regional scales which requires thousands of model runs neither winapex nor i apex require users to have arcgis software which is required when using arcapex the apexonline interface was recently developed to perform quick evaluations of various land management choices feng et al 2019 users of this interface only need to specify basic properties of their field such as area location soil management etc the interface incorporates soil climate and management databases which are used to prepare corresponding input files for the apex model in this paper we present a new web gis interface for conducting apex runs at the whole farm level or watershed scale called geoapexol the major features of this interface include 1 incorporation of an online map enabling the identification of simulation sites 2 two methods of model setup including watershed and field simulation 3 complete database of the continental us for dem land use soil climate and management enabling automation of model setup processes without tedious data collection and preparation and 4 incorporation of web geographic information system gis technologies for visualizing and processing necessary gis layers into input files required as well as rasterizing results simulated by the apex model with these features the development of this interface was intended to provide a user friendly tool with only a few buttons for setting up the apex model and rapidly obtaining onsite evaluation of nps pollution status for simulation sites of interest 2 structure of the geoapexol interface 2 1 overview of the interface fig 1 shows a screenshot of the geoapexol interface to use the interface users first need to zoom in to a location of interest either through the search box by typing a us zip code or text in the format of city name state abbreviation and press the go button or by the zoom in and out button on the map then they can choose to conduct a watershed or a field simulation a watershed simulation starts with generating a stream network stream network button followed by outlet selection set outlet button and watershed delineation get watersheds button a field simulation begins with drawing a boundary for a field of interest draw a field boundary button followed by delineating all watersheds covered by the field boundary get watersheds button the map provides three background views for navigation a topography view a satellite view and a street view the best view for drawing a field boundary is to choose the satellite view option once users click the stream network button the interface will close the choice of field simulation by disabling the draw a field boundary and get watersheds button for conducting a field simulation after the apex model is setup for either a watershed simulation as shown in fig 2 or a field simulation the run apex model button will appear users can select up to 4 land cover scenarios to run the model and obtain results the simulation results for runoff mm soil erosion ton ha total nitrogen kg ha and total phosphorus kg ha of each subarea of the watershed under all user specified land cover scenarios will be displayed in two forms as shown in fig 2 the average annual values for the four variables of each subarea are rasterized and displayed in the map in addition the exact values are displayed in a table below the button area 2 2 the techniques and database behind the interface the web gis interface includes three major components the front end webpage background database and data processing modules users actions include entering text into the interface to specify the location through the search box clicking the buttons for setting up the apex model and viewing the output tables and maps these are realized with html for webpage construction javascript for user operations like button click and displaying gis layers and php for sending user inputs to the server calling server side functions and receiving server operation status and data these languages are executed through an apache webserver version 2 4 https apache org on an ubuntu server operation system version 1804 www ubuntu com the background database includes spatial layers for the continental us location soil climate and management spatial layers include dem 30 m resolution by u s geological survey and cropland data layer cdl generated by the national agricultural statistics service nass https nassgeodata gmu edu cropscape other databases are stored in an sql postgresql database as described in detail in feng et al 2019 the data processing modules include functions for processing gis layers using the gdal library for generating stream networks and watershed boundaries by calling taudem functions and for processing input output files of the apex model with some python scripts the background display maps for location search are provided by arcgis rest services https services arcgisonline com arcgis rest services the general steps for performing runs of the apex model include 1 obtaining a watershed which will be divided into several subareas with unique combinations of land management soil slope and climate properties 2 writing information on subarea properties to text files in the format required by the apex model and 3 executing the apex model to perform runs when a user zooms to a desired simulation site and decides to conduct a watershed simulation by clicking the stream network button the interface first clips a dem layer at the extent of the map view a map here refers to the map object of the openlayers ol library version 4 6 5 https openlayers org the dem is then used to generate a stream network with the taudem functions listed in fig 3 the next step is to determine the location of the watershed outlet by clicking on the map near a streamline by clicking the set outlet button the map object will generate a point feature and send it to the server as a json file then the user needs to click the get watershed button invoking the execution of another set of taudem functions taking the outlet json file and the output layers generated by the stream network generation step the raster layers for the streamline and watershed are displayed with the mapserver package version 7 2 https mapserver org at this point the gis layers for a watershed with several subareas are prepared for setting up apex model simulations if a user chooses to conduct a field simulation by clicking the draw a field boundary button the map object will generate a polygon feature of the boundary drawn and send it to the server as a json file then by clicking the get watersheds button the interface first creates a buffer polygon of 0 025 around the field polygon and clips a dem layer for the buffer polygon the watershed delineation for a field simulation is different from that for a watershed simulation in that an outlet is not specified without an outlet the taudem library will delineate subareas covering the whole extent of the dem and they are not grouped into a single watershed each subarea has one streamline and they are assigned with the same unique id as well as the ids of their upstream and downstream subareas based on the information the subareas covered by the field boundary are selected and grouped into several watersheds fig 4 after obtaining the watersheds either for the watershed or field simulation the interface will clip the soil and land use layers nass to the extent of the watershed s then the major combinations of soil land use and slope the combination with the largest area in a subarea are identified this information combined with values of climate and management is then written into input files for the apex model finally the executable file of the apex model version 1501 is called and the output files are generated the simulation results are then extracted processed and displayed in the interface 3 summary geoapexol is a web gis interface for quick on site evaluations of the nps pollution status for a watershed or field with comparison of the status under different land cover types currently based on nass cdl 2016 all converted to fallow perennial grass or trees the scripts apply several popular techniques to setup a web gis interface and are highly modular facilitating maintenance in addition we provide comments to assist customizing sharing and expansion of the interface by interested users such as including more control on agricultural management options or evaluation of the effects of best management practices to control nps pollution a detailed introduction on how to redeploy this system is provided in the supporting information the main feature of this interface is the inclusion of both watershed and field simulations compared to setting up the apex model for a watershed simulation the field simulation enables the combination of field and watershed boundaries in evaluating nps pollution this is important for the following reasons usually farms are managed by field boundary instead of watershed boundary nps pollution evaluation conducted at the field scale normally only consider what is happening within the field boundary such evaluation ignores the contribution of pollutants from the upstream area as shown in fig 3 especially when the upstream area is large compared to the field area not considering upstream contributions can introduce huge uncertainties into the evaluation results and may lead to improper and even incorrect decisions in addition the visualization of the results at subarea levels helps identify the location contributing the greatest nps pollution this information can be used in deciding where control practices can be implemented with better effects software and data availability name of software geoapexol developers qingyu feng software required internet browser tested on firefox chrome edge safari and internet explorer programming language php html javascript python and c url of the web page http horizon nserl purdue edu geoapexol source code available https github com qingyufeng geoapexol background database available at https dataverse harvard edu dataset xhtml persistentid doi 3a10 7910 2fdvn 2f2cjw0m first available april 2019 declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgement the authors would like to thank james frankenberger at the usda ars national soil erosion research laboratory for help in designing the framework and debugging the scripts throughout the interface development process the work was supported by the farm service agency united states department of agriculture solicitation number 12315118r0009 we also would like to appreciate the efforts by dr feng pan on testing and providing valuable feedbacks on improving this system appendix a supplementary data the following is the supplementary data to this article multimedia component 1 multimedia component 1 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2019 104569 
26103,dam construction in mainland southeast asia has increased substantially in recent years most dams have the potential to generate value however potential and existing impacts include alterations in water regimes loss and degradation of natural forests and bio diversity for mapping impacts we have developed the reservoir mapping tool for the greater mekong region designed in arcgis desktop 10 5 modelbuilder with dam point location and digital elevation model srtm 30 m on a publicly available web interface which provides inundated area and dam volume based on user inputs we validate our results and find excellent agreement with ground data from the lanh ra dam located in vietnam further validation and error quantification are done comparing results of three different dems s and compared with reported values we also illustrate various application areas using this information in combination with other geospatial layers which could provide key inputs towards assessing overall social impacts of dams keywords water resource management reservoir monitoring lower mekong region gis 1 introduction natural resources are under great pressure in southeast asia due to population dynamics and rapid economic development pech and sunada 2008 smith et al 2016 demand for housing infrastructure development mining and energy are key drivers for rapid land cover changes as many people move to the lower middle and upper middle income brackets for example forest resources are being removed for wood production or replaced by plantations because of their high productivity and profitability poortinga et al 2019 there is growing demand for energy water and flood mitigation driving the development of more dams hydropower projects are often though controversial considered as a clean and renewable source of energy while hydropower production is usually considered as the primary goal of hydropower dams other benefits such as water storage reducing flood risk and upstream environmental protection projects to reduce sedimentation risks should also not be ignored ziv et al 2012 grumbine and xu 2011 densely populated countries in southeast asia such as vietnam and thailand have moved from water abundant to water scarce countries in the last decades pressure on forest and water resources combined with climate variability and increasing downstream water demands are main causes of water shortages with insufficient water to meet water supply to all sectors and sustain freshwater and estuarine ecosystems however the wet season is plagued with excessive and non utilizable water resources often in the form of floods causing large scale economic damage and loss of human lives poortinga et al 2017 simons et al 2017 potential climate change impacts might exacerbate this in the future arnell and gosling 2016 tolentino et al 2016 lawrence and vandecar 2015 dams are a popular mitigation measure to balance water between the wet and dry season but bring unique challenges and trade offs to the local environment ziv et al 2012 hurford et al 2014 wang et al 2013 as such there is a growing awareness that hydro power production should be embedded in a broader framework of ecosystem services to align and balance the multiple often competing interests and goals of all stakeholders zarfl et al 2015 dam construction has traditionally been a top down decision making process with large social economic and ecological ramification for local communities and other downstream stakeholders hughes 2017 kirchherr et al 2017 hirsch 2016 direct impacts of dam construction include the involuntarily displacement of large numbers of people but more recent work also focus on the longer term environmental impacts tilt et al 2009 wu et al 2004 nhung and thang 2017 central governments have the capacity and resources to investigate the wide range of potential impacts of dam construction however these analyses are usually focused around technical details and often do not include the vast local institutional and ecological complexity égré and senécal 2003 hence dam construction projects have been the subject of controversy and debate because of their social and environmental impacts along with unequal distribution of costs and benefits siciliano et al 2018 local stakeholders ngos and other stakeholders oftentimes have a good understanding of the institutional and social context but lack technical information to understand the full impact of a dam as such it is evident that there is a need for a simple to use tool to study potential social and ecological impacts of potential dam construction for better planning and management here we present state of the art geographic information science gis technology to model dam construction impacts in the context of the servir mekong program servir mekong is a joint usaid and nasa collaborative project aimed to provide support to dedicated development and sustainable landscape projects in the mekong region servir mekong developed various user friendly tools for example to monitor vegetation development poortinga et al 2018 surface water quality markert et al 2018 and land cover change poortinga et al 2019 using state of the art cloud computing technologies in this work and to address the environmental issues related to dams we present the newly developed online reservoir mapping tool for the greater mekong region it is an open source tool which is available online and provides users estimates of the area and volume of inundation of existing and proposed dams we also provide details on an innovative aspect of this tool which is the utilization of esri s ready to use watershed service tool this tool covers the area of greater mekong region it is primarily developed for the majority of regional water resource management organizations ngos river basin planning organizations regional disaster management organizations and conservation societies in the mekong region which lack required gis skills and resources to carry out the modeling of these dams this tool is presented with use cases from the greater mekong region but can be extended for areas across the globe it is expected to be used for evaluating cumulative impacts of dams on land cover and carbon emissions preliminary alternative scenario analyses for planned dams and application in the risk assessment contexts mentioned above 2 methodology the main focus of the reservoir mapping tool is to map the inundated area and volume of a potential dam in the following sections we first introduce the computational model for the reservoir which is used for area and volume calculations in the following sub section we delineate the software workflow of the tool and next provide details on its system architecture 2 1 computational model both the inundated area and volume are dependent on the full supply level height of the dam and the topography of the upstream area we incorporate these considerations for the reservoir area and volume as follows 2 1 1 reservoir area a schematic overview of the dam area calculation is shown in fig 1 we used a pixel based approach in the application where the potential dam is constructed on a specific base elevation measured in meters above mean sea level the inundated area was then calculated from the dam full supply level height and surface elevation the topography and upstream area was derived from the global shuttle radar topography mission srtm digital elevation model farr et al 2007 jarvis et al 2008 while we have chosen a specific base elevation value at mean sea level generally the reservoir inundation area can be calculated from the dam full supply level height above the ground in meters and lowest elevation value pixel elevation value and pixel resolution as shown in eq 1 below where a reservoir inundation area m2 e i a pixel elevation value of clipped watershed dataset m px i pixel resolution m2 m e lowest elevation value m d h dam full supply level height value meters above ground n number of pixels 2 1 2 reservoir volume the reservoir volume was calculated from the average height value of the reservoir as shown in eq 2 the calculation can be done for each pixel in the upstream area but to speed up the process we multiplied the total area of the reservoir with the average depth 2 v i n h m e a n p x i e i m e d h 0 e i m e d h where 3 h m e a n 1 n i n h i and v reservoir inundation volume m3 h mean mean elevation value of clipped watershed dataset m px i pixel resolution m2 h i d h m e e i m 2 2 workflow the system was built with arcgis modelbuilder using the esri ready to use watershed service tool the ready to use suite is an online set of geoprocessing services which use arcgis online data and analysis capabilities and does not require local data and computational resources the workflow is graphically depicted in fig 2 the required inputs are the location of dam and dam full supply level height which are used in the ready to use watershed service tool to calculate the upstream area as illustrated in fig 3 the upstream area is then used to clip the digital elevation model as illustrated below in fig 4 zonal statistics tool is then used to calculate the minimum elevation of the clipped area which should be equal to the pixel where the potential dam is constructed the full supply level height of the dam is then used to calculate the inundated upstream area on from the gridded data the raster domain tool is then used to construct a polygon of the reservoir area from which the total area is calculated this reservoir area map is then combined with the dem and dam full supply level height information to calculate the volume in some specific case a reservoir polygon includes small non contiguous inundated areas in order to eliminate these small polygons all feature area value will be sorted in descending order and then only the feature with the largest area will be kept as a reservoir area 2 3 system architecture the reservoir mapping tool requires user to input 1 dam point location and 2 estimated dam full supply level height meters above ground as the primary inputs the output of the model is an estimated reservoir polygon area and total storage for a particular dam an innovative part in this study is the use of the esri ready to use watershed service tool which is hosted on arcgis online and can be accessed through an arcgis server connection the watershed service tool is imported to a custom toolbox to identify watershed area based on a particular location the model was published as geoprocessing service in arcgis server 10 5 the arcgis web appbuilder 2 11 is used to consume the service and also to create and customize the application interface and functionalities fig 5 see table 1 2 4 error analysis we compiled a list of dams that are planned commissioned and under construction table 2 provides an overview including the status commercial operation date cod reservoir area and total storage reliable numbers on the full supply level were difficult to obtain most data were derived from a report that was prepared for the worldbank m l in association with lahmeyer gm 2004 the numbers for nam ngiep 1 nam ngum 2 nam ngum 5 and theun hinboun exp were obtained from technical documents of the respective companies operating the dams the reported area and storage estimates were compared with estimate from our tool using different digital elevation models we used the srtm digital elevation data version 4 alos world 3d aw3d30 global digital surface model dsm takaku and tadono 2017 tadono et al 2016 and the aster global digital elevation model version 3 gdem v3 these three products were developed using different methodologies and sensors srtm was created using radar technology whereas the others used stereo photography as the main technology all three datasets have a spatial resolution of approximate 30 m however the data sets have differences in reported vertical accuracy a study of farr et al 2007 reported a vertical absolute height error of less than 16 m for srtm whereas a vertical accuracy of 5 m was reported for alos tadono et al 2014 the aster gdem v3 was recently released therefore we used the 17 m vertical accuracy that was reported for the previous version of the data product elkhrachy 2018 meyer et al 2012 3 results 3 1 web platform interface and functionality the web platform interface is shown in fig 6 and can be accessed from https damtool servir adpc net the interface shows a basemap with the main rivers in the lower mekong region furthermore the river basin and administrative boundaries are shown information on dams was extracted from the water land ecosystems wle database cgiar 2019 this database contains information on the dams that are commissioned planned under construction cancelled proposed and suspended metadata of the dam is included and displayed upon clicking this metadata includes information on commission date capacity mw irrigation area height length costs etc fig 7 the same panel can be used to scroll through the country basin and river information the unep wcmc layer from the un environment world conservation monitoring centre was also included in the interface this layer contains information on areas with special importance such as protected areas and locations of other historical and ecological significance the quick start guide provides users an easy to follow step by step how to operate the tool it contains the five steps of 1 zooming to a location 2 adding a dam 3 specifying the dam full supply level height 4 executing the tool and 5 inspecting the results the results include a shapefile of the inundated area for example the inundated area and volume of the proposed xekong 4 dam is shown in fig 8 users can interactively pan and zoom to a specific dam location or enter the dam name in the search box the tool enables users to analyze the impact of multiple dams 3 2 comparison of area and volume calculation with ground survey record we used the tool to validate estimations on reservoir area and volume with measured ones the lanh ra reservoir in ninh thuan province vietnam fig 9 was selected for the analysis we compared the model outputs with the ground survey record provided by institute for water and environment iwe in vietnam unpublished report iwe 2017 the results for the area and volume estimations are shown in fig 10 a and b respectively it can be seen that there is a good agreement between the estimated and measured values a linear regression analysis shows a coefficient of determination r2 0 99 for both area and volume 3 3 error analysis we estimated the reservoir area and volume for 14 planned dams using three different digital elevation models and compared them to the number we found in reports fig 11 shows reported values in blue and the estimated values from srtm alos and aster in green red en yellow respectively the error bars show the range of values including dem specific uncertainties it can be seen that reported values are generally higher than the estimated values from the dem moreover it was found that the srtm has higher estimates than alos and aster for bigger dams srtm and alos are generally close to the reported values whereas aster underestimates the total reservoir area reservoir volumes were also estimated using the different dem s result of the volume analysis are shown in fig 12 it was found that volume estimations are close to the reported ones for smaller reservoirs estimates for xekong 4 are in line with the reported values for srtm but show smaller volumes for alos and aster volume estimates for xe kaman 1 are lower than the reported ones for all digital elevation models theun hinboen exp also shows an underestimation fig 13 shows the estimated inundation area for 14 dams the color gradient and the inundated areas from the srtm as displayed with different colors for the completed dams san 3a nam ngum 2 anbd 5 theun hinboun exp xe kaman 1 and 3 we found good agreement with inundated areas from high resolution satellite data for the nam dams 4 discussion dams can potentially have profound environmental and social impacts while their primary purpose is oftentimes only cited as either provision of electricity production or flood control and or irrigation the overall negative consequences can be significant and at multiple levels of the society hence the conceptualization of their existing and potential impacts of dam construction are inherently complex due to the multiple dimensions involved which includes consideration of social physical and environmental domains to consider these important facets in assessing the social impacts of dams kirchherr and charles 2016 have developed a matrix framework from the perspective of considering key dimensions and components space time and value are considered as dimensions while components are considered to be infrastructure livelihood and community based on the examples of applications of our tool provided above such information can provide spatio temporal information which can be integrated along with other information into this framework for assessing potential social impacts in a reliable and consistent manner it should be noted that the tool only produces reliable estimates for potential reservoirs of which the bathymetry is represented in the srtm we use the srtm as this is the most widely used and tested digital elevation model however new products like the alos global digital surface model takaku and tadono 2017 tadono et al 2016 can also be used higher quality surface elevation estimates will improve the quality of the outputs of the product the tool was presented in the context of the mekong region but could be extended to all geographical regions in the world using global elevation products whereas user friendliness was a main objective in creating the tool for many stakeholders information on area and volume are not very informative as they require information on e g potential electricity generations number of involuntary displacements or cultural heritage that could be affected also it should be noted that results should be interpreted with caution by qualified end users future improvements might include more exhaustive testing of different scenarios and incorporating the different components and dimensions in ready to use formats the error analysis shows the range of area estimates included the reported vertical error for different products we found best performance for srtm in this study as alos and aster generally underestimated area and volume this could be due to the various differences in these products srtm is a digital terrain model wheres aster and alos are digital surface models area estimates of the tool were in agreement with the reported values whereas volume estimations were generally lower than the reported ones it should be noted here that there is lot of ambiguity in reported area and volume estimates as differences exist between data from different sources as was also reported by räsänen et al 2017 for example the reported value for total reservoir storage for nam kong 1 in this study was 297 million m3 other sources report a 505 million m3 active storage boungnong and phonekeo 2012 and a 683 million m3 wild and loucks 2014 storage capacity our estimates range between 170 and 667 million m3 using aster and srtm respectively the output reservoir inundation polygon can be used to assess the impacts of prospective dams on various built and environmental assets fig 14 shows the impact of a potential dam on environmentally protected areas it can be seen that part of the area will be inundated flooding of natural habitats will lead to the loss of terrestrial wildlife but might promote other terrestrial and aquatic fauna ledec and quintero 2003 protected areas located upstream of a dam might increase the longevity of the dam because of the important role of natural landscapes in soil and water conservation another example we highlight is the impact of dams on deep pools deep pools are defined as confined relatively deep areas within a river channel deep pools are important for a number of important fish species as they act as a refuge for the dry season poulsen and valbo jorgensen 2001 fig 15 shows the location of deep pools and the inundated area of a dam it can be seen that the dam would severely impact deep pools in the area as the reservoir is located upstream and on top of the locations it should be noted that many riverine fish species do not thrive well in artificial lakes ledec and quintero 2003 future versions of the tool could also include explicit information on land cover fig 16 shows the inundated area of the lower sesan 3 dam overlaying this information with data on landcover will provide important information on the potential impacts on e g food production and carbon emissions artificially inundated areas can have a large ecological footprint when vast areas of natural habitat are inundated yu et al 2016 also impacts on other existing infrastructure should be evaluated fig 17 shows effects of a dam on transportation as part of the national highway will be inundated the tool could also include more outputs on e g reservoir yield and dam costs using the comprehensive method proposed by petheram et al 2017 fig 10a and b shows the estimated area and volume for a range of water levels these were created by iteratively running the tool for different water levels the esri storage capacity tool has this functionality built in and could be added to the platform as all required inputs are already calculated 5 conclusion in this work we have provided details on the computational model software development performance and applicability of the reservoir mapping tool developed by servir mekong this production was spurred by needs expressed by local and regional stakeholders due to intensive construction of dams in the lower mekong basin it is available online and is based on open source software features provided by esri we highlight the innovative features of the tool in incorporating esri s ready to use services in particular the watershed service tool while the tool computes dam inundation area and reservoir volume at a high accuracy in comparison with ground data we also provide instances on how other layers of information can be combined with this such outputs to assess impacts on environmentally protected areas biodiversity land cover and transportation acknowledgements the authors would like to thank kim geheb from cgiar research program on water land and ecosystems for supplying the dam data used in this study also the authors would like to thank ariel walcutt for assistance in developing the model and esri for their technical support thanks goes to the three anonymous reviewers for their comments that improved the quality of the manuscript support for this work was provided through the joint us agency for international development usaid and national aeronautics and space administration nasa initiative servir particularly through the nasa applied sciences capacity building program appendix a supplementary data the following is the supplementary data to this article multimedia component 1 multimedia component 1 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2019 104552 
26103,dam construction in mainland southeast asia has increased substantially in recent years most dams have the potential to generate value however potential and existing impacts include alterations in water regimes loss and degradation of natural forests and bio diversity for mapping impacts we have developed the reservoir mapping tool for the greater mekong region designed in arcgis desktop 10 5 modelbuilder with dam point location and digital elevation model srtm 30 m on a publicly available web interface which provides inundated area and dam volume based on user inputs we validate our results and find excellent agreement with ground data from the lanh ra dam located in vietnam further validation and error quantification are done comparing results of three different dems s and compared with reported values we also illustrate various application areas using this information in combination with other geospatial layers which could provide key inputs towards assessing overall social impacts of dams keywords water resource management reservoir monitoring lower mekong region gis 1 introduction natural resources are under great pressure in southeast asia due to population dynamics and rapid economic development pech and sunada 2008 smith et al 2016 demand for housing infrastructure development mining and energy are key drivers for rapid land cover changes as many people move to the lower middle and upper middle income brackets for example forest resources are being removed for wood production or replaced by plantations because of their high productivity and profitability poortinga et al 2019 there is growing demand for energy water and flood mitigation driving the development of more dams hydropower projects are often though controversial considered as a clean and renewable source of energy while hydropower production is usually considered as the primary goal of hydropower dams other benefits such as water storage reducing flood risk and upstream environmental protection projects to reduce sedimentation risks should also not be ignored ziv et al 2012 grumbine and xu 2011 densely populated countries in southeast asia such as vietnam and thailand have moved from water abundant to water scarce countries in the last decades pressure on forest and water resources combined with climate variability and increasing downstream water demands are main causes of water shortages with insufficient water to meet water supply to all sectors and sustain freshwater and estuarine ecosystems however the wet season is plagued with excessive and non utilizable water resources often in the form of floods causing large scale economic damage and loss of human lives poortinga et al 2017 simons et al 2017 potential climate change impacts might exacerbate this in the future arnell and gosling 2016 tolentino et al 2016 lawrence and vandecar 2015 dams are a popular mitigation measure to balance water between the wet and dry season but bring unique challenges and trade offs to the local environment ziv et al 2012 hurford et al 2014 wang et al 2013 as such there is a growing awareness that hydro power production should be embedded in a broader framework of ecosystem services to align and balance the multiple often competing interests and goals of all stakeholders zarfl et al 2015 dam construction has traditionally been a top down decision making process with large social economic and ecological ramification for local communities and other downstream stakeholders hughes 2017 kirchherr et al 2017 hirsch 2016 direct impacts of dam construction include the involuntarily displacement of large numbers of people but more recent work also focus on the longer term environmental impacts tilt et al 2009 wu et al 2004 nhung and thang 2017 central governments have the capacity and resources to investigate the wide range of potential impacts of dam construction however these analyses are usually focused around technical details and often do not include the vast local institutional and ecological complexity égré and senécal 2003 hence dam construction projects have been the subject of controversy and debate because of their social and environmental impacts along with unequal distribution of costs and benefits siciliano et al 2018 local stakeholders ngos and other stakeholders oftentimes have a good understanding of the institutional and social context but lack technical information to understand the full impact of a dam as such it is evident that there is a need for a simple to use tool to study potential social and ecological impacts of potential dam construction for better planning and management here we present state of the art geographic information science gis technology to model dam construction impacts in the context of the servir mekong program servir mekong is a joint usaid and nasa collaborative project aimed to provide support to dedicated development and sustainable landscape projects in the mekong region servir mekong developed various user friendly tools for example to monitor vegetation development poortinga et al 2018 surface water quality markert et al 2018 and land cover change poortinga et al 2019 using state of the art cloud computing technologies in this work and to address the environmental issues related to dams we present the newly developed online reservoir mapping tool for the greater mekong region it is an open source tool which is available online and provides users estimates of the area and volume of inundation of existing and proposed dams we also provide details on an innovative aspect of this tool which is the utilization of esri s ready to use watershed service tool this tool covers the area of greater mekong region it is primarily developed for the majority of regional water resource management organizations ngos river basin planning organizations regional disaster management organizations and conservation societies in the mekong region which lack required gis skills and resources to carry out the modeling of these dams this tool is presented with use cases from the greater mekong region but can be extended for areas across the globe it is expected to be used for evaluating cumulative impacts of dams on land cover and carbon emissions preliminary alternative scenario analyses for planned dams and application in the risk assessment contexts mentioned above 2 methodology the main focus of the reservoir mapping tool is to map the inundated area and volume of a potential dam in the following sections we first introduce the computational model for the reservoir which is used for area and volume calculations in the following sub section we delineate the software workflow of the tool and next provide details on its system architecture 2 1 computational model both the inundated area and volume are dependent on the full supply level height of the dam and the topography of the upstream area we incorporate these considerations for the reservoir area and volume as follows 2 1 1 reservoir area a schematic overview of the dam area calculation is shown in fig 1 we used a pixel based approach in the application where the potential dam is constructed on a specific base elevation measured in meters above mean sea level the inundated area was then calculated from the dam full supply level height and surface elevation the topography and upstream area was derived from the global shuttle radar topography mission srtm digital elevation model farr et al 2007 jarvis et al 2008 while we have chosen a specific base elevation value at mean sea level generally the reservoir inundation area can be calculated from the dam full supply level height above the ground in meters and lowest elevation value pixel elevation value and pixel resolution as shown in eq 1 below where a reservoir inundation area m2 e i a pixel elevation value of clipped watershed dataset m px i pixel resolution m2 m e lowest elevation value m d h dam full supply level height value meters above ground n number of pixels 2 1 2 reservoir volume the reservoir volume was calculated from the average height value of the reservoir as shown in eq 2 the calculation can be done for each pixel in the upstream area but to speed up the process we multiplied the total area of the reservoir with the average depth 2 v i n h m e a n p x i e i m e d h 0 e i m e d h where 3 h m e a n 1 n i n h i and v reservoir inundation volume m3 h mean mean elevation value of clipped watershed dataset m px i pixel resolution m2 h i d h m e e i m 2 2 workflow the system was built with arcgis modelbuilder using the esri ready to use watershed service tool the ready to use suite is an online set of geoprocessing services which use arcgis online data and analysis capabilities and does not require local data and computational resources the workflow is graphically depicted in fig 2 the required inputs are the location of dam and dam full supply level height which are used in the ready to use watershed service tool to calculate the upstream area as illustrated in fig 3 the upstream area is then used to clip the digital elevation model as illustrated below in fig 4 zonal statistics tool is then used to calculate the minimum elevation of the clipped area which should be equal to the pixel where the potential dam is constructed the full supply level height of the dam is then used to calculate the inundated upstream area on from the gridded data the raster domain tool is then used to construct a polygon of the reservoir area from which the total area is calculated this reservoir area map is then combined with the dem and dam full supply level height information to calculate the volume in some specific case a reservoir polygon includes small non contiguous inundated areas in order to eliminate these small polygons all feature area value will be sorted in descending order and then only the feature with the largest area will be kept as a reservoir area 2 3 system architecture the reservoir mapping tool requires user to input 1 dam point location and 2 estimated dam full supply level height meters above ground as the primary inputs the output of the model is an estimated reservoir polygon area and total storage for a particular dam an innovative part in this study is the use of the esri ready to use watershed service tool which is hosted on arcgis online and can be accessed through an arcgis server connection the watershed service tool is imported to a custom toolbox to identify watershed area based on a particular location the model was published as geoprocessing service in arcgis server 10 5 the arcgis web appbuilder 2 11 is used to consume the service and also to create and customize the application interface and functionalities fig 5 see table 1 2 4 error analysis we compiled a list of dams that are planned commissioned and under construction table 2 provides an overview including the status commercial operation date cod reservoir area and total storage reliable numbers on the full supply level were difficult to obtain most data were derived from a report that was prepared for the worldbank m l in association with lahmeyer gm 2004 the numbers for nam ngiep 1 nam ngum 2 nam ngum 5 and theun hinboun exp were obtained from technical documents of the respective companies operating the dams the reported area and storage estimates were compared with estimate from our tool using different digital elevation models we used the srtm digital elevation data version 4 alos world 3d aw3d30 global digital surface model dsm takaku and tadono 2017 tadono et al 2016 and the aster global digital elevation model version 3 gdem v3 these three products were developed using different methodologies and sensors srtm was created using radar technology whereas the others used stereo photography as the main technology all three datasets have a spatial resolution of approximate 30 m however the data sets have differences in reported vertical accuracy a study of farr et al 2007 reported a vertical absolute height error of less than 16 m for srtm whereas a vertical accuracy of 5 m was reported for alos tadono et al 2014 the aster gdem v3 was recently released therefore we used the 17 m vertical accuracy that was reported for the previous version of the data product elkhrachy 2018 meyer et al 2012 3 results 3 1 web platform interface and functionality the web platform interface is shown in fig 6 and can be accessed from https damtool servir adpc net the interface shows a basemap with the main rivers in the lower mekong region furthermore the river basin and administrative boundaries are shown information on dams was extracted from the water land ecosystems wle database cgiar 2019 this database contains information on the dams that are commissioned planned under construction cancelled proposed and suspended metadata of the dam is included and displayed upon clicking this metadata includes information on commission date capacity mw irrigation area height length costs etc fig 7 the same panel can be used to scroll through the country basin and river information the unep wcmc layer from the un environment world conservation monitoring centre was also included in the interface this layer contains information on areas with special importance such as protected areas and locations of other historical and ecological significance the quick start guide provides users an easy to follow step by step how to operate the tool it contains the five steps of 1 zooming to a location 2 adding a dam 3 specifying the dam full supply level height 4 executing the tool and 5 inspecting the results the results include a shapefile of the inundated area for example the inundated area and volume of the proposed xekong 4 dam is shown in fig 8 users can interactively pan and zoom to a specific dam location or enter the dam name in the search box the tool enables users to analyze the impact of multiple dams 3 2 comparison of area and volume calculation with ground survey record we used the tool to validate estimations on reservoir area and volume with measured ones the lanh ra reservoir in ninh thuan province vietnam fig 9 was selected for the analysis we compared the model outputs with the ground survey record provided by institute for water and environment iwe in vietnam unpublished report iwe 2017 the results for the area and volume estimations are shown in fig 10 a and b respectively it can be seen that there is a good agreement between the estimated and measured values a linear regression analysis shows a coefficient of determination r2 0 99 for both area and volume 3 3 error analysis we estimated the reservoir area and volume for 14 planned dams using three different digital elevation models and compared them to the number we found in reports fig 11 shows reported values in blue and the estimated values from srtm alos and aster in green red en yellow respectively the error bars show the range of values including dem specific uncertainties it can be seen that reported values are generally higher than the estimated values from the dem moreover it was found that the srtm has higher estimates than alos and aster for bigger dams srtm and alos are generally close to the reported values whereas aster underestimates the total reservoir area reservoir volumes were also estimated using the different dem s result of the volume analysis are shown in fig 12 it was found that volume estimations are close to the reported ones for smaller reservoirs estimates for xekong 4 are in line with the reported values for srtm but show smaller volumes for alos and aster volume estimates for xe kaman 1 are lower than the reported ones for all digital elevation models theun hinboen exp also shows an underestimation fig 13 shows the estimated inundation area for 14 dams the color gradient and the inundated areas from the srtm as displayed with different colors for the completed dams san 3a nam ngum 2 anbd 5 theun hinboun exp xe kaman 1 and 3 we found good agreement with inundated areas from high resolution satellite data for the nam dams 4 discussion dams can potentially have profound environmental and social impacts while their primary purpose is oftentimes only cited as either provision of electricity production or flood control and or irrigation the overall negative consequences can be significant and at multiple levels of the society hence the conceptualization of their existing and potential impacts of dam construction are inherently complex due to the multiple dimensions involved which includes consideration of social physical and environmental domains to consider these important facets in assessing the social impacts of dams kirchherr and charles 2016 have developed a matrix framework from the perspective of considering key dimensions and components space time and value are considered as dimensions while components are considered to be infrastructure livelihood and community based on the examples of applications of our tool provided above such information can provide spatio temporal information which can be integrated along with other information into this framework for assessing potential social impacts in a reliable and consistent manner it should be noted that the tool only produces reliable estimates for potential reservoirs of which the bathymetry is represented in the srtm we use the srtm as this is the most widely used and tested digital elevation model however new products like the alos global digital surface model takaku and tadono 2017 tadono et al 2016 can also be used higher quality surface elevation estimates will improve the quality of the outputs of the product the tool was presented in the context of the mekong region but could be extended to all geographical regions in the world using global elevation products whereas user friendliness was a main objective in creating the tool for many stakeholders information on area and volume are not very informative as they require information on e g potential electricity generations number of involuntary displacements or cultural heritage that could be affected also it should be noted that results should be interpreted with caution by qualified end users future improvements might include more exhaustive testing of different scenarios and incorporating the different components and dimensions in ready to use formats the error analysis shows the range of area estimates included the reported vertical error for different products we found best performance for srtm in this study as alos and aster generally underestimated area and volume this could be due to the various differences in these products srtm is a digital terrain model wheres aster and alos are digital surface models area estimates of the tool were in agreement with the reported values whereas volume estimations were generally lower than the reported ones it should be noted here that there is lot of ambiguity in reported area and volume estimates as differences exist between data from different sources as was also reported by räsänen et al 2017 for example the reported value for total reservoir storage for nam kong 1 in this study was 297 million m3 other sources report a 505 million m3 active storage boungnong and phonekeo 2012 and a 683 million m3 wild and loucks 2014 storage capacity our estimates range between 170 and 667 million m3 using aster and srtm respectively the output reservoir inundation polygon can be used to assess the impacts of prospective dams on various built and environmental assets fig 14 shows the impact of a potential dam on environmentally protected areas it can be seen that part of the area will be inundated flooding of natural habitats will lead to the loss of terrestrial wildlife but might promote other terrestrial and aquatic fauna ledec and quintero 2003 protected areas located upstream of a dam might increase the longevity of the dam because of the important role of natural landscapes in soil and water conservation another example we highlight is the impact of dams on deep pools deep pools are defined as confined relatively deep areas within a river channel deep pools are important for a number of important fish species as they act as a refuge for the dry season poulsen and valbo jorgensen 2001 fig 15 shows the location of deep pools and the inundated area of a dam it can be seen that the dam would severely impact deep pools in the area as the reservoir is located upstream and on top of the locations it should be noted that many riverine fish species do not thrive well in artificial lakes ledec and quintero 2003 future versions of the tool could also include explicit information on land cover fig 16 shows the inundated area of the lower sesan 3 dam overlaying this information with data on landcover will provide important information on the potential impacts on e g food production and carbon emissions artificially inundated areas can have a large ecological footprint when vast areas of natural habitat are inundated yu et al 2016 also impacts on other existing infrastructure should be evaluated fig 17 shows effects of a dam on transportation as part of the national highway will be inundated the tool could also include more outputs on e g reservoir yield and dam costs using the comprehensive method proposed by petheram et al 2017 fig 10a and b shows the estimated area and volume for a range of water levels these were created by iteratively running the tool for different water levels the esri storage capacity tool has this functionality built in and could be added to the platform as all required inputs are already calculated 5 conclusion in this work we have provided details on the computational model software development performance and applicability of the reservoir mapping tool developed by servir mekong this production was spurred by needs expressed by local and regional stakeholders due to intensive construction of dams in the lower mekong basin it is available online and is based on open source software features provided by esri we highlight the innovative features of the tool in incorporating esri s ready to use services in particular the watershed service tool while the tool computes dam inundation area and reservoir volume at a high accuracy in comparison with ground data we also provide instances on how other layers of information can be combined with this such outputs to assess impacts on environmentally protected areas biodiversity land cover and transportation acknowledgements the authors would like to thank kim geheb from cgiar research program on water land and ecosystems for supplying the dam data used in this study also the authors would like to thank ariel walcutt for assistance in developing the model and esri for their technical support thanks goes to the three anonymous reviewers for their comments that improved the quality of the manuscript support for this work was provided through the joint us agency for international development usaid and national aeronautics and space administration nasa initiative servir particularly through the nasa applied sciences capacity building program appendix a supplementary data the following is the supplementary data to this article multimedia component 1 multimedia component 1 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2019 104552 
26104,technological advances in the development of tools for modelling and simulating the behaviour and characteristics of urban drainage networks such as epa swmm represent a great opportunity for managers and infrastructure planners to analyse and evaluate infrastructure resilience against several scenarios in this sense the digitalization of existing urban drainage networks becomes an important bottleneck for many small water entities that having the required data lack the expertise to complete all the preparatory steps before running the simulation the solution comes hand in hand with developing an appropriate unique tool that enables a seamless integration of heterogeneous data sources and makes simple their comprehension by the implementation of customised and integrated gis systems that work as a specialist assistant keywords smart digitalisation heterogeneous data sources integrated urban drainage system quantitative risk analysis urban flooding 1 introduction the operation and maintenance of sewer and drainage networks make possible the management of grey and black water avoiding flooding episodes the spread of diseases and the treatment of wastewater before its release to the receiving waters these aging infrastructures are now being analysed and updated in view of new requirements related to updated land uses economic activities and climate change zischg et al 2019 yazdi et al 2018 the complexity of the process involved and the huge uncertainty related to some of the main variables rainfall events population habits difficult this analysis haasnoot et al 2018 urich and rauch 2014 beck 1987 there are several software packages that simulate the integrated urban drainage system storm water management model swmm huber 2001 hydroworks infoworks cs wallingford software ltd 1995 or epa swmm swmm huber 2001 and many other scientific proposals mannina and viviani 2010 freni et al 2009 achleitner et al 2007 the open software tools are many times the preferred option for many small water utilities or city townhalls in charge of providing water services even though the dynamic simulation of the drainage is not the bottleneck many practitioners are not able to fulfil the simulation analysis liu et al 2015 willems 2014 willems et al 2014 indeed expert knowledge and the use of additional tools is generally required vezzaro et al 2014 bach et al 2014 for the digitization of network maps generation of bases of data that show updated information on the network inventory the definition of basins and sub basins the definition of realistic rain patterns close to local data etc the implementation of tools that facilitate the integration and use different data layers through gis based techniques kulkarni et al 2014 has been recognised as a great facilitator for a more generalised use of simulation models that integration would certainly help derive most uncertain variables and the impacts of different long term perspective scenarios urich and rauch 2014 2 software architecture estolzain platform presents a flexible modular design fig 1 that allows an easy reuse of data and results by using a a dynamic data management system b analyser module that checks the topology consistence the hydraulic continuity estimates the sub basins and assess the infiltration processes c connectivity to epa swmm as simulation engine visualization facilities that include friendly tables and charts integrated into a gis system 2 1 data importer topology importer topography information of urban drainage networks are generally defined in dwg or dxf format the dwg format is a computerized drawing file format mainly used by the autocad program dwg files describe the layout of the drainage network through a set of geographical elements that characterise the properties of the collectors and conduits such as a unique identifier localization geographical coordinates or purpose rainfall fecal or mixed etc geotiff importer elevation models are defined in digital files that contain either points vector or pixels raster with each point or pixel having specific elevation value this module allows the importing and processing of these type of files that contain information needed to for example calculate the watersheds of the urban drainage network shapefile importer information about land use demographic distribution or the existence of special geographic features such as water wells rivers and lakes are commonly defined in shapefiles represented and spatial vectors point lines and polygons 2 2 data management system all data imported to build the urban drainage network management model as well as the results from simulation from epa swmm are stored in a relational database for future reference the database allows to reuse the information from the original files into different simulation studies the tool facilitates a catalogue of network elements basin area characterisation and simulation features as nodes identification conduits identification terrain properties rainfall historical data simulation times numerical solvers etc 2 3 communication with simulation engine epa swmm the integration of the epaswmm software in estolzain has been done natively that is the source code of the simulation engine has been compiled in a library and integrated in estolzain architecture the simulation module of estozain allows to define the main parameters that epa swmm needs to execute the simulation these parameters refer to the start and end date of the simulation the parameters of the numerical resolution the frequency of reporting results etc the communication between the data management system and the simulation engine is possible thanks to two text files an inp file describes the input data and after the simulation a rpt output file is retrieved so that estolzain is able to represent the results in user friendly tables and maps 2 4 gis visualization traditional static maps have a limited exploratory capability the graphical representations are inextricably linked to the geographical information beneath gis visualization represents a set of cartographic technologies and practices that take advantage of the ability of modern microprocessors to render changes to a map in real time this module visualizes all the information imported to the platform estolzain software package implements an alarm system that helps the decision making process the results of the uncertainty propagation are showed up in terms of network flooding the areas with major probability of being overpassed are pointed out in red colour those with a medium probability in orange and those with minor risk of flooding in yellow the state of charge of the conduits uses a similar colour code 3 model building process estolzain presents a set of facilities that guides and helps the model building process 3 1 topology analyser this tool facilitates a first realistic assessment of network topology including the estimation of the deep of wells and absolute height slope of conduits etc the continuity of the network is checked and if necessary corrected by implementing tolerance checking tools introducing virtual nodes where different slopes are apparent connecting nodes within a tolerance distance checking and pointing out slopes negative or higher than 20 when not corresponding with terrain data etc these corrective actions ensure the network continuity and the hydrology consistency preventing from simulation errors 3 2 basin and sub basin estimation based on the terrain elevation model this module delineates watershed boundaries for hydrologic analysis and simulation purposes each watershed is divided in smaller areas so that each subarea discharges into a specific collector this operation is performed in two steps estimation of the entire basin area related to a principal collector starting from each node the algorithm moves into increasing values of terrain elevation until defining the contour of the entire basin the sub basins are defined using voronoid diagrams so that each sub area discharges in a single node or well the algorithm is able to assess the different catchment areas and to distribute evenly the runoff on the sewer system the hydrological sub basins gather the rain flow values defined manually time series data or stochastically generated by a rain emulator see section 3 4 the wastewater generation is estimated from water consumption data and distinguishes residential commercial and industrial daily generation patterns 3 3 land use characterisation and run off model the simulation tool integrates land use data based on corine land cover clc clc2012 system with shape files shp and symbolic layers clcl lyr based on etrs89 utm 30 n reference system https land copernicus eu eagle files eagle related projects pt clc conversion to fao lccs3 dec2010 the system includes clc specifications using 3 classification levels and 44 classes current version does not consider the lithology of the area or groundwater as modelling premise we understand that the run off is produced after overpassing the water height admission capacity of each subcatchment estolzain software tool produce a runoff flow per area depending on the rainfall pattern sub catchment area average width of catchment area slope terrain permeability etc the infiltration rate is estimated according to horton model as a geographical variable for each land use and each catchment area considering a percentage value of the manning coefficient for each of the 44 land classes 3 4 stochastic rainfall emulator estolzain software tool integrates a rainfall generator able to generate realistic rainfall data series based on historical data the software uses a markov chain to define the raining and non raining days and a weibull distribution to adjust the quantity frequencies on a daily basis mm day current version considers seasonal variability of precipitations using ad hoc model parameters for winter spring summer and autumn and leading as many rain events as statistically observed for any given historical set of rainfall data mm day estolzain is able to produce other synthetically generated series of realistic scenarios months or years featured on the same frequency mm day as an additional functionality estolzain can make a temporal downscaling of rainfall data providing synthetically generated rainfall events mm 5 min if this option is selected the total precipitation of the day mm day will be distributed featuring the duration and intensity of the events accordingly to given information that is either intensity duration frequency curves featured by historical data or other local methods able to characterise the duration pattern and return period of registered precipitations 3 5 quantitative risk analysis estolzain implements latin hypercube sampling lhs method to draw random sets of uncertain variables given a probability distribution it can also consider as uncertain variables those patterns generated by other stochastic engines at this stage we have considered rainfall intensity mm h generated as realistic emulation of historical data see section 3 4 wastewater generation as a function of a uniformly distributed variable representing the water consumption of a certain sector or district that would be evenly distributed among the water consumption points and multiplied by corresponding pattern domestic commercial or industrial random sets of these parameters are launched as inputs for uncertainty propagation the assessment of results is made in terms of epa swmm outputs sewer flow water volume and height of water column at wells etc the 95 85 and 75 percentiles of these variables are computed at each conduit and well using a colour code as explained at 2 4 representing important risk threshold values in view of decision making 4 san sebastian case study an illustrative example 4 1 catchment area characterisation we analyse the most west district area of donostia san sebastián including the districts of intxurrondo larratxo herrera and trintxerpe this area covers 7 535 km2 and a set of basins discharging to pasajes bay this area covers various neighbourhoods about 6 500 inhabitants industrial areas open green fields siustegi castelao basotxiki etc and industrial ports the network length is about 90 km and has about 5 200 wells the analysis of the results is based on local data about the catchment area wastewater generation is featured according to population references and land use data extended intensive urban areas green areas etc economic activity commercial activity industrial sites etc and population data historical rainfall data from 87 years feature the daily precipitation and serve to generate realistic rainfall scenarios we characterise intensity duration frequency curves according to five local stations txalin ulia oriabenta merkelin and urdaneta located in a short distance less than 5 kms that have registered 5 min intensity precipitations for 5 years the pattern of rain events is analysed according to local data 5 rain stations and verified that it holds well with stochastic engine rainfall generator section 3 4 the scenario analysis is implemented based on local data and methods nevertheless estolzain software is ready to use intensity duration frequency curves of any other location these curves can easily be updated accordingly to climate change scenarios based on local studies for example those published for madrid region lastra et al 2015 4 2 network and catchment characterisation the sanitation network is loaded from an autcad file in which the location of the wells and conduits is defined estolzain reads all the elements and digitize them applying the required checking and corrective actions the software facilitates the simulation of the entire network given a minimum set of information that includes the area to be analysed and the original projection of the data of nodes and conduits in the absence of more information the system assumes a default value of 1 2 m for the diameter of the nodes representing the wells and the material of the conduits pvc equivalent fig 2 shows the interactive windows in which users move from the map to the table checking the details of selected elements the conduits in red depict too high slopes 20 higher than the terrain indicating the modeller to review and verify that the data related to the elevation of the land and the land uses in the study area has allowed to define the basins and watersheds both pluvial and faecal around the main nodes of the network fig 3 the delimitation of the basins is an important step to define the area of influence and the impact of the runoff on the drainage system in this way it is possible to assign the faecal contribution to each basin and consequently to each well for the subsequent simulations at all times the user can access the properties of the elements of the network by clicking on them 4 3 simulation results estolzain provides a friendly gis based graphical interface that facilitates the understanding of the results fig 4 shows the 24 h simulation result where the network receives a 6 h rain event with respect to flow depth the most charged conduits over 85 of their capacity are depicted in yellow the detail of most critical wells shows that the street cross between peruene kalea and herrera pasalekua is a critical point the time series data may also help in the sense that explains when occur the peak values which is directly related to the impact of possible collapses or damages notice that this is a real example where detailed rainfall data 5 min frequency can provide valuable information of an entire district the graphs may be customised to show different simulation variables and to use adequate thresholds depending on the strategic management decisions 4 4 quantitative risk assessment rainfall simulator is first validated producing a rainfall series of 365 days in which storm events appear to happen according to analysed statistics fig 5 shows the time series together with anderson darling test p value 0 05 certifying that there is not significant difference between the generated and historical data once the rainfall simulator is validated we generate rainfall events mm 5 min using idf curves that feature return periods of 10 years and 25 years the synthetic generator works in this case using lhs on the duration scale and leading realistic rainfall patters according to different return periods fig 6 shows the results of the uncertainty propagation when using a 25 year return period a set of 500 simulations provides the variability of flow level flow velocity volume height etc their 75 85 and 95 percentage values provide the result of the quantitative risk assessment the results make clear the zones of the drainage system with higher probability of being collapsed in brown colour in this sense the area around perune kalea circled in red comprises the main sewer and seems to be most affected by intense rainfall events and represent the area with the highest risk of being flooded the system has been built in a heavily constructed area that accumulates draining waters from large districts these results can serve as a first starting point to make a more complete behavioural analysis of the entire city 5 conclusions a new software package called estolzain for the simulation of urban drainage systems has been presented the main novelty is connected to the gis based assistant able to connect different sources of information characterising terrain elevation digital terrain models land uses network topology etc the software includes two simulation engines a stochastic emulator of rainfall that generates synthetic data and is connected to epa swmm software for open channel hydrology which makes the new tool independent and easily useable by many small water utilities estolzain is built on a dynamic and flexible programming approach being easily extensible with new modules or new data the gis based visualization of results is ready to work on line providing update real time information about sewer network status and facilitating the maintenance of the system the decision making process can also be helped by the quantitative risk assessment that points out the most sensitive areas where new investments would make a clear impact in terms of resilience software availability requests should be sent to jose luis badiola jlbadiola seipa es for exploitation and use and will be available under specific commercial agreements please contact cristina martín andonegui cristina andonegui deusto es for further research purposes or innovative developments acknowledgments this work has been carried out with the financial support of estolzain project id 00541 granted by the basque government and also supported by plan for the promotion of innovation 2015 of the regional government of biscay appendix a supplementary data the following is the supplementary data to this article multimedia component 1 multimedia component 1 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2019 104568 
26104,technological advances in the development of tools for modelling and simulating the behaviour and characteristics of urban drainage networks such as epa swmm represent a great opportunity for managers and infrastructure planners to analyse and evaluate infrastructure resilience against several scenarios in this sense the digitalization of existing urban drainage networks becomes an important bottleneck for many small water entities that having the required data lack the expertise to complete all the preparatory steps before running the simulation the solution comes hand in hand with developing an appropriate unique tool that enables a seamless integration of heterogeneous data sources and makes simple their comprehension by the implementation of customised and integrated gis systems that work as a specialist assistant keywords smart digitalisation heterogeneous data sources integrated urban drainage system quantitative risk analysis urban flooding 1 introduction the operation and maintenance of sewer and drainage networks make possible the management of grey and black water avoiding flooding episodes the spread of diseases and the treatment of wastewater before its release to the receiving waters these aging infrastructures are now being analysed and updated in view of new requirements related to updated land uses economic activities and climate change zischg et al 2019 yazdi et al 2018 the complexity of the process involved and the huge uncertainty related to some of the main variables rainfall events population habits difficult this analysis haasnoot et al 2018 urich and rauch 2014 beck 1987 there are several software packages that simulate the integrated urban drainage system storm water management model swmm huber 2001 hydroworks infoworks cs wallingford software ltd 1995 or epa swmm swmm huber 2001 and many other scientific proposals mannina and viviani 2010 freni et al 2009 achleitner et al 2007 the open software tools are many times the preferred option for many small water utilities or city townhalls in charge of providing water services even though the dynamic simulation of the drainage is not the bottleneck many practitioners are not able to fulfil the simulation analysis liu et al 2015 willems 2014 willems et al 2014 indeed expert knowledge and the use of additional tools is generally required vezzaro et al 2014 bach et al 2014 for the digitization of network maps generation of bases of data that show updated information on the network inventory the definition of basins and sub basins the definition of realistic rain patterns close to local data etc the implementation of tools that facilitate the integration and use different data layers through gis based techniques kulkarni et al 2014 has been recognised as a great facilitator for a more generalised use of simulation models that integration would certainly help derive most uncertain variables and the impacts of different long term perspective scenarios urich and rauch 2014 2 software architecture estolzain platform presents a flexible modular design fig 1 that allows an easy reuse of data and results by using a a dynamic data management system b analyser module that checks the topology consistence the hydraulic continuity estimates the sub basins and assess the infiltration processes c connectivity to epa swmm as simulation engine visualization facilities that include friendly tables and charts integrated into a gis system 2 1 data importer topology importer topography information of urban drainage networks are generally defined in dwg or dxf format the dwg format is a computerized drawing file format mainly used by the autocad program dwg files describe the layout of the drainage network through a set of geographical elements that characterise the properties of the collectors and conduits such as a unique identifier localization geographical coordinates or purpose rainfall fecal or mixed etc geotiff importer elevation models are defined in digital files that contain either points vector or pixels raster with each point or pixel having specific elevation value this module allows the importing and processing of these type of files that contain information needed to for example calculate the watersheds of the urban drainage network shapefile importer information about land use demographic distribution or the existence of special geographic features such as water wells rivers and lakes are commonly defined in shapefiles represented and spatial vectors point lines and polygons 2 2 data management system all data imported to build the urban drainage network management model as well as the results from simulation from epa swmm are stored in a relational database for future reference the database allows to reuse the information from the original files into different simulation studies the tool facilitates a catalogue of network elements basin area characterisation and simulation features as nodes identification conduits identification terrain properties rainfall historical data simulation times numerical solvers etc 2 3 communication with simulation engine epa swmm the integration of the epaswmm software in estolzain has been done natively that is the source code of the simulation engine has been compiled in a library and integrated in estolzain architecture the simulation module of estozain allows to define the main parameters that epa swmm needs to execute the simulation these parameters refer to the start and end date of the simulation the parameters of the numerical resolution the frequency of reporting results etc the communication between the data management system and the simulation engine is possible thanks to two text files an inp file describes the input data and after the simulation a rpt output file is retrieved so that estolzain is able to represent the results in user friendly tables and maps 2 4 gis visualization traditional static maps have a limited exploratory capability the graphical representations are inextricably linked to the geographical information beneath gis visualization represents a set of cartographic technologies and practices that take advantage of the ability of modern microprocessors to render changes to a map in real time this module visualizes all the information imported to the platform estolzain software package implements an alarm system that helps the decision making process the results of the uncertainty propagation are showed up in terms of network flooding the areas with major probability of being overpassed are pointed out in red colour those with a medium probability in orange and those with minor risk of flooding in yellow the state of charge of the conduits uses a similar colour code 3 model building process estolzain presents a set of facilities that guides and helps the model building process 3 1 topology analyser this tool facilitates a first realistic assessment of network topology including the estimation of the deep of wells and absolute height slope of conduits etc the continuity of the network is checked and if necessary corrected by implementing tolerance checking tools introducing virtual nodes where different slopes are apparent connecting nodes within a tolerance distance checking and pointing out slopes negative or higher than 20 when not corresponding with terrain data etc these corrective actions ensure the network continuity and the hydrology consistency preventing from simulation errors 3 2 basin and sub basin estimation based on the terrain elevation model this module delineates watershed boundaries for hydrologic analysis and simulation purposes each watershed is divided in smaller areas so that each subarea discharges into a specific collector this operation is performed in two steps estimation of the entire basin area related to a principal collector starting from each node the algorithm moves into increasing values of terrain elevation until defining the contour of the entire basin the sub basins are defined using voronoid diagrams so that each sub area discharges in a single node or well the algorithm is able to assess the different catchment areas and to distribute evenly the runoff on the sewer system the hydrological sub basins gather the rain flow values defined manually time series data or stochastically generated by a rain emulator see section 3 4 the wastewater generation is estimated from water consumption data and distinguishes residential commercial and industrial daily generation patterns 3 3 land use characterisation and run off model the simulation tool integrates land use data based on corine land cover clc clc2012 system with shape files shp and symbolic layers clcl lyr based on etrs89 utm 30 n reference system https land copernicus eu eagle files eagle related projects pt clc conversion to fao lccs3 dec2010 the system includes clc specifications using 3 classification levels and 44 classes current version does not consider the lithology of the area or groundwater as modelling premise we understand that the run off is produced after overpassing the water height admission capacity of each subcatchment estolzain software tool produce a runoff flow per area depending on the rainfall pattern sub catchment area average width of catchment area slope terrain permeability etc the infiltration rate is estimated according to horton model as a geographical variable for each land use and each catchment area considering a percentage value of the manning coefficient for each of the 44 land classes 3 4 stochastic rainfall emulator estolzain software tool integrates a rainfall generator able to generate realistic rainfall data series based on historical data the software uses a markov chain to define the raining and non raining days and a weibull distribution to adjust the quantity frequencies on a daily basis mm day current version considers seasonal variability of precipitations using ad hoc model parameters for winter spring summer and autumn and leading as many rain events as statistically observed for any given historical set of rainfall data mm day estolzain is able to produce other synthetically generated series of realistic scenarios months or years featured on the same frequency mm day as an additional functionality estolzain can make a temporal downscaling of rainfall data providing synthetically generated rainfall events mm 5 min if this option is selected the total precipitation of the day mm day will be distributed featuring the duration and intensity of the events accordingly to given information that is either intensity duration frequency curves featured by historical data or other local methods able to characterise the duration pattern and return period of registered precipitations 3 5 quantitative risk analysis estolzain implements latin hypercube sampling lhs method to draw random sets of uncertain variables given a probability distribution it can also consider as uncertain variables those patterns generated by other stochastic engines at this stage we have considered rainfall intensity mm h generated as realistic emulation of historical data see section 3 4 wastewater generation as a function of a uniformly distributed variable representing the water consumption of a certain sector or district that would be evenly distributed among the water consumption points and multiplied by corresponding pattern domestic commercial or industrial random sets of these parameters are launched as inputs for uncertainty propagation the assessment of results is made in terms of epa swmm outputs sewer flow water volume and height of water column at wells etc the 95 85 and 75 percentiles of these variables are computed at each conduit and well using a colour code as explained at 2 4 representing important risk threshold values in view of decision making 4 san sebastian case study an illustrative example 4 1 catchment area characterisation we analyse the most west district area of donostia san sebastián including the districts of intxurrondo larratxo herrera and trintxerpe this area covers 7 535 km2 and a set of basins discharging to pasajes bay this area covers various neighbourhoods about 6 500 inhabitants industrial areas open green fields siustegi castelao basotxiki etc and industrial ports the network length is about 90 km and has about 5 200 wells the analysis of the results is based on local data about the catchment area wastewater generation is featured according to population references and land use data extended intensive urban areas green areas etc economic activity commercial activity industrial sites etc and population data historical rainfall data from 87 years feature the daily precipitation and serve to generate realistic rainfall scenarios we characterise intensity duration frequency curves according to five local stations txalin ulia oriabenta merkelin and urdaneta located in a short distance less than 5 kms that have registered 5 min intensity precipitations for 5 years the pattern of rain events is analysed according to local data 5 rain stations and verified that it holds well with stochastic engine rainfall generator section 3 4 the scenario analysis is implemented based on local data and methods nevertheless estolzain software is ready to use intensity duration frequency curves of any other location these curves can easily be updated accordingly to climate change scenarios based on local studies for example those published for madrid region lastra et al 2015 4 2 network and catchment characterisation the sanitation network is loaded from an autcad file in which the location of the wells and conduits is defined estolzain reads all the elements and digitize them applying the required checking and corrective actions the software facilitates the simulation of the entire network given a minimum set of information that includes the area to be analysed and the original projection of the data of nodes and conduits in the absence of more information the system assumes a default value of 1 2 m for the diameter of the nodes representing the wells and the material of the conduits pvc equivalent fig 2 shows the interactive windows in which users move from the map to the table checking the details of selected elements the conduits in red depict too high slopes 20 higher than the terrain indicating the modeller to review and verify that the data related to the elevation of the land and the land uses in the study area has allowed to define the basins and watersheds both pluvial and faecal around the main nodes of the network fig 3 the delimitation of the basins is an important step to define the area of influence and the impact of the runoff on the drainage system in this way it is possible to assign the faecal contribution to each basin and consequently to each well for the subsequent simulations at all times the user can access the properties of the elements of the network by clicking on them 4 3 simulation results estolzain provides a friendly gis based graphical interface that facilitates the understanding of the results fig 4 shows the 24 h simulation result where the network receives a 6 h rain event with respect to flow depth the most charged conduits over 85 of their capacity are depicted in yellow the detail of most critical wells shows that the street cross between peruene kalea and herrera pasalekua is a critical point the time series data may also help in the sense that explains when occur the peak values which is directly related to the impact of possible collapses or damages notice that this is a real example where detailed rainfall data 5 min frequency can provide valuable information of an entire district the graphs may be customised to show different simulation variables and to use adequate thresholds depending on the strategic management decisions 4 4 quantitative risk assessment rainfall simulator is first validated producing a rainfall series of 365 days in which storm events appear to happen according to analysed statistics fig 5 shows the time series together with anderson darling test p value 0 05 certifying that there is not significant difference between the generated and historical data once the rainfall simulator is validated we generate rainfall events mm 5 min using idf curves that feature return periods of 10 years and 25 years the synthetic generator works in this case using lhs on the duration scale and leading realistic rainfall patters according to different return periods fig 6 shows the results of the uncertainty propagation when using a 25 year return period a set of 500 simulations provides the variability of flow level flow velocity volume height etc their 75 85 and 95 percentage values provide the result of the quantitative risk assessment the results make clear the zones of the drainage system with higher probability of being collapsed in brown colour in this sense the area around perune kalea circled in red comprises the main sewer and seems to be most affected by intense rainfall events and represent the area with the highest risk of being flooded the system has been built in a heavily constructed area that accumulates draining waters from large districts these results can serve as a first starting point to make a more complete behavioural analysis of the entire city 5 conclusions a new software package called estolzain for the simulation of urban drainage systems has been presented the main novelty is connected to the gis based assistant able to connect different sources of information characterising terrain elevation digital terrain models land uses network topology etc the software includes two simulation engines a stochastic emulator of rainfall that generates synthetic data and is connected to epa swmm software for open channel hydrology which makes the new tool independent and easily useable by many small water utilities estolzain is built on a dynamic and flexible programming approach being easily extensible with new modules or new data the gis based visualization of results is ready to work on line providing update real time information about sewer network status and facilitating the maintenance of the system the decision making process can also be helped by the quantitative risk assessment that points out the most sensitive areas where new investments would make a clear impact in terms of resilience software availability requests should be sent to jose luis badiola jlbadiola seipa es for exploitation and use and will be available under specific commercial agreements please contact cristina martín andonegui cristina andonegui deusto es for further research purposes or innovative developments acknowledgments this work has been carried out with the financial support of estolzain project id 00541 granted by the basque government and also supported by plan for the promotion of innovation 2015 of the regional government of biscay appendix a supplementary data the following is the supplementary data to this article multimedia component 1 multimedia component 1 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2019 104568 
