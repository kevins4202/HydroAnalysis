index,text
25670,although it is widely acknowledged as a fundamental principle that models be fit for purpose there remains lack of clarity on what this notion actually means and therefore how it is achieved we contend that fitness for purpose must go beyond the functional use of the model to include its management problem and project contexts accordingly we propose a practical framework that considers fit for purpose modeling as the intersection of three requirements in that the modeling be useful addressing the needs of the end user reliable obtaining an adequate level of certainty or trust and feasible within practical constraints of the project modeling choices including the selection of spatial and temporal scales system features and processes to include and the type of model can be better informed when the bounds of these fit for purpose requirements are defined we focus on modeling in decision and management support settings and demonstrate the framework with ecohydrological models designed for managing environmental flows by explicitly linking its intended functional use and context to modeling choices this framework can facilitate the design and development of environmental models that more effectively bridge science and management keywords fit for purpose modeling best practices model design e flows ecological models environmental water 1 introduction good modeling practice and fitness for purpose as with any scientific methodology to be of value modeling must be conducted with rigor using good practices good modeling practice is critical in ensuring the development of credible models that are appropriately applied and thereby enhance science or management outcomes whereas there is considerable agreement and indeed encouragement that models be fit for purpose e g jakeman et al 2006 beven and young 2013 anderson et al 2015 guillera arroita et al 2015 there remains ambiguity on what this actually means and therefore how it is achieved we argue that fitness for purpose must embrace the whole modeling process and consider the broader setting of the modeling and problem context to ensure it is realized effectively a model must not only be useful in producing information relevant to user management or policy needs such as the desired quantities at appropriate scales under different forcing conditions as argued in section 2 in addition to meeting user needs a model that is fit for purpose must also be trustworthy and scientifically credible i e reliable as well as practically feasible to achieve its intended outcomes this paper adopts the broad definition of models encompassing both numerical simulation and statistical models and conceptual diagrams and heuristics whereas the model itself can be a key output from a modeling exercise or project substantial value comes from the process of developing and evaluating the model from this view modeling can play various roles including 1 as a methodology in processing knowledge whereby data knowledge and assumptions are systematically organized for a specific purpose jakeman et al 2008 2 as a learning process where models are considered working hypotheses of how a system functions and model evaluation is a form of hypothesis testing beven 2000 2006 and 3 as an artefact or boundary object that serves as a point of interaction between scientists managers and stakeholders to facilitate communication and knowledge exchange white et al 2010 therefore modeling is not only a technical procedure but also a learning and social process hamilton et al 2019 iwanaga et al 2021 a number of good modeling practice guidelines are available and are recommended to readers including those for environmental modeling jakeman et al 2006 modeling for integrated water resources management badham et al 2019 black et al 2014 hydrological modeling harmel et al 2014 and ecological modeling schuwirth et al 2019 the key components of good modeling practice as shared in those papers include 1 clear definition of model purpose 2 selection of scales and model features that align with the management or research question 3 uncertainty analysis 4 model evaluation and 5 transparent communication of the model and its assumptions while those papers contain valuable advice in improving modeling practices and explain reasons why these practices are needed we contend that they provide limited advice to practitioners on specific modeling choices which typically are highly contextual while it is of course not possible to provide detailed advice for each specific modeling choice we propose a practical framework that enables the practitioner to think through the various options they are presented with throughout the modeling process and to justify why certain decisions were made this paper aims to provide a framework to guide the design of fit for purpose models for supporting environmental management and decision making models are recognized as being uniquely characterized by their purpose and contextual requirements anderson et al 2015 we advocate that practitioners strive to develop models that are holistically fit for purpose as opposed to focusing solely on more usual standards such as predictive or forecasting ability which are not the only nor necessarily the most appropriate goal we also stress that fit for purpose is not necessarily a black or white concept that is a model is either fit for purpose or not in reality there are different levels of fitness given that we regard fitness for purpose as involving multiple considerations it is unlikely that there is a single most fit for purpose model for any given environmental project but rather a qualitative type of pareto set of most appropriate models practitioners are therefore encouraged to think in terms of fitness for purpose as a spectrum of possibilities and to consider and assess their modeling choices in view of how well the model purpose is addressed and how well the given contexts are accommodated in the next section we describe the notion of fitness for purpose present a framework that characterizes the components of this notion and explain how it can be applied in guiding choices made during the modeling process section 2 we focus on modeling in decision and management support settings although the proposed concepts in the framework can be used beyond this we also describe and demonstrate the use of the conceptual framework through the lens of ecohydrological modeling so as to provide more concrete examples of its application section 3 2 fitness for purpose framework there has not been a clear or at least well accepted definition of what constitutes fit for purpose modeling beven and young 2013 on the one hand this may be due to the answer seeming obvious i e fit for purpose equates to being well suited for the intended purpose of the model on the other hand the term is often ambiguous and left open to interpretation for example while some may view purpose simply as how the model is used e g the model is to be used to compare the ecological impact of alternative water allocation options we contend that the purpose of the model goes beyond just its functional use to include its contextual factors including the management context e g how will the information be used to inform decisions what is the goal of the management the problem context e g who will be affected by the decision how well are the system processes understood and the project context e g what resources are available to develop the model therefore we assert the notion of fitness for purpose needs improved framing and clarification so that it can be consistently and successfully applied in modeling exercises in making choices about the scope and features of the model and selecting the modeling approach we propose that the following three requirements are considered 1 usefulness how well the model addresses the needs of the end user in particular the intended purpose of the model 2 reliability the level of certainty and trust in the model and or the modeling process and 3 feasibility whether the model can be developed given the practical constraints of the exercise or project including scope data time funding and other resources if the modeling is useful reliable and feasible for its given context it can be regarded as fit for purpose fig 1 usefulness reliability and feasibility can be considered in three contexts respectively the user and use setting e g management the problem and the project each of these contexts imposes modeling needs and constraints that ought to be addressed if not totally determined from the outset in order to design a model that is fit for purpose in table 1 we list a range of key considerations that are pertinent to describing these three contexts and a number of criteria that may be relevant in evaluating what is useful reliable and feasible see hamilton et al 2019 for more details on the various evaluation criteria many of these considerations interact both within and across the three requirements there is some overlap between our proposed fitness for purpose framework and the work of cash et al 2003 which asserted that science and technology needs to be salient credible and legitimate to be effective in linking knowledge to action we extend the three conditions highlighted by cash et al 2003 to incorporate other more practical criteria with specific relation to modeling and provide a practice framework for considering each to be useful the model should aim to meet end user needs and be suitable in the management or decision making context this necessitates understanding what the intended purpose and function of the model is who will use the model including their skills and competencies how it is to be used and what model features are needed to enable the user to run interpret and update the model the evaluation criteria corresponding to usefulness relate to salience how relevant the modeling is in terms of addressing the end user questions incorporating end user input and providing timely scientific advice and model accessibility the usability of the model its software platform and its outputs both in the short and long term hamilton et al 2019 for the modeling in a project or exercise to be reliable there needs to be an appropriate level of certainty and trust in the model its mechanisms its results the modeler and or in the process of developing and applying the model determining the acceptable level of certainty in the model requires understanding of the problem context in particular how well the system and its processes are understood the nature of the problem is it a well defined routine problem or a wicked ill defined one with no single correct solution and the social and environmental complexity of the problem which can include for example the level of conflict between stakeholders the level of reliability may also relate to the use of the model for instance models used as evidence in decisions facing legal challenges will face a high level of scrutiny and must demonstrate their adherence to rigorous scientific method özkundakci et al 2018 the criteria for judging reliability can relate to the credibility and legitimacy of the modeling hamilton et al 2019 and how uncertainty is treated and if it can be reduced credibility pertains to the technical and scientific validity of the modeling whereas legitimacy describes the fairness of the process and the representation of values views and concerns of different stakeholders cash et al 2003 feasibility concerns the resources for example data knowledge computational budget and expertise that can be accessed for the given modeling project the criteria for assessing feasibility may also need to relate to whether the modeling can be completed by the project deadline and within the monetary budget hamilton et al 2019 these various constraints do not necessarily preclude modeling but rather set the bounds on what type of modeling can be undertaken in a practical sense including its scope and level of detail therefore these constraints are crucial to consider in the planning and design stage of the modeling process in many respects data and knowledge falls in the intersection between feasibility and reliability we attempt to distinguish feasibility as a component in regard to the specific project e g data and knowledge held by or accessible to the project team whereas reliability concerns the problem and its system in general e g existing data and knowledge on the system we describe each of the three fit for purpose components further with examples from ecohydrological modeling in section 3 2 1 interpreting and applying the framework the fit for purpose modeling framework fig 1 can be viewed to represent the potential space of models that can be developed for a given project and problem ideally a model should be designed and developed to sit within the intersection between the three requirement circles so that it is fit for purpose we argue that models outside of the fit for purpose space may be ineffective and have problems being completed or used as intended for instance a model designed with reliability and usefulness but not feasibility in mind may be idealistic and not possible to complete within the given timeframe or budget a model that is considered scientifically reliable and feasible but not useful to the end user may be shelved achieving no impact and representing a waste of money and resources on the other hand a model that is considered useful and feasible but not reliable e g prioritizing client needs above all else may achieve some impact perhaps limited to some groups but its results may be heavily scrutinized and deemed untrustworthy particularly in a management context it is therefore fundamental that the relevant criteria and bounds of each of the fit for purpose requirement sets are clarified from the outset in the model planning and design and are at the forefront of decisions taken during the model building and evaluation phases although these requirements interact the goal is to maximize the overlap rather than trade off between attributes as a conceptual framework the venn diagram of fig 1 shows the space of modeling options not the level of fitness and the size of the circles and their intersections can be used to represent the relative space of modeling choices clearly the size and overlap of the circles representing the three fit for purpose requirements would vary for each project some of the challenges faced by projects that subsequently limit options available to model developers can be conceptualized by the framework fig 2 examples of common challenges in modeling projects are limited resources including insufficient time funding computational budget data or expertise i e the feasibility circle is small where there is high uncertainty in understanding or representing the system processes i e the reliability circle is small and the end user has limited technical skills a narrow perspective of how models are used or there is a high ambiguity in the delivery space i e the usefulness circle is small we also propose that there may be scope to expand the size of individual fit for purpose requirement circles to increase the modeling options available to developers thereby better enabling the design of a fit for purpose model for example the abovementioned challenges may be overcome by increasing the resources available to the project e g more funds or the collection of more data investing in improving the specific type of modeling or understanding the system processes i e increasing the range of appropriate tools available and increasing the end user s capacity to use the tool e g through capacity training better documentation or developing a user friendly software interface fig 2 while the options to expand the bounds of the sets may not always be possible for the project at hand particularly for feasibility and reliability they could represent areas for future research and investment there may however be more practical options available for increasing the usefulness and accessibility of the model by providing end users with training or technical support material for running and interpreting the model such as user guides tutorials and shared workflows e g moeck et al 2015 pianosi et al 2020 or by allocating resources to a well designed and intuitive graphical user interface that enables better accessibility and ease of use e g wi et al 2017 there are mounting calls for closer and more effective engagement between modelers scientists and end users e g by voinov and bousquet 2010 falconi and palmer 2017 and gray et al 2018 which can only facilitate the achievement of fit for purpose modeling this can be represented as a shifting of the fit for purpose requirement circles so there is a higher level of agreement about the model purpose and contexts and subsequently a larger intersection of the circles fig 3 this greater overlap and opportunity can be achieved for example through refining and better articulating the research question to key goals more realistic expectations of outcomes from end users aligning the modeling approach with available data and through reuse and adaptation of existing modeling approaches to the current project context 2 2 linking the fitness for purpose framework to the main modeling choices with a model defined as an abstraction of reality we can then consider modeling to provide a specific perspective of a real system depending on assumptions made and decisions taken any one system can be viewed from many perspectives and thus be represented by many different models the critical choices made in developing a model coincide with selecting a perspective and finding the most appropriate modeling approach to represent that chosen perspective we argue that those choices around the perspectives of the system to be modeled and how it should be modeled should target the fit for purpose intersection of the framework fig 1 in this paper we consider four main sets of choices made during the modeling 1 setting the bounds of the modeling 2 selecting the key features of the perspective 3 deciding on an appropriate modeling approach and 4 testing and evaluating the model fig 2 as model development progresses various considerations are assembled with regard to the usefulness reliability and feasibility of the model which subsequently guide modeling choices these considerations are summarized in table 2 the first choices made relate to setting the bounds of the modeling and are equivalent to the scoping and planning phase described in hamilton et al 2015 and badham et al 2019 wherein the modeling objectives problem definition system boundaries and model functions are clarified the mantra that a model s objectives should guide all the modeling phases goes back a long way e g forrester 1961 in terms of the fitness for purpose framework the first modeling phase is centered on not only identifying the modeling objectives but also understanding the user and management context the problem context and the project context in order to clarify what is required for ensuring that the modeling is useful reliable and feasible the second set of choices revolves around selecting a perspective of the system to represent that is within the usefulness reliability and feasibility bounds determined in the previous stage this includes consideration of what information is being sought by the end user what variables and scales are critical to the relevant system processes and what level of complexity can be incorporated given the project resources in selecting the modeling approach the practitioner is to consider the range of available approaches that can capture the key model features determined earlier the selection of that approach should take into account the accessibility of the model and its outputs to the end user the level of understanding of system processes required to build the model the complexity of the model build and the data requirements in some cases there may be a need to adopt legacy models due to client preference or legislation which can constrain choices after building the model with the selected approach the model is to be tested and evaluated using methods appropriate for the given contexts the choice of model testing and evaluation methods should take into account the acceptable level of uncertainty given the problem at hand how the uncertainties and limitations are best communicated to users how any model uncertainty can be identified and if required reduced as well as the methods that are feasible given available resources these methods may comprise a mix of quantitative and qualitative types e g refsgaard et al 2007 the application of this fitness for purpose framework in guiding modeling choices is illustrated below by examining a range of options available in developing ecohydrological models for supporting the management of environmental flows 3 case study models for managing environmental flows 3 1 background to environmental flow models the diverse mosaic of species and communities across rivers and their floodplains reflects a legacy of hydrologic conditions under which river flows shape the physical and biological environments and drive many ecological processes ward et al 1999 merritt et al 2010 water resource development including infrastructure and extraction has been a major driver in the degradation of freshwater and estuarine ecosystems across the globe nilsson et al 2005 poff and zimmerman 2010 environmental flow management is about ensuring that river environments receive the water necessary to sustain their ecosystems in terms of the quantity timing and quality of flows arthington et al 2018 ecohydrological modeling which explicitly considers the interplay between hydrology and ecosystems has become a critical tool for determining environmental flow needs and assessing ecological outcomes of altered flow regimes in river systems tharme 2003 poff et al 2010 practitioners developing ecohydrological models are faced with a number of fundamental challenges one is the difficulty in isolating changes in biota due to hydrological variations from changes due to other factors such as physical habitat destruction or the introduction of alien species craig et al 2017 moreover ecosystems are affected by a myriad of diverse factors many of which interact with one another potentially obscuring or exacerbating the impact of changes to the water resource a second challenge relates to the variable response of biota to hydrological changes not only between species but also within e g life stage and origin a third challenge relates to scale issues including how ecological responses can vary at different spatial or temporal scales this is partly due to the numerous causal mechanisms functioning over various temporal and spatial scales these aforementioned challenges cannot be completely overcome and as a result of these features and others inherent in natural systems substantial uncertainties will remain unavoidable in ecohydrological modeling the use and uptake of ecohydrological models and indeed ecological models in general over the last couple of decades have been hindered by criticisms including poor match between model predictions and field observations high variability and or uncertainty of outputs and models being too complex and difficult to understand or to parameterize and implement lester 2020 webb et al 2017 these criticisms can stem from a misunderstanding of the role of modeling on the one hand but also from models that were not developed appropriately to suit their intended purpose on the other hand in addition to this there are different norms and philosophies held in the hydrological and ecological domains regarding how to handle uncertainty hunt and wilcox 2003a b for example many hydrologists are comfortable with hard predictions and estimating quantifiable changes to system states while most ecologists are less comfortable with such hard quantities and prefer using system tendencies captured by statistics we contend that the gap between the development and use of ecohydrological models can be closed through a better understanding of the role of modeling by modelers users and other practitioners and through better modeling choices that are aligned with the given contexts below we discuss options available when developing ecohydrological models and demonstrate how the fitness for purpose framework can be applied to guide these modeling choices 3 2 setting the bounds the initial stage of model development should involve understanding the purpose and contexts of the modeling especially the user and management context in order to set out the key requirements the problem and project contexts also set the bounds on what type of modeling is possible and appropriate table 1 outlines the key questions that need to be considered to set the fit for purpose bounds that is what is required to make the modeling useful reliable and feasible limited data availability is a common issue in ecohydrology including the fundamental fact that many species lack data on their basic biology tonkin et al 2019 clarifying these three bounds early is critical as this information determines choices made in the other stages of the modeling process ecohydrological models can be applied to explore ecological consequences of changes in flow to determine the flow regimes needed to meet or maintain ecological targets tharme 2003 webb et al 2017 or to set or refine management objectives through iterative discovery fu et al 2015 the models can summarize the state of knowledge of dependencies between flow and ecology including the sensitivity of ecology to flow change and based on these relationships provide guidance for decision making on structural or policy interventions or investments in future monitoring schuwirth et al 2019 webb et al 2015 typically the purpose of an ecohydrological model is a combination of these identifying the decision or management context entails understanding what information is being sought for example new developments may require decision makers to consider what impact the anticipated modifications to the water resource may have on particular species or ecological communities ahn et al 2014 alternatively decision makers may be faced with the challenge of allocating water between different uses e g irrigation public water supply industry mining and the environment and need to know how much water can be taken from the system without overly affecting ecosystems chen and olden 2017 in other cases the decision makers may already have a volume of water set aside for the environment and be looking at how the water can be best distributed or released to maximize ecological outcomes hough et al 2019 richter and thomas 2007 understanding the management goals including the ecological assets or values being targeted and over what spatial scale will help inform the appropriate perspective that the modeling needs to take and the type of output required thus there is a contrast for instance between a model output in the form of a generalizable solution that aims to be applicable across larger spatial extents with one seeking a specific solution that is focused on a certain place or target species there would also be different expectations of the reliability of the modeling if used to support management of a threatened species compared to a more common species for the former the potential implications of the model being wrong e g contributing to the demise of a species means higher levels of model uncertainty may be considered unacceptable understanding the end user group and their requirements is also critical this includes understanding their technical skill level in ecology hydrology and modeling why the information is being sought and how the information will be used as well as how they intend to run maintain and update the model this information can be used to help ensure that the model is created to be accessible to the user in the short to long term 3 3 selecting key model features the set of choices regarding key model features entails selecting the different aspects of the system to represent in the model and at what spatiotemporal scale s and level of detail or complexity table 3 in most if not all cases it is expected that at least one of these aspects i e related to scale or focal point are pre determined by the purpose of the model and the primary management goal e g protection of a particular ecological community kingsford and auld 2005 or water allocation within a given catchment or area wei et al 2012 the challenge comes in deciding on what other aspects to capture particularly where there may be interactions between attributes such as spatial and temporal scales to employ while there is no universally correct perspective on any problem it is crucial to recognize that the choice of perspective affects how the system is portrayed and understood including what elements or processes are considered or excluded and the nature of the relationship between elements thus impacting the outputs and outcomes of the modeling the focal point of the model should be driven by management priorities or stakeholder interest the selection of a particular taxa ecological community or environmental asset to focus on may be based on its ecological and or cultural value in terms of its significance or rarity e g iconic internationally recognized or threatened species its high diversity its sensitivity or its representativeness of a particular habitat umbrella species or group of species e g functional group casanova 2011 roberge and angelstam 2004 stratford et al 2016 there may be management interest in focusing on particular hydrological conditions for example critical periods of extreme low flow when competition for water between users is highest or periods of high flow which may represent flood risks juarez lucas and kibler 2016 stewart et al 2020 the spatial and temporal scale of the chosen perspective may be based on the management scale e g management jurisdiction planning horizon including existing monitoring or the scale of key ecological and hydrological processes the temporal scales of most ecological and hydrological processes are linked to their spatial scales processes that occur at micro or site scales tend to occur over short timeframes and those that operate over larger e g catchment or basin spatial scales tend to have larger temporal scales of change frissell et al 1986 the selection of scale can thus impact how well relevant processes can be captured in investigations or represented in the model iwanaga et al 2021 newman et al 2006 schuwirth et al 2019 the scale of detail or level of complexity that can be captured in an ecohydrological model is constrained by project resources data and knowledge it is necessary to choose a degree of abstraction that allows a representation of the system that is within practical constraints yet is consistent with the modeling purpose the complexity of the actual system and the state of knowledge this will involve a number of tradeoffs while also trying to ensure that the model is reliable useful and feasible for example for a large scale basin study where data at appropriate temporal and spatial scales were lacking swirepik et al 2016 adopted an umbrella environmental asset approach that established environmental water requirements based on several information rich areas assets which were assumed to be representative of the broader rivers a model that is considered either too simple or too complex can potentially represent a failure to appropriately understand and represent the problem domain or decision context addison et al 2013 much research has suggested that species responses to their environment are often individualistic limiting our ability to find general hydrology ecology relationships for a particular community elith and leathwick 2009 rosenfeld 2017 this relates to both the sensitivity of the response to a particular species as well as to the environmental or hydrological context lindenmayer and luck 2005 models that capture the general trends across a community or communities can be valuable for informing the management of streams that involve less conflict over water use for example or have limited resources available for further monitoring or research smakhtin et al 2006 on the other hand such broader models may be inadequate in identifying or protecting the idiosyncratic water requirements of specialist or threatened species rosenfeld 2017 this tradeoff between generality and complexity or realism levins 1966 reverberates through to many of the choices made when developing an ecohydrological model including the choice of spatiotemporal scale and ecological and hydrological features ultimately the selection of spatiotemporal scales and level of complexity is guided by the desired model output which in turn may need to be modified depending on feasibility constraints or the need to improve the model s reliability these choices can also be constrained by the modeling approach used which will be discussed in the next section across large areas such as river basins a generalizable approach limits bias through promoting consistency although there are some methodological constraints this was the case in evaluating an adjustment to the sustainable diversion limit underpinning the murray darling basin plan in australia which had such a requirement for generality overton et al 2014 the approach used a limited set of causal factors and drew upon a broad base of literature as underpinning evidence whilst attempting to limit knowledge bias this is in contrast to place e g catchment reach or site or species specific assessments which are often data based and require more details about the ecohydrological relationships and other system drivers e g kingsford and auld 2005 the depth of ecological and hydrological detail selected for the model should reflect the user needs the actual complexity of the system as well as the data available to support its representation 3 4 selecting the appropriate modeling approach the selection of modeling approach is guided and constrained by the usefulness reliability and feasibility bounds defined in section 3 2 the selection typically involves consideration of whether the various approaches are able to represent the features including the scales indicators and model inputs and outputs deemed essential section 3 3 given the available data and knowledge table 4 summarizes the details of a range of ecohydrological modeling approaches including those commonly used in planning contexts for riverine systems these approaches are not mutually exclusive with some models adopting aspects of more than one approach the various modeling approaches described in table 4 represent different conceptualizations of the hydrology ecology relationship at one end of the spectrum hydrological methods offer the simplest conceptualization with the hydrological variables themselves being proxies for ecological response while being the least data intensive without a need for detailed understanding of ecological processes they provide the least level of reliability in representing ecological response of flow variability or change at the other end of the spectrum there are population models offering a high depth of detail about a particular species and the responses of the various life stages to flow variability stratford et al 2016 or trophic models that represent the broad view of an ecological community and their network of interactions however these two more detailed model types tend to be more theoretical with their applications limited to site scale due to their data requirements habitat models which span the middle ground tend to be more practical for management contexts in terms of data requirements and applicability to various scales habitat models represent a very broad range of models from habitat suitability indices to species distribution models and from static models to state and transition models the major tradeoffs here in selecting a modeling approach are between the depth of detail in the hydrology ecology relationship represented and the feasibility of the model development especially in terms of data requirements and between how accurately or realistically the model captures the system processes and the accessibility or ease of development and use of the model while the type of modeling approach infers a certain level of accessibility of the model by the end user especially in terms of the difficulty in applying the model and understanding its outputs this accessibility can also be influenced by how the developer delivers and presents the model to the end user as mentioned in section 2 1 there are opportunities to improve the usability of the model including through the development of a user friendly interface to allow those with limited skill levels to easily run and understand the model e g smakhtin and eriyagama 2008 or through the provision of training or user guides tutorials communities of practice and other technical documentation although these provisions may cost further project resources they can also vastly improve the usefulness and therefore the adoption of the model also in cases where insufficient data or knowledge about the system severely limits the modeling approach options there may be a need to consider additional resources going towards collecting more data or improving the science in ecohydrological modeling or ecohydrology these opportunities of course may not be feasible given project budget and timeline but recommendations on where efforts for future monitoring are to be targeted can be considered a useful outcome of the modeling process 3 5 testing and evaluating the model regardless of the modeling approach selected it is vital that the model is tested and evaluated before being applied in any management context undiscerning application of ecohydrological models and their outputs including overlooking the limitations of the model or relying too much on its outputs can potentially lead to inefficient and ineffective delivery of environmental flows gippel 2001 or distrust in the model itself or modeling more broadly aber 1997 as mentioned in section 3 1 uncertainties in ecohydrological models are unavoidable this includes uncertainties that stem from imperfect knowledge inherent variability and incomplete or imprecise data walker et al 2003 refsgaard et al 2007 these uncertainties will subsequently propagate into the decisions that these models ultimately aim to support so it is critical that they are explicitly considered that is not to say uncertainties are limited to models rather all types of knowledge assumptions and data are subject to uncertainty regardless of the type of information support available water managers are forced to make decisions despite being faced with uncertainties for example about the future climate or flow conditions horne et al 2016 or on how an ecosystem will respond to a specific change it is important that uncertainties and limitations in modeling are understood and communicated so that managers can make more informed decisions hunt 2017 the key path to understanding these uncertainties and limitations is through model testing and evaluation as argued in hamilton et al 2019 model evaluation benefits from being an ongoing iterative process that commences from the early stages of the project to provide clarity on what the modeling is trying to achieve one of the common challenges of model testing and evaluation for ecohydrological models is the paucity of empirical ecological data to validate the model this is compounded by the complexity of ecological response to flow changes for example a change to a particular flow component can induce different responses in various life history stages of a single species shenton et al 2012 or different responses at different locations or times given these challenges practitioners may not be able to rely solely on more traditional validation techniques that quantify model performance based on model fits to historical data such as pearson s r root mean square error and bias see bennett et al 2013 for a comprehensive set of metrics and methods for evaluating model fit performance but may need to consider other evaluation methods including those summarized in table 5 the feasibility and suitability of these methods depends heavily on the type of model selected and its characteristics generally the more complex the model the higher the dimensionality or resolution the more data and computational power that are needed to test the model population and trophic models for example are notoriously difficult to validate and therefore often remain theoretical robson et al 2017 these evaluation methods can be very instructive which principally includes characterizing the model performance identifying which variables are most influential or better understanding of model limitations and the workings of the model the methods listed in table 5 examine different types and sources of uncertainty and therefore it is often beneficial to evaluate a model using a suite of techniques many of the methods particularly global sensitivity analysis norton 2015 razavi et al 2021 crash testing andréassian et al 2009 coron et al 2012 bayesian inference methods renard et al 2010 and monte carlo approaches may be difficult to implement interpret or communicate methods such as these are based on intensive sampling of parameter space and running the model forward each time a sample is taken so can be computationally intensive especially when analyzing more complex models with many parameters and or models with slow runtimes to some extent computational expense is now offset with increasing processing power though this can invite modelers to make their models even more complex methods that rely on more qualitative assessments such as expert elicitation or assumption hunting are potentially more achievable but still require time and commitment from participants those methods involving judgments from experts or other stakeholders can also utilize knowledge that is not easily formalized and may be subject to uncertainties themselves with results potentially biased and sensitive to the individuals involved refsgaard et al 2007 documentation is a central means of communicating uncertainties and model limitations to end users schemes such as trace transparent and comprehensive ecological modeling provide a structure for documenting a model s development analysis testing and application schmolke et al 2010 grimm et al 2014 such documentation provides transparency to allow the reliability of the models to be assessed by the end users or other external parties 4 discussion this paper emphasizes the importance of developing models that are fit for purpose meaning they not only address the needs of the end user but are also feasible and reliable given the project and problem context the simple fit for purpose framework provides a structure to think through the various modeling choices and consequent design of the model without explicitly considering the fitness for purpose requirements suboptimal design choices may be made in practice the selection of modeling approach is too often dependent on what the modeler has experience with rather than what is more suitable for the given problem and modeling context addor and melsen 2019 to be able to make appropriate modeling choices including the selection of approach the bounds of what is useful reliable and feasible should be understood from the outset these bounds subsequently guide choices about what key features to represent in the model and how they are represented there is however a number of tradeoffs that need to be made in regard to the modeling choices common tradeoffs illustrated with the ecohydrological models include those between the spatiotemporal extent and the level of complexity represented in the model in terms of number and detail of system processes captured the model s generality and the depth of detail represented as well as how realistically the model captures the system and the ease of development and use of the model such tradeoffs have been explored by levins 1966 siegenfeld and bar yam 2020 and others the uncertainties and limitations of the models are then assessed through model testing and evaluation which again should be guided by the bounds of what is useful reliable and feasible we urge that this fitness for purpose framework is applied in conjunction with existing good modeling practice guidelines e g jakeman et al 2006 badham et al 2019 to ensure the effective development and application of models the framework could also be used in a retrospective analysis of existing models to help identify weaknesses in the applied modeling process robson et al 2008 in some respects the framework incorporates many aspects of good modeling practice for example clearly defining the model purpose selecting scales and model features that align with the management or research question and evaluation of the model and its uncertainties one component of good modeling practice that isn t explicitly covered by the framework is transparent communication of the modeling process including its assumptions and reliability of its outputs in connection with the fit for purpose framework we suggest that this transparency be accomplished through the documentation of the modeling choices including the rationale behind them it is increasingly recognized that the final model and its outcomes can be dependent on specific choices made in a modeling process which can lock the project on a specific problem solving path e g hämäläinen and lahtinen 2016 zare et al 2020 therefore it is important that the decisions made in the modeling process are transparent and justified this is also consistent with our emphasis on the process of developing the model not just the final product it is proposed that all four sets of modeling choices outlined in section 2 2 and demonstrated in sections 3 2 3 5 including the model testing methods are considered during the planning and design stage of the modeling process rather than later in the project this will allow many possible future challenges to be anticipated and avoided or at least mitigated for example the options available for model testing may be limited if the model is too complex and contains too many parameters however technological advances in computing power and the development of new methodologies e g white et al 2020 hunt et al 2021 may provide an ability to overcome some of the limitations that has hindered this complexity in the past the principle of occam s razor or model parsimony i e a model should be as simple as possible but no simpler is often advocated in model building to enable more statistically rigorous model development and testing jakeman and hornberger 1993 and better transferability of the model and communication particularly with multiple stakeholders webb et al 2017 however this must also be balanced with consideration of the complexity of the actual system and the management or research questions the model is intended to address sun et al 2016 as well as the increased uncertainty in the model structure which may not be accounted for in the model testing reichert and omlin 1997 therefore the choice of model complexity is case dependent thereby reiterating the framework s position that model choices are contingent on the model purpose and contexts we also advocate that the framework not only be a guide for practitioners to make modeling choices but also as a tool for them to communicate and engage with end users and other stakeholders in the modeling process poor understanding of end user needs by model developers as well as poor understanding of the role of models by end users are basic reasons behind the failing of environmental models to be adopted as intended addison et al 2013 borowski and hare 2007 we propose that the framework can facilitate both sides to communicate their needs and constraints and help justify decisions for example the framework may help the model developer demonstrate that what the end user wants is not feasible given the resources available alternatively the framework can be used as a prompt for the end user to communicate what information they would like from the model the framework can also aid a discussion with a funder for example to argue for a larger budget or longer project timeframe if what the funder wants is not feasible we propose that the simplicity of the fitness for purpose framework can facilitate better communication between all parties about the modeling process as implied in fig 3 we posit that closer dialogue between modelers end users the client and scientists can potentially shift the bounds of what is useful reliable and feasible closer together to provide greater prospects for achieving fit for purpose modeling 5 conclusions designing a fit for purpose model as argued in this paper is about making choices that ensure the model 1 meets end user and management needs 2 achieves an appropriate level of certainty and trust in the modeling and 3 meets the practical constraints of the project by providing clarity on the requirements of fitness for purpose the framework presented in this paper intends to guide practitioners in making appropriate choices in the design of a model to help ensure the model and its outputs are used as intended and if possible contribute to achieving positive outcomes for the end users examples from ecohydrological modeling were provided to illustrate how different purposes and contexts can guide and constrain modeling choices often forcing the practitioner to make tradeoffs between desirable options or some level of sacrifice in one criterion to satisfy another what is deemed an acceptable balance can be a matter of judgement and given the subjective nature of choices it is suggested that these modeling decisions are documented along with their rationale this documentation will support transparency of the modeling process and is aligned with endeavors to improve reproducibility in environmental modeling janssen et al 2020 we also suggest that closer dialogue between modelers scientists and end users can facilitate the achievement of fit for purpose modeling for example by enabling better refinement of the modeling purpose generating more realistic expectations of outcomes from end users and aligning the modeling approach with available data and end user needs george box s famous quote all models are wrong but some are useful box 1979 p 202 is a reminder that models are never perfect representations of the target phenomena or systems but useful insights can potentially be provided and shared from these imperfect representations models will inevitably fall short of the true complexities of the actual systems and processes they represent therefore effective design of models is about capturing those aspects that matter with respect to the purpose of the model and also capturing it in a way that makes the process and product reliable and this must all be realized within the practical constraints of the project at hand another famous quote this time by john tukey far better an approximate answer to the right question which is often vague than an exact answer to the wrong question which can always be made precise tukey 1962 p 13 14 provides valuable insight into how to best approach the challenge of designing a fit for purpose model firstly tukey s quote points to the fact that a fundamental part of the challenge is properly understanding its question which we assert as the model purpose and all the contextual factors tied to it this suggests that sizeable effort is dedicated to understanding what the right question is which aligns with our setting the bounds phase of the modeling process secondly tukey s quote emphasizes the uncertain nature of our understanding including that of the question itself it is critical that we acknowledge the uncertainties pervading our understanding and the subsequent limitations of the resulting model it is important that uncertainty is considered throughout the modeling process refsgaard et al 2007 from determining the appropriate level of uncertainty when setting the reliability bound and deciding what features are to be represented in the model and how through to testing and evaluating the model even though there can never be a right model we can achieve models that are useful reliable and feasible and therefore fit for purpose declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements the authors would like to thank randy hunt and barry croke for their valuable comments on the manuscript 
25670,although it is widely acknowledged as a fundamental principle that models be fit for purpose there remains lack of clarity on what this notion actually means and therefore how it is achieved we contend that fitness for purpose must go beyond the functional use of the model to include its management problem and project contexts accordingly we propose a practical framework that considers fit for purpose modeling as the intersection of three requirements in that the modeling be useful addressing the needs of the end user reliable obtaining an adequate level of certainty or trust and feasible within practical constraints of the project modeling choices including the selection of spatial and temporal scales system features and processes to include and the type of model can be better informed when the bounds of these fit for purpose requirements are defined we focus on modeling in decision and management support settings and demonstrate the framework with ecohydrological models designed for managing environmental flows by explicitly linking its intended functional use and context to modeling choices this framework can facilitate the design and development of environmental models that more effectively bridge science and management keywords fit for purpose modeling best practices model design e flows ecological models environmental water 1 introduction good modeling practice and fitness for purpose as with any scientific methodology to be of value modeling must be conducted with rigor using good practices good modeling practice is critical in ensuring the development of credible models that are appropriately applied and thereby enhance science or management outcomes whereas there is considerable agreement and indeed encouragement that models be fit for purpose e g jakeman et al 2006 beven and young 2013 anderson et al 2015 guillera arroita et al 2015 there remains ambiguity on what this actually means and therefore how it is achieved we argue that fitness for purpose must embrace the whole modeling process and consider the broader setting of the modeling and problem context to ensure it is realized effectively a model must not only be useful in producing information relevant to user management or policy needs such as the desired quantities at appropriate scales under different forcing conditions as argued in section 2 in addition to meeting user needs a model that is fit for purpose must also be trustworthy and scientifically credible i e reliable as well as practically feasible to achieve its intended outcomes this paper adopts the broad definition of models encompassing both numerical simulation and statistical models and conceptual diagrams and heuristics whereas the model itself can be a key output from a modeling exercise or project substantial value comes from the process of developing and evaluating the model from this view modeling can play various roles including 1 as a methodology in processing knowledge whereby data knowledge and assumptions are systematically organized for a specific purpose jakeman et al 2008 2 as a learning process where models are considered working hypotheses of how a system functions and model evaluation is a form of hypothesis testing beven 2000 2006 and 3 as an artefact or boundary object that serves as a point of interaction between scientists managers and stakeholders to facilitate communication and knowledge exchange white et al 2010 therefore modeling is not only a technical procedure but also a learning and social process hamilton et al 2019 iwanaga et al 2021 a number of good modeling practice guidelines are available and are recommended to readers including those for environmental modeling jakeman et al 2006 modeling for integrated water resources management badham et al 2019 black et al 2014 hydrological modeling harmel et al 2014 and ecological modeling schuwirth et al 2019 the key components of good modeling practice as shared in those papers include 1 clear definition of model purpose 2 selection of scales and model features that align with the management or research question 3 uncertainty analysis 4 model evaluation and 5 transparent communication of the model and its assumptions while those papers contain valuable advice in improving modeling practices and explain reasons why these practices are needed we contend that they provide limited advice to practitioners on specific modeling choices which typically are highly contextual while it is of course not possible to provide detailed advice for each specific modeling choice we propose a practical framework that enables the practitioner to think through the various options they are presented with throughout the modeling process and to justify why certain decisions were made this paper aims to provide a framework to guide the design of fit for purpose models for supporting environmental management and decision making models are recognized as being uniquely characterized by their purpose and contextual requirements anderson et al 2015 we advocate that practitioners strive to develop models that are holistically fit for purpose as opposed to focusing solely on more usual standards such as predictive or forecasting ability which are not the only nor necessarily the most appropriate goal we also stress that fit for purpose is not necessarily a black or white concept that is a model is either fit for purpose or not in reality there are different levels of fitness given that we regard fitness for purpose as involving multiple considerations it is unlikely that there is a single most fit for purpose model for any given environmental project but rather a qualitative type of pareto set of most appropriate models practitioners are therefore encouraged to think in terms of fitness for purpose as a spectrum of possibilities and to consider and assess their modeling choices in view of how well the model purpose is addressed and how well the given contexts are accommodated in the next section we describe the notion of fitness for purpose present a framework that characterizes the components of this notion and explain how it can be applied in guiding choices made during the modeling process section 2 we focus on modeling in decision and management support settings although the proposed concepts in the framework can be used beyond this we also describe and demonstrate the use of the conceptual framework through the lens of ecohydrological modeling so as to provide more concrete examples of its application section 3 2 fitness for purpose framework there has not been a clear or at least well accepted definition of what constitutes fit for purpose modeling beven and young 2013 on the one hand this may be due to the answer seeming obvious i e fit for purpose equates to being well suited for the intended purpose of the model on the other hand the term is often ambiguous and left open to interpretation for example while some may view purpose simply as how the model is used e g the model is to be used to compare the ecological impact of alternative water allocation options we contend that the purpose of the model goes beyond just its functional use to include its contextual factors including the management context e g how will the information be used to inform decisions what is the goal of the management the problem context e g who will be affected by the decision how well are the system processes understood and the project context e g what resources are available to develop the model therefore we assert the notion of fitness for purpose needs improved framing and clarification so that it can be consistently and successfully applied in modeling exercises in making choices about the scope and features of the model and selecting the modeling approach we propose that the following three requirements are considered 1 usefulness how well the model addresses the needs of the end user in particular the intended purpose of the model 2 reliability the level of certainty and trust in the model and or the modeling process and 3 feasibility whether the model can be developed given the practical constraints of the exercise or project including scope data time funding and other resources if the modeling is useful reliable and feasible for its given context it can be regarded as fit for purpose fig 1 usefulness reliability and feasibility can be considered in three contexts respectively the user and use setting e g management the problem and the project each of these contexts imposes modeling needs and constraints that ought to be addressed if not totally determined from the outset in order to design a model that is fit for purpose in table 1 we list a range of key considerations that are pertinent to describing these three contexts and a number of criteria that may be relevant in evaluating what is useful reliable and feasible see hamilton et al 2019 for more details on the various evaluation criteria many of these considerations interact both within and across the three requirements there is some overlap between our proposed fitness for purpose framework and the work of cash et al 2003 which asserted that science and technology needs to be salient credible and legitimate to be effective in linking knowledge to action we extend the three conditions highlighted by cash et al 2003 to incorporate other more practical criteria with specific relation to modeling and provide a practice framework for considering each to be useful the model should aim to meet end user needs and be suitable in the management or decision making context this necessitates understanding what the intended purpose and function of the model is who will use the model including their skills and competencies how it is to be used and what model features are needed to enable the user to run interpret and update the model the evaluation criteria corresponding to usefulness relate to salience how relevant the modeling is in terms of addressing the end user questions incorporating end user input and providing timely scientific advice and model accessibility the usability of the model its software platform and its outputs both in the short and long term hamilton et al 2019 for the modeling in a project or exercise to be reliable there needs to be an appropriate level of certainty and trust in the model its mechanisms its results the modeler and or in the process of developing and applying the model determining the acceptable level of certainty in the model requires understanding of the problem context in particular how well the system and its processes are understood the nature of the problem is it a well defined routine problem or a wicked ill defined one with no single correct solution and the social and environmental complexity of the problem which can include for example the level of conflict between stakeholders the level of reliability may also relate to the use of the model for instance models used as evidence in decisions facing legal challenges will face a high level of scrutiny and must demonstrate their adherence to rigorous scientific method özkundakci et al 2018 the criteria for judging reliability can relate to the credibility and legitimacy of the modeling hamilton et al 2019 and how uncertainty is treated and if it can be reduced credibility pertains to the technical and scientific validity of the modeling whereas legitimacy describes the fairness of the process and the representation of values views and concerns of different stakeholders cash et al 2003 feasibility concerns the resources for example data knowledge computational budget and expertise that can be accessed for the given modeling project the criteria for assessing feasibility may also need to relate to whether the modeling can be completed by the project deadline and within the monetary budget hamilton et al 2019 these various constraints do not necessarily preclude modeling but rather set the bounds on what type of modeling can be undertaken in a practical sense including its scope and level of detail therefore these constraints are crucial to consider in the planning and design stage of the modeling process in many respects data and knowledge falls in the intersection between feasibility and reliability we attempt to distinguish feasibility as a component in regard to the specific project e g data and knowledge held by or accessible to the project team whereas reliability concerns the problem and its system in general e g existing data and knowledge on the system we describe each of the three fit for purpose components further with examples from ecohydrological modeling in section 3 2 1 interpreting and applying the framework the fit for purpose modeling framework fig 1 can be viewed to represent the potential space of models that can be developed for a given project and problem ideally a model should be designed and developed to sit within the intersection between the three requirement circles so that it is fit for purpose we argue that models outside of the fit for purpose space may be ineffective and have problems being completed or used as intended for instance a model designed with reliability and usefulness but not feasibility in mind may be idealistic and not possible to complete within the given timeframe or budget a model that is considered scientifically reliable and feasible but not useful to the end user may be shelved achieving no impact and representing a waste of money and resources on the other hand a model that is considered useful and feasible but not reliable e g prioritizing client needs above all else may achieve some impact perhaps limited to some groups but its results may be heavily scrutinized and deemed untrustworthy particularly in a management context it is therefore fundamental that the relevant criteria and bounds of each of the fit for purpose requirement sets are clarified from the outset in the model planning and design and are at the forefront of decisions taken during the model building and evaluation phases although these requirements interact the goal is to maximize the overlap rather than trade off between attributes as a conceptual framework the venn diagram of fig 1 shows the space of modeling options not the level of fitness and the size of the circles and their intersections can be used to represent the relative space of modeling choices clearly the size and overlap of the circles representing the three fit for purpose requirements would vary for each project some of the challenges faced by projects that subsequently limit options available to model developers can be conceptualized by the framework fig 2 examples of common challenges in modeling projects are limited resources including insufficient time funding computational budget data or expertise i e the feasibility circle is small where there is high uncertainty in understanding or representing the system processes i e the reliability circle is small and the end user has limited technical skills a narrow perspective of how models are used or there is a high ambiguity in the delivery space i e the usefulness circle is small we also propose that there may be scope to expand the size of individual fit for purpose requirement circles to increase the modeling options available to developers thereby better enabling the design of a fit for purpose model for example the abovementioned challenges may be overcome by increasing the resources available to the project e g more funds or the collection of more data investing in improving the specific type of modeling or understanding the system processes i e increasing the range of appropriate tools available and increasing the end user s capacity to use the tool e g through capacity training better documentation or developing a user friendly software interface fig 2 while the options to expand the bounds of the sets may not always be possible for the project at hand particularly for feasibility and reliability they could represent areas for future research and investment there may however be more practical options available for increasing the usefulness and accessibility of the model by providing end users with training or technical support material for running and interpreting the model such as user guides tutorials and shared workflows e g moeck et al 2015 pianosi et al 2020 or by allocating resources to a well designed and intuitive graphical user interface that enables better accessibility and ease of use e g wi et al 2017 there are mounting calls for closer and more effective engagement between modelers scientists and end users e g by voinov and bousquet 2010 falconi and palmer 2017 and gray et al 2018 which can only facilitate the achievement of fit for purpose modeling this can be represented as a shifting of the fit for purpose requirement circles so there is a higher level of agreement about the model purpose and contexts and subsequently a larger intersection of the circles fig 3 this greater overlap and opportunity can be achieved for example through refining and better articulating the research question to key goals more realistic expectations of outcomes from end users aligning the modeling approach with available data and through reuse and adaptation of existing modeling approaches to the current project context 2 2 linking the fitness for purpose framework to the main modeling choices with a model defined as an abstraction of reality we can then consider modeling to provide a specific perspective of a real system depending on assumptions made and decisions taken any one system can be viewed from many perspectives and thus be represented by many different models the critical choices made in developing a model coincide with selecting a perspective and finding the most appropriate modeling approach to represent that chosen perspective we argue that those choices around the perspectives of the system to be modeled and how it should be modeled should target the fit for purpose intersection of the framework fig 1 in this paper we consider four main sets of choices made during the modeling 1 setting the bounds of the modeling 2 selecting the key features of the perspective 3 deciding on an appropriate modeling approach and 4 testing and evaluating the model fig 2 as model development progresses various considerations are assembled with regard to the usefulness reliability and feasibility of the model which subsequently guide modeling choices these considerations are summarized in table 2 the first choices made relate to setting the bounds of the modeling and are equivalent to the scoping and planning phase described in hamilton et al 2015 and badham et al 2019 wherein the modeling objectives problem definition system boundaries and model functions are clarified the mantra that a model s objectives should guide all the modeling phases goes back a long way e g forrester 1961 in terms of the fitness for purpose framework the first modeling phase is centered on not only identifying the modeling objectives but also understanding the user and management context the problem context and the project context in order to clarify what is required for ensuring that the modeling is useful reliable and feasible the second set of choices revolves around selecting a perspective of the system to represent that is within the usefulness reliability and feasibility bounds determined in the previous stage this includes consideration of what information is being sought by the end user what variables and scales are critical to the relevant system processes and what level of complexity can be incorporated given the project resources in selecting the modeling approach the practitioner is to consider the range of available approaches that can capture the key model features determined earlier the selection of that approach should take into account the accessibility of the model and its outputs to the end user the level of understanding of system processes required to build the model the complexity of the model build and the data requirements in some cases there may be a need to adopt legacy models due to client preference or legislation which can constrain choices after building the model with the selected approach the model is to be tested and evaluated using methods appropriate for the given contexts the choice of model testing and evaluation methods should take into account the acceptable level of uncertainty given the problem at hand how the uncertainties and limitations are best communicated to users how any model uncertainty can be identified and if required reduced as well as the methods that are feasible given available resources these methods may comprise a mix of quantitative and qualitative types e g refsgaard et al 2007 the application of this fitness for purpose framework in guiding modeling choices is illustrated below by examining a range of options available in developing ecohydrological models for supporting the management of environmental flows 3 case study models for managing environmental flows 3 1 background to environmental flow models the diverse mosaic of species and communities across rivers and their floodplains reflects a legacy of hydrologic conditions under which river flows shape the physical and biological environments and drive many ecological processes ward et al 1999 merritt et al 2010 water resource development including infrastructure and extraction has been a major driver in the degradation of freshwater and estuarine ecosystems across the globe nilsson et al 2005 poff and zimmerman 2010 environmental flow management is about ensuring that river environments receive the water necessary to sustain their ecosystems in terms of the quantity timing and quality of flows arthington et al 2018 ecohydrological modeling which explicitly considers the interplay between hydrology and ecosystems has become a critical tool for determining environmental flow needs and assessing ecological outcomes of altered flow regimes in river systems tharme 2003 poff et al 2010 practitioners developing ecohydrological models are faced with a number of fundamental challenges one is the difficulty in isolating changes in biota due to hydrological variations from changes due to other factors such as physical habitat destruction or the introduction of alien species craig et al 2017 moreover ecosystems are affected by a myriad of diverse factors many of which interact with one another potentially obscuring or exacerbating the impact of changes to the water resource a second challenge relates to the variable response of biota to hydrological changes not only between species but also within e g life stage and origin a third challenge relates to scale issues including how ecological responses can vary at different spatial or temporal scales this is partly due to the numerous causal mechanisms functioning over various temporal and spatial scales these aforementioned challenges cannot be completely overcome and as a result of these features and others inherent in natural systems substantial uncertainties will remain unavoidable in ecohydrological modeling the use and uptake of ecohydrological models and indeed ecological models in general over the last couple of decades have been hindered by criticisms including poor match between model predictions and field observations high variability and or uncertainty of outputs and models being too complex and difficult to understand or to parameterize and implement lester 2020 webb et al 2017 these criticisms can stem from a misunderstanding of the role of modeling on the one hand but also from models that were not developed appropriately to suit their intended purpose on the other hand in addition to this there are different norms and philosophies held in the hydrological and ecological domains regarding how to handle uncertainty hunt and wilcox 2003a b for example many hydrologists are comfortable with hard predictions and estimating quantifiable changes to system states while most ecologists are less comfortable with such hard quantities and prefer using system tendencies captured by statistics we contend that the gap between the development and use of ecohydrological models can be closed through a better understanding of the role of modeling by modelers users and other practitioners and through better modeling choices that are aligned with the given contexts below we discuss options available when developing ecohydrological models and demonstrate how the fitness for purpose framework can be applied to guide these modeling choices 3 2 setting the bounds the initial stage of model development should involve understanding the purpose and contexts of the modeling especially the user and management context in order to set out the key requirements the problem and project contexts also set the bounds on what type of modeling is possible and appropriate table 1 outlines the key questions that need to be considered to set the fit for purpose bounds that is what is required to make the modeling useful reliable and feasible limited data availability is a common issue in ecohydrology including the fundamental fact that many species lack data on their basic biology tonkin et al 2019 clarifying these three bounds early is critical as this information determines choices made in the other stages of the modeling process ecohydrological models can be applied to explore ecological consequences of changes in flow to determine the flow regimes needed to meet or maintain ecological targets tharme 2003 webb et al 2017 or to set or refine management objectives through iterative discovery fu et al 2015 the models can summarize the state of knowledge of dependencies between flow and ecology including the sensitivity of ecology to flow change and based on these relationships provide guidance for decision making on structural or policy interventions or investments in future monitoring schuwirth et al 2019 webb et al 2015 typically the purpose of an ecohydrological model is a combination of these identifying the decision or management context entails understanding what information is being sought for example new developments may require decision makers to consider what impact the anticipated modifications to the water resource may have on particular species or ecological communities ahn et al 2014 alternatively decision makers may be faced with the challenge of allocating water between different uses e g irrigation public water supply industry mining and the environment and need to know how much water can be taken from the system without overly affecting ecosystems chen and olden 2017 in other cases the decision makers may already have a volume of water set aside for the environment and be looking at how the water can be best distributed or released to maximize ecological outcomes hough et al 2019 richter and thomas 2007 understanding the management goals including the ecological assets or values being targeted and over what spatial scale will help inform the appropriate perspective that the modeling needs to take and the type of output required thus there is a contrast for instance between a model output in the form of a generalizable solution that aims to be applicable across larger spatial extents with one seeking a specific solution that is focused on a certain place or target species there would also be different expectations of the reliability of the modeling if used to support management of a threatened species compared to a more common species for the former the potential implications of the model being wrong e g contributing to the demise of a species means higher levels of model uncertainty may be considered unacceptable understanding the end user group and their requirements is also critical this includes understanding their technical skill level in ecology hydrology and modeling why the information is being sought and how the information will be used as well as how they intend to run maintain and update the model this information can be used to help ensure that the model is created to be accessible to the user in the short to long term 3 3 selecting key model features the set of choices regarding key model features entails selecting the different aspects of the system to represent in the model and at what spatiotemporal scale s and level of detail or complexity table 3 in most if not all cases it is expected that at least one of these aspects i e related to scale or focal point are pre determined by the purpose of the model and the primary management goal e g protection of a particular ecological community kingsford and auld 2005 or water allocation within a given catchment or area wei et al 2012 the challenge comes in deciding on what other aspects to capture particularly where there may be interactions between attributes such as spatial and temporal scales to employ while there is no universally correct perspective on any problem it is crucial to recognize that the choice of perspective affects how the system is portrayed and understood including what elements or processes are considered or excluded and the nature of the relationship between elements thus impacting the outputs and outcomes of the modeling the focal point of the model should be driven by management priorities or stakeholder interest the selection of a particular taxa ecological community or environmental asset to focus on may be based on its ecological and or cultural value in terms of its significance or rarity e g iconic internationally recognized or threatened species its high diversity its sensitivity or its representativeness of a particular habitat umbrella species or group of species e g functional group casanova 2011 roberge and angelstam 2004 stratford et al 2016 there may be management interest in focusing on particular hydrological conditions for example critical periods of extreme low flow when competition for water between users is highest or periods of high flow which may represent flood risks juarez lucas and kibler 2016 stewart et al 2020 the spatial and temporal scale of the chosen perspective may be based on the management scale e g management jurisdiction planning horizon including existing monitoring or the scale of key ecological and hydrological processes the temporal scales of most ecological and hydrological processes are linked to their spatial scales processes that occur at micro or site scales tend to occur over short timeframes and those that operate over larger e g catchment or basin spatial scales tend to have larger temporal scales of change frissell et al 1986 the selection of scale can thus impact how well relevant processes can be captured in investigations or represented in the model iwanaga et al 2021 newman et al 2006 schuwirth et al 2019 the scale of detail or level of complexity that can be captured in an ecohydrological model is constrained by project resources data and knowledge it is necessary to choose a degree of abstraction that allows a representation of the system that is within practical constraints yet is consistent with the modeling purpose the complexity of the actual system and the state of knowledge this will involve a number of tradeoffs while also trying to ensure that the model is reliable useful and feasible for example for a large scale basin study where data at appropriate temporal and spatial scales were lacking swirepik et al 2016 adopted an umbrella environmental asset approach that established environmental water requirements based on several information rich areas assets which were assumed to be representative of the broader rivers a model that is considered either too simple or too complex can potentially represent a failure to appropriately understand and represent the problem domain or decision context addison et al 2013 much research has suggested that species responses to their environment are often individualistic limiting our ability to find general hydrology ecology relationships for a particular community elith and leathwick 2009 rosenfeld 2017 this relates to both the sensitivity of the response to a particular species as well as to the environmental or hydrological context lindenmayer and luck 2005 models that capture the general trends across a community or communities can be valuable for informing the management of streams that involve less conflict over water use for example or have limited resources available for further monitoring or research smakhtin et al 2006 on the other hand such broader models may be inadequate in identifying or protecting the idiosyncratic water requirements of specialist or threatened species rosenfeld 2017 this tradeoff between generality and complexity or realism levins 1966 reverberates through to many of the choices made when developing an ecohydrological model including the choice of spatiotemporal scale and ecological and hydrological features ultimately the selection of spatiotemporal scales and level of complexity is guided by the desired model output which in turn may need to be modified depending on feasibility constraints or the need to improve the model s reliability these choices can also be constrained by the modeling approach used which will be discussed in the next section across large areas such as river basins a generalizable approach limits bias through promoting consistency although there are some methodological constraints this was the case in evaluating an adjustment to the sustainable diversion limit underpinning the murray darling basin plan in australia which had such a requirement for generality overton et al 2014 the approach used a limited set of causal factors and drew upon a broad base of literature as underpinning evidence whilst attempting to limit knowledge bias this is in contrast to place e g catchment reach or site or species specific assessments which are often data based and require more details about the ecohydrological relationships and other system drivers e g kingsford and auld 2005 the depth of ecological and hydrological detail selected for the model should reflect the user needs the actual complexity of the system as well as the data available to support its representation 3 4 selecting the appropriate modeling approach the selection of modeling approach is guided and constrained by the usefulness reliability and feasibility bounds defined in section 3 2 the selection typically involves consideration of whether the various approaches are able to represent the features including the scales indicators and model inputs and outputs deemed essential section 3 3 given the available data and knowledge table 4 summarizes the details of a range of ecohydrological modeling approaches including those commonly used in planning contexts for riverine systems these approaches are not mutually exclusive with some models adopting aspects of more than one approach the various modeling approaches described in table 4 represent different conceptualizations of the hydrology ecology relationship at one end of the spectrum hydrological methods offer the simplest conceptualization with the hydrological variables themselves being proxies for ecological response while being the least data intensive without a need for detailed understanding of ecological processes they provide the least level of reliability in representing ecological response of flow variability or change at the other end of the spectrum there are population models offering a high depth of detail about a particular species and the responses of the various life stages to flow variability stratford et al 2016 or trophic models that represent the broad view of an ecological community and their network of interactions however these two more detailed model types tend to be more theoretical with their applications limited to site scale due to their data requirements habitat models which span the middle ground tend to be more practical for management contexts in terms of data requirements and applicability to various scales habitat models represent a very broad range of models from habitat suitability indices to species distribution models and from static models to state and transition models the major tradeoffs here in selecting a modeling approach are between the depth of detail in the hydrology ecology relationship represented and the feasibility of the model development especially in terms of data requirements and between how accurately or realistically the model captures the system processes and the accessibility or ease of development and use of the model while the type of modeling approach infers a certain level of accessibility of the model by the end user especially in terms of the difficulty in applying the model and understanding its outputs this accessibility can also be influenced by how the developer delivers and presents the model to the end user as mentioned in section 2 1 there are opportunities to improve the usability of the model including through the development of a user friendly interface to allow those with limited skill levels to easily run and understand the model e g smakhtin and eriyagama 2008 or through the provision of training or user guides tutorials communities of practice and other technical documentation although these provisions may cost further project resources they can also vastly improve the usefulness and therefore the adoption of the model also in cases where insufficient data or knowledge about the system severely limits the modeling approach options there may be a need to consider additional resources going towards collecting more data or improving the science in ecohydrological modeling or ecohydrology these opportunities of course may not be feasible given project budget and timeline but recommendations on where efforts for future monitoring are to be targeted can be considered a useful outcome of the modeling process 3 5 testing and evaluating the model regardless of the modeling approach selected it is vital that the model is tested and evaluated before being applied in any management context undiscerning application of ecohydrological models and their outputs including overlooking the limitations of the model or relying too much on its outputs can potentially lead to inefficient and ineffective delivery of environmental flows gippel 2001 or distrust in the model itself or modeling more broadly aber 1997 as mentioned in section 3 1 uncertainties in ecohydrological models are unavoidable this includes uncertainties that stem from imperfect knowledge inherent variability and incomplete or imprecise data walker et al 2003 refsgaard et al 2007 these uncertainties will subsequently propagate into the decisions that these models ultimately aim to support so it is critical that they are explicitly considered that is not to say uncertainties are limited to models rather all types of knowledge assumptions and data are subject to uncertainty regardless of the type of information support available water managers are forced to make decisions despite being faced with uncertainties for example about the future climate or flow conditions horne et al 2016 or on how an ecosystem will respond to a specific change it is important that uncertainties and limitations in modeling are understood and communicated so that managers can make more informed decisions hunt 2017 the key path to understanding these uncertainties and limitations is through model testing and evaluation as argued in hamilton et al 2019 model evaluation benefits from being an ongoing iterative process that commences from the early stages of the project to provide clarity on what the modeling is trying to achieve one of the common challenges of model testing and evaluation for ecohydrological models is the paucity of empirical ecological data to validate the model this is compounded by the complexity of ecological response to flow changes for example a change to a particular flow component can induce different responses in various life history stages of a single species shenton et al 2012 or different responses at different locations or times given these challenges practitioners may not be able to rely solely on more traditional validation techniques that quantify model performance based on model fits to historical data such as pearson s r root mean square error and bias see bennett et al 2013 for a comprehensive set of metrics and methods for evaluating model fit performance but may need to consider other evaluation methods including those summarized in table 5 the feasibility and suitability of these methods depends heavily on the type of model selected and its characteristics generally the more complex the model the higher the dimensionality or resolution the more data and computational power that are needed to test the model population and trophic models for example are notoriously difficult to validate and therefore often remain theoretical robson et al 2017 these evaluation methods can be very instructive which principally includes characterizing the model performance identifying which variables are most influential or better understanding of model limitations and the workings of the model the methods listed in table 5 examine different types and sources of uncertainty and therefore it is often beneficial to evaluate a model using a suite of techniques many of the methods particularly global sensitivity analysis norton 2015 razavi et al 2021 crash testing andréassian et al 2009 coron et al 2012 bayesian inference methods renard et al 2010 and monte carlo approaches may be difficult to implement interpret or communicate methods such as these are based on intensive sampling of parameter space and running the model forward each time a sample is taken so can be computationally intensive especially when analyzing more complex models with many parameters and or models with slow runtimes to some extent computational expense is now offset with increasing processing power though this can invite modelers to make their models even more complex methods that rely on more qualitative assessments such as expert elicitation or assumption hunting are potentially more achievable but still require time and commitment from participants those methods involving judgments from experts or other stakeholders can also utilize knowledge that is not easily formalized and may be subject to uncertainties themselves with results potentially biased and sensitive to the individuals involved refsgaard et al 2007 documentation is a central means of communicating uncertainties and model limitations to end users schemes such as trace transparent and comprehensive ecological modeling provide a structure for documenting a model s development analysis testing and application schmolke et al 2010 grimm et al 2014 such documentation provides transparency to allow the reliability of the models to be assessed by the end users or other external parties 4 discussion this paper emphasizes the importance of developing models that are fit for purpose meaning they not only address the needs of the end user but are also feasible and reliable given the project and problem context the simple fit for purpose framework provides a structure to think through the various modeling choices and consequent design of the model without explicitly considering the fitness for purpose requirements suboptimal design choices may be made in practice the selection of modeling approach is too often dependent on what the modeler has experience with rather than what is more suitable for the given problem and modeling context addor and melsen 2019 to be able to make appropriate modeling choices including the selection of approach the bounds of what is useful reliable and feasible should be understood from the outset these bounds subsequently guide choices about what key features to represent in the model and how they are represented there is however a number of tradeoffs that need to be made in regard to the modeling choices common tradeoffs illustrated with the ecohydrological models include those between the spatiotemporal extent and the level of complexity represented in the model in terms of number and detail of system processes captured the model s generality and the depth of detail represented as well as how realistically the model captures the system and the ease of development and use of the model such tradeoffs have been explored by levins 1966 siegenfeld and bar yam 2020 and others the uncertainties and limitations of the models are then assessed through model testing and evaluation which again should be guided by the bounds of what is useful reliable and feasible we urge that this fitness for purpose framework is applied in conjunction with existing good modeling practice guidelines e g jakeman et al 2006 badham et al 2019 to ensure the effective development and application of models the framework could also be used in a retrospective analysis of existing models to help identify weaknesses in the applied modeling process robson et al 2008 in some respects the framework incorporates many aspects of good modeling practice for example clearly defining the model purpose selecting scales and model features that align with the management or research question and evaluation of the model and its uncertainties one component of good modeling practice that isn t explicitly covered by the framework is transparent communication of the modeling process including its assumptions and reliability of its outputs in connection with the fit for purpose framework we suggest that this transparency be accomplished through the documentation of the modeling choices including the rationale behind them it is increasingly recognized that the final model and its outcomes can be dependent on specific choices made in a modeling process which can lock the project on a specific problem solving path e g hämäläinen and lahtinen 2016 zare et al 2020 therefore it is important that the decisions made in the modeling process are transparent and justified this is also consistent with our emphasis on the process of developing the model not just the final product it is proposed that all four sets of modeling choices outlined in section 2 2 and demonstrated in sections 3 2 3 5 including the model testing methods are considered during the planning and design stage of the modeling process rather than later in the project this will allow many possible future challenges to be anticipated and avoided or at least mitigated for example the options available for model testing may be limited if the model is too complex and contains too many parameters however technological advances in computing power and the development of new methodologies e g white et al 2020 hunt et al 2021 may provide an ability to overcome some of the limitations that has hindered this complexity in the past the principle of occam s razor or model parsimony i e a model should be as simple as possible but no simpler is often advocated in model building to enable more statistically rigorous model development and testing jakeman and hornberger 1993 and better transferability of the model and communication particularly with multiple stakeholders webb et al 2017 however this must also be balanced with consideration of the complexity of the actual system and the management or research questions the model is intended to address sun et al 2016 as well as the increased uncertainty in the model structure which may not be accounted for in the model testing reichert and omlin 1997 therefore the choice of model complexity is case dependent thereby reiterating the framework s position that model choices are contingent on the model purpose and contexts we also advocate that the framework not only be a guide for practitioners to make modeling choices but also as a tool for them to communicate and engage with end users and other stakeholders in the modeling process poor understanding of end user needs by model developers as well as poor understanding of the role of models by end users are basic reasons behind the failing of environmental models to be adopted as intended addison et al 2013 borowski and hare 2007 we propose that the framework can facilitate both sides to communicate their needs and constraints and help justify decisions for example the framework may help the model developer demonstrate that what the end user wants is not feasible given the resources available alternatively the framework can be used as a prompt for the end user to communicate what information they would like from the model the framework can also aid a discussion with a funder for example to argue for a larger budget or longer project timeframe if what the funder wants is not feasible we propose that the simplicity of the fitness for purpose framework can facilitate better communication between all parties about the modeling process as implied in fig 3 we posit that closer dialogue between modelers end users the client and scientists can potentially shift the bounds of what is useful reliable and feasible closer together to provide greater prospects for achieving fit for purpose modeling 5 conclusions designing a fit for purpose model as argued in this paper is about making choices that ensure the model 1 meets end user and management needs 2 achieves an appropriate level of certainty and trust in the modeling and 3 meets the practical constraints of the project by providing clarity on the requirements of fitness for purpose the framework presented in this paper intends to guide practitioners in making appropriate choices in the design of a model to help ensure the model and its outputs are used as intended and if possible contribute to achieving positive outcomes for the end users examples from ecohydrological modeling were provided to illustrate how different purposes and contexts can guide and constrain modeling choices often forcing the practitioner to make tradeoffs between desirable options or some level of sacrifice in one criterion to satisfy another what is deemed an acceptable balance can be a matter of judgement and given the subjective nature of choices it is suggested that these modeling decisions are documented along with their rationale this documentation will support transparency of the modeling process and is aligned with endeavors to improve reproducibility in environmental modeling janssen et al 2020 we also suggest that closer dialogue between modelers scientists and end users can facilitate the achievement of fit for purpose modeling for example by enabling better refinement of the modeling purpose generating more realistic expectations of outcomes from end users and aligning the modeling approach with available data and end user needs george box s famous quote all models are wrong but some are useful box 1979 p 202 is a reminder that models are never perfect representations of the target phenomena or systems but useful insights can potentially be provided and shared from these imperfect representations models will inevitably fall short of the true complexities of the actual systems and processes they represent therefore effective design of models is about capturing those aspects that matter with respect to the purpose of the model and also capturing it in a way that makes the process and product reliable and this must all be realized within the practical constraints of the project at hand another famous quote this time by john tukey far better an approximate answer to the right question which is often vague than an exact answer to the wrong question which can always be made precise tukey 1962 p 13 14 provides valuable insight into how to best approach the challenge of designing a fit for purpose model firstly tukey s quote points to the fact that a fundamental part of the challenge is properly understanding its question which we assert as the model purpose and all the contextual factors tied to it this suggests that sizeable effort is dedicated to understanding what the right question is which aligns with our setting the bounds phase of the modeling process secondly tukey s quote emphasizes the uncertain nature of our understanding including that of the question itself it is critical that we acknowledge the uncertainties pervading our understanding and the subsequent limitations of the resulting model it is important that uncertainty is considered throughout the modeling process refsgaard et al 2007 from determining the appropriate level of uncertainty when setting the reliability bound and deciding what features are to be represented in the model and how through to testing and evaluating the model even though there can never be a right model we can achieve models that are useful reliable and feasible and therefore fit for purpose declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements the authors would like to thank randy hunt and barry croke for their valuable comments on the manuscript 
25671,optimisation methods are applied increasingly to environmental problems much research in this area is concerned with the behaviour of optimisation algorithms however the effectiveness of these algorithms is also a function of the features of the problem being solved although a number of metrics have been developed to quantify these features they have not been applied to environmental problems the primary reason for this is that the computational cost associated with the calculation of many of these metrics increases significantly with problem size making them unsuitable for real world problems in this paper 28 fitness landscape metrics that have low dependence on problem size are identified through extensive computational experiments on a range of benchmark functions and testing on a number of environmental modelling problems these metrics can be applied to real world optimisation problems in a computationally efficient manner to better understand their features and determine which optimisation algorithms are most suitable ela exploratory landscape analysis bbob black box optimisation benchmark ann artificial neural network icofs information content of fitness sequences keywords optimisation calibration fitness landscape error function exploratory landscape analysis ela evolutionary algorithms 1 introduction optimisation methods are being used extensively to assist with the identification of the most appropriate solutions for a range of environmental problems maier et al 2014 2019 such as stormwater management liu et al 2016 di matteo et al 2019 wastewater treatment hamed et al 2004 land use management emirhüseyinoğlu and ryan 2020 newland et al 2020 environmental management kasprzyk et al 2013 water energy system design guidici et al 2019 water distribution system design zecchin et al 2006 and irrigation scheduling nguyen et al 2017 sedighkia et al 2021 as well as the development of environmental models including input variable selection grivas and chaloulakou 2006 galelli et al 2014 and model calibration pelletier et al 2006 burton et al 2008 existing research in this field has primarily focused on the development of improved optimisation algorithms such as galaxy wang et al 2020 dream vrugt 2016 borg hadka and reed 2015 particle swarm optimisation chau 2007 nsga ii fu et al 2008 ant colony optimisation emami skardi et al 2015 and policy tree optimisation herman and giuliani 2018 as well as the comparison of the performance of different algorithms on different problems e g tikhamarine et al 2020 piotrowski and napiorkowski 2011 kisi et al 2012 bullinaria and alyahya 2014 wang et al 2020 however in accordance with the no free lunch theorem wolpert and macready 1997 no optimisation algorithm can outperform all others across every single problem consequently there is a need to better understand the features of different optimisation problems so that algorithms that are better suited to particular problem types can be selected maier et al 2014 the features of optimisation problems can be represented geometrically by considering the fitness landscape which depicts the shape of the fitness function otherwise termed objective function for a particular objective with respect to the decision variables e g model error as a function of different values of model parameters for model calibration problems see maier et al 2019 as the aim of the optimisation process is to find the highest or lowest points in this landscape depending on whether the aim is to maximise or minimise the objective function the ease or difficulty with which this can be done is a function of the features of this landscape for example if the landscape is smooth with a single well defined high or low point global optimum this point is relatively easy to find conversely if the landscape is rough with many minima or maxima of similar or equal value local optima the overall best solution global optimum is more difficult to find similarly the presence of flat regions or plateaus in the fitness landscape makes it more difficult to guide the search towards the highest or lowest point in the landscape it should be noted that for multi objective optimisation problems each objective has its own fitness landscape as variations in objective values with changes in decision variable values are likely to be different for different objectives see maier et al 2019 in order to enable a better understanding of the features of optimisation problems to be obtained a number of exploratory landscape analysis ela metrics have been developed mersmann et al 2010 munoz et al 2015a for example such metrics can provide an indication of the global structure of the fitness landscape e g its curvature its degree of multi modality e g the prevalence of local optima or the presence of plateaus mersmann et al 2011 however application of these metrics to environmental optimisation problems has been extremely limited e g gibbs et al 2011 bi et al 2016 instead an empirical brute force approach is often used to determine which algorithm or parameterisation to use on a case study by case study basis maier et al 2014 one potential reason for this is that there are different metrics for different landscape features mersmann et al 2010 malan and engelbrecht 2013 maier et al 2014 munoz et al 2015a as well as different metrics for the same features all with particular biases munoz et al 2015a making it difficult to know which metrics to use however the main reason for the lack of adoption of ela metrics is likely to be related to the computational effort required to calculate them as these metrics are calculated based on samples from the fitness landscape pitzer and affenzeller 2012 the number of samples required to obtain meaningful metric values can increase significantly with the size of the search space munoz et al 2015a when addressing real world environmental optimisation problems which are often characterised by large search spaces this can either lead to computational intractability or the case where the computational effort associated with calculating the metrics is greater than that required as part of the brute force approach of applying different algorithms or algorithm parameterisations to determine which works best consequently to enable ela metrics to be used for developing a better understanding of fitness landscape features and selecting the most appropriate optimisation algorithms for real world problems there is a need to determine i which ela metrics if any have low dependence on problem dimensionality and sample size so that they can be applied to real world problems in a computationally efficient manner and ii what information about the features of the fitness landscape can be ascertained from these metrics in order to address these shortcomings the objectives of this paper are 1 to identify which ela metrics have low dependence on problem dimensionality and sample size for a range of benchmark functions with a wide variety of known fitness landscape properties this indicates which ela metrics can be applied to real world environmental optimisation problems from the perspective of computational tractability it also opens the door to assessing the potential practical value of using ela metrics to assist with determining which optimisation algorithm or settings might be most appropriate from a computational efficiency perspective as the computational effort associated with the calculation of ela statistics should be less than that associated with the brute force approach determining which optimisation algorithm performs best 2 to check whether the ela metrics identified as having low dependence on problem dimensionality and sample size for benchmark functions also have low dependence on these factors for a number of real life environmental modelling problems 3 to map the ela metrics that have low dependence on problem dimensionality and sample size to the fitness landscape features they are designed to provide information on thereby providing a desktop assessment of the potential usefulness of the ela metrics that are suitable to determining the features of real world optimisation problems the remainder of this paper is organised as follows details of the methodology used to achieve the above objectives are given in section 2 followed by the results and discussion in section 3 summary and conclusions are provided in section 4 2 methodology 2 1 overview an overview of the methodology used to achieve the three objectives stated in the introduction is given in fig 1 with further details provided in appendix a as can be seen the first three steps of the identification of ela metrics with low dependence on problem dimensionality and sample size with the aid of benchmark functions objective 1 and checking whether these metrics also have low dependence on problem dimensionality and sample size for real life environmental modelling problems objective 2 are the same the first of these steps includes the sampling of fitness landscapes with different features and dimensionality using a range of samples sizes as these samples are required for the calculation of the different ela metrics in step 2 for objective 1 fitness landscapes with different features are represented by the noiseless bbob suite of 24 benchmark functions hansen et al 2009 as these contain a wide range of known landscape features see section 2 2 1 for details can be scaled to different dimensionalities hansen et al 2009 and have been used in a number of fitness landscape studies mersmann et al 2010 2011 shirakawa and nagao 2014 2016 munoz et al 2015b munoz and smith miles 2017 he et al 2007 twenty replicates are generated for five different dimensionalities 2 5 10 20 30 for each of the 24 benchmark functions resulting in 2 400 24 functions x 5 dimensionalities x 20 replicates fitness landscapes each of these is sampled 30 times with different sampling lengths ranging from 100 to 120 000 resulting in 72 000 2 400 fitness landscapes x 30 sample lengths sets of fitness landscape samples a maximum dimensionality of 30 is selected as this corresponds to the upper end of dimensionalities used in previous studies using these benchmark functions mersmann et al 2011 munoz et al 2015b munoz and smith miles 2017 shirakawa and nagao 2014 2016 kerschke et al 2015 garden and engelbrecht 2014 a maximum sample length of 120 000 is used as this has been found to be sufficient for the convergence of ela metric values in preliminary analyses and is significantly greater than sample sizes used in previous ela studies which are generally on the order of 6 000 or 1000 dimension mersmann et al 2011 munoz et al 2015b munoz and smith miles 2017 shirakawa and nagao 2014 2016 kerschke et al 2015 garden and engelbrecht 2014 for objective 2 the fitness landscapes with different features correspond to those for the calibration training of artificial neural network ann models used to predict a number of environmental variables runoff turbidity salinity see section 2 2 2 for details these environmental modelling problems have been selected as i model calibration is a common environmental optimisation problem maier et al 2019 ii anns have been used extensively for environmental modelling see maier et al 2010 wu et al 2014 cabaneros et al 2019 iii the parametric dimensionality of anns can be changed within a single model framework by increasing the number of hidden nodes maier et al 2010 and iv the fitness landscapes associated with the calibration of ann models have been shown to vary in complexity e g kingston et al 2005 samarasinghe 2010 ten replicates are generated for 11 model structures 0 10 hidden nodes corresponding to problem dimensionalities ranging from 1 to 70 see section 2 2 2 for details for each of the 3 modelling problems considered resulting in 330 3 problems x 11 dimensionalities x 10 replicates fitness landscapes each of these is sampled 23 times with different sampling lengths ranging from 100 to 50 000 resulting in 7 590 330 fitness landscapes x 23 sample lengths sets of fitness landscape samples a maximum sample length of 50 000 is selected as this has been found to be sufficient for convergence of ela metric values in preliminary analyses the second of these three steps involves the calculation of the desired ela metrics for each of the sets of fitness landscape samples generated in the previous step for objective 1 89 ela metrics are considered which constitute the full set of metrics used in previous fitness landscape analysis studies mersmann et al 2011 munoz et al 2015b munoz and smith miles 2017 to maximise the chances of identifying metrics with low dependence on problem dimensionality and sample size that also provide useful information about a range of fitness landscape features for the environmental modelling problems objective 2 only the ela metrics that are found to have low dependency on problem dimensionality and sample size for the benchmark functions see fig 1 step 4 are used in order to check whether the findings from the benchmark functions apply in real life environmental modelling contexts the third and last of these steps involves the calculation of the degree of dependence of the ela metrics considered in the previous step on both problem dimensionality and sample size if a metric has low dependence on both it is likely to be able to be applied to real world environmental optimisation problems if this is not the case the computational effort required to calculate the metric is likely to be too large for practical purposes for the benchmark functions objective 1 dependence is represented by the relationship between sample size problem dimensionality and the reject rate which is the fraction of the 24 test functions for which the hypothesis that a particular sample size gives the true value of a particular fitness landscape metric does not hold based on the wilcoxon rank sum test calculated using 20 replicates see section 2 4 1 for details in this context the true value is taken as the value obtained for the largest number of samples considered i e 120 000 if the reject rate for a particular metric is low and remains so with increasing dimensionality over the 5 dimensionalities considered and sample size over the 30 sample lengths considered calculation of the true value of this metric can be considered to have low dependence on problem dimensionality and sample size making it a suitable candidate for application to real world environmental optimisation problems the wilcoxon rank sum test is used for this purpose as it provides a statistically rigorous approach to testing dependence for a desired confidence level for the environmental modelling problems objective 2 whether the low degree of dependence on problem dimensionality and sample size holds for the selected metrics is checked by calculating the number of samples required for a particular ela metric value to be within 10 of the true value of this metric for different problem dimensionalities averaged over the three case studies as represented by ann models with different numbers of hidden nodes and hence model parameters see section 2 4 2 for details in this context the true value is taken as the value obtained for the largest number of samples considered i e 50 000 if the number of samples required to achieve accurate results is relatively small e g less than 5 000 then the low dependence of the metric under consideration on sample size is confirmed if the number of required samples is small for ann models with different numbers of hidden nodes i e different problem dimensionalities then the low dependence of the metric under consideration on problem dimensionality is also confirmed the fourth step involves the categorisation of the different ela metrics in terms of their relative degree of dependence on problem dimensionality and sample size for the benchmark functions objective 1 this is achieved by grouping the 89 ela metrics considered based on the centroids of the euclidean distances of the slopes of the linear regression relationships with logarithm transformation between reject rate see step 4 and sample size or problem dimensionality using a hierarchical clustering approach nielsen 2016 see section 2 5 for details this provides an indication of whether there are natural groupings of metrics with differing degrees of dependence on problem dimensionality and sample size and potential reasons for this as well as which metrics if any have low dependence on both problem dimensionality and sample size and are hence suitable for application to real world optimisation problems as part of the fifth and final step the landscape features that can be quantified with the fitness landscape metrics that have low dependence on problem dimensionality and sample size identified as part of objective 1 and validated in objective 2 are identified by mapping these metrics to the landscape features they are designed to provide information on via a desktop assessment objective 3 this provides information on the types of landscape features that can be obtained from ela metrics that can be applied to real world problems which can then be used to gain a better understanding of the characteristics of different environmental optimisation problems and which optimisation methods and parameterisations might be most appropriate for these however performing such assessments is beyond the scope of this paper the above analyses are conducted using the university of adelaide s supercomputing facilities which consist of 48 skylake nodes with 80 cpus and 377 gb of memory per node samples from the 24 noiseless bbob benchmark functions are generated using the r package flacco kerschke and trautmann 2016 the r package validann humphrey et al 2017 is used for ann development the r package fastcluster müllner 2013 is used for hierarchical clustering the r codes wilcox test and lm are used for wilcoxon rank sum test and regression analysis respectively and the matlab code plhs sheikholeslami and razavi 2017 is used for sample generation r code for how to calculate the ela metrics for a given set of fitness landscape samples is provided as supplementary material 2 2 sampling of fitness landscapes 2 2 1 benchmark functions as mentioned in section 2 1 the noiseless bbob suite of 24 benchmark functions hansen et al 2009 has been used extensively for a number of fitness landscape studies the functions have been designed specifically to represent fitness landscapes with a wide range of features such as the degree of multi modality global structure variable scaling in different directions i e variable sensitivity degree of similarity in different areas of the search space size of basins of attraction optima and differences in the magnitudes between global and local optima and degrees of flatness of the search space plateaus see table 1 the degree to which these different features are represented in the 24 functions is summarised in table b1 in appendix b 2 2 2 environmental modelling problems as mentioned in section 2 1 three different environmental modelling problems are considered including rainfall runoff modelling in the kentucky river usa the prediction of filtered water turbidity from a range of raw water quality parameters and the added alum dose treatment for surface waters in south australia and the forecasting of salinity in the river murray at murray bridge south australia based on values of upstream salinities and flows these are selected as they represent a diversity of environmental problems that have been used in a number of previous benchmarking studies e g wu et al 2013 humphrey et al 2017 as was the case in these studies the selected ann model architecture is a multi layer perceptron mlp as this type of model architecture has been used widely and successfully in practice and enables fitness landscapes i e the calibration error functions with different dimensionalities in terms of number of model parameters to be generated within the same model structural framework simply by changing the number of hidden nodes maier et al 2010 wu et al 2014 details of the model inputs and outputs as well as the available data are given in table 2 which are identical to those used in previous studies as mentioned in section 2 1 the number of hidden nodes for each ann is varied between 0 and 10 to ensure that fitness landscapes with different features are obtained see table 1 the root mean square error rmse is used as the objective function for model calibration as was the case in previous studies wu et al 2013 humphrey et al 2017 it should be noted that as the aim of this study is to identify the characteristics of the fitness landscapes of different ann models model calibration training is not required only samples generated from the fitness landscapes are needed 2 3 fitness landscape metrics as mentioned in section 2 1 a total of 89 ela metrics are considered these consist of the six low level groups of metrics developed by mersmann et al 2011 including convexity y distribution level set meta model local search and curvature and the information content of fitness sequences icofs metrics as shown in table 3 each of these groups of metrics is designed to provide information on different combinations of fitness landscape features including global structure multimodality separability global to local optima contrast search space homogeneity plateaus variable scaling and basin size homogeneity tables 1 and 3 brief outlines of these metrics are given below more detailed information such as the statistics used to summarise the results of these metrics and the mapping between the metrics and features are summarised in appendix c unfortunately there is no direct correlation between particular metrics and individual fitness landscape features making it difficult to provide a more intuitive understanding of the metrics convexity metrics convexity metrics use the deviation between linear regressed fitness values y and the true fitness value y to analyse the shape of fitness landscapes random pairs of points x i x j are selected from the total sample pool x and a third point is selected from the line between these two at this new point the actual fitness landscape value is compared to the linear interpolation of fitness values from the original points whether the landscape is positively or negatively convex can provide information about the overall shape of the fitness landscape y distribution metrics y distribution metrics use the probability density function pdf of fitness values of samples to provide information on the scaling and distribution of fitness landscapes in terms of the fitness values the pdf is estimated based on the frequency of fitness values identified by selected samples on the search space the distribution shown by the pdf can provide information about how easy it is to identify solutions with better fitness values as if more samples that have good fitness values are shown to have a higher probability to be identified it should be relatively easier to find the globally optimal solution unless the fitness landscape is deceptive in which case the global optimum is not in the vicinity of good local optima making it more difficult to locate e g needle in the haystack problems deb and goldberg 1994 maier et al 2014 level set metrics level set metrics use discriminant analysis to check the complexity of fitness landscapes samples are assigned to high and low quality groups based on their fitness values next different predictive models are used to check whether they can re classify the samples accurately meta model metrics meta model metrics involve the building of regression models based on sampling points and checking how well these fit to the fitness landscapes this is in order to show the similarity between the regression models and the corresponding problem s fitness landscape different regression models including both independent simple and cross term parameters are used to check the separability of a fitness landscape separable fitness landscapes should be more easily fitted to simple models whereas non separable fitness landscapes should be better fitted by the cross term models local search metrics local search metrics use the information provided by a set of local optima obtained by a gradient algorithm using random starting points to assess the properties of the distribution of optimal solutions across a fitness landscape of importance here is estimating the size of the basin of attraction for local optima local optima with short distances between each other are clustered within a common basin the size of these basins and how they are distributed across the fitness landscape are related to the features of the local optima curvature metrics curvature metrics assess the information provided by first order derivatives and hessian matrices of sample points on the fitness landscape this information can help to assess whether each variable has the same influence in guiding the searching process and whether the fitness landscape provides enough information to guide the searching to find good solutions especially for derivative perturbation based algorithms icofs metrics the information content of fitness sequences icofs metrics use a set of samples to construct a sequence of fitness landscape values based on either nearest neighbour in the parameter space or a random ordering these sequences are then used to create an indicator sequence of values from 1 0 1 depending on the comparison of sequential values in the original sequence that is for consecutive sequence values y n and y n 1 a value of 1 is assigned if y n y n 1 0 for y n y n 1 and 1 for y n y n 1 note that a threshold is used for the inequality comparisons a smooth near monotonic fitness landscape e g single modal one would be expected to have a sequence without frequent signal change e g 1 1 1 1 1 or 1 1 1 1 1 a multi model fitness landscape on the other hand would be expected to have a sequence with frequent changes in the signal e g 1 1 1 1 0 1 0 1 finally a flat fitness landscape would have a sequence of zeros due to its only slight difference in fitness values the information contained in the sequence is processed and used to provide characterisations of the level of roughness in the fitness landscape which is highly related to multi modality 2 4 calculation of degree of dependence of metrics on dimensionality and sample size as mentioned in the introduction the objective of this paper is to identify which ela metrics have low dependence on problem dimensionality and sample size so that they can be applied to real world environmental optimisation problems it should be noted that although the primary features of the benchmark functions are known assessing the degree to which the ela metrics under consideration are able to correctly assess these features is beyond the scope of this paper in addition there is no direct one to one correspondence between different metrics and different features as mentioned in section 2 3 2 4 1 benchmark functions as mentioned in section 2 1 the degree of dependence of the benchmark functions on problem dimensionality and sample size is assessed with the aid of the two tailed wilcoxon rank sum test which is a nonparametric test with a null hypothesis that the probability of the selected populations arises from the same underlying distribution in this case the two populations under consideration consist of values of fitness landscape metrics calculated using a given number of samples e g 100 to 120 000 see fig 1 and those calculated using the largest number of 120 000 samples the reasoning behind this is that the metric value at the largest number of samples is taken as the most accurate computation of this metric that is the hypothesis test is given as 1 h 0 e l a i j d k e l a i j d 120 000 h 1 e l a i j d k e l a i j d 120 000 where e l a i j d k and e l a i j d 120 000 represent the ela metric results of the i th metric the j th test function and the dth dimension with k samples and 120 000 samples respectively consequently if the null hypothesis is satisfied for relatively small sample sizes for a particular fitness landscape metric this metric can be considered to have low dependence on sample size in this study a 95 confidence level is used to test the hypotheses to enable the results of the different computational experiments to be compared more easily they are represented in terms of the reject rate symbolized by r which is the percentage of experiments for which the above null hypothesis is rejected and is calculated as follows 2 r i d k j 1 n i p i j d k 0 05 n where r i d k refers to the reject rate of metric i among the 24 test functions for problems with d dimensions and k samples i x is an indicator function which is equal to 1 if the boolean statement x is true and zero otherwise and n is the total number of test functions which is 24 in this study consequently lower values of the reject rate indicate that a metric is more independent of sample size and dimensionality and hence more suited to being applied to real world environmental optimisation problems 2 4 2 environmental modelling problems as mentioned in section 2 1 fitness landscape metrics that are found to have low dependence on both problem dimensionality and sample size i e a low reject rate for the test functions see section 2 4 3 are applied to the real world environmental modelling problems to check if the low dependence of these metrics on sample size and dimensionality holds for the real world environmental optimisation problems considered as also mentioned in section 2 1 this check is achieved by calculating the number of samples required for a particular ela metric value to be within 10 of the true value of this metric for different problem dimensionalities which is considered reasonable for practical purposes the percentage difference is calculated using the normalized difference between the true value of a given ela metric obtained for a sample size of 50 000 see section 2 1 and the corresponding value for a smaller sample size k e g 100 50 000 see fig 1 as follows 3 e r r i h c k m e d i h c k m e d i h c 50 000 m e d i h c 50 000 100 where e r r i h c k is the normalized error which is calculated by using the corresponding medians of metric i case c and number of hidden nodes h corresponding to different problem dimensionalities see table 2 and m e d i h c k is the median of the metric values in the data set as identified by the subscripts the subscripts have the same meaning as in e r r i h c k in addition to metrics that were considered unsuitable based on the computational criteria five ela metrics that were not considered to provide useful information on the real world environmental case studies considered were also excluded these include metrics that apply to linear non continuous and low impact relationships which is not the case for anns as they are highly non linear and continuous consequently values of these metrics are equal to zero for the case studies considered therefore not providing any useful information 2 5 categorisation of fitness landscape metrics as mentioned in section 2 1 using the results from the analysis on the benchmark functions the assessed ela metrics are categorized based on their degree of dependence on problem dimensionality and sample size in order to identify metrics with low dependence on both this is achieved via a two step process the first step involves the quantification of the degree of dependence of metric values on sample size and problem dimensionality this is achieved by developing a regression model that relates the reject rates calculated in eq 4 to sample size and problem dimension as follows 4 r a ln d i m b ln s s c where r represents the reject rate d i m and s s represent dimension and sample size respectively and a b and c represent the slope of dimension and sample size and intercept respectively by way of interpretation for example a large value of a implies that the reject rate is highly influenced by the dimension example r code for performing these calculations is provided as supplementary material this form of the relationship was considered most appropriate based on visual inspection of the plots of reject rate versus sample size and problem dimensionality with a logarithm transformation used to scale the magnitude of the coefficients as shown in appendix d the r 2 values of these relationships generally range between 0 3 and 0 86 indicating the ability to discriminate between relationships of different strengths for a small number 10 relationships r 2 values were less than 0 2 however as shown in appendix d this was for relationships with very low dependence on sample size and problem dimensionality where the fluctuations in the relationship i e noise had a significant impact on the r 2 values however this did not affect the correct quantification of the relative impact of sample size and problem dimensionality on ela metric values which is the primary objective the second step involves hierarchical clustering nielsen 2016 of the values of the slopes for dimensionality i e values of a in eq 6 and sample size i e values of b in eq 6 for different ela metrics based on the centroids of their euclidean distances to identify groups of ela metrics with different degrees of dependence on sample size and problem dimensionality it should be noted that in order for the above results to be meaningful the absolute values of r also need to be checked while low dependence on sample size and problem dimensionality are pre conditions for the application of ela metrics to real world environmental problems metrics belonging to this category only provide useful information if the values of r are consistently low rather than consistently high in this study this check is performed by visual inspection of the plots of r versus sample size and problem dimensionality 3 results and discussion 3 1 categorisation of fitness landscape metrics 3 1 1 cluster location as can be seen in fig 2 the ela metrics considered form five distinct clusters with different degrees of dependence on problem dimensionality and sample size typical relationships between reject rate dimensionality and sample sizes for metrics in these clusters are shown in fig 3 the 39 metrics belonging to cluster 1 have a low dependence on problem dimensionality and sample size fig 2 as evidenced by the flat slopes in the relationships between reject rate and both problem dimensionality and sample size as seen in fig 3 a the fact that the reject rate for metrics belonging to this cluster is very low across the full range of sample sizes and problem dimensionalities investigated fig 3 a suggests that metrics belonging to this cluster are suitable for application to real world environmental optimisation problems in contrast this does not appear to be the case for the metrics belonging to the remaining clusters for example the 23 metrics belonging to clusters 2 and 3 have high dependence on sample size and medium high dependence on problem dimensionality with typical plots of the relationships between reject rate problem dimensionality and sample size for metrics belonging to these clusters shown in fig 3 b to 3 d this is likely to make the application of these metrics to real world environmental optimisation problems computationally intractable the 5 metrics belonging to cluster 4 have low dependence on dimensionality but high dependence on sample size as evidenced by a typical plot of reject rate versus these two factors in fig 3 e this makes metrics belonging to this cluster difficult to apply in practice as large sample sizes are required for even relatively simple problems the 7 metrics belonging to cluster 5 have low dependence on sample size but medium dependence on problem dimensionality see fig 3 f for a typical plot of the relationship of reject rate versus sample size and dimensionality this makes them applicable to relatively simple real world problems but computational tractability is likely to become an issue for higher dimensional problems it should be noted that 15 metrics are excluded from clusters 1 to 5 in fig 2 as their reject rates are very high see fig 3 g and h making them unsuitable for application to real world optimisation problems as discussed in section 2 5 as can be seen in fig 3 g there are some cases where the reject rate is low when the sample size is small but this increases rapidly when the sample size increases to a given level this is because when the sample size is small the values of these metrics are highly variable providing greater opportunities for the median values to be close to the true value however this variability decreases with an increase in sample size reducing the chance that the median values are close to the true value of the metric indicating that the actual reject rates for these metrics are very high and therefore not suitable for application to real world environmental problems cluster dimension impact sample size impact 1 low low 2 median high 3 high high 4 low high 5 median low 3 1 2 cluster composition a summary of the composition of each of the five clusters in fig 3 in terms of metric class is given in table 4 as can be seen cluster 1 contains at least one metric from each class with all of the 4 convexity and 10 icofs metrics belonging to this cluster the majority of the 3 y distribution 66 7 the 9 meta model 66 7 and 21 local search 57 1 metrics also belong to cluster 1 while only one out of the 18 level set 5 6 and 4 out of the 24 curvature 16 7 metrics fall into this cluster a common feature of all metrics belonging to cluster 1 irrespective of which metric class they are part of is that their calculation only requires fitness values and the relative distance between samples without knowledge of the location of each sample in the search space as is the case with many of the other metrics this is a likely cause for the low dependence of the calculation of these metrics on sample size and problem dimensionality the majority of the metrics belonging to clusters 2 and 3 are part of the level set and meta model classes and have relatively high levels of dependence on both sample size and problem dimensionality the likely reason for this is that calculation of metrics in these two classes requires the development of regression models using the available samples consequently the values of the metrics obtained are a function of sample size and dimensionality as the development of representative regression models generally requires a larger number of samples for higher dimensional problems however the degree to which this is the case is a function of the complexity and non linearity of the required regression models for example as the calculation of some of these metrics is based on simple linear regression models some of the metrics belonging to these classes have low dependence on sample size and problem dimensionality and hence belong to cluster 1 table 4 all of the metrics belonging to cluster 4 are part of the curvature metric class table 4 and have a high dependence on sample size but a low dependence on dimensionality this is because the values of the curvature metrics are based on the first order derivative and hessian matrix of each sample point consequently calculation of these metric values is a function of individual samples without considering the spatial dependencies of their relationships and is therefore the likely reason they are not affected significantly by dimensionality in contrast sample size has a significant impact on curvature metric values this is because sample points in different regions of the search space are likely to provide different gradient information resulting in high variability unless the sample size is sufficient this issue is likely to be exacerbated for some curvature metrics metrics related to c g and c h in eq c9 and c12 respectively that rely on information about relative gradients especially in flat regions of the search space as this is likely to result in infinite values consequently these curvature metrics are the ones that result in high reject rates even for low dimensional problems and large sample sizes e g fig 3 g and h and have therefore been excluded from the clusters in fig 2 cluster 5 consists of metrics belonging to the local search class which have low dependence on sample size but high dependence on problem dimensionality this is because these metrics are related to the size of the local basins within the search space which is calculated based on the number of local optima identified in each basin as problem dimensionality increases the number of basins grows dramatically making it virtually impossible to identify more than one local optimum in each basin as a result the values of the metrics become meaningless even for relatively large sample sizes 3 2 validation of categorisation of fitness landscape metrics as mentioned in section 2 4 2 five of the 39 cluster 1 metrics are not suitable for application to real world environmental problems as a result only the 34 remaining metrics are validated using the real world environmental modelling problems to summarise these results fig 4 shows the number of samples required for each of these metrics to achieve convergence for the anns with different numbers of hidden nodes as discussed in section 2 4 2 convergence was taken as the number of samples required for metric values to be within 10 of the true value obtained for the maximum number of samples considered as can be seen from figs 4 and 28 of these 34 metrics 82 4 converge within 2 000 samples which is typically within 4 of the number of samples used to generate the true metric values i e 50 000 for most metrics see figure a1 for the vast majority of these metrics 22 converge occurs within 500 samples which is typically within 1 of the number of samples used to generate the true metric values the results in fig 4 clearly illustrate that there is no increase in the number of samples required for convergence with an increase in problem dimensionality i e the number of hidden nodes consequently these 28 metrics can be considered to have low dependence on sample size and problem dimensionality from a practical perspective indicating that they are likely to provide a computationally efficient means for better understanding the fitness landscapes of a range of complex highly dimensional real world environmental optimisation problems it should be noted that for some of the metrics there is a slight increase in the number of samples required for convergence for lower problem dimensionalities i e smaller number of hidden nodes however these variations are very small i e on the order of hundreds of samples compared with the 50 000 samples used to obtain the true metric values the six metrics that showed low dependence on sample size and problem dimensionality for the benchmark problems but not for the real world environmental problems include four metrics belonging to the meta model class one belonging to the icofs class and one belonging to the y distribution class these four meta model metrics all utilise single regression models that do not consider interactions between parameters as interactions between parameters are likely to be a feature of real world environmental modelling problems the single regression models used in the four metrics in question are unlikely to represent the fitness landscapes of the environmental modelling problems considered consequently these metric values are likely to become non informative requiring a larger number of samples for accurate calculation in contrast the lack of rapid convergence of the y distribution i e kurtosis and icofs i e ε s metrics that appear to not be suitable for real world environmental modelling problems is likely to be related to the scaling of fitness values for the problems considered suggesting that the scaling of fitness values is more difficult to recognise by using samples from real world problems than test functions it is likely that different samples provide fitness values with different scaling making these two metric values unstable for environmental modelling problems 3 3 interpretation of metrics with low dependence on sample size and dimensionality based on the results presented in sections 3 1 and 3 2 there are 28 ela metrics that appear to be suitable for application to real world environmental optimisation problems as they have been shown to have low dependence on sample size and problem dimensionality for a wide range of benchmark and real world problems however in addition to their computational tractability the usefulness of these ela metrics is also a function of the type of information they can provide about the different features of the fitness landscapes of environmental optimisation problems as shown in table 5 the 28 suitable metrics cover six metric classes excluding curvature metrics and provide information on six of the eight major fitness landscape features this provides the opportunity to obtain a better understanding of different of attributes of a range of environmental optimisation problems in a computationally efficient manner for example application of metrics such as nn rand ic eps max and nn rand ic eps ratio can provide information on the potential identifiability of environmental models shin et al 2013 bastidas et al 2006 that can complement information provided by more commonly used sensitivity analysis approaches e g razavi and gupta 2015 guillaume et al 2019 razavi et al 2021 specifically information about the magnitude of multimodality and plateaus of fitness landscapes can provide information on the size of regions with non unique parameters and information about search space homogeneity can provide insight into the distribution and location of these regions alternatively ela metrics can provide insight into which optimisation algorithm or optimisation algorithm parameterisation are most appropriate for a given problem maier et al 2014 gibbs et al 2011 2015 for example nn rand ic h max ela local center dist mean and ela local fun evals median can be used to obtain information about the degree of multimodality distribution of optima regions and overall depth of optima region of the fitness landscape in a computationally efficient manner prior to the optimisation process if the fitness landscape is found to have low multimodality and the optima regions are shallow and converged to a small area on the fitness landscape use of a gradient based optimisation approach might be most appropriate maier et al 2019 in contrast if the fitness landscape is found to have high multimodality with deep and widely distributed optima regions use of global search evolutionary algorithms might be preferred maier et al 2019 the degree of multimodality and the presence of plateaus is also able to inform which values of the parameters that control the searching behaviour of evolutionary algorithms are most appropriate see munoz and smith miles 2017 wang et al 2020 zecchin et al 2005 2012 and the degree of homogeneity of the search space is able to assist with determining whether there is value is adapting the values of the parameters that control the searching behaviour of evolutionary algorithms e g zheng et al 2017 4 summary and conclusions optimisation algorithms are used extensively for the development of environmental models and the identification of solutions to environmental problems how well a particular algorithm performs on a given problem is a function of both algorithm behaviour and the characteristics of the problem being solved as represented by the fitness landscape while significant attention has been given to the development of algorithms with different behaviours little effort has been devoted to better understanding problem characteristics generally resulting in a brute force approach to identifying algorithms and parameterisations that perform acceptably for a particular problem this is despite the fact that a number of metrics have been developed to assist with identifying features of fitness landscapes such as their global structure their degree of multimodality and the presence of plateaus the identification of which would assist in the selection of appropriate optimisation algorithms and parameterisations without the need for a brute force approach the primary reason for the lack of adoption of fitness landscape metrics in practice is that the calculation of these metrics is based on samples from the fitness landscape which can be computationally expensive for real world environmental problems as they are often based on complex and highly dimensional simulation models in order to test whether this is the case the degree of dependence on problem dimensionality and sample size of 89 fitness landscape metrics was assessed each metric was calculated for 72 000 different sets of fitness landscape samples obtained from 2 400 fitness landscapes derived from commonly used benchmark functions and their degree of dependence on problem dimensionality and sample size was assessed results show that 39 of the 89 metrics have low dependence on dimensionality and sample size 34 of which are considered suitable for application to environmental problems the low degree of dependence on problem dimensionality and sample size of these 34 metrics was tested on a number of real world environmental modelling problems corresponding to 7 590 sets of fitness landscape samples from 390 fitness landscapes results indicate that 28 of the 34 aforementioned fitness landscape metrics also have low dependence on problem dimensionality and sample size for the real world environmental modelling problems often requiring fewer than 500 fitness landscape samples for convergence these 28 metrics cover a wide range of fitness landscape features including their global structure multimodality separability search space and basin size homogeneity and the presence of plateaus a limitation of this study is that although ela metrics that have low dependence on problem dimensionality and sample size were identified using a large number of test functions with different dimensionalities and a wide variety of fitness landscape features these mathematical functions are unlikely to represent all of the complexities and features of fitness landscapes associated with real world optimisation problems however the fact that the majority of the ela metrics that were found to have low dependence on sample size and problem dimensionality for the test functions also had low dependence on sample size and problem dimensionality for the real world problems considered provides confidence in the generality of the findings presented another limitation of this study is that the ela metrics can only be calculated for continuous optimisation problems which excludes certain types of problems encountered in practice such as the optimisation of water distribution systems using discrete pipe sizes e g zheng et al 2017 wang et al 2020 and the optimisation of best practice stormwater management options e g di matteo et al 2019 the findings that there are 28 fitness landscape metrics that are able to provide insight on a range of fitness landscape characteristics that appear to be suitable for application to real world environmental optimisation problems opens the door to gaining greater insights and improving the efficiency of a range of environmental optimisation problems for example these metrics can provide insight into the potential identifiability of the parameters of different environmental models as well as information on the suitability of different optimisation algorithms and parameterisations for particular environmental optimisation problems consequently future research efforts should focus on testing the applicability of the identified metrics to a wide range of real world optimisation problems in order to better understand the features of their fitness landscapes and to check whether the features of these landscapes identified with the aid of the ela metrics align with those identified in previous studies providing further confidence in the usefulness of the metrics in addition there would be value in applying the metrics to the fitness landscapes of the individual objective functions for multi and many objective optimisation problems and to better understand the extent to which knowledge of the features of fitness landscapes can inform the selection of appropriate optimisation algorithms and their parameterisations declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments the first author is supported by an adelaide graduate research scholarship which is gratefully acknowledged the authors would also like to thank joseph guillaume and the anonymous reviewer of this paper whose comments have improved the quality of this paper significantly appendix e supplementary data the following are the supplementary data to this article multimedia component 1 multimedia component 1 multimedia component 2 multimedia component 2 multimedia component 3 multimedia component 3 multimedia component 4 multimedia component 4 appendix a detailed outline of methodology figure a 1 detailed outline of methodology figure a 1 appendix b details of benchmark functions table b 1 detailed features of benchmark functions adapted from mersmann et al 2010 table b 1 function multi modality global structure separability variable scaling space homogeneity basin homogeneity global to local contrast 1 none none high none high none none 2 none none high high high none none 3 high strong none low high low low 4 high strong high low high med low 5 none none high none high none none 6 none none high low med none none 7 none none high low high none none 8 low none none none med low low 9 low none none none med low low 10 none none none high high none none 11 none none none high high none none 12 none none none high high none none 13 none none none low med none none 14 none none none low med none none 15 high strong none low high low low 16 high med none med high med low 17 high med none low med med high 18 high med none high med med high 19 high strong none none high low low 20 med weak none none high low low 21 med none none med high med low 22 low none none med high med med 23 high none none none high low low 24 high weak none low high low low as plateaus is not specified in bbob suite it is not included in this table appendix c details of ela metrics a summary of the ela metrics used is given in table c1 and details on how different groups of metrics are calculated and how they assist with the characterisation of different fitness landscape features are given below table c 1 summary of ela metrics used table c 1 no metric cluster class 1 ela conv conv prob 1 convexity 2 ela conv lin prob 1 convexity 3 ela conv lin dev orig 1 convexity 4 ela conv lin dev abs 1 convexity 5 ela distr skewness 1 y distribution 6 ela distr kurtosis 1 y distribution 7 ela distr number of peaks 2 y distribution 8 ela level mmce lda 10 2 level set 9 ela level mmce qda 10 3 level set 10 ela level mmce mda 10 2 level set 11 ela level lda qda 10 3 level set 12 ela level lda mda 10 1 level set 13 ela level qda mda 10 3 level set 14 ela level mmce lda 25 2 level set 15 ela level mmce qda 25 3 level set 16 ela level mmce mda 25 2 level set 17 ela level lda qda 25 3 level set 18 ela level lda mda 25 2 level set 19 ela level qda mda 25 3 level set 20 ela level mmce lda 50 2 level set 21 ela level mmce qda 50 3 level set 22 ela level mmce mda 50 2 level set 23 ela level lda qda 50 3 level set 24 ela level lda mda 50 2 level set 25 ela level qda mda 50 3 level set 26 ela meta lin simple adj r2 1 meta model 27 ela meta lin simple intercept 1 meta model 28 ela meta lin simple coef min 1 meta model 29 ela meta lin simple coef max 1 meta model 30 ela meta lin simple coef max by min 1 meta model 31 ela meta lin w interact adj r2 2 meta model 32 ela meta quad simple adj r2 1 meta model 33 ela meta quad simple cond 2 meta model 34 ela meta quad w interact adj r2 3 meta model 35 nn ic h max 1 icofs 36 nn ic eps s 1 icofs 37 nn ic eps max 1 icofs 38 nn ic eps ratio 1 icofs 39 nn ic m0 1 icofs 40 rand ic h max 1 icofs 41 rand ic eps s 1 icofs 42 rand ic eps max 1 icofs 43 rand ic eps ratio 1 icofs 44 rand ic m0 1 icofs 45 ela curv grad norm min 4 curvature 46 ela curv grad norm lq uc curvature 47 ela curv grad norm mean uc curvature 48 ela curv grad norm med uc curvature 49 ela curv grad norm uq uc curvature 50 ela curv grad norm max 4 curvature 51 ela curv grad norm sd uc curvature 52 ela curv grad norm nas 1 curvature 53 ela curv grad scale min 4 curvature 54 ela curv grad scale lq uc curvature 55 ela curv grad scale mean uc curvature 56 ela curv grad scale med uc curvature 57 ela curv grad scale uq uc curvature 58 ela curv grad scale max 4 curvature 59 ela curv grad scale sd uc curvature 60 ela curv grad scale nas 1 curvature 61 ela curv hessian cond min 4 curvature 62 ela curv hessian cond lq uc curvature 63 ela curv hessian cond mean uc curvature 64 ela curv hessian cond med uc curvature 65 ela curv hessian cond muq uc curvature 66 ela curv hessian cond max 1 curvature 67 ela curv hessian cond sd uc curvature 68 ela curv hessian cond nas 1 curvature 69 ela local n loc opt abs 5 local search 70 ela local n loc opt rel 2 local search 71 ela local best2mean contr orig 5 local search 72 ela local best2mean contr ratio 5 local search 73 ela local center dist min 5 local search 74 ela local center dist lq 1 local search 75 ela local center dist mean 1 local search 76 ela local center dist median 1 local search 77 ela local center dist uq 1 local search 78 ela local center dist max 5 local search 79 ela local center dist sd 1 local search 80 ela local basin sizes avg best 5 local search 81 ela local basin sizes avg non best 5 local search 82 ela local basin sizes avg worst 5 local search 83 ela local fun evals min 1 local search 84 ela local fun evals lq 1 local search 85 ela local fun evals mean 1 local search 86 ela local fun evals median 1 local search 87 ela local fun evals uq 1 local search 88 ela local fun evals max 1 local search 89 ela local fun evals sd 1 local search uc in cluster column represents metrics not classified in this study c1 convexity metrics as can be seen in table 3 convexity metrics are able to provide information on a number of fitness landscape features including global structure multimodality and search space homogeneity their calculation requires implementation of the following general steps i select random pairs of points x i x j from the total number of samples of the fitness landscape considered i e 100 to 120 000 samples see fig 1 to ensure most of the samples are included in the calculation we use n random pairs of points in this study where n is the number of initial samples ii calculate a linear combination of x i x j to select a new point x n between the two points where c1 x n w x i 1 w x j where w is a random number between 0 and 1 calculate the fitness value y n of x n based on the corresponding test function iii calculate the fitness values y i y j of x i x j based on the corresponding test function use linear regression to calculate the approximated linear fitness value y n at x n c2 y n w y i 1 w y j where w is the same w as in ii iv calculate the difference δ between y n and y n by δ y n y n a if δ is negative the landscape between the selected two points is convex providing good gradient information to guide the search in this region of the fitness landscape b if δ is positive the landscape between the selected two points is not convex providing poor gradient information to guide the search in this region of the fitness landscape v in total 4 convexity metrics are considered which differ in terms of statistics methods to summarise the results of δ obtained from n pairs of samples they are the probability of convexity ela conv conv prob which relates to the probability of negative δ probability of linearity ela conv lin prob which relates to the probability of δ 0 mean original deviation ela conv lin dev orig which relates to the mean value of δ from n pairs of samples mean absolute deviation ela conv lin dev abs which relates to the mean value of δ from n pairs of samples convexity metrics are able to provide information on the global structure of fitness landscapes as they present information about the general shape of fitness landscapes and can therefore provide information on whether fitness landscapes have a clear structure to guide searching or not they are also able to provide information on search space homogeneity and multimodality as they take the probability of convexity into account a high or low convexity rate indicates fitness landscapes maintain the same trend and shape in most areas of the fitness landscape which is representative of greater homogeneity and reduced multimodality in contrast middle range values of the convexity rate indicate that fitness landscapes have different trends and shapes in different areas increasing changes of inhomogeneity and multi modality b2 y distribution metrics as can be seen in table 3 y distribution metrics are able to provide information on a number of fitness landscape features including global structure multimodality and search space homogeneity their calculation requires implementation of the general following steps i generate the pdf of the fitness values y s of samples x s ii in total 3 y distribution metrics were considered which relates to the properties of the pdf of y s including skewness ela distr skewness kurtosis ela distr kurtosis and the number of peaks ela distr number of peaks y distribution metrics are able to provide information on the global structure of fitness landscapes for example if the skewness of the pdf is negative most of the obtained y are small indicating that the bottom region of a fitness landscape is bigger than the top region referring to a bigger bowl bottom than for a fitness landscapes with a positive skewness they are also able to provide information on multimodality as a multi modal fitness landscapes are likely to have several peaks in their pdfs which refers to different bottom regions of the fitness landscape additionally y distribution metrics are able to provide information on the prevalence of plateaus within the landscape as plateau like landscapes contain region s with the same fitness values as a result they would tend to have high kurtosis values which indicates that most of the fitness values have no significant difference c3 level set metrics as can be seen in table 3 level set metrics are able to provide information concerning a number of fitness landscape features including global structure multimodality and plateaus their calculation requires implementation of the following steps i split all the samples to high quality and low quality ones based on a given quantile threshold of their fitness values y s in this study 10 25 and 50 quantiles are used as thresholds ii linear lda quadratic qda and mixture mda discriminant analysis are used to predict whether the fitness values y s are high or low quality the number of y s which are classified to a wrong quality group are recorded iii calculate the mean misclassification error mmce which refers to the probability of misclassification of y s by using corresponding discriminant analysis methods iv in total 18 level set metrics are considered which differ in terms of quantile thresholds and discriminant analysis methods i e ela level mmce lda qda mda 10 25 50 the quotient of mmce of lda divided by mmce of qda ela level lda qda 10 25 50 the quotient of mmce of lda divided by mmce of mda ela level lda mda 10 25 50 and the quotient of mmce of qda divided by mmce of mda ela level qda mda 10 25 50 are also included in the metrics as they can show the differences of mmce between simple models lda and qda and complex models mda level set metrics are able to provide information on global structure and multimodality as through the mmce of different discriminant analysis the distribution of fitness values can be determined for example if mmces of lda and qda are low and mmces of mda are high fitness values on fitness landscapes can be easily classified by these two simple models indicating high quality values and low quality values are not located in the same region so that high quality values can be easily identified as finding these values will not be interrupted by low quality values in this case as differences between fitness values are not big in the small region the landscapes should not contain multiple optima in contrast if mmces of lda and qda are high the global structure of a fitness landscape is likely to be complex as this indicates that there are no clear top and bottom regions of the fitness landscapes but very frequent variation in fitness values this can also result in a high level of multimodality of fitness landscapes furthermore if mmces of mda are also high the structure of fitness landscapes can be quite complex and multi modal level set metrics are also able to provide information on plateaus as plateau like landscapes have many similar fitness values the threshold of high and low quality values is not clear for such problems as a result plateau like landscapes are more likely to have high mmces for all discriminant analysis methods c4 meta model metrics as can be seen in table 3 meta model metrics are able to provide information on a number of fitness landscape features including global structure multimodality plateaus and variable scaling their calculation requires implementation of the general following steps i build the corresponding regression models between samples x s and corresponding fitness values y s four regression models are built in this study which are simple linear regression c3 y i 1 n a i v i where a i is the coefficient of corresponding variable v i interacted linear regression c4 y i 1 n a i v i i 1 n j 1 n b k v i v j where b k is the coefficient of corresponding variable v i v j simple quadratic regression c5 y i 1 n a i v i i 1 n c i v i 2 where c i is the coefficient of corresponding variable v i 2 which is the square of v i interacted quadratic regression c6 y i 1 n a i v i i 1 n c i v i 2 i 1 n j 1 n b k v i v j i 1 n j 1 n t k v i v j 2 i 1 n j 1 n l k v i 2 v j 2 where t k and l k are the coefficient of corresponding variable v i v j 2 and v i 2 v j 2 respectively ii in total 9 meta model metrics are considered which are related to adjusted coefficients of determination r2 ela meta lin quad simple w interact adj r2 of four regression models maximum minimum and intercept coefficients of simple linear regression models ela meta lin simple coef max coef min intercept and the quotient between maximum and minimum coefficients ela meta lin quad simple cond of simple linear and simple quadratic regression models meta model metrics are able to provide information on the global structure and multimodality of fitness landscapes as adjusted r2 can show how well global structure matches the corresponding models and a goodness of fit shown by r2 also indicates models with a low level of multimodality as all these regression models are not multi modal they are also able to provide information on separability as separate fitness landscapes are likely to have higher adjusted r2 as they are likely to be represented more easily by simpler models additionally meta model metrics are able to provide information on variable scaling as shown by the maximum and minimum of coefficients of the models low degree variable scaling fitness landscapes should have models with maximum and minimum coefficients the values of which are close to each other indicating that all variables make similar contributions to the fitness values on the other hand high degree variable scaling fitness landscapes should have models with significant different maximum and minimum coefficients indicating that the contributions of different variables to the fitness values are not the same c5 local search metrics as can be seen in table 3 local search metrics are able to provide information on a number of fitness landscape features including multimodality global to local optima contrast basin size homogeneity and search space homogeneity their calculation requires implementation of the general following steps i use a gradient algorithm to find local optima starting from initial samples x s in this study the l bfgs b algorithm zhu et al 1997 was used due to its capacity to setup the range of calculation to avoid the identified local optima being beyond the range of the fitness landscape ii use of hierarchical clustering to cluster identified local optima in i local optima within a given euclidean distance e are included in the same cluster which refers to a corresponding local basin in this study e is 5 of total euclidean distance length of the whole fitness landscape as this distance performs well in distinguishing different clusters without resulting in a computational burden that results in intractability if e is too small a larger number of clusters is likely to be generated by hierarchical clustering which increases complexity and the computational requirements of subsequent calculations iii calculate the centroid x c of each local basin identified in ii based on the local optima in the corresponding basin calculate the fitness value y c of all centroids iv in total 21 local search metrics are considered which relates to the number of identified local basins ela local n loc opt abs rep fitness value differences between high quality basins global optima and low quality basins local optima ela local best2mean contr orig ratio basin size difference between high average and low quality basins i e difference between number of optima in high and low quality basins ela local basin sizes avg best avg non best avg worst statistics of basin centroids euclidean distances and statistics of number of evaluated functions to find optima from initial samples i e minimum maximum lower quantile median mean upper quantile and standard deviation of basin centroids euclidean distances and number of evaluated functions ela local center dist fun evals min max lq median mean uq sd local search metrics are able to provide information on multimodality as the level of multimodality is highly related to the identified number of optima by using a gradient algorithm they are also able to provide information on global to local optima contrast as fitness landscapes with a high degree of global and local contrast are likely to have significantly different fitness values between global and local basins optima and vice versa additionally local search metrics are able to provide information on basin size homogeneity as they can present the size difference between high and low quality basins in order to check whether basins have the same quality on the other hand the evaluated number of functions can also show the depth of different basins finally they are able to provide information on search space homogeneity as they are able to show the distribution of centroids on fitness landscapes the distance between centroids can indicate whether basins are converged to a small region or widely distributed on the whole fitness landscape referring to whether different regions in search space have the same feature c6 curvature metrics as can be seen in table 3 curvature metrics are able to provide information on a number of fitness landscape features including plateaus and variable scaling their calculation requires implementation of the general following steps i calculate the gradient information c7 f x i d f x i d x where f x is the fitness function and f x i is the first order derivative of f at the variable x i based on f x in all directions the total gradient length l of a sample is calculated as c8 l i 1 n f x i 2 where n is the number of variables dimensions of the fitness function the gradient condition c g of a sample is calculated as c9 c g max f x min f x ii calculate the hessian matrix c10 h x 2 f x x i x j so the eigenvalues λ of h x can be calculated from c11 h x λ i n 0 where i n is the identity matrix with size n the hessian condition c h of a sample is calculated as c12 c h max λ min λ iii in total 24 curvature metrics are considered which differ in terms of 8 statistics of l c g and c h of all samples which are the minimum maximum lower quantile median mean upper quantile standard deviation and proportion of samples with no l c g and c h i e ela curv grad norm grad scale hessian cond min max lq med mean uq sd nas curvature metrics are able to provide information on the plateaus of fitness landscapes as they refer to the gradient information of fitness landscapes plateau like fitness landscapes contain limited gradient information resulting in small l of all samples in general they are also able to provide information on variable scaling as c g and c h can show the differences of contribution between variables to the fitness values large c g and c h values generally indicate that there are variables which have very small contributions to the fitness values prviding little guidance to the search algorithm c7 icofs metrics as can be seen in table 3 icofs metrics are able to provide information on a number of fitness landscape features including global structure multimodality and plateaus their calculation requires implementation of the following general steps i firstly sort all samples into a sequence in this study two sampling ordering methods are used to generate different icofs metrics 1 nearest neighbouring nn as part of which the following sample x i 1 of one sample x i is the closest sample to the corresponding by euclidean distance 2 random rand order as part of which the following sample x i 1 of one sample x i is randomly selected from the entire set of samples ii build a symbol sequence ε by using the following rule c13 i 1 i f y i 1 y i ε 0 i f y i 1 y i 1 i f y i 1 y i ε ε where ε 0 is the accuracy parameter of the symbol sequence and y i is the fitness value of sample x i it can be seen that ε is controlled by the value of ε if ε is small ε can be quite sensitive and contain frequent symbol changes in the sequence for example sequence 1 1 1 1 1 if ε is big on the other hand ε can be insensitive and contain many 0 values in the sequence for example sequence 0 0 0 0 0 iii calculate the information content h ε of the sequence based on the definition c14 h ε a b p a b l o g 6 p a b where a b 1 0 1 and p a b is the probability that two neighboured symbols a b are different iv build a new sequence ε by removing all 0 values in ε and calculate the partial information content m ε which is defined as c15 m ε n 1 where n is the length of sequence ε v in total 10 icofs metrics are considered and both of two sample orders contain 5 metrics the typical result curves of h ε and m ε against ε are shown in figure c1 munoz et al 2015b provides 5 metrics to summarise the curves which are c16 h m a x max h ε c17 ε m a x l o g 10 ε w h e r e h ε h m a x c18 ε s l o g 10 min ε w h e r e h ε 0 05 c19 m 0 m ε 0 c20 ε r a t i o l o g 10 max ε w h e r e m ε 0 5 m 0 the detailed dot plots of the 5 metrics are also shown in figure c1 the total 10 metrics are shown as format nn rand ic h max eps max eps s m0 eps ratio figure c 1 typical results of icofs figure c 1 icofs metrics are able to provide information on multimodality as the symbols in the sequence represent information about the smoothness of fitness landscapes rough landscapes are likely to have high values of h m a x and m 0 and if landscapes are rough they have the potential to have a high degree of multimodality additionally they are able to provide information on plateaus and global structure as plateau like landscapes should contain many 0s in their symbol sequence even when ε is very small this is likely to return a small ε s ε m a x and ε r a t i o for a plateau like landscape in contrast fitness landscapes with good global structure should have a level of scaling in terms of fitness values as a result small ε s ε m a x and ε r a t i o should be bigger than those for flat fitness landscapes appendix d slopes and coefficients of determination r 2 of benchmark test for all tested metrics table d 1 slope of dimension sample size and r 2 of plane regression for all metrics in different classes and clusters unclassified metrics are not included table d 1 metric number metric slope of dim slope of sample size plane r 2 cluster class 1 ela conv conv prob 0 00 0 02 0 39 1 convexity 2 ela conv lin prob 0 01 0 00 0 41 1 convexity 3 ela conv lin dev orig 0 01 0 01 0 11 1 convexity 4 ela conv lin dev abs 0 00 0 01 0 12 1 convexity 5 ela distr skewness 0 02 0 05 0 70 1 y distribution 6 ela distr kurtosis 0 05 0 07 0 79 1 y distribution 7 ela distr number of peaks 0 13 0 08 0 70 2 y distribution 8 ela level mmce lda 10 0 12 0 10 0 66 2 levelset 9 ela level mmce qda 10 0 26 0 12 0 85 3 levelset 10 ela level mmce mda 10 0 12 0 12 0 70 2 levelset 11 ela level lda qda 10 0 27 0 12 0 84 3 levelset 12 ela level lda mda 10 0 05 0 08 0 57 1 levelset 13 ela level qda mda 10 0 27 0 11 0 82 3 levelset 14 ela level mmce lda 25 0 13 0 11 0 76 2 levelset 15 ela level mmce qda 25 0 24 0 13 0 84 3 levelset 16 ela level mmce mda 25 0 12 0 16 0 86 2 levelset 17 ela level lda qda 25 0 25 0 12 0 82 3 levelset 18 ela level lda mda 25 0 08 0 13 0 81 2 levelset 19 ela level qda mda 25 0 25 0 10 0 73 3 levelset 20 ela level mmce lda 50 0 13 0 11 0 80 2 levelset 21 ela level mmce qda 50 0 24 0 10 0 80 3 levelset 22 ela level mmce mda 50 0 12 0 15 0 86 2 levelset 23 ela level lda qda 50 0 28 0 09 0 80 3 levelset 24 ela level lda mda 50 0 08 0 12 0 82 2 levelset 25 ela level qda mda 50 0 24 0 09 0 82 3 levelset 26 ela meta lin simple adj r2 0 01 0 04 0 49 1 metamodel 27 ela meta lin simple intercept 0 00 0 01 0 16 1 metamodel 28 ela meta lin simple coef min 0 08 0 08 0 79 1 metamodel 29 ela meta lin simple coef max 0 08 0 05 0 74 1 metamodel 30 ela meta lin simple coef max by min 0 07 0 07 0 78 1 metamodel 31 ela meta lin w interact adj r2 0 13 0 10 0 62 2 metamodel 32 ela meta quad simple adj r2 0 02 0 05 0 46 1 metamodel 33 ela meta quad simple cond 0 16 0 09 0 83 2 metamodel 34 ela meta quad w interact adj r2 0 19 0 11 0 79 3 metamodel 35 nn ic h max 0 01 0 06 0 42 1 icofs 36 nn ic eps s 0 07 0 08 0 54 1 icofs 37 nn ic eps max 0 03 0 06 0 50 1 icofs 38 nn ic eps ratio 0 06 0 06 0 42 1 icofs 39 nn ic m0 0 03 0 04 0 24 1 icofs 40 rand ic h max 0 03 0 08 0 81 1 icofs 41 rand ic eps s 0 01 0 02 0 19 1 icofs 42 rand ic eps max 0 01 0 01 0 14 1 icofs 43 rand ic eps ratio 0 00 0 03 0 22 1 icofs 44 rand ic m0 0 00 0 01 0 11 1 icofs 45 ela curv grad norm min 0 01 0 12 0 69 4 curvature 50 ela curv grad norm max 0 01 0 12 0 64 4 curvature 52 ela curv grad norm nas 0 00 0 00 na 1 curvature 53 ela curv grad scale min 0 01 0 12 0 68 4 curvature 58 ela curv grad scale max 0 01 0 11 0 64 4 curvature 60 ela curv grad scale nas 0 04 0 00 0 70 1 curvature 61 ela curv hessian cond min 0 01 0 12 0 68 4 curvature 66 ela curv hessian cond max 0 00 0 06 0 41 1 curvature 68 ela curv hessian cond nas 0 01 0 01 0 41 1 curvature 69 ela local n loc opt abs 0 17 0 04 0 70 5 localsearch 70 ela local n loc opt rel 0 06 0 12 0 62 2 localsearch 71 ela local best2mean contr orig 0 14 0 05 0 61 5 localsearch 72 ela local best2mean contr ratio 0 12 0 06 0 30 5 localsearch 73 ela local center dist min 0 15 0 05 0 70 5 localsearch 74 ela local center dist lq 0 07 0 04 0 33 1 localsearch 75 ela local center dist mean 0 08 0 04 0 36 1 localsearch 76 ela local center dist median 0 08 0 04 0 35 1 localsearch 77 ela local center dist uq 0 07 0 04 0 34 1 localsearch 78 ela local center dist max 0 15 0 05 0 73 5 localsearch 79 ela local center dist sd 0 09 0 02 0 30 1 localsearch 80 ela local basin sizes avg best 0 16 0 03 0 73 5 localsearch 81 ela local basin sizes avg non best 0 17 0 03 0 70 5 localsearch 82 ela local basin sizes avg worst 0 18 0 04 0 76 5 localsearch 83 ela local fun evals min 0 10 0 07 0 69 1 localsearch 84 ela local fun evals lq 0 02 0 03 0 35 1 localsearch 85 ela local fun evals mean 0 01 0 00 0 01 1 localsearch 86 ela local fun evals median 0 00 0 02 0 30 1 localsearch 87 ela local fun evals uq 0 00 0 02 0 18 1 localsearch 88 ela local fun evals max 0 02 0 07 0 33 1 localsearch 89 ela local fun evals sd 0 00 0 02 0 11 1 localsearch figure d 1 reject rate plots of metrics with low r 2 figure d 1 appendix e supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2021 105281 
25671,optimisation methods are applied increasingly to environmental problems much research in this area is concerned with the behaviour of optimisation algorithms however the effectiveness of these algorithms is also a function of the features of the problem being solved although a number of metrics have been developed to quantify these features they have not been applied to environmental problems the primary reason for this is that the computational cost associated with the calculation of many of these metrics increases significantly with problem size making them unsuitable for real world problems in this paper 28 fitness landscape metrics that have low dependence on problem size are identified through extensive computational experiments on a range of benchmark functions and testing on a number of environmental modelling problems these metrics can be applied to real world optimisation problems in a computationally efficient manner to better understand their features and determine which optimisation algorithms are most suitable ela exploratory landscape analysis bbob black box optimisation benchmark ann artificial neural network icofs information content of fitness sequences keywords optimisation calibration fitness landscape error function exploratory landscape analysis ela evolutionary algorithms 1 introduction optimisation methods are being used extensively to assist with the identification of the most appropriate solutions for a range of environmental problems maier et al 2014 2019 such as stormwater management liu et al 2016 di matteo et al 2019 wastewater treatment hamed et al 2004 land use management emirhüseyinoğlu and ryan 2020 newland et al 2020 environmental management kasprzyk et al 2013 water energy system design guidici et al 2019 water distribution system design zecchin et al 2006 and irrigation scheduling nguyen et al 2017 sedighkia et al 2021 as well as the development of environmental models including input variable selection grivas and chaloulakou 2006 galelli et al 2014 and model calibration pelletier et al 2006 burton et al 2008 existing research in this field has primarily focused on the development of improved optimisation algorithms such as galaxy wang et al 2020 dream vrugt 2016 borg hadka and reed 2015 particle swarm optimisation chau 2007 nsga ii fu et al 2008 ant colony optimisation emami skardi et al 2015 and policy tree optimisation herman and giuliani 2018 as well as the comparison of the performance of different algorithms on different problems e g tikhamarine et al 2020 piotrowski and napiorkowski 2011 kisi et al 2012 bullinaria and alyahya 2014 wang et al 2020 however in accordance with the no free lunch theorem wolpert and macready 1997 no optimisation algorithm can outperform all others across every single problem consequently there is a need to better understand the features of different optimisation problems so that algorithms that are better suited to particular problem types can be selected maier et al 2014 the features of optimisation problems can be represented geometrically by considering the fitness landscape which depicts the shape of the fitness function otherwise termed objective function for a particular objective with respect to the decision variables e g model error as a function of different values of model parameters for model calibration problems see maier et al 2019 as the aim of the optimisation process is to find the highest or lowest points in this landscape depending on whether the aim is to maximise or minimise the objective function the ease or difficulty with which this can be done is a function of the features of this landscape for example if the landscape is smooth with a single well defined high or low point global optimum this point is relatively easy to find conversely if the landscape is rough with many minima or maxima of similar or equal value local optima the overall best solution global optimum is more difficult to find similarly the presence of flat regions or plateaus in the fitness landscape makes it more difficult to guide the search towards the highest or lowest point in the landscape it should be noted that for multi objective optimisation problems each objective has its own fitness landscape as variations in objective values with changes in decision variable values are likely to be different for different objectives see maier et al 2019 in order to enable a better understanding of the features of optimisation problems to be obtained a number of exploratory landscape analysis ela metrics have been developed mersmann et al 2010 munoz et al 2015a for example such metrics can provide an indication of the global structure of the fitness landscape e g its curvature its degree of multi modality e g the prevalence of local optima or the presence of plateaus mersmann et al 2011 however application of these metrics to environmental optimisation problems has been extremely limited e g gibbs et al 2011 bi et al 2016 instead an empirical brute force approach is often used to determine which algorithm or parameterisation to use on a case study by case study basis maier et al 2014 one potential reason for this is that there are different metrics for different landscape features mersmann et al 2010 malan and engelbrecht 2013 maier et al 2014 munoz et al 2015a as well as different metrics for the same features all with particular biases munoz et al 2015a making it difficult to know which metrics to use however the main reason for the lack of adoption of ela metrics is likely to be related to the computational effort required to calculate them as these metrics are calculated based on samples from the fitness landscape pitzer and affenzeller 2012 the number of samples required to obtain meaningful metric values can increase significantly with the size of the search space munoz et al 2015a when addressing real world environmental optimisation problems which are often characterised by large search spaces this can either lead to computational intractability or the case where the computational effort associated with calculating the metrics is greater than that required as part of the brute force approach of applying different algorithms or algorithm parameterisations to determine which works best consequently to enable ela metrics to be used for developing a better understanding of fitness landscape features and selecting the most appropriate optimisation algorithms for real world problems there is a need to determine i which ela metrics if any have low dependence on problem dimensionality and sample size so that they can be applied to real world problems in a computationally efficient manner and ii what information about the features of the fitness landscape can be ascertained from these metrics in order to address these shortcomings the objectives of this paper are 1 to identify which ela metrics have low dependence on problem dimensionality and sample size for a range of benchmark functions with a wide variety of known fitness landscape properties this indicates which ela metrics can be applied to real world environmental optimisation problems from the perspective of computational tractability it also opens the door to assessing the potential practical value of using ela metrics to assist with determining which optimisation algorithm or settings might be most appropriate from a computational efficiency perspective as the computational effort associated with the calculation of ela statistics should be less than that associated with the brute force approach determining which optimisation algorithm performs best 2 to check whether the ela metrics identified as having low dependence on problem dimensionality and sample size for benchmark functions also have low dependence on these factors for a number of real life environmental modelling problems 3 to map the ela metrics that have low dependence on problem dimensionality and sample size to the fitness landscape features they are designed to provide information on thereby providing a desktop assessment of the potential usefulness of the ela metrics that are suitable to determining the features of real world optimisation problems the remainder of this paper is organised as follows details of the methodology used to achieve the above objectives are given in section 2 followed by the results and discussion in section 3 summary and conclusions are provided in section 4 2 methodology 2 1 overview an overview of the methodology used to achieve the three objectives stated in the introduction is given in fig 1 with further details provided in appendix a as can be seen the first three steps of the identification of ela metrics with low dependence on problem dimensionality and sample size with the aid of benchmark functions objective 1 and checking whether these metrics also have low dependence on problem dimensionality and sample size for real life environmental modelling problems objective 2 are the same the first of these steps includes the sampling of fitness landscapes with different features and dimensionality using a range of samples sizes as these samples are required for the calculation of the different ela metrics in step 2 for objective 1 fitness landscapes with different features are represented by the noiseless bbob suite of 24 benchmark functions hansen et al 2009 as these contain a wide range of known landscape features see section 2 2 1 for details can be scaled to different dimensionalities hansen et al 2009 and have been used in a number of fitness landscape studies mersmann et al 2010 2011 shirakawa and nagao 2014 2016 munoz et al 2015b munoz and smith miles 2017 he et al 2007 twenty replicates are generated for five different dimensionalities 2 5 10 20 30 for each of the 24 benchmark functions resulting in 2 400 24 functions x 5 dimensionalities x 20 replicates fitness landscapes each of these is sampled 30 times with different sampling lengths ranging from 100 to 120 000 resulting in 72 000 2 400 fitness landscapes x 30 sample lengths sets of fitness landscape samples a maximum dimensionality of 30 is selected as this corresponds to the upper end of dimensionalities used in previous studies using these benchmark functions mersmann et al 2011 munoz et al 2015b munoz and smith miles 2017 shirakawa and nagao 2014 2016 kerschke et al 2015 garden and engelbrecht 2014 a maximum sample length of 120 000 is used as this has been found to be sufficient for the convergence of ela metric values in preliminary analyses and is significantly greater than sample sizes used in previous ela studies which are generally on the order of 6 000 or 1000 dimension mersmann et al 2011 munoz et al 2015b munoz and smith miles 2017 shirakawa and nagao 2014 2016 kerschke et al 2015 garden and engelbrecht 2014 for objective 2 the fitness landscapes with different features correspond to those for the calibration training of artificial neural network ann models used to predict a number of environmental variables runoff turbidity salinity see section 2 2 2 for details these environmental modelling problems have been selected as i model calibration is a common environmental optimisation problem maier et al 2019 ii anns have been used extensively for environmental modelling see maier et al 2010 wu et al 2014 cabaneros et al 2019 iii the parametric dimensionality of anns can be changed within a single model framework by increasing the number of hidden nodes maier et al 2010 and iv the fitness landscapes associated with the calibration of ann models have been shown to vary in complexity e g kingston et al 2005 samarasinghe 2010 ten replicates are generated for 11 model structures 0 10 hidden nodes corresponding to problem dimensionalities ranging from 1 to 70 see section 2 2 2 for details for each of the 3 modelling problems considered resulting in 330 3 problems x 11 dimensionalities x 10 replicates fitness landscapes each of these is sampled 23 times with different sampling lengths ranging from 100 to 50 000 resulting in 7 590 330 fitness landscapes x 23 sample lengths sets of fitness landscape samples a maximum sample length of 50 000 is selected as this has been found to be sufficient for convergence of ela metric values in preliminary analyses the second of these three steps involves the calculation of the desired ela metrics for each of the sets of fitness landscape samples generated in the previous step for objective 1 89 ela metrics are considered which constitute the full set of metrics used in previous fitness landscape analysis studies mersmann et al 2011 munoz et al 2015b munoz and smith miles 2017 to maximise the chances of identifying metrics with low dependence on problem dimensionality and sample size that also provide useful information about a range of fitness landscape features for the environmental modelling problems objective 2 only the ela metrics that are found to have low dependency on problem dimensionality and sample size for the benchmark functions see fig 1 step 4 are used in order to check whether the findings from the benchmark functions apply in real life environmental modelling contexts the third and last of these steps involves the calculation of the degree of dependence of the ela metrics considered in the previous step on both problem dimensionality and sample size if a metric has low dependence on both it is likely to be able to be applied to real world environmental optimisation problems if this is not the case the computational effort required to calculate the metric is likely to be too large for practical purposes for the benchmark functions objective 1 dependence is represented by the relationship between sample size problem dimensionality and the reject rate which is the fraction of the 24 test functions for which the hypothesis that a particular sample size gives the true value of a particular fitness landscape metric does not hold based on the wilcoxon rank sum test calculated using 20 replicates see section 2 4 1 for details in this context the true value is taken as the value obtained for the largest number of samples considered i e 120 000 if the reject rate for a particular metric is low and remains so with increasing dimensionality over the 5 dimensionalities considered and sample size over the 30 sample lengths considered calculation of the true value of this metric can be considered to have low dependence on problem dimensionality and sample size making it a suitable candidate for application to real world environmental optimisation problems the wilcoxon rank sum test is used for this purpose as it provides a statistically rigorous approach to testing dependence for a desired confidence level for the environmental modelling problems objective 2 whether the low degree of dependence on problem dimensionality and sample size holds for the selected metrics is checked by calculating the number of samples required for a particular ela metric value to be within 10 of the true value of this metric for different problem dimensionalities averaged over the three case studies as represented by ann models with different numbers of hidden nodes and hence model parameters see section 2 4 2 for details in this context the true value is taken as the value obtained for the largest number of samples considered i e 50 000 if the number of samples required to achieve accurate results is relatively small e g less than 5 000 then the low dependence of the metric under consideration on sample size is confirmed if the number of required samples is small for ann models with different numbers of hidden nodes i e different problem dimensionalities then the low dependence of the metric under consideration on problem dimensionality is also confirmed the fourth step involves the categorisation of the different ela metrics in terms of their relative degree of dependence on problem dimensionality and sample size for the benchmark functions objective 1 this is achieved by grouping the 89 ela metrics considered based on the centroids of the euclidean distances of the slopes of the linear regression relationships with logarithm transformation between reject rate see step 4 and sample size or problem dimensionality using a hierarchical clustering approach nielsen 2016 see section 2 5 for details this provides an indication of whether there are natural groupings of metrics with differing degrees of dependence on problem dimensionality and sample size and potential reasons for this as well as which metrics if any have low dependence on both problem dimensionality and sample size and are hence suitable for application to real world optimisation problems as part of the fifth and final step the landscape features that can be quantified with the fitness landscape metrics that have low dependence on problem dimensionality and sample size identified as part of objective 1 and validated in objective 2 are identified by mapping these metrics to the landscape features they are designed to provide information on via a desktop assessment objective 3 this provides information on the types of landscape features that can be obtained from ela metrics that can be applied to real world problems which can then be used to gain a better understanding of the characteristics of different environmental optimisation problems and which optimisation methods and parameterisations might be most appropriate for these however performing such assessments is beyond the scope of this paper the above analyses are conducted using the university of adelaide s supercomputing facilities which consist of 48 skylake nodes with 80 cpus and 377 gb of memory per node samples from the 24 noiseless bbob benchmark functions are generated using the r package flacco kerschke and trautmann 2016 the r package validann humphrey et al 2017 is used for ann development the r package fastcluster müllner 2013 is used for hierarchical clustering the r codes wilcox test and lm are used for wilcoxon rank sum test and regression analysis respectively and the matlab code plhs sheikholeslami and razavi 2017 is used for sample generation r code for how to calculate the ela metrics for a given set of fitness landscape samples is provided as supplementary material 2 2 sampling of fitness landscapes 2 2 1 benchmark functions as mentioned in section 2 1 the noiseless bbob suite of 24 benchmark functions hansen et al 2009 has been used extensively for a number of fitness landscape studies the functions have been designed specifically to represent fitness landscapes with a wide range of features such as the degree of multi modality global structure variable scaling in different directions i e variable sensitivity degree of similarity in different areas of the search space size of basins of attraction optima and differences in the magnitudes between global and local optima and degrees of flatness of the search space plateaus see table 1 the degree to which these different features are represented in the 24 functions is summarised in table b1 in appendix b 2 2 2 environmental modelling problems as mentioned in section 2 1 three different environmental modelling problems are considered including rainfall runoff modelling in the kentucky river usa the prediction of filtered water turbidity from a range of raw water quality parameters and the added alum dose treatment for surface waters in south australia and the forecasting of salinity in the river murray at murray bridge south australia based on values of upstream salinities and flows these are selected as they represent a diversity of environmental problems that have been used in a number of previous benchmarking studies e g wu et al 2013 humphrey et al 2017 as was the case in these studies the selected ann model architecture is a multi layer perceptron mlp as this type of model architecture has been used widely and successfully in practice and enables fitness landscapes i e the calibration error functions with different dimensionalities in terms of number of model parameters to be generated within the same model structural framework simply by changing the number of hidden nodes maier et al 2010 wu et al 2014 details of the model inputs and outputs as well as the available data are given in table 2 which are identical to those used in previous studies as mentioned in section 2 1 the number of hidden nodes for each ann is varied between 0 and 10 to ensure that fitness landscapes with different features are obtained see table 1 the root mean square error rmse is used as the objective function for model calibration as was the case in previous studies wu et al 2013 humphrey et al 2017 it should be noted that as the aim of this study is to identify the characteristics of the fitness landscapes of different ann models model calibration training is not required only samples generated from the fitness landscapes are needed 2 3 fitness landscape metrics as mentioned in section 2 1 a total of 89 ela metrics are considered these consist of the six low level groups of metrics developed by mersmann et al 2011 including convexity y distribution level set meta model local search and curvature and the information content of fitness sequences icofs metrics as shown in table 3 each of these groups of metrics is designed to provide information on different combinations of fitness landscape features including global structure multimodality separability global to local optima contrast search space homogeneity plateaus variable scaling and basin size homogeneity tables 1 and 3 brief outlines of these metrics are given below more detailed information such as the statistics used to summarise the results of these metrics and the mapping between the metrics and features are summarised in appendix c unfortunately there is no direct correlation between particular metrics and individual fitness landscape features making it difficult to provide a more intuitive understanding of the metrics convexity metrics convexity metrics use the deviation between linear regressed fitness values y and the true fitness value y to analyse the shape of fitness landscapes random pairs of points x i x j are selected from the total sample pool x and a third point is selected from the line between these two at this new point the actual fitness landscape value is compared to the linear interpolation of fitness values from the original points whether the landscape is positively or negatively convex can provide information about the overall shape of the fitness landscape y distribution metrics y distribution metrics use the probability density function pdf of fitness values of samples to provide information on the scaling and distribution of fitness landscapes in terms of the fitness values the pdf is estimated based on the frequency of fitness values identified by selected samples on the search space the distribution shown by the pdf can provide information about how easy it is to identify solutions with better fitness values as if more samples that have good fitness values are shown to have a higher probability to be identified it should be relatively easier to find the globally optimal solution unless the fitness landscape is deceptive in which case the global optimum is not in the vicinity of good local optima making it more difficult to locate e g needle in the haystack problems deb and goldberg 1994 maier et al 2014 level set metrics level set metrics use discriminant analysis to check the complexity of fitness landscapes samples are assigned to high and low quality groups based on their fitness values next different predictive models are used to check whether they can re classify the samples accurately meta model metrics meta model metrics involve the building of regression models based on sampling points and checking how well these fit to the fitness landscapes this is in order to show the similarity between the regression models and the corresponding problem s fitness landscape different regression models including both independent simple and cross term parameters are used to check the separability of a fitness landscape separable fitness landscapes should be more easily fitted to simple models whereas non separable fitness landscapes should be better fitted by the cross term models local search metrics local search metrics use the information provided by a set of local optima obtained by a gradient algorithm using random starting points to assess the properties of the distribution of optimal solutions across a fitness landscape of importance here is estimating the size of the basin of attraction for local optima local optima with short distances between each other are clustered within a common basin the size of these basins and how they are distributed across the fitness landscape are related to the features of the local optima curvature metrics curvature metrics assess the information provided by first order derivatives and hessian matrices of sample points on the fitness landscape this information can help to assess whether each variable has the same influence in guiding the searching process and whether the fitness landscape provides enough information to guide the searching to find good solutions especially for derivative perturbation based algorithms icofs metrics the information content of fitness sequences icofs metrics use a set of samples to construct a sequence of fitness landscape values based on either nearest neighbour in the parameter space or a random ordering these sequences are then used to create an indicator sequence of values from 1 0 1 depending on the comparison of sequential values in the original sequence that is for consecutive sequence values y n and y n 1 a value of 1 is assigned if y n y n 1 0 for y n y n 1 and 1 for y n y n 1 note that a threshold is used for the inequality comparisons a smooth near monotonic fitness landscape e g single modal one would be expected to have a sequence without frequent signal change e g 1 1 1 1 1 or 1 1 1 1 1 a multi model fitness landscape on the other hand would be expected to have a sequence with frequent changes in the signal e g 1 1 1 1 0 1 0 1 finally a flat fitness landscape would have a sequence of zeros due to its only slight difference in fitness values the information contained in the sequence is processed and used to provide characterisations of the level of roughness in the fitness landscape which is highly related to multi modality 2 4 calculation of degree of dependence of metrics on dimensionality and sample size as mentioned in the introduction the objective of this paper is to identify which ela metrics have low dependence on problem dimensionality and sample size so that they can be applied to real world environmental optimisation problems it should be noted that although the primary features of the benchmark functions are known assessing the degree to which the ela metrics under consideration are able to correctly assess these features is beyond the scope of this paper in addition there is no direct one to one correspondence between different metrics and different features as mentioned in section 2 3 2 4 1 benchmark functions as mentioned in section 2 1 the degree of dependence of the benchmark functions on problem dimensionality and sample size is assessed with the aid of the two tailed wilcoxon rank sum test which is a nonparametric test with a null hypothesis that the probability of the selected populations arises from the same underlying distribution in this case the two populations under consideration consist of values of fitness landscape metrics calculated using a given number of samples e g 100 to 120 000 see fig 1 and those calculated using the largest number of 120 000 samples the reasoning behind this is that the metric value at the largest number of samples is taken as the most accurate computation of this metric that is the hypothesis test is given as 1 h 0 e l a i j d k e l a i j d 120 000 h 1 e l a i j d k e l a i j d 120 000 where e l a i j d k and e l a i j d 120 000 represent the ela metric results of the i th metric the j th test function and the dth dimension with k samples and 120 000 samples respectively consequently if the null hypothesis is satisfied for relatively small sample sizes for a particular fitness landscape metric this metric can be considered to have low dependence on sample size in this study a 95 confidence level is used to test the hypotheses to enable the results of the different computational experiments to be compared more easily they are represented in terms of the reject rate symbolized by r which is the percentage of experiments for which the above null hypothesis is rejected and is calculated as follows 2 r i d k j 1 n i p i j d k 0 05 n where r i d k refers to the reject rate of metric i among the 24 test functions for problems with d dimensions and k samples i x is an indicator function which is equal to 1 if the boolean statement x is true and zero otherwise and n is the total number of test functions which is 24 in this study consequently lower values of the reject rate indicate that a metric is more independent of sample size and dimensionality and hence more suited to being applied to real world environmental optimisation problems 2 4 2 environmental modelling problems as mentioned in section 2 1 fitness landscape metrics that are found to have low dependence on both problem dimensionality and sample size i e a low reject rate for the test functions see section 2 4 3 are applied to the real world environmental modelling problems to check if the low dependence of these metrics on sample size and dimensionality holds for the real world environmental optimisation problems considered as also mentioned in section 2 1 this check is achieved by calculating the number of samples required for a particular ela metric value to be within 10 of the true value of this metric for different problem dimensionalities which is considered reasonable for practical purposes the percentage difference is calculated using the normalized difference between the true value of a given ela metric obtained for a sample size of 50 000 see section 2 1 and the corresponding value for a smaller sample size k e g 100 50 000 see fig 1 as follows 3 e r r i h c k m e d i h c k m e d i h c 50 000 m e d i h c 50 000 100 where e r r i h c k is the normalized error which is calculated by using the corresponding medians of metric i case c and number of hidden nodes h corresponding to different problem dimensionalities see table 2 and m e d i h c k is the median of the metric values in the data set as identified by the subscripts the subscripts have the same meaning as in e r r i h c k in addition to metrics that were considered unsuitable based on the computational criteria five ela metrics that were not considered to provide useful information on the real world environmental case studies considered were also excluded these include metrics that apply to linear non continuous and low impact relationships which is not the case for anns as they are highly non linear and continuous consequently values of these metrics are equal to zero for the case studies considered therefore not providing any useful information 2 5 categorisation of fitness landscape metrics as mentioned in section 2 1 using the results from the analysis on the benchmark functions the assessed ela metrics are categorized based on their degree of dependence on problem dimensionality and sample size in order to identify metrics with low dependence on both this is achieved via a two step process the first step involves the quantification of the degree of dependence of metric values on sample size and problem dimensionality this is achieved by developing a regression model that relates the reject rates calculated in eq 4 to sample size and problem dimension as follows 4 r a ln d i m b ln s s c where r represents the reject rate d i m and s s represent dimension and sample size respectively and a b and c represent the slope of dimension and sample size and intercept respectively by way of interpretation for example a large value of a implies that the reject rate is highly influenced by the dimension example r code for performing these calculations is provided as supplementary material this form of the relationship was considered most appropriate based on visual inspection of the plots of reject rate versus sample size and problem dimensionality with a logarithm transformation used to scale the magnitude of the coefficients as shown in appendix d the r 2 values of these relationships generally range between 0 3 and 0 86 indicating the ability to discriminate between relationships of different strengths for a small number 10 relationships r 2 values were less than 0 2 however as shown in appendix d this was for relationships with very low dependence on sample size and problem dimensionality where the fluctuations in the relationship i e noise had a significant impact on the r 2 values however this did not affect the correct quantification of the relative impact of sample size and problem dimensionality on ela metric values which is the primary objective the second step involves hierarchical clustering nielsen 2016 of the values of the slopes for dimensionality i e values of a in eq 6 and sample size i e values of b in eq 6 for different ela metrics based on the centroids of their euclidean distances to identify groups of ela metrics with different degrees of dependence on sample size and problem dimensionality it should be noted that in order for the above results to be meaningful the absolute values of r also need to be checked while low dependence on sample size and problem dimensionality are pre conditions for the application of ela metrics to real world environmental problems metrics belonging to this category only provide useful information if the values of r are consistently low rather than consistently high in this study this check is performed by visual inspection of the plots of r versus sample size and problem dimensionality 3 results and discussion 3 1 categorisation of fitness landscape metrics 3 1 1 cluster location as can be seen in fig 2 the ela metrics considered form five distinct clusters with different degrees of dependence on problem dimensionality and sample size typical relationships between reject rate dimensionality and sample sizes for metrics in these clusters are shown in fig 3 the 39 metrics belonging to cluster 1 have a low dependence on problem dimensionality and sample size fig 2 as evidenced by the flat slopes in the relationships between reject rate and both problem dimensionality and sample size as seen in fig 3 a the fact that the reject rate for metrics belonging to this cluster is very low across the full range of sample sizes and problem dimensionalities investigated fig 3 a suggests that metrics belonging to this cluster are suitable for application to real world environmental optimisation problems in contrast this does not appear to be the case for the metrics belonging to the remaining clusters for example the 23 metrics belonging to clusters 2 and 3 have high dependence on sample size and medium high dependence on problem dimensionality with typical plots of the relationships between reject rate problem dimensionality and sample size for metrics belonging to these clusters shown in fig 3 b to 3 d this is likely to make the application of these metrics to real world environmental optimisation problems computationally intractable the 5 metrics belonging to cluster 4 have low dependence on dimensionality but high dependence on sample size as evidenced by a typical plot of reject rate versus these two factors in fig 3 e this makes metrics belonging to this cluster difficult to apply in practice as large sample sizes are required for even relatively simple problems the 7 metrics belonging to cluster 5 have low dependence on sample size but medium dependence on problem dimensionality see fig 3 f for a typical plot of the relationship of reject rate versus sample size and dimensionality this makes them applicable to relatively simple real world problems but computational tractability is likely to become an issue for higher dimensional problems it should be noted that 15 metrics are excluded from clusters 1 to 5 in fig 2 as their reject rates are very high see fig 3 g and h making them unsuitable for application to real world optimisation problems as discussed in section 2 5 as can be seen in fig 3 g there are some cases where the reject rate is low when the sample size is small but this increases rapidly when the sample size increases to a given level this is because when the sample size is small the values of these metrics are highly variable providing greater opportunities for the median values to be close to the true value however this variability decreases with an increase in sample size reducing the chance that the median values are close to the true value of the metric indicating that the actual reject rates for these metrics are very high and therefore not suitable for application to real world environmental problems cluster dimension impact sample size impact 1 low low 2 median high 3 high high 4 low high 5 median low 3 1 2 cluster composition a summary of the composition of each of the five clusters in fig 3 in terms of metric class is given in table 4 as can be seen cluster 1 contains at least one metric from each class with all of the 4 convexity and 10 icofs metrics belonging to this cluster the majority of the 3 y distribution 66 7 the 9 meta model 66 7 and 21 local search 57 1 metrics also belong to cluster 1 while only one out of the 18 level set 5 6 and 4 out of the 24 curvature 16 7 metrics fall into this cluster a common feature of all metrics belonging to cluster 1 irrespective of which metric class they are part of is that their calculation only requires fitness values and the relative distance between samples without knowledge of the location of each sample in the search space as is the case with many of the other metrics this is a likely cause for the low dependence of the calculation of these metrics on sample size and problem dimensionality the majority of the metrics belonging to clusters 2 and 3 are part of the level set and meta model classes and have relatively high levels of dependence on both sample size and problem dimensionality the likely reason for this is that calculation of metrics in these two classes requires the development of regression models using the available samples consequently the values of the metrics obtained are a function of sample size and dimensionality as the development of representative regression models generally requires a larger number of samples for higher dimensional problems however the degree to which this is the case is a function of the complexity and non linearity of the required regression models for example as the calculation of some of these metrics is based on simple linear regression models some of the metrics belonging to these classes have low dependence on sample size and problem dimensionality and hence belong to cluster 1 table 4 all of the metrics belonging to cluster 4 are part of the curvature metric class table 4 and have a high dependence on sample size but a low dependence on dimensionality this is because the values of the curvature metrics are based on the first order derivative and hessian matrix of each sample point consequently calculation of these metric values is a function of individual samples without considering the spatial dependencies of their relationships and is therefore the likely reason they are not affected significantly by dimensionality in contrast sample size has a significant impact on curvature metric values this is because sample points in different regions of the search space are likely to provide different gradient information resulting in high variability unless the sample size is sufficient this issue is likely to be exacerbated for some curvature metrics metrics related to c g and c h in eq c9 and c12 respectively that rely on information about relative gradients especially in flat regions of the search space as this is likely to result in infinite values consequently these curvature metrics are the ones that result in high reject rates even for low dimensional problems and large sample sizes e g fig 3 g and h and have therefore been excluded from the clusters in fig 2 cluster 5 consists of metrics belonging to the local search class which have low dependence on sample size but high dependence on problem dimensionality this is because these metrics are related to the size of the local basins within the search space which is calculated based on the number of local optima identified in each basin as problem dimensionality increases the number of basins grows dramatically making it virtually impossible to identify more than one local optimum in each basin as a result the values of the metrics become meaningless even for relatively large sample sizes 3 2 validation of categorisation of fitness landscape metrics as mentioned in section 2 4 2 five of the 39 cluster 1 metrics are not suitable for application to real world environmental problems as a result only the 34 remaining metrics are validated using the real world environmental modelling problems to summarise these results fig 4 shows the number of samples required for each of these metrics to achieve convergence for the anns with different numbers of hidden nodes as discussed in section 2 4 2 convergence was taken as the number of samples required for metric values to be within 10 of the true value obtained for the maximum number of samples considered as can be seen from figs 4 and 28 of these 34 metrics 82 4 converge within 2 000 samples which is typically within 4 of the number of samples used to generate the true metric values i e 50 000 for most metrics see figure a1 for the vast majority of these metrics 22 converge occurs within 500 samples which is typically within 1 of the number of samples used to generate the true metric values the results in fig 4 clearly illustrate that there is no increase in the number of samples required for convergence with an increase in problem dimensionality i e the number of hidden nodes consequently these 28 metrics can be considered to have low dependence on sample size and problem dimensionality from a practical perspective indicating that they are likely to provide a computationally efficient means for better understanding the fitness landscapes of a range of complex highly dimensional real world environmental optimisation problems it should be noted that for some of the metrics there is a slight increase in the number of samples required for convergence for lower problem dimensionalities i e smaller number of hidden nodes however these variations are very small i e on the order of hundreds of samples compared with the 50 000 samples used to obtain the true metric values the six metrics that showed low dependence on sample size and problem dimensionality for the benchmark problems but not for the real world environmental problems include four metrics belonging to the meta model class one belonging to the icofs class and one belonging to the y distribution class these four meta model metrics all utilise single regression models that do not consider interactions between parameters as interactions between parameters are likely to be a feature of real world environmental modelling problems the single regression models used in the four metrics in question are unlikely to represent the fitness landscapes of the environmental modelling problems considered consequently these metric values are likely to become non informative requiring a larger number of samples for accurate calculation in contrast the lack of rapid convergence of the y distribution i e kurtosis and icofs i e ε s metrics that appear to not be suitable for real world environmental modelling problems is likely to be related to the scaling of fitness values for the problems considered suggesting that the scaling of fitness values is more difficult to recognise by using samples from real world problems than test functions it is likely that different samples provide fitness values with different scaling making these two metric values unstable for environmental modelling problems 3 3 interpretation of metrics with low dependence on sample size and dimensionality based on the results presented in sections 3 1 and 3 2 there are 28 ela metrics that appear to be suitable for application to real world environmental optimisation problems as they have been shown to have low dependence on sample size and problem dimensionality for a wide range of benchmark and real world problems however in addition to their computational tractability the usefulness of these ela metrics is also a function of the type of information they can provide about the different features of the fitness landscapes of environmental optimisation problems as shown in table 5 the 28 suitable metrics cover six metric classes excluding curvature metrics and provide information on six of the eight major fitness landscape features this provides the opportunity to obtain a better understanding of different of attributes of a range of environmental optimisation problems in a computationally efficient manner for example application of metrics such as nn rand ic eps max and nn rand ic eps ratio can provide information on the potential identifiability of environmental models shin et al 2013 bastidas et al 2006 that can complement information provided by more commonly used sensitivity analysis approaches e g razavi and gupta 2015 guillaume et al 2019 razavi et al 2021 specifically information about the magnitude of multimodality and plateaus of fitness landscapes can provide information on the size of regions with non unique parameters and information about search space homogeneity can provide insight into the distribution and location of these regions alternatively ela metrics can provide insight into which optimisation algorithm or optimisation algorithm parameterisation are most appropriate for a given problem maier et al 2014 gibbs et al 2011 2015 for example nn rand ic h max ela local center dist mean and ela local fun evals median can be used to obtain information about the degree of multimodality distribution of optima regions and overall depth of optima region of the fitness landscape in a computationally efficient manner prior to the optimisation process if the fitness landscape is found to have low multimodality and the optima regions are shallow and converged to a small area on the fitness landscape use of a gradient based optimisation approach might be most appropriate maier et al 2019 in contrast if the fitness landscape is found to have high multimodality with deep and widely distributed optima regions use of global search evolutionary algorithms might be preferred maier et al 2019 the degree of multimodality and the presence of plateaus is also able to inform which values of the parameters that control the searching behaviour of evolutionary algorithms are most appropriate see munoz and smith miles 2017 wang et al 2020 zecchin et al 2005 2012 and the degree of homogeneity of the search space is able to assist with determining whether there is value is adapting the values of the parameters that control the searching behaviour of evolutionary algorithms e g zheng et al 2017 4 summary and conclusions optimisation algorithms are used extensively for the development of environmental models and the identification of solutions to environmental problems how well a particular algorithm performs on a given problem is a function of both algorithm behaviour and the characteristics of the problem being solved as represented by the fitness landscape while significant attention has been given to the development of algorithms with different behaviours little effort has been devoted to better understanding problem characteristics generally resulting in a brute force approach to identifying algorithms and parameterisations that perform acceptably for a particular problem this is despite the fact that a number of metrics have been developed to assist with identifying features of fitness landscapes such as their global structure their degree of multimodality and the presence of plateaus the identification of which would assist in the selection of appropriate optimisation algorithms and parameterisations without the need for a brute force approach the primary reason for the lack of adoption of fitness landscape metrics in practice is that the calculation of these metrics is based on samples from the fitness landscape which can be computationally expensive for real world environmental problems as they are often based on complex and highly dimensional simulation models in order to test whether this is the case the degree of dependence on problem dimensionality and sample size of 89 fitness landscape metrics was assessed each metric was calculated for 72 000 different sets of fitness landscape samples obtained from 2 400 fitness landscapes derived from commonly used benchmark functions and their degree of dependence on problem dimensionality and sample size was assessed results show that 39 of the 89 metrics have low dependence on dimensionality and sample size 34 of which are considered suitable for application to environmental problems the low degree of dependence on problem dimensionality and sample size of these 34 metrics was tested on a number of real world environmental modelling problems corresponding to 7 590 sets of fitness landscape samples from 390 fitness landscapes results indicate that 28 of the 34 aforementioned fitness landscape metrics also have low dependence on problem dimensionality and sample size for the real world environmental modelling problems often requiring fewer than 500 fitness landscape samples for convergence these 28 metrics cover a wide range of fitness landscape features including their global structure multimodality separability search space and basin size homogeneity and the presence of plateaus a limitation of this study is that although ela metrics that have low dependence on problem dimensionality and sample size were identified using a large number of test functions with different dimensionalities and a wide variety of fitness landscape features these mathematical functions are unlikely to represent all of the complexities and features of fitness landscapes associated with real world optimisation problems however the fact that the majority of the ela metrics that were found to have low dependence on sample size and problem dimensionality for the test functions also had low dependence on sample size and problem dimensionality for the real world problems considered provides confidence in the generality of the findings presented another limitation of this study is that the ela metrics can only be calculated for continuous optimisation problems which excludes certain types of problems encountered in practice such as the optimisation of water distribution systems using discrete pipe sizes e g zheng et al 2017 wang et al 2020 and the optimisation of best practice stormwater management options e g di matteo et al 2019 the findings that there are 28 fitness landscape metrics that are able to provide insight on a range of fitness landscape characteristics that appear to be suitable for application to real world environmental optimisation problems opens the door to gaining greater insights and improving the efficiency of a range of environmental optimisation problems for example these metrics can provide insight into the potential identifiability of the parameters of different environmental models as well as information on the suitability of different optimisation algorithms and parameterisations for particular environmental optimisation problems consequently future research efforts should focus on testing the applicability of the identified metrics to a wide range of real world optimisation problems in order to better understand the features of their fitness landscapes and to check whether the features of these landscapes identified with the aid of the ela metrics align with those identified in previous studies providing further confidence in the usefulness of the metrics in addition there would be value in applying the metrics to the fitness landscapes of the individual objective functions for multi and many objective optimisation problems and to better understand the extent to which knowledge of the features of fitness landscapes can inform the selection of appropriate optimisation algorithms and their parameterisations declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments the first author is supported by an adelaide graduate research scholarship which is gratefully acknowledged the authors would also like to thank joseph guillaume and the anonymous reviewer of this paper whose comments have improved the quality of this paper significantly appendix e supplementary data the following are the supplementary data to this article multimedia component 1 multimedia component 1 multimedia component 2 multimedia component 2 multimedia component 3 multimedia component 3 multimedia component 4 multimedia component 4 appendix a detailed outline of methodology figure a 1 detailed outline of methodology figure a 1 appendix b details of benchmark functions table b 1 detailed features of benchmark functions adapted from mersmann et al 2010 table b 1 function multi modality global structure separability variable scaling space homogeneity basin homogeneity global to local contrast 1 none none high none high none none 2 none none high high high none none 3 high strong none low high low low 4 high strong high low high med low 5 none none high none high none none 6 none none high low med none none 7 none none high low high none none 8 low none none none med low low 9 low none none none med low low 10 none none none high high none none 11 none none none high high none none 12 none none none high high none none 13 none none none low med none none 14 none none none low med none none 15 high strong none low high low low 16 high med none med high med low 17 high med none low med med high 18 high med none high med med high 19 high strong none none high low low 20 med weak none none high low low 21 med none none med high med low 22 low none none med high med med 23 high none none none high low low 24 high weak none low high low low as plateaus is not specified in bbob suite it is not included in this table appendix c details of ela metrics a summary of the ela metrics used is given in table c1 and details on how different groups of metrics are calculated and how they assist with the characterisation of different fitness landscape features are given below table c 1 summary of ela metrics used table c 1 no metric cluster class 1 ela conv conv prob 1 convexity 2 ela conv lin prob 1 convexity 3 ela conv lin dev orig 1 convexity 4 ela conv lin dev abs 1 convexity 5 ela distr skewness 1 y distribution 6 ela distr kurtosis 1 y distribution 7 ela distr number of peaks 2 y distribution 8 ela level mmce lda 10 2 level set 9 ela level mmce qda 10 3 level set 10 ela level mmce mda 10 2 level set 11 ela level lda qda 10 3 level set 12 ela level lda mda 10 1 level set 13 ela level qda mda 10 3 level set 14 ela level mmce lda 25 2 level set 15 ela level mmce qda 25 3 level set 16 ela level mmce mda 25 2 level set 17 ela level lda qda 25 3 level set 18 ela level lda mda 25 2 level set 19 ela level qda mda 25 3 level set 20 ela level mmce lda 50 2 level set 21 ela level mmce qda 50 3 level set 22 ela level mmce mda 50 2 level set 23 ela level lda qda 50 3 level set 24 ela level lda mda 50 2 level set 25 ela level qda mda 50 3 level set 26 ela meta lin simple adj r2 1 meta model 27 ela meta lin simple intercept 1 meta model 28 ela meta lin simple coef min 1 meta model 29 ela meta lin simple coef max 1 meta model 30 ela meta lin simple coef max by min 1 meta model 31 ela meta lin w interact adj r2 2 meta model 32 ela meta quad simple adj r2 1 meta model 33 ela meta quad simple cond 2 meta model 34 ela meta quad w interact adj r2 3 meta model 35 nn ic h max 1 icofs 36 nn ic eps s 1 icofs 37 nn ic eps max 1 icofs 38 nn ic eps ratio 1 icofs 39 nn ic m0 1 icofs 40 rand ic h max 1 icofs 41 rand ic eps s 1 icofs 42 rand ic eps max 1 icofs 43 rand ic eps ratio 1 icofs 44 rand ic m0 1 icofs 45 ela curv grad norm min 4 curvature 46 ela curv grad norm lq uc curvature 47 ela curv grad norm mean uc curvature 48 ela curv grad norm med uc curvature 49 ela curv grad norm uq uc curvature 50 ela curv grad norm max 4 curvature 51 ela curv grad norm sd uc curvature 52 ela curv grad norm nas 1 curvature 53 ela curv grad scale min 4 curvature 54 ela curv grad scale lq uc curvature 55 ela curv grad scale mean uc curvature 56 ela curv grad scale med uc curvature 57 ela curv grad scale uq uc curvature 58 ela curv grad scale max 4 curvature 59 ela curv grad scale sd uc curvature 60 ela curv grad scale nas 1 curvature 61 ela curv hessian cond min 4 curvature 62 ela curv hessian cond lq uc curvature 63 ela curv hessian cond mean uc curvature 64 ela curv hessian cond med uc curvature 65 ela curv hessian cond muq uc curvature 66 ela curv hessian cond max 1 curvature 67 ela curv hessian cond sd uc curvature 68 ela curv hessian cond nas 1 curvature 69 ela local n loc opt abs 5 local search 70 ela local n loc opt rel 2 local search 71 ela local best2mean contr orig 5 local search 72 ela local best2mean contr ratio 5 local search 73 ela local center dist min 5 local search 74 ela local center dist lq 1 local search 75 ela local center dist mean 1 local search 76 ela local center dist median 1 local search 77 ela local center dist uq 1 local search 78 ela local center dist max 5 local search 79 ela local center dist sd 1 local search 80 ela local basin sizes avg best 5 local search 81 ela local basin sizes avg non best 5 local search 82 ela local basin sizes avg worst 5 local search 83 ela local fun evals min 1 local search 84 ela local fun evals lq 1 local search 85 ela local fun evals mean 1 local search 86 ela local fun evals median 1 local search 87 ela local fun evals uq 1 local search 88 ela local fun evals max 1 local search 89 ela local fun evals sd 1 local search uc in cluster column represents metrics not classified in this study c1 convexity metrics as can be seen in table 3 convexity metrics are able to provide information on a number of fitness landscape features including global structure multimodality and search space homogeneity their calculation requires implementation of the following general steps i select random pairs of points x i x j from the total number of samples of the fitness landscape considered i e 100 to 120 000 samples see fig 1 to ensure most of the samples are included in the calculation we use n random pairs of points in this study where n is the number of initial samples ii calculate a linear combination of x i x j to select a new point x n between the two points where c1 x n w x i 1 w x j where w is a random number between 0 and 1 calculate the fitness value y n of x n based on the corresponding test function iii calculate the fitness values y i y j of x i x j based on the corresponding test function use linear regression to calculate the approximated linear fitness value y n at x n c2 y n w y i 1 w y j where w is the same w as in ii iv calculate the difference δ between y n and y n by δ y n y n a if δ is negative the landscape between the selected two points is convex providing good gradient information to guide the search in this region of the fitness landscape b if δ is positive the landscape between the selected two points is not convex providing poor gradient information to guide the search in this region of the fitness landscape v in total 4 convexity metrics are considered which differ in terms of statistics methods to summarise the results of δ obtained from n pairs of samples they are the probability of convexity ela conv conv prob which relates to the probability of negative δ probability of linearity ela conv lin prob which relates to the probability of δ 0 mean original deviation ela conv lin dev orig which relates to the mean value of δ from n pairs of samples mean absolute deviation ela conv lin dev abs which relates to the mean value of δ from n pairs of samples convexity metrics are able to provide information on the global structure of fitness landscapes as they present information about the general shape of fitness landscapes and can therefore provide information on whether fitness landscapes have a clear structure to guide searching or not they are also able to provide information on search space homogeneity and multimodality as they take the probability of convexity into account a high or low convexity rate indicates fitness landscapes maintain the same trend and shape in most areas of the fitness landscape which is representative of greater homogeneity and reduced multimodality in contrast middle range values of the convexity rate indicate that fitness landscapes have different trends and shapes in different areas increasing changes of inhomogeneity and multi modality b2 y distribution metrics as can be seen in table 3 y distribution metrics are able to provide information on a number of fitness landscape features including global structure multimodality and search space homogeneity their calculation requires implementation of the general following steps i generate the pdf of the fitness values y s of samples x s ii in total 3 y distribution metrics were considered which relates to the properties of the pdf of y s including skewness ela distr skewness kurtosis ela distr kurtosis and the number of peaks ela distr number of peaks y distribution metrics are able to provide information on the global structure of fitness landscapes for example if the skewness of the pdf is negative most of the obtained y are small indicating that the bottom region of a fitness landscape is bigger than the top region referring to a bigger bowl bottom than for a fitness landscapes with a positive skewness they are also able to provide information on multimodality as a multi modal fitness landscapes are likely to have several peaks in their pdfs which refers to different bottom regions of the fitness landscape additionally y distribution metrics are able to provide information on the prevalence of plateaus within the landscape as plateau like landscapes contain region s with the same fitness values as a result they would tend to have high kurtosis values which indicates that most of the fitness values have no significant difference c3 level set metrics as can be seen in table 3 level set metrics are able to provide information concerning a number of fitness landscape features including global structure multimodality and plateaus their calculation requires implementation of the following steps i split all the samples to high quality and low quality ones based on a given quantile threshold of their fitness values y s in this study 10 25 and 50 quantiles are used as thresholds ii linear lda quadratic qda and mixture mda discriminant analysis are used to predict whether the fitness values y s are high or low quality the number of y s which are classified to a wrong quality group are recorded iii calculate the mean misclassification error mmce which refers to the probability of misclassification of y s by using corresponding discriminant analysis methods iv in total 18 level set metrics are considered which differ in terms of quantile thresholds and discriminant analysis methods i e ela level mmce lda qda mda 10 25 50 the quotient of mmce of lda divided by mmce of qda ela level lda qda 10 25 50 the quotient of mmce of lda divided by mmce of mda ela level lda mda 10 25 50 and the quotient of mmce of qda divided by mmce of mda ela level qda mda 10 25 50 are also included in the metrics as they can show the differences of mmce between simple models lda and qda and complex models mda level set metrics are able to provide information on global structure and multimodality as through the mmce of different discriminant analysis the distribution of fitness values can be determined for example if mmces of lda and qda are low and mmces of mda are high fitness values on fitness landscapes can be easily classified by these two simple models indicating high quality values and low quality values are not located in the same region so that high quality values can be easily identified as finding these values will not be interrupted by low quality values in this case as differences between fitness values are not big in the small region the landscapes should not contain multiple optima in contrast if mmces of lda and qda are high the global structure of a fitness landscape is likely to be complex as this indicates that there are no clear top and bottom regions of the fitness landscapes but very frequent variation in fitness values this can also result in a high level of multimodality of fitness landscapes furthermore if mmces of mda are also high the structure of fitness landscapes can be quite complex and multi modal level set metrics are also able to provide information on plateaus as plateau like landscapes have many similar fitness values the threshold of high and low quality values is not clear for such problems as a result plateau like landscapes are more likely to have high mmces for all discriminant analysis methods c4 meta model metrics as can be seen in table 3 meta model metrics are able to provide information on a number of fitness landscape features including global structure multimodality plateaus and variable scaling their calculation requires implementation of the general following steps i build the corresponding regression models between samples x s and corresponding fitness values y s four regression models are built in this study which are simple linear regression c3 y i 1 n a i v i where a i is the coefficient of corresponding variable v i interacted linear regression c4 y i 1 n a i v i i 1 n j 1 n b k v i v j where b k is the coefficient of corresponding variable v i v j simple quadratic regression c5 y i 1 n a i v i i 1 n c i v i 2 where c i is the coefficient of corresponding variable v i 2 which is the square of v i interacted quadratic regression c6 y i 1 n a i v i i 1 n c i v i 2 i 1 n j 1 n b k v i v j i 1 n j 1 n t k v i v j 2 i 1 n j 1 n l k v i 2 v j 2 where t k and l k are the coefficient of corresponding variable v i v j 2 and v i 2 v j 2 respectively ii in total 9 meta model metrics are considered which are related to adjusted coefficients of determination r2 ela meta lin quad simple w interact adj r2 of four regression models maximum minimum and intercept coefficients of simple linear regression models ela meta lin simple coef max coef min intercept and the quotient between maximum and minimum coefficients ela meta lin quad simple cond of simple linear and simple quadratic regression models meta model metrics are able to provide information on the global structure and multimodality of fitness landscapes as adjusted r2 can show how well global structure matches the corresponding models and a goodness of fit shown by r2 also indicates models with a low level of multimodality as all these regression models are not multi modal they are also able to provide information on separability as separate fitness landscapes are likely to have higher adjusted r2 as they are likely to be represented more easily by simpler models additionally meta model metrics are able to provide information on variable scaling as shown by the maximum and minimum of coefficients of the models low degree variable scaling fitness landscapes should have models with maximum and minimum coefficients the values of which are close to each other indicating that all variables make similar contributions to the fitness values on the other hand high degree variable scaling fitness landscapes should have models with significant different maximum and minimum coefficients indicating that the contributions of different variables to the fitness values are not the same c5 local search metrics as can be seen in table 3 local search metrics are able to provide information on a number of fitness landscape features including multimodality global to local optima contrast basin size homogeneity and search space homogeneity their calculation requires implementation of the general following steps i use a gradient algorithm to find local optima starting from initial samples x s in this study the l bfgs b algorithm zhu et al 1997 was used due to its capacity to setup the range of calculation to avoid the identified local optima being beyond the range of the fitness landscape ii use of hierarchical clustering to cluster identified local optima in i local optima within a given euclidean distance e are included in the same cluster which refers to a corresponding local basin in this study e is 5 of total euclidean distance length of the whole fitness landscape as this distance performs well in distinguishing different clusters without resulting in a computational burden that results in intractability if e is too small a larger number of clusters is likely to be generated by hierarchical clustering which increases complexity and the computational requirements of subsequent calculations iii calculate the centroid x c of each local basin identified in ii based on the local optima in the corresponding basin calculate the fitness value y c of all centroids iv in total 21 local search metrics are considered which relates to the number of identified local basins ela local n loc opt abs rep fitness value differences between high quality basins global optima and low quality basins local optima ela local best2mean contr orig ratio basin size difference between high average and low quality basins i e difference between number of optima in high and low quality basins ela local basin sizes avg best avg non best avg worst statistics of basin centroids euclidean distances and statistics of number of evaluated functions to find optima from initial samples i e minimum maximum lower quantile median mean upper quantile and standard deviation of basin centroids euclidean distances and number of evaluated functions ela local center dist fun evals min max lq median mean uq sd local search metrics are able to provide information on multimodality as the level of multimodality is highly related to the identified number of optima by using a gradient algorithm they are also able to provide information on global to local optima contrast as fitness landscapes with a high degree of global and local contrast are likely to have significantly different fitness values between global and local basins optima and vice versa additionally local search metrics are able to provide information on basin size homogeneity as they can present the size difference between high and low quality basins in order to check whether basins have the same quality on the other hand the evaluated number of functions can also show the depth of different basins finally they are able to provide information on search space homogeneity as they are able to show the distribution of centroids on fitness landscapes the distance between centroids can indicate whether basins are converged to a small region or widely distributed on the whole fitness landscape referring to whether different regions in search space have the same feature c6 curvature metrics as can be seen in table 3 curvature metrics are able to provide information on a number of fitness landscape features including plateaus and variable scaling their calculation requires implementation of the general following steps i calculate the gradient information c7 f x i d f x i d x where f x is the fitness function and f x i is the first order derivative of f at the variable x i based on f x in all directions the total gradient length l of a sample is calculated as c8 l i 1 n f x i 2 where n is the number of variables dimensions of the fitness function the gradient condition c g of a sample is calculated as c9 c g max f x min f x ii calculate the hessian matrix c10 h x 2 f x x i x j so the eigenvalues λ of h x can be calculated from c11 h x λ i n 0 where i n is the identity matrix with size n the hessian condition c h of a sample is calculated as c12 c h max λ min λ iii in total 24 curvature metrics are considered which differ in terms of 8 statistics of l c g and c h of all samples which are the minimum maximum lower quantile median mean upper quantile standard deviation and proportion of samples with no l c g and c h i e ela curv grad norm grad scale hessian cond min max lq med mean uq sd nas curvature metrics are able to provide information on the plateaus of fitness landscapes as they refer to the gradient information of fitness landscapes plateau like fitness landscapes contain limited gradient information resulting in small l of all samples in general they are also able to provide information on variable scaling as c g and c h can show the differences of contribution between variables to the fitness values large c g and c h values generally indicate that there are variables which have very small contributions to the fitness values prviding little guidance to the search algorithm c7 icofs metrics as can be seen in table 3 icofs metrics are able to provide information on a number of fitness landscape features including global structure multimodality and plateaus their calculation requires implementation of the following general steps i firstly sort all samples into a sequence in this study two sampling ordering methods are used to generate different icofs metrics 1 nearest neighbouring nn as part of which the following sample x i 1 of one sample x i is the closest sample to the corresponding by euclidean distance 2 random rand order as part of which the following sample x i 1 of one sample x i is randomly selected from the entire set of samples ii build a symbol sequence ε by using the following rule c13 i 1 i f y i 1 y i ε 0 i f y i 1 y i 1 i f y i 1 y i ε ε where ε 0 is the accuracy parameter of the symbol sequence and y i is the fitness value of sample x i it can be seen that ε is controlled by the value of ε if ε is small ε can be quite sensitive and contain frequent symbol changes in the sequence for example sequence 1 1 1 1 1 if ε is big on the other hand ε can be insensitive and contain many 0 values in the sequence for example sequence 0 0 0 0 0 iii calculate the information content h ε of the sequence based on the definition c14 h ε a b p a b l o g 6 p a b where a b 1 0 1 and p a b is the probability that two neighboured symbols a b are different iv build a new sequence ε by removing all 0 values in ε and calculate the partial information content m ε which is defined as c15 m ε n 1 where n is the length of sequence ε v in total 10 icofs metrics are considered and both of two sample orders contain 5 metrics the typical result curves of h ε and m ε against ε are shown in figure c1 munoz et al 2015b provides 5 metrics to summarise the curves which are c16 h m a x max h ε c17 ε m a x l o g 10 ε w h e r e h ε h m a x c18 ε s l o g 10 min ε w h e r e h ε 0 05 c19 m 0 m ε 0 c20 ε r a t i o l o g 10 max ε w h e r e m ε 0 5 m 0 the detailed dot plots of the 5 metrics are also shown in figure c1 the total 10 metrics are shown as format nn rand ic h max eps max eps s m0 eps ratio figure c 1 typical results of icofs figure c 1 icofs metrics are able to provide information on multimodality as the symbols in the sequence represent information about the smoothness of fitness landscapes rough landscapes are likely to have high values of h m a x and m 0 and if landscapes are rough they have the potential to have a high degree of multimodality additionally they are able to provide information on plateaus and global structure as plateau like landscapes should contain many 0s in their symbol sequence even when ε is very small this is likely to return a small ε s ε m a x and ε r a t i o for a plateau like landscape in contrast fitness landscapes with good global structure should have a level of scaling in terms of fitness values as a result small ε s ε m a x and ε r a t i o should be bigger than those for flat fitness landscapes appendix d slopes and coefficients of determination r 2 of benchmark test for all tested metrics table d 1 slope of dimension sample size and r 2 of plane regression for all metrics in different classes and clusters unclassified metrics are not included table d 1 metric number metric slope of dim slope of sample size plane r 2 cluster class 1 ela conv conv prob 0 00 0 02 0 39 1 convexity 2 ela conv lin prob 0 01 0 00 0 41 1 convexity 3 ela conv lin dev orig 0 01 0 01 0 11 1 convexity 4 ela conv lin dev abs 0 00 0 01 0 12 1 convexity 5 ela distr skewness 0 02 0 05 0 70 1 y distribution 6 ela distr kurtosis 0 05 0 07 0 79 1 y distribution 7 ela distr number of peaks 0 13 0 08 0 70 2 y distribution 8 ela level mmce lda 10 0 12 0 10 0 66 2 levelset 9 ela level mmce qda 10 0 26 0 12 0 85 3 levelset 10 ela level mmce mda 10 0 12 0 12 0 70 2 levelset 11 ela level lda qda 10 0 27 0 12 0 84 3 levelset 12 ela level lda mda 10 0 05 0 08 0 57 1 levelset 13 ela level qda mda 10 0 27 0 11 0 82 3 levelset 14 ela level mmce lda 25 0 13 0 11 0 76 2 levelset 15 ela level mmce qda 25 0 24 0 13 0 84 3 levelset 16 ela level mmce mda 25 0 12 0 16 0 86 2 levelset 17 ela level lda qda 25 0 25 0 12 0 82 3 levelset 18 ela level lda mda 25 0 08 0 13 0 81 2 levelset 19 ela level qda mda 25 0 25 0 10 0 73 3 levelset 20 ela level mmce lda 50 0 13 0 11 0 80 2 levelset 21 ela level mmce qda 50 0 24 0 10 0 80 3 levelset 22 ela level mmce mda 50 0 12 0 15 0 86 2 levelset 23 ela level lda qda 50 0 28 0 09 0 80 3 levelset 24 ela level lda mda 50 0 08 0 12 0 82 2 levelset 25 ela level qda mda 50 0 24 0 09 0 82 3 levelset 26 ela meta lin simple adj r2 0 01 0 04 0 49 1 metamodel 27 ela meta lin simple intercept 0 00 0 01 0 16 1 metamodel 28 ela meta lin simple coef min 0 08 0 08 0 79 1 metamodel 29 ela meta lin simple coef max 0 08 0 05 0 74 1 metamodel 30 ela meta lin simple coef max by min 0 07 0 07 0 78 1 metamodel 31 ela meta lin w interact adj r2 0 13 0 10 0 62 2 metamodel 32 ela meta quad simple adj r2 0 02 0 05 0 46 1 metamodel 33 ela meta quad simple cond 0 16 0 09 0 83 2 metamodel 34 ela meta quad w interact adj r2 0 19 0 11 0 79 3 metamodel 35 nn ic h max 0 01 0 06 0 42 1 icofs 36 nn ic eps s 0 07 0 08 0 54 1 icofs 37 nn ic eps max 0 03 0 06 0 50 1 icofs 38 nn ic eps ratio 0 06 0 06 0 42 1 icofs 39 nn ic m0 0 03 0 04 0 24 1 icofs 40 rand ic h max 0 03 0 08 0 81 1 icofs 41 rand ic eps s 0 01 0 02 0 19 1 icofs 42 rand ic eps max 0 01 0 01 0 14 1 icofs 43 rand ic eps ratio 0 00 0 03 0 22 1 icofs 44 rand ic m0 0 00 0 01 0 11 1 icofs 45 ela curv grad norm min 0 01 0 12 0 69 4 curvature 50 ela curv grad norm max 0 01 0 12 0 64 4 curvature 52 ela curv grad norm nas 0 00 0 00 na 1 curvature 53 ela curv grad scale min 0 01 0 12 0 68 4 curvature 58 ela curv grad scale max 0 01 0 11 0 64 4 curvature 60 ela curv grad scale nas 0 04 0 00 0 70 1 curvature 61 ela curv hessian cond min 0 01 0 12 0 68 4 curvature 66 ela curv hessian cond max 0 00 0 06 0 41 1 curvature 68 ela curv hessian cond nas 0 01 0 01 0 41 1 curvature 69 ela local n loc opt abs 0 17 0 04 0 70 5 localsearch 70 ela local n loc opt rel 0 06 0 12 0 62 2 localsearch 71 ela local best2mean contr orig 0 14 0 05 0 61 5 localsearch 72 ela local best2mean contr ratio 0 12 0 06 0 30 5 localsearch 73 ela local center dist min 0 15 0 05 0 70 5 localsearch 74 ela local center dist lq 0 07 0 04 0 33 1 localsearch 75 ela local center dist mean 0 08 0 04 0 36 1 localsearch 76 ela local center dist median 0 08 0 04 0 35 1 localsearch 77 ela local center dist uq 0 07 0 04 0 34 1 localsearch 78 ela local center dist max 0 15 0 05 0 73 5 localsearch 79 ela local center dist sd 0 09 0 02 0 30 1 localsearch 80 ela local basin sizes avg best 0 16 0 03 0 73 5 localsearch 81 ela local basin sizes avg non best 0 17 0 03 0 70 5 localsearch 82 ela local basin sizes avg worst 0 18 0 04 0 76 5 localsearch 83 ela local fun evals min 0 10 0 07 0 69 1 localsearch 84 ela local fun evals lq 0 02 0 03 0 35 1 localsearch 85 ela local fun evals mean 0 01 0 00 0 01 1 localsearch 86 ela local fun evals median 0 00 0 02 0 30 1 localsearch 87 ela local fun evals uq 0 00 0 02 0 18 1 localsearch 88 ela local fun evals max 0 02 0 07 0 33 1 localsearch 89 ela local fun evals sd 0 00 0 02 0 11 1 localsearch figure d 1 reject rate plots of metrics with low r 2 figure d 1 appendix e supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2021 105281 
25672,with the advancement in scientific understanding and computing technologies fire practitioners have started relying on operational fire simulation tools to make better informed decisions during wildfire emergencies this increased use has created an opportunity to employ an emerging data driven approach for wildfire risk estimation as an alternative to running computationally expensive simulations in an investigative attempt we propose a probability based risk metric that gives a series of probability values for fires starting at any possible start location under any given weather condition falling into different categories we investigate the validity of the proposed approach by applying it to use cases in tasmania australia results show that the proposed risk metric can be a convenient and accurate method of estimating imminent risk during operational wildfire management additionally the knowledge base of our proposed risk metric based on a data driven approach can be constantly updated to improve its accuracy keywords wildfire risk management data driven approach risk metric wildfire simulations spark risk reduction software data availability the fire propagation tool spark is available at https research csiro au spark the fire simulation data set based on which the inference model has been built can be found at https data csiro au collections collection cicsiro 49133v1 ditrue ujjwal et al 2021a 1 introduction thanks to the recent advancements in observational science and other technologies such as engineering and computing natural disasters such as wildfires can be studied in greater detail with more complex models razavi et al burn kaizer et al 2015 for wildfire management the risk associated with wildfires can be quantified by running several fire simulations collectively referred to as an ensemble in a geographical location and performing statistical analyses thereafter ujjwal et al 2020a 2021b c despite the computationally complex nature of ensemble predictions their extensive use in wildfire management has become possible due to the rise of computing technologies such as cloud computing ujjwal et al 2019 2020b consequently fire authorities have started relying on operational fire simulations tools such as spark miller et al 2015 phoenix tolhurst et al 2008 farsite finney 2004 and prometheus tymstra et al armitageet al to make better informed decision at various stages of wildfire emergencies with massive data being generated such practices have created an opportunity to incorporate an ever emerging data driven approach in wildfire management the data driven approach can offer a rapid yet efficient estimation of risk associated with wildfires under any fire weather conditions without having to run computationally expensive ensembles that can be prohibitively time consuming in local computers or small pools of computers fire risk maps produced with the estimation results can help the practitioners to prioritize preparation and response activities for risk mitigation the paper has a two fold purpose firstly the paper investigates the application of a data driven approach for rapid wildfire risk estimation using a probaility based risk metric secondly the paper shows how the knowledge base of the proposed metric can be continuously updated to improve the accuracy of the metric we estimate the dangers of wildfires with a risk metric calculated using the probability values conditioned on the fire start location and several meteorological inputs such as air temperature relative humidity wind speed and wind direction without running a single fire simulation we also apply the proposed risk metric to a real use case scenario of the tasmanian region to estimate and identify high fire risk areas based on millions of fire simulations run using spark such ability of rapid risk estimation without any computational requirements can be crucial during fire emergencies furthermore to assess the performance of the proposed risk metric we build an inference model based on the proposed risk metric that categorizes the fire into relevant risk categories we also outline the ways how the proposed risk metric can be improved over time we also hint at how the operational fire simulation data can be stored to continuously update the knowledge base of the proposed risk metric for improved accuracy the rest of the paper is organized as follows section 2 briefly discusses the related works while section 3 details the problem description of estimating fire risk under various risk categories based on a data driven approach section 4 explains the methodology to calculate the risk metrics section 5 presents a case study of tasmania to demonstrate the application of the proposed risk metric section 6 discusses the performance analysis of an inference model based on the proposed risk metric while section 7 concludes the paper with future works 2 related works several studies have been carried out to quantify and assess the possible fire risks with various fire danger indices the australian fire authorities use the mcarthur forest fire danger index ffdi mcarthur 1967 and grassland fire danger index gfdi mcarthur 1966 to provide fire danger ratings across australia these indices are calculated using weather inputs and fuel conditions logistic regression was used to estimate the linear relationship between each fire index and the logit of the probability of a fire day and rate the fire danger indices at a given location in the work andrews 1997 similarly statistical methods such as the poisson model have also been used to predict forest fires in the form of indices dayananda 1977 proposed a stochastic poisson model to estimate the forest fire risks under different environmental conditions mandallaz and ye 1997 used the poisson model to statistically predict the forest fire and demonstrated the superior performance of such models incorporating fire danger index with important explanatory variables to the use of a single fire hazard index brillinger et al 2003 tried to express the total number of forest fires as a function of time location and other explanatory variables on the other hand fire authorities around the world have used fire danger indices to develop fire rating index systems australian fire authorities have recently developed the national fire danger rating system nfdrs mathews et al 2019 that categorizes various types of forest fires into six different categories in canada the canadian forest fire danger rating system cffdrs van wagner forestet al 1987 estimates the fire danger ratings on a daily basis the cffdrs is based on the canadian forest fire weather index fwi system van wagner forestet al 1987 and the canadian forest fire behavior prediction fbp system hirschet al 1996 to quantitatively assess the forest fire behaviors using weather condition data and site specific information such as land topography fuel load and slope the rating system in the us national fire danger rating system nfdrs cohen 1985 uses instantaneous weather data and statistical summary of weather data along with fuel load and land topography to calculate the fire danger indices other noticeable systems with a single index are russian nesterov index nesterov european forest fire information system effis san miguel ayanz et al 2012 and the italian risico rischio incendi e coordinamento fiorucci et al 2005 that estimate the fire potential on any given day of a year under different fire weather conditions following the general principle of calculation of the fire danger indices we propose a risk metric that gives a series of probability values for a fire starting at any location to fall into various predefined risk categories the estimation is based on the meteorological conditions and location specific inputs and already available simulation data the estimation of fire risks in the proposed risk metric is based on the output metric of the fire simulations run on the operational fire simulation framework which is different from the usual calculations of danger ratings in existing systems that involve mathematical relationships among several input factors amid the growing use of operational fire simulation tools in wildfire management this particular aspect in the proposed metric is expected to create room for more expansive application of data driven solutions for rapid yet effective estimation of wildfire dangers the wildfire danger estimation using the data driven approach as in the proposed metric does not require any computationally expensive fire simulations to be run during the estimation as it utilizes the data from previously run simulations 3 problem description for a region r with n set of possible fire start locations l l 1 l 2 l n given a weather condition w at a particular time instant with k different weather parameters w 1 w 2 w k the ability to categorize any fire starting at any location within r into c different categories fr 1 fr 2 fr c can significantly help fire practitioners to make better informed decisions during operational fire management for such an ability we derive a risk metric r m l j w k which quantifies the possible risks of a fire starting at a location l i under the weather condition w k as a series of probability values for the fire to be labeled under different pre defined categories fr 1 fr 2 fr c based on a model output o for example the possible area that can be burned by the fire within a particular period of time mathematically r m l j w k can be expressed as given in equation 1 1 r m l j w k p f r 1 l j w k p f r 2 l j w k p f r c l j w k 4 methodology in order to calculate the risk metric r m l j w k we rely on a data driven approach to build an inference model based on a massive simulated fire data set we use the inference model to estimate the probabilities of a fire starting at any location under any weather condition into different categories the knowledge base of the inference model can be regularly updated to improve the accuracy of the metric first we run a large number of fire simulations over a region r with several fire start locations l under different weather conditions w using an operational fire simulator then we analyze the output metrics o of all fire simulations to define c different categories fr 1 fr 2 fr c with fr 1 being the lowest risk category and fr c the highest risk category based on the number of output metrics from the fire simulator r m l j w k can be expanded to include all the output metrics as well as such r m o b l j w k is the risk metric for o b t h output metric given by the fire simulator of a fire starting at l j under the weather condition w k based on the analytical literature on the effects of each weather parameter w k the entire range for the possible values of the parameter is discretized into c categories as such any values of weather parameter w k would be discretized to one of the elements of the set w k 1 w k 2 w k c for the knowledge base of the inference model we take ns different fire simulations and use the laws of probability to calculate the likelihood of several possibilities for a fire p fr i w k is the likelihood of a fire starting under a discretized weather condition w k falling into the risk category fr i it should be noted that the weather inputs in our study are uniformly sampled within their allowed ranges without any relevance to historical weather streams p fr i l j is the likelihood of a fire starting at a location l j falling into the risk category fr i these probabilities can be calculated as follows 2 p f r i w k n f i w k n f w k 3 p f r i l j n f i l j n f l j where n is the cardinality while f i is the set of fire simulations corresponding to the category fr i and f is the set of fire simulations for all the risk categories finally the probability of a fire starting at l j under weather condition w k falling into risk category fr i with the assumption of conditional independence between the location and weather events for any given fire risk category can be calculated using the naive bayesian theorem as follows 4 p f r i l j w k p l j f r i p w j f r i p f r i p l j w k the risk metric r m l i w j can thus be given as 5 r m l j w k p l j f r 1 p w j f r 1 p f r i p l j w k p l j f r c p w j f r c p f r c p l j w k 5 case study tasmania in this section we present a case study for the proposed risk metric along with the details on how the proposed approach is applied to a real case study 5 1 study area the tasmanian region is one of the australian regions with frequent reporting of wildfires during summer in the 2018 2019 australian bushfire season 841 wildfires were reported that burned over 310 311 ha of forest area tfs 2019 moreover as a part of the commitment shown by tasmania fire service tfs and state emergency service ses to the nationwide effective management strategy tasmania boasts high quality land data sets that are readily useable in operational wildfire simulation tools additionally the tasmanian region is a well studied area for wildfires with systematic grid configuration for possible fire start locations such a systematic grid configuration facilitates the convenient operation of any model and maintains a level of standard while performing any analyses for all these reasons we choose tasmania as the study area for our work tfs has maintained a grid configuration with each possible fire start location at an interval of 1 km irrespective of land classification in the configuration tfs has used 68 048 different points to represent the entire tasmanian region additionally for the start locations falling on the water bodies all such points are shifted to the nearest land location 5 2 wildfire simulations we used the spark wildfire modeling system as the fire simulation tool in this study spark is a flexible platform to simulate wildfires and their behaviors in different vegetation types apart from fire behaviors spark also allows simulating rates of fire spread in different fuel types firebrand dynamics and risk metrics for fire severity and impact the fire simulations in spark tool typically require the input data sets for the fire behavior models maps of land classification fuel type fuel information meteorological data and topographical data sets the calculations involved while predicting the propagation of fire in spark are parallelized using the opencl framework for efficient execution a sample fire simulation in the spark tool is shown in fig 1 that shows the regions burned by the fire staring at a location at different time steps such fire maps can be used to make better decisions during operational fire management 5 3 weather inputs the weather inputs to the simulations are air temperature relative humidity wind speed and building on our previous work kc et al 2020b wind direction is also included the ranges for these weather input factors as listed in table 1 to represent the entire ranges for the input parameters we consider five equally spaced discrete values within the range interval for the first three input factors for wind direction we consider four discrete values to represent four main directions east west north and south it should be noted that fires grow rapidly in weather condition characterized by high values of air temperature and wind speed and a low value of relative humidity when the wind direction is towards the burnable forest area rather than towards the water bodies the ranges for the weather inputs can if required be easily changed depending on the requirements and purpose of the analysis 5 4 fire simulations fire simulations were run over 68 048 possible start locations in tasmania under five discrete values for each of air temperature relative humidity and wind speed and four discrete values for wind direction the simulations were run for 5 h of simulation time using the weather parameters as well as land elevation and fuel classification data set for the region these 34 024 000 different fire simulations were run over a nectar nectar cloud and https nec 2018 and google cloud google cloud based system as described in our previous work ujjwal et al 2020a for our analysis we considered the cumulative area burnt by the fire as the output metric consequently based on discrete values of the fire area a fire starting at a location under given weather condition is labeled under different categories as given in table 2 these categories have been created based on the quartile values of the fire simulation data set and can easily be altered and adapted to suit any location as per requirement for two risk categories for the evaluation the low and medium categories are combined into the low category 5 5 inference model the bayesian approach fits a probabilistic model to a given set of data and predicts new observations the approach offers additional flexibility as prior knowledge on the phenomena can be complemented with the observations to construct a reliable and accurate model moreover the priors used in the approach can be constantly updated to further improve and optimize the model as such in a data driven approach based on the foundation of millions of fire simulations we use laws of bayes conditional probability to construct an inference model that can be used to predict the fire risk metric for any given location and weather condition to classify a fire under a risk category the fire is labeled with the fire category for which the calculated probability value is the maximum in a bid to establish a standard practice we expect the data coming out from operational fire simulation tool such as spark to be collected compiled and stored in a consistent format which would allow us to constantly update the knowledge base of the inference model as proposed in the study 5 6 evaluation our proposed risk metric gives a series of probability values relating to different risk categories for a fire starting at a given location under a given weather condition based on already available simulation data the accuracy of the categorization of risk categories of imminent fires by our risk metric can be assessed by comparing the categorization against the areas burnt by those fires as such we follow a conventional 75 25 train test rule ayyadevara 2018 with five fold cross validation and report the average values to access the accuracy of the proposed risk metric for any fire that the inference model of the proposed risk metric may not have trained on the fire risk is estimated by the likelihood of different risk categories conditioned on the fire location and the weather conditions additionally to investigate the robustness of the proposed risk metric we compare the accuracy against the mcarthur s forest fire danger index ffdi the mcarthur ffdi is an empirical relationship that includes short term meteorological variables and drought factor used to indicate the chances of a fire starting and its risks in relation to its intensity and difficulty of suppression as defined by noble et al 1980 ffdi can be defined as follows 6 f f d i 2 0 e 0 450 0 987 l n d f 0 0 345 r h 0 0 338 t 0 0 234 v where df is the drought factor rh is the relative humidity t 0c is the air temperature and v kmh 1 is the average wind velocity in an open flat location at a height of 10m the mcarthur ffdi is chosen as the closest metric for the performance evaluation as the key meteorological inputs air temperature relative humidity and wind speed are common to both the long term drought factor was fixed at the value of nine the classification of ffdi into fire danger rating fdr categories based on wain and kepert wain et al 2013 was adapted as listed in table 3 for comparison 5 7 model application we applied the proposed fire risk metric to estimate wildfire risks in the entire tasmanian region rather than using the real weather data from the weather stations in the study area as is used in operational management we used synthetic weather data generated within the entirety of possible ranges this was done to make the inference model more robust that would closely predict the fire risks under any circumstances the computationally time intensive task of running millions of simulations under the considered ranges for the weather inputs was carried out using cloud infrastructure after all the fire simulations were executed we constructed a robust knowledge base for the inference model by performing further analyses on the simulation output all the intermediate terms as given in equation 1 were calculated using equations 2 and 3 the series of probability values as obtained with the proposed risk metric can be interpreted in several ways fig 2 shows the tasmanian map with the locations where the fire risks are more likely to be extremely high for a fire weather condition when temperature relative humidity and wind speed have the highest influence on the fire growth and the a southerly 180 wind direction it should be noted that for the figurative representation as included in fig 2 the fire risk category is labeled based on the maximum of all the values for each risk category such risk mapping can help the practitioners to quickly identify the potential high fire risk areas for any given weather condition without having to run a large number of simulations ensembles which is usually computationally complex and time consuming ujjwal et al 2020a additionally the risk metric as calculated in the validation can be quite useful for prioritization of resource mobilization for example for a possible fire start location identified by an identifier value of 34 112 with corresponding geographic coordinates the calculated risk metric r m l j w k weather condition identified by t 90 r 10 ws 60 wd 180 o is 6 82 32 14 61 40 this risk metric estimates any fire starting at the location under the weather condition w k has only about 7 chance of having a low fire risk and about 62 chance of having extreme fire risks we assessed the accuracy of the proposed risk metric during the validation phase by checking the total number of fires at different locations labeled with the correct risk category for a given weather condition identified by t 40 r 10 ws 60 wd 180 fig 3 shows correctly and incorrectly labeled fires all over the tasmanian region for different weather combinations the proposed risk metric was able to correctly categorize 6 340 816 out of 8 506 000 approximately 75 fires moreover 1 584 987 out of a total 2 127 009 approximately 75 high risk fires were correctly predicted by the inference model the incorrectly labeled fires by the inference model are the ones starting at the locations closer to un burnable area such as water bodies rocks and stones where the degree of risks can significantly for different wind directions further discussion on the performance analysis of the proposed risk metric is given in section 6 6 performance analysis of the inference models in this section we present a detailed performance analysis of the inference model that estimates the risk metric along with the discussion on the confusion matrix and the possible ways how the inference model can be improved 6 1 accuracy for three different fire risk categories of high medium and low as listed in table 2 the accuracy of the trained inference model on the test data was found to be about 75 with only two categories of low and medium the risk categories of medium and low were combined to form a single category of low the accuracy increased to around 88 the inference model was found to underfit estimate lower fire risk than the actual for about 16 and 11 of the fires and overfit estimate higher fire risk than the actual about 10 and 3 for three and two fire risk categories respectively fig 4 shows the confusion matrix for the inference model for two and three different fire risk categories as can be seen from fig 4a the inference model had 75 true positives for the high risk category when compared to 89 and 45 for the low and medium risk categories about 38 of the fires belonging to the medium risk category have been identified by the inference model as the low risk category fires the inference model mislabeled about 17 of the medium risk fires as high risk fires 15 of the high risk fires as medium risk fires 11 of the high risk fires as low risk fires about 9 of low risk fires as medium risk fires and about 3 of the low risk fires as high risk fires similarly for the two fire risk categories as seen from fig 4b more than 97 of the low risk fires are correctly identified by the inference model the inference model identified only about 59 of the high risk fires about 41 of the high risk fires have been falsely labeled as low risk fires while only about 3 of the low risk fires have been mislabeled as high risk fires the accuracy of the proposed risk metric compared against the adapted mcarthur ffdi is presented in table 4 the adapted mcarthur uses three meteorological inputs only to estimate the severity of fire risks whereas the proposed metric includes additional information on location and wind direction consequently the proposed metric has superior performance for two and three risk categories interestingly the adapted mcarthur ffdi has fewer overfits than the proposed metric for three risk categories for the two risk categories the overfits for the mcarthur ffdi are more than for the proposed metric the performance of the adapted mcarthur ffdi may have suffered from the fact that the experimental setup for generating the samples of input factors considered did not follow any historical relevance as the generation was done uniformly within their allowed ranges additionally the drought factor was also fixed at a nominal value of nine for the entire test case the sampling of inputs to the proposed risk metric relevant to historical information will be a future extension nonetheless for the same experimental setup our proposed metric demonstrated a better performance against an adapted mcaruthur ffdi thereby validating the applicability of the data approach in the development of a simulation driven risk metric to further investigate the performance of the inference model we classified the mislabeled fires with respect to each meteorological input factor in table 5 for wind direction we have considered four different directions 0 90 180 and 270 the underfit proportion for mislabeled fires with respect to wind direction is 3 99 3 926 4 063 and 3 82 while the overfit proportion is 2 38 2 44 2 42 and 2 42 the proportionate distribution of the mislabeled fires with respect to the wind direction shows that wind direction plays a significant role in estimating fire risk for example even in conditions highly favorable to a fire a starting location near an un burnable region such as a water body may grow rapidly only if the prevailing wind moves the fire away from the un burnable region wildfires usually grow aggressively under hot and dry conditions with stronger winds as described in ujjwal et al 2020b the influence of relative humidity and wind speed is higher on the fire growth when compared to that of the temperature so the higher proportions of underfits an underestimation when compared to the risk from the simulation at lower values of air temperature could be due to the higher influence of the relative humidity and the wind speed similarly the higher proportion of underfits when the influence of all the meteorological inputs is the highest can be attributed to the dominant influence of the fire starting location and wind direction the inference model has significantly low proportions of underfits the notable overfits under the meteorological condition highly favorable for fire growth is possibly due to the presence of un burnable regions such as water bodies rock sand and swamp closer to the locations where fires start and the wind direction acting to drive the fire towards these regions for example fig 5 shows low risk fires starting at different locations incorrectly labeled as high fire risk by the inference model for westerly and easterly winds the inset in the figure focuses on the fire start locations closer to water bodies the king island where red points are mislabeled fires under easterly wind conditions while the green ones are for fire under westerly wind conditions easterly wind conditions drove any fires starting at locations near the western shoreline into the water and correspondingly westerly wind conditions drove any fires starting at locations near the eastern shoreline into the water consequently any such fires rapidly stopped resulting in low risk values the findings with the other three directions are the same thus concluding that fires starting closer to un burnable regions bodies are more likely to be incorrectly labeled by the inference model 6 2 improving accuracy in our study we formulated a computationally inexpensive yet effective way of estimating the risk of wildfire based on a data driven approach in the following sub sections we suggest several ways how the proposed metric can be made more accurate based on our metric analysis and testing 6 2 1 more training data in a data driven approach the accuracy usually improves with the increase in the training data size in hand and this is the case with our proposed metric the entire principle of calculating the risk metrics based on the probability conditioned on a number of input factors in a fire growth model was tested with several data sizes for a training data size of about 5 5 million fire simulations for three fire risk categories the accuracy was found to be 73 24 the accuracy increased to about 75 when the training data size was about 25 5 million for two fire risk categories the accuracy improved from 86 2 to 87 43 for the same comparison for our proposed risk metric we expect the accuracy to improve with the addition of more fire simulation data into the knowledge base of the inference model the improvement of the accuracy seems to be marginal in our analysis as the number of fire simulations for a single fire start location has not increased significantly only 500 at a location as expected in a data driven approach one approach may be to collect ongoing simulation data to gradually improve the accuracy of the proposed risk metric 6 3 improved categorization rule in our inference model we have categorized a fire starting at a possible start location into a risk category based on the maximum value of the risk probabilities for example the risk metric r m l j w k under two risk categories of high and low for a location l j for weather condition w k is estimated as is 55 45 as per our rule defined for risk categorization the fire would be labeled as a low risk fire despite having 45 of turning into a high risk fire for a two fire risk category the training data has about 75 of low risk fires as can be seen from fig 4b the inference model is quite effective while identifying low risk fires 97 when compared to identifying the high risk fires 59 the fire dangers as estimated by our proposed risk metric have to be interpreted differently by prioritizing the risk probabilities calculated for higher fire risks in the previous example the probability of the fire falling into a high risk category is 45 which despite being less than the probability of the fire falling into the low risk category is still relatively high such fires could be intuitively treated as the fires possessing significant risks during fire emergencies following this in one of the possible ways to improvise the categorization rule for the inference model we labeled the fires with at least 40 estimated probability for the high fire risk category as high risk fires the accuracy of the inference model increased marginally to about 89 but the noticeable finding was the drastically improved value of correctly identified high risk fires from 75 to 59 the underfitting from the inference model decreased to 25 from 41 with the maximum value rule as such the proposed risk metric has to be interpreted more intuitively when used 6 4 unbiased prior for a given fire weather in a large geographical area usually only a fraction of all the possible fire start locations is likely to have extremely high risk fires as expressed in equation 4 the term p fr i is the prior of a fire belonging to a risk category i the value of the term for the low fire risk category is quite high compared to the values of the term for higher fire risk as such the rule of maximum probability value to label the fire into a risk category can be biased towards the low risk category and so has been our findings we briefly estimated the risk metric with unbiased priors all risk categories equally likely for all the fire risk categories for the two fire risk categories the accuracy was found to be 88 17 with 8 15 underfit and 3 68 overfit estimation the correct identification of low risk fires was at 95 while less than 5 of the low risk fires were mislabeled as high risk fires most importantly the correct identification of high risk fires increased to 67 from 59 moreover the threshold used to categorize the fires can easily be changed within our proposed risk metric as such the prior knowledge can be adapted and changed accordingly to improve the accuracy of the inference model based on the proposed risk metric 7 conclusions in this paper we proposed a probability based metric that estimated the risk of wildfire and investigated the applicability of a data driven approach to calculate this metric the proposed risk metric offers a computationally inexpensive yet rapid and reliable way of predicting high fire risk areas the application of our proposed data driven risk estimation to the tasmanian region shows the efficacy of determining the proposed risk metric over a large region the accuracy of an inference model based on the risk metric for the categorization of fire into risk categories was about 88 we have also suggested means by which the accuracy of the proposed risk metric and the inference model could be improved the proposed risk metric is model agnostic and can easily be replicated for other natural hazard models such as flood simulations the input factors to wildfire simulation models based on which the probability values are conditioned can easily be added or removed while estimating the risk metric combining the findings of other fire related studies such as sensitivity analyses and big training data sets with the proposed risk metric could help estimate regions of high wildfire risk rapidly and accurately inclusion of the metric into operational wildfire management systems could provide a further level of information for time critical decision making and strategy formation during wildfire emergencies such a risk metric can be used as a rapid tool for assessing disaster risks and consequently reducing the associated risks through effective risk management strategies such as early warnings declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgement the authors would like to extend their heartfelt gratitude to everyone helped improve the quality of the paper at different stages of the work 
25672,with the advancement in scientific understanding and computing technologies fire practitioners have started relying on operational fire simulation tools to make better informed decisions during wildfire emergencies this increased use has created an opportunity to employ an emerging data driven approach for wildfire risk estimation as an alternative to running computationally expensive simulations in an investigative attempt we propose a probability based risk metric that gives a series of probability values for fires starting at any possible start location under any given weather condition falling into different categories we investigate the validity of the proposed approach by applying it to use cases in tasmania australia results show that the proposed risk metric can be a convenient and accurate method of estimating imminent risk during operational wildfire management additionally the knowledge base of our proposed risk metric based on a data driven approach can be constantly updated to improve its accuracy keywords wildfire risk management data driven approach risk metric wildfire simulations spark risk reduction software data availability the fire propagation tool spark is available at https research csiro au spark the fire simulation data set based on which the inference model has been built can be found at https data csiro au collections collection cicsiro 49133v1 ditrue ujjwal et al 2021a 1 introduction thanks to the recent advancements in observational science and other technologies such as engineering and computing natural disasters such as wildfires can be studied in greater detail with more complex models razavi et al burn kaizer et al 2015 for wildfire management the risk associated with wildfires can be quantified by running several fire simulations collectively referred to as an ensemble in a geographical location and performing statistical analyses thereafter ujjwal et al 2020a 2021b c despite the computationally complex nature of ensemble predictions their extensive use in wildfire management has become possible due to the rise of computing technologies such as cloud computing ujjwal et al 2019 2020b consequently fire authorities have started relying on operational fire simulations tools such as spark miller et al 2015 phoenix tolhurst et al 2008 farsite finney 2004 and prometheus tymstra et al armitageet al to make better informed decision at various stages of wildfire emergencies with massive data being generated such practices have created an opportunity to incorporate an ever emerging data driven approach in wildfire management the data driven approach can offer a rapid yet efficient estimation of risk associated with wildfires under any fire weather conditions without having to run computationally expensive ensembles that can be prohibitively time consuming in local computers or small pools of computers fire risk maps produced with the estimation results can help the practitioners to prioritize preparation and response activities for risk mitigation the paper has a two fold purpose firstly the paper investigates the application of a data driven approach for rapid wildfire risk estimation using a probaility based risk metric secondly the paper shows how the knowledge base of the proposed metric can be continuously updated to improve the accuracy of the metric we estimate the dangers of wildfires with a risk metric calculated using the probability values conditioned on the fire start location and several meteorological inputs such as air temperature relative humidity wind speed and wind direction without running a single fire simulation we also apply the proposed risk metric to a real use case scenario of the tasmanian region to estimate and identify high fire risk areas based on millions of fire simulations run using spark such ability of rapid risk estimation without any computational requirements can be crucial during fire emergencies furthermore to assess the performance of the proposed risk metric we build an inference model based on the proposed risk metric that categorizes the fire into relevant risk categories we also outline the ways how the proposed risk metric can be improved over time we also hint at how the operational fire simulation data can be stored to continuously update the knowledge base of the proposed risk metric for improved accuracy the rest of the paper is organized as follows section 2 briefly discusses the related works while section 3 details the problem description of estimating fire risk under various risk categories based on a data driven approach section 4 explains the methodology to calculate the risk metrics section 5 presents a case study of tasmania to demonstrate the application of the proposed risk metric section 6 discusses the performance analysis of an inference model based on the proposed risk metric while section 7 concludes the paper with future works 2 related works several studies have been carried out to quantify and assess the possible fire risks with various fire danger indices the australian fire authorities use the mcarthur forest fire danger index ffdi mcarthur 1967 and grassland fire danger index gfdi mcarthur 1966 to provide fire danger ratings across australia these indices are calculated using weather inputs and fuel conditions logistic regression was used to estimate the linear relationship between each fire index and the logit of the probability of a fire day and rate the fire danger indices at a given location in the work andrews 1997 similarly statistical methods such as the poisson model have also been used to predict forest fires in the form of indices dayananda 1977 proposed a stochastic poisson model to estimate the forest fire risks under different environmental conditions mandallaz and ye 1997 used the poisson model to statistically predict the forest fire and demonstrated the superior performance of such models incorporating fire danger index with important explanatory variables to the use of a single fire hazard index brillinger et al 2003 tried to express the total number of forest fires as a function of time location and other explanatory variables on the other hand fire authorities around the world have used fire danger indices to develop fire rating index systems australian fire authorities have recently developed the national fire danger rating system nfdrs mathews et al 2019 that categorizes various types of forest fires into six different categories in canada the canadian forest fire danger rating system cffdrs van wagner forestet al 1987 estimates the fire danger ratings on a daily basis the cffdrs is based on the canadian forest fire weather index fwi system van wagner forestet al 1987 and the canadian forest fire behavior prediction fbp system hirschet al 1996 to quantitatively assess the forest fire behaviors using weather condition data and site specific information such as land topography fuel load and slope the rating system in the us national fire danger rating system nfdrs cohen 1985 uses instantaneous weather data and statistical summary of weather data along with fuel load and land topography to calculate the fire danger indices other noticeable systems with a single index are russian nesterov index nesterov european forest fire information system effis san miguel ayanz et al 2012 and the italian risico rischio incendi e coordinamento fiorucci et al 2005 that estimate the fire potential on any given day of a year under different fire weather conditions following the general principle of calculation of the fire danger indices we propose a risk metric that gives a series of probability values for a fire starting at any location to fall into various predefined risk categories the estimation is based on the meteorological conditions and location specific inputs and already available simulation data the estimation of fire risks in the proposed risk metric is based on the output metric of the fire simulations run on the operational fire simulation framework which is different from the usual calculations of danger ratings in existing systems that involve mathematical relationships among several input factors amid the growing use of operational fire simulation tools in wildfire management this particular aspect in the proposed metric is expected to create room for more expansive application of data driven solutions for rapid yet effective estimation of wildfire dangers the wildfire danger estimation using the data driven approach as in the proposed metric does not require any computationally expensive fire simulations to be run during the estimation as it utilizes the data from previously run simulations 3 problem description for a region r with n set of possible fire start locations l l 1 l 2 l n given a weather condition w at a particular time instant with k different weather parameters w 1 w 2 w k the ability to categorize any fire starting at any location within r into c different categories fr 1 fr 2 fr c can significantly help fire practitioners to make better informed decisions during operational fire management for such an ability we derive a risk metric r m l j w k which quantifies the possible risks of a fire starting at a location l i under the weather condition w k as a series of probability values for the fire to be labeled under different pre defined categories fr 1 fr 2 fr c based on a model output o for example the possible area that can be burned by the fire within a particular period of time mathematically r m l j w k can be expressed as given in equation 1 1 r m l j w k p f r 1 l j w k p f r 2 l j w k p f r c l j w k 4 methodology in order to calculate the risk metric r m l j w k we rely on a data driven approach to build an inference model based on a massive simulated fire data set we use the inference model to estimate the probabilities of a fire starting at any location under any weather condition into different categories the knowledge base of the inference model can be regularly updated to improve the accuracy of the metric first we run a large number of fire simulations over a region r with several fire start locations l under different weather conditions w using an operational fire simulator then we analyze the output metrics o of all fire simulations to define c different categories fr 1 fr 2 fr c with fr 1 being the lowest risk category and fr c the highest risk category based on the number of output metrics from the fire simulator r m l j w k can be expanded to include all the output metrics as well as such r m o b l j w k is the risk metric for o b t h output metric given by the fire simulator of a fire starting at l j under the weather condition w k based on the analytical literature on the effects of each weather parameter w k the entire range for the possible values of the parameter is discretized into c categories as such any values of weather parameter w k would be discretized to one of the elements of the set w k 1 w k 2 w k c for the knowledge base of the inference model we take ns different fire simulations and use the laws of probability to calculate the likelihood of several possibilities for a fire p fr i w k is the likelihood of a fire starting under a discretized weather condition w k falling into the risk category fr i it should be noted that the weather inputs in our study are uniformly sampled within their allowed ranges without any relevance to historical weather streams p fr i l j is the likelihood of a fire starting at a location l j falling into the risk category fr i these probabilities can be calculated as follows 2 p f r i w k n f i w k n f w k 3 p f r i l j n f i l j n f l j where n is the cardinality while f i is the set of fire simulations corresponding to the category fr i and f is the set of fire simulations for all the risk categories finally the probability of a fire starting at l j under weather condition w k falling into risk category fr i with the assumption of conditional independence between the location and weather events for any given fire risk category can be calculated using the naive bayesian theorem as follows 4 p f r i l j w k p l j f r i p w j f r i p f r i p l j w k the risk metric r m l i w j can thus be given as 5 r m l j w k p l j f r 1 p w j f r 1 p f r i p l j w k p l j f r c p w j f r c p f r c p l j w k 5 case study tasmania in this section we present a case study for the proposed risk metric along with the details on how the proposed approach is applied to a real case study 5 1 study area the tasmanian region is one of the australian regions with frequent reporting of wildfires during summer in the 2018 2019 australian bushfire season 841 wildfires were reported that burned over 310 311 ha of forest area tfs 2019 moreover as a part of the commitment shown by tasmania fire service tfs and state emergency service ses to the nationwide effective management strategy tasmania boasts high quality land data sets that are readily useable in operational wildfire simulation tools additionally the tasmanian region is a well studied area for wildfires with systematic grid configuration for possible fire start locations such a systematic grid configuration facilitates the convenient operation of any model and maintains a level of standard while performing any analyses for all these reasons we choose tasmania as the study area for our work tfs has maintained a grid configuration with each possible fire start location at an interval of 1 km irrespective of land classification in the configuration tfs has used 68 048 different points to represent the entire tasmanian region additionally for the start locations falling on the water bodies all such points are shifted to the nearest land location 5 2 wildfire simulations we used the spark wildfire modeling system as the fire simulation tool in this study spark is a flexible platform to simulate wildfires and their behaviors in different vegetation types apart from fire behaviors spark also allows simulating rates of fire spread in different fuel types firebrand dynamics and risk metrics for fire severity and impact the fire simulations in spark tool typically require the input data sets for the fire behavior models maps of land classification fuel type fuel information meteorological data and topographical data sets the calculations involved while predicting the propagation of fire in spark are parallelized using the opencl framework for efficient execution a sample fire simulation in the spark tool is shown in fig 1 that shows the regions burned by the fire staring at a location at different time steps such fire maps can be used to make better decisions during operational fire management 5 3 weather inputs the weather inputs to the simulations are air temperature relative humidity wind speed and building on our previous work kc et al 2020b wind direction is also included the ranges for these weather input factors as listed in table 1 to represent the entire ranges for the input parameters we consider five equally spaced discrete values within the range interval for the first three input factors for wind direction we consider four discrete values to represent four main directions east west north and south it should be noted that fires grow rapidly in weather condition characterized by high values of air temperature and wind speed and a low value of relative humidity when the wind direction is towards the burnable forest area rather than towards the water bodies the ranges for the weather inputs can if required be easily changed depending on the requirements and purpose of the analysis 5 4 fire simulations fire simulations were run over 68 048 possible start locations in tasmania under five discrete values for each of air temperature relative humidity and wind speed and four discrete values for wind direction the simulations were run for 5 h of simulation time using the weather parameters as well as land elevation and fuel classification data set for the region these 34 024 000 different fire simulations were run over a nectar nectar cloud and https nec 2018 and google cloud google cloud based system as described in our previous work ujjwal et al 2020a for our analysis we considered the cumulative area burnt by the fire as the output metric consequently based on discrete values of the fire area a fire starting at a location under given weather condition is labeled under different categories as given in table 2 these categories have been created based on the quartile values of the fire simulation data set and can easily be altered and adapted to suit any location as per requirement for two risk categories for the evaluation the low and medium categories are combined into the low category 5 5 inference model the bayesian approach fits a probabilistic model to a given set of data and predicts new observations the approach offers additional flexibility as prior knowledge on the phenomena can be complemented with the observations to construct a reliable and accurate model moreover the priors used in the approach can be constantly updated to further improve and optimize the model as such in a data driven approach based on the foundation of millions of fire simulations we use laws of bayes conditional probability to construct an inference model that can be used to predict the fire risk metric for any given location and weather condition to classify a fire under a risk category the fire is labeled with the fire category for which the calculated probability value is the maximum in a bid to establish a standard practice we expect the data coming out from operational fire simulation tool such as spark to be collected compiled and stored in a consistent format which would allow us to constantly update the knowledge base of the inference model as proposed in the study 5 6 evaluation our proposed risk metric gives a series of probability values relating to different risk categories for a fire starting at a given location under a given weather condition based on already available simulation data the accuracy of the categorization of risk categories of imminent fires by our risk metric can be assessed by comparing the categorization against the areas burnt by those fires as such we follow a conventional 75 25 train test rule ayyadevara 2018 with five fold cross validation and report the average values to access the accuracy of the proposed risk metric for any fire that the inference model of the proposed risk metric may not have trained on the fire risk is estimated by the likelihood of different risk categories conditioned on the fire location and the weather conditions additionally to investigate the robustness of the proposed risk metric we compare the accuracy against the mcarthur s forest fire danger index ffdi the mcarthur ffdi is an empirical relationship that includes short term meteorological variables and drought factor used to indicate the chances of a fire starting and its risks in relation to its intensity and difficulty of suppression as defined by noble et al 1980 ffdi can be defined as follows 6 f f d i 2 0 e 0 450 0 987 l n d f 0 0 345 r h 0 0 338 t 0 0 234 v where df is the drought factor rh is the relative humidity t 0c is the air temperature and v kmh 1 is the average wind velocity in an open flat location at a height of 10m the mcarthur ffdi is chosen as the closest metric for the performance evaluation as the key meteorological inputs air temperature relative humidity and wind speed are common to both the long term drought factor was fixed at the value of nine the classification of ffdi into fire danger rating fdr categories based on wain and kepert wain et al 2013 was adapted as listed in table 3 for comparison 5 7 model application we applied the proposed fire risk metric to estimate wildfire risks in the entire tasmanian region rather than using the real weather data from the weather stations in the study area as is used in operational management we used synthetic weather data generated within the entirety of possible ranges this was done to make the inference model more robust that would closely predict the fire risks under any circumstances the computationally time intensive task of running millions of simulations under the considered ranges for the weather inputs was carried out using cloud infrastructure after all the fire simulations were executed we constructed a robust knowledge base for the inference model by performing further analyses on the simulation output all the intermediate terms as given in equation 1 were calculated using equations 2 and 3 the series of probability values as obtained with the proposed risk metric can be interpreted in several ways fig 2 shows the tasmanian map with the locations where the fire risks are more likely to be extremely high for a fire weather condition when temperature relative humidity and wind speed have the highest influence on the fire growth and the a southerly 180 wind direction it should be noted that for the figurative representation as included in fig 2 the fire risk category is labeled based on the maximum of all the values for each risk category such risk mapping can help the practitioners to quickly identify the potential high fire risk areas for any given weather condition without having to run a large number of simulations ensembles which is usually computationally complex and time consuming ujjwal et al 2020a additionally the risk metric as calculated in the validation can be quite useful for prioritization of resource mobilization for example for a possible fire start location identified by an identifier value of 34 112 with corresponding geographic coordinates the calculated risk metric r m l j w k weather condition identified by t 90 r 10 ws 60 wd 180 o is 6 82 32 14 61 40 this risk metric estimates any fire starting at the location under the weather condition w k has only about 7 chance of having a low fire risk and about 62 chance of having extreme fire risks we assessed the accuracy of the proposed risk metric during the validation phase by checking the total number of fires at different locations labeled with the correct risk category for a given weather condition identified by t 40 r 10 ws 60 wd 180 fig 3 shows correctly and incorrectly labeled fires all over the tasmanian region for different weather combinations the proposed risk metric was able to correctly categorize 6 340 816 out of 8 506 000 approximately 75 fires moreover 1 584 987 out of a total 2 127 009 approximately 75 high risk fires were correctly predicted by the inference model the incorrectly labeled fires by the inference model are the ones starting at the locations closer to un burnable area such as water bodies rocks and stones where the degree of risks can significantly for different wind directions further discussion on the performance analysis of the proposed risk metric is given in section 6 6 performance analysis of the inference models in this section we present a detailed performance analysis of the inference model that estimates the risk metric along with the discussion on the confusion matrix and the possible ways how the inference model can be improved 6 1 accuracy for three different fire risk categories of high medium and low as listed in table 2 the accuracy of the trained inference model on the test data was found to be about 75 with only two categories of low and medium the risk categories of medium and low were combined to form a single category of low the accuracy increased to around 88 the inference model was found to underfit estimate lower fire risk than the actual for about 16 and 11 of the fires and overfit estimate higher fire risk than the actual about 10 and 3 for three and two fire risk categories respectively fig 4 shows the confusion matrix for the inference model for two and three different fire risk categories as can be seen from fig 4a the inference model had 75 true positives for the high risk category when compared to 89 and 45 for the low and medium risk categories about 38 of the fires belonging to the medium risk category have been identified by the inference model as the low risk category fires the inference model mislabeled about 17 of the medium risk fires as high risk fires 15 of the high risk fires as medium risk fires 11 of the high risk fires as low risk fires about 9 of low risk fires as medium risk fires and about 3 of the low risk fires as high risk fires similarly for the two fire risk categories as seen from fig 4b more than 97 of the low risk fires are correctly identified by the inference model the inference model identified only about 59 of the high risk fires about 41 of the high risk fires have been falsely labeled as low risk fires while only about 3 of the low risk fires have been mislabeled as high risk fires the accuracy of the proposed risk metric compared against the adapted mcarthur ffdi is presented in table 4 the adapted mcarthur uses three meteorological inputs only to estimate the severity of fire risks whereas the proposed metric includes additional information on location and wind direction consequently the proposed metric has superior performance for two and three risk categories interestingly the adapted mcarthur ffdi has fewer overfits than the proposed metric for three risk categories for the two risk categories the overfits for the mcarthur ffdi are more than for the proposed metric the performance of the adapted mcarthur ffdi may have suffered from the fact that the experimental setup for generating the samples of input factors considered did not follow any historical relevance as the generation was done uniformly within their allowed ranges additionally the drought factor was also fixed at a nominal value of nine for the entire test case the sampling of inputs to the proposed risk metric relevant to historical information will be a future extension nonetheless for the same experimental setup our proposed metric demonstrated a better performance against an adapted mcaruthur ffdi thereby validating the applicability of the data approach in the development of a simulation driven risk metric to further investigate the performance of the inference model we classified the mislabeled fires with respect to each meteorological input factor in table 5 for wind direction we have considered four different directions 0 90 180 and 270 the underfit proportion for mislabeled fires with respect to wind direction is 3 99 3 926 4 063 and 3 82 while the overfit proportion is 2 38 2 44 2 42 and 2 42 the proportionate distribution of the mislabeled fires with respect to the wind direction shows that wind direction plays a significant role in estimating fire risk for example even in conditions highly favorable to a fire a starting location near an un burnable region such as a water body may grow rapidly only if the prevailing wind moves the fire away from the un burnable region wildfires usually grow aggressively under hot and dry conditions with stronger winds as described in ujjwal et al 2020b the influence of relative humidity and wind speed is higher on the fire growth when compared to that of the temperature so the higher proportions of underfits an underestimation when compared to the risk from the simulation at lower values of air temperature could be due to the higher influence of the relative humidity and the wind speed similarly the higher proportion of underfits when the influence of all the meteorological inputs is the highest can be attributed to the dominant influence of the fire starting location and wind direction the inference model has significantly low proportions of underfits the notable overfits under the meteorological condition highly favorable for fire growth is possibly due to the presence of un burnable regions such as water bodies rock sand and swamp closer to the locations where fires start and the wind direction acting to drive the fire towards these regions for example fig 5 shows low risk fires starting at different locations incorrectly labeled as high fire risk by the inference model for westerly and easterly winds the inset in the figure focuses on the fire start locations closer to water bodies the king island where red points are mislabeled fires under easterly wind conditions while the green ones are for fire under westerly wind conditions easterly wind conditions drove any fires starting at locations near the western shoreline into the water and correspondingly westerly wind conditions drove any fires starting at locations near the eastern shoreline into the water consequently any such fires rapidly stopped resulting in low risk values the findings with the other three directions are the same thus concluding that fires starting closer to un burnable regions bodies are more likely to be incorrectly labeled by the inference model 6 2 improving accuracy in our study we formulated a computationally inexpensive yet effective way of estimating the risk of wildfire based on a data driven approach in the following sub sections we suggest several ways how the proposed metric can be made more accurate based on our metric analysis and testing 6 2 1 more training data in a data driven approach the accuracy usually improves with the increase in the training data size in hand and this is the case with our proposed metric the entire principle of calculating the risk metrics based on the probability conditioned on a number of input factors in a fire growth model was tested with several data sizes for a training data size of about 5 5 million fire simulations for three fire risk categories the accuracy was found to be 73 24 the accuracy increased to about 75 when the training data size was about 25 5 million for two fire risk categories the accuracy improved from 86 2 to 87 43 for the same comparison for our proposed risk metric we expect the accuracy to improve with the addition of more fire simulation data into the knowledge base of the inference model the improvement of the accuracy seems to be marginal in our analysis as the number of fire simulations for a single fire start location has not increased significantly only 500 at a location as expected in a data driven approach one approach may be to collect ongoing simulation data to gradually improve the accuracy of the proposed risk metric 6 3 improved categorization rule in our inference model we have categorized a fire starting at a possible start location into a risk category based on the maximum value of the risk probabilities for example the risk metric r m l j w k under two risk categories of high and low for a location l j for weather condition w k is estimated as is 55 45 as per our rule defined for risk categorization the fire would be labeled as a low risk fire despite having 45 of turning into a high risk fire for a two fire risk category the training data has about 75 of low risk fires as can be seen from fig 4b the inference model is quite effective while identifying low risk fires 97 when compared to identifying the high risk fires 59 the fire dangers as estimated by our proposed risk metric have to be interpreted differently by prioritizing the risk probabilities calculated for higher fire risks in the previous example the probability of the fire falling into a high risk category is 45 which despite being less than the probability of the fire falling into the low risk category is still relatively high such fires could be intuitively treated as the fires possessing significant risks during fire emergencies following this in one of the possible ways to improvise the categorization rule for the inference model we labeled the fires with at least 40 estimated probability for the high fire risk category as high risk fires the accuracy of the inference model increased marginally to about 89 but the noticeable finding was the drastically improved value of correctly identified high risk fires from 75 to 59 the underfitting from the inference model decreased to 25 from 41 with the maximum value rule as such the proposed risk metric has to be interpreted more intuitively when used 6 4 unbiased prior for a given fire weather in a large geographical area usually only a fraction of all the possible fire start locations is likely to have extremely high risk fires as expressed in equation 4 the term p fr i is the prior of a fire belonging to a risk category i the value of the term for the low fire risk category is quite high compared to the values of the term for higher fire risk as such the rule of maximum probability value to label the fire into a risk category can be biased towards the low risk category and so has been our findings we briefly estimated the risk metric with unbiased priors all risk categories equally likely for all the fire risk categories for the two fire risk categories the accuracy was found to be 88 17 with 8 15 underfit and 3 68 overfit estimation the correct identification of low risk fires was at 95 while less than 5 of the low risk fires were mislabeled as high risk fires most importantly the correct identification of high risk fires increased to 67 from 59 moreover the threshold used to categorize the fires can easily be changed within our proposed risk metric as such the prior knowledge can be adapted and changed accordingly to improve the accuracy of the inference model based on the proposed risk metric 7 conclusions in this paper we proposed a probability based metric that estimated the risk of wildfire and investigated the applicability of a data driven approach to calculate this metric the proposed risk metric offers a computationally inexpensive yet rapid and reliable way of predicting high fire risk areas the application of our proposed data driven risk estimation to the tasmanian region shows the efficacy of determining the proposed risk metric over a large region the accuracy of an inference model based on the risk metric for the categorization of fire into risk categories was about 88 we have also suggested means by which the accuracy of the proposed risk metric and the inference model could be improved the proposed risk metric is model agnostic and can easily be replicated for other natural hazard models such as flood simulations the input factors to wildfire simulation models based on which the probability values are conditioned can easily be added or removed while estimating the risk metric combining the findings of other fire related studies such as sensitivity analyses and big training data sets with the proposed risk metric could help estimate regions of high wildfire risk rapidly and accurately inclusion of the metric into operational wildfire management systems could provide a further level of information for time critical decision making and strategy formation during wildfire emergencies such a risk metric can be used as a rapid tool for assessing disaster risks and consequently reducing the associated risks through effective risk management strategies such as early warnings declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgement the authors would like to extend their heartfelt gratitude to everyone helped improve the quality of the paper at different stages of the work 
25673,global spatially explicit land system models are important tools to gain a better scientific understanding and to investigate future development trajectories in form of scenarios however uncertainties e g of input data and model structure affect their simulation results this article investigates the impact of model initialization on land change simulations and builds a single model ensemble based on the global land system model landshift each ensemble member is initialized with a unique combination of a remote sensing land cover dataset a specific method to derive model parameter values is used for land suitability evaluation to identify and quantify significant discrepancies between model outcomes obtained with different ensemble members the discrepancies related to both the land use maps generated as starting conditions of model runs and to the simulated land change and natural vegetation loss the results underline the importance to account for the above uncertainties in global simulation studies and communicating them transparently keywords model initialization global land use modelling uncertainties ensemble analysis global land change and loss of natural vegetation 1 introduction land use and land cover change henceforth land change has strong impacts on climate and environmental processes foley et al 2007 in light of the still growing human consumption of biomass for food feed fiber and energy the expansion and intensification of agriculture are important drivers of land change and its associated impacts such as the conversion of natural ecosystems biodiversity loss and carbon stock reduction erb et al 2016 hinz et al 2020 molotoks et al 2018 zabel et al 2019 to study land change on the global scale a number of spatially explicit land use models have been developed that integrate socio economic and environmental aspects to simulate historic patterns of land change as well as future scenarios national research council 2014 in a wide range of case studies these models have been deployed to gain a better understanding of land system dynamics turner 2017 verburg et al 2019 to assess environmental impacts e g lotze campen et al 2018 molotoks et al 2020 and to provide spatial information for planners and decision makers ipcc 2014 szantoi et al 2020 zagaria et al 2017 however the outcomes of these models are affected by uncertainties as was shown in a number of studies at the continental and global scale e g alcamo et al 2011 alexander et al 2017 prestele et al 2016 burnicki et al 2010 verburg et al 2011 2013 part of these uncertainties is related to the model structure and includes the way how land change processes e g the expansion of cropland or deforestation are modelled göpel et al 2018a others relate to the input data and include drivers of land change such as human population development and crop production which are typically derived from statistical databases or in case of scenario analyses from economic models e g dalla nora 2014 göpel et al 2018a one central element of uncertainty not yet considered in detail is the effect of model initialization on the calculated land use patterns alexander et al 2017 prestele et al 2016 looking beyond land use science we see that especially in climate research and hydrology there is already a large body of literature related to this topic krysanova et al 2017 tegegne et al 2020 touzé peiffer et al 2020 our study is a contribution to fill this research gap for land use science referring to the example of the global land use model landshift schaldach et al 2011 we investigate and especially quantify the effects of uncertainties related to model initialization on results obtained with this model two important elements of the landshift initialization process are taken into account 1 the selected global land cover map and 2 the method for calculating the values of model parameters which are used in landshift to determine suitable areas for the spatial allocation of land use types that are needed to fulfil the demand of crop and livestock production both elements are important factors in the modelling process göpel et al 2018a for instance the land cover map determines the location and spatial extent of natural land cover types such as forest and therefore has a direct effect on the calculation of environmental impact indicators e g göpel 2020 hinz et al 2020 objective of our study is to assess and quantify the impact of the above two elements of model initialization on the simulated changes of location and extent of cropland and pasture and in consequence on the loss of natural vegetation 2 methods and data 2 1 land use modelling as already noted we used the spatially explicit global land use model landshift for our study schaldach et al 2011 the model builds on the concept of land systems to describe the interplay between anthropogenic and environmental processes as causes of land change turner et al 2007 it has been applied and validated in various case studies among others for the african continent alcamo et al 2011 koch et al 2019 van soesbergen et al 2017 brazil lapola et al 2011 göpel et al 2018b asia mason d croz et al 2016 hinz et al 2020 europe humpenöder et al 2013 and the global level thrän et al 2016 landshift includes three modules for the land use activities urbanization crop cultivation and livestock grazing an additional module provides information on potential biomass yields of major crops and grazing lands this data serves as input to the crop cultivation and grazing modules and is calculated with the dynamic global vegetation model lpjml bondeau et al 2007 landshift operates on two hierarchically organized spatial scale levels 1 the macro level includes 190 countries country groups cf appendix a and is used to specify drivers of land change e g human population crop production in metric tons ruminant livestock numbers and information on crop yield increases due to technological change or improvement of agricultural management practices appendices b 3 to b 6 2 the micro level defines the geographic extent of each country as a rectangular raster with a cell size of 5 arc minutes 9 km 9 km at the equator each cell has one dominant land use cover type and comprises information about its potential biomass yields as well as information describing its socio economic and landscape characteristics including human population density road infrastructure terrain slope and constraints like protected areas appendices b 1 dominant land use cover types include cropland pasture urban and 7 types of natural vegetation appendix b 2 cropland is further assigned to one of 12 crop classes or marked as unused appendix b 4 landshift has two modes of operation 1 in initialization mode the starting conditions of a simulation experiment are set up section2 3 2 in simulation mode the actual simulation is performed over time both modes convert the macro level data into spatial land use patterns on the micro level difference is primarily that the initialization is based on data on physical cropland area and grazing area while the simulation is driven by data on crop production and livestock numbers in each time step the model first determines the preference of each micro level cell for a particular land use 12 crop classes and grazing by a multi criteria analysis mca eastman et al 1995 based on the cell s biomass productivity and socio economic landscape characteristics henceforth criteria more specifically a preference value ψ l u t k for a specific land use type lut and a cell k is determined with the following eq 1 eq 1 ψ l u t k i 1 n w i f i s i k s u i t a b i l i t y j 1 m c j k c o n s t r a i n t w i t h i 1 n w i 1 f i s i k 0 1 a n d c j k 0 1 in the suitability term n denotes the number of criteria n 8 for crop cultivation and n 5 for livestock grazing w i is a weight for the criterion s i e g terrain slope s7 that reflects its importance relatively to the other criteria s i k is the cell specific score of s i on cell k and f i is a value function that normalizes s i k to fit into 0 1 the constraint term covers m constraint criteria m 3 for both activities that define the non availability of a cell for a specific land use type where the boolean score c j k is the cell specific value of criterion c j on cell k a detailed description of how criteria are determined as part of the model initialization process is given in chapter 2 3 after all ψ l u t k are calculated crop production and livestock grazing are allocated to the raster cells with the highest preference value schaldach et al 2011 in the model setup used for our study crop production is allocated first followed by livestock grazing when determining the total area required to fulfil the crop production the biomass productivity of the different cells is taken into consideration schaldach et al 2011 similarly the allocation of livestock grazing relies on the potential biomass productivity of grassland in the grid cells which is determined on the basis of the livestock forage supply demand logic described by koch et al 2019 the allocation is done for each land use activity until the whole crop production and livestock numbers specified on the macro level have been distributed depending on the allocated livestock numbers grazing cells are classified either as intensive pasture and extensive rangeland only the former cause a change in land cover while the latter retain their natural land cover e g grassland or shrubland cf appendix b 5 model output is a series of global raster maps that can be further processed e g with a geographic information system gis maps are generated in 5 year time steps 2 2 input data 2 2 1 remote sensing land cover data we derived the land cover data from three state of the art global remote sensing datasets that are available for multiple time steps grekousis et al 2015 pérez hoyos et al 2017 1 the climate change initiative land cover dataset cci defourny et al 2017 has a spatial resolution of 300 m and a temporal coverage of the years 1992 2018 as of this writing it distinguishes 22 land cover classes based on the standardized land cover classification system lccs level 1 or global by di gregorio and jansen 2000 2 the moderate resolution imaging spectroradiometer land cover dataset modis sulla menashe et al 2019 has a spatial resolution of 500 m and a temporal coverage of the years 2001 2018 as of this writing here we selected the international geosphere biosphere programme igbp land cover classification scheme which distinguishes 17 land cover classes loveland and belward 1997 3 the globland30 land cover dataset gl30 chen et al 2017 has a spatial resolution of 30 m and a temporal coverage of the two years 2000 and 2010 in its latest version at study time the dataset uses its own classification system which distinguishes 10 land cover classes an overview of the main characteristics of the three datasets is given in table 1 the datasets were adapted to the requirements of landshift regarding 1 land cover classification and 2 micro level cell size for the former we translated the respective classification schemes into landshift s nine land cover classes by applying the lccs translation scheme based cross walking approach from tsendbazar et al 2015 the translation rules are explained in appendix b 2 for the latter we aggregated the raster cells to the 5 arc minutes cells of landshift with the help of the majority resampling filter of gdal ogr contributors 2020 this filter determines the most frequent value in the source raster cells 300 m 500 m or 30 m located inside the corresponding 5 arc minutes cell both steps were performed for the datasets of the years 2000 and 2010 in the case of modis we decided to use the 2001 map as no dataset was available for the year 2000 the uncertainties introduced by this aggregation are described in appendix b 2 the resulting year 2000 land cover maps henceforth reference maps 2000 were used for model initialization section 2 3 while the year 2010 land cover maps henceforth reference maps 2010 were used for comparison with the simulated land use patterns section 2 4 2 2 2 statistical land use data in addition to the land cover data discussed in the previous section landshift requires data on agricultural land use these data are needed as input for model initialization in the year 2000 and as drivers for the land change simulations between 2000 and 2010 see section 2 4 we derived this data from the database of the statistics division of the food and agriculture organization fao of the united nations faostat 2019 where it is provided on a per country basis and contains 1 annual physical cropland and grazing area 2 harvested area and production quantities of the 36 most relevant cultivated crops and crop aggregates 3 ruminant livestock numbers of cattle sheep and goats and 4 human population number appendix b 3 we used 3 year averages of the data for the years 2000 1999 2001 2005 2004 2006 and 2010 2009 2011 in order to compensate fluctuations between individual years moreover we grouped the 36 crops and crop aggregates into the 12 crop classes considered by landshift appendix b 4 during model initialization an additional preparation step is needed to determine the physical area of each crop class which is not contained in the faostat data instead they include the harvested area a c t h a r v per crop class ct and the total physical cropland area a c o u n t r y p h y s per country due to missing information on multi cropping that is required to calculate the physical area per crop we estimated the physical area a c t p h y s of crop class c t as the relative share of the total harvested area of all h crops h 36 according to eq 2 a c t p h y s a c t h a r v a c o u n t r y h a r v a c o u n t r y p h y s w i t h a c o u n t r y h a r v c t 1 h a c t h a r v the faostat data on grazing area can be used without any recalculation to prevent a disproportionate accounting of small crop and grazing areas crop classes with a recalculated a c t p h y s below half of the average cell area of the respective country were aggregated to the class other crops while left over grazing area below this threshold were not considered as a last step livestock numbers of the three types of ruminants cattle sheep goats which are used as drivers for the land change simulations were converted to livestock units to make them comparable between the different world regions appendix b 5 2 3 model initialization 2 3 1 overview the first step of model initialization is the determination of the parameter values for eq 1 then we generate one land use map per global remote sensing land cover dataset by merging the respective reference map 2000 cf section 2 2 1 with the faostat data on the physical area of crop types and grazing area cf section 2 2 2 the resulting map henceforth base map 2000 forms the starting point for the simulation runs 2 3 2 calculation of parameter values for the preference ranking as explained in section 2 1 landshift requires parameter values for the cell preference calculations namely weights w i and value functions f i see eq 1 in our study we determined these values for the agricultural land use activities crop cultivation and livestock grazing on the basis of the respective reference map 2000 we calculated the values independently for each of the macro level regions thereby we excluded regions with less than ten agricultural crop or grassland cells as no valid calculations were possible due to the lack of information this entails instead we used global estimates of the weights w i and value functions f i for these regions moreover we only considered cells that are potentially useable for crop cultivation and grazing thus excluding cells occupied by urban area snow ice and water bodies as well as cells that contain either marginal biomass yields or that are designated as protected areas the weights w i were determined by using either the critic diakoulaki et al 1995 or mean saaty 2008 method both methods are common to evaluate the criteria importance in multi criteria decision problems and both have previously been tested and evaluated in regional studies with the landshift model goepel et al 2018a lapola et al 2011 schaldach et al 2013 critic is a statistical estimation technique based on the measures of contrast intensity entropy in criterion and conflict correlation between criteria of the different landscape criteria s i in contrast mean strongly relies on the land cover information it considers the empirical relationship between two average values obtained by categorizing the values of a landscape criterion s i e g terrain slope with respect to the presence and absence of cropland or grassland in the respective reference map 2000 section 2 2 1 the value functions f i were determined by logistic regression considering the correlation between the landscape criteria and the occurrence of cropland or grassland ananth and kleinbaum 1997 mccullagh 1980 the logistic function indicates the range of the criterion in which the criterion scores significantly increase or decrease and thus the probability for the occurrence of cropland or grassland appendix c describes the methods in detail for most landscape criteria s i we considered all potentially possible scores with exceptions for the population s2 and crop yields s8 for the population s2 we only considered scores between 0 and 2000 inhabitants km2 as cells with a density over 2000 inhabitants km2 are declared as urban cells in landshift schaldach et al 2011 as the location of crop types is not yet included in the remote sensing land cover maps potential crop yields s8 were calculated as the average of our 12 crop classes using all cells with potential crop yields greater than zero and therefore potentially useable for agriculture detailed information on the data sources used for our study can be found in appendix b 1 2 3 3 generation of base maps in addition to the parameter values for the preference calculation landshift requires spatially explicit land use information as starting point for the land change simulations as this information is not contained in the reference maps we generate base maps for the year 2000 by merging the respective reference map 2000 with the faostat land use area statistics of the 12 crop classes and of grazing cf section 2 2 2 for that purpose we use a variant of landshift s allocation process with the weights w i and value functions f i calculated as described in section 2 3 2 the variant differs from the normal allocation procedure in two aspects first it assigns areas rather than production quantities making the resulting base map consistent with the faostat data and second it additionally ranks the preference values ψ l u t k by our land cover classes to ensure that a particular land use class e g wheat is preferably allocated onto cells with a more typical land cover for that land use e g cropland therefore eq 1 is extended by a rank term r lc k which depends on the land cover lc on cell k and dominates in the calculation eq 3 ψ l u t k i 1 n w i f i s i k s u i t a b i l i t y j 1 m c j k c o n s t r a i n t r l c k r a n k w i t h i 1 n w i 1 f i s i k 0 1 c j k 0 1 a n d r l c k ℕ r k 1 the rank order for crop cultivation is cropland r 4 shrubland grassland wetlands r 3 forest r 2 and barren or sparse vegetation r 1 and the rank order for grazing is grassland r 5 cropland r 4 shrubland wetlands r 3 forest r 2 and barren or sparse vegetation r 1 the rank orders are based on a transition analysis investigating changes to cropland or grassland in the three remote sensing land cover datasets cci modis gl30 between the years 2000 and 2010 as well as on assumptions on the suitability of the land cover classes for crop cultivation and grazing respectively as described in section 2 3 2 we only consider cells that are potentially available for these two land use activities 2 4 design and analysis of experiments 2 4 1 experimental design with our experiments we aim to investigate how model initialization with different remote sensing land cover datasets in combination with different parameter calculation methods affect the landshift results fig 1 shows a flowchart of the experimental design as depicted we construct a single model ensemble with six members which are initialized for the starting year 2000 with a specific land cover product cci modis or gl30 in combination with a specific method for parameter calculation mean or critic in the following we denote these ensemble members by cci mean cci critic modis mean modis critic gl30 mean and gl30 critic then with each ensemble member we simulate global land change between the years 2000 and 2010 in two 5 year time steps in the following the simulated maps for the year 2010 are addressed as change maps 2010 drivers of change for these simulations include macro level information on crop production in metric tons and livestock numbers section 2 3 recall that landshift converts this data to changes of both the areal extent and the location of agricultural land cropland and pasture based on the model outcomes we first assess the differences of extent and location of agricultural land among the ensemble members in the years 2000 and 2010 second we analyse for each ensemble member the spatial disagreement of agricultural land between modelled maps and reference maps third we assess differences in the loss of natural vegetation due to the simulated land change between 2000 and 2010 the three analyses are conducted for 12 world regions appendix a and on the global level 2 4 2 determine differences between the modelled maps of the ensemble members in our first type of analysis we use a gis to determine the differences between the six ensemble members 1 in the base maps 2000 and 2 in the change maps 2010 first we compare the agricultural area in the respective maps as a second aspect of the analysis we apply the gis raster calculator tool to calculate for each raster cell the frequency of occurrence of cropland and pasture for instance for the land use type cropland a frequency of 5 means that 5 out of the 6 ensemble members allocated a crop type to that cell 2 4 3 determine the disagreement between modelled maps and reference maps our second type of analysis compares for each ensemble member the agricultural area of its base map 2000 with its reference map 2000 and the agricultural area of its change map 2010 with its reference map 2010 the comparisons themselves are based on the measures of quantity disagreement qd and allocation disagreement ad as suggested by pontius jr and millones 2011 and pontius jr and santacruz 2014 respectively the qd measure expresses the difference between the total area of a specific land use cover type in the two maps disregarding the spatial distribution for instance a qd of 0 2 indicates that cropland area differs by 20 between the base map 2000 of an ensemble member and its reference map 2000 while a qd of 0 indicates a perfect match of the quantities in both maps the ad measure assesses the difference in the location of a specific land use cover type while considering only the correct quantities this means that ad only takes into account the area contained in both maps since the surplus area is already accounted in qd for instance an ad of 0 1 indicates that 10 of the cells in the base map 2000 are in a different location than in the respective reference map 2000 not counting for the 20 of cells from the previous example that appear in only one of these maps thus an ad of 0 indicates a perfect match in location although one of the maps may contain more cells of the type the sum of qd and ad is the total disagreement td a low td value indicates a good agreement between modelled and remote sensing maps 2 4 4 determine the loss of natural vegetation in our third type of analysis we use the gis raster calculator to assess the impact of the expansion of agricultural area on the loss of natural vegetation for each ensemble member we determine the area of forest shrubland and grassland converted to cropland or pasture during the simulation period we define a cell as converted if it has a natural land use type in the base map 2000 and one of twelve crop types or pasture in the change map 2010 3 results 3 1 differences between the modelled maps of the ensemble members in table 2 we compare the areas of the land use cover types in the base maps 2000 of the six ensemble members we find large differences in the area of natural land cover types for example forest area ranges from 40 4 million km2 gl30 mean to 46 4 million km2 modis mean and shrubland area from 6 7 million km2 gl30 critic to 12 6 million km2 modis mean this reflects discrepancies in the underlying remote sensing land cover datasets in contrast there is a very high agreement for cropland 15 4 million km2 and pasture 16 8 million km2 as these land use types were harmonized with faostat data during the model initialization section 2 3 3 despite this similarity the comparison of the six base maps by map overlay reveals large differences in the spatial pattern of cropland and pasture fig 2 a and c and table 3 only about 36 8 of cropland and 30 1 of pasture occur in five or six base maps 2000 blue and violet while 39 3 of cropland and 43 1 of pasture occur in only one or two base maps 2000 red and orange for cropland our analysis shows particularly high inconsistencies in parts of south america africa and central asia red and orange whereas a high agreement is found in north america europe india and china blue and purple the situation is different for pasture where inconsistencies mainly occur in north and south america europe and central africa whereas high agreement can be observed in large parts of asia australia and africa the region specific data is summarized in appendix d 1 analysing the changes from the base maps 2000 to the change maps 2010 on the global level we find an expansion of both cropland and pasture for all six ensemble members table 4 while the increase of pasture is in a similar order of magnitude 1 700 000 km2 strong differences between the ensemble members regarding cropland expansion 141 438 km2 592 259 km2 become obvious in addition we see that the increase of cropland area is higher for ensemble members using the mean method compared to the ensemble members using critic fig 2b and d shows the frequency of occurrence of cropland and pasture in the change maps 2010 comparing these values with those for the base maps we see changes in all frequency classes table 3 for cropland these changes are stronger pronounced between 4 32 and 3 73 than for pasture 0 93 2 24 in both cases the number of cells with only one calculated occurrence of cropland or pasture is increasing while the number of cells where all ensemble members agree is decreasing globally about 70 of the agricultural land use changes occur in the simulations of only one or two ensemble members appendix d 2 provides region specific data 3 2 disagreement between modelled maps and reference maps the quantity and allocation disagreement of cropland and pasture between the base maps 2000 and the corresponding reference maps 2000 are depicted in fig 3 the full set of numerical results is given in appendix d 3 on the global level fig 3 bottom td ranges from 0 20 cci critic and gl30 critic to 0 22 gl30 critic for the ensemble members using the cci and gl30 datasets td is dominated by ad or qd respectively in contrast for the ensemble members using the modis dataset ad is in a similar order of magnitude with qd for all ensemble members the effect of the parameter estimation method is small with slightly higher tds for critic on the regional level we find a higher variance some regions such as europe eur and brazil bra have low differences in td among the ensemble members while others such as canada can russia rus and australia anz have large differences in td for example for gl30 the td in can is barely present while cci and modis have significantly higher tds this strong discrepancy is also evident in anz where modis has a significantly lower td than cci and gl30 the regional findings confirm the global observation that the method used to calculate the model parameters for the preference analysis has only a rather small effect on td another remarkable aspect are the strong differences in qd and ad in many regions while td is nearly the same this is most noticeable in eur and anz for example in eur where the tds are nearly the same for the three land cover datasets gl30 has a high qd and a low ad whereas modis has a low qd and a high ad and cci lies somewhere in between analysing the disagreements of the change maps 2010 and reference maps 2010 we find only minor differences to the results for the year 2000 the highest change in td on the regional level amounts to 0 066 in ssa calculated with the ensemble member gl30 critic 3 3 loss of natural vegetation the simulated loss of natural vegetation due to the expansion of cropland and pasture differs between the ensemble members in some cases substantially fig 4 the detailed numbers are summarized in appendix d 4 on the global level the total losses are between 1 800 000 km2 cci critic and 2 900 000 km2 modis critic on the regional level we find similar tendencies for example in ssa the total loss is between 686 000 km2 cci critic and 1 060 000 km2 modis critic and in bra between 174 000 km2 gl30 critic and 315 000 km2 modis mean in all 12 regions the simulations based on modis indicate the highest losses the largest differences occur in regions with a strong expansion of cropland and pasture such as in ssa chn and bra whereas the results are more homogeneous in regions with only a small expansion such as in eur usa and anz our simulation results also show large differences in the converted area of the main types of natural vegetation which are forest grassland and shrubland global forest loss is by a factor of more than 1 5 higher in ensemble simulations based on modis 1 190 000 km2 than in ensemble simulations based on gl30 740 000 km2 or cci 680 000 km2 similar differences can also be observed in individual regions in particular in ssa osa and bra as for forest the highest global and regional losses of grassland are predicted by simulations based on modis followed by gl30 and cci for shrubland we see a different picture here the highest losses occur in simulations based on cci 390 000 km2 followed by gl30 185 000 km2 and modis 50 000 km2 this result is particularly interesting as modis has the largest initial shrubland area of the three datasets with 12 5 million km2 worldwide it is about 30 larger than in cci and twice as large as in gl30 cf table 2 on the regional level the strong difference in shrubland loss is also evident in ssa whereas in bra shrubland losses occur mainly in gl30 and are small in cci and almost non existent in modis 4 discussion 4 1 differences in the base maps an important element of the initialization of global spatially explicit land use models is the generation of an initial map that serves as starting point for simulation runs li 2017 van asselen and verburg 2013 in our study we set up a single model ensemble based on the landshift model for each ensemble member we produced a base map for the year 2000 that merges remote sensing land cover data with statistical data on agricultural land use we found that the extent location and type of natural land cover types varies strongly between the base maps 2000 of the ensemble members reflecting the inherent discrepancies of the underlying remote sensing land cover datasets e g due to the satellite sensors processing methods and classification systems applied for their generation congalton et al 2014 tsendbazar et al 2015 for agricultural area the discrepancies between the base maps 2000 are almost solely in location by merging the land cover maps with statistical land use data information on the areal extent of cropland and pasture is harmonized on the national level this consistency is a prerequisite for coupling landshift to economic models that for example in context of scenario analyses provide information on future changes of important input variables such as crop production and livestock numbers rosegrant 2017 thrän et al 2016 comparing the base maps 2000 of the ensemble members to their respective reference maps 2000 we see that model initialization leads to a disagreement between these two types of maps this can be explained mainly by existing differences between the cropland area of the remote sensing land cover map and the statistical land use data derived from faostat further our experiments indicate that the selection of the remote sensing land cover map has a stronger impact on the spatial pattern of the calculated base map 2000 than the selected model parameter estimation method looking at the individual regions we find a high variances of map disagreement with lower disagreement values indicating a better match between the remote sensing map and statistical land use data in this sense our study design allows to identify regional uncertainties due to the differences in the merged datasets that are apparent already in the model s starting conditions 4 2 land change and loss of natural vegetation the simulated land change between 2000 and 2010 shows large discrepancies in the expansion of cropland among the ensemble members the smallest expansion 68 658 km2 was calculated by modis critic the largest 592 259 km2 by cci mean in comparison the differences in pasture expansion are minor in contrast to the base map 2000 generation the described discrepancies are evident both for the underlying remote sensing products and the parameter estimation methods at the same time the map overlay of the change maps 2010 shows only relatively small changes in the frequency of occurrence classes of land use types most striking is that the area with only one occurrence of cropland slightly increases while the area with full agreement of the six ensemble members slightly decreases altogether these results support the findings from prestele et al 2016 who point out that the initial land cover map significantly influences the outcome of global land change simulations also the disagreement measures show only minor changes compared to the base maps this indicates that the differences between the remote sensing land cover maps and the statistical data inherent in the base map 2000 are still dominating over additional uncertainties introduced by the simulation of land change it is likely that variations between the ensemble members become larger when the simulation time frame is extended as for example doelman et al 2018 demonstrate in their scenario analysis of global land use dynamics until 2100 in consequence the variance of the calculated expansion of agricultural land leads to large discrepancies in the calculated loss of natural vegetation 1 1 million km2 on the global scale particularly in regions where in the year 2000 cropland area in the remote sensing and cover maps is larger than in the statistical data cells with land use type unused cropland are often transformed to cultivated cropland or pasture which prevents the conversion of natural vegetation since the loss of natural vegetation affects other environmental processes as well it is very likely that model initialization and in particular the selection of the remote sensing land cover dataset has a significant impact on for example calculated loss of biodiversity or changes in biogenic carbon pools e g balima et al 2020 dinerstein et al 2019 göpel 2020 hinz et al 2020 a more detailed analysis of these downstream effects was beyond the scope of our paper 4 3 insights from our analysis as pointed out in the previous paragraph our study exemplifies that the initialization process of the landshift model has a significant effect on the simulated land change and its related environmental impacts in this sense our results help 1 to assess uncertainties and mismatches in the data used to initialize our land use model and 2 to portray the resulting range of expansion of agricultural area respectively loss of natural vegetation we think that the quantification and transparent reporting of these uncertainties are an important element of simulation experiments to make land use models more applicable as policy and decision making tools as discussed for example by ascough 2008 and uusitalo et al 2015 with our experiments we have demonstrated how a model ensemble similar to state of the art approaches in other scientific disciplines such as climate science and hydrology e g krysanova et al 2017 tegegne et al 2020 touzé peiffer et al 2020 can be constructed and applied for this type of analysis in the case that time and resources do not permit the use of a full ensemble analysis our findings can also help to select a best suited model set up for global or regional analyses here the major selection criterion could be the smallest disagreement between statistical data and remote sensing land cover dataset both in the base map 2000 and the change map 2010 another promising approach to account for differences in remote sensing land cover data discussed in the literature is the construction of a hybrid map that combines the best data sources and remote sensing land cover maps for each region recent studies show that these hybrid or fusion maps provide a good or better land cover representation than the stand alone land cover datasets e g pérez hoyos et al 2020 in our opinion the main advantage of our ensemble approach is its higher transparency as it does not hide the inherent uncertainties of the underlying datasets moreover it allows to relatively easily include additional remote sensing land cover datasets future experiments with our model ensemble should incorporate at least one hybrid map to explore how this type of input data affects model results compared to the remote sensing land cover maps used in this study our experiments have also highlighted the need to further reduce the uncertainties in the remote sensing land cover datasets themselves szantoi et al 2020 improving these datasets is a steady process as remote sensing technology is rapidly evolving with new sensors and satellite missions becoming available feng and li 2020 wulder et al 2018 also the processing algorithms for land cover classification will improve with new methods and processing technologies building on approaches from the fields of artificial intelligence and deep learning e g gharbia et al 2021 4 4 limitations and further research needs first it is important to note that a systematic uncertainty analysis as described for example by pianosi et al 2016 or refsgaard et al 2007 was beyond the scope of this paper rather we concentrated on quantifying the effects that the choice of a remote sensing land cover map and parameter calculation method has on base map generation and land change simulations other uncertainties related to input data and model structure were not considered in particular our analyses rely on just one source of statistical data of agricultural production and area according to our knowledge faostat is the most comprehensive global database which is freely available and widely used for global modelling purposes e g anderson et al 2015 however as ramankutty et al 2008 point out its data quality in some countries especially regarding pasture area suffers from significant inaccuracies further we concentrated our analysis on the comparison of land use cover maps as direct model outputs future studies should take a deeper look into the model structure itself and compare the estimated parameter values and or preference values generated by the model e g göpel et al 2018a another limitation of our analysis is the relatively short time period of the simulations this was due to the situation that one of the remote sensing products was available only until the year 2010 gl30 at the time when we conducted our study a longer time horizon with higher land change would have helped to get a clearer picture of the impacts of model initialization on the simulated land change moreover our study setup was inconsistent to a certain degree as one land cover dataset modis was not available for 2000 but only for 2001 based on the use of three year averaged statistics for initialization this influence is unlikely to be significant nevertheless both limitations should be resolved in future studies the landshift model uses a relatively coarse resolution of 5 arc minutes and a relatively simplistic classification system of land use land cover types here additional uncertainties potentially arise from the pre processing to adapt the higher resolution source data to the model requirements according to herold et al 2008 the generalizations as they were done in this study are likely to have only little effect on the overall thematic accuracy at the global level because most of the land surface has a homogeneous landscape that can be readily classified and generalized we found that heterogeneous areas are slightly reduced at the cost of homogeneous areas but this affects all datasets to a similar degree and therefore should not have had a strong influence on the results of our comparison finally our study used only a single model ensemble although we took into account some uncertainties related to the way how the behaviour of land use activities is represented in the model by considering different parameter value estimation methods the use of multiple global land use models would greatly improve the overall quality and scope of the study similar to the current state of the art in climate modelling touzé peiffer et al 2020 multi model ensemble analyses and systematic inter comparison studies of spatial land change models should be an important aspect of future research e g alexander et al 2017 prestele et al 2016 5 conclusion from the authors point of view the use of a model ensemble is a suitable approach to address uncertainties related to model initialization as an important element to improve the scientific soundness and robustness of land change simulations it increases the transparency of the modelling process by clearly communicating these uncertainties to potential users of the simulation results e g planners or decision makers the variability of the simulated land change allows considering these uncertainties also in environmental downstream analyses software and data availability source code available on request output results provided in an open repository input data comes from open sources listed in the manuscript and the appendix declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements this study was conducted in the framework of the integrated projects bepaso grant number 031b0232c and transregbio grant number 031b0901f funded by the german federal ministry of education and research bmbf we thank our reviewers for their valuable comments that helped to improve our article appendix a supplementary data the following is the supplementary data to this article multimedia component 1 multimedia component 1 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2021 105287 
25673,global spatially explicit land system models are important tools to gain a better scientific understanding and to investigate future development trajectories in form of scenarios however uncertainties e g of input data and model structure affect their simulation results this article investigates the impact of model initialization on land change simulations and builds a single model ensemble based on the global land system model landshift each ensemble member is initialized with a unique combination of a remote sensing land cover dataset a specific method to derive model parameter values is used for land suitability evaluation to identify and quantify significant discrepancies between model outcomes obtained with different ensemble members the discrepancies related to both the land use maps generated as starting conditions of model runs and to the simulated land change and natural vegetation loss the results underline the importance to account for the above uncertainties in global simulation studies and communicating them transparently keywords model initialization global land use modelling uncertainties ensemble analysis global land change and loss of natural vegetation 1 introduction land use and land cover change henceforth land change has strong impacts on climate and environmental processes foley et al 2007 in light of the still growing human consumption of biomass for food feed fiber and energy the expansion and intensification of agriculture are important drivers of land change and its associated impacts such as the conversion of natural ecosystems biodiversity loss and carbon stock reduction erb et al 2016 hinz et al 2020 molotoks et al 2018 zabel et al 2019 to study land change on the global scale a number of spatially explicit land use models have been developed that integrate socio economic and environmental aspects to simulate historic patterns of land change as well as future scenarios national research council 2014 in a wide range of case studies these models have been deployed to gain a better understanding of land system dynamics turner 2017 verburg et al 2019 to assess environmental impacts e g lotze campen et al 2018 molotoks et al 2020 and to provide spatial information for planners and decision makers ipcc 2014 szantoi et al 2020 zagaria et al 2017 however the outcomes of these models are affected by uncertainties as was shown in a number of studies at the continental and global scale e g alcamo et al 2011 alexander et al 2017 prestele et al 2016 burnicki et al 2010 verburg et al 2011 2013 part of these uncertainties is related to the model structure and includes the way how land change processes e g the expansion of cropland or deforestation are modelled göpel et al 2018a others relate to the input data and include drivers of land change such as human population development and crop production which are typically derived from statistical databases or in case of scenario analyses from economic models e g dalla nora 2014 göpel et al 2018a one central element of uncertainty not yet considered in detail is the effect of model initialization on the calculated land use patterns alexander et al 2017 prestele et al 2016 looking beyond land use science we see that especially in climate research and hydrology there is already a large body of literature related to this topic krysanova et al 2017 tegegne et al 2020 touzé peiffer et al 2020 our study is a contribution to fill this research gap for land use science referring to the example of the global land use model landshift schaldach et al 2011 we investigate and especially quantify the effects of uncertainties related to model initialization on results obtained with this model two important elements of the landshift initialization process are taken into account 1 the selected global land cover map and 2 the method for calculating the values of model parameters which are used in landshift to determine suitable areas for the spatial allocation of land use types that are needed to fulfil the demand of crop and livestock production both elements are important factors in the modelling process göpel et al 2018a for instance the land cover map determines the location and spatial extent of natural land cover types such as forest and therefore has a direct effect on the calculation of environmental impact indicators e g göpel 2020 hinz et al 2020 objective of our study is to assess and quantify the impact of the above two elements of model initialization on the simulated changes of location and extent of cropland and pasture and in consequence on the loss of natural vegetation 2 methods and data 2 1 land use modelling as already noted we used the spatially explicit global land use model landshift for our study schaldach et al 2011 the model builds on the concept of land systems to describe the interplay between anthropogenic and environmental processes as causes of land change turner et al 2007 it has been applied and validated in various case studies among others for the african continent alcamo et al 2011 koch et al 2019 van soesbergen et al 2017 brazil lapola et al 2011 göpel et al 2018b asia mason d croz et al 2016 hinz et al 2020 europe humpenöder et al 2013 and the global level thrän et al 2016 landshift includes three modules for the land use activities urbanization crop cultivation and livestock grazing an additional module provides information on potential biomass yields of major crops and grazing lands this data serves as input to the crop cultivation and grazing modules and is calculated with the dynamic global vegetation model lpjml bondeau et al 2007 landshift operates on two hierarchically organized spatial scale levels 1 the macro level includes 190 countries country groups cf appendix a and is used to specify drivers of land change e g human population crop production in metric tons ruminant livestock numbers and information on crop yield increases due to technological change or improvement of agricultural management practices appendices b 3 to b 6 2 the micro level defines the geographic extent of each country as a rectangular raster with a cell size of 5 arc minutes 9 km 9 km at the equator each cell has one dominant land use cover type and comprises information about its potential biomass yields as well as information describing its socio economic and landscape characteristics including human population density road infrastructure terrain slope and constraints like protected areas appendices b 1 dominant land use cover types include cropland pasture urban and 7 types of natural vegetation appendix b 2 cropland is further assigned to one of 12 crop classes or marked as unused appendix b 4 landshift has two modes of operation 1 in initialization mode the starting conditions of a simulation experiment are set up section2 3 2 in simulation mode the actual simulation is performed over time both modes convert the macro level data into spatial land use patterns on the micro level difference is primarily that the initialization is based on data on physical cropland area and grazing area while the simulation is driven by data on crop production and livestock numbers in each time step the model first determines the preference of each micro level cell for a particular land use 12 crop classes and grazing by a multi criteria analysis mca eastman et al 1995 based on the cell s biomass productivity and socio economic landscape characteristics henceforth criteria more specifically a preference value ψ l u t k for a specific land use type lut and a cell k is determined with the following eq 1 eq 1 ψ l u t k i 1 n w i f i s i k s u i t a b i l i t y j 1 m c j k c o n s t r a i n t w i t h i 1 n w i 1 f i s i k 0 1 a n d c j k 0 1 in the suitability term n denotes the number of criteria n 8 for crop cultivation and n 5 for livestock grazing w i is a weight for the criterion s i e g terrain slope s7 that reflects its importance relatively to the other criteria s i k is the cell specific score of s i on cell k and f i is a value function that normalizes s i k to fit into 0 1 the constraint term covers m constraint criteria m 3 for both activities that define the non availability of a cell for a specific land use type where the boolean score c j k is the cell specific value of criterion c j on cell k a detailed description of how criteria are determined as part of the model initialization process is given in chapter 2 3 after all ψ l u t k are calculated crop production and livestock grazing are allocated to the raster cells with the highest preference value schaldach et al 2011 in the model setup used for our study crop production is allocated first followed by livestock grazing when determining the total area required to fulfil the crop production the biomass productivity of the different cells is taken into consideration schaldach et al 2011 similarly the allocation of livestock grazing relies on the potential biomass productivity of grassland in the grid cells which is determined on the basis of the livestock forage supply demand logic described by koch et al 2019 the allocation is done for each land use activity until the whole crop production and livestock numbers specified on the macro level have been distributed depending on the allocated livestock numbers grazing cells are classified either as intensive pasture and extensive rangeland only the former cause a change in land cover while the latter retain their natural land cover e g grassland or shrubland cf appendix b 5 model output is a series of global raster maps that can be further processed e g with a geographic information system gis maps are generated in 5 year time steps 2 2 input data 2 2 1 remote sensing land cover data we derived the land cover data from three state of the art global remote sensing datasets that are available for multiple time steps grekousis et al 2015 pérez hoyos et al 2017 1 the climate change initiative land cover dataset cci defourny et al 2017 has a spatial resolution of 300 m and a temporal coverage of the years 1992 2018 as of this writing it distinguishes 22 land cover classes based on the standardized land cover classification system lccs level 1 or global by di gregorio and jansen 2000 2 the moderate resolution imaging spectroradiometer land cover dataset modis sulla menashe et al 2019 has a spatial resolution of 500 m and a temporal coverage of the years 2001 2018 as of this writing here we selected the international geosphere biosphere programme igbp land cover classification scheme which distinguishes 17 land cover classes loveland and belward 1997 3 the globland30 land cover dataset gl30 chen et al 2017 has a spatial resolution of 30 m and a temporal coverage of the two years 2000 and 2010 in its latest version at study time the dataset uses its own classification system which distinguishes 10 land cover classes an overview of the main characteristics of the three datasets is given in table 1 the datasets were adapted to the requirements of landshift regarding 1 land cover classification and 2 micro level cell size for the former we translated the respective classification schemes into landshift s nine land cover classes by applying the lccs translation scheme based cross walking approach from tsendbazar et al 2015 the translation rules are explained in appendix b 2 for the latter we aggregated the raster cells to the 5 arc minutes cells of landshift with the help of the majority resampling filter of gdal ogr contributors 2020 this filter determines the most frequent value in the source raster cells 300 m 500 m or 30 m located inside the corresponding 5 arc minutes cell both steps were performed for the datasets of the years 2000 and 2010 in the case of modis we decided to use the 2001 map as no dataset was available for the year 2000 the uncertainties introduced by this aggregation are described in appendix b 2 the resulting year 2000 land cover maps henceforth reference maps 2000 were used for model initialization section 2 3 while the year 2010 land cover maps henceforth reference maps 2010 were used for comparison with the simulated land use patterns section 2 4 2 2 2 statistical land use data in addition to the land cover data discussed in the previous section landshift requires data on agricultural land use these data are needed as input for model initialization in the year 2000 and as drivers for the land change simulations between 2000 and 2010 see section 2 4 we derived this data from the database of the statistics division of the food and agriculture organization fao of the united nations faostat 2019 where it is provided on a per country basis and contains 1 annual physical cropland and grazing area 2 harvested area and production quantities of the 36 most relevant cultivated crops and crop aggregates 3 ruminant livestock numbers of cattle sheep and goats and 4 human population number appendix b 3 we used 3 year averages of the data for the years 2000 1999 2001 2005 2004 2006 and 2010 2009 2011 in order to compensate fluctuations between individual years moreover we grouped the 36 crops and crop aggregates into the 12 crop classes considered by landshift appendix b 4 during model initialization an additional preparation step is needed to determine the physical area of each crop class which is not contained in the faostat data instead they include the harvested area a c t h a r v per crop class ct and the total physical cropland area a c o u n t r y p h y s per country due to missing information on multi cropping that is required to calculate the physical area per crop we estimated the physical area a c t p h y s of crop class c t as the relative share of the total harvested area of all h crops h 36 according to eq 2 a c t p h y s a c t h a r v a c o u n t r y h a r v a c o u n t r y p h y s w i t h a c o u n t r y h a r v c t 1 h a c t h a r v the faostat data on grazing area can be used without any recalculation to prevent a disproportionate accounting of small crop and grazing areas crop classes with a recalculated a c t p h y s below half of the average cell area of the respective country were aggregated to the class other crops while left over grazing area below this threshold were not considered as a last step livestock numbers of the three types of ruminants cattle sheep goats which are used as drivers for the land change simulations were converted to livestock units to make them comparable between the different world regions appendix b 5 2 3 model initialization 2 3 1 overview the first step of model initialization is the determination of the parameter values for eq 1 then we generate one land use map per global remote sensing land cover dataset by merging the respective reference map 2000 cf section 2 2 1 with the faostat data on the physical area of crop types and grazing area cf section 2 2 2 the resulting map henceforth base map 2000 forms the starting point for the simulation runs 2 3 2 calculation of parameter values for the preference ranking as explained in section 2 1 landshift requires parameter values for the cell preference calculations namely weights w i and value functions f i see eq 1 in our study we determined these values for the agricultural land use activities crop cultivation and livestock grazing on the basis of the respective reference map 2000 we calculated the values independently for each of the macro level regions thereby we excluded regions with less than ten agricultural crop or grassland cells as no valid calculations were possible due to the lack of information this entails instead we used global estimates of the weights w i and value functions f i for these regions moreover we only considered cells that are potentially useable for crop cultivation and grazing thus excluding cells occupied by urban area snow ice and water bodies as well as cells that contain either marginal biomass yields or that are designated as protected areas the weights w i were determined by using either the critic diakoulaki et al 1995 or mean saaty 2008 method both methods are common to evaluate the criteria importance in multi criteria decision problems and both have previously been tested and evaluated in regional studies with the landshift model goepel et al 2018a lapola et al 2011 schaldach et al 2013 critic is a statistical estimation technique based on the measures of contrast intensity entropy in criterion and conflict correlation between criteria of the different landscape criteria s i in contrast mean strongly relies on the land cover information it considers the empirical relationship between two average values obtained by categorizing the values of a landscape criterion s i e g terrain slope with respect to the presence and absence of cropland or grassland in the respective reference map 2000 section 2 2 1 the value functions f i were determined by logistic regression considering the correlation between the landscape criteria and the occurrence of cropland or grassland ananth and kleinbaum 1997 mccullagh 1980 the logistic function indicates the range of the criterion in which the criterion scores significantly increase or decrease and thus the probability for the occurrence of cropland or grassland appendix c describes the methods in detail for most landscape criteria s i we considered all potentially possible scores with exceptions for the population s2 and crop yields s8 for the population s2 we only considered scores between 0 and 2000 inhabitants km2 as cells with a density over 2000 inhabitants km2 are declared as urban cells in landshift schaldach et al 2011 as the location of crop types is not yet included in the remote sensing land cover maps potential crop yields s8 were calculated as the average of our 12 crop classes using all cells with potential crop yields greater than zero and therefore potentially useable for agriculture detailed information on the data sources used for our study can be found in appendix b 1 2 3 3 generation of base maps in addition to the parameter values for the preference calculation landshift requires spatially explicit land use information as starting point for the land change simulations as this information is not contained in the reference maps we generate base maps for the year 2000 by merging the respective reference map 2000 with the faostat land use area statistics of the 12 crop classes and of grazing cf section 2 2 2 for that purpose we use a variant of landshift s allocation process with the weights w i and value functions f i calculated as described in section 2 3 2 the variant differs from the normal allocation procedure in two aspects first it assigns areas rather than production quantities making the resulting base map consistent with the faostat data and second it additionally ranks the preference values ψ l u t k by our land cover classes to ensure that a particular land use class e g wheat is preferably allocated onto cells with a more typical land cover for that land use e g cropland therefore eq 1 is extended by a rank term r lc k which depends on the land cover lc on cell k and dominates in the calculation eq 3 ψ l u t k i 1 n w i f i s i k s u i t a b i l i t y j 1 m c j k c o n s t r a i n t r l c k r a n k w i t h i 1 n w i 1 f i s i k 0 1 c j k 0 1 a n d r l c k ℕ r k 1 the rank order for crop cultivation is cropland r 4 shrubland grassland wetlands r 3 forest r 2 and barren or sparse vegetation r 1 and the rank order for grazing is grassland r 5 cropland r 4 shrubland wetlands r 3 forest r 2 and barren or sparse vegetation r 1 the rank orders are based on a transition analysis investigating changes to cropland or grassland in the three remote sensing land cover datasets cci modis gl30 between the years 2000 and 2010 as well as on assumptions on the suitability of the land cover classes for crop cultivation and grazing respectively as described in section 2 3 2 we only consider cells that are potentially available for these two land use activities 2 4 design and analysis of experiments 2 4 1 experimental design with our experiments we aim to investigate how model initialization with different remote sensing land cover datasets in combination with different parameter calculation methods affect the landshift results fig 1 shows a flowchart of the experimental design as depicted we construct a single model ensemble with six members which are initialized for the starting year 2000 with a specific land cover product cci modis or gl30 in combination with a specific method for parameter calculation mean or critic in the following we denote these ensemble members by cci mean cci critic modis mean modis critic gl30 mean and gl30 critic then with each ensemble member we simulate global land change between the years 2000 and 2010 in two 5 year time steps in the following the simulated maps for the year 2010 are addressed as change maps 2010 drivers of change for these simulations include macro level information on crop production in metric tons and livestock numbers section 2 3 recall that landshift converts this data to changes of both the areal extent and the location of agricultural land cropland and pasture based on the model outcomes we first assess the differences of extent and location of agricultural land among the ensemble members in the years 2000 and 2010 second we analyse for each ensemble member the spatial disagreement of agricultural land between modelled maps and reference maps third we assess differences in the loss of natural vegetation due to the simulated land change between 2000 and 2010 the three analyses are conducted for 12 world regions appendix a and on the global level 2 4 2 determine differences between the modelled maps of the ensemble members in our first type of analysis we use a gis to determine the differences between the six ensemble members 1 in the base maps 2000 and 2 in the change maps 2010 first we compare the agricultural area in the respective maps as a second aspect of the analysis we apply the gis raster calculator tool to calculate for each raster cell the frequency of occurrence of cropland and pasture for instance for the land use type cropland a frequency of 5 means that 5 out of the 6 ensemble members allocated a crop type to that cell 2 4 3 determine the disagreement between modelled maps and reference maps our second type of analysis compares for each ensemble member the agricultural area of its base map 2000 with its reference map 2000 and the agricultural area of its change map 2010 with its reference map 2010 the comparisons themselves are based on the measures of quantity disagreement qd and allocation disagreement ad as suggested by pontius jr and millones 2011 and pontius jr and santacruz 2014 respectively the qd measure expresses the difference between the total area of a specific land use cover type in the two maps disregarding the spatial distribution for instance a qd of 0 2 indicates that cropland area differs by 20 between the base map 2000 of an ensemble member and its reference map 2000 while a qd of 0 indicates a perfect match of the quantities in both maps the ad measure assesses the difference in the location of a specific land use cover type while considering only the correct quantities this means that ad only takes into account the area contained in both maps since the surplus area is already accounted in qd for instance an ad of 0 1 indicates that 10 of the cells in the base map 2000 are in a different location than in the respective reference map 2000 not counting for the 20 of cells from the previous example that appear in only one of these maps thus an ad of 0 indicates a perfect match in location although one of the maps may contain more cells of the type the sum of qd and ad is the total disagreement td a low td value indicates a good agreement between modelled and remote sensing maps 2 4 4 determine the loss of natural vegetation in our third type of analysis we use the gis raster calculator to assess the impact of the expansion of agricultural area on the loss of natural vegetation for each ensemble member we determine the area of forest shrubland and grassland converted to cropland or pasture during the simulation period we define a cell as converted if it has a natural land use type in the base map 2000 and one of twelve crop types or pasture in the change map 2010 3 results 3 1 differences between the modelled maps of the ensemble members in table 2 we compare the areas of the land use cover types in the base maps 2000 of the six ensemble members we find large differences in the area of natural land cover types for example forest area ranges from 40 4 million km2 gl30 mean to 46 4 million km2 modis mean and shrubland area from 6 7 million km2 gl30 critic to 12 6 million km2 modis mean this reflects discrepancies in the underlying remote sensing land cover datasets in contrast there is a very high agreement for cropland 15 4 million km2 and pasture 16 8 million km2 as these land use types were harmonized with faostat data during the model initialization section 2 3 3 despite this similarity the comparison of the six base maps by map overlay reveals large differences in the spatial pattern of cropland and pasture fig 2 a and c and table 3 only about 36 8 of cropland and 30 1 of pasture occur in five or six base maps 2000 blue and violet while 39 3 of cropland and 43 1 of pasture occur in only one or two base maps 2000 red and orange for cropland our analysis shows particularly high inconsistencies in parts of south america africa and central asia red and orange whereas a high agreement is found in north america europe india and china blue and purple the situation is different for pasture where inconsistencies mainly occur in north and south america europe and central africa whereas high agreement can be observed in large parts of asia australia and africa the region specific data is summarized in appendix d 1 analysing the changes from the base maps 2000 to the change maps 2010 on the global level we find an expansion of both cropland and pasture for all six ensemble members table 4 while the increase of pasture is in a similar order of magnitude 1 700 000 km2 strong differences between the ensemble members regarding cropland expansion 141 438 km2 592 259 km2 become obvious in addition we see that the increase of cropland area is higher for ensemble members using the mean method compared to the ensemble members using critic fig 2b and d shows the frequency of occurrence of cropland and pasture in the change maps 2010 comparing these values with those for the base maps we see changes in all frequency classes table 3 for cropland these changes are stronger pronounced between 4 32 and 3 73 than for pasture 0 93 2 24 in both cases the number of cells with only one calculated occurrence of cropland or pasture is increasing while the number of cells where all ensemble members agree is decreasing globally about 70 of the agricultural land use changes occur in the simulations of only one or two ensemble members appendix d 2 provides region specific data 3 2 disagreement between modelled maps and reference maps the quantity and allocation disagreement of cropland and pasture between the base maps 2000 and the corresponding reference maps 2000 are depicted in fig 3 the full set of numerical results is given in appendix d 3 on the global level fig 3 bottom td ranges from 0 20 cci critic and gl30 critic to 0 22 gl30 critic for the ensemble members using the cci and gl30 datasets td is dominated by ad or qd respectively in contrast for the ensemble members using the modis dataset ad is in a similar order of magnitude with qd for all ensemble members the effect of the parameter estimation method is small with slightly higher tds for critic on the regional level we find a higher variance some regions such as europe eur and brazil bra have low differences in td among the ensemble members while others such as canada can russia rus and australia anz have large differences in td for example for gl30 the td in can is barely present while cci and modis have significantly higher tds this strong discrepancy is also evident in anz where modis has a significantly lower td than cci and gl30 the regional findings confirm the global observation that the method used to calculate the model parameters for the preference analysis has only a rather small effect on td another remarkable aspect are the strong differences in qd and ad in many regions while td is nearly the same this is most noticeable in eur and anz for example in eur where the tds are nearly the same for the three land cover datasets gl30 has a high qd and a low ad whereas modis has a low qd and a high ad and cci lies somewhere in between analysing the disagreements of the change maps 2010 and reference maps 2010 we find only minor differences to the results for the year 2000 the highest change in td on the regional level amounts to 0 066 in ssa calculated with the ensemble member gl30 critic 3 3 loss of natural vegetation the simulated loss of natural vegetation due to the expansion of cropland and pasture differs between the ensemble members in some cases substantially fig 4 the detailed numbers are summarized in appendix d 4 on the global level the total losses are between 1 800 000 km2 cci critic and 2 900 000 km2 modis critic on the regional level we find similar tendencies for example in ssa the total loss is between 686 000 km2 cci critic and 1 060 000 km2 modis critic and in bra between 174 000 km2 gl30 critic and 315 000 km2 modis mean in all 12 regions the simulations based on modis indicate the highest losses the largest differences occur in regions with a strong expansion of cropland and pasture such as in ssa chn and bra whereas the results are more homogeneous in regions with only a small expansion such as in eur usa and anz our simulation results also show large differences in the converted area of the main types of natural vegetation which are forest grassland and shrubland global forest loss is by a factor of more than 1 5 higher in ensemble simulations based on modis 1 190 000 km2 than in ensemble simulations based on gl30 740 000 km2 or cci 680 000 km2 similar differences can also be observed in individual regions in particular in ssa osa and bra as for forest the highest global and regional losses of grassland are predicted by simulations based on modis followed by gl30 and cci for shrubland we see a different picture here the highest losses occur in simulations based on cci 390 000 km2 followed by gl30 185 000 km2 and modis 50 000 km2 this result is particularly interesting as modis has the largest initial shrubland area of the three datasets with 12 5 million km2 worldwide it is about 30 larger than in cci and twice as large as in gl30 cf table 2 on the regional level the strong difference in shrubland loss is also evident in ssa whereas in bra shrubland losses occur mainly in gl30 and are small in cci and almost non existent in modis 4 discussion 4 1 differences in the base maps an important element of the initialization of global spatially explicit land use models is the generation of an initial map that serves as starting point for simulation runs li 2017 van asselen and verburg 2013 in our study we set up a single model ensemble based on the landshift model for each ensemble member we produced a base map for the year 2000 that merges remote sensing land cover data with statistical data on agricultural land use we found that the extent location and type of natural land cover types varies strongly between the base maps 2000 of the ensemble members reflecting the inherent discrepancies of the underlying remote sensing land cover datasets e g due to the satellite sensors processing methods and classification systems applied for their generation congalton et al 2014 tsendbazar et al 2015 for agricultural area the discrepancies between the base maps 2000 are almost solely in location by merging the land cover maps with statistical land use data information on the areal extent of cropland and pasture is harmonized on the national level this consistency is a prerequisite for coupling landshift to economic models that for example in context of scenario analyses provide information on future changes of important input variables such as crop production and livestock numbers rosegrant 2017 thrän et al 2016 comparing the base maps 2000 of the ensemble members to their respective reference maps 2000 we see that model initialization leads to a disagreement between these two types of maps this can be explained mainly by existing differences between the cropland area of the remote sensing land cover map and the statistical land use data derived from faostat further our experiments indicate that the selection of the remote sensing land cover map has a stronger impact on the spatial pattern of the calculated base map 2000 than the selected model parameter estimation method looking at the individual regions we find a high variances of map disagreement with lower disagreement values indicating a better match between the remote sensing map and statistical land use data in this sense our study design allows to identify regional uncertainties due to the differences in the merged datasets that are apparent already in the model s starting conditions 4 2 land change and loss of natural vegetation the simulated land change between 2000 and 2010 shows large discrepancies in the expansion of cropland among the ensemble members the smallest expansion 68 658 km2 was calculated by modis critic the largest 592 259 km2 by cci mean in comparison the differences in pasture expansion are minor in contrast to the base map 2000 generation the described discrepancies are evident both for the underlying remote sensing products and the parameter estimation methods at the same time the map overlay of the change maps 2010 shows only relatively small changes in the frequency of occurrence classes of land use types most striking is that the area with only one occurrence of cropland slightly increases while the area with full agreement of the six ensemble members slightly decreases altogether these results support the findings from prestele et al 2016 who point out that the initial land cover map significantly influences the outcome of global land change simulations also the disagreement measures show only minor changes compared to the base maps this indicates that the differences between the remote sensing land cover maps and the statistical data inherent in the base map 2000 are still dominating over additional uncertainties introduced by the simulation of land change it is likely that variations between the ensemble members become larger when the simulation time frame is extended as for example doelman et al 2018 demonstrate in their scenario analysis of global land use dynamics until 2100 in consequence the variance of the calculated expansion of agricultural land leads to large discrepancies in the calculated loss of natural vegetation 1 1 million km2 on the global scale particularly in regions where in the year 2000 cropland area in the remote sensing and cover maps is larger than in the statistical data cells with land use type unused cropland are often transformed to cultivated cropland or pasture which prevents the conversion of natural vegetation since the loss of natural vegetation affects other environmental processes as well it is very likely that model initialization and in particular the selection of the remote sensing land cover dataset has a significant impact on for example calculated loss of biodiversity or changes in biogenic carbon pools e g balima et al 2020 dinerstein et al 2019 göpel 2020 hinz et al 2020 a more detailed analysis of these downstream effects was beyond the scope of our paper 4 3 insights from our analysis as pointed out in the previous paragraph our study exemplifies that the initialization process of the landshift model has a significant effect on the simulated land change and its related environmental impacts in this sense our results help 1 to assess uncertainties and mismatches in the data used to initialize our land use model and 2 to portray the resulting range of expansion of agricultural area respectively loss of natural vegetation we think that the quantification and transparent reporting of these uncertainties are an important element of simulation experiments to make land use models more applicable as policy and decision making tools as discussed for example by ascough 2008 and uusitalo et al 2015 with our experiments we have demonstrated how a model ensemble similar to state of the art approaches in other scientific disciplines such as climate science and hydrology e g krysanova et al 2017 tegegne et al 2020 touzé peiffer et al 2020 can be constructed and applied for this type of analysis in the case that time and resources do not permit the use of a full ensemble analysis our findings can also help to select a best suited model set up for global or regional analyses here the major selection criterion could be the smallest disagreement between statistical data and remote sensing land cover dataset both in the base map 2000 and the change map 2010 another promising approach to account for differences in remote sensing land cover data discussed in the literature is the construction of a hybrid map that combines the best data sources and remote sensing land cover maps for each region recent studies show that these hybrid or fusion maps provide a good or better land cover representation than the stand alone land cover datasets e g pérez hoyos et al 2020 in our opinion the main advantage of our ensemble approach is its higher transparency as it does not hide the inherent uncertainties of the underlying datasets moreover it allows to relatively easily include additional remote sensing land cover datasets future experiments with our model ensemble should incorporate at least one hybrid map to explore how this type of input data affects model results compared to the remote sensing land cover maps used in this study our experiments have also highlighted the need to further reduce the uncertainties in the remote sensing land cover datasets themselves szantoi et al 2020 improving these datasets is a steady process as remote sensing technology is rapidly evolving with new sensors and satellite missions becoming available feng and li 2020 wulder et al 2018 also the processing algorithms for land cover classification will improve with new methods and processing technologies building on approaches from the fields of artificial intelligence and deep learning e g gharbia et al 2021 4 4 limitations and further research needs first it is important to note that a systematic uncertainty analysis as described for example by pianosi et al 2016 or refsgaard et al 2007 was beyond the scope of this paper rather we concentrated on quantifying the effects that the choice of a remote sensing land cover map and parameter calculation method has on base map generation and land change simulations other uncertainties related to input data and model structure were not considered in particular our analyses rely on just one source of statistical data of agricultural production and area according to our knowledge faostat is the most comprehensive global database which is freely available and widely used for global modelling purposes e g anderson et al 2015 however as ramankutty et al 2008 point out its data quality in some countries especially regarding pasture area suffers from significant inaccuracies further we concentrated our analysis on the comparison of land use cover maps as direct model outputs future studies should take a deeper look into the model structure itself and compare the estimated parameter values and or preference values generated by the model e g göpel et al 2018a another limitation of our analysis is the relatively short time period of the simulations this was due to the situation that one of the remote sensing products was available only until the year 2010 gl30 at the time when we conducted our study a longer time horizon with higher land change would have helped to get a clearer picture of the impacts of model initialization on the simulated land change moreover our study setup was inconsistent to a certain degree as one land cover dataset modis was not available for 2000 but only for 2001 based on the use of three year averaged statistics for initialization this influence is unlikely to be significant nevertheless both limitations should be resolved in future studies the landshift model uses a relatively coarse resolution of 5 arc minutes and a relatively simplistic classification system of land use land cover types here additional uncertainties potentially arise from the pre processing to adapt the higher resolution source data to the model requirements according to herold et al 2008 the generalizations as they were done in this study are likely to have only little effect on the overall thematic accuracy at the global level because most of the land surface has a homogeneous landscape that can be readily classified and generalized we found that heterogeneous areas are slightly reduced at the cost of homogeneous areas but this affects all datasets to a similar degree and therefore should not have had a strong influence on the results of our comparison finally our study used only a single model ensemble although we took into account some uncertainties related to the way how the behaviour of land use activities is represented in the model by considering different parameter value estimation methods the use of multiple global land use models would greatly improve the overall quality and scope of the study similar to the current state of the art in climate modelling touzé peiffer et al 2020 multi model ensemble analyses and systematic inter comparison studies of spatial land change models should be an important aspect of future research e g alexander et al 2017 prestele et al 2016 5 conclusion from the authors point of view the use of a model ensemble is a suitable approach to address uncertainties related to model initialization as an important element to improve the scientific soundness and robustness of land change simulations it increases the transparency of the modelling process by clearly communicating these uncertainties to potential users of the simulation results e g planners or decision makers the variability of the simulated land change allows considering these uncertainties also in environmental downstream analyses software and data availability source code available on request output results provided in an open repository input data comes from open sources listed in the manuscript and the appendix declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements this study was conducted in the framework of the integrated projects bepaso grant number 031b0232c and transregbio grant number 031b0901f funded by the german federal ministry of education and research bmbf we thank our reviewers for their valuable comments that helped to improve our article appendix a supplementary data the following is the supplementary data to this article multimedia component 1 multimedia component 1 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2021 105287 
25674,global sensitivity analysis gsa is a valuable tool for filtering out non influential model inputs in combination with robustness convergence and validation analyses gsa can be particularly beneficial in interpreting and simplifying models with tens of thousands of independent inputs however there is lack of research on robust screening of such large models where the curse of dimensionality can make existing analyses obsolete we aim to close this gap by evaluating the computational performance of spearman rank correlation coefficients sobol and delta indices and gradient boosted trees regression numerical experiments are conducted for the morris test function and a life cycle assessment model with 10 000 inputs each our results enable us to recommend a standardized procedure for higher dimensional models which efficiently tests for model linearity gsa screening and convergence and robustness analyses of sensitivity indices screening and rankings graphical abstract image 1 keywords global sensitivity analysis high dimensional models robustness convergence validation life cycle assessment environmental impact assessment 1 introduction life cycle assessment lca is a well established tool used for quantification of the environmental impact of goods and services throughout the global value chain it supports environmentally informed decisions in policy making product development and consumer choices hellweg and i canals 2014 however lca results can be highly uncertain due in part to the large amounts of measured and simulated data that are used therefore results can only be interpreted confidently if this uncertainty is sufficiently narrow the number of uncertain inputs in lca models can reach hundreds of thousands because each model incorporates global regionally resolved supply chains of a wide variety of products and services ranging from for example desktop computers in switzerland to power plants in china improving the quality of all of them is not only infeasible but also unnecessary because of the sparsity of factors paradigm a heuristic that only a small fraction of factors influences the model output uncertainty razavi et al 2020 in order to support prioritized data collection we use sensitivity analysis sa the study of how uncertainty in the output of a model numerical or otherwise can be apportioned to different sources of uncertainty in the model input saltelli et al 2008 model inputs are often called factors in the sa field broadly speaking sa can be categorized into i local analysis where only one factor is varied at a time ii screening procedures such as morris method that is based on averaging model outputs from multiple local changes in each factor morris 1991 campolongo et al 2011 and iii global techniques where all factors are varied simultaneously global sensitivity analysis gsa is a more comprehensive technique that can account for interaction effects between combinations of factors and is nowadays computationally feasible for more and more models therefore we will only consider global analysis in this paper unlike problems with a definite correct solution the outcome of sa depends on the purpose of the analysis saltelli et al 2008 define setting as a way of framing the sensitivity quest in such a way that the answer can be confidently entrusted to a well identified measure or in other words to sensitivity index per each model input that is computed with the chosen sa method commonly recognized settings are i factor prioritization in which the aim is to obtain ranked list of factors according to their importance ii factor fixing screening or dimensionality reduction to identify redundant factors that can be fixed to any value without significantly affecting model output iii factor mapping in case we are interested in a particular portion of output distribution iv variance cutting to ensure that the model output variance is under a certain threshold the focus of this study lies in the screening setting one of the first applications of gsa in lca was conducted by lo et al 2005 to compute contribution to variance sensitivity indices and determine key inputs in the municipal waste management lca model the authors demonstrated that uncertainty of the impact estimate can be reduced by almost 50 by collecting site specific data padey et al 2013 then proposed a generic methodology for construction of simplified lca models they calculated sobol indices to determine the most influential factors in an lca model and used them to derive a robust but simple tool for assessing environmental impacts from onshore wind turbines these are typical applications of gsa in lca with the goals of uncertainty reduction and model simplification see pfister and scherer 2015 lacirignola et al 2017 blanco et al 2020 for similar studies a complete protocol for conducting gsa in lca was given by cucurachi et al 2016 who performed sa by first screening the input space and fixing non influential factors to default values and then ranking the remaining factors based on their importance typical gsa applications deal with complex models that have tens to hundreds of inputs but can take minutes or hours per model run see saltelli et al 2004 for more applications there exist various cost effective gsa strategies for such computationally expensive nonlinear models which include improved sampling techniques use of emulators or surrogate models to replace the original model with a faster one and given data approaches that do not rely on a particular sampling strategy and hence can reuse model runs from other analyses sheikholeslami et al 2021 a notable property of lca models however is that one run is computationally very cheap requiring less than 1 second of cpu time moreover all inputs in lca are assumed to be independent and sampled as such given the data in lca models this assumption is reasonable in many cases the rice yield in uttar pradesh has no correlation to coal combustion efficiency in kentucky though we note that in some cases this assumption of independence is due to missing data particularly in the case of single datasets clear relationships between individual inputs and outputs e g fuel consumption and co2 production irrigation water consumed and water evapotranspirated could be included however such techniques are still early in development see lesage et al 2019 lesage 2021a lesage 2021b and are not covered in this work the combination of many independent parameters and quick model evaluations make the application of gsa to lca models unique and interesting due to historically limited computational resources most of the existing works applied gsa only to partial lca models with tens or a maximum of a few hundreds of factors to the best of our knowledge only one work applied a two step gsa protocol to a complete lca model with 30 000 inputs mutel et al 2013 key challenges in the state of the art gsa razavi et al 2020 include computational performance robustness convergence of gsa methods and validation of results and these aspects are also crucial parts of the analysis of high dimensional models only a limited number of studies have addressed the issues of convergence and robustness of gsa indices yang 2011 proposed two procedures to monitor uncertainty in estimation of indices for five gsa methods computation of confidence intervals using the central limit theorem and bootstrap resampling sarrazin et al 2016 focused on quantitative metrics for two important choices number of model runs and threshold for non influential indices sheikholeslami et al 2019 showed that convergence can be achieved much faster by grouping factors based on the proposed bootstrap based clustering while showing promising results no studies have been carried out to investigate applicability of these methods to models with tens or hundreds of thousands of inputs moreover there is a clear lack of benchmark functions that allow testing of gsa methods on fast high dimensional models most functions contain a fixed number of inputs while others allow us to define model dimensionality but restrict it to a certain maximum standard practice in introducing more dimensions is to add dummy variables with zero influence on the model output morris et al 2006 this is not aligned with models that represent our understanding of the real world and can have numerous lowly influential or noisy variables therefore the objective of this work is to investigate the scalability of common gsa methods for screening of very high dimensional models in combination with robustness and convergence analysis and compare their performance on a reasonable test model and an lca case study the output of this analysis will reveal issues that we can only encounter as the number of model inputs increases and aid in deriving standardized procedure for robust and efficient high dimensional screening depending on the complexity of the model the structure of the paper is as follows first section 2 1 looks at computational performance of gsa methods covering convergence robustness and validation of gsa then in section 2 2 we provide a description of three classical gsa methods spearman correlations sobol indices and delta method and study their performance in terms of memory speed and applicability to factor fixing setting section 2 3 gives an overview of dimensionality reduction and observes these properties for one feature selection method gradient boosted trees then section 2 4 describes a method for estimating degree of model linearity in section 2 5 we propose a new morris test model that is adjusted to be suitable as a benchmark in high dimensions and outline an lca case study that assesses global warming environmental impacts of average swiss household food consumption in section 2 6 we provide details on the developed software that we used for numerical simulations we investigate scalability of gsa methods on the modified morris models with 1 000 5 000 and 10 000 inputs as well as study performance of all methods on the lca model in section 3 finally we discuss results in section 4 and provide conclusions and outlook in section 5 together with clear procedure that combines efficient assessment of the degree of model linearity and robust high dimensional screening mathematical notation is given in table 1 2 methods 2 1 computational performance of gsa methods given the complexity of analytical computation of sensitivity indices for nonlinear high dimensional models sampling based methods are the only feasible tools to investigate importance of inputs saltelli et al 2008 a typical approach for analyzing a function y g x g x 1 x k with k inputs is by means of monte carlo mc simulations which involves i creating an input sampling x x i i 1 n ii computing model response y g x i i 1 n at each point x i where n is the number of simulations iii estimating sensitivity indices by using x y tuple or only model outputs y computational burden of model runs is usually the most expensive part of the analysis the amount of time highly depends on the complexity of the model but in most cases this step can be easily parallelized as a general rule methods that need both x and y to compute sensitivity indices tend to require more memory resources and runtime as there are more data to handle on the other hand methods with elaborate sampling designs such as sobol quasi random sequences need more time to generate x and demand more model runs but are very efficient at computing the indices broadly speaking efficiency refers to the total computational resources an algorithm needs which most commonly are memory and runtime the field of analysis of algorithms studies these notions extensively sedgewick and flajolet 2013 in this paper we limit ourselves to a more practical benchmark approach which compares relative performance of algorithms when applied to specific test cases more concretely we want to address scalability of gsa methods as the number of model inputs increases to tens of thousands since the indices are evaluated on a limited number of samples they are also subject to uncertainties therefore notions of convergence and robustness become of great importance pianosi et al 2016 convergence analysis shows how gsa results change as the number of samples increases robustness or stability in the gsa field can be viewed as the ability of an algorithm to produce consistent results under similar conditions e g same number of mc iterations but varying other parameters such as x y tuple chandrashekar and sahin 2014 apart from efficiency convergence and robustness analyses a complete assessment of computational performance requires consideration of reliability of obtained gsa results or in other words their correctness for simple test models true sensitivity indices can be computed analytically but for more complex problems the only option is to numerically validate results in the rest of the section we address quantitative metrics to monitor computational performance of algorithms 2 1 1 convergence and robustness with respect to convergence and given different gsa settings one might be interested in the following aspects convergence of sensitivity indices convergence of screening and convergence of model inputs ranking this analysis can answer the question of how many samples are needed for the gsa setting of interest sarrazin et al 2016 at the same time one can study robustness of algorithms by assessing confidence intervals of an estimated statistic this can be done by i redoing the entire gsa multiple times and applying central limit theorem or ii employing bootstrap with resampling efron and tibshirani 1994 in this work we follow the second approach since it does not require additional sampling designs and new model runs and hence is more computationally efficient yang 2011 if possible it is recommended to use 1 000 or more bootstrap samples efron and tibshirani 1994 oftentimes convergence and robustness analyses are performed together for multiple mc iterations or in other words at multiple convergence steps these steps should be chosen in a range from the minimum number of iterations that can produce sensitivity results to a recommended or computationally feasible maximum number of model runs and with a desired step size or number of steps in this work at each convergence step c which ranges from 1 to the predefined number of steps with the corresponding number of iterations n c we compute the statistic of interest for the first n c simulations in x y we then perform bootstrap analysis by sampling with replacement from these n c simulations to obtain another set of samples of size n c for which we recompute the statistic of interest finally we calculate confidence intervals if one plans to conduct convergence and robustness analyses after performing gsa with the chosen number of mc simulations n it is important to understand specificities of its sampling design and ensure that it allows subsampling some methods have an acceptable performance already with random sampling correlation coefficients and gradient boosting others employ sensitivity indices estimators that require particular sampling designs saltelli estimator described in section 2 2 2 finally there are methods that have better performance when design is optimized but the indices can also be estimated with simpler designs delta method where latin hypercube sampling is more optimal than random sampling see section 2 2 3 for methods with particular sampling strategies the design x of maximum size n should be chosen such that n mn b where m is a user defined constant and each block of samples of size n b has the required design structure then the convergence steps are multiples of n b and the bootstrap procedure should sample with replacement among blocks of size n b and not the individual samples saltelli et al 2010 the only particular design used in this paper is sobol quasi random sequence in sobol indices estimation section 2 2 2 latin hypercube has been subsampled in the same way as random sampling but there exists an extension of classical version that has a block structure sheikholeslami and razavi 2017 in the following we focus on quantitative measures that can be useful to monitor convergence and robustness of screening confidence intervals of sensitivity indices and screening one such measure can be maximum confidence interval of the estimated sensitivity indices s among all model inputs j 1 k sarrazin et al 2016 1 s t a t i n d i c e s s max j 1 k s j u b s j l b where superscripts ub lb refer to upper and lower bounds of the interval calculated with standard error and student s t distribution for all robustness statistics of this study this statistics can be computed at increasing number of mc iterations and convergence is achieved when the width is sufficiently low for the purposes of screening it is also possible to compute stat indices only for a subset j of model inputs which can be i selected to be below a certain threshold s th j j s j s t h or ii based on a predefined number k th of lowest sensitivity values j j s j s k t h sorted where s sorted are sensitivity indices sorted in increasing order sarrazin et al 2016 then this screening statistic can be defined as follows 2 s t a t s c r e e n i n g s max j j s j u b s j l b absolute thresholds s th can be hard to define a priori since confidence intervals depend on standard error and not all sensitivity indices are normalized to a specific interval moreover the complexity of a specific model or its effective dimensionality level of interactions and nonlinearity affect values of sensitivity indices even if they are normalized in this paper along with stat indices we choose to investigate stat screening based on a predefined number of lowest sensitivity values k th confidence intervals of rankings the two previous statistics indicate whether sensitivity index estimates get more robust as the number of iterations increases but do not show whether inputs ranking retains more specifically in the context of screening it does not clarify whether inputs identified as lowly influential on one step stay lowly influential at no additional cost it is possible to capture this more complete picture by monitoring the convergence of ranking which is stricter than the convergence of screening sarrazin et al 2016 two rankings can be compared by means of savage scores rank correlation coefficients and their weighted versions that put more emphasis on agreement between higher or lower ranks sarrazin et al 2016 while being suitable measures for models with up to hundreds of inputs and factor prioritization sa setting these statistics would fail in higher dimensions where the focus is more on screening because slight differences or ties in the values of sensitivity indices can place two similar inputs into ranking positions that are hundreds or thousands ranks apart then the computed statistic would indicate disagreement between two rankings whereas given factor fixing purpose in mind it is not necessarily the case to alleviate this issue we propose to assign same ranks to inputs with similar sensitivity indices by applying jenks natural breaks optimization jenks 1967 this is a clustering method that arranges data into a predefined number of clusters such that variances within each cluster are minimized and variances between clusters are maximized we chose jenks optimization due to its particular suitability for grouping of one dimensional data where the task is reduced to defining optimal borders between data points then statistics that are typically used in lower dimensional cases can be applied to the clustered rankings in this paper we employ spearman correlation ρ see section 2 2 1 in appendix a 1 we outline the benefits of spearman correlations applied to clustered rankings over statistics used on non clustered rankings that have been introduced in previous studies assuming that we are interested in comparing converging rankings with ranking obtained from the highest number of iterations the following can be used 3 s t a t r a n k i n g s ρ r jenks s r jenks s n where r jenks s denotes ranking obtained with jenks optimization procedure for sensitivity indices s and superscript in s n indicates that sensitivity indices were computed with the maximum number of mc simulations available in other words last convergence step similarly to comparing converging rankings in order to study their robustness we compare rankings obtained from bootstrapping with r jenks s n and compute confidence intervals to estimate range of stat ranking at each convergence step it is also possible to assess robustness by comparing pairs of bootstrap rankings within each step but we chose to monitor their similarity with the maximum simulations ranking to ensure that stat ranking value at each step is not incidental we restrict ourselves to computing stat ranking for all convergence steps but the last one where it is trivially equal to 1 algorithm 1 in appendix a 2 describes the overall procedure to obtain mean value and confidence intervals of stat ranking for all convergence steps since s n are not necessarily converged estimates of sensitivities stat ranking should be analyzed with care as the number of iterations increases this statistic should steadily approach 1 to indicate convergence of clustered rankings if its value stabilizes around 1 before the last convergence step then more samples do not change the clustered rankings and their convergence has been reached if stat ranking is still low at the last convergence steps e g below 0 5 then ranking at the last step is not reliable and more mc iterations are needed 2 1 2 validation there exist various ways of validating gsa results multiple studies consider three sets of model simulations i unconditional outputs y all g x are obtained when all model inputs vary randomly ii conditional outputs y inf g x x inf are computed when influential inputs vary consistently with the first set and the non influential ones are fixed to the most probable or prescribed values iii finally conditional outputs y inf g x x inf are calculated from varying non influential inputs consistently with the first set and setting the influential ones to fixed values andres 1997 proposes to visually assess differences between pairs of simulations in the two cases a y all y inf and b y all y inf by means of scatter plots nossent et al 2011 and tang et al 2007 use correlation coefficients to quantify these differences low correlations in the first case and high correlations in the second case point towards reasonable choice of non influential inputs sarrazin et al 2016 a variation of this method was originally proposed by pianosi and wagener 2015 it computes cumulative distribution functions of unconditional y all and conditional y inf model outputs and quantifies their discrepancy with the kolmogorov smirnov ks statistic since this approach is conditioned on non influential inputs it should be repeated for multiple x inf values that sufficiently cover the input space then a summary ks statistic mean median or maximum of ks statistics obtained at different conditioning values is calculated and a two sample ks test is applied sarrazin et al 2016 this is a recommended way of performing validation to avoid bias from a particular set of conditioning values interestingly the first approach based on correlation coefficients is usually performed only at one set of values andres 1997 nossent et al 2011 tang et al 2007 as the second approach based on the ks statistic is computationally expensive for high dimensional models in this paper we perform validation by computing spearman correlations between y all and y inf but note that other validation techniques are also suitable additionally we can go back to the definition of gsa and look at the overlap between histograms or probability distributions derived from y all and y inf a numeric value of validation performance can be any distance measure between them in this study we employ the first wasserstein distance between distributions u and v derived from y all and y inf respectively which is defined as w 1 u v inf π γ u v r r x y d π x y where γ u v is the set of all joint probability measures on r r whose marginals are u and v ramdas et al 2017 this distance can range in 0 where smaller values indicate bigger similarity between two distributions numerically it is computed as the distance between respective cumulative distributions virtanen et al 2020 in the following we provide an overview of the three selected sensitivity methods and discuss their intrinsic properties that can be taken into account in convergence and robustness analyses at no additional cost 2 2 sensitivity analysis in recent years a plethora of good literature surveys on sa have been conducted pianosi et al 2016 comprehensively review sensitivity methods and include practical guidelines on conducting the analysis gan et al 2014 study effectiveness and efficiency of ten gsa methods and compare their performance on a hydrological model with thirteen factors that have uncertainties borgonovo and plischke 2016 provide overview of various local and global methods and discuss interpretation of gsa results in detail in the following we will provide description of the chosen gsa methods 2 2 1 correlation coefficients correlation coefficients are widely used as sensitivity measures due to their simplicity lee rodgers and nicewander 1988 they measure degree of association between two variables x and y and are applied on the samples observations of the variables pearson product moment correlation coefficient describes linear relationships for quantitative data and is given by r x y cov x y σ x σ y where cov x y is covariance and σ x σ y are standard deviations expressed as cov x y e x e x y e y σ x 2 e x e x 2 and σ y 2 e y e y 2 for the nonlinear models with monotonic relations rank correlation coefficients can be used such as spearman ρ kendall τ partial rank correlation coefficient etc for example to compute spearman correlation first x and y are converted into rank variables x r y r and then pearson coefficient is computed 6 ρ x y r x r y r cov x r y r σ x r σ y r correlation coefficients are dimensionless indices that take values from 1 to 1 ranging from perfect negative to perfect positive correlation where values close to 0 indicate weak correlations these coefficients have well characterized estimators such as ρ x y 1 6 i 1 n x r y r n n 2 1 where n is the number of observations they are also invariant under linear transformations in the case of gsa correlation coefficients between x j and the output y for all j 1 k can be chosen as sensitivity indices a drawback of correlation coefficients lies in the underlying assumptions i they are suitable only for linear pearson and monotone rank coefficients relations helton and davis 2002 ii for the pearson coefficient variables should be continuous and normally distributed iii for the rank coefficients variables should be measured on an ordinal interval or ratio scale the advantage however is that correlation estimators are well defined and require relatively low number of samples to produce confident estimates that is invariant of the number of model inputs convergence and robustness bonett and wright proposed a procedure to compute sample size that yields estimation of the pearson spearman or kendall τ coefficients in a desired confidence interval bonett and wright 2000 this procedure takes correlation value to which the estimate should converge desired confidence width and confidence level as inputs and the output is the sample size needed for such an estimation if the approximate value of the coefficient is unknown one can obtain sample sizes for a range of correlation values and choose the maximum number of samples to ensure the desired confidence width higher values of correlations tend to require fewer samples thus irrespectively of the model dimensionality hundreds to few thousands of iterations is enough for a confident estimation of correlation based sensitivity indices schönbrodt and perugini 2013 alternatively utilizing the same procedure but with the aim of convergence and robustness analyses one can compute confidence intervals for a given sample size and correlation value if the approximate correlation value is unknown widths of confidence intervals for a range of correlation values can be derived analytically and in line with stat indices in equation 1 maximum confidence interval width can be estimated without the need for bootstrapping the simplicity of this method makes it very attractive for gsa applications if the underlying model monotonicity assumption is satisfied if this is not the case variance based sensitivity methods are capable of handling model nonlinearities 2 2 2 variance based methods the idea of variance based methods lies in decomposition of the model output variance into fractions that can be attributed to different sources of variation such as individual model inputs or their combinations methods such as fourier amplitude sensitivity test fast by cukier et al 1973 analysis of variance anova decomposition of a function by sobol 2001 importance measures of homma and saltelli 1996 all refer to similar concepts and represent variance based methods among these first and total order sobol indices gained the most popularity for each model input x j with j 1 k first order index takes into account only main first order effect and the total order considers also interaction effects between x j and all other inputs together first and total order show a comprehensive picture of inputs importances and interactions in this paper however we only focus on total order indices due to their applicability for screening purposes their expression is given as follows 8 s j t e x j v a r x j y x j v a r y where x j denotes all inputs but j intuitively s j t computes fraction of output variance when all inputs are fixed and x j varies low values of s j t indicate non important inputs numerically sobol indices convergence is proportional to 1 n where n is the sample size per model input nossent et al 2011 to reduce computational cost elaborate sampling procedures and numerical estimators of the indices have been proposed saltelli estimators saltelli et al 2010 developed an estimation procedure that uses same model runs for both first and total order indices it is based on a b a b j r m k sampling matrices generated from sobol quasi random sequence joe and kuo 2003 and radial basis design saltelli et al 2010 input sampling a and b are two independent matrices whereas a b j contains all columns from the matrix a except for the j th column which takes value from the respective j th column of b this essentially reflects the case when only j th parameter varies then the estimator for the total index is given as saltelli et al 2010 nossent et al 2011 10 s j t 1 2 m i 1 m g a i g a b i j 2 v a r y where v a r y 1 2 m 1 i 1 m g 2 a i g 2 b i 1 m i 1 m g a i g b i and a i b i a b i j are i th rows of the respective matrices computational burden of sobol indices is n m k 2 where k is the number of model inputs and m is a user defined constant the higher m results in better convergence of the indices but the exact number of simulations depends on the model complexity convergence and robustness there are a number of possible intrinsic metrics to monitor convergence of sobol indices i both first and total sobol indices should be non negative ii first order indices should be less than or equal to one iii total indices should be larger or equal to first order because in addition to main effects they take into account interactions convergence and robustness analyses are well studied and can be done efficiently nossent et al 2011 while being easy to implement and interpret there are multiple drawbacks associated with the method of sobol da veiga 2015 first of all the number of simulations required for convergence of the indices varies from model to model and can reach millions for high dimensional cases secondly by definition variance based methods are limited to computation of variances meaning that distribution of the model variability is collected into a single value statistic and potentially useful information is lost moreover there is no known way of handling multimodal distributions in the next section we describe the delta method that overcomes all of these drawbacks 2 2 3 delta moment independent measure the delta moment independent measure has been first introduced by borgonovo as means to identify inputs that influence the entire output distribution the most borgonovo 2007 the idea of the method is to measure the shift in uncertainty distribution of y when parameter x j is set to a fixed value this is expressed as the following integral 11 δ j 1 2 f x j x j f y y f y x j y x j d y d x j where f x j x j f y y are density functions and f y x j y x j are conditional density functions of y given fixed x j this sensitivity index takes values from 0 to 1 where δ 0 if and only if y is independent from x j and 1 if y depends only on x j another property of delta indices is invariance to monotonic transformations in principle the inner integral is a separation dissimilarity measure between two distributions the idea is to compare f y and f y x j by computing various f divergences between them the authors chose l1 norm for the delta index by choosing a different measure this integral can be reduced to for example sobol first order index da veiga 2015 plischke et al 2013 describe a consistent estimator of delta indices that utilizes kernel density estimators the total cost of the computation is n mk where m should be sufficiently large to estimate the two integrals in equation 11 convergence and robustness by definition delta indices take values in 0 1 interval which is automatically taken into account when employing kernel probability density estimators plischke et al 2013 showed that theoretical asymptotic convergence of the delta estimator is guaranteed the authors also tackled the issue of gsa results robustness for two cases at large sample size they introduced a bias controlling filter and at a small sample size robust determination of confidence intervals through the bias reduction bootstrap approach given that delta indices are computed using both x and y bootstrap approach is computationally expensive as the number of model inputs increases 2 3 dimensionality reduction and feature selection in this section we provide brief introduction to dimensionality reduction algorithms whose importance for gsa applications has been recognized by a number of authors da veiga 2015 razavi et al 2020 one of the main motivation to investigate these methods lies in their ability to handle high dimensional input spaces together with nonlinearity in models despite the fact that relevancy metric is not necessarily defined in the same sense as in gsa contribution to model output uncertainty this family of methods can serve as a good starting point for factor fixing in machine learning ml dimensionality reduction is the process of reducing the number of dimensions in the factor feature or variable in the ml terminology space guyon and elisseeff 2003 people distinguish between two types of dimensionality reduction techniques feature construction and feature selection huang et al 2019 the former is also called feature extraction these methods are aimed at achieving good reconstruction of the original input data unsupervised or being best for predicting model output supervised with a more compact subset of features lee and verleysen 2007 they provide a lower dimensional and hence less memory intense approximation of the initial input data however with most of the nonlinear feature construction methods it is impossible to derive the importance of individual features on the other hand feature selection methods are needed to select relevant features from the original ones without modifying them they are often used in combination with other statistical methods regression and classification where their intrinsic objective function determines the quality of the selected features chandrashekar and sahin 2014 give an overview of feature selection methods with application to few standard datasets most notable are algorithms based on decision trees such as random forests and gradient boosted trees in this study we chose to focus on gradient boosting that being an ensemble method based on trees has high predictive capability provides natural metrics for feature selection and gained popularity in numerous kaggle competitions due to its efficient implementations raschka et al 2020 gradient boosting is a popular supervised machine learning data driven approach that produces a prediction model as an ensemble of weak learners friedman 2001 this numerical optimization problem aims at minimizing the loss of the model by adding weak learners in a stage wise manner with a gradient descent procedure the loss can be chosen arbitrarily to fit regression classification or ranking purposes and has to be differentiable if the weak predictors are decision trees then it is called gradient boosted trees based on which input variables were used to build a tree ensemble model that predicts the output one can gain understanding of relative importance of particular inputs to the model variation hence by building a good prediction model feature importances are obtained as a by product moreover friedman 2001 explicitly mentions analysis of variance anova decomposition in the paper signifying the link between the maximum tree depth and interactions effects for example if a model achieves good prediction results with trees of depth 3 then important interactions are combinations of 3 inputs such as x 1 x 2 x 3 or x 6 x 8 x 9 gradient boosting algorithm as in all supervised algorithms x i y i i 1 n are divided into n train samples of the training set and n test n n train samples of the testing set the objective is to use training set to obtain an approximation f of the function f x that maps x to y while minimizing the expected value of some loss function l f x y over x y values chen and guestrin 2016 then the tree ensemble model uses p additive functions to predict the output y i and has the following form 12 y i f x p 1 p h p x i where h p s are weak learners which in the case of gradient boosted trees correspond to tree structures and all p trees constitute an ensemble of trees the loss function is given by 13 l f x y i 1 n train l y i y i p 1 p ω h p where ω h γ t 1 2 λ w 2 here l is a loss function for the difference between the prediction y i and observed output y i the ω term is needed to penalize complexity of a tree h it contains number of leaves t n multiplied by the tuning parameter γ r 0 called gain that encourages pruning when set to a positive value and regularization term on the norm of leaf weights w r t where λ r 0 determines degree of smoothing to avoid over fitting trees are added sequentially based on optimization of the loss function such that each subsequent tree improves the model notable feature of gradient boosting is that the training can be done iteratively and it is possible for other boosted tree models to serve as a warm start for speed gains moreover tree models can be inputs to shapley values analysis another nonlinear gsa method that can be used for factor ranking owen 2014 as the focus of this paper is on screening techniques we do not explore shapley values further feature importance measures for relative importance of features arise naturally in tree based algorithms their exact computation can differ but the core idea stems from estimating each feature s contribution to the construction of the trees for a tree h the estimate of relative importance of the model input x j can be computed as proposed by breiman et al 1984 14 i j 2 h t 1 j i t 2 1 v t j where j is the number of nonterminal nodes of tree h or in other words nodes that are not leaves v t is the splitting variable associated with node t i t is the empirical improvement in the objective function that comes from the split and 1 is the indicator function that is zero everywhere except for the case v t j when the split variable v t is equal to the considered j th model input friedman 2001 for the case of regression trees to evaluate the split of the current node r into left and right nodes r r r l the expression i t 2 takes the form 15 i t 2 r r r l w l w r w l w r y l y r 2 where y l y r are means of observations that fall into the left and right nodes respectively as a result of the split and w l w r are the sums of observations weights which are measures of the influence of each observation on the objective function during training stage these can be chosen to favor splits with more equal number of observations in the left and right nodes or such that variances of observations in the left and right nodes are closer friedman 2001 for a collection of p decision trees expression 14 can be averaged as follows 16 φ j 1 p p 1 p i j 2 h p since the range of variation of this metric depends on the constructed trees we normalized it by the sum of φ j for all model inputs j the gradient boosting sensitivity index is then given as 17 i j 2 φ j j 1 k φ j and takes values from 0 to 1 convergence and robustness regression based supervised methods including gradient boosted trees used in this study have an intrinsic measure to evaluate performance of trained models examples can be coefficient of determination explained variance mean squared error and others these metrics can be viewed during convergence and robustness analyses at no extra cost in this paper we will only consider coefficient of determination denoted as r 2 the proportion of the variance in the model output that is predictable from the model inputs this metric s range is from 0 to 1 where values close to 1 indicate substantial correlation between predicted and given model outputs from the test set 2 4 degree of model linearity coefficient of determination can also be used as a measure of model s degree of linearity that will prove to be useful for deriving a standardized procedure for robust high dimensional screening to that end saltelli et al 2008 first compute standardized regression coefficients src for all model inputs j 1 k 18 β j b j σ x j σ y where coefficients b j can be determined by ordinary least squares with a solution b j x t x 1 x t y and are normalized by σ x j σ y standard deviations of the model input x j and output y then the degree of model linearity can be estimated with the coefficient of determination as r 2 j 1 k β j 2 it is the fraction of model variance that is explained by the given linear regression model where values closer to 1 point to higher degree of model linearity given the existence of the closed form solution src can be computed efficiently even for high dimensional models 2 5 test cases in the following we introduce two test models new analytical function that we derive by modifying the standard morris model to include arbitrary levels of input importances in a straightforward manner and an environmental lca model 2 5 1 morris function morris function was first introduced by morris et al morris et al 2006 its expression is given by 19 z x α i 1 k x i β i 1 k 1 j i 1 k x i x j where α 12 6 0 1 k 1 β 12 0 1 k 1 the function is evaluated on the 0 1 k dimensional hypercube the total number of inputs k as well as the number of influential inputs k can be chosen arbitrarily all influential inputs are equivalent and interact with each other in order to compare performance of sensitivity methods in identifying inputs of different importance levels we propose to modify the given morris function as follows 20 y x λ z x l μ z x m ν z x n where x l x 1 x k x m x k 1 x 2 k and x n x 2 k 1 x 3 k all k inputs from one group l m or n have the same importance level which is determined by constants λ μ ν 0 1 such that higher values indicate higher importance in this paper the authors consider three cases k 1 000 5 000 10 000 with k k 100 and the constants chosen as λ 1 μ 1 2 ν 1 10 this way sobol indices of inputs in x l are two times larger than inputs in x m and ten times larger than inputs in x n the reader is referred to appendix c for the analytical expressions of sobol indices knowing the indices of morris function allows an easy computation of the indices for the modified morris function the rest k 3k inputs are dummy variables that have zero influence on the model output 2 5 2 lca model lca is a well established and widely used methodology for environmental impact assessment hauschild et al 2018 it allows estimation of various impacts along the supply chains of a product life cycle inventory analysis compiles information about physical flows which are inputs of resources and materials and outputs of emissions waste and products these are often taken from proprietary databases life cycle impact assessment groups together the previously collected flows according to impact categories of interest these can be climate change human health land and water use resource depletion etc lca model can be written in matrix based form as shown in fig 1 or symbolically 21 y c b a 1 d components of the matrix based lca demand vector d consists of amounts of products whose environmental impacts we need to estimate for example to estimate impacts from a production of one passenger car the demand vector would contain zeros everywhere but in the row corresponding to passenger cars where its value would be 1 unit the exact location of this row is in accordance with the technosphere technosphere a is a square matrix that contains information about manufacturing activities needed to produce various products each element in the a matrix is called an exchange for a product passenger car in one of the rows it would be linked to for example electricity steel water production activities that are needed for its manufacturing the inverse of the technosphere matrix is taken for the purposes of scaling the physical amounts of all activities in such a way that they produce the desired demand biosphere b is a non square matrix that provides mapping from manufacturing activities to environmental flows that appear due to these activities for instance to produce a car certain amount of electricity is needed that requires combustion of fossil fuels which in turn causes carbon dioxide emission into the atmosphere characterization c assigns weights to these emissions environmental flows that characterize contribution of flows to specific impact categories finally impact values or scores y are computed for each impact category typically information about manufacturing activities and their environmental flows is stored in databases that consist of tens to hundreds of thousands of uncertain inputs their uncertainty distributions are commonly chosen to be lognormal normal uniform and triangular and are specified by experts in relevant fields treating elements in the technosphere matrix as random variables x j with predefined distributions results in the model y g x by nature a is a sparse matrix with independent entries because e g production of a car in europe does not require agricultural activities from canada functional unit d governs which of the activities contribute to the assessment of environmental impact however due to nonlinearity of lca and interconnected global supply chains analytical derivation of influential inputs is not possible moreover solving a 1 d as a system of linear equations results in very fast computations in the order of milliseconds or seconds per model run hence even though lca model is very high dimensional it is also possible to run it thousands or millions of times froemelt et al 2018 2020 provided a detailed model of household consumption based on the data from swiss household budget survey which contains information on expenditures income purchased goods and consumer behavior of 9955 households in this study we chose functional unit to reflect food consumption of an average swiss household per month and the impacts of interest are greenhouse gas emissions estimated in kilograms of co2 equivalent abbreviated as kg co2 eq 2 6 software we ran numerical simulations using python package gsa framework that we are currently actively developing implementation includes base class for a generic gsa method that links sampling design model runs and computation of sensitivity indices as well as saving of all necessary matrices in a memory and runtime efficient way that means whenever the time for generating certain output was substantial we choose to save it under a file name that uniquely identifies parameters necessary to reproduce the output then next time the same output is required it is simply read from file this also contributes to reuse of the sampling designs and model runs the base gsa class can be passed to child classes to add specific sensitivity methods so far we have implemented estimators for correlations sobol and delta moment indices for gradient boosting we used python implementation of xgboost a scalable tree boosting system that gained a lot of popularity in the ml community for its extreme computational performance and state of the art prediction capabilities chen and guestrin 2016 additionally gsa framework package includes classes for custom models convergence robustness and validation and functions for results visualization see comprehensive description in the technical documentation 1 1 https gsa framework readthedocs io whenever it was beneficial we parallelized the code using native multiprocessing library among contributions of this software is scalability and computational performance improvements the former is needed in terms of ram requirements for high dimensional models regarding the latter performance comparison with the existing salib 2 2 https salib readthedocs io implementations is given in appendix b testing of the software is done with unit tests where outputs of our framework were compared with the salib implementations taken as ground truth all code is open source and is available at https github com aleksandra kim gsa framework 3 3 further inquiries at aleksandra kim psi ch under the bsd 3 clause license 3 results in order to study scalability of gsa methods we consider morris models with increasing number of inputs 1 000 5 000 and 10 000 numerical simulations encompass gsa results robustness and convergence for spearman correlations sobol total order indices delta indices and feature importances from gradient boosted trees we further apply same analysis to the lca model as well as validate results the number of convergence steps has been chosen as 50 for all models and methods except for sobol total order indices where it was adjusted to the sizes of sampling blocks and resulted in 97 steps for the morris functions and 38 steps for the lca case study numerical simulations for all morris models were conducted under the same conditions on a 2 6 ghz 6 core intel core i7 personal computer with 16 gb 2400 mhz ddr4 memory to tune gradient boosting we ran grid search optimization with cross validation a greedy procedure for performing exhaustive search for optimal tuning parameters see concrete values in appendix d 1 for all models for the convergence and robustness of ranking the number of clusters for the jenks optimization procedure has been set to 4 for the morris model which is based on the prior knowledge of the chosen number of importance levels and arbitrarily to 10 for the lca case study see section 2 1 1 number of bootstrap resampling p has been set to 120 in all cases where the recommended p 1 000 was not possible due to computational limitations by increasing p width of a confidence interval might i increase if new estimates of sensitivity indices lead to higher standard error used for the confidence interval calculation ii decrease if the standard error reduces iii not change substantially if the number of mc samples is high enough to estimate sensitivity indices confidently imagine infinite mc samples efron and tibshirani 1994 one way of analyzing whether higher p should be used is to look at histograms of bootstrap data if they are bell shaped unimodal and somewhat symmetric and do not have large outliers then the chosen number of bootstraps is acceptable in our case see section d 2 in the appendix for the lca model example the considered histograms are visually symmetric and without outliers therefore p 120 and student s t distribution are appropriate choices when computing confidence intervals 3 1 morris function we set the number of iterations n for all models consistently and as a function of model s dimensionality k a priori gsa methods require different number of iterations for the spearman correlations we chose n 4k based on the procedure for determining sample size for confident estimation of correlation coefficients see section 2 2 1 in the previous works sobol indices have been estimated on 500 1 000 saltelli et al 2008 pianosi et al 2016 12 000 nossent et al 2011 500 50 000 sarrazin et al 2016 iterations per model input given the high dimensionality of our models and that low sensitivity indices tend to reach convergence faster than high ones nossent et al 2011 we set n 100k for sobol indices due to computational requirements of the delta method we could only afford 8k iterations even though this method can require hundreds to thousands samples per input plischke et al 2013 regression methods recommend at least 4k iterations jenkins and quintana ascencio 2020 which was in the end chosen for the gradient boosting 3 1 1 gsa results fig 2 shows gsa results for morris models with 1 000 5 000 and 10 000 inputs additionally we plot analytical values of sobol total order indices with derivations given in appendix c one can see that all methods show similar results and are able to distinguish between the four chosen levels of input importances sobol total order and xgboost indices are best at identifying inputs with zero importance whereas spearman correlations and delta assess those inputs as lowly influential but still with non zero estimates delta indices can separate highly influential inputs from lowly but at the given number of iterations performs worse in distinguishing between lowly and non influential inputs for all morris models however the bootstrap correcting procedure which can greatly improve delta estimates has not been included due to already high computational cost given that true values for this model are known validation of gsa results was not needed 3 1 2 scalability of gsa methods gsa results indicate that all methods are scalable but the question is at which computational cost table 2 provides a summary and visualization of the following aspects i number of mc runs as factor of respective model dimensionality k ii time for generating required sampling design x in unitcube space iii time needed to compute sensitivity indices iv amount of space required to store data for gsa computations by looking at the table we can draw a conclusion that no matter the model dimensionality sampling usually does not pose a serious computational problem the maximum time was needed for the saltelli estimates but x was also generated for as many as 100k iterations we can also observe that those methods that have elaborate sampling designs require more time to generate x and y but take almost no time for computing sensitivity indices hence when employing these methods one should definitely provide convergence and robustness results 3 1 3 confidence intervals of sensitivity indices and screening fig 3 shows maximum confidence intervals of sensitivity indices stat indices in blue equation 1 and stat screening in purple equation 2 obtained with bootstrapping for all morris models and all gsa methods since these statistics are linked to confidence intervals we center them around y 0 for each convergence step such that the blue purple part is equal to stat indices stat screening the interval s lower bound is below zero and its upper bound is above zero in this case bounds happen to be symmetric with respect to y 0 because we used student s t distribution to compute confidence intervals other types of confidence intervals might result in asymmetric plots note that we visualize these statistics differently than the authors of the original publication sarrazin et al 2016 we set k th 0 98k in stat screening for all morris models with k number of inputs comparing stat indices and stat screening one can see that convergence of screening is reached faster than convergence of sensitivity indices estimation for all methods spearman correlations and delta method exhibit smooth convergence that does not differ substantially for stat indices and stat screening indicating that confidence intervals for lowly and highly influential inputs are similar for the spearman correlations analytical confidence intervals are rather conservative because these are computed as maximum intervals within entire range of correlation values which we do not necessarily encounter in practice hence bootstrap intervals are narrower sobol total order indices have higher fluctuations in stat indices than stat screening confidence intervals on the first few steps indeed fig 3 shows that higher values of total indices have more variability than lower values given that groups of similarly important inputs in the modified morris model should be estimated at the same value this is in line with previous studies which suggest that convergence for sobol total indices is reached faster for lower values of the indices than for higher nossent et al 2011 delta indices require all the iterations that we can afford with given computational resources since even at the last convergence step the separation between different levels of inputs importances is not clearly visible see fig 2 we can also observe an interesting pattern in the results of gradient boosting feature importance seems to converge smoothly for morris models with 1 000 and 5 000 inputs but exhibits a sudden drop in value closer to 20 000 iterations for the model with 10 000 inputs the drop can be a consequence of tuning namely there is a user defined parameter that determines minimum number of samples required in tree leaves for growing the tree when set to a high value with respect to the total number of samples this parameter would encourage tree pruning which results in an ensemble of simpler but also less capable trees that do not account for the desired level of interactions bias variance tradeoff in this case trained tree would produce worse predictions that also affects feature importance since tuning xgboost for all convergence steps was a tedious task we set all tuning parameters including minimum number of samples to same values for all steps such that the tuning produces best results for the maximum number of iterations last convergence step by optimizing tuning parameters at each convergence step we believe it is possible to achieve faster convergence by comparing these results with the absolute values of sensitivity indices depicted in fig 2 one can define approximate threshold values s t a t s c r e e n i n g t h that determine minimum number of iterations to reach convergence for the purposes of screening we chose to define this threshold as the difference between sensitivity indices of the lowest moderately influential input and the highest lowly influential input table 3 contains thresholds and number of iterations when convergence is reached for all gsa methods and all morris models 3 1 4 confidence intervals of rankings fig 4 shows evolution of statistic for rankings and its confidence intervals for all morris models obtained from bootstrapping between rankings at the current convergence step and ranking at the last convergence step with respect to convergence for spearman correlations we can see that stat ranking statistics are robust at all steps and become more similar to the final ranking as the number of iterations increases almost reaching perfect correlation closer to 4k iterations stat ranking convergence has same tendency for all models but is slower for models with more inputs which is in line with stat screening results see table 3 for sobol indices this statistic converges after approximately 30k iterations which is a bit lower than stat screening for 1 000 inputs but higher than stat screening for 5 000 and 10 000 inputs the rather fast convergence can be explained by the strength of this method in determining non influential inputs that constitute large fraction of all inputs in high dimensional models however there is quite some variation within each convergence step for morris models with 1 000 and 5 000 inputs as the number of model inputs and hence also number of mc iterations increases confidence intervals become narrower in all cases even the lower bound is still high enough to show agreement between rankings for the delta method we can observe a step at around 7 7 5k iterations from this moment on stat ranking has a larger value interestingly we did not see any steps while studying convergence of sensitivity indices in fig 3 this can be an artefact of sampling because we used regular latin hypercube sampling instead of block designs so the properties of latin hypercube are only fully present for the last convergence step and it is possible that an important portion of the input space has not been analyzed until 7 7 5k iterations see section 2 1 1 notably despite having low mean values stat ranking is quite robust for delta moment finally xgboost shows convergence of both ranking and r 2 score for all morris models the pattern similar to the one in fig 3 for the model with 10 000 inputs can be observed here as well and after 18 000 iterations rankings are robust by collecting convergence and robustness results for sensitivity indices screening and ranking we can conclude for the goal of factor fixing that for all morris models i spearman correlations reach convergence closer to the last step ii sobol indices do not require as many iterations and convergence is reached faster than anticipated therefore approximately 40k iterations would have been sufficient iii delta method requires more samples iv number of iterations for gradient boosted trees has been chosen appropriately 3 2 lca model in the following section we present results for the lca model with 10 000 inputs we first show how all gsa methods perform then validate results and finally similarly to the morris model above demonstrate convergence and robustness of sensitivity indices and ranking from the convergence results of morris model we adjusted some of the settings in lca for example to save computational cost we restricted ourselves to fewer iterations in the sobol method 400 000 as opposed to one million otherwise number of iterations for each gsa method remained the same as for the morris functions we also tuned gradient boosted trees such that tuning parameters are suitable for all convergence steps 3 2 1 gsa results fig 5 shows results for the lca model and all gsa methods in rows where inputs for all methods were sorted according to descending spearman correlations values for better understanding visually one can see that few model inputs were identified as influential by all methods the non influential inputs are best seen by sobol and xgboost approaches as there are many values close to zero when training the model for gradient boosted trees we noticed that predictive performance improves when maximum tree depth is set to 4 which means that degree of significant interaction effects between model inputs is also close to 4 at least for the case study of swiss household food consumption in order to quantify the agreement between results of different methods we intersected the 1 most influential inputs and 90 least non influential inputs between all methods leaving some inputs unclassified for the sake of being conservative in the screening analysis and avoid filtering out influential inputs we chose to separate inputs based on the percentages of the total number of inputs as opposed to threshold values for sensitivity indices because the thresholds would need to be specific for each gsa method and it is unclear how to choose them for a fair comparison additionally choosing the number of inputs is more intuitive in lca models because it directly relates to the number of lca datasets for which better quality data can be collected given available resources comparative table 4 shows agreement of influential in orange at the lower triangular block and agreement for non influential in blue in the upper triangular block as the ratio between cardinality of inputs intersection between a pair of methods to the total number of considered inputs 1 and 90 respectively the agreement seems to be low for influential inputs for some method pairs which could mean that a number of non influential inputs have been identified as influential at this stage of the analysis it is important to remember that 1 of 10 000 inputs is already 100 inputs so an agreement higher than 0 7 refers to a rather high number of 70 inputs further gsa for factor prioritization might be needed to derive a prioritized list of influential inputs as the focus of the paper is on screening we are more interested in the non influential part of the table that shows sufficiently high agreement above 0 9 between all pairs of methods which yields more than 8 000 inputs for all methods combinations finally the degree of model linearity computed with src is equal to 0 988 indicating that lca model is quite linear see section 2 4 this can explain good performance of correlation coefficients however it does not mean that all lca models are linear but rather that the particular case study of swiss food consumption has this property 3 2 2 validation of gsa results validation was done with both correlation coefficients and distances between histograms as described in section 2 1 2 table 5 shows results of validation for all gsa methods to compute y inf we chose to vary only the first 60 influential inputs any other number of inputs can be chosen depending on the goal of the analysis the non influential inputs have been set to means of their distributions which are typically used in deterministic lca computations when uncertainty and sensitivity analysis are not performed since validation results are similar for all methods we only show figure for spearman correlations for other methods see fig d6 in appendix d 4 fig 6 shows i on the left scatter plot between y all on the x axis in blue all inputs vary and y inf on the y axis in orange only influential inputs vary ii on the right the histogram of y all and y inf both visually and numerically we can see that the filtered out non influential model inputs do not significantly affect variation of the model output correlation coefficient between y all and y inf is high and distance is low 3 2 3 confidence intervals of sensitivity indices and screening similarly to the morris model we show results for convergence and robustness of sensitivity indices and screening in fig 7 where stat indices and stat screening are centered symmetrically around y 0 and k th 0 9k 9 000 inputs see equation 2 maximum confidence interval among all model inputs decreases as the number of iterations increases results are similar to morris models in that stat indices and stat screening are alike for spearman correlations and delta and screening converges much faster than sensitivity indices for sobol and xgboost methods in case of the lca model given that we want to separate the 1 most influential and 90 least influential inputs we could identify the following convergence results for the spearman correlations s t a t s c r e e n i n g t h 0 005 59 which corresponds to 32 000 iterations for the sobol total order indices s t a t s c r e e n i n g t h 0 000 31 and number of iterations is 20 000 delta method needs more samples for convergence and finally for the xgboost indices we obtain s t a t s c r e e n i n g t h 0 000 19 and 18 400 iterations interestingly lower values of sobol indices converge much quicker compared to higher values 3 2 4 confidence intervals of rankings fig 8 a shows convergence and robustness of stat ranking thick blue line is the mean value of stat ranking whereas transparent blue regions are its 95 confidence intervals spearman correlations statistic steadily converges to the ranking of the last convergence step similar to stat screening results and is quite robust with respect to sobol 150 000 iterations seem to be enough to have comparable ranking results as at the 400 000 model runs note that this value is much larger compared to 20 000 iterations obtained with stat screening even with relatively wider confidence intervals than in other methods stat ranking indicates good agreement with the maximum iterations ranking as before we observe a step in delta indices at around 75 000 iterations but stat ranking is robust at each convergence step finally gradient boosting in the lca model exhibits convergence properties of both ranking and r 2 score with a very similar behavior the statistic stat ranking is both robust and close to 1 at roughly half the maximum number of mc iterations same as obtained with the screening statistic all in all convergence and robustness analyses for the lca model in the scope of factor fixing was obtained by taking a more conservative number of iterations among all the statistics it reveals that convergence has been established for spearman correlations at ca 40 000 for sobol total order indices at 150 000 and for xgboost importances at 20 000 iterations but has not been reached by the delta method at the given 80 000 iterations 4 discussion 4 1 curse of dimensionality one of the first encounters in this work of the curse of dimensionality phenomena that appear in high dimensions but are not apparent in low dimensions bellman 1966 was the lack of high dimensional benchmark functions to test the scalability of gsa methods on fast models models with many independent inputs can be rare in practice hence the lack of appropriate benchmarks common approach in increasing dimensionality of standard models lies in introducing dummy variables that have zero effect on the model output morris et al 2006 this does not reasonably portray real case studies where numerous noisy variables can be present the addition of dummy variables allows gsa methods such as sobol total order to perform better than other methods since part of their sampling design includes explicit computation of changes in the model output when only one input varies see how matrices a and a b j are constructed in the saltelli estimator in equation 10 therefore we introduced a modified morris model with an arbitrary number of user defined levels of importance existing gsa software implementations are not optimized for higher dimensional models numerous great libraries are available in different programming languages douglas smith et al 2020 but we found that we had to improve their performance for this study for better memory efficiency it is helpful to generate sampling design x in a block iterative manner the main reason is the following given that sampling designs are generated in a unitcube space they afterwards need to be rescaled according to the specified input distributions which means that by working only on a portion of x we actually save memory twice this becomes particularly important during parallelization of mc simulations where each parallel worker needs to generate only a portion of x which consequently simplifies overall resource allocation naturally performing computations on fewer samples is also faster regarding overall run time one should use parallel computations whenever possible and when computing gsa indices especially for methods that employ both x and y in many cases gsa indices are computed independently and are therefore embarrassingly parallel the same suggested modifications apply to convergence and robustness analyses we find that it is not possible to set an absolute or a priori threshold to automatically determine the convergence of sensitivity index confidence intervals as these intervals are derived from the standard error of the chosen sensitivity indices moreover even for normalized indices the actual importance of inputs depends on model complexity regarding ranking of inputs the task of comparing two rankings becomes non trivial in high dimensions where the focus might be shifted from factor prioritization to factor fixing previous works put more emphasis on highly ranked inputs by introducing weighted rank coefficients sarrazin et al 2016 our experiments in appendix a 1 showed that for the purposes of screening this would fail in the presence of many similarly important inputs or tied ranks instead we chose to modify ranks of the inputs by clustering rankings into a finite number of ranks 4 2 convergence and robustness once we obtain sensitivity indices in the bootstrap procedure the three convergence and robustness statistics can be computed efficiently they complement each other and enhance screening analysis by simultaneously plotting stat indices and stat screening one can iteratively adjust tuning parameters k th or s th that determine set of lowly influential inputs in stat screening see section 2 1 1 it also becomes possible to see whether confidence intervals of lowly influential inputs differ from the highly influential ones where higher difference can indicate that sensitivity method is well suited for factor fixing setting the benefit of using stat ranking adjusted for high dimensional screening lies in its comprehensiveness because it considers all sensitivity indices and their relative ranking positions simultaneously given that many parameters are lowly influential stat ranking shows when the separation between highly and lowly ranked inputs becomes more well defined 4 3 comments on gsa methods as we observed in the numerical simulations spearman correlations should be used whenever model is monotone as they require least number of iterations and converge smoothly they can be generally considered useful when local nonlinearities in model are not the main interest of the analysis sobol indices are very powerful as they provide information about model interactions but require a lot of model runs for convergence whenever sobol analysis is performed convergence and robustness in the form of confidence intervals come at no additional cost this is the only considered method that can efficiently produce hundreds or thousands of bootstrap samples delta indices is another powerful gsa method suitable for nonlinear models the computation of indices requires more time than other methods considered in this paper moreover if one intends to run convergence and robustness analyses it is important to use block latin hypercube design finally gradient boosting is a very promising method applicable to nonlinear models this method seems to be as powerful as sobol indices but requires roughly 10 times fewer model runs its high predictive performance on the test set indicates confidence in feature importance results the main drawback is that it needs time consuming tuning one of the most important tuning parameters are maximum depth of trees as it relates to degree of model interactions see section 2 3 we could see in morris models that setting this parameter to the actual degree of interactions led to significant performance benefits and in the lca model high predictive capability was achieved with tree depth set to 4 4 4 insights into lca model what we have learned about lca models from gsa is that they can be more linear than we expect by looking at the inverse of technosphere matrix in the lca model see section 2 5 2 despite the inverse it seems that the effective dimensionality of lca models is low kucherenko et al 2011 it can also be a consequence of reducing all the information from technosphere into a single lca score this finding can greatly reduce the effort in performing gsa in lca because simple correlation or linear regression coefficients are sufficient for reliable analysis another insight that can be obtained from boosted trees is regarding degree of significant interactions effects between model inputs the tuning of gradient boosting suggests that this value is close to 4 for the lca case study of swiss household food consumption on one hand this value can be tested while tuning xgboost for other lca models as many of them are based on similar life cycle inventory datasets and on the other hand this information can help in prioritizing analysis when looking into supply chain of the given system 4 5 limitations of the study 1 one of the limitations of this study is that morris screening procedure has not been tested morris 1991 campolongo et al 2007 2011 the reason for that lies in our understanding that this approach is based on computing distances in high dimensions see section 3 of campolongo et al 2007 which is well known to be affected by the curse of dimensionality see appendix e more specifically as the number of dimensions increases differences between two distances become less prominent which can render morris screening ineffective maximum input space dimensionality for which this approach is still applicable remains to be tested in fairness morris indices serve as proxy for sobol total indices that we did include in the analysis campolongo et al 2011 2 when computing indices for convergence latin hypercube structure needs to be preserved one way of doing that is to use a more sophisticated block sampling design such as progressive latin hypercube sheikholeslami and razavi 2017 3 when studying convergence and robustness of ranking number of clusters has been chosen based on the knowledge of the model for morris function and arbitrarily for lca the algorithm could benefit from a more automated procedure such as one described by sheikholeslami et al 2019 5 conclusions and outlook in the present study we examined the computational performance of spearman correlations sobol indices delta moment independent indices and gradient boosting importances on very high dimensional models for the purposes of screening to benchmark these methods we first proposed a modified morris model with a variable number of influential inputs we then explored the scalability of the gsa methods by applying them to the morris function with the number of model inputs increasing from one to ten thousand sobol total order indices and gradient boosting importances showed the best capabilities for determining lowly and non influential inputs however gradient boosting is based on simple random sampling and needed 5 10 times fewer mc simulations computation of sampling and sensitivity indices were efficient for all methods as a general rule methods that employ only model outputs y for the indices computations tend to be fast which renders convergence and robustness analyses with bootstrapping very efficient on the other hand methods that require both x and y can be slow therefore fewer convergence steps and bootstrap resamples should be chosen for spearman correlations it is possible to compute analytical confidence intervals and consequently stat indices that would serve as an upper bound to values from bootstrapping next we computed maximum confidence intervals for all sensitivity indices subset of lowly influential sensitivity indices and inputs rankings statistic by bootstrap resampling for the convergence of ranking we proposed a quantitative measure stat ranking that compares sensitivity indices clustered into a predefined number of ranks and is particularly suitable for screening in high dimensions in that it is capable of handling tied or very similar ranks in cases where exact ranking is of lesser interest the three considered statistics stat indices stat screening and stat ranking allowed us to monitor convergence and robustness of sensitivity indices screening and ranking for the purposes of factor fixing based on the cumulative analysis we concluded for all morris models that spearman correlations were estimated with reasonable level of confidence at the given number of iterations sobol exhibited robustness and convergence at fewer model runs than anticipated delta indices could benefit from additional samples and xgboost indices could perform well with fewer iterations for some models but overall number of samples has been chosen appropriately we adjusted gsa settings for an lca model used for environmental impact assessment that comes from manufacturing of goods or providing service performed analysis shows high agreement between all methods for the factor fixing sa setting where spearman correlations sobol and gradient boosting show most similar results when determining 90 of the least non influential inputs see table 4 while spearman is a known gsa method for linear models saltelli et al 2008 sobol and gradient boosting are suitable methods for nonlinear models total order sobol indices in combination with first order provide a comprehensive picture of input importances and model interactions but might be too computationally expensive requiring hundreds of thousands of mc simulations for convergence of the indices for high dimensional models at the same time gradient boosting needed roughly 5 10 times fewer model runs while providing as reliable screening results the maximum confidence intervals stat indices and stat screening smoothly decrease as the number of mc iterations grows when comparing final ranking with rankings at fewer samples sobol and gradient boosting provide faster convergence than anticipated for all methods ranking shows high robustness to input samples as the ground truth for non influential inputs was unknown we validated results in two ways and can conclude that the filtered out inputs indeed do not affect model output variability primary conclusion of the study is that there is a clear procedure for robust high dimensional screening summarized in fig 9 it consists of the following steps i generate at least 4k model outputs based on random sampling where k is the number of model inputs ii compute src to estimate degree of model linearity iii perform gsa on the same samples with spearman correlations if the model is monotone and has a high degree of linearity and using gradient boosted trees otherwise iv if computational resources allow perform convergence and robustness analyses with bootstrapping v validate results we suggest 4k mc iterations empirically because a it is recommended as a rule of thumb for regression coefficients jenkins and quintana ascencio 2020 b it is an upper bound on analytically computed number of iterations for spearman correlations bonett and wright 2000 and c it enables convergence and robustness of indices and rankings in gradient boosting analysis for the considered models when using spearman correlations methods also compute stat indices analytically the decision of whether the model is linear enough depends on the purpose of the analysis and is determined by users for instance values higher than 0 8 would mean that less than 20 of variance is not explained by the model which could already be satisfactory step ii is very efficient since linear regression has a closed form solution note that linear regression spearman correlations and gradient boosting can all use the same mc simulations or real data if available validation step can also reuse already generated model outputs but would need additional hundreds to thousands model runs where only influential inputs vary then the total cost of the analysis is around 4k 1 000 model runs it is however possible that for complex models src and boosting would need additional samples in this case results for src can be efficiently recomputed with newly generated samples whereas xgboost trees can be trained iteratively by passing saved trained trees as a warm start to further training an interesting trait of xgboost that remains to be tested further is its feature importance similarity to sobol total indices at significantly fewer mc iterations one particular benefit of using gradient boosting for lca models is that standard lca case studies have very similar model structure and complexity this means that the effort for parameter tuning can be greatly reduced by starting with the tuning parameters identified in this paper as lca models get progressively more complicated by way of introducing physical nonlinear models behind elements of technosphere and biosphere or using real data and measurements gsa with gradient boosting could bring significant benefits to data quality improvement and eventually more reliable assessment of environmental impacts author contributions all authors contributed to the presented study in conceiving and designing the analysis and have reviewed and given approval to the final version of the manuscript a k conducted literature review set up the models performed the analytic calculations the numerical simulations and drafted the paper a f provided inputs on building upon a precursor model a k and c m developed open source python software for the presented study both c m and a f supervised the project software availability name of software gsa framework type python package developers aleksandra kim christopher mutel contact aleksandra kim psi ch christopher mutel psi ch year first available 2020 hardware requirement general purpose computer software requirement python 3 6 or later availability https github com aleksandra kim gsa framework declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements this work was supported by the swiss national science foundation snsf grant 407 340 172445 within the framework of the national research program sustainable economy resource friendly future oriented innovative nrp73 further information can be found at http www nrp73 ch the authors would like to thank dr stefano marelli for the valuable insights and productive discussions moreover we are grateful to the reviewers for their extensive and detailed feedback that allowed us to improve the quality of the paper a convergence and robustness of ranking a 1 statistics to compare two rankings sarrazin et al 2016 introduced a number of statistics that aid in monitoring convergence and robustness of rankings they measure degree of agreement between two rankings r m and r n that are derived from sensitivity indices s m and s n obtained at different convergence steps or during bootstrap procedure in the following we show that the existing statistics do not perform well in the case of high dimensional models because same input can be assigned very different ranks in two independent gsa estimations even if it is clearly recognized as e g non influential in both cases note that we employ consistent notation for rank coefficients ρ but omit weighted versions of spearman coefficients for the sake of brevity then given k model inputs we consider the following statistics 1 unweighted spearman rank coefficient estimated as a 1 ρ 1 1 6 j 1 k r j m r j n 2 k k 2 1 2 correlation coefficient computed on savage scores a 2 ρ 5 1 j 1 k s s j m s s j n 2 2 k h 1 k 1 h where s s j h r k 1 h and r is the rank of input j 3 adjusted and weighted rank coefficient introduced in sarrazin et al 2016 a 3 ρ 6 j 1 k r j m r j n max m n s j m s j n 2 j 1 k max m n s j m s j n 2 sarrazin et al 2016 redefine these expressions in terms of contributions f j of each model input j to the corresponding statistic then ρ 1 ρ 5 can be written as 1 j 1 k f j and ρ 6 j 1 k f j where lower f j leads to higher ρ 1 ρ 5 and lower ρ 6 both ρ 1 and ρ 5 vary between 1 and 1 where higher values indicate higher agreement between two rankings whereas ρ 6 is always non negative with lower values pointing to higher agreement we test performance of ρ 1 ρ 5 ρ 6 on the i non clustered rankings when all model inputs have distinct ranks and ii clustered rankings where each input is assigned one of 4 ranks as described in section 2 1 1 the rankings are computed for the morris function with k 1 000 inputs where x 1 x 10 are highly influential x 11 x 20 are moderately influential x 21 x 30 have low influence on the model output and the rest are non influential see section 2 5 1 sensitivity indices s m s n are spearman correlations estimated on 4 000 samples fig a1 shows results in case of i non clustered rankings on the left and ii clustered rankings on the right we depict only first 50 model inputs on the x axis since the results are similar for the rest of the non influential inputs row 1 shows the ranks assigned to model inputs and rows 2 4 show contributions f j to the statistics ρ 1 ρ 5 ρ 6 whose values are given as titles of respective subplots it is clear that in the non clustered rankings case most of the non influential inputs have high f j simply because same input has been assigned very different ranks by r m and r n even though it is recognized as non influential in both cases this issue is not visible in case of clustered rankings where all three statistics show high agreement between r m and r n which is in line with our expectations therefore in this paper we apply convergence and robustness statistics to clustered rankings fig a 1 analysis of rank coefficients ρ 1 ρ 5 ρ 6 for i non clustered rankings when model inputs have distinct ranks and ii clustered rankings where each input is assigned one of 4 ranks fig a 1 next in order to choose between ρ 1 ρ 5 and ρ 6 we performed further experiments and generated convergence and robustness plots for stat ranking based on these different rank coefficients applied to clustered rankings similar to figs 4 and 8 fig a2 shows all gsa methods in rows and different rankings statistics in columns for the morris model with 1 000 inputs the values of the unweighted spearman are very close to 1 for all gsa methods and convergence steps because for clustered rankings the simplified estimator a 1 would have small values in the numerator of f j ranks differences become smaller in clustered rankings and high constant value in the denominator therefore we added one more column to figure a2 that depicts stat ranking based on spearman coefficient ρ that can account for tied ranks see equation 6 spearman 1961 correlation coefficient computed on savage scores exhibits the same behavior as unweighted spearman by examining figure a1 one can see that both ρ 1 and ρ 5 have very low f i values for inputs that have been ranked differently by r m and r n on the other hand ρ 6 and ρ provide reasonable convergence results in this paper we chose to use the latter because weighted rank coefficient proposed by sarrazin et al 2016 was deemed unnecessary in the presence of clustered rankings with only few distinct ranks moreover its range of variability from 0 to infinity makes it harder to analyze nonetheless it has a satisfactory performance and from the experiments we have seen can be used as a convergence and robustness measure as well fig a 2 convergence and robustness of ranking for morris model with 1 000 inputs for all gsa methods in rows and different rank coefficients in columns the thick blue line shows mean of stat ranking transparent blue region its 95 confidence intervals dashed orange line is the value of perfect agreement between two rankings of the given statistic fig a 2 a 2 algorithm to compute stat ranking algorithm 1 outlines a procedure to monitor convergence and robustness of ranking through stat ranking described in section 2 1 1 convergence of ranking is observed via its mean s t a t r a n k i n g mean whilst robustness by computing confidence intervals s t a t r a n k i n g ci inputs to the algorithm are i sensitivity indices s i b b 1 b for k model inputs are given for convergence steps i 1 n c and bootstrap samples b 1 b ii number of clusters q for sensitivity indices values here we are interested in comparing rankings at all convergence and bootstrap steps to the ranking r obtained from a best guess sensitivity result s for the sake of readability we denote statistics for ranking stat ranking and ranking r obtained from bootstrap sample b at convergence step i as s t a t r a n k i n g i b and r ib respectively algorithm 1 convergence and robustness of rankings image 9 in the absence of ground truth r can be chosen as ranking obtained from sensitivity indices at the maximum number of mc iterations or on the last convergence step from a random bootstrap sample one can also obtain r from bootstrap mean of sensitivity estimates for all model inputs but only in the case when indices for all inputs are independent from each other as in for example spearman correlations and delta gsa method counterexamples are i sobol indices where the sum of all first order indices needs to be less or equal to 1 ii indices that are normalized across all inputs e g by dividing them by their maximum or sum b computational performance fig b1 shows performance gains of gsa framework compared to salib in two scenarios with respect to increasing number of model inputs with constant iterations on the left and increasing number of iterations given constant inputs on the right gains are computed as ratios between the time needed by salib to the time needed by gsa framework to generate saltelli sampling latin hypercube design and compute delta indices note that y axis has a logarithmic scale in all cases gain is higher than one which points to better performance of gsa framework fig b 1 computational performance comparison between salib and gsa framework fig b 1 c analytical sobol indices for morris functions c 1 morris function c 1 z x α i 1 k x i β i 1 k 1 j i 1 k x i x j where α 12 6 0 1 k 1 β 12 0 1 k 1 the expectation and variance of this function are given as follows c 2 e z 1 2 α k 1 8 β k k 1 v a r z e z 2 e z 2 to derive e z 2 let us denote the summands of z x as f x α i 1 k x i and g x β i 1 k 1 j i 1 k x i x j then c 3 e z 2 e f g 2 e f 2 e 2 f g e g 2 where e f 2 1 2 2 3 α 2 k 3 k 1 e 2 f g 1 2 3 3 α β k k 1 3 k 2 e g 2 1 2 6 3 2 β 2 k k 1 9 k 2 3 k 10 the first and total order indices are c 4 s 1 i v a r x i e x i z x i v a r z s t i e x i v a r x i z x i v a r z where c 5 v a r x i e x i z x i 1 12 α 1 2 β k 1 2 e x i v a r x i z x i 1 12 α 2 1 12 α β k 1 1 144 β 2 k 1 3 k 2 c 2 modified morris function c 6 y x λ z x l μ z x m ν z x n where x l x 1 x k x m x k 1 x 2 k x n x 2 k 1 x 3 k λ μ ν 0 1 then numerator and denominator expressions in sobol indices become c 7 v a r x i e x i y x i c 2 v a r x i e x i z x i e x i v a r x i y x i c 2 e x i v a r x i z x i v a r y λ 2 μ 2 ν 2 v a r z where the constant c λ resp μ or ν if x i is from x l resp x m or x n d gsa results d 1 xgboost tuning parameters gradient boosting tuning parameters for all models can be found in table d1 table d 1 gradient boosting tuning parameters table d 1 xgboost parameter description morris 1 000 morris 5 000 morris 10 000 lca model test size fraction of test samples 0 2 0 2 0 2 0 2 learning rate step size in optimization 0 1 0 2 0 2 0 15 gamma gain tree complexity 0 0 0 0 min child weight cover minimum number of residuals in each leaf 30 300 600 300 max depth maximum tree depth 2 2 2 4 reg lambda l2 regularization 10 0 0 0 reg alpha l1 regularization 0 0 0 0 n estimators number of decision trees 500 800 1500 600 subsample fraction of selected samples 0 6 0 3 0 2 0 3 colsample bytree fraction of selected features 0 3 0 3 0 2 0 2 d 2 histograms of bootstrap data we show histograms of p 120 bootstrap samples in figure d1 for a random input in the lca model all gsa methods in columns and increasing number of mc samples in rows one can see that all histograms are somewhat symmetric and do not contain large outliers as we cannot visualize bootstrap data for all model inputs and convergence steps we select these randomly overall number of bootstraps p 120 seems appropriate even though p 1 000 is recommended efron and tibshirani 1994 fig d 1 histograms of p 120 sensitivity indices estimates obtained with bootstrapping for a random input in the lca model all gsa methods in columns and increasing number of mc samples in rows fig d 1 d 3 morris function figures fig d2 shows gsa results for all morris models in columns all gsa methods in rows and all model inputs on the x axis same results for the first 10 of model inputs are given in fig 2 fig d 2 estimates of sensitivity indices for morris models with 1 000 5 000 and 10 000 inputs in columns and for each gsa method in rows are given in blue all model inputs are on the x axis and estimates of sensitivity indices are on the y axis analytical values of sobol total order indices are given in orange fig d 2 fig d3 shows convergence and robustness results of sensitivity indices and screening for all morris models in columns all gsa methods in rows and mc iterations on the x axis with same scale for all methods these results are also given in fig 3 but with different x axis scales fig d 3 convergence and robustness of sensitivity indices for morris models with same scale on the x axis maximum confidence intervals obtained with bootstrapping for all sensitivity indices are given in blue for 98 lowest sensitivity indices in purple and analytical intervals for spearman correlations in orange mc iterations are given on the x axis and stat indices stat screening are on the y axis centered symmetrically around y 0 where at each convergence step blue and purple parts are equal to stat indices and stat screening respectively fig d 3 fig d4 shows convergence and robustness results of stat ranking for all morris models in columns all gsa methods in rows and mc iterations on the x axis with same scale for all methods these results are also given in fig 4 but with different x axis scales fig d 4 convergence and robustness of stat ranking for morris models with same scale on the x axis and stat ranking on the y axis the thick blue line shows mean of stat ranking transparent blue region its 95 confidence intervals and orange trace corresponds to convergence of the internal xgboost metric r 2 score fig d 4 d 4 lca model figures fig d5 shows results for the lca model and all gsa methods in rows where inputs for all methods were sorted according to descending spearman correlations values same results for the first 10 of model inputs are given in fig 5 fig d 5 estimates of sensitivity indices for lca model for each gsa method in rows are given in blue model inputs are on the x axis and estimates of sensitivity indices are on the y axis inputs for all methods are sorted according to descending spearman correlations fig d 5 fig d6 shows validation results for sobol total order delta indices and xgboost importances number of influential inputs has been chosen as 60 for all methods validation results for the spearman correlations are given in fig 6 fig d 6 validation of sobol total delta and xgboost indices gsa results for lca model where monthly lca impacts of an average household food consumption are given in kg co2 eq fig d 6 e similarity measures in high dimensions a number of statistical algorithms depend on similarity measures between two vectors x y r k common similarity measures include euclidean minkowski chebyshev distances etc however in high dimensions all vectors look alike namely given a vector x and a fixed number n of randomly generated vectors y y 1 y 2 y n the difference between maximum and minimum distance from x to y becomes small as the number of dimensions k increases beyer et al 1999 formally this can be written as e 1 lim k d i s t max k d i s t min k d i s t min k 0 where d i s t max k d i s t min k is the maximum minimum distance between x and y in a k dimensional space fig e 1 reduction of differences between distances as number of dimensions increases fig e1 depicts the ratio under the limit for an increasing number of dimensions and few of the most common distance metrics between numeric vectors it is clear that as the number of dimensions increases the difference between distances becomes less and less prominent 33 1 2 2 
25674,global sensitivity analysis gsa is a valuable tool for filtering out non influential model inputs in combination with robustness convergence and validation analyses gsa can be particularly beneficial in interpreting and simplifying models with tens of thousands of independent inputs however there is lack of research on robust screening of such large models where the curse of dimensionality can make existing analyses obsolete we aim to close this gap by evaluating the computational performance of spearman rank correlation coefficients sobol and delta indices and gradient boosted trees regression numerical experiments are conducted for the morris test function and a life cycle assessment model with 10 000 inputs each our results enable us to recommend a standardized procedure for higher dimensional models which efficiently tests for model linearity gsa screening and convergence and robustness analyses of sensitivity indices screening and rankings graphical abstract image 1 keywords global sensitivity analysis high dimensional models robustness convergence validation life cycle assessment environmental impact assessment 1 introduction life cycle assessment lca is a well established tool used for quantification of the environmental impact of goods and services throughout the global value chain it supports environmentally informed decisions in policy making product development and consumer choices hellweg and i canals 2014 however lca results can be highly uncertain due in part to the large amounts of measured and simulated data that are used therefore results can only be interpreted confidently if this uncertainty is sufficiently narrow the number of uncertain inputs in lca models can reach hundreds of thousands because each model incorporates global regionally resolved supply chains of a wide variety of products and services ranging from for example desktop computers in switzerland to power plants in china improving the quality of all of them is not only infeasible but also unnecessary because of the sparsity of factors paradigm a heuristic that only a small fraction of factors influences the model output uncertainty razavi et al 2020 in order to support prioritized data collection we use sensitivity analysis sa the study of how uncertainty in the output of a model numerical or otherwise can be apportioned to different sources of uncertainty in the model input saltelli et al 2008 model inputs are often called factors in the sa field broadly speaking sa can be categorized into i local analysis where only one factor is varied at a time ii screening procedures such as morris method that is based on averaging model outputs from multiple local changes in each factor morris 1991 campolongo et al 2011 and iii global techniques where all factors are varied simultaneously global sensitivity analysis gsa is a more comprehensive technique that can account for interaction effects between combinations of factors and is nowadays computationally feasible for more and more models therefore we will only consider global analysis in this paper unlike problems with a definite correct solution the outcome of sa depends on the purpose of the analysis saltelli et al 2008 define setting as a way of framing the sensitivity quest in such a way that the answer can be confidently entrusted to a well identified measure or in other words to sensitivity index per each model input that is computed with the chosen sa method commonly recognized settings are i factor prioritization in which the aim is to obtain ranked list of factors according to their importance ii factor fixing screening or dimensionality reduction to identify redundant factors that can be fixed to any value without significantly affecting model output iii factor mapping in case we are interested in a particular portion of output distribution iv variance cutting to ensure that the model output variance is under a certain threshold the focus of this study lies in the screening setting one of the first applications of gsa in lca was conducted by lo et al 2005 to compute contribution to variance sensitivity indices and determine key inputs in the municipal waste management lca model the authors demonstrated that uncertainty of the impact estimate can be reduced by almost 50 by collecting site specific data padey et al 2013 then proposed a generic methodology for construction of simplified lca models they calculated sobol indices to determine the most influential factors in an lca model and used them to derive a robust but simple tool for assessing environmental impacts from onshore wind turbines these are typical applications of gsa in lca with the goals of uncertainty reduction and model simplification see pfister and scherer 2015 lacirignola et al 2017 blanco et al 2020 for similar studies a complete protocol for conducting gsa in lca was given by cucurachi et al 2016 who performed sa by first screening the input space and fixing non influential factors to default values and then ranking the remaining factors based on their importance typical gsa applications deal with complex models that have tens to hundreds of inputs but can take minutes or hours per model run see saltelli et al 2004 for more applications there exist various cost effective gsa strategies for such computationally expensive nonlinear models which include improved sampling techniques use of emulators or surrogate models to replace the original model with a faster one and given data approaches that do not rely on a particular sampling strategy and hence can reuse model runs from other analyses sheikholeslami et al 2021 a notable property of lca models however is that one run is computationally very cheap requiring less than 1 second of cpu time moreover all inputs in lca are assumed to be independent and sampled as such given the data in lca models this assumption is reasonable in many cases the rice yield in uttar pradesh has no correlation to coal combustion efficiency in kentucky though we note that in some cases this assumption of independence is due to missing data particularly in the case of single datasets clear relationships between individual inputs and outputs e g fuel consumption and co2 production irrigation water consumed and water evapotranspirated could be included however such techniques are still early in development see lesage et al 2019 lesage 2021a lesage 2021b and are not covered in this work the combination of many independent parameters and quick model evaluations make the application of gsa to lca models unique and interesting due to historically limited computational resources most of the existing works applied gsa only to partial lca models with tens or a maximum of a few hundreds of factors to the best of our knowledge only one work applied a two step gsa protocol to a complete lca model with 30 000 inputs mutel et al 2013 key challenges in the state of the art gsa razavi et al 2020 include computational performance robustness convergence of gsa methods and validation of results and these aspects are also crucial parts of the analysis of high dimensional models only a limited number of studies have addressed the issues of convergence and robustness of gsa indices yang 2011 proposed two procedures to monitor uncertainty in estimation of indices for five gsa methods computation of confidence intervals using the central limit theorem and bootstrap resampling sarrazin et al 2016 focused on quantitative metrics for two important choices number of model runs and threshold for non influential indices sheikholeslami et al 2019 showed that convergence can be achieved much faster by grouping factors based on the proposed bootstrap based clustering while showing promising results no studies have been carried out to investigate applicability of these methods to models with tens or hundreds of thousands of inputs moreover there is a clear lack of benchmark functions that allow testing of gsa methods on fast high dimensional models most functions contain a fixed number of inputs while others allow us to define model dimensionality but restrict it to a certain maximum standard practice in introducing more dimensions is to add dummy variables with zero influence on the model output morris et al 2006 this is not aligned with models that represent our understanding of the real world and can have numerous lowly influential or noisy variables therefore the objective of this work is to investigate the scalability of common gsa methods for screening of very high dimensional models in combination with robustness and convergence analysis and compare their performance on a reasonable test model and an lca case study the output of this analysis will reveal issues that we can only encounter as the number of model inputs increases and aid in deriving standardized procedure for robust and efficient high dimensional screening depending on the complexity of the model the structure of the paper is as follows first section 2 1 looks at computational performance of gsa methods covering convergence robustness and validation of gsa then in section 2 2 we provide a description of three classical gsa methods spearman correlations sobol indices and delta method and study their performance in terms of memory speed and applicability to factor fixing setting section 2 3 gives an overview of dimensionality reduction and observes these properties for one feature selection method gradient boosted trees then section 2 4 describes a method for estimating degree of model linearity in section 2 5 we propose a new morris test model that is adjusted to be suitable as a benchmark in high dimensions and outline an lca case study that assesses global warming environmental impacts of average swiss household food consumption in section 2 6 we provide details on the developed software that we used for numerical simulations we investigate scalability of gsa methods on the modified morris models with 1 000 5 000 and 10 000 inputs as well as study performance of all methods on the lca model in section 3 finally we discuss results in section 4 and provide conclusions and outlook in section 5 together with clear procedure that combines efficient assessment of the degree of model linearity and robust high dimensional screening mathematical notation is given in table 1 2 methods 2 1 computational performance of gsa methods given the complexity of analytical computation of sensitivity indices for nonlinear high dimensional models sampling based methods are the only feasible tools to investigate importance of inputs saltelli et al 2008 a typical approach for analyzing a function y g x g x 1 x k with k inputs is by means of monte carlo mc simulations which involves i creating an input sampling x x i i 1 n ii computing model response y g x i i 1 n at each point x i where n is the number of simulations iii estimating sensitivity indices by using x y tuple or only model outputs y computational burden of model runs is usually the most expensive part of the analysis the amount of time highly depends on the complexity of the model but in most cases this step can be easily parallelized as a general rule methods that need both x and y to compute sensitivity indices tend to require more memory resources and runtime as there are more data to handle on the other hand methods with elaborate sampling designs such as sobol quasi random sequences need more time to generate x and demand more model runs but are very efficient at computing the indices broadly speaking efficiency refers to the total computational resources an algorithm needs which most commonly are memory and runtime the field of analysis of algorithms studies these notions extensively sedgewick and flajolet 2013 in this paper we limit ourselves to a more practical benchmark approach which compares relative performance of algorithms when applied to specific test cases more concretely we want to address scalability of gsa methods as the number of model inputs increases to tens of thousands since the indices are evaluated on a limited number of samples they are also subject to uncertainties therefore notions of convergence and robustness become of great importance pianosi et al 2016 convergence analysis shows how gsa results change as the number of samples increases robustness or stability in the gsa field can be viewed as the ability of an algorithm to produce consistent results under similar conditions e g same number of mc iterations but varying other parameters such as x y tuple chandrashekar and sahin 2014 apart from efficiency convergence and robustness analyses a complete assessment of computational performance requires consideration of reliability of obtained gsa results or in other words their correctness for simple test models true sensitivity indices can be computed analytically but for more complex problems the only option is to numerically validate results in the rest of the section we address quantitative metrics to monitor computational performance of algorithms 2 1 1 convergence and robustness with respect to convergence and given different gsa settings one might be interested in the following aspects convergence of sensitivity indices convergence of screening and convergence of model inputs ranking this analysis can answer the question of how many samples are needed for the gsa setting of interest sarrazin et al 2016 at the same time one can study robustness of algorithms by assessing confidence intervals of an estimated statistic this can be done by i redoing the entire gsa multiple times and applying central limit theorem or ii employing bootstrap with resampling efron and tibshirani 1994 in this work we follow the second approach since it does not require additional sampling designs and new model runs and hence is more computationally efficient yang 2011 if possible it is recommended to use 1 000 or more bootstrap samples efron and tibshirani 1994 oftentimes convergence and robustness analyses are performed together for multiple mc iterations or in other words at multiple convergence steps these steps should be chosen in a range from the minimum number of iterations that can produce sensitivity results to a recommended or computationally feasible maximum number of model runs and with a desired step size or number of steps in this work at each convergence step c which ranges from 1 to the predefined number of steps with the corresponding number of iterations n c we compute the statistic of interest for the first n c simulations in x y we then perform bootstrap analysis by sampling with replacement from these n c simulations to obtain another set of samples of size n c for which we recompute the statistic of interest finally we calculate confidence intervals if one plans to conduct convergence and robustness analyses after performing gsa with the chosen number of mc simulations n it is important to understand specificities of its sampling design and ensure that it allows subsampling some methods have an acceptable performance already with random sampling correlation coefficients and gradient boosting others employ sensitivity indices estimators that require particular sampling designs saltelli estimator described in section 2 2 2 finally there are methods that have better performance when design is optimized but the indices can also be estimated with simpler designs delta method where latin hypercube sampling is more optimal than random sampling see section 2 2 3 for methods with particular sampling strategies the design x of maximum size n should be chosen such that n mn b where m is a user defined constant and each block of samples of size n b has the required design structure then the convergence steps are multiples of n b and the bootstrap procedure should sample with replacement among blocks of size n b and not the individual samples saltelli et al 2010 the only particular design used in this paper is sobol quasi random sequence in sobol indices estimation section 2 2 2 latin hypercube has been subsampled in the same way as random sampling but there exists an extension of classical version that has a block structure sheikholeslami and razavi 2017 in the following we focus on quantitative measures that can be useful to monitor convergence and robustness of screening confidence intervals of sensitivity indices and screening one such measure can be maximum confidence interval of the estimated sensitivity indices s among all model inputs j 1 k sarrazin et al 2016 1 s t a t i n d i c e s s max j 1 k s j u b s j l b where superscripts ub lb refer to upper and lower bounds of the interval calculated with standard error and student s t distribution for all robustness statistics of this study this statistics can be computed at increasing number of mc iterations and convergence is achieved when the width is sufficiently low for the purposes of screening it is also possible to compute stat indices only for a subset j of model inputs which can be i selected to be below a certain threshold s th j j s j s t h or ii based on a predefined number k th of lowest sensitivity values j j s j s k t h sorted where s sorted are sensitivity indices sorted in increasing order sarrazin et al 2016 then this screening statistic can be defined as follows 2 s t a t s c r e e n i n g s max j j s j u b s j l b absolute thresholds s th can be hard to define a priori since confidence intervals depend on standard error and not all sensitivity indices are normalized to a specific interval moreover the complexity of a specific model or its effective dimensionality level of interactions and nonlinearity affect values of sensitivity indices even if they are normalized in this paper along with stat indices we choose to investigate stat screening based on a predefined number of lowest sensitivity values k th confidence intervals of rankings the two previous statistics indicate whether sensitivity index estimates get more robust as the number of iterations increases but do not show whether inputs ranking retains more specifically in the context of screening it does not clarify whether inputs identified as lowly influential on one step stay lowly influential at no additional cost it is possible to capture this more complete picture by monitoring the convergence of ranking which is stricter than the convergence of screening sarrazin et al 2016 two rankings can be compared by means of savage scores rank correlation coefficients and their weighted versions that put more emphasis on agreement between higher or lower ranks sarrazin et al 2016 while being suitable measures for models with up to hundreds of inputs and factor prioritization sa setting these statistics would fail in higher dimensions where the focus is more on screening because slight differences or ties in the values of sensitivity indices can place two similar inputs into ranking positions that are hundreds or thousands ranks apart then the computed statistic would indicate disagreement between two rankings whereas given factor fixing purpose in mind it is not necessarily the case to alleviate this issue we propose to assign same ranks to inputs with similar sensitivity indices by applying jenks natural breaks optimization jenks 1967 this is a clustering method that arranges data into a predefined number of clusters such that variances within each cluster are minimized and variances between clusters are maximized we chose jenks optimization due to its particular suitability for grouping of one dimensional data where the task is reduced to defining optimal borders between data points then statistics that are typically used in lower dimensional cases can be applied to the clustered rankings in this paper we employ spearman correlation ρ see section 2 2 1 in appendix a 1 we outline the benefits of spearman correlations applied to clustered rankings over statistics used on non clustered rankings that have been introduced in previous studies assuming that we are interested in comparing converging rankings with ranking obtained from the highest number of iterations the following can be used 3 s t a t r a n k i n g s ρ r jenks s r jenks s n where r jenks s denotes ranking obtained with jenks optimization procedure for sensitivity indices s and superscript in s n indicates that sensitivity indices were computed with the maximum number of mc simulations available in other words last convergence step similarly to comparing converging rankings in order to study their robustness we compare rankings obtained from bootstrapping with r jenks s n and compute confidence intervals to estimate range of stat ranking at each convergence step it is also possible to assess robustness by comparing pairs of bootstrap rankings within each step but we chose to monitor their similarity with the maximum simulations ranking to ensure that stat ranking value at each step is not incidental we restrict ourselves to computing stat ranking for all convergence steps but the last one where it is trivially equal to 1 algorithm 1 in appendix a 2 describes the overall procedure to obtain mean value and confidence intervals of stat ranking for all convergence steps since s n are not necessarily converged estimates of sensitivities stat ranking should be analyzed with care as the number of iterations increases this statistic should steadily approach 1 to indicate convergence of clustered rankings if its value stabilizes around 1 before the last convergence step then more samples do not change the clustered rankings and their convergence has been reached if stat ranking is still low at the last convergence steps e g below 0 5 then ranking at the last step is not reliable and more mc iterations are needed 2 1 2 validation there exist various ways of validating gsa results multiple studies consider three sets of model simulations i unconditional outputs y all g x are obtained when all model inputs vary randomly ii conditional outputs y inf g x x inf are computed when influential inputs vary consistently with the first set and the non influential ones are fixed to the most probable or prescribed values iii finally conditional outputs y inf g x x inf are calculated from varying non influential inputs consistently with the first set and setting the influential ones to fixed values andres 1997 proposes to visually assess differences between pairs of simulations in the two cases a y all y inf and b y all y inf by means of scatter plots nossent et al 2011 and tang et al 2007 use correlation coefficients to quantify these differences low correlations in the first case and high correlations in the second case point towards reasonable choice of non influential inputs sarrazin et al 2016 a variation of this method was originally proposed by pianosi and wagener 2015 it computes cumulative distribution functions of unconditional y all and conditional y inf model outputs and quantifies their discrepancy with the kolmogorov smirnov ks statistic since this approach is conditioned on non influential inputs it should be repeated for multiple x inf values that sufficiently cover the input space then a summary ks statistic mean median or maximum of ks statistics obtained at different conditioning values is calculated and a two sample ks test is applied sarrazin et al 2016 this is a recommended way of performing validation to avoid bias from a particular set of conditioning values interestingly the first approach based on correlation coefficients is usually performed only at one set of values andres 1997 nossent et al 2011 tang et al 2007 as the second approach based on the ks statistic is computationally expensive for high dimensional models in this paper we perform validation by computing spearman correlations between y all and y inf but note that other validation techniques are also suitable additionally we can go back to the definition of gsa and look at the overlap between histograms or probability distributions derived from y all and y inf a numeric value of validation performance can be any distance measure between them in this study we employ the first wasserstein distance between distributions u and v derived from y all and y inf respectively which is defined as w 1 u v inf π γ u v r r x y d π x y where γ u v is the set of all joint probability measures on r r whose marginals are u and v ramdas et al 2017 this distance can range in 0 where smaller values indicate bigger similarity between two distributions numerically it is computed as the distance between respective cumulative distributions virtanen et al 2020 in the following we provide an overview of the three selected sensitivity methods and discuss their intrinsic properties that can be taken into account in convergence and robustness analyses at no additional cost 2 2 sensitivity analysis in recent years a plethora of good literature surveys on sa have been conducted pianosi et al 2016 comprehensively review sensitivity methods and include practical guidelines on conducting the analysis gan et al 2014 study effectiveness and efficiency of ten gsa methods and compare their performance on a hydrological model with thirteen factors that have uncertainties borgonovo and plischke 2016 provide overview of various local and global methods and discuss interpretation of gsa results in detail in the following we will provide description of the chosen gsa methods 2 2 1 correlation coefficients correlation coefficients are widely used as sensitivity measures due to their simplicity lee rodgers and nicewander 1988 they measure degree of association between two variables x and y and are applied on the samples observations of the variables pearson product moment correlation coefficient describes linear relationships for quantitative data and is given by r x y cov x y σ x σ y where cov x y is covariance and σ x σ y are standard deviations expressed as cov x y e x e x y e y σ x 2 e x e x 2 and σ y 2 e y e y 2 for the nonlinear models with monotonic relations rank correlation coefficients can be used such as spearman ρ kendall τ partial rank correlation coefficient etc for example to compute spearman correlation first x and y are converted into rank variables x r y r and then pearson coefficient is computed 6 ρ x y r x r y r cov x r y r σ x r σ y r correlation coefficients are dimensionless indices that take values from 1 to 1 ranging from perfect negative to perfect positive correlation where values close to 0 indicate weak correlations these coefficients have well characterized estimators such as ρ x y 1 6 i 1 n x r y r n n 2 1 where n is the number of observations they are also invariant under linear transformations in the case of gsa correlation coefficients between x j and the output y for all j 1 k can be chosen as sensitivity indices a drawback of correlation coefficients lies in the underlying assumptions i they are suitable only for linear pearson and monotone rank coefficients relations helton and davis 2002 ii for the pearson coefficient variables should be continuous and normally distributed iii for the rank coefficients variables should be measured on an ordinal interval or ratio scale the advantage however is that correlation estimators are well defined and require relatively low number of samples to produce confident estimates that is invariant of the number of model inputs convergence and robustness bonett and wright proposed a procedure to compute sample size that yields estimation of the pearson spearman or kendall τ coefficients in a desired confidence interval bonett and wright 2000 this procedure takes correlation value to which the estimate should converge desired confidence width and confidence level as inputs and the output is the sample size needed for such an estimation if the approximate value of the coefficient is unknown one can obtain sample sizes for a range of correlation values and choose the maximum number of samples to ensure the desired confidence width higher values of correlations tend to require fewer samples thus irrespectively of the model dimensionality hundreds to few thousands of iterations is enough for a confident estimation of correlation based sensitivity indices schönbrodt and perugini 2013 alternatively utilizing the same procedure but with the aim of convergence and robustness analyses one can compute confidence intervals for a given sample size and correlation value if the approximate correlation value is unknown widths of confidence intervals for a range of correlation values can be derived analytically and in line with stat indices in equation 1 maximum confidence interval width can be estimated without the need for bootstrapping the simplicity of this method makes it very attractive for gsa applications if the underlying model monotonicity assumption is satisfied if this is not the case variance based sensitivity methods are capable of handling model nonlinearities 2 2 2 variance based methods the idea of variance based methods lies in decomposition of the model output variance into fractions that can be attributed to different sources of variation such as individual model inputs or their combinations methods such as fourier amplitude sensitivity test fast by cukier et al 1973 analysis of variance anova decomposition of a function by sobol 2001 importance measures of homma and saltelli 1996 all refer to similar concepts and represent variance based methods among these first and total order sobol indices gained the most popularity for each model input x j with j 1 k first order index takes into account only main first order effect and the total order considers also interaction effects between x j and all other inputs together first and total order show a comprehensive picture of inputs importances and interactions in this paper however we only focus on total order indices due to their applicability for screening purposes their expression is given as follows 8 s j t e x j v a r x j y x j v a r y where x j denotes all inputs but j intuitively s j t computes fraction of output variance when all inputs are fixed and x j varies low values of s j t indicate non important inputs numerically sobol indices convergence is proportional to 1 n where n is the sample size per model input nossent et al 2011 to reduce computational cost elaborate sampling procedures and numerical estimators of the indices have been proposed saltelli estimators saltelli et al 2010 developed an estimation procedure that uses same model runs for both first and total order indices it is based on a b a b j r m k sampling matrices generated from sobol quasi random sequence joe and kuo 2003 and radial basis design saltelli et al 2010 input sampling a and b are two independent matrices whereas a b j contains all columns from the matrix a except for the j th column which takes value from the respective j th column of b this essentially reflects the case when only j th parameter varies then the estimator for the total index is given as saltelli et al 2010 nossent et al 2011 10 s j t 1 2 m i 1 m g a i g a b i j 2 v a r y where v a r y 1 2 m 1 i 1 m g 2 a i g 2 b i 1 m i 1 m g a i g b i and a i b i a b i j are i th rows of the respective matrices computational burden of sobol indices is n m k 2 where k is the number of model inputs and m is a user defined constant the higher m results in better convergence of the indices but the exact number of simulations depends on the model complexity convergence and robustness there are a number of possible intrinsic metrics to monitor convergence of sobol indices i both first and total sobol indices should be non negative ii first order indices should be less than or equal to one iii total indices should be larger or equal to first order because in addition to main effects they take into account interactions convergence and robustness analyses are well studied and can be done efficiently nossent et al 2011 while being easy to implement and interpret there are multiple drawbacks associated with the method of sobol da veiga 2015 first of all the number of simulations required for convergence of the indices varies from model to model and can reach millions for high dimensional cases secondly by definition variance based methods are limited to computation of variances meaning that distribution of the model variability is collected into a single value statistic and potentially useful information is lost moreover there is no known way of handling multimodal distributions in the next section we describe the delta method that overcomes all of these drawbacks 2 2 3 delta moment independent measure the delta moment independent measure has been first introduced by borgonovo as means to identify inputs that influence the entire output distribution the most borgonovo 2007 the idea of the method is to measure the shift in uncertainty distribution of y when parameter x j is set to a fixed value this is expressed as the following integral 11 δ j 1 2 f x j x j f y y f y x j y x j d y d x j where f x j x j f y y are density functions and f y x j y x j are conditional density functions of y given fixed x j this sensitivity index takes values from 0 to 1 where δ 0 if and only if y is independent from x j and 1 if y depends only on x j another property of delta indices is invariance to monotonic transformations in principle the inner integral is a separation dissimilarity measure between two distributions the idea is to compare f y and f y x j by computing various f divergences between them the authors chose l1 norm for the delta index by choosing a different measure this integral can be reduced to for example sobol first order index da veiga 2015 plischke et al 2013 describe a consistent estimator of delta indices that utilizes kernel density estimators the total cost of the computation is n mk where m should be sufficiently large to estimate the two integrals in equation 11 convergence and robustness by definition delta indices take values in 0 1 interval which is automatically taken into account when employing kernel probability density estimators plischke et al 2013 showed that theoretical asymptotic convergence of the delta estimator is guaranteed the authors also tackled the issue of gsa results robustness for two cases at large sample size they introduced a bias controlling filter and at a small sample size robust determination of confidence intervals through the bias reduction bootstrap approach given that delta indices are computed using both x and y bootstrap approach is computationally expensive as the number of model inputs increases 2 3 dimensionality reduction and feature selection in this section we provide brief introduction to dimensionality reduction algorithms whose importance for gsa applications has been recognized by a number of authors da veiga 2015 razavi et al 2020 one of the main motivation to investigate these methods lies in their ability to handle high dimensional input spaces together with nonlinearity in models despite the fact that relevancy metric is not necessarily defined in the same sense as in gsa contribution to model output uncertainty this family of methods can serve as a good starting point for factor fixing in machine learning ml dimensionality reduction is the process of reducing the number of dimensions in the factor feature or variable in the ml terminology space guyon and elisseeff 2003 people distinguish between two types of dimensionality reduction techniques feature construction and feature selection huang et al 2019 the former is also called feature extraction these methods are aimed at achieving good reconstruction of the original input data unsupervised or being best for predicting model output supervised with a more compact subset of features lee and verleysen 2007 they provide a lower dimensional and hence less memory intense approximation of the initial input data however with most of the nonlinear feature construction methods it is impossible to derive the importance of individual features on the other hand feature selection methods are needed to select relevant features from the original ones without modifying them they are often used in combination with other statistical methods regression and classification where their intrinsic objective function determines the quality of the selected features chandrashekar and sahin 2014 give an overview of feature selection methods with application to few standard datasets most notable are algorithms based on decision trees such as random forests and gradient boosted trees in this study we chose to focus on gradient boosting that being an ensemble method based on trees has high predictive capability provides natural metrics for feature selection and gained popularity in numerous kaggle competitions due to its efficient implementations raschka et al 2020 gradient boosting is a popular supervised machine learning data driven approach that produces a prediction model as an ensemble of weak learners friedman 2001 this numerical optimization problem aims at minimizing the loss of the model by adding weak learners in a stage wise manner with a gradient descent procedure the loss can be chosen arbitrarily to fit regression classification or ranking purposes and has to be differentiable if the weak predictors are decision trees then it is called gradient boosted trees based on which input variables were used to build a tree ensemble model that predicts the output one can gain understanding of relative importance of particular inputs to the model variation hence by building a good prediction model feature importances are obtained as a by product moreover friedman 2001 explicitly mentions analysis of variance anova decomposition in the paper signifying the link between the maximum tree depth and interactions effects for example if a model achieves good prediction results with trees of depth 3 then important interactions are combinations of 3 inputs such as x 1 x 2 x 3 or x 6 x 8 x 9 gradient boosting algorithm as in all supervised algorithms x i y i i 1 n are divided into n train samples of the training set and n test n n train samples of the testing set the objective is to use training set to obtain an approximation f of the function f x that maps x to y while minimizing the expected value of some loss function l f x y over x y values chen and guestrin 2016 then the tree ensemble model uses p additive functions to predict the output y i and has the following form 12 y i f x p 1 p h p x i where h p s are weak learners which in the case of gradient boosted trees correspond to tree structures and all p trees constitute an ensemble of trees the loss function is given by 13 l f x y i 1 n train l y i y i p 1 p ω h p where ω h γ t 1 2 λ w 2 here l is a loss function for the difference between the prediction y i and observed output y i the ω term is needed to penalize complexity of a tree h it contains number of leaves t n multiplied by the tuning parameter γ r 0 called gain that encourages pruning when set to a positive value and regularization term on the norm of leaf weights w r t where λ r 0 determines degree of smoothing to avoid over fitting trees are added sequentially based on optimization of the loss function such that each subsequent tree improves the model notable feature of gradient boosting is that the training can be done iteratively and it is possible for other boosted tree models to serve as a warm start for speed gains moreover tree models can be inputs to shapley values analysis another nonlinear gsa method that can be used for factor ranking owen 2014 as the focus of this paper is on screening techniques we do not explore shapley values further feature importance measures for relative importance of features arise naturally in tree based algorithms their exact computation can differ but the core idea stems from estimating each feature s contribution to the construction of the trees for a tree h the estimate of relative importance of the model input x j can be computed as proposed by breiman et al 1984 14 i j 2 h t 1 j i t 2 1 v t j where j is the number of nonterminal nodes of tree h or in other words nodes that are not leaves v t is the splitting variable associated with node t i t is the empirical improvement in the objective function that comes from the split and 1 is the indicator function that is zero everywhere except for the case v t j when the split variable v t is equal to the considered j th model input friedman 2001 for the case of regression trees to evaluate the split of the current node r into left and right nodes r r r l the expression i t 2 takes the form 15 i t 2 r r r l w l w r w l w r y l y r 2 where y l y r are means of observations that fall into the left and right nodes respectively as a result of the split and w l w r are the sums of observations weights which are measures of the influence of each observation on the objective function during training stage these can be chosen to favor splits with more equal number of observations in the left and right nodes or such that variances of observations in the left and right nodes are closer friedman 2001 for a collection of p decision trees expression 14 can be averaged as follows 16 φ j 1 p p 1 p i j 2 h p since the range of variation of this metric depends on the constructed trees we normalized it by the sum of φ j for all model inputs j the gradient boosting sensitivity index is then given as 17 i j 2 φ j j 1 k φ j and takes values from 0 to 1 convergence and robustness regression based supervised methods including gradient boosted trees used in this study have an intrinsic measure to evaluate performance of trained models examples can be coefficient of determination explained variance mean squared error and others these metrics can be viewed during convergence and robustness analyses at no extra cost in this paper we will only consider coefficient of determination denoted as r 2 the proportion of the variance in the model output that is predictable from the model inputs this metric s range is from 0 to 1 where values close to 1 indicate substantial correlation between predicted and given model outputs from the test set 2 4 degree of model linearity coefficient of determination can also be used as a measure of model s degree of linearity that will prove to be useful for deriving a standardized procedure for robust high dimensional screening to that end saltelli et al 2008 first compute standardized regression coefficients src for all model inputs j 1 k 18 β j b j σ x j σ y where coefficients b j can be determined by ordinary least squares with a solution b j x t x 1 x t y and are normalized by σ x j σ y standard deviations of the model input x j and output y then the degree of model linearity can be estimated with the coefficient of determination as r 2 j 1 k β j 2 it is the fraction of model variance that is explained by the given linear regression model where values closer to 1 point to higher degree of model linearity given the existence of the closed form solution src can be computed efficiently even for high dimensional models 2 5 test cases in the following we introduce two test models new analytical function that we derive by modifying the standard morris model to include arbitrary levels of input importances in a straightforward manner and an environmental lca model 2 5 1 morris function morris function was first introduced by morris et al morris et al 2006 its expression is given by 19 z x α i 1 k x i β i 1 k 1 j i 1 k x i x j where α 12 6 0 1 k 1 β 12 0 1 k 1 the function is evaluated on the 0 1 k dimensional hypercube the total number of inputs k as well as the number of influential inputs k can be chosen arbitrarily all influential inputs are equivalent and interact with each other in order to compare performance of sensitivity methods in identifying inputs of different importance levels we propose to modify the given morris function as follows 20 y x λ z x l μ z x m ν z x n where x l x 1 x k x m x k 1 x 2 k and x n x 2 k 1 x 3 k all k inputs from one group l m or n have the same importance level which is determined by constants λ μ ν 0 1 such that higher values indicate higher importance in this paper the authors consider three cases k 1 000 5 000 10 000 with k k 100 and the constants chosen as λ 1 μ 1 2 ν 1 10 this way sobol indices of inputs in x l are two times larger than inputs in x m and ten times larger than inputs in x n the reader is referred to appendix c for the analytical expressions of sobol indices knowing the indices of morris function allows an easy computation of the indices for the modified morris function the rest k 3k inputs are dummy variables that have zero influence on the model output 2 5 2 lca model lca is a well established and widely used methodology for environmental impact assessment hauschild et al 2018 it allows estimation of various impacts along the supply chains of a product life cycle inventory analysis compiles information about physical flows which are inputs of resources and materials and outputs of emissions waste and products these are often taken from proprietary databases life cycle impact assessment groups together the previously collected flows according to impact categories of interest these can be climate change human health land and water use resource depletion etc lca model can be written in matrix based form as shown in fig 1 or symbolically 21 y c b a 1 d components of the matrix based lca demand vector d consists of amounts of products whose environmental impacts we need to estimate for example to estimate impacts from a production of one passenger car the demand vector would contain zeros everywhere but in the row corresponding to passenger cars where its value would be 1 unit the exact location of this row is in accordance with the technosphere technosphere a is a square matrix that contains information about manufacturing activities needed to produce various products each element in the a matrix is called an exchange for a product passenger car in one of the rows it would be linked to for example electricity steel water production activities that are needed for its manufacturing the inverse of the technosphere matrix is taken for the purposes of scaling the physical amounts of all activities in such a way that they produce the desired demand biosphere b is a non square matrix that provides mapping from manufacturing activities to environmental flows that appear due to these activities for instance to produce a car certain amount of electricity is needed that requires combustion of fossil fuels which in turn causes carbon dioxide emission into the atmosphere characterization c assigns weights to these emissions environmental flows that characterize contribution of flows to specific impact categories finally impact values or scores y are computed for each impact category typically information about manufacturing activities and their environmental flows is stored in databases that consist of tens to hundreds of thousands of uncertain inputs their uncertainty distributions are commonly chosen to be lognormal normal uniform and triangular and are specified by experts in relevant fields treating elements in the technosphere matrix as random variables x j with predefined distributions results in the model y g x by nature a is a sparse matrix with independent entries because e g production of a car in europe does not require agricultural activities from canada functional unit d governs which of the activities contribute to the assessment of environmental impact however due to nonlinearity of lca and interconnected global supply chains analytical derivation of influential inputs is not possible moreover solving a 1 d as a system of linear equations results in very fast computations in the order of milliseconds or seconds per model run hence even though lca model is very high dimensional it is also possible to run it thousands or millions of times froemelt et al 2018 2020 provided a detailed model of household consumption based on the data from swiss household budget survey which contains information on expenditures income purchased goods and consumer behavior of 9955 households in this study we chose functional unit to reflect food consumption of an average swiss household per month and the impacts of interest are greenhouse gas emissions estimated in kilograms of co2 equivalent abbreviated as kg co2 eq 2 6 software we ran numerical simulations using python package gsa framework that we are currently actively developing implementation includes base class for a generic gsa method that links sampling design model runs and computation of sensitivity indices as well as saving of all necessary matrices in a memory and runtime efficient way that means whenever the time for generating certain output was substantial we choose to save it under a file name that uniquely identifies parameters necessary to reproduce the output then next time the same output is required it is simply read from file this also contributes to reuse of the sampling designs and model runs the base gsa class can be passed to child classes to add specific sensitivity methods so far we have implemented estimators for correlations sobol and delta moment indices for gradient boosting we used python implementation of xgboost a scalable tree boosting system that gained a lot of popularity in the ml community for its extreme computational performance and state of the art prediction capabilities chen and guestrin 2016 additionally gsa framework package includes classes for custom models convergence robustness and validation and functions for results visualization see comprehensive description in the technical documentation 1 1 https gsa framework readthedocs io whenever it was beneficial we parallelized the code using native multiprocessing library among contributions of this software is scalability and computational performance improvements the former is needed in terms of ram requirements for high dimensional models regarding the latter performance comparison with the existing salib 2 2 https salib readthedocs io implementations is given in appendix b testing of the software is done with unit tests where outputs of our framework were compared with the salib implementations taken as ground truth all code is open source and is available at https github com aleksandra kim gsa framework 3 3 further inquiries at aleksandra kim psi ch under the bsd 3 clause license 3 results in order to study scalability of gsa methods we consider morris models with increasing number of inputs 1 000 5 000 and 10 000 numerical simulations encompass gsa results robustness and convergence for spearman correlations sobol total order indices delta indices and feature importances from gradient boosted trees we further apply same analysis to the lca model as well as validate results the number of convergence steps has been chosen as 50 for all models and methods except for sobol total order indices where it was adjusted to the sizes of sampling blocks and resulted in 97 steps for the morris functions and 38 steps for the lca case study numerical simulations for all morris models were conducted under the same conditions on a 2 6 ghz 6 core intel core i7 personal computer with 16 gb 2400 mhz ddr4 memory to tune gradient boosting we ran grid search optimization with cross validation a greedy procedure for performing exhaustive search for optimal tuning parameters see concrete values in appendix d 1 for all models for the convergence and robustness of ranking the number of clusters for the jenks optimization procedure has been set to 4 for the morris model which is based on the prior knowledge of the chosen number of importance levels and arbitrarily to 10 for the lca case study see section 2 1 1 number of bootstrap resampling p has been set to 120 in all cases where the recommended p 1 000 was not possible due to computational limitations by increasing p width of a confidence interval might i increase if new estimates of sensitivity indices lead to higher standard error used for the confidence interval calculation ii decrease if the standard error reduces iii not change substantially if the number of mc samples is high enough to estimate sensitivity indices confidently imagine infinite mc samples efron and tibshirani 1994 one way of analyzing whether higher p should be used is to look at histograms of bootstrap data if they are bell shaped unimodal and somewhat symmetric and do not have large outliers then the chosen number of bootstraps is acceptable in our case see section d 2 in the appendix for the lca model example the considered histograms are visually symmetric and without outliers therefore p 120 and student s t distribution are appropriate choices when computing confidence intervals 3 1 morris function we set the number of iterations n for all models consistently and as a function of model s dimensionality k a priori gsa methods require different number of iterations for the spearman correlations we chose n 4k based on the procedure for determining sample size for confident estimation of correlation coefficients see section 2 2 1 in the previous works sobol indices have been estimated on 500 1 000 saltelli et al 2008 pianosi et al 2016 12 000 nossent et al 2011 500 50 000 sarrazin et al 2016 iterations per model input given the high dimensionality of our models and that low sensitivity indices tend to reach convergence faster than high ones nossent et al 2011 we set n 100k for sobol indices due to computational requirements of the delta method we could only afford 8k iterations even though this method can require hundreds to thousands samples per input plischke et al 2013 regression methods recommend at least 4k iterations jenkins and quintana ascencio 2020 which was in the end chosen for the gradient boosting 3 1 1 gsa results fig 2 shows gsa results for morris models with 1 000 5 000 and 10 000 inputs additionally we plot analytical values of sobol total order indices with derivations given in appendix c one can see that all methods show similar results and are able to distinguish between the four chosen levels of input importances sobol total order and xgboost indices are best at identifying inputs with zero importance whereas spearman correlations and delta assess those inputs as lowly influential but still with non zero estimates delta indices can separate highly influential inputs from lowly but at the given number of iterations performs worse in distinguishing between lowly and non influential inputs for all morris models however the bootstrap correcting procedure which can greatly improve delta estimates has not been included due to already high computational cost given that true values for this model are known validation of gsa results was not needed 3 1 2 scalability of gsa methods gsa results indicate that all methods are scalable but the question is at which computational cost table 2 provides a summary and visualization of the following aspects i number of mc runs as factor of respective model dimensionality k ii time for generating required sampling design x in unitcube space iii time needed to compute sensitivity indices iv amount of space required to store data for gsa computations by looking at the table we can draw a conclusion that no matter the model dimensionality sampling usually does not pose a serious computational problem the maximum time was needed for the saltelli estimates but x was also generated for as many as 100k iterations we can also observe that those methods that have elaborate sampling designs require more time to generate x and y but take almost no time for computing sensitivity indices hence when employing these methods one should definitely provide convergence and robustness results 3 1 3 confidence intervals of sensitivity indices and screening fig 3 shows maximum confidence intervals of sensitivity indices stat indices in blue equation 1 and stat screening in purple equation 2 obtained with bootstrapping for all morris models and all gsa methods since these statistics are linked to confidence intervals we center them around y 0 for each convergence step such that the blue purple part is equal to stat indices stat screening the interval s lower bound is below zero and its upper bound is above zero in this case bounds happen to be symmetric with respect to y 0 because we used student s t distribution to compute confidence intervals other types of confidence intervals might result in asymmetric plots note that we visualize these statistics differently than the authors of the original publication sarrazin et al 2016 we set k th 0 98k in stat screening for all morris models with k number of inputs comparing stat indices and stat screening one can see that convergence of screening is reached faster than convergence of sensitivity indices estimation for all methods spearman correlations and delta method exhibit smooth convergence that does not differ substantially for stat indices and stat screening indicating that confidence intervals for lowly and highly influential inputs are similar for the spearman correlations analytical confidence intervals are rather conservative because these are computed as maximum intervals within entire range of correlation values which we do not necessarily encounter in practice hence bootstrap intervals are narrower sobol total order indices have higher fluctuations in stat indices than stat screening confidence intervals on the first few steps indeed fig 3 shows that higher values of total indices have more variability than lower values given that groups of similarly important inputs in the modified morris model should be estimated at the same value this is in line with previous studies which suggest that convergence for sobol total indices is reached faster for lower values of the indices than for higher nossent et al 2011 delta indices require all the iterations that we can afford with given computational resources since even at the last convergence step the separation between different levels of inputs importances is not clearly visible see fig 2 we can also observe an interesting pattern in the results of gradient boosting feature importance seems to converge smoothly for morris models with 1 000 and 5 000 inputs but exhibits a sudden drop in value closer to 20 000 iterations for the model with 10 000 inputs the drop can be a consequence of tuning namely there is a user defined parameter that determines minimum number of samples required in tree leaves for growing the tree when set to a high value with respect to the total number of samples this parameter would encourage tree pruning which results in an ensemble of simpler but also less capable trees that do not account for the desired level of interactions bias variance tradeoff in this case trained tree would produce worse predictions that also affects feature importance since tuning xgboost for all convergence steps was a tedious task we set all tuning parameters including minimum number of samples to same values for all steps such that the tuning produces best results for the maximum number of iterations last convergence step by optimizing tuning parameters at each convergence step we believe it is possible to achieve faster convergence by comparing these results with the absolute values of sensitivity indices depicted in fig 2 one can define approximate threshold values s t a t s c r e e n i n g t h that determine minimum number of iterations to reach convergence for the purposes of screening we chose to define this threshold as the difference between sensitivity indices of the lowest moderately influential input and the highest lowly influential input table 3 contains thresholds and number of iterations when convergence is reached for all gsa methods and all morris models 3 1 4 confidence intervals of rankings fig 4 shows evolution of statistic for rankings and its confidence intervals for all morris models obtained from bootstrapping between rankings at the current convergence step and ranking at the last convergence step with respect to convergence for spearman correlations we can see that stat ranking statistics are robust at all steps and become more similar to the final ranking as the number of iterations increases almost reaching perfect correlation closer to 4k iterations stat ranking convergence has same tendency for all models but is slower for models with more inputs which is in line with stat screening results see table 3 for sobol indices this statistic converges after approximately 30k iterations which is a bit lower than stat screening for 1 000 inputs but higher than stat screening for 5 000 and 10 000 inputs the rather fast convergence can be explained by the strength of this method in determining non influential inputs that constitute large fraction of all inputs in high dimensional models however there is quite some variation within each convergence step for morris models with 1 000 and 5 000 inputs as the number of model inputs and hence also number of mc iterations increases confidence intervals become narrower in all cases even the lower bound is still high enough to show agreement between rankings for the delta method we can observe a step at around 7 7 5k iterations from this moment on stat ranking has a larger value interestingly we did not see any steps while studying convergence of sensitivity indices in fig 3 this can be an artefact of sampling because we used regular latin hypercube sampling instead of block designs so the properties of latin hypercube are only fully present for the last convergence step and it is possible that an important portion of the input space has not been analyzed until 7 7 5k iterations see section 2 1 1 notably despite having low mean values stat ranking is quite robust for delta moment finally xgboost shows convergence of both ranking and r 2 score for all morris models the pattern similar to the one in fig 3 for the model with 10 000 inputs can be observed here as well and after 18 000 iterations rankings are robust by collecting convergence and robustness results for sensitivity indices screening and ranking we can conclude for the goal of factor fixing that for all morris models i spearman correlations reach convergence closer to the last step ii sobol indices do not require as many iterations and convergence is reached faster than anticipated therefore approximately 40k iterations would have been sufficient iii delta method requires more samples iv number of iterations for gradient boosted trees has been chosen appropriately 3 2 lca model in the following section we present results for the lca model with 10 000 inputs we first show how all gsa methods perform then validate results and finally similarly to the morris model above demonstrate convergence and robustness of sensitivity indices and ranking from the convergence results of morris model we adjusted some of the settings in lca for example to save computational cost we restricted ourselves to fewer iterations in the sobol method 400 000 as opposed to one million otherwise number of iterations for each gsa method remained the same as for the morris functions we also tuned gradient boosted trees such that tuning parameters are suitable for all convergence steps 3 2 1 gsa results fig 5 shows results for the lca model and all gsa methods in rows where inputs for all methods were sorted according to descending spearman correlations values for better understanding visually one can see that few model inputs were identified as influential by all methods the non influential inputs are best seen by sobol and xgboost approaches as there are many values close to zero when training the model for gradient boosted trees we noticed that predictive performance improves when maximum tree depth is set to 4 which means that degree of significant interaction effects between model inputs is also close to 4 at least for the case study of swiss household food consumption in order to quantify the agreement between results of different methods we intersected the 1 most influential inputs and 90 least non influential inputs between all methods leaving some inputs unclassified for the sake of being conservative in the screening analysis and avoid filtering out influential inputs we chose to separate inputs based on the percentages of the total number of inputs as opposed to threshold values for sensitivity indices because the thresholds would need to be specific for each gsa method and it is unclear how to choose them for a fair comparison additionally choosing the number of inputs is more intuitive in lca models because it directly relates to the number of lca datasets for which better quality data can be collected given available resources comparative table 4 shows agreement of influential in orange at the lower triangular block and agreement for non influential in blue in the upper triangular block as the ratio between cardinality of inputs intersection between a pair of methods to the total number of considered inputs 1 and 90 respectively the agreement seems to be low for influential inputs for some method pairs which could mean that a number of non influential inputs have been identified as influential at this stage of the analysis it is important to remember that 1 of 10 000 inputs is already 100 inputs so an agreement higher than 0 7 refers to a rather high number of 70 inputs further gsa for factor prioritization might be needed to derive a prioritized list of influential inputs as the focus of the paper is on screening we are more interested in the non influential part of the table that shows sufficiently high agreement above 0 9 between all pairs of methods which yields more than 8 000 inputs for all methods combinations finally the degree of model linearity computed with src is equal to 0 988 indicating that lca model is quite linear see section 2 4 this can explain good performance of correlation coefficients however it does not mean that all lca models are linear but rather that the particular case study of swiss food consumption has this property 3 2 2 validation of gsa results validation was done with both correlation coefficients and distances between histograms as described in section 2 1 2 table 5 shows results of validation for all gsa methods to compute y inf we chose to vary only the first 60 influential inputs any other number of inputs can be chosen depending on the goal of the analysis the non influential inputs have been set to means of their distributions which are typically used in deterministic lca computations when uncertainty and sensitivity analysis are not performed since validation results are similar for all methods we only show figure for spearman correlations for other methods see fig d6 in appendix d 4 fig 6 shows i on the left scatter plot between y all on the x axis in blue all inputs vary and y inf on the y axis in orange only influential inputs vary ii on the right the histogram of y all and y inf both visually and numerically we can see that the filtered out non influential model inputs do not significantly affect variation of the model output correlation coefficient between y all and y inf is high and distance is low 3 2 3 confidence intervals of sensitivity indices and screening similarly to the morris model we show results for convergence and robustness of sensitivity indices and screening in fig 7 where stat indices and stat screening are centered symmetrically around y 0 and k th 0 9k 9 000 inputs see equation 2 maximum confidence interval among all model inputs decreases as the number of iterations increases results are similar to morris models in that stat indices and stat screening are alike for spearman correlations and delta and screening converges much faster than sensitivity indices for sobol and xgboost methods in case of the lca model given that we want to separate the 1 most influential and 90 least influential inputs we could identify the following convergence results for the spearman correlations s t a t s c r e e n i n g t h 0 005 59 which corresponds to 32 000 iterations for the sobol total order indices s t a t s c r e e n i n g t h 0 000 31 and number of iterations is 20 000 delta method needs more samples for convergence and finally for the xgboost indices we obtain s t a t s c r e e n i n g t h 0 000 19 and 18 400 iterations interestingly lower values of sobol indices converge much quicker compared to higher values 3 2 4 confidence intervals of rankings fig 8 a shows convergence and robustness of stat ranking thick blue line is the mean value of stat ranking whereas transparent blue regions are its 95 confidence intervals spearman correlations statistic steadily converges to the ranking of the last convergence step similar to stat screening results and is quite robust with respect to sobol 150 000 iterations seem to be enough to have comparable ranking results as at the 400 000 model runs note that this value is much larger compared to 20 000 iterations obtained with stat screening even with relatively wider confidence intervals than in other methods stat ranking indicates good agreement with the maximum iterations ranking as before we observe a step in delta indices at around 75 000 iterations but stat ranking is robust at each convergence step finally gradient boosting in the lca model exhibits convergence properties of both ranking and r 2 score with a very similar behavior the statistic stat ranking is both robust and close to 1 at roughly half the maximum number of mc iterations same as obtained with the screening statistic all in all convergence and robustness analyses for the lca model in the scope of factor fixing was obtained by taking a more conservative number of iterations among all the statistics it reveals that convergence has been established for spearman correlations at ca 40 000 for sobol total order indices at 150 000 and for xgboost importances at 20 000 iterations but has not been reached by the delta method at the given 80 000 iterations 4 discussion 4 1 curse of dimensionality one of the first encounters in this work of the curse of dimensionality phenomena that appear in high dimensions but are not apparent in low dimensions bellman 1966 was the lack of high dimensional benchmark functions to test the scalability of gsa methods on fast models models with many independent inputs can be rare in practice hence the lack of appropriate benchmarks common approach in increasing dimensionality of standard models lies in introducing dummy variables that have zero effect on the model output morris et al 2006 this does not reasonably portray real case studies where numerous noisy variables can be present the addition of dummy variables allows gsa methods such as sobol total order to perform better than other methods since part of their sampling design includes explicit computation of changes in the model output when only one input varies see how matrices a and a b j are constructed in the saltelli estimator in equation 10 therefore we introduced a modified morris model with an arbitrary number of user defined levels of importance existing gsa software implementations are not optimized for higher dimensional models numerous great libraries are available in different programming languages douglas smith et al 2020 but we found that we had to improve their performance for this study for better memory efficiency it is helpful to generate sampling design x in a block iterative manner the main reason is the following given that sampling designs are generated in a unitcube space they afterwards need to be rescaled according to the specified input distributions which means that by working only on a portion of x we actually save memory twice this becomes particularly important during parallelization of mc simulations where each parallel worker needs to generate only a portion of x which consequently simplifies overall resource allocation naturally performing computations on fewer samples is also faster regarding overall run time one should use parallel computations whenever possible and when computing gsa indices especially for methods that employ both x and y in many cases gsa indices are computed independently and are therefore embarrassingly parallel the same suggested modifications apply to convergence and robustness analyses we find that it is not possible to set an absolute or a priori threshold to automatically determine the convergence of sensitivity index confidence intervals as these intervals are derived from the standard error of the chosen sensitivity indices moreover even for normalized indices the actual importance of inputs depends on model complexity regarding ranking of inputs the task of comparing two rankings becomes non trivial in high dimensions where the focus might be shifted from factor prioritization to factor fixing previous works put more emphasis on highly ranked inputs by introducing weighted rank coefficients sarrazin et al 2016 our experiments in appendix a 1 showed that for the purposes of screening this would fail in the presence of many similarly important inputs or tied ranks instead we chose to modify ranks of the inputs by clustering rankings into a finite number of ranks 4 2 convergence and robustness once we obtain sensitivity indices in the bootstrap procedure the three convergence and robustness statistics can be computed efficiently they complement each other and enhance screening analysis by simultaneously plotting stat indices and stat screening one can iteratively adjust tuning parameters k th or s th that determine set of lowly influential inputs in stat screening see section 2 1 1 it also becomes possible to see whether confidence intervals of lowly influential inputs differ from the highly influential ones where higher difference can indicate that sensitivity method is well suited for factor fixing setting the benefit of using stat ranking adjusted for high dimensional screening lies in its comprehensiveness because it considers all sensitivity indices and their relative ranking positions simultaneously given that many parameters are lowly influential stat ranking shows when the separation between highly and lowly ranked inputs becomes more well defined 4 3 comments on gsa methods as we observed in the numerical simulations spearman correlations should be used whenever model is monotone as they require least number of iterations and converge smoothly they can be generally considered useful when local nonlinearities in model are not the main interest of the analysis sobol indices are very powerful as they provide information about model interactions but require a lot of model runs for convergence whenever sobol analysis is performed convergence and robustness in the form of confidence intervals come at no additional cost this is the only considered method that can efficiently produce hundreds or thousands of bootstrap samples delta indices is another powerful gsa method suitable for nonlinear models the computation of indices requires more time than other methods considered in this paper moreover if one intends to run convergence and robustness analyses it is important to use block latin hypercube design finally gradient boosting is a very promising method applicable to nonlinear models this method seems to be as powerful as sobol indices but requires roughly 10 times fewer model runs its high predictive performance on the test set indicates confidence in feature importance results the main drawback is that it needs time consuming tuning one of the most important tuning parameters are maximum depth of trees as it relates to degree of model interactions see section 2 3 we could see in morris models that setting this parameter to the actual degree of interactions led to significant performance benefits and in the lca model high predictive capability was achieved with tree depth set to 4 4 4 insights into lca model what we have learned about lca models from gsa is that they can be more linear than we expect by looking at the inverse of technosphere matrix in the lca model see section 2 5 2 despite the inverse it seems that the effective dimensionality of lca models is low kucherenko et al 2011 it can also be a consequence of reducing all the information from technosphere into a single lca score this finding can greatly reduce the effort in performing gsa in lca because simple correlation or linear regression coefficients are sufficient for reliable analysis another insight that can be obtained from boosted trees is regarding degree of significant interactions effects between model inputs the tuning of gradient boosting suggests that this value is close to 4 for the lca case study of swiss household food consumption on one hand this value can be tested while tuning xgboost for other lca models as many of them are based on similar life cycle inventory datasets and on the other hand this information can help in prioritizing analysis when looking into supply chain of the given system 4 5 limitations of the study 1 one of the limitations of this study is that morris screening procedure has not been tested morris 1991 campolongo et al 2007 2011 the reason for that lies in our understanding that this approach is based on computing distances in high dimensions see section 3 of campolongo et al 2007 which is well known to be affected by the curse of dimensionality see appendix e more specifically as the number of dimensions increases differences between two distances become less prominent which can render morris screening ineffective maximum input space dimensionality for which this approach is still applicable remains to be tested in fairness morris indices serve as proxy for sobol total indices that we did include in the analysis campolongo et al 2011 2 when computing indices for convergence latin hypercube structure needs to be preserved one way of doing that is to use a more sophisticated block sampling design such as progressive latin hypercube sheikholeslami and razavi 2017 3 when studying convergence and robustness of ranking number of clusters has been chosen based on the knowledge of the model for morris function and arbitrarily for lca the algorithm could benefit from a more automated procedure such as one described by sheikholeslami et al 2019 5 conclusions and outlook in the present study we examined the computational performance of spearman correlations sobol indices delta moment independent indices and gradient boosting importances on very high dimensional models for the purposes of screening to benchmark these methods we first proposed a modified morris model with a variable number of influential inputs we then explored the scalability of the gsa methods by applying them to the morris function with the number of model inputs increasing from one to ten thousand sobol total order indices and gradient boosting importances showed the best capabilities for determining lowly and non influential inputs however gradient boosting is based on simple random sampling and needed 5 10 times fewer mc simulations computation of sampling and sensitivity indices were efficient for all methods as a general rule methods that employ only model outputs y for the indices computations tend to be fast which renders convergence and robustness analyses with bootstrapping very efficient on the other hand methods that require both x and y can be slow therefore fewer convergence steps and bootstrap resamples should be chosen for spearman correlations it is possible to compute analytical confidence intervals and consequently stat indices that would serve as an upper bound to values from bootstrapping next we computed maximum confidence intervals for all sensitivity indices subset of lowly influential sensitivity indices and inputs rankings statistic by bootstrap resampling for the convergence of ranking we proposed a quantitative measure stat ranking that compares sensitivity indices clustered into a predefined number of ranks and is particularly suitable for screening in high dimensions in that it is capable of handling tied or very similar ranks in cases where exact ranking is of lesser interest the three considered statistics stat indices stat screening and stat ranking allowed us to monitor convergence and robustness of sensitivity indices screening and ranking for the purposes of factor fixing based on the cumulative analysis we concluded for all morris models that spearman correlations were estimated with reasonable level of confidence at the given number of iterations sobol exhibited robustness and convergence at fewer model runs than anticipated delta indices could benefit from additional samples and xgboost indices could perform well with fewer iterations for some models but overall number of samples has been chosen appropriately we adjusted gsa settings for an lca model used for environmental impact assessment that comes from manufacturing of goods or providing service performed analysis shows high agreement between all methods for the factor fixing sa setting where spearman correlations sobol and gradient boosting show most similar results when determining 90 of the least non influential inputs see table 4 while spearman is a known gsa method for linear models saltelli et al 2008 sobol and gradient boosting are suitable methods for nonlinear models total order sobol indices in combination with first order provide a comprehensive picture of input importances and model interactions but might be too computationally expensive requiring hundreds of thousands of mc simulations for convergence of the indices for high dimensional models at the same time gradient boosting needed roughly 5 10 times fewer model runs while providing as reliable screening results the maximum confidence intervals stat indices and stat screening smoothly decrease as the number of mc iterations grows when comparing final ranking with rankings at fewer samples sobol and gradient boosting provide faster convergence than anticipated for all methods ranking shows high robustness to input samples as the ground truth for non influential inputs was unknown we validated results in two ways and can conclude that the filtered out inputs indeed do not affect model output variability primary conclusion of the study is that there is a clear procedure for robust high dimensional screening summarized in fig 9 it consists of the following steps i generate at least 4k model outputs based on random sampling where k is the number of model inputs ii compute src to estimate degree of model linearity iii perform gsa on the same samples with spearman correlations if the model is monotone and has a high degree of linearity and using gradient boosted trees otherwise iv if computational resources allow perform convergence and robustness analyses with bootstrapping v validate results we suggest 4k mc iterations empirically because a it is recommended as a rule of thumb for regression coefficients jenkins and quintana ascencio 2020 b it is an upper bound on analytically computed number of iterations for spearman correlations bonett and wright 2000 and c it enables convergence and robustness of indices and rankings in gradient boosting analysis for the considered models when using spearman correlations methods also compute stat indices analytically the decision of whether the model is linear enough depends on the purpose of the analysis and is determined by users for instance values higher than 0 8 would mean that less than 20 of variance is not explained by the model which could already be satisfactory step ii is very efficient since linear regression has a closed form solution note that linear regression spearman correlations and gradient boosting can all use the same mc simulations or real data if available validation step can also reuse already generated model outputs but would need additional hundreds to thousands model runs where only influential inputs vary then the total cost of the analysis is around 4k 1 000 model runs it is however possible that for complex models src and boosting would need additional samples in this case results for src can be efficiently recomputed with newly generated samples whereas xgboost trees can be trained iteratively by passing saved trained trees as a warm start to further training an interesting trait of xgboost that remains to be tested further is its feature importance similarity to sobol total indices at significantly fewer mc iterations one particular benefit of using gradient boosting for lca models is that standard lca case studies have very similar model structure and complexity this means that the effort for parameter tuning can be greatly reduced by starting with the tuning parameters identified in this paper as lca models get progressively more complicated by way of introducing physical nonlinear models behind elements of technosphere and biosphere or using real data and measurements gsa with gradient boosting could bring significant benefits to data quality improvement and eventually more reliable assessment of environmental impacts author contributions all authors contributed to the presented study in conceiving and designing the analysis and have reviewed and given approval to the final version of the manuscript a k conducted literature review set up the models performed the analytic calculations the numerical simulations and drafted the paper a f provided inputs on building upon a precursor model a k and c m developed open source python software for the presented study both c m and a f supervised the project software availability name of software gsa framework type python package developers aleksandra kim christopher mutel contact aleksandra kim psi ch christopher mutel psi ch year first available 2020 hardware requirement general purpose computer software requirement python 3 6 or later availability https github com aleksandra kim gsa framework declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements this work was supported by the swiss national science foundation snsf grant 407 340 172445 within the framework of the national research program sustainable economy resource friendly future oriented innovative nrp73 further information can be found at http www nrp73 ch the authors would like to thank dr stefano marelli for the valuable insights and productive discussions moreover we are grateful to the reviewers for their extensive and detailed feedback that allowed us to improve the quality of the paper a convergence and robustness of ranking a 1 statistics to compare two rankings sarrazin et al 2016 introduced a number of statistics that aid in monitoring convergence and robustness of rankings they measure degree of agreement between two rankings r m and r n that are derived from sensitivity indices s m and s n obtained at different convergence steps or during bootstrap procedure in the following we show that the existing statistics do not perform well in the case of high dimensional models because same input can be assigned very different ranks in two independent gsa estimations even if it is clearly recognized as e g non influential in both cases note that we employ consistent notation for rank coefficients ρ but omit weighted versions of spearman coefficients for the sake of brevity then given k model inputs we consider the following statistics 1 unweighted spearman rank coefficient estimated as a 1 ρ 1 1 6 j 1 k r j m r j n 2 k k 2 1 2 correlation coefficient computed on savage scores a 2 ρ 5 1 j 1 k s s j m s s j n 2 2 k h 1 k 1 h where s s j h r k 1 h and r is the rank of input j 3 adjusted and weighted rank coefficient introduced in sarrazin et al 2016 a 3 ρ 6 j 1 k r j m r j n max m n s j m s j n 2 j 1 k max m n s j m s j n 2 sarrazin et al 2016 redefine these expressions in terms of contributions f j of each model input j to the corresponding statistic then ρ 1 ρ 5 can be written as 1 j 1 k f j and ρ 6 j 1 k f j where lower f j leads to higher ρ 1 ρ 5 and lower ρ 6 both ρ 1 and ρ 5 vary between 1 and 1 where higher values indicate higher agreement between two rankings whereas ρ 6 is always non negative with lower values pointing to higher agreement we test performance of ρ 1 ρ 5 ρ 6 on the i non clustered rankings when all model inputs have distinct ranks and ii clustered rankings where each input is assigned one of 4 ranks as described in section 2 1 1 the rankings are computed for the morris function with k 1 000 inputs where x 1 x 10 are highly influential x 11 x 20 are moderately influential x 21 x 30 have low influence on the model output and the rest are non influential see section 2 5 1 sensitivity indices s m s n are spearman correlations estimated on 4 000 samples fig a1 shows results in case of i non clustered rankings on the left and ii clustered rankings on the right we depict only first 50 model inputs on the x axis since the results are similar for the rest of the non influential inputs row 1 shows the ranks assigned to model inputs and rows 2 4 show contributions f j to the statistics ρ 1 ρ 5 ρ 6 whose values are given as titles of respective subplots it is clear that in the non clustered rankings case most of the non influential inputs have high f j simply because same input has been assigned very different ranks by r m and r n even though it is recognized as non influential in both cases this issue is not visible in case of clustered rankings where all three statistics show high agreement between r m and r n which is in line with our expectations therefore in this paper we apply convergence and robustness statistics to clustered rankings fig a 1 analysis of rank coefficients ρ 1 ρ 5 ρ 6 for i non clustered rankings when model inputs have distinct ranks and ii clustered rankings where each input is assigned one of 4 ranks fig a 1 next in order to choose between ρ 1 ρ 5 and ρ 6 we performed further experiments and generated convergence and robustness plots for stat ranking based on these different rank coefficients applied to clustered rankings similar to figs 4 and 8 fig a2 shows all gsa methods in rows and different rankings statistics in columns for the morris model with 1 000 inputs the values of the unweighted spearman are very close to 1 for all gsa methods and convergence steps because for clustered rankings the simplified estimator a 1 would have small values in the numerator of f j ranks differences become smaller in clustered rankings and high constant value in the denominator therefore we added one more column to figure a2 that depicts stat ranking based on spearman coefficient ρ that can account for tied ranks see equation 6 spearman 1961 correlation coefficient computed on savage scores exhibits the same behavior as unweighted spearman by examining figure a1 one can see that both ρ 1 and ρ 5 have very low f i values for inputs that have been ranked differently by r m and r n on the other hand ρ 6 and ρ provide reasonable convergence results in this paper we chose to use the latter because weighted rank coefficient proposed by sarrazin et al 2016 was deemed unnecessary in the presence of clustered rankings with only few distinct ranks moreover its range of variability from 0 to infinity makes it harder to analyze nonetheless it has a satisfactory performance and from the experiments we have seen can be used as a convergence and robustness measure as well fig a 2 convergence and robustness of ranking for morris model with 1 000 inputs for all gsa methods in rows and different rank coefficients in columns the thick blue line shows mean of stat ranking transparent blue region its 95 confidence intervals dashed orange line is the value of perfect agreement between two rankings of the given statistic fig a 2 a 2 algorithm to compute stat ranking algorithm 1 outlines a procedure to monitor convergence and robustness of ranking through stat ranking described in section 2 1 1 convergence of ranking is observed via its mean s t a t r a n k i n g mean whilst robustness by computing confidence intervals s t a t r a n k i n g ci inputs to the algorithm are i sensitivity indices s i b b 1 b for k model inputs are given for convergence steps i 1 n c and bootstrap samples b 1 b ii number of clusters q for sensitivity indices values here we are interested in comparing rankings at all convergence and bootstrap steps to the ranking r obtained from a best guess sensitivity result s for the sake of readability we denote statistics for ranking stat ranking and ranking r obtained from bootstrap sample b at convergence step i as s t a t r a n k i n g i b and r ib respectively algorithm 1 convergence and robustness of rankings image 9 in the absence of ground truth r can be chosen as ranking obtained from sensitivity indices at the maximum number of mc iterations or on the last convergence step from a random bootstrap sample one can also obtain r from bootstrap mean of sensitivity estimates for all model inputs but only in the case when indices for all inputs are independent from each other as in for example spearman correlations and delta gsa method counterexamples are i sobol indices where the sum of all first order indices needs to be less or equal to 1 ii indices that are normalized across all inputs e g by dividing them by their maximum or sum b computational performance fig b1 shows performance gains of gsa framework compared to salib in two scenarios with respect to increasing number of model inputs with constant iterations on the left and increasing number of iterations given constant inputs on the right gains are computed as ratios between the time needed by salib to the time needed by gsa framework to generate saltelli sampling latin hypercube design and compute delta indices note that y axis has a logarithmic scale in all cases gain is higher than one which points to better performance of gsa framework fig b 1 computational performance comparison between salib and gsa framework fig b 1 c analytical sobol indices for morris functions c 1 morris function c 1 z x α i 1 k x i β i 1 k 1 j i 1 k x i x j where α 12 6 0 1 k 1 β 12 0 1 k 1 the expectation and variance of this function are given as follows c 2 e z 1 2 α k 1 8 β k k 1 v a r z e z 2 e z 2 to derive e z 2 let us denote the summands of z x as f x α i 1 k x i and g x β i 1 k 1 j i 1 k x i x j then c 3 e z 2 e f g 2 e f 2 e 2 f g e g 2 where e f 2 1 2 2 3 α 2 k 3 k 1 e 2 f g 1 2 3 3 α β k k 1 3 k 2 e g 2 1 2 6 3 2 β 2 k k 1 9 k 2 3 k 10 the first and total order indices are c 4 s 1 i v a r x i e x i z x i v a r z s t i e x i v a r x i z x i v a r z where c 5 v a r x i e x i z x i 1 12 α 1 2 β k 1 2 e x i v a r x i z x i 1 12 α 2 1 12 α β k 1 1 144 β 2 k 1 3 k 2 c 2 modified morris function c 6 y x λ z x l μ z x m ν z x n where x l x 1 x k x m x k 1 x 2 k x n x 2 k 1 x 3 k λ μ ν 0 1 then numerator and denominator expressions in sobol indices become c 7 v a r x i e x i y x i c 2 v a r x i e x i z x i e x i v a r x i y x i c 2 e x i v a r x i z x i v a r y λ 2 μ 2 ν 2 v a r z where the constant c λ resp μ or ν if x i is from x l resp x m or x n d gsa results d 1 xgboost tuning parameters gradient boosting tuning parameters for all models can be found in table d1 table d 1 gradient boosting tuning parameters table d 1 xgboost parameter description morris 1 000 morris 5 000 morris 10 000 lca model test size fraction of test samples 0 2 0 2 0 2 0 2 learning rate step size in optimization 0 1 0 2 0 2 0 15 gamma gain tree complexity 0 0 0 0 min child weight cover minimum number of residuals in each leaf 30 300 600 300 max depth maximum tree depth 2 2 2 4 reg lambda l2 regularization 10 0 0 0 reg alpha l1 regularization 0 0 0 0 n estimators number of decision trees 500 800 1500 600 subsample fraction of selected samples 0 6 0 3 0 2 0 3 colsample bytree fraction of selected features 0 3 0 3 0 2 0 2 d 2 histograms of bootstrap data we show histograms of p 120 bootstrap samples in figure d1 for a random input in the lca model all gsa methods in columns and increasing number of mc samples in rows one can see that all histograms are somewhat symmetric and do not contain large outliers as we cannot visualize bootstrap data for all model inputs and convergence steps we select these randomly overall number of bootstraps p 120 seems appropriate even though p 1 000 is recommended efron and tibshirani 1994 fig d 1 histograms of p 120 sensitivity indices estimates obtained with bootstrapping for a random input in the lca model all gsa methods in columns and increasing number of mc samples in rows fig d 1 d 3 morris function figures fig d2 shows gsa results for all morris models in columns all gsa methods in rows and all model inputs on the x axis same results for the first 10 of model inputs are given in fig 2 fig d 2 estimates of sensitivity indices for morris models with 1 000 5 000 and 10 000 inputs in columns and for each gsa method in rows are given in blue all model inputs are on the x axis and estimates of sensitivity indices are on the y axis analytical values of sobol total order indices are given in orange fig d 2 fig d3 shows convergence and robustness results of sensitivity indices and screening for all morris models in columns all gsa methods in rows and mc iterations on the x axis with same scale for all methods these results are also given in fig 3 but with different x axis scales fig d 3 convergence and robustness of sensitivity indices for morris models with same scale on the x axis maximum confidence intervals obtained with bootstrapping for all sensitivity indices are given in blue for 98 lowest sensitivity indices in purple and analytical intervals for spearman correlations in orange mc iterations are given on the x axis and stat indices stat screening are on the y axis centered symmetrically around y 0 where at each convergence step blue and purple parts are equal to stat indices and stat screening respectively fig d 3 fig d4 shows convergence and robustness results of stat ranking for all morris models in columns all gsa methods in rows and mc iterations on the x axis with same scale for all methods these results are also given in fig 4 but with different x axis scales fig d 4 convergence and robustness of stat ranking for morris models with same scale on the x axis and stat ranking on the y axis the thick blue line shows mean of stat ranking transparent blue region its 95 confidence intervals and orange trace corresponds to convergence of the internal xgboost metric r 2 score fig d 4 d 4 lca model figures fig d5 shows results for the lca model and all gsa methods in rows where inputs for all methods were sorted according to descending spearman correlations values same results for the first 10 of model inputs are given in fig 5 fig d 5 estimates of sensitivity indices for lca model for each gsa method in rows are given in blue model inputs are on the x axis and estimates of sensitivity indices are on the y axis inputs for all methods are sorted according to descending spearman correlations fig d 5 fig d6 shows validation results for sobol total order delta indices and xgboost importances number of influential inputs has been chosen as 60 for all methods validation results for the spearman correlations are given in fig 6 fig d 6 validation of sobol total delta and xgboost indices gsa results for lca model where monthly lca impacts of an average household food consumption are given in kg co2 eq fig d 6 e similarity measures in high dimensions a number of statistical algorithms depend on similarity measures between two vectors x y r k common similarity measures include euclidean minkowski chebyshev distances etc however in high dimensions all vectors look alike namely given a vector x and a fixed number n of randomly generated vectors y y 1 y 2 y n the difference between maximum and minimum distance from x to y becomes small as the number of dimensions k increases beyer et al 1999 formally this can be written as e 1 lim k d i s t max k d i s t min k d i s t min k 0 where d i s t max k d i s t min k is the maximum minimum distance between x and y in a k dimensional space fig e 1 reduction of differences between distances as number of dimensions increases fig e1 depicts the ratio under the limit for an increasing number of dimensions and few of the most common distance metrics between numeric vectors it is clear that as the number of dimensions increases the difference between distances becomes less and less prominent 33 1 2 2 
