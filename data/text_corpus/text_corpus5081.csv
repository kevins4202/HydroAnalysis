index,text
25405,hydrological models are essential in water resources management but the expertise required to operate them often exceeds that of potential stakeholders we present an approach that facilitates the dissemination of hydrological models and its implementation in the model integration mint framework our approach follows principles from software engineering to create software components that reveal only selected functionality of models which is of interest to users while abstracting from implementation complexity and to generate metadata for the model components this methodology makes the models more findable accessible interoperable and reusable in support of fair principles we showcase our methodology and its implementation in mint using two case studies we illustrate how the models swat and modflow are turned into software components by hydrology experts and how users without hydrology expertise can find adapt and execute them the two models differ in terms of represented processes and in model design and structure our approach also benefits expert modelers by simplifying model sharing and the execution of model ensembles mint is a general modeling framework that uses artificial intelligence techniques to assist users and is released as open source software keywords software metadata model metadata model encapsulation model catalogs mint hydrological models data availability all data is referenced in the text and can be accessed using the corresponding links 1 introduction hydrological models hms are commonly used for water resources management and are mainly developed and used by expert researchers or engineers working in the water sector the results of hms are important and considered in decision making processes of government agencies ruiz ortiz et al 2019 andreu et al 1996 hm applications include estimation of water availability döll and lehner 2003 development of water management strategies haasnoot et al 2011 flood risk assessment merz et al 2010 climate impact analysis krysanova and hattermann 2017 lobanova et al 2018 hattermann et al 2018 solute transport konikow 2010 morales et al 2010 and spatial characterization of hydrological system variables such as soil water content brocca et al 2017 desalination and industrial wastewater treatment panagopoulos 2022 as well as groundwater heads reinecke et al 2019 hms vary widely in terms of their mathematical description of prevalent hydrological processes and their spatial model structure ranging from lumped conceptual models bittner et al 2018 booij and krol 2010 to distributed physical models brunner and simmons 2012 newman et al 2017 a fundamental understanding of hydrological processes is needed in order to reasonably set up a hydrological model for a new region or modeling problem this may become an obstacle for the use of hms by decision makers and other users lüke and hack 2018 in practice model results are presented to decision makers as a summary focusing only on a few specific variables of interest such as streamflow or groundwater heads the interests and requirements of decision makers and various stakeholders can diverge widely from what may be hydrologically interesting decision makers in water resources management are usually interested in the assessment of the water balance primarily the availability of water in space and time hms allow a holistic view on the components of the water cycle from which insightful information e g limiting factors in space and or time can be derived these variables do not necessarily be restricted to water availability but could also refer to evapotranspiration soil water or precipitation miscommunication between science and non expert groups is therefore not a rarity timmerman and langaas 2005 this increases the science policy gap due to differences in the level of knowledge between the information producer and receiver bernstein et al 1993 bradshaw and borchers 2000 consequently it is a challenging task for modelers to provide information that is practically usable and interpretable by a broader community of end users fatichi et al 2016 ideally hms would be accessible to any potential users so that they are able to test different decisions and scenarios themselves potential users who are not hydrology experts can include data analysts decision makers and also scientists in other disciplines who aim to incorporate water related topics into their models in situations where different disciplines need to work closely together and where models from different areas such as economics hydrology climatology or ecology may need to be integrated further obstacles often emerge as hms often need to be designed exchanged and run by different user groups moreover several models with overlapping features may be available and selecting an appropriate model for a task can be challenging even for experienced modelers surfleet et al 2012 in addition enabling different capabilities of a model can lead to different data and input requirements even for hydrology experts it can be difficult to understand how processes are represented in different hms making comparison studies very time consuming hms tend to have special computational requirements and use heterogeneous file formats for spatio temporal data so that data pre processing usually requires basic programming skills additional technical challenges arise when hms require different operating systems or complex model configurations which can limit the applicability and transferability of models even for hydrology experts therefore there is a great need for new approaches to facilitate the dissemination of hms to users who lack the expertise to develop them but are invested in using them for decision making purposes over the last few decades efforts have been made to make hms more accessible by integrating them into geographic information systems gis bittner et al 2020 rossetto et al 2018 refsgaard et al 2010 in this regard gis based interfaces to hms often act as an essential component of a decision support system dss lautenbach et al 2009 pezij et al 2019 zhang et al 2014 executable and well structured dsss make hms even applicable by non expert groups but dsss usually lack transferability as they are strongly tailored to the individual conditions of a defined case study an example of how dsss are often developed in the course of a project to combine different stand alone software tools can be found in kinzelbach et al 2021 however a limitation of many dss is that they are desktop based and therefore show limited accessibility moreover they often focus on one area such as groundwater and even on one model and are thus lacking interoperability gis based interfaces have been used in the soil water assessment tool swat arnold et al 1998 the free and open source software tools water resources management system freewat koltsida and kallioras 2019 or the hydrologic engineering center hydrologic modeling system hec hms us army corps of engineers 2000 but models must be set up from scratch by experienced users furthermore these platforms only include a single hm while users often want to use several hms to compare their results consequently initiatives like the community surface dynamics modeling system csdms peckham and syvitski 2007 peckham et al 2013 the earth system modeling framework esmf hill et al 2004 or the hydroshare platform horsburgh et al 2016 have already taken a step forward to provide and combine multiple models from different disciplines csdms and esmf include the dissemination of final and calibrated models combined with their results from a variety of disciplines in the field of geo and earth science overeem et al 2013 collins et al 2005 keller et al 2014 while hydroshare is explicitly designed for the exchange storage or management of hydrological datasets and models gan et al 2020 however these efforts are focused on users who are modeling experts pursuing science research rather than non expert users in order to ease the dissemination of expert models to non experts our previous work introduced the model integration framework mint gil et al 2018 2021 mint defined the components and interfaces needed to assist expert modelers when setting up pre existing hms for non experts but adding new hms to the framework required advanced software engineering skills making it challenging for expert users to contribute this paper builds on our previous work with the following novel contributions 1 a methodology that follows principles of software engineering to create software components for hms with a simple invocation function with pre set inputs and parameters capturing metadata about the model that can be used to provide guidance to non expert users 2 an implementation of this methodology that guides expert modelers to create model components integrated in the model integration framework mint gil et al 2018 2021 3 two use cases that demonstrate the use of this methodology and implementation for two models that differ in terms of hydrological processes they consider as well as in terms of their individual code structure swat arnold et al 1998 and modflow harbaugh 2005 this methodology makes models more findable accessible interoperable and reusable in support of fair principles wilkinson et al 2016 the paper begins with a description of our proposed methodology for creating software components for models section 2 next in section 4 we illustrate how the methodology is implemented in the mint model insertion checker a standalone application designed to guide users through the proposed methodology steps section 5 describes two examples that follow our methodology to deliver two different hm configurations for two different regions of the globe section 6 shows how each of these configurations can be accessed and run in the mint platform section 7 discusses the main advantages and limitations of our approach and section 8 presents conclusions and future work 2 background hms differ in the way they conceptualize the characteristics and flow processes in a natural system as a result hms usually have dozens of parameters and input files which vary across different scenarios for example models like swat may use an input file with snowmelt observations in regions with mountains but may not take snowmelt into consideration if there are no mountains around the basin of interest expert hydrologists who we will refer to here as modelers need to make decisions about which hydrological processes and corresponding parameters are relevant to the intended non expert users e g decision makers analysts researchers with expertise in other areas or domains students in training or citizens who are active in non governmental organization who we will refer to as users 2 1 software components encapsulating software into portable components allows other users to easily run software on their own machine without worrying about the environment and set up needed boettiger 2015 kurtzer et al 2017 following well established component based software engineering principles we aim to create self contained software components that only reveal functionality that is of interest to third party users this is important because scientific software components are often implemented in large packages or libraries that can be used for various steps such as data preparation and visualization in addition to writing software to simulate specific processes such as atmospheric dynamics for climate models runoff and infiltration for hydrology models fuel density for fire modeling etc software packages can be quite overwhelming for users even when they are familiar with the scientific domain for which the package was written usability becomes even more challenging for users outside of the domain although these users are precisely the ones who may benefit the most from the results of the respective packages existing graphical user interfaces guis and gis systems are often difficult to reuse from other programs user interfaces usually have a specific function to call the software with a button using a form which users operate to define specific parameters typically the most relevant ones that function call sometimes called a command line invocation is reusable from different programs provided that the software tool can be run from a machine with its specific execution environment the function call uses inputs that can be provided when invoking the software component as it is done in a user interface where the values for some input parameters are set other inputs can be pre set within the component including data files if there are no reasons for third party users to change them given a specific use case a software component corresponds to a single invocation function for software given a sophisticated software package with multiple purposes a software component may be created to include only certain processes and variables a specific pre processing step or a specific visualization for example a hydrology model software may be pre set to be applicable to hot arid regions only and ignore the processes and therefore inputs describing snowmelt 2 2 model configurations encapsulation we use the term model encapsulation to refer to the process of creating easy to use self standing executable software components from models for a target scenario we refer to these software components as model configurations expert modelers are responsible for designing these model configurations for a region by identifying the key parameters that non experts should be able to modify model configurations declare only those relevant parameters or input files that users should be able to change so the model configuration can be easily set up and run to explore different scenarios the remainder of this section illustrates model configurations through a simple example introduces key concepts in model encapsulation and describes the main steps of our methodology 2 3 model configurations an example for illustration purposes let us consider alice an expert hydrology modeler and bob a decision maker with little hydrological expertise bob needs to regulate policies for the water budget at a country scale and therefore he is interested in obtaining a rough estimate on water availability during the spring season in particular bob seeks to understand 1 whether the water demand of specific crops can be met under different assumptions and 2 the impact of runoff for energy production i e from hydro electrical plants given her expertise bob asks alice to provide an environment where he can run model simulations according to his requirements alice anticipates that bob may want to modify some of the simulation parameters affecting snowmelt the dominant runoff component and source of water in spring a shift in the onset and duration of snowmelt usually affects the temporal water availability of the agricultural and energy sector alice decides to use the swat model and creates two model configurations to predict streamflow as a proxy representation for water availability the first model configuration is designed at the country level letting bob modify the snowmelt temperature and the maximum possible snowmelt to explore the effects on agriculture e g what crop yield can be achieved by different crops the second model configuration focuses on a small basin located in the northeastern region of the country in order to study the conditions and effects of snowmelt for a potential small hydroelectric power plant both model configurations first undergo a strict and rigorous calibration and validation procedure by alice a necessary expert step to ensure a reliable baseline for the further usage the calibration and validation serve as fundamental steps to provide robust and credible models fig 1 shows an overview of the model configurations prepared by alice with the country level configuration on the left and the basin configuration on the right each configuration has one or multiple inputs and outputs representing the files accepted and produced by a configuration we use the term parameters to refer to values a user may be interested in changing in a model such as snowmelt temperature even if these values are declared within configuration files we consider as parameters hydrological or process based variables together with temporal information such as simulation length or time step here referred to as general settings gs a code wrapper captures how to invoke a model configuration by indicating how the command line should be invoked and specifies any fixed values of inputs when creating a model configuration a modeler like alice may have to choose which of the inputs or parameters should be adjustable by the final user among the dozens or hundreds of input files and parameters hms have we use the term expose to indicate that a parameter or input file can be adjustable by a user in a model configuration for example swat contains hundreds of files but alice estimates that the relevant ones for the country level configuration are two input files with snow and elevation information as shown in fig 1 the input file containing snow information further includes the parameters that will be exposed to users namely snowmelt temperature and the maximum melt factor of snow adapting the threshold temperature when snow begins to melt is an easy way to shift the melt season within the country the second parameter provides information on the amount of snowmelt one could expect alice thus provides a meaningful range of values within which bob is able to increase or decrease the amount of snowmelt in addition alice decides to expose a file that includes basic information on gs like the time for which the model was set up or its temporal resolution daily in this case thanks to this information alice expects bob to be able to compare the effects of a very high and a very low value for snowmelt temperature as well as the maximum melt factor on the water availability as for the basin configuration alice is familiar with the area from her previous work therefore she decides to set up all default values of the model according to her knowledge of the region she exposes snowmelt temperature by making only this parameter available in the basin configuration this configuration is more restricted but more precisely tailored to the region at hand therefore this model configuration is simplified by allowing bob to only modify snowmelt temperature hence bob can now obtain alternative estimates with respect to the accumulation of snow during winter which is then available as melt water this enables the decision maker to infer whether a small hydropower plant might be of value or not or how much energy could be produced under various snowfall conditions in summary with these model configurations the modeling expert is able to hide the complexity of a general model exposing only what is relevant for a country and its hydrology narrowing it down to a much more usable model component for other users to explore scenarios and make decisions accordingly it should also be mentioned that bob does not necessarily must be a decision maker however he could also be an interested member of a ngo which deals with environmental issues for example or just an interested citizen increasingly affected by hydrological events such as drought or heavy rain table 1 provides an overview about the terminology we use in this paper especially to distinguish terms which might ambiguous and are used differently in other fields 3 a methodology for model encapsulation we propose a methodology for creating model configurations our methodology requires expert modelers to determine the main parameters and input files that need to be exposed for a given executable model including steps for guiding and testing the final model configuration so other users can use it effectively our methodology comprises six main steps start a new environment trace execution dependencies expose parameters expose input output files wrap execution and model upload table 2 provides a summary of all steps which are further described here step 1 start a new environment modelers start by specifying the location for the folder structure of the new model component they want to create this should be started in a clean computing environment free from other software dependencies installed on the local machine for example if a model is available in python starting in a clean environment makes it easy to isolate the model needs from other python libraries installed in the machine for other purposes this can be achieved by using virtual environments that create a clean python installation with no installed package dependencies in our methodology we adopt software containers a common approach to capture computational environments software containers enable capturing the dependencies of a software component at the operating system level i e including not only the dependencies of a software component but all the system dependencies as well hence ensuring that it can be run in other environments because containers can be complicated to set up and use for non computer scientists our system will be automatically creating the container and installing the dependencies and files needed to run the model the modeler can see everything that the system is adding in the folder that they specified step 2 trace execution dependencies and run model once an environment has been set up the dependencies needed to install the model must be incorporated into the environment this includes compilers system libraries and other files the modeler carries out a test run that is representative of how the model configuration will be used during the run the system automatically detects the model input configuration and output files used by the model during the run this information is added to the container environment and used by the system in subsequent steps in order to assist the modeler to specify inputs and outputs step 3 expose model parameters and define configuration files most hms have dozens of parameters and gss which specify constants like hydraulic conductivity bulk density or the general settings of the simulation within this step modelers have to define which of these parameters and gss they want to expose to users in the new model configuration for example the cn2 curve number ii parameter of swat is usually one of the parameters which is typically changed during the model setup and calibration the process of estimating relevant parameters and their corresponding values and might be a useful parameter to expose in a model configuration hms usually adjust numerical values for their simulations in two different ways 1 with the invocation command used to run the model or 2 through configuration files that can be edited directly or accessed via user interfaces if a file is used it needs to be specified by the modeler fig 2 illustrates this with an example of how the snowmelt base temperature parameter is exposed for the swat hydrology model smtmp through a configuration file step 4 expose model inputs and outputs next modelers have to decide which input and output files they want to expose which depends on the intended use cases that users will want to simulate as with parameters and gss expert modelers usually provide the relevant input files required by a model likewise models produce all sorts of output variables and for a given configuration only a certain subset of outputs may be relevant for the intended use cases for instance a modeler may expose only output files containing drought related variables such as evapotranspiration and soil moisture step 5 create a wrapper script once the parameters gss and files to be exposed have been specified the next step is to write a shell script which captures how to run the model configuration we refer to this script as the wrapper script as it wraps the model configuration as an executable component the wrapper script will make sure that the component can run with the inputs and outputs selected by the modeler and may include pre set files or values for other inputs and parameters in order to verify that the model works appropriately with the wrapper script it is necessary for the modeler to provide sample input files which are used in a test run if everything works successfully the model configuration is completed and will be executable in other computational environments step 6 upload the model configuration the final step is to deposit the model configuration in shared repositories first the script and test data used to wrap up the model configuration should be deposited in a code repository second an archival version of the model software code must be created in a code repository to ensure that version can always be accessed by users in the future third the container environment should be uploaded to a container registry finally the model configuration should be uploaded to a model catalog with proper model configuration metadata provided by the modeler to enable discovery and reuse 4 methodology implementation we implemented our methodology in the mint model insertion checker mic a standalone application developed to guide users through the process of creating new model configurations mic performs all the steps of our methodology in a semi automated manner integrating the results with commonly used software and container image repositories such as github and dockerhub mic also integrates new model configurations and the metadata in the mint modeling framework and its model catalog garijo et al 2019 mic is implemented as a unix based tool that runs in the command line and is available as open source software osorio et al 2022 1 1 software respository is at https github com mintproject mic with documentation at https mic cli readthedocs io en latest overview a step by step tutorial is available online 2 2 https mic cli readthedocs io en latest model configuration 03a step1 to help users and disseminate the steps of our methodology fig 3 provides an overview on how mic implements all the steps of our methodology capturing the main software dependencies input parameters and generated files and showing how the methodology steps are related to one another mic guides users through the six steps outlined in our methodology it starts with a blank unix environment generated with a basic docker image where users are asked to install and run their model from scratch step 1 once a sample run is finished mic tracks which files have been used and generated using reprozip rampin et al 2016 an application designed to trace all dependencies and system calls of a program step 2 using the output from reprozip mic drafts an initial component based on the inputs and outputs detected in the test run next mic works with the modeler to get information about the inputs parameters and gss of hms should be exposed in the model configuration among all the candidates detected automatically steps 3 and 4 the preparation of the configuration file is one of the few activities that has to be carried out manually by the modeler as it involves information highly dependent on the use cases required by the intended users for example swat may be used to create multiple configurations depending on whether modelers need to expose snowmelt temperature hydraulic conductivity or a factor to delay groundwater flow the parameters and gss exposed with mic will be adjustable by users when running the model configuration if one of the exposed parameters or gss are stored in a configuration file an additional step is required to indicate where to replace the target value in that file an example can be seen in fig 2 where snowmelt temperature in swat is exposed through the smtmp parameter which can be provided by users at runtime all the information provided to mic is stored in a mic settings file that can be inspected and edited by modelers at any time e g to change default values for parameters or to make adjustments on what is exposed to users once all the inputs outputs parameters and gss form a specific model configuration are set mic will prompt users to perform a test run using all default values mic automatically creates an execution wrapper script step 5 and runs the model using the local environment created earlier in the second step if successful the model configuration is ready to be run by others and mic will prompt users to double check if the results from the execution are correct as a final step mic saves the model configuration step 6 including 1 the computational environment used in the test run saved as a docker image in dockerhub 3 3 https hub docker com 2 the wrapper script and settings file containing the exposed inputs outputs parameters and gss mic will store these files in a new github repository owned by the modeler who created the model configuration 3 basic metadata about the model configuration including its main title description version of the model geographic location execution details and brief parameter and input descriptions these metadata are submitted to the mint model catalog producing the results shown in fig 4 in the following sections we provide several screenshots of mic to familiarize the reader and potential users with the platform fig 4 shows an example of how the model configuration can be accessed by a user after being created with mic fig 4 a depicts a model configuration where four parameters are exposed i e minimum and maximum melt factors snowfall temperature and snowmelt temperature out of the dozens of parameters that are available in swat fig 4 b shows an example where only one of the four parameters snowfall temperature may be changed by users when running a second different configuration of swat the other three parameters are fixed both configurations of the model are integrated in the mint framework where they can be executed through a gui 5 creating model components two practical use cases in this section we showcase our methodology by encapsulating two different and widely used hydrological models i e swat and modflow using mic to create model components and running them in the mint platform by pointing out the specific differences of swat and modflow we illustrate the main concepts of our methodology as well as the technical features of mic that facilitate model dissemination for any type of hm we show model configurations for swat and modflow for two different case studies each case study was defined prior to our work by a different research group working with stakeholders in different regions of the world 5 1 swat background and model structure the soil water assessment tool swat is a semi distributed time continuous model developed by the blackland research extension center of the united states department for agriculture usda arnold et al 1998 swat is based on the concept of the hydrologic response units hru and was originally developed to assess the impact of land management practices in large watersheds while the applications nowadays range from water quality or sediment transport studies up to snow hydrological in basins all over the world arnold and fohrer 2005 hrus are the smallest spatial unit within the model and defined on the subbasin scale a further subdivision of the watershed however hrus are not spatially located and are formed by unique combinations of land use soil and slope within each subbasin to consider spatial heterogeneity the hm is organized by input files grouped by different processes or characteristics such as land management or soil inputs for the individual spatial units besides the model includes few general files where basic settings can be done swat separates its calculations in a land and a water phase it first calculates all loadings for the hrus in each subbasin which are then transferred to the stream in a second step the in stream processes covering routing processes as well as chemical processes are calculated 5 2 modflow background and model structure the modular finite difference flow model modflow is a fully distributed and physically based groundwater model developed by the united states geological survey harbaugh 2005 hanson et al 2014 modflow is organized in modules which allow for user customization of specific case studies i e by selecting only those modules that are relevant for instance a module can represent different solvers for the groundwater flow equation moreover various modules exist to account for different hydrological processes in a natural system e g stream flow evapotranspiration or groundwater recharge given the grid based nature of the model several modules can be coupled by providing grid coordinates in the input files if specific modules should be used in a model run an input file is required for each respective module these input files are ascii files either organized in a table format or grid based all modules to be used for a model simulation have to be included in a configuration file i e a name nam file depending on the interaction of different hydrological processes modflow solves the groundwater flow equation and provides water budgets for each pre defined discrete time step in an output file the list lst file 5 3 model implementation in the following we describe how our methodology described in section 3 is implemented for two different hms namely swat and modflow most of the steps are similar for both models and to other hms despite how different their software and approach are therefore we focus on demonstrating how users can describe the models following our methodology using different use cases 5 3 1 case studies the location of our study areas and their geographical characteristics are illustrated in fig 5 our case studies focus on two very distinct hydrological systems the naryn river in kyrgyzstan for swat and the barton springs segment of the edwards aquifer in texas for modflow for each case study we emphasize which part of the proposed methodology is similar and where differences occur which mainly concern the exposed inputs and outputs in the respective model configurations the gss such as simulation period and time step have been set by experts for both case studies the target user groups of both cases are non expert analysts and decision makers our intention is to grant the respective users access to the model configurations so that they are able to run alternative scenarios on their own a summary of the case studies can be found in table 3 5 3 2 snow dynamics in the naryn basin case study our first case study focuses on a part of the naryn basin located in kyrgyzstan where high flow occurs mainly in spring and summer due to snow and glacier melt in contrast low flow phases are mostly restricted to the winter season the basin belongs to one of the headwater streams of the syr darya one of the two major tributaries of the aral sea and drains an area of around 50 000 km 2 our case study focuses on the headwaters of the basin which originate in the tianshan mountains snow and glacier melt are of great concern for the local population as it provides water for energy and agriculture unger shayesteh et al 2013 gan et al 2015 the parameters exposed concern snowmelt and snowfall a full list is provided in table 3 the choice is based on preliminary investigations that comprised a comprehensive sensitivity analysis and calibration of our swat model schaffhauser et al 2023 these parameters snowfall temperature snowmelt temperature maximum and minimum melt rate proved to be among the most sensitive ones providing a reasonable model performance the case study represents an example where the model is intended to be used by local authorities our configuration provides an example of an abstraction that can be used by both non experts and more experienced users in this case the non experts will be decision makers in an agricultural agency while the more advanced users will be in the local water authority who will have a broader expertise in water related questions the model component shall finally be used by these decision makers to examine the effects of changes in snow processes on streamflow snow processes constitute the dominant source of water and serves as a proxy of water availability in spring for the region an exploration of the timing and amount of snowmelt provides decision makers with valuable insights on the available water for different sectors such as agriculture or energy this information is important in many aspects for example authorities can deduce how much water is expected to be available for agriculture this enables an estimate of the expected yield within the crop season one of the major economic factors for the region in addition this water is required to be stored for energy production of the whole country besides the period is prone to floods frequently causing at least local threats by having model outputs of water availability decision makers can allocate water to different purposes advance or delay planting dates and generally prepare for the specific seasonal requirements such as energy or irrigation demand accordingly we share the model configuration to enable these users to adjust the snow related parameters namely snowmelt temperature snow fall temperature as well as the minimum and maximum snowmelt factors users can then explore their own scenarios and monitor the actual conditions of the basin to assess which of their scenarios correspond to the actual conditions of the current season to provide some initial scenarios we provide a set of default values for all snow parameters to provide users with a starting point as the response variable of interest for the end user is discharge only the corresponding output file is exposed in our component for simplicity we decided to predefine all input files so that users cannot make any changes 5 3 3 drought impact on the water budget in barton springs case study the second study refers to the edwards aquifer and more precisely the barton springs segment in austin texas a region increasingly affected by droughts passarello et al 2012 2014 a numerical simulation using the modflow model was developed for use as a groundwater availability model gam in the state of texas scanlon et al 2001 2003 the modflow configuration was prepared as part of a state wide planning activity the components underwent a scientific vetting process to assess groundwater availability the intended end users are groundwater managers for state designated management districts as well as stakeholders involved in the recurring groundwater aquifer management program of the state of texas they are not hydrology experts necessarily although they have expertise in groundwater water availability fluctuates rapidly in the region due to normal variability in weather and climate conditions as urban areas have expanded in the past decade water consumption has increased and habitats for vulnerable species are at greater risk for impact during dry conditions table 3 shows an overview of the models we created a model component m b that reflects a baseline model for drought conditions with default pumping rates we created a separate component m a for average conditions also with default pumping rates m b was explicitly designed to investigate and emphasize potential adverse effects of pumping under dry conditions in contrast m a shows the impacts of similar pumping conditions under normal non drought conditions we also created a third component m i where the user can specify infiltrated water as a recharge input file and pumping rates as a wells input file the components are designed to expose key model outputs concerning water table levels hds output file representing hydraulic head levels storage cbb output file representing volumes and actual pumping rates cbb output file the recharge zones were developed for barton springs gam because it represents a baseline interpretation of groundwater behavior the model is readily accessible the recharge zones were originally completed as part of a groundwater decision support system developed to assess the sustainable yield pierce et al 2006 pierce 2006 5 3 4 model encapsulation the following subsections demonstrate the model encapsulation of each case study a summarized overview of the steps and the differences in the procedure where users have to perform manual adaptions for each case study is shown in table 4 the encapsulation process follows the model preparation steps usually including calibration and validation which are performed by the expert modeler 5 3 4 1 step 1 start new environment an environment has to be created for each model configuration see section 2 3 for case study 1 the modeler would create a single model component focused on the snow processes of swat for case study 2 the modeler chose to create three separate model components one for baseline drought conditions one for baseline average conditions and a third one for analyzing different scenarios in average conditions the modeler starts mic from the command line where he provides the name of the model configuration in our case the names are naryn swat and barton springs modflow 1 to 3 mic automatically creates the folder structure for each model configuration 5 3 4 2 step 2 trace execution dependencies the modeler then does a test run to check if the respective model is installed in a new environment and to trace the execution dependencies then mic is used to trace input and output dependencies through reprozip since mic is a unix based tool the invocation command for swat refers to the unix based execution file which can be downloaded via the swat homepage 4 4 https swat tamu edu software swat executables as for modflow we used the python based flopy tool for the model encapsulation 5 5 https www usgs gov software flopy python package creating running and post processing modflow based models flopy serves as a tool which is used to execute existing modflow based models 5 3 4 3 steps 3 4 expose parameters inputs and outputs for the swat model configuration several snow parameters were exposed which were snowfall temperature snowmelt temperature and the maximum and minimum melt factors the parameter selection was based on a preliminary study done by the modeler with relevant stakeholders to identify the dominant parameters see also table 3 each parameter exposed must be manually specified in mic as described in section 4 subsequently the parameters must be indicated in the corresponding swat input files as shown in fig 2 adjustments of default parameter values are possible during this step as well next the modeler declares the input files that contain the exposed parameters as configuration files since all snow parameters of swat are stored in the basin file basin bsn it is the only configuration file relevant to the model configuration the users in the naryn case study such as authorities related to the agricultural energy or water sector do not need all the output files so only the output rch file is exposed as it contains all required information on streamflow within the basin for the configurations of the modflow model in case study 2 no parameters were exposed for the drought model component only the hds and lst input files were exposed where the relevant information of the hydraulic head and the water budget can be specified by users 5 3 4 4 step 5 create wrapper script mic helps wrap model configurations by taking into account the execution settings and prepares the files to test the model components the test runs done by mic were based on the default parameter settings defined in the previous step and double checked manually after the test run the model configuration was finalized and ready for upload 5 3 4 5 step 6 model upload finally mic uploads the model configurations to relevant repositories the docker image of the model component was uploaded to dockerhub 6 6 https hub docker com r mosorio naryn nival setup tags 7 7 https hub docker com r mintproject modflow 2005 tags a github repository containing the input data and results was also created 8 8 components are archived in zenodo https zenodo org record 6948339 yue0vhzbymq finally an entry in the mint model catalog was created 9 9 https mint isi edu kyrgyzstan models explore swat 8cc84426 d849 471b 9a5e 47bcaf094607 6a36a2e5 73bf 4098 9acd 1aaaab383d4a 14580635 c7ca 4256 935a 4ddbdacfbfe2 10 10 https mint isi edu texas models explore modflow modflow 2005 modflow 2005 cfg modflow 2005 bartonsprings avg and the model can be easily run from the mint user interface 6 scenario exploration by non expert users with new model configurations this section describes how users can access the newly created model configurations of the two case studies it highlights how users can easily specify simulation scenarios using the model configurations 6 1 accessing model components users can browse all model configurations for example by bringing up the corresponding regions kyrgyzstan and texas or browsing entries in the mint model catalog typically a user starts in the use models tab and specifies a problem statement by selecting a time period for the simulation a region of interest and desired response variables i e simulation outputs once the problem statement is specified mint will show the user relevant model configurations that can be run fig 6 shows the mint user interface to access model configurations in the different regions more details are provided in the next section 6 2 model dataset parameter selection the naryn case study aims to simulate discharge by adjusting the snow parameters that govern the predominant processes in the region in detail these processes involve snowmelt and snowfall and therefore the snowpack distribution in the region these processes control discharge generation thus a task was created where river discharge was used as response variable as shown in fig 7 a it would also be possible to use other models to obtain discharge such as topoflow peckham 2009 a similar overview for the barton springs case study is provided in fig 8 multiple model components may be selected which would allow users to easily create a model ensemble to provide a more differentiated picture at this point the user would choose among all available input datasets such as meteorological information with precipitation and temperature an example can be found in fig a 1 where users can choose between two alternative well files with different pumping rates for the barton springs area which would allow them to evaluate the effects on the groundwater levels under various pumping rates users can also easily specify different parameter values to reflect different scenarios or assumptions as described in section 5 3 2 four snow parameters were exposed and the user can assign them different values to explore different scenarios as shown in fig 7 b this allows users to explore different dates for the onset of the snowmelt season represented by various temperature thresholds if users provide several values mint runs the model multiple times all outputs are provided individually for each run in the parameter tab fig 7 b users can also check the default values of each parameter this is particularly relevant if the default values are based on expert knowledge and refer to a specific baseline which can be used for the comparison with developed scenarios the static design of the barton springs components does not allow for any parameter changes thus the corresponding tab is empty see fig 8 b 6 3 execute analyze model components when users set parameter values the model component can be executed and the results tab in mint provides a summary of all the outputs in the naryn use case the outputs were limited to the output rch file where the discharge information is stored the results represent the discharge response to the adjusted snow dynamics in the basin which were compared with the baseline simulation results obtained with the default parameter values no shift for every parameter combination a model run was executed and the corresponding outputs were generated an example is shown in fig a 2 where the user specified two values for each of the four parameters which led to 16 combinations each resulting in an execution and each with its output files to eliminate unnecessary computations mint caches the results of executions mint can generate some standard visualizations but users often generate their own custom visualizations after downloading the results in a post processing step we show a visualization of the results from the swat case study comparing a particular scenario versus the baseline in fig 9 users can now directly derive the desired information depending on how strong the average shift in river discharge would be under changing snow conditions the results shown in the figure represent the mean of a five year period from the plot it becomes apparent that the conditions represented by the scenario would lead to a strong rise in discharge in early march already besides the earlier onset and steep rise of snowmelt would cause higher discharge in april and may compared to the baseline while it would be reduced during the summer months the annual peak would already occur 1 5 2 months earlier than in the baseline this significant change in the flow regime may have far reaching consequences for the water sector for example the summer reduction may affect agricultural production as irrigation water is missing while the strong increase after the winter months may promote the damage potential of flood events ultimately water authorities may conclude to assess the potential of a reservoir to mitigate those undesired effects for the second case study we declared a problem statement where we included the different barton springs model components to access the model configurations the study area in mint has first to be changed to texas analogously to fig 6 where we can then select barton springs modflow users can create tasks which reflect a specific scenario and select an appropriate model configuration one of the scenarios may focus on drought assessment using the m b configuration another task may be for average conditions using the m a configuration a third task may focus on the impacts of specific recharge and pumping conditions using the m i configuration users have the possibility to compare the three different setups and easily analyze the differences in groundwater availability in the region in detail one can evaluate the effects of pumping on groundwater levels and study how the aquifer should be managed to maintain flow under specific conditions users might infer that under drought conditions pumping alone is not sufficient in contrast to the swat model configuration where only one output file is accessible the modflow model configurations offer four different output files it is worth noting that the application of the scenarios does not require any computing programming skills for users however if users want to run encapsulated models locally basic container skills are required in general this is seldom the case since mint relies on user friendly gui figs 6 and 8 7 discussion models created by experts are usually difficult to use by modelers in other disciplines despite the need by decision makers to access sophisticated models they remain inaccessible to non experts bagstad et al 2013 even experts within a discipline find that it takes significant effort to setup and compare models from other modelers lüke and hack 2018 francesconi et al 2016 our work shows that two very different hydrological models could be encapsulated using the same methodology to simplify model dissemination by experts for use by non experts our mic tool can be used by expert modelers without major knowledge of software engineering e g using software containers managing execution dependencies or setting up code repositories we demonstrated the methodology for different model domains purposes technical details and model structures our case studies illustrate that modelers only have to determine the parameters and input and output files to be exposed according to the intended scenarios different uses of a model e g snow related analysis or studies focusing on crop yield lead to different model configurations and are organized and easily accessible in mint the methodology enables expert modelers to create useful abstractions of existing models the abstraction hides the part of the model complexity that is not necessarily required for the target users therefore once a model has been encapsulated with our methodology non expert users are relieved from dealing with the technical details of the model execution or its structure different types of non experts may benefit from our effort depending on their expertise and background for example citizens of hydrological extremes drought and floods who become relevant stakeholders and develop a certain level of expertise to understand their own scenarios ngo members who are interested in model applications in the environmental sector or decision makers who usually have a decent hydrological know how but may not be familiar with modeling water authorities are often busy with administrative work which means that there is little time for the construction and calibration of complex models additionally we envision expert modelers to benefit from this effort as it facilitates the creation of model ensembles for model comparisons or for benchmarking our methodology may be used to share and use pre agreed scenarios as in our barton springs case study and support users developing their own scenarios independently by modifying the exposed parameters we also included the possibility of exposing input datasets in model configurations so users can select their own for example several meteorological data sets may be used for the execution of a model configuration processing all required input data is time consuming and hms often have different requirements exchanging these data often represents an obstacle gardner et al 2018 that can be at least partially overcome by using mint modelers are also encouraged to describe their configurations with metadata so that users can search flexibly for models and use those that are suitable for their scenarios a region specific search which corresponds to kyrgyzstan or texas in our examples allows users finding all available models for that region modelers should also provide code for output visualizations see section 6 3 the integration of a general visualization environment in mint would facilitate the usability in extended scenarios for example by integrating other datasets that may be relevant to the modeling scenarios e g population density road access etc although the examples of this paper focus on hydrological models our methodology has been applied to models in other domains including agriculture and economics we assume all encapsulated models to be open source or have an open source executable that can be shared in a software container this methodology helps aligning a software component with the findable accessible interoperable and reusable principles fair for data wilkinson et al 2016 following current best practices for open science by creating software components that have specific functionality and clear invocation and results modelers provide self contained and pre prepared model components that are well characterized and become easier to reuse than the original modeling software model components are more accessible than the original modeling software as they are encapsulated in a software container that can be executed in any platform model components include extensive machine readable software metadata that makes them more findable and interoperable finally it is worth noting that we used pre calibrated models for our case studies future work will address this limitation by integrating model calibration capabilities into our framework and methodology 8 conclusions this paper introduced a methodology to simplify the dissemination of expert models to non expert users the methodology guides modeling experts when creating software components that explore specific modeling scenarios the methodology is applicable to any kind of model regardless of its discipline processes or technical details the implementation of the methodology in the mic tool enables a simple model encapsulation process for modelers this does not only facilitate model dissemination and provision but can also improve mutual work within or across disciplines and groups in addition the complexity of the model can be simplified by creating model configurations that suit the needs of non expert users our proposed methodology thus creates new possibilities in model abstractions and promotes the satisfaction of end user needs this is also supported by the easy access options of model configurations in mint which greatly simplifies their re use we illustrated our methodology with two case studies using two different hydrological models in two different regions of the world the case studies provide examples how potential scenarios and use cases for the application of the methodology could look like however the universal applicability of the methodology within any modeling discipline enables a free design of scenarios with numerous potential use cases that can help both the expert modeler as well as the end user mint users can easily compare the effects of pumpage under different conditions on groundwater levels moreover they can infer whether pumping is suitable to maintain flow under drought conditions or if additional measures should be taken into account additionally we showed how a restriction of the parameter space to a useful minimum can facilitate the exploration of discharge shifts by decision makers the methodology encourages the possibility of independently investigating scenarios and to derive valuable insights for example resulting discharge shifts may lead to several consequences for the water sector e g increased flood risk or decreased agricultural production to mention only two out of dozens that call for action our work supports the fair principles helping model components to be more findable accessible interoperable and reusable however our methodology also presents some limitations which are part of our future work for example while our methodology helps non experts executing models created by expert modelers some expertise is still needed to interpret the results of the simulations in some cases this is addressed by adding documentation and metadata in the scenario in order to provide the right context for end users in other cases expert modelers include ad hoc visualizations that are executed with the model itself helping to interpret the outputs extending our methodology to ensure that visualization components are described for each model output would help address this issue we are also exploring extending mint with general purpose visualizations e g variables obtained in tabular model results another point of improvement involves expanding the supported actions for modeling experts in mint for example including additional data transformations and model calibration right now models are calibrated by experts independently finally additional case studies in other domains are part of our future work in order to further refine the applicability of our approach when disseminating models across disciplines lowering the barrier of adoption of models by modeling experts software and data availability name of the software model component 1 snow dynamics developer timo schaffhauser t schaffhauser tum de maximiliano osorio mosorio isi edu software availability https hub docker com r mosorio naryn nival setup tags docker image compressed size 286 97 mb docker image name of the software model component 2 drought impact developer suzanne pierce spierce tacc utexas edu maximiliano osorio mosorio isi edu software availability https hub docker com r mintproject modflow 2005 tags docker image compressed size 733 55 mb docker image name of the software model insertion checker mic developer maximiliano osorio mosorio isi edu software availability https zenodo org record 6024985 yvpflnzbymo programming language python compressed size 19 9 mb name of the dataset swat modflow model components developer timo schaffhauser t schaffhauser tum de daniel garijo maximiliano osorio daniel bittner suzanne pierce hernan vargas markus disse yolanda gil data availability https zenodo org record 6948339 yvj6v3zbymr form of repository zenodo archive compressed size 51 7 mb further access to the model components is possible via https mint isi edu kyrgyzstan models explore swat 8cc84426 d849 471b 9a5e 47bcaf094607 6a36a2e5 73bf 4098 9acd 1aaaab383d4a 14580635 c7ca 4256 935a 4ddbdacfbfe2and https mint isi edu texas models explore modflow modflow 2005 modflow 2005 cfg modflow 2005 bartonsprings avg declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments we would like to thank our collaborators in the mint project particularly modeling experts and users that contributed to the design of the overall mint framework we gratefully acknowledge support from the us defense advanced research projects agency through award w911nf 18 1 0027 and the us office of naval research through award n00014 21 1 2437 the authors from tum also want to thank the bmbf bundesministerium für bildung und forschung for the funding of the oekoflussplan project grant number 01lz1802b the author from upm has been supported by the madrid government comunidad de madrid spain under the multiannual agreement with universidad politécnica de madrid in the line support for r d projects for beatriz galindo researchers in the context of the v pricit regional programme of research and technological innovation and the call research grants for young investigators from universidad politécnica de madrid appendix a 1 see fig a 1 a 2 see fig a 2 
25405,hydrological models are essential in water resources management but the expertise required to operate them often exceeds that of potential stakeholders we present an approach that facilitates the dissemination of hydrological models and its implementation in the model integration mint framework our approach follows principles from software engineering to create software components that reveal only selected functionality of models which is of interest to users while abstracting from implementation complexity and to generate metadata for the model components this methodology makes the models more findable accessible interoperable and reusable in support of fair principles we showcase our methodology and its implementation in mint using two case studies we illustrate how the models swat and modflow are turned into software components by hydrology experts and how users without hydrology expertise can find adapt and execute them the two models differ in terms of represented processes and in model design and structure our approach also benefits expert modelers by simplifying model sharing and the execution of model ensembles mint is a general modeling framework that uses artificial intelligence techniques to assist users and is released as open source software keywords software metadata model metadata model encapsulation model catalogs mint hydrological models data availability all data is referenced in the text and can be accessed using the corresponding links 1 introduction hydrological models hms are commonly used for water resources management and are mainly developed and used by expert researchers or engineers working in the water sector the results of hms are important and considered in decision making processes of government agencies ruiz ortiz et al 2019 andreu et al 1996 hm applications include estimation of water availability döll and lehner 2003 development of water management strategies haasnoot et al 2011 flood risk assessment merz et al 2010 climate impact analysis krysanova and hattermann 2017 lobanova et al 2018 hattermann et al 2018 solute transport konikow 2010 morales et al 2010 and spatial characterization of hydrological system variables such as soil water content brocca et al 2017 desalination and industrial wastewater treatment panagopoulos 2022 as well as groundwater heads reinecke et al 2019 hms vary widely in terms of their mathematical description of prevalent hydrological processes and their spatial model structure ranging from lumped conceptual models bittner et al 2018 booij and krol 2010 to distributed physical models brunner and simmons 2012 newman et al 2017 a fundamental understanding of hydrological processes is needed in order to reasonably set up a hydrological model for a new region or modeling problem this may become an obstacle for the use of hms by decision makers and other users lüke and hack 2018 in practice model results are presented to decision makers as a summary focusing only on a few specific variables of interest such as streamflow or groundwater heads the interests and requirements of decision makers and various stakeholders can diverge widely from what may be hydrologically interesting decision makers in water resources management are usually interested in the assessment of the water balance primarily the availability of water in space and time hms allow a holistic view on the components of the water cycle from which insightful information e g limiting factors in space and or time can be derived these variables do not necessarily be restricted to water availability but could also refer to evapotranspiration soil water or precipitation miscommunication between science and non expert groups is therefore not a rarity timmerman and langaas 2005 this increases the science policy gap due to differences in the level of knowledge between the information producer and receiver bernstein et al 1993 bradshaw and borchers 2000 consequently it is a challenging task for modelers to provide information that is practically usable and interpretable by a broader community of end users fatichi et al 2016 ideally hms would be accessible to any potential users so that they are able to test different decisions and scenarios themselves potential users who are not hydrology experts can include data analysts decision makers and also scientists in other disciplines who aim to incorporate water related topics into their models in situations where different disciplines need to work closely together and where models from different areas such as economics hydrology climatology or ecology may need to be integrated further obstacles often emerge as hms often need to be designed exchanged and run by different user groups moreover several models with overlapping features may be available and selecting an appropriate model for a task can be challenging even for experienced modelers surfleet et al 2012 in addition enabling different capabilities of a model can lead to different data and input requirements even for hydrology experts it can be difficult to understand how processes are represented in different hms making comparison studies very time consuming hms tend to have special computational requirements and use heterogeneous file formats for spatio temporal data so that data pre processing usually requires basic programming skills additional technical challenges arise when hms require different operating systems or complex model configurations which can limit the applicability and transferability of models even for hydrology experts therefore there is a great need for new approaches to facilitate the dissemination of hms to users who lack the expertise to develop them but are invested in using them for decision making purposes over the last few decades efforts have been made to make hms more accessible by integrating them into geographic information systems gis bittner et al 2020 rossetto et al 2018 refsgaard et al 2010 in this regard gis based interfaces to hms often act as an essential component of a decision support system dss lautenbach et al 2009 pezij et al 2019 zhang et al 2014 executable and well structured dsss make hms even applicable by non expert groups but dsss usually lack transferability as they are strongly tailored to the individual conditions of a defined case study an example of how dsss are often developed in the course of a project to combine different stand alone software tools can be found in kinzelbach et al 2021 however a limitation of many dss is that they are desktop based and therefore show limited accessibility moreover they often focus on one area such as groundwater and even on one model and are thus lacking interoperability gis based interfaces have been used in the soil water assessment tool swat arnold et al 1998 the free and open source software tools water resources management system freewat koltsida and kallioras 2019 or the hydrologic engineering center hydrologic modeling system hec hms us army corps of engineers 2000 but models must be set up from scratch by experienced users furthermore these platforms only include a single hm while users often want to use several hms to compare their results consequently initiatives like the community surface dynamics modeling system csdms peckham and syvitski 2007 peckham et al 2013 the earth system modeling framework esmf hill et al 2004 or the hydroshare platform horsburgh et al 2016 have already taken a step forward to provide and combine multiple models from different disciplines csdms and esmf include the dissemination of final and calibrated models combined with their results from a variety of disciplines in the field of geo and earth science overeem et al 2013 collins et al 2005 keller et al 2014 while hydroshare is explicitly designed for the exchange storage or management of hydrological datasets and models gan et al 2020 however these efforts are focused on users who are modeling experts pursuing science research rather than non expert users in order to ease the dissemination of expert models to non experts our previous work introduced the model integration framework mint gil et al 2018 2021 mint defined the components and interfaces needed to assist expert modelers when setting up pre existing hms for non experts but adding new hms to the framework required advanced software engineering skills making it challenging for expert users to contribute this paper builds on our previous work with the following novel contributions 1 a methodology that follows principles of software engineering to create software components for hms with a simple invocation function with pre set inputs and parameters capturing metadata about the model that can be used to provide guidance to non expert users 2 an implementation of this methodology that guides expert modelers to create model components integrated in the model integration framework mint gil et al 2018 2021 3 two use cases that demonstrate the use of this methodology and implementation for two models that differ in terms of hydrological processes they consider as well as in terms of their individual code structure swat arnold et al 1998 and modflow harbaugh 2005 this methodology makes models more findable accessible interoperable and reusable in support of fair principles wilkinson et al 2016 the paper begins with a description of our proposed methodology for creating software components for models section 2 next in section 4 we illustrate how the methodology is implemented in the mint model insertion checker a standalone application designed to guide users through the proposed methodology steps section 5 describes two examples that follow our methodology to deliver two different hm configurations for two different regions of the globe section 6 shows how each of these configurations can be accessed and run in the mint platform section 7 discusses the main advantages and limitations of our approach and section 8 presents conclusions and future work 2 background hms differ in the way they conceptualize the characteristics and flow processes in a natural system as a result hms usually have dozens of parameters and input files which vary across different scenarios for example models like swat may use an input file with snowmelt observations in regions with mountains but may not take snowmelt into consideration if there are no mountains around the basin of interest expert hydrologists who we will refer to here as modelers need to make decisions about which hydrological processes and corresponding parameters are relevant to the intended non expert users e g decision makers analysts researchers with expertise in other areas or domains students in training or citizens who are active in non governmental organization who we will refer to as users 2 1 software components encapsulating software into portable components allows other users to easily run software on their own machine without worrying about the environment and set up needed boettiger 2015 kurtzer et al 2017 following well established component based software engineering principles we aim to create self contained software components that only reveal functionality that is of interest to third party users this is important because scientific software components are often implemented in large packages or libraries that can be used for various steps such as data preparation and visualization in addition to writing software to simulate specific processes such as atmospheric dynamics for climate models runoff and infiltration for hydrology models fuel density for fire modeling etc software packages can be quite overwhelming for users even when they are familiar with the scientific domain for which the package was written usability becomes even more challenging for users outside of the domain although these users are precisely the ones who may benefit the most from the results of the respective packages existing graphical user interfaces guis and gis systems are often difficult to reuse from other programs user interfaces usually have a specific function to call the software with a button using a form which users operate to define specific parameters typically the most relevant ones that function call sometimes called a command line invocation is reusable from different programs provided that the software tool can be run from a machine with its specific execution environment the function call uses inputs that can be provided when invoking the software component as it is done in a user interface where the values for some input parameters are set other inputs can be pre set within the component including data files if there are no reasons for third party users to change them given a specific use case a software component corresponds to a single invocation function for software given a sophisticated software package with multiple purposes a software component may be created to include only certain processes and variables a specific pre processing step or a specific visualization for example a hydrology model software may be pre set to be applicable to hot arid regions only and ignore the processes and therefore inputs describing snowmelt 2 2 model configurations encapsulation we use the term model encapsulation to refer to the process of creating easy to use self standing executable software components from models for a target scenario we refer to these software components as model configurations expert modelers are responsible for designing these model configurations for a region by identifying the key parameters that non experts should be able to modify model configurations declare only those relevant parameters or input files that users should be able to change so the model configuration can be easily set up and run to explore different scenarios the remainder of this section illustrates model configurations through a simple example introduces key concepts in model encapsulation and describes the main steps of our methodology 2 3 model configurations an example for illustration purposes let us consider alice an expert hydrology modeler and bob a decision maker with little hydrological expertise bob needs to regulate policies for the water budget at a country scale and therefore he is interested in obtaining a rough estimate on water availability during the spring season in particular bob seeks to understand 1 whether the water demand of specific crops can be met under different assumptions and 2 the impact of runoff for energy production i e from hydro electrical plants given her expertise bob asks alice to provide an environment where he can run model simulations according to his requirements alice anticipates that bob may want to modify some of the simulation parameters affecting snowmelt the dominant runoff component and source of water in spring a shift in the onset and duration of snowmelt usually affects the temporal water availability of the agricultural and energy sector alice decides to use the swat model and creates two model configurations to predict streamflow as a proxy representation for water availability the first model configuration is designed at the country level letting bob modify the snowmelt temperature and the maximum possible snowmelt to explore the effects on agriculture e g what crop yield can be achieved by different crops the second model configuration focuses on a small basin located in the northeastern region of the country in order to study the conditions and effects of snowmelt for a potential small hydroelectric power plant both model configurations first undergo a strict and rigorous calibration and validation procedure by alice a necessary expert step to ensure a reliable baseline for the further usage the calibration and validation serve as fundamental steps to provide robust and credible models fig 1 shows an overview of the model configurations prepared by alice with the country level configuration on the left and the basin configuration on the right each configuration has one or multiple inputs and outputs representing the files accepted and produced by a configuration we use the term parameters to refer to values a user may be interested in changing in a model such as snowmelt temperature even if these values are declared within configuration files we consider as parameters hydrological or process based variables together with temporal information such as simulation length or time step here referred to as general settings gs a code wrapper captures how to invoke a model configuration by indicating how the command line should be invoked and specifies any fixed values of inputs when creating a model configuration a modeler like alice may have to choose which of the inputs or parameters should be adjustable by the final user among the dozens or hundreds of input files and parameters hms have we use the term expose to indicate that a parameter or input file can be adjustable by a user in a model configuration for example swat contains hundreds of files but alice estimates that the relevant ones for the country level configuration are two input files with snow and elevation information as shown in fig 1 the input file containing snow information further includes the parameters that will be exposed to users namely snowmelt temperature and the maximum melt factor of snow adapting the threshold temperature when snow begins to melt is an easy way to shift the melt season within the country the second parameter provides information on the amount of snowmelt one could expect alice thus provides a meaningful range of values within which bob is able to increase or decrease the amount of snowmelt in addition alice decides to expose a file that includes basic information on gs like the time for which the model was set up or its temporal resolution daily in this case thanks to this information alice expects bob to be able to compare the effects of a very high and a very low value for snowmelt temperature as well as the maximum melt factor on the water availability as for the basin configuration alice is familiar with the area from her previous work therefore she decides to set up all default values of the model according to her knowledge of the region she exposes snowmelt temperature by making only this parameter available in the basin configuration this configuration is more restricted but more precisely tailored to the region at hand therefore this model configuration is simplified by allowing bob to only modify snowmelt temperature hence bob can now obtain alternative estimates with respect to the accumulation of snow during winter which is then available as melt water this enables the decision maker to infer whether a small hydropower plant might be of value or not or how much energy could be produced under various snowfall conditions in summary with these model configurations the modeling expert is able to hide the complexity of a general model exposing only what is relevant for a country and its hydrology narrowing it down to a much more usable model component for other users to explore scenarios and make decisions accordingly it should also be mentioned that bob does not necessarily must be a decision maker however he could also be an interested member of a ngo which deals with environmental issues for example or just an interested citizen increasingly affected by hydrological events such as drought or heavy rain table 1 provides an overview about the terminology we use in this paper especially to distinguish terms which might ambiguous and are used differently in other fields 3 a methodology for model encapsulation we propose a methodology for creating model configurations our methodology requires expert modelers to determine the main parameters and input files that need to be exposed for a given executable model including steps for guiding and testing the final model configuration so other users can use it effectively our methodology comprises six main steps start a new environment trace execution dependencies expose parameters expose input output files wrap execution and model upload table 2 provides a summary of all steps which are further described here step 1 start a new environment modelers start by specifying the location for the folder structure of the new model component they want to create this should be started in a clean computing environment free from other software dependencies installed on the local machine for example if a model is available in python starting in a clean environment makes it easy to isolate the model needs from other python libraries installed in the machine for other purposes this can be achieved by using virtual environments that create a clean python installation with no installed package dependencies in our methodology we adopt software containers a common approach to capture computational environments software containers enable capturing the dependencies of a software component at the operating system level i e including not only the dependencies of a software component but all the system dependencies as well hence ensuring that it can be run in other environments because containers can be complicated to set up and use for non computer scientists our system will be automatically creating the container and installing the dependencies and files needed to run the model the modeler can see everything that the system is adding in the folder that they specified step 2 trace execution dependencies and run model once an environment has been set up the dependencies needed to install the model must be incorporated into the environment this includes compilers system libraries and other files the modeler carries out a test run that is representative of how the model configuration will be used during the run the system automatically detects the model input configuration and output files used by the model during the run this information is added to the container environment and used by the system in subsequent steps in order to assist the modeler to specify inputs and outputs step 3 expose model parameters and define configuration files most hms have dozens of parameters and gss which specify constants like hydraulic conductivity bulk density or the general settings of the simulation within this step modelers have to define which of these parameters and gss they want to expose to users in the new model configuration for example the cn2 curve number ii parameter of swat is usually one of the parameters which is typically changed during the model setup and calibration the process of estimating relevant parameters and their corresponding values and might be a useful parameter to expose in a model configuration hms usually adjust numerical values for their simulations in two different ways 1 with the invocation command used to run the model or 2 through configuration files that can be edited directly or accessed via user interfaces if a file is used it needs to be specified by the modeler fig 2 illustrates this with an example of how the snowmelt base temperature parameter is exposed for the swat hydrology model smtmp through a configuration file step 4 expose model inputs and outputs next modelers have to decide which input and output files they want to expose which depends on the intended use cases that users will want to simulate as with parameters and gss expert modelers usually provide the relevant input files required by a model likewise models produce all sorts of output variables and for a given configuration only a certain subset of outputs may be relevant for the intended use cases for instance a modeler may expose only output files containing drought related variables such as evapotranspiration and soil moisture step 5 create a wrapper script once the parameters gss and files to be exposed have been specified the next step is to write a shell script which captures how to run the model configuration we refer to this script as the wrapper script as it wraps the model configuration as an executable component the wrapper script will make sure that the component can run with the inputs and outputs selected by the modeler and may include pre set files or values for other inputs and parameters in order to verify that the model works appropriately with the wrapper script it is necessary for the modeler to provide sample input files which are used in a test run if everything works successfully the model configuration is completed and will be executable in other computational environments step 6 upload the model configuration the final step is to deposit the model configuration in shared repositories first the script and test data used to wrap up the model configuration should be deposited in a code repository second an archival version of the model software code must be created in a code repository to ensure that version can always be accessed by users in the future third the container environment should be uploaded to a container registry finally the model configuration should be uploaded to a model catalog with proper model configuration metadata provided by the modeler to enable discovery and reuse 4 methodology implementation we implemented our methodology in the mint model insertion checker mic a standalone application developed to guide users through the process of creating new model configurations mic performs all the steps of our methodology in a semi automated manner integrating the results with commonly used software and container image repositories such as github and dockerhub mic also integrates new model configurations and the metadata in the mint modeling framework and its model catalog garijo et al 2019 mic is implemented as a unix based tool that runs in the command line and is available as open source software osorio et al 2022 1 1 software respository is at https github com mintproject mic with documentation at https mic cli readthedocs io en latest overview a step by step tutorial is available online 2 2 https mic cli readthedocs io en latest model configuration 03a step1 to help users and disseminate the steps of our methodology fig 3 provides an overview on how mic implements all the steps of our methodology capturing the main software dependencies input parameters and generated files and showing how the methodology steps are related to one another mic guides users through the six steps outlined in our methodology it starts with a blank unix environment generated with a basic docker image where users are asked to install and run their model from scratch step 1 once a sample run is finished mic tracks which files have been used and generated using reprozip rampin et al 2016 an application designed to trace all dependencies and system calls of a program step 2 using the output from reprozip mic drafts an initial component based on the inputs and outputs detected in the test run next mic works with the modeler to get information about the inputs parameters and gss of hms should be exposed in the model configuration among all the candidates detected automatically steps 3 and 4 the preparation of the configuration file is one of the few activities that has to be carried out manually by the modeler as it involves information highly dependent on the use cases required by the intended users for example swat may be used to create multiple configurations depending on whether modelers need to expose snowmelt temperature hydraulic conductivity or a factor to delay groundwater flow the parameters and gss exposed with mic will be adjustable by users when running the model configuration if one of the exposed parameters or gss are stored in a configuration file an additional step is required to indicate where to replace the target value in that file an example can be seen in fig 2 where snowmelt temperature in swat is exposed through the smtmp parameter which can be provided by users at runtime all the information provided to mic is stored in a mic settings file that can be inspected and edited by modelers at any time e g to change default values for parameters or to make adjustments on what is exposed to users once all the inputs outputs parameters and gss form a specific model configuration are set mic will prompt users to perform a test run using all default values mic automatically creates an execution wrapper script step 5 and runs the model using the local environment created earlier in the second step if successful the model configuration is ready to be run by others and mic will prompt users to double check if the results from the execution are correct as a final step mic saves the model configuration step 6 including 1 the computational environment used in the test run saved as a docker image in dockerhub 3 3 https hub docker com 2 the wrapper script and settings file containing the exposed inputs outputs parameters and gss mic will store these files in a new github repository owned by the modeler who created the model configuration 3 basic metadata about the model configuration including its main title description version of the model geographic location execution details and brief parameter and input descriptions these metadata are submitted to the mint model catalog producing the results shown in fig 4 in the following sections we provide several screenshots of mic to familiarize the reader and potential users with the platform fig 4 shows an example of how the model configuration can be accessed by a user after being created with mic fig 4 a depicts a model configuration where four parameters are exposed i e minimum and maximum melt factors snowfall temperature and snowmelt temperature out of the dozens of parameters that are available in swat fig 4 b shows an example where only one of the four parameters snowfall temperature may be changed by users when running a second different configuration of swat the other three parameters are fixed both configurations of the model are integrated in the mint framework where they can be executed through a gui 5 creating model components two practical use cases in this section we showcase our methodology by encapsulating two different and widely used hydrological models i e swat and modflow using mic to create model components and running them in the mint platform by pointing out the specific differences of swat and modflow we illustrate the main concepts of our methodology as well as the technical features of mic that facilitate model dissemination for any type of hm we show model configurations for swat and modflow for two different case studies each case study was defined prior to our work by a different research group working with stakeholders in different regions of the world 5 1 swat background and model structure the soil water assessment tool swat is a semi distributed time continuous model developed by the blackland research extension center of the united states department for agriculture usda arnold et al 1998 swat is based on the concept of the hydrologic response units hru and was originally developed to assess the impact of land management practices in large watersheds while the applications nowadays range from water quality or sediment transport studies up to snow hydrological in basins all over the world arnold and fohrer 2005 hrus are the smallest spatial unit within the model and defined on the subbasin scale a further subdivision of the watershed however hrus are not spatially located and are formed by unique combinations of land use soil and slope within each subbasin to consider spatial heterogeneity the hm is organized by input files grouped by different processes or characteristics such as land management or soil inputs for the individual spatial units besides the model includes few general files where basic settings can be done swat separates its calculations in a land and a water phase it first calculates all loadings for the hrus in each subbasin which are then transferred to the stream in a second step the in stream processes covering routing processes as well as chemical processes are calculated 5 2 modflow background and model structure the modular finite difference flow model modflow is a fully distributed and physically based groundwater model developed by the united states geological survey harbaugh 2005 hanson et al 2014 modflow is organized in modules which allow for user customization of specific case studies i e by selecting only those modules that are relevant for instance a module can represent different solvers for the groundwater flow equation moreover various modules exist to account for different hydrological processes in a natural system e g stream flow evapotranspiration or groundwater recharge given the grid based nature of the model several modules can be coupled by providing grid coordinates in the input files if specific modules should be used in a model run an input file is required for each respective module these input files are ascii files either organized in a table format or grid based all modules to be used for a model simulation have to be included in a configuration file i e a name nam file depending on the interaction of different hydrological processes modflow solves the groundwater flow equation and provides water budgets for each pre defined discrete time step in an output file the list lst file 5 3 model implementation in the following we describe how our methodology described in section 3 is implemented for two different hms namely swat and modflow most of the steps are similar for both models and to other hms despite how different their software and approach are therefore we focus on demonstrating how users can describe the models following our methodology using different use cases 5 3 1 case studies the location of our study areas and their geographical characteristics are illustrated in fig 5 our case studies focus on two very distinct hydrological systems the naryn river in kyrgyzstan for swat and the barton springs segment of the edwards aquifer in texas for modflow for each case study we emphasize which part of the proposed methodology is similar and where differences occur which mainly concern the exposed inputs and outputs in the respective model configurations the gss such as simulation period and time step have been set by experts for both case studies the target user groups of both cases are non expert analysts and decision makers our intention is to grant the respective users access to the model configurations so that they are able to run alternative scenarios on their own a summary of the case studies can be found in table 3 5 3 2 snow dynamics in the naryn basin case study our first case study focuses on a part of the naryn basin located in kyrgyzstan where high flow occurs mainly in spring and summer due to snow and glacier melt in contrast low flow phases are mostly restricted to the winter season the basin belongs to one of the headwater streams of the syr darya one of the two major tributaries of the aral sea and drains an area of around 50 000 km 2 our case study focuses on the headwaters of the basin which originate in the tianshan mountains snow and glacier melt are of great concern for the local population as it provides water for energy and agriculture unger shayesteh et al 2013 gan et al 2015 the parameters exposed concern snowmelt and snowfall a full list is provided in table 3 the choice is based on preliminary investigations that comprised a comprehensive sensitivity analysis and calibration of our swat model schaffhauser et al 2023 these parameters snowfall temperature snowmelt temperature maximum and minimum melt rate proved to be among the most sensitive ones providing a reasonable model performance the case study represents an example where the model is intended to be used by local authorities our configuration provides an example of an abstraction that can be used by both non experts and more experienced users in this case the non experts will be decision makers in an agricultural agency while the more advanced users will be in the local water authority who will have a broader expertise in water related questions the model component shall finally be used by these decision makers to examine the effects of changes in snow processes on streamflow snow processes constitute the dominant source of water and serves as a proxy of water availability in spring for the region an exploration of the timing and amount of snowmelt provides decision makers with valuable insights on the available water for different sectors such as agriculture or energy this information is important in many aspects for example authorities can deduce how much water is expected to be available for agriculture this enables an estimate of the expected yield within the crop season one of the major economic factors for the region in addition this water is required to be stored for energy production of the whole country besides the period is prone to floods frequently causing at least local threats by having model outputs of water availability decision makers can allocate water to different purposes advance or delay planting dates and generally prepare for the specific seasonal requirements such as energy or irrigation demand accordingly we share the model configuration to enable these users to adjust the snow related parameters namely snowmelt temperature snow fall temperature as well as the minimum and maximum snowmelt factors users can then explore their own scenarios and monitor the actual conditions of the basin to assess which of their scenarios correspond to the actual conditions of the current season to provide some initial scenarios we provide a set of default values for all snow parameters to provide users with a starting point as the response variable of interest for the end user is discharge only the corresponding output file is exposed in our component for simplicity we decided to predefine all input files so that users cannot make any changes 5 3 3 drought impact on the water budget in barton springs case study the second study refers to the edwards aquifer and more precisely the barton springs segment in austin texas a region increasingly affected by droughts passarello et al 2012 2014 a numerical simulation using the modflow model was developed for use as a groundwater availability model gam in the state of texas scanlon et al 2001 2003 the modflow configuration was prepared as part of a state wide planning activity the components underwent a scientific vetting process to assess groundwater availability the intended end users are groundwater managers for state designated management districts as well as stakeholders involved in the recurring groundwater aquifer management program of the state of texas they are not hydrology experts necessarily although they have expertise in groundwater water availability fluctuates rapidly in the region due to normal variability in weather and climate conditions as urban areas have expanded in the past decade water consumption has increased and habitats for vulnerable species are at greater risk for impact during dry conditions table 3 shows an overview of the models we created a model component m b that reflects a baseline model for drought conditions with default pumping rates we created a separate component m a for average conditions also with default pumping rates m b was explicitly designed to investigate and emphasize potential adverse effects of pumping under dry conditions in contrast m a shows the impacts of similar pumping conditions under normal non drought conditions we also created a third component m i where the user can specify infiltrated water as a recharge input file and pumping rates as a wells input file the components are designed to expose key model outputs concerning water table levels hds output file representing hydraulic head levels storage cbb output file representing volumes and actual pumping rates cbb output file the recharge zones were developed for barton springs gam because it represents a baseline interpretation of groundwater behavior the model is readily accessible the recharge zones were originally completed as part of a groundwater decision support system developed to assess the sustainable yield pierce et al 2006 pierce 2006 5 3 4 model encapsulation the following subsections demonstrate the model encapsulation of each case study a summarized overview of the steps and the differences in the procedure where users have to perform manual adaptions for each case study is shown in table 4 the encapsulation process follows the model preparation steps usually including calibration and validation which are performed by the expert modeler 5 3 4 1 step 1 start new environment an environment has to be created for each model configuration see section 2 3 for case study 1 the modeler would create a single model component focused on the snow processes of swat for case study 2 the modeler chose to create three separate model components one for baseline drought conditions one for baseline average conditions and a third one for analyzing different scenarios in average conditions the modeler starts mic from the command line where he provides the name of the model configuration in our case the names are naryn swat and barton springs modflow 1 to 3 mic automatically creates the folder structure for each model configuration 5 3 4 2 step 2 trace execution dependencies the modeler then does a test run to check if the respective model is installed in a new environment and to trace the execution dependencies then mic is used to trace input and output dependencies through reprozip since mic is a unix based tool the invocation command for swat refers to the unix based execution file which can be downloaded via the swat homepage 4 4 https swat tamu edu software swat executables as for modflow we used the python based flopy tool for the model encapsulation 5 5 https www usgs gov software flopy python package creating running and post processing modflow based models flopy serves as a tool which is used to execute existing modflow based models 5 3 4 3 steps 3 4 expose parameters inputs and outputs for the swat model configuration several snow parameters were exposed which were snowfall temperature snowmelt temperature and the maximum and minimum melt factors the parameter selection was based on a preliminary study done by the modeler with relevant stakeholders to identify the dominant parameters see also table 3 each parameter exposed must be manually specified in mic as described in section 4 subsequently the parameters must be indicated in the corresponding swat input files as shown in fig 2 adjustments of default parameter values are possible during this step as well next the modeler declares the input files that contain the exposed parameters as configuration files since all snow parameters of swat are stored in the basin file basin bsn it is the only configuration file relevant to the model configuration the users in the naryn case study such as authorities related to the agricultural energy or water sector do not need all the output files so only the output rch file is exposed as it contains all required information on streamflow within the basin for the configurations of the modflow model in case study 2 no parameters were exposed for the drought model component only the hds and lst input files were exposed where the relevant information of the hydraulic head and the water budget can be specified by users 5 3 4 4 step 5 create wrapper script mic helps wrap model configurations by taking into account the execution settings and prepares the files to test the model components the test runs done by mic were based on the default parameter settings defined in the previous step and double checked manually after the test run the model configuration was finalized and ready for upload 5 3 4 5 step 6 model upload finally mic uploads the model configurations to relevant repositories the docker image of the model component was uploaded to dockerhub 6 6 https hub docker com r mosorio naryn nival setup tags 7 7 https hub docker com r mintproject modflow 2005 tags a github repository containing the input data and results was also created 8 8 components are archived in zenodo https zenodo org record 6948339 yue0vhzbymq finally an entry in the mint model catalog was created 9 9 https mint isi edu kyrgyzstan models explore swat 8cc84426 d849 471b 9a5e 47bcaf094607 6a36a2e5 73bf 4098 9acd 1aaaab383d4a 14580635 c7ca 4256 935a 4ddbdacfbfe2 10 10 https mint isi edu texas models explore modflow modflow 2005 modflow 2005 cfg modflow 2005 bartonsprings avg and the model can be easily run from the mint user interface 6 scenario exploration by non expert users with new model configurations this section describes how users can access the newly created model configurations of the two case studies it highlights how users can easily specify simulation scenarios using the model configurations 6 1 accessing model components users can browse all model configurations for example by bringing up the corresponding regions kyrgyzstan and texas or browsing entries in the mint model catalog typically a user starts in the use models tab and specifies a problem statement by selecting a time period for the simulation a region of interest and desired response variables i e simulation outputs once the problem statement is specified mint will show the user relevant model configurations that can be run fig 6 shows the mint user interface to access model configurations in the different regions more details are provided in the next section 6 2 model dataset parameter selection the naryn case study aims to simulate discharge by adjusting the snow parameters that govern the predominant processes in the region in detail these processes involve snowmelt and snowfall and therefore the snowpack distribution in the region these processes control discharge generation thus a task was created where river discharge was used as response variable as shown in fig 7 a it would also be possible to use other models to obtain discharge such as topoflow peckham 2009 a similar overview for the barton springs case study is provided in fig 8 multiple model components may be selected which would allow users to easily create a model ensemble to provide a more differentiated picture at this point the user would choose among all available input datasets such as meteorological information with precipitation and temperature an example can be found in fig a 1 where users can choose between two alternative well files with different pumping rates for the barton springs area which would allow them to evaluate the effects on the groundwater levels under various pumping rates users can also easily specify different parameter values to reflect different scenarios or assumptions as described in section 5 3 2 four snow parameters were exposed and the user can assign them different values to explore different scenarios as shown in fig 7 b this allows users to explore different dates for the onset of the snowmelt season represented by various temperature thresholds if users provide several values mint runs the model multiple times all outputs are provided individually for each run in the parameter tab fig 7 b users can also check the default values of each parameter this is particularly relevant if the default values are based on expert knowledge and refer to a specific baseline which can be used for the comparison with developed scenarios the static design of the barton springs components does not allow for any parameter changes thus the corresponding tab is empty see fig 8 b 6 3 execute analyze model components when users set parameter values the model component can be executed and the results tab in mint provides a summary of all the outputs in the naryn use case the outputs were limited to the output rch file where the discharge information is stored the results represent the discharge response to the adjusted snow dynamics in the basin which were compared with the baseline simulation results obtained with the default parameter values no shift for every parameter combination a model run was executed and the corresponding outputs were generated an example is shown in fig a 2 where the user specified two values for each of the four parameters which led to 16 combinations each resulting in an execution and each with its output files to eliminate unnecessary computations mint caches the results of executions mint can generate some standard visualizations but users often generate their own custom visualizations after downloading the results in a post processing step we show a visualization of the results from the swat case study comparing a particular scenario versus the baseline in fig 9 users can now directly derive the desired information depending on how strong the average shift in river discharge would be under changing snow conditions the results shown in the figure represent the mean of a five year period from the plot it becomes apparent that the conditions represented by the scenario would lead to a strong rise in discharge in early march already besides the earlier onset and steep rise of snowmelt would cause higher discharge in april and may compared to the baseline while it would be reduced during the summer months the annual peak would already occur 1 5 2 months earlier than in the baseline this significant change in the flow regime may have far reaching consequences for the water sector for example the summer reduction may affect agricultural production as irrigation water is missing while the strong increase after the winter months may promote the damage potential of flood events ultimately water authorities may conclude to assess the potential of a reservoir to mitigate those undesired effects for the second case study we declared a problem statement where we included the different barton springs model components to access the model configurations the study area in mint has first to be changed to texas analogously to fig 6 where we can then select barton springs modflow users can create tasks which reflect a specific scenario and select an appropriate model configuration one of the scenarios may focus on drought assessment using the m b configuration another task may be for average conditions using the m a configuration a third task may focus on the impacts of specific recharge and pumping conditions using the m i configuration users have the possibility to compare the three different setups and easily analyze the differences in groundwater availability in the region in detail one can evaluate the effects of pumping on groundwater levels and study how the aquifer should be managed to maintain flow under specific conditions users might infer that under drought conditions pumping alone is not sufficient in contrast to the swat model configuration where only one output file is accessible the modflow model configurations offer four different output files it is worth noting that the application of the scenarios does not require any computing programming skills for users however if users want to run encapsulated models locally basic container skills are required in general this is seldom the case since mint relies on user friendly gui figs 6 and 8 7 discussion models created by experts are usually difficult to use by modelers in other disciplines despite the need by decision makers to access sophisticated models they remain inaccessible to non experts bagstad et al 2013 even experts within a discipline find that it takes significant effort to setup and compare models from other modelers lüke and hack 2018 francesconi et al 2016 our work shows that two very different hydrological models could be encapsulated using the same methodology to simplify model dissemination by experts for use by non experts our mic tool can be used by expert modelers without major knowledge of software engineering e g using software containers managing execution dependencies or setting up code repositories we demonstrated the methodology for different model domains purposes technical details and model structures our case studies illustrate that modelers only have to determine the parameters and input and output files to be exposed according to the intended scenarios different uses of a model e g snow related analysis or studies focusing on crop yield lead to different model configurations and are organized and easily accessible in mint the methodology enables expert modelers to create useful abstractions of existing models the abstraction hides the part of the model complexity that is not necessarily required for the target users therefore once a model has been encapsulated with our methodology non expert users are relieved from dealing with the technical details of the model execution or its structure different types of non experts may benefit from our effort depending on their expertise and background for example citizens of hydrological extremes drought and floods who become relevant stakeholders and develop a certain level of expertise to understand their own scenarios ngo members who are interested in model applications in the environmental sector or decision makers who usually have a decent hydrological know how but may not be familiar with modeling water authorities are often busy with administrative work which means that there is little time for the construction and calibration of complex models additionally we envision expert modelers to benefit from this effort as it facilitates the creation of model ensembles for model comparisons or for benchmarking our methodology may be used to share and use pre agreed scenarios as in our barton springs case study and support users developing their own scenarios independently by modifying the exposed parameters we also included the possibility of exposing input datasets in model configurations so users can select their own for example several meteorological data sets may be used for the execution of a model configuration processing all required input data is time consuming and hms often have different requirements exchanging these data often represents an obstacle gardner et al 2018 that can be at least partially overcome by using mint modelers are also encouraged to describe their configurations with metadata so that users can search flexibly for models and use those that are suitable for their scenarios a region specific search which corresponds to kyrgyzstan or texas in our examples allows users finding all available models for that region modelers should also provide code for output visualizations see section 6 3 the integration of a general visualization environment in mint would facilitate the usability in extended scenarios for example by integrating other datasets that may be relevant to the modeling scenarios e g population density road access etc although the examples of this paper focus on hydrological models our methodology has been applied to models in other domains including agriculture and economics we assume all encapsulated models to be open source or have an open source executable that can be shared in a software container this methodology helps aligning a software component with the findable accessible interoperable and reusable principles fair for data wilkinson et al 2016 following current best practices for open science by creating software components that have specific functionality and clear invocation and results modelers provide self contained and pre prepared model components that are well characterized and become easier to reuse than the original modeling software model components are more accessible than the original modeling software as they are encapsulated in a software container that can be executed in any platform model components include extensive machine readable software metadata that makes them more findable and interoperable finally it is worth noting that we used pre calibrated models for our case studies future work will address this limitation by integrating model calibration capabilities into our framework and methodology 8 conclusions this paper introduced a methodology to simplify the dissemination of expert models to non expert users the methodology guides modeling experts when creating software components that explore specific modeling scenarios the methodology is applicable to any kind of model regardless of its discipline processes or technical details the implementation of the methodology in the mic tool enables a simple model encapsulation process for modelers this does not only facilitate model dissemination and provision but can also improve mutual work within or across disciplines and groups in addition the complexity of the model can be simplified by creating model configurations that suit the needs of non expert users our proposed methodology thus creates new possibilities in model abstractions and promotes the satisfaction of end user needs this is also supported by the easy access options of model configurations in mint which greatly simplifies their re use we illustrated our methodology with two case studies using two different hydrological models in two different regions of the world the case studies provide examples how potential scenarios and use cases for the application of the methodology could look like however the universal applicability of the methodology within any modeling discipline enables a free design of scenarios with numerous potential use cases that can help both the expert modeler as well as the end user mint users can easily compare the effects of pumpage under different conditions on groundwater levels moreover they can infer whether pumping is suitable to maintain flow under drought conditions or if additional measures should be taken into account additionally we showed how a restriction of the parameter space to a useful minimum can facilitate the exploration of discharge shifts by decision makers the methodology encourages the possibility of independently investigating scenarios and to derive valuable insights for example resulting discharge shifts may lead to several consequences for the water sector e g increased flood risk or decreased agricultural production to mention only two out of dozens that call for action our work supports the fair principles helping model components to be more findable accessible interoperable and reusable however our methodology also presents some limitations which are part of our future work for example while our methodology helps non experts executing models created by expert modelers some expertise is still needed to interpret the results of the simulations in some cases this is addressed by adding documentation and metadata in the scenario in order to provide the right context for end users in other cases expert modelers include ad hoc visualizations that are executed with the model itself helping to interpret the outputs extending our methodology to ensure that visualization components are described for each model output would help address this issue we are also exploring extending mint with general purpose visualizations e g variables obtained in tabular model results another point of improvement involves expanding the supported actions for modeling experts in mint for example including additional data transformations and model calibration right now models are calibrated by experts independently finally additional case studies in other domains are part of our future work in order to further refine the applicability of our approach when disseminating models across disciplines lowering the barrier of adoption of models by modeling experts software and data availability name of the software model component 1 snow dynamics developer timo schaffhauser t schaffhauser tum de maximiliano osorio mosorio isi edu software availability https hub docker com r mosorio naryn nival setup tags docker image compressed size 286 97 mb docker image name of the software model component 2 drought impact developer suzanne pierce spierce tacc utexas edu maximiliano osorio mosorio isi edu software availability https hub docker com r mintproject modflow 2005 tags docker image compressed size 733 55 mb docker image name of the software model insertion checker mic developer maximiliano osorio mosorio isi edu software availability https zenodo org record 6024985 yvpflnzbymo programming language python compressed size 19 9 mb name of the dataset swat modflow model components developer timo schaffhauser t schaffhauser tum de daniel garijo maximiliano osorio daniel bittner suzanne pierce hernan vargas markus disse yolanda gil data availability https zenodo org record 6948339 yvj6v3zbymr form of repository zenodo archive compressed size 51 7 mb further access to the model components is possible via https mint isi edu kyrgyzstan models explore swat 8cc84426 d849 471b 9a5e 47bcaf094607 6a36a2e5 73bf 4098 9acd 1aaaab383d4a 14580635 c7ca 4256 935a 4ddbdacfbfe2and https mint isi edu texas models explore modflow modflow 2005 modflow 2005 cfg modflow 2005 bartonsprings avg declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments we would like to thank our collaborators in the mint project particularly modeling experts and users that contributed to the design of the overall mint framework we gratefully acknowledge support from the us defense advanced research projects agency through award w911nf 18 1 0027 and the us office of naval research through award n00014 21 1 2437 the authors from tum also want to thank the bmbf bundesministerium für bildung und forschung for the funding of the oekoflussplan project grant number 01lz1802b the author from upm has been supported by the madrid government comunidad de madrid spain under the multiannual agreement with universidad politécnica de madrid in the line support for r d projects for beatriz galindo researchers in the context of the v pricit regional programme of research and technological innovation and the call research grants for young investigators from universidad politécnica de madrid appendix a 1 see fig a 1 a 2 see fig a 2 
25406,hydrological modelling efforts tend to ignore the impacts of lakes or explicitly simulate the behavior of only the largest lakes in a watershed as deriving information required to explicitly represent thousands of lakes is difficult we introduce an open source gis toolbox basinmaker that can efficiently build vector based hydrological routing networks including an arbitrary number of rivers and lakes with attributes e g network topology subbasin and lake geometry channel characteristics that provide the inputs required for hydrological routing models basinmaker functionality is demonstrated to build two high resolution vector based lake river routing products each defining a collection of routing networks across large regions the north american lake river routing and ontario lake river routing products each includes all lakes over 10 ha identified in the hydrolakes dataset basinmaker is unique in terms of lake representation and is especially helpful for modelers who need to explicitly represent numerous lakes in their watershed simulation models keywords lakes rivers routing network vector based routing gis toolbox network delineation software and data availability software name basinmaker developer hydrology research group at the university of waterloo program language python 3 year first available 2020 license open source under the gnu general public license v3 0 software availability http hydrology uwaterloo ca basinmaker index html data availability north american lake river routing product v2 1 basinmaker application example https zenodo org record 4728185 http hydrology uwaterloo ca basinmaker index html ontario lake river routing product v1 basinmaker application example https doi org 10 5281 zenodo 6536085 https lake river routing products uwaterloo hub arcgis com 1 introduction lakes play a critical role in the regional scale water balance first they can retain runoff from the upstream catchment and release water later to the downstream rivers which influences the timing and volume of runoff that is transmitted downstream bowling and lettenmaier 2010 huziy and sushama 2017 spence 2000 woo and mielko 2007 zajac et al 2017 further the active contributing area or the flow connectivity of the watershed is often controlled by lakes costa et al 2020 woo and mielko 2007 moreover the interactions between lakes and groundwater are key components in hydrological processes that can affect both water quantity and quality hughes et al 2014 spence 2006 therefore considering lakes as a component in hydrological models can increase the model s capacity to simulate critical processes and potentially improve model performance golden et al 2016 karan et al 2014 shaw et al 2013 lakes are defined here consistent with hydrolakes messager et al 2016 where they include water bodies that are lakes reservoirs and natural lakes with regulation structure lakes also play a crucial role in biogeochemical processes cheng and basu 2017 analyzed data across 600 sites to show the importance of small water bodies including lakes for retaining nitrogen n and phosphorus p thus regulating their movement to downstream waters harrison et al 2009 reports that small lakes account for 65 of the n removal in all lakes downing 2010 reports that organic carbon burial rates in small lakes exceed those of larger lakes by more than an order of magnitude to build an environmental simulation model of any watershed that explicitly represents lakes modelers need to define the lake characteristics and build the lake river routing structure evenson et al 2015 2016 han et al 2020 the lake river routing structure defines the connectivity between the lake river and its surrounding landscape the hydrological model requires the lake characteristics to calculate lake outflow evaporation and seepage at a given lake level necessary lake characteristics typically include the location and extent of the lake the crest height of the lake outlet and a relationship that defines the lake surface area and volume as a function of lake depth a detailed lake characterization may require lake bathymetry which is unavailable for most lakes several global lake databases such as hydrolakes messager et al 2016 and globathy khazaei et al 2022 provide the estimated location and extent of the lake lake area lake volume and averaged depth for lakes with a lake area larger than 10 ha 0 1 km2 with information from these databases missing lake characteristics such as crest height may be estimated with some assumptions for example han et al 2020 assume that lakes are prismatic which means that the lake area does not change with lake depth and the lake crest elevation relative to the lake bottom is the same as the lake averaged depth in hydrological models lakes are represented as a part of the routing structure lehner and grill 2013 the routing structure is generally used to formally represent water flow pathways from land to rivers lakes and eventually converging to the watershed outlet it is a topologically consistent system of river networks and local contributing areas of lakes and river channels e g subbasins and the connections between them typically in the form of a hierarchical network here a watershed routing structure only defines how the subbasins river channels and lakes in the watershed are connected hydrological routing simulation e g the muskingum or diffusive wave approach using such a routing structure also requires routing relevant parameters e g main channel and floodplain manning s coefficient and attributes e g lake area and river channel slope throughout this paper we consider a hydrological routing network to be the combination of routing structure topology plus the corresponding routing attributes of the lakes channels and subbasins necessary for running a hydrological routing model e g a subcomponent of spatially distributed hydrologic models we also define a collection of routing networks covering a large geographic region and delineated from the same source dem and lake polygons as a routing product as used here the term subbasin is consistent for instance with the definition of a catchment in the waterml2 data model blodgett et al 2021 in that catchments can fully partition the landscape each subbasin may contain a main flowpath and subbasins may be aggregated into a drainage basin watershed the entire contributing area for any single subbasin outlet a routing structure and hence a hydrological routing network can be represented in a gridded approach gaborit et al 2017 miller et al 1994 or vector based approach lin et al 2018 mizukami et al 2016 thober et al 2019 yamazaki et al 2013 when we derive the routing network with the gridded and vector based approaches using the same dem the vector based routing approach can accurately represent the drainage area boundaries and location of hydrological features with fewer computational objects with fewer computational objects the vector based approach is computationally more efficient lin et al 2018 yamazaki et al 2013 gharari et al 2020 also call for land surface modelers to adopt flexible vector based spatial configurations to better model at the scale of interest hydrological components in a vector based routing structure can be well represented by gis based features such as polygons and polylines and the topology relationships between these gis features gis features for a vector based routing structure without lakes can be readily generated from a digital elevation model dem using various gis tools such as arcswat winchell et al 2007 archydro maidment 2002 tatoo mitterer 2022 taudem tarboton 2015 or other hydrology toolsets within gis platforms esri 2012 neteler et al 2012 in the resultant stream only routing structures water is transmitted from one subbasin to another and the flow pathways are simulated as converging from land to rivers and then flowing to the outlet however it is more complicated to produce a vector based routing structure explicitly including lake geometry a lake river routing structure because we need to delineate the lake s upstream contributing area and build the topology between lakes and river channels in the network evenson et al 2015 2016 han et al 2020 wang et al 2021 generally the approaches to generating a lake river vector based routing structure can be classified into three categories first the lake could be simply represented as a node on the delineated routing network whose geometry exists outside of the network li et al 2022 vanderkelen et al 2022 this approach introduces errors in the network due to the fact it does not distinguish the local lake drainage area along the shoreline of the lake from the drainage areas of the upstream delineated river channels defining an inlet for the lake without explicitly incorporating the lake geometry into the delineated routing network the local lake drainage area lake subbasin is unknown this error could be significant in upstream subbasins with lakes and the error will increase as the resolution of the routing network is decreased less lake inflow from upstream river channels more inflow from the local lake subbasin moreover this approach is unable to represent lakes that are not located on the delineated river channels as such the number of unique lakes that can be independently represented in the routing network is reduced when the resolution of the routing network is reduced second a lake river vector based routing structure can be generated by collectively overlaying the lake shoreline polygon onto the subbasin polygons of a vector based routing structure without lakes and then updating the properties of the newly derived polygons and the new network topology molina navarro et al 2018 as noted by the molina navarro et al 2018 the limitation of this approach includes 1 many small polygons around the lakes would be generated after the overlay operation and these small polygons are data redundancies rather than true subbasins molina navarro et al 2018 2 the drainage area of the lake is not accurately represented lehner and grill 2013 third a vector based lake river routing structure can be alternatively generated by directly modifying the flow direction dataset the flow direction dataset is a raster dataset derived from a digital elevation model dem indicating the flow direction of each dem grid cell in this approach the flow direction dataset is manipulated to ensure that water in grid cells that are covered by the lake will drain to the closest derived river channels under the lake maidment 2002 the result of this manipulation can divide a lake into multiple separate subbasins based on unreliable dem elevations under the lake potentially leading to more than one lake outflow river channel han et al 2020 avoided the problem by modifying flow direction grids such that water flows to the designated hydrolakes lake subbasin outlet however the approach in han et al 2020 cannot be applied to the high resolution dem due to the excessive computational burden in addition the implementation of han et al 2020 relied on proprietary gis software this paper introduces a new open source gis toolbox basinmaker 3 0 which can efficiently and correctly delineate and conveniently simplify via post processing a vector based hydrological routing network topology plus the corresponding routing attributes in lake dominated watersheds from any hydrologically consistent dem the basinmaker toolbox was initially developed to encapsulate the delineation and attribute estimation methods presented for the pan canadian lake river routing products developed by han et al 2020 but has since evolved to include new and revised delineation methods and greater functionality and flexibility this paper also reports on some of these new and revised delineation methods behind the toolbox results show the flexibility and utility of the toolbox for example basinmaker can alternatively delineate directly from either a user provided dem or flow direction grid can optionally delineate lakes that are not explicitly connected to the delineated river channels and provides unique post processing functions that can simplify without further terrain analysis pre existing higher resolution lake river routing products to our knowledge such a collection of efficient post processing tools for generating and simplifying lake river routing networks has not been reported elsewhere the remainder of this paper is structured as follows section 2 presents the methodology overview how we define a lake river routing network an overview of the basinmaker software to produce such a routing network and some additional detail on methods behind the software section 3 describes the results of the experiments designed to showcase the software utility and flexibility production of large scale of routing products and reports on some comparisons and validation with existing watershed boundary delineations section 4 is the discussion and limitations while section 5 is the conclusions 2 material and methods 2 1 definition of subbasins and lakes subbasins subdivide a watershed into distinct non overlapping units that are topologically connected and indicate how surface water moves from within the watershed to the watershed outlet this basic watershed discretization strategy is common and is used for example in spatially distributed hydrological models such as swat winchell et al 2007 hype lindström et al 2010 and raven craig et al 2020 basinmaker subbasins are specifically defined as below 1 a subbasin is not a watershed drainage basin and instead can be considered a local drainage basin that excludes the upstream watersheds of all subbasin inlet points it could alternatively be defined as the intersection two or more drainage basins 2 the single subbasin outlet defines the point where all water from the subbasin exits including all internally generated runoff and baseflow and flow received from upstream subbasins 3 headwater subbasins have no upstream subbasin 4 endorheic subbasins are terminal and do not drain to a downstream subbasin 5 each lake subbasin contains one and only one lake and can have an unlimited number of inlet points 6 a non lake subbasin includes a single river channel typical or no river channel e g for a headwater subbasins and can have at most only one inlet point often this inlet is at the confluence of two upstream river channels each of which is contained within their own subbasin 7 all subbasins have a single outlet the outlet connects it to a downstream subbasin except in the cases of a endorheic subbasin small headwater subbasins on the coast draining directly to an external water body e g an ocean or the primary outlet of the watershed of interest 8 a subbasin water body either the lake or river channel connects all upstream subbasins to the downstream subbasin a lake is defined as depression storage compartment on the landscape with the spatial extent defined by a single contiguous polygon which only naturally transmits surface water downstream when some threshold storage volume depth is exceeded in contrast a river channel is a flowpath moving water through a subbasin and it has no permanent storage capacity like a lake basinmaker requires users to distinguish lake water bodies from river water bodies and this is done by users providing lake polygons as an input in order to discretize the lake river routing network and thus merge lake polygons with a routing network basinmaker assumes lake polygons represent the maximum areal extent of the lake body the treatment of lakes with temporally variable surface area can be readily handled in hydrological models using a basinmaker routing network if the maximum lake spatial extent is the input to basinmaker as lake contraction will not impact the geometry of a lake s contributing area how to parameterize lakes with highly variable surface areas appropriately within hydrologic models is beyond the scope of this manuscript reservoirs with managed outflows and natural lakes can both be represented as a lake in basinmaker generated lake river routing networks but when such networks are used as the basis of a hydrologic routing model users will want to carefully determine and then characterize how the case study reservoir levels and outflows are managed likewise hydrologic routing model applications should consider if the prismatic lake area assumption should be replaced with a site specific stage storage relationship 2 2 overview of the lake river routing networks produced by basinmaker in this section we introduce the differences between a routing network without considering lakes and a routing network with lakes defined by basinmaker differences in routing networks are reflected in the routing structure defining how subbasins river channels and lakes are connected an example vector based routing structure without considering lakes is shown in fig 1 a subbasins in such a routing structure are only determined by river channels and subbasin outlets are defined by control points control point locations can be based on river channel junctions points of interest e g monitoring locations or flow accumulation thresholds an example of a routing structure with lakes is shown in fig 1b for the same watershed depicted in fig 1a a lake subbasin is defined as in han et al 2020 by the following rules 1 the extent of a lake subbasin will fully cover the lake 2 a lake outlet defines the outlet of the lake subbasin 3 each of a lake s significant inlets i e those associated with rivers above a specified flow accumulation threshold is defined by an upstream subbasin outlet in this way both inflow and outflow of each lake can be explicitly simulated by hydrological routing models lakes are divided into two categories by basinmaker 1 connected lakes cl which are lakes with an outlet that is explicitly connected to a downstream non zero length river channel in the routing structure and 2 non connected lakes ncl which denote lakes that are defined to drain to the downstream routing network but do not flow through any explicit river channel see small yellow subbasin in fig 1b by default both cl and ncl within a watershed are considered to be contributing areas of the watershed though this assumption can be relaxed i e the area can be disconnected from the rest of the network for ncls when appropriate the only difference between cl and ncl is that a cl always drains into an explicitly represented river channel that is connected to the lake outlet while a ncl does not ncls can exist because the user defined minimum drainage area flow accumulation threshold for defining starting a new river channel can sometimes be larger than the drainage area of smaller lakes particularly for headwater lakes in this way basinmaker separately distinguishes ncls and enables users to optionally define them separately from the cls relative to the lake river routing product defined in han et al 2020 the use of non connected lake subbasins in basinmaker is new the control points shown in fig 1 define the location of a subbasin outlet and for any non lake subbasin the basinmaker delineation strategy described in section 2 2 determines the control point locations in a way that favors the creation of fewer subbasins the proposed routing structure generated with basinmaker describes the watershed routing structure connectedness and flow directions of the component subbasins river channels and lakes the watershed routing structure defines how routing components in the watershed are connected in order to simulate hydrological routing through such a structure additional component attributes physical characteristics and parameters are required these are estimated and output by basinmaker the additional component attributes produced by basinmaker can be categorized into four groups the attributes of the lake attributes of the river channel the attributes of subbasins and the attributes of points of interest the points of interest define the desired locations of subbasin outlets in the delineated routing network and would generally include various monitoring locations and potentially other case study dependent locations lakes in basinmaker are assumed by default to be prismatic with a broad crested weir as the outlet structure and as such basinmaker reports the lake area lake depth and crest width for the lake outlet basinmaker also includes an attribute indicating whether the subbasin has one cl one ncl or no lake with a maximum of one river channel per subbasin basinmaker derives the river channel length cross section profile slope and both the main channel and floodplain manning s coefficients for the subbasin attributes basinmaker includes the subbasin area and the average subbasin slope the methodology to calculate each of these attributes is described in section 2 2 see function nd5 the relation between these various geometric entities as used within the basinmaker toolkit are depicted in the feature diagram of fig 2 2 3 overview of basinmaker functions basinmaker is a gis toolbox written in the python 3 programming language basinmaker functions all rely on geoprocessing functions from the gis environment it is configured to use users can apply basinmaker post processing mode functions in either arcgis pro or google colab which uses geopandas gis environment created by jordahl et al 2020 environments basinmaker delineation mode functions rely on both qgis and grass gis environments although not precisely the same in each gis environment basinmaker light post processing mode uses equivalent geoprocessing functions from geopandas jordahl et al 2020 qgis or arcgis pro 2 3 1 overview of basinmaker functions in network delineation mode fig 3 describes the workflow associated with the five key network delineation functions in basinmaker applied sequentially to generate a routing network from a hydrologically conditioned dem and optional inputs including lake polygons flow direction raster datasets landuse raster datasets bankfull width and depth of channel polyline datasets and a dataset of additional points of interest the define project spatial extent function generates the spatial extent to be processed of the current project basinmaker will clip all input data with this spatial extent and delineate lake river routing network s within this spatial extent the define project spatial extent function includes several options which allow the user to directly specify the spatial extent of the input dem as the spatial extent of the project or alternatively determine the spatial extent of the project from a watershed outlet latitude and longitude the delineation initial subbasins without lakes function generates an initial routing structure without considering lakes fig 1a the add new subbasin outlet points function can be used to add lake inlets lake outlets or other points of interest as new subbasin outlets into the initial routing structure see fig 4 for an example result the generate hydrologic routing attributes function updates geometry attributes and estimates hydrological parameters for the subbasins lakes and rivers generated previously from the add new subbasin outlet points function appendix a lists all attributes generated by the generate hydrologic routing attributes function the output of the generate hydrologic routing attributes function is only an intermediate hydrological routing network since it still represents some lakes being located within multiple subbasins fig 4 the function combine subbasins covered by the same lake generates the finalized hydrological routing network fig 1b note that depending on the user specified option for the first define project spatial extent function the resulting routing network may not be a single watershed and instead may include a number of subbasins that drain to a location external to the chosen dem extents the post processing mode function select subregion of routing structure pp3 in table 1 below can be applied to focus the delineated network on a single watershed and or strip away small watersheds along the boundary of the original spatial extent we refer to section 2 3 1 and appendix b1 for more details of the above briefly explained functions the finalized hydrological routing network output of basinmaker network delineation mode includes the following esri standard shapefiles a lake polygons fig 1b b points of interest c subbasin polygons and river polylines for intermediate hydrological routing network output of the function generate hydrologic routing attributes shown in fig 4 and d subbasin polygons and river polylines for the finalized hydrological routing network fig 1b we will refer to gis files from a to d as basinmaker delineation mode outputs in the following section the intermediate hydrological routing network outputs of the function generate hydrologic routing attributes are included in basinmaker delineation mode outputs because they are the required inputs of basinmaker post processing functions that simplify a basinmaker delineated hydrological routing network section 2 2 2 any basinmaker produced hydrological routing network in either network delineation mode or network post processing mode is defined by the above four esri standard shapefiles note that the attributes of all four attribute groups are all assigned to subbasin polygon shapefile of the hydrological routing network 2 3 2 overview of basinmaker functions in network post processing mode given a pre existing hydrological routing network delineated by basinmaker users can apply network post processing functions pp1 pp2 and pp3 in table 1 independently to customize and simplify an existing hydrological routing network e g remove detail although the core basinmaker generated routing network data are model agnostic users can apply functions pp4 and pp5 in table 1 sequentially to export routing relevant input files for the raven hydrological modelling framework craig et al 2020 functions pp1 and pp2 are described here in detail while the basinmaker website http hydrology uwaterloo ca basinmaker index html describes functions pp3 pp4 and pp5 the input for functions pp1 and pp2 in table 1 is the intermediate hydrological routing network fig 5 a in basinmaker delineation mode outputs note the intermediate routing network gis files not the finalized network gis files are the required inputs here they include the information required to properly simplify and thus recalculate attributes for a lower resolution routing network the output of simplifying function pp1 fig 5c and d and function pp2 fig 5e and f for the simplified post processed network are equivalent to basinmaker delineation mode outputs defined in section 2 2 1 the remove small lakes function removes lakes from the input network based on lake area thresholds for connected lakes and non connected lakes fig 5c and d demonstrate the outputs from the function remove small lakes using the routing network shown in fig 5a as input the decrease river network resolution function increases the size of subbasins and reduces the routing network s resolution from the input when subjected to a larger drainage area threshold subbasins with their drainage area smaller than this threshold are merged with their downstream subbasins fig 5e and f demonstrate the outputs from the function decrease river network resolution using the routing network shown in fig 5a as input we refer to appendix b for more details on the briefly introduced functions that can be used in network post processing mode 2 4 summary of methods for basinmaker functions this section is a high level summary of the methods encapsulated in key basinmaker functions detailed description of the methods for basinmaker functions are found in 2 4 and appendix b 2 4 1 basinmaker network delineation methods once the spatial extent of the project is defined basinmaker network delineation mode requires four steps to produce a hydrological routing network from a hydrologically consistent digital dem these steps are functions nd2 through nd5 in fig 3 we will summarize the methods applied by function nd3 and nd4 in the manuscript while the detailed method applied within other functions are detailed further in appendix b1 function nd2 in fig 3 will create an initial routing network and routing structure without considering lakes using input dem or flow direction the result of function nd2 were shown in fig 6 a given a set of lake polygons and an optional set of points of interest function nd3 in fig 3 will add lake inlets outlets the initiation points of select headwater river segments and points of interest as new control points new subbasin outlets into the intermediate routing network an updated routing structure including subbasin polygons will be generated based on the updated control points and the flow direction map from function nd2 in fig 3 as shown in fig 6b after that function nd3 will identify lakes for which part of the lake area did not belong to the intermediate lake outlet contributing area fig 6b and will correct the flow direction of cells within the identified lakes as shown in fig 6b the correction needed is to ensure the water in the left part of the lake flows to the lake outlet the correction is achieved with a minimum of processing time by modifying the flow direction of only the lake boundary cells for example any of the cells red boundary cells in fig 6c with their flow directions not pointing towards the three right partial lake subbasins referred to as the un converged boundary cells ubcs are corrected next function nd4 in fig 3 estimates physical attributes for each subbasin river reach lake and point of interest shown in figure b1c appendix a lists all attributes added by this function most of these attributes are estimated using the methods described in han et al 2020 except for bankfull width w in m bankfull depth d in m and bankfull discharge q in m3 s the rest of the attributes are estimated using the same methods as described in han et al 2020 andreadis et al 2013 use the following two equations to calculate the w and d of each subbasin w 7 2q 0 5 and d 0 27q 0 39 respectively they estimated q using a power law relationship between q and the drainage area da in km2 q k da c thus to calculate the w and d of each subbasin we need to assign k and c for each subbasin which will be referred to as the channel profile coefficients two options are available the user can provide constant channel profile coefficients for all subbasins or the user can provide a channel profile coefficient polygon gis layer each channel profile coefficient zone in the channel profile coefficient layer has a unique pair of k and c values this function will use the k and c values from the channel profile coefficient zone with the largest intersecting area with each subbasin if a sufficiently large sample of local measurements bankfull width bankfull depth and discharge are available the user could use these local measurements to estimate k and c when local measurements are unavailable users could utilize the bankfull width bankfull depth and discharge provided in andreadis et al 2013 to determine k and c for a specific watershed finally function nd5 in fig 3 merges the three partial lake subbasins in fig 6c and generates one lake subbasin completely containing the lake fig 6d 2 4 2 basinmaker post processing methods although basinmaker provides five post processing gis functions table 1 this section introduces the summary of methodologies behind the two functions used for network simplification 1 remove small lakes and 2 decrease river network resolution we focus on these functions because these are unique in basinmaker compared to other software tools both post processing functions are used to modify the intermediate hydrological routing networks generated by basinmaker in delineation mode fig 5a in general both network simplification functions will efficiently create a new routing network as showed in figs 5c and 4d by applying the following steps more detailed description is in appendix b2 1 identify the subbasins that needs to be merged based on user provided threshold 2 from upstream to downstream merge subbasins of the intermediate hydrological routing networks fig 5a when the subbasins meet certain conditions 3 update the attribute values of merged subbasins like the function nd5 described in section 2 2 1 attributes of merged subbasins are carefully merged into a single attribute value for the various attribute groups for example select river channel properties are merged using channel length weighted averaging and select subbasins attributes are merged using subbasin area weighted averaging 4 assign existing subbasin id values to the merged subbasins merged subbasins are all defined by subbasin outlet locations that were pre existing in the input hydrological routing network as such the subbasin id for a merged subbasin is assigned as the subbasin id from the input network with the same subbasin outlet location this means that once basinmaker initially assigns a subbasin id in network delineation mode e g the subbasin id whose outlet is associated with a streamflow gauging station for all simplified network resolutions derived from the original most detailed delineation subbasins draining to the same outlet location e g a streamflow gauge are always assigned the same subbasin id value 2 5 summary of basinmaker delineation experiments and analyses in order to demonstrate and test the basinmaker software we designed a number of experiments and analyses these function to demonstrate the flexibility and inherent efficiency of the delineation software as well as validate the delineated networks against pre existing delineated watersheds these are reported on in the subsequent results section in the order they are defined here basinmaker application 1 delineates a large scale lake river routing product covering north america the core input data for producing the north american lake river routing product v2 1 han et al 2021 includes the merit dem yamazaki et al 2017 which is 3 arcsecond resolution 90 m at the equator hydrolakes messager et al 2016 databases for lake polygons and points of interest comprising of 7791 streamflow gauges from water survey of canada wsc and 28 164 streamflow gauge from united states geological survey usgs secondary input data and delineation settings for the north american product are detailed in appendix c an example watershed is used to demonstrate select basinmaker post processing functions basinmaker application 2 delineates a large scale lake river routing product covering the province of ontario canada an area of approximately 1 million km2 the core input data for producing the ontario lake and river routing product v1 0 han et al 2022 includes the enhanced flow direction fdr raster layer 30 m 30 m from the ontario integrated hydrology oih dataset hydrolakes messager et al 2016 databases and points of interest including 1539 streamflow plus water quality gauge locations and 1981 subbasin outlets extracted from the quaternary ontario watershed boundaries in the oih dataset provincial mapping unit mapping and information resources branch 2022 unlike the north american product application this application delineates the drainage network from an enhanced fdr enhanced to reflect known watercourse drainage patterns instead of a dem and includes additional points of interest beyond streamflow gauging stations secondary input data and delineation settings for the ontario product are detailed in appendix c the above delineated networks above are used to assess and validate the basinmaker generated watershed boundaries relative to streamflow gauge drainage areas and datasets reported by streamflow monitoring agencies the area comparison is completed for all gauges in the routing products watershed drainage boundary comparisons are assessed for two locations 3 result and discussions 3 1 basinmaker applications basinmaker software was utilized to build the hydrological routing networks for the great lakes region in the mai et al 2022 model intercomparison project where the raven hydrological modelling framework craig et al 2020 in routing only mode was utilized to route distributed runoff fluxes from six different hydrological simulation models through the network basinmaker has successfully produced vector based lake river routing networks for dem flow direction datasets with the following resolutions 500 m hydroshed dem dataset delineating networks as large as 1 788 586 km2 90 m merit dem delineating networks as large as 324 372 km2 30 m oih flow direction delineating networks as large as 100 228 km2 10 m nhd plus flow direction delineating two networks up to 3159 km2 and a 5 m lidar dem delineating a 0 57 km2 watershed however basinmaker software memory limitations might be exceeded when delineating some networks e g a large watershed with a high resolution dem in such a case users need to break up their region watershed into smaller subregions before proceeding with the delineation we have applied basinmaker across a range of applications to successfully produce vector based lake river routing networks for dem flow direction datasets with the following resolutions 500 m hydroshed dem dataset delineating networks as large as 1 788 586 km2 90 m merit dem delineating networks as large as 324 372 km2 30 m oih flow direction delineating networks as large as 100 228 km2 10 m nhd plus flow direction delineating two networks up to 3159 km2 and a 5 m lidar dem delineating a 0 57 km2 watershed however basinmaker software memory limitations might be exceeded when delineating some networks e g a large watershed with a high resolution dem in such a case users need to break up their region watershed into smaller subregions before proceeding with the delineation in this section we will summarize two large scale basinmaker applications the development of the north american lake river routing product v2 1 section 3 1 1 and the development of the ontario lake river routing product v1 0 section 3 1 2 only the high level summary is provided here and the more detailed development documentation for the two routing products can be found in appendix c 3 1 1 the north american lake river routing product v2 1 the north american lake river routing product na routing product v2 1 dataset is available in han et al 2021 and includes 650 971 lakes in total with areas ranging from 0 1 km2 to 117 326 km2 the na routing product v2 1 does not include coastal watersheds with a drainage area smaller than 8500 km2 attribute values were checked to ensure physical plausibility and the allowable ranges for various attributes are provided in table a1 attribute values that could be measured in the field such as river reach length and slope were not evaluated against field data and hence should be taken as an initial starting point in any analysis or modelling of the delineated network in total the na routing product successfully includes 19 391 points of interest which is 53 of the initially specified points of interest 28 164 usgs and 7791 wsc gauges the 47 that could not be included in the na routing product include 4648 streamflow gauges from wsc and 11 916 streamflow gauges from usgs reasons for non inclusion include gauges in coastal watersheds not included in the product gauges falling within a lake polygon and gauges whose co ordinates were deemed too far from the nearest delineated river channel a map based portal to access gauges or regions of the na routing product in han et al 2021 is found at http hydrology uwaterloo ca basinmaker fig 7 demonstrates an example routing network from the na routing product v2 1 and multiple post processed simplified routing networks produced with basinmaker v3 0 for the approximately 4000 km2 petawawa river watershed in ontario canada 3 1 2 the ontario lake and river routing product v1 0 the ontario lake river routing product has an average subbasin size around 4 5 km2 and the dataset is available in han et al 2022 the product includes in total 82 001 lakes with areas ranging from 0 1 km2 to 4506 km2 the ontario routing product does not include coastal watersheds with a drainage area smaller than 250 km2 attribute values were checked to ensure physical plausibility and the allowable ranges for various attributes are provided in table a1 attribute values were not evaluated against field data and hence should be taken as an initial starting point in any analysis or modelling of the delineated network in total the ontario routing product successfully includes 3226 points of interest 94 of the specified points of interest for inclusion made up of 991 ontario observation gauges from the water survey of canada wsc database 457 active water quality monitoring gauges from the ontario provincial water quality monitoring network pwqmn and 1878 subbasin outlets extracted from the quaternary ontario watershed boundaries unlike the north american product the points of interest inclusion success rate is high for the ontario product because there was a substantial manual effort to try and correctly include these points on the delineated network a map based portal to access gauges or regions of the ontario routing product in han et al 2022 is found at https lake river routing products uwaterloo hub arcgis com 3 2 validation of basinmaker generated watershed boundaries the accuracy of the delineated watershed boundary from basinmaker depends on the accuracy of the input flow direction or digital elevation model grids basinmaker uses to delineate the watershed we evaluated the accuracy of select basinmaker generated watershed boundaries from the merit e g what is used in the north american lake river routing product and oih e g what is used in the ontario lake river routing product datasets from two perspectives first we assessed the accuracy by comparing the drainage area of usgs streamflow gauges in the routing networks generated by basinmaker using each dataset and the usgs reported drainage areas buto and anderson 2020 second we compared the watershed boundary from usgs nhd buto and anderson 2020 dataset taken as accurate with the watershed boundary generated by basinmaker with each flow direction dataset 3 2 1 validation of basinmaker generated watershed boundary using merit dataset in this section we compared the drainage area of usgs streamflow gauges in the north american lake river routing product generated by basinmaker da bm and the drainage area from usgs reports da usgs which we assume is correct the north american routing products include 16 248 usgs streamflow gauges and all of them have a usgs reported drainage area the number of usgs gauges with absolute drainage error smaller than 15 is 15 010 the absolute drainage error of each gauge is defined as 100 da bm da usgs da usgs thus the watershed drainage areas generated by basinmaker using merit datasets yamazaki et al 2019 are consistent with the vast majority of usgs streamflow gauges note that this drainage error metric is an attribute available in the north american product for each of the included 16 428 usgs gauges second we compared the watershed boundaries from the usgs nhd dataset taken as accurate with two basinmaker delineated watersheds a basinmaker using merit flow direction dataset e g what is used in the north american lake river routing product and b basinmaker using the usgs nhd plus h flow direction dataset buto and anderson 2020 these comparisons are made for the mill creek watershed in indiana fig 8 the hu10 code of the mill creek watershed a unique number for each watershed in nhd plus h is 0512010607 the two watershed boundaries generated by basinmaker using two different raster datasets are compared with the watershed boundary polygon of mill creek provided in nhd plus h datasets fig 8a demonstrates the watershed boundary generated by basinmaker using merit dem and the watershed boundary polygon provided in nhd plus h dataset buto and anderson 2020 for the mill creek watershed a large discrepancy is evident between the two watershed boundaries especially in the upstream region of the watershed the discrepancy is due to the errors in the merit datasets the blue grids represent the routing network from the merit datasets yamazaki et al 2019 it was created by coloring all grids with the number of upstream drainage pixels larger than 100 in the number of upstream drainage pixels raster of merit datasets yamazaki et al 2019 some river channels of merit dataset do not drain to the mill creek watershed outlet fig 8a fig 8b demonstrates the watershed boundary generated by basinmaker using flow direction grids of nhd plus h and the watershed boundary polygon provided in nhd plus h dataset for the mill creek watershed only a small discrepancy was observed between the two watershed boundaries in some parts of the watershed fig 8b the discrepancy is due to the input of the flow direction grids in nhd plus h the blue line represents the river polyline from the nhd plus h datasets it is the nhdplusburnlineevent polyline data in nhd plus h datasets some river channels of nhd plus h dataset do not drain to the mill creek watershed outlet fig 8b 3 2 2 validation of basinmaker generated watershed boundary using oih dataset we compared the drainage area of usgs streamflow gauges in the ontario lake river routing product generated by basinmaker da bm and the drainage area from usgs reports da usgs the ontario lake river routing product includes 34 usgs streamflow gauges five of them do not have a reported drainage area in the usgs the number of gauges with absolute drainage error smaller than 15 is 23 out of 29 thus the watershed drainage areas generated by basinmaker using oih datasets are consistent with most usgs streamflow gauges fig 9 demonstrates the watershed boundary generated by basinmaker using oih flow direction dataset and the watershed boundary polygon provided in nhd plus h dataset for the watershed drainage to usgs streamflow gauge 05127000 the watershed boundaries from basinmaker and nhd plus h datasets are very consistent with each other an even better alignment of the wdbs is achieved when basinmaker used the flow direction dataset from nhd plus h results not shown the oih dataset is the authoritative source of watersheds and sub watersheds for the province of ontario provincial mapping unit mapping and information resources branch 2022 the polygons defining various oih secondary to quaternary watersheds were all derived from the same 30 m oih enhanced flow direction grid we used to produce the ontario routing product as such the wdb for any oih quaternary watershed point of interest included in the ontario routing product completely corresponds with the quaternary watershed wdb available from the oih dataset provincial mapping unit mapping and information resources branch 2022 in summary basinmaker could generate the lake river routing network represented in the input dem or flow direction dataset the resulting watershed boundaries from basinmaker are very consistent with watershed boundaries produced by other agencies when the same core flow direction grids are utilized for each delineation exercise however basinmaker could not correct the dem or flow direction errors related to watershed boundaries or routing networks and so basinmaker delineated watershed drainage boundaries wdbs can deviate from existing wdbs that have been delineated using a different core flow direction grid or dem dataset 4 discussion and limitations currently basinmaker cannot correctly handle the non contributing areas present in the source dem unless the source dem is specially pre processed such non contributing areas are prevalent for example in the prairie pothole region of north america in reality the runoff generated in a non contributing area does not drain to any downstream routing network however the default delineated routing network from basinmaker will connect subbasins in the non contributing area to the downstream routing network because of this limitation non contributing areas are not represented in either the ontario lake river routing product or the na routing product a special version of the na routing product is available for the nelson river drainage watershed covering a large part of the prairie pothole region that correctly delineates non contributing areas e g the sink subbasin for each non contributing area is simply assigned a downstream subbasin id of 1 this special version is available upon request unlike for permanent non contributing areas identified as a sink in the dem basinmaker is currently unable to properly include non permanent non contributing areas in the routing network but work is ongoing to do so in one of the next versions of basinmaker at the same time basinmaker can include non riparian wetlands in the routing network in addition to lakes provided the set of water polygons input to basinmaker includes both lake and wetland polygons however basinmaker currently assigns the same waterbody outlet structure characteristics to a wetland and lake and thus if users include wetland waterbodies for inclusion in their basinmaker delineated network they will very carefully need to assess the validity of the wetland outflow assumption a broad crested weir with a crest width derived as a function of wetland drainage area work is ongoing to determine how to uniquely parameterize wetland outflows many routing product attributes generated by basinmaker e g manning s n bankfull width bankfull depth in the north american routing product and the ontario product were not evaluated against field data therefore attributes such as these in current basinmaker routing products as well as attributes estimated in subsequent basinmaker delineated hydrological routing networks should be viewed as initial starting points in any analysis utilizing the routing network where available local data for such attributes will likely prove useful to incorporate in the routing network 5 conclusions an open source gis toolbox called basinmaker that can automatically and efficiently build vector based hydrological routing networks including an arbitrary number of lakes is introduced here basinmaker has two main functions 1 delineate the lake river routing network from a dem and 2 post process an existing lake river routing network generated by basinmaker the routing network from basinmaker includes all the necessary hydrological routing model inputs and can be used to build semi distributed hydrological models discretizing raven craig et al 2020 semi distributed hydrological models or routing only models is particularly efficient given basinmaker also has a post processing function to write raven model specific input files describing the watershed discretization key basinmaker post processing operations on an existing hydrological routing network include a extract a subregion of interest b simplify the lake river routing network by decreasing the resolution of the routing network and thus increasing the size of subbasins or by removing the lakes from it c define hrus to subdivide each subbasin and generate model setup files for the raven hydrological modelling framework craig et al 2020 based on user provided landuse and soil layer polygons these post processing tools are helpful for users to utilize a routing network of interest already delineated by basinmaker such as those networks included in the north american lake river routing product v2 1 available at http hydrology uwaterloo ca basinmaker index html and the ontario lake river routing product v1 0 available at https lake river routing products uwaterloo hub arcgis com with the basinmaker toolbox the hydrological community can now easily create a lake river routing network and explicitly model each lake in their spatially distributed hydrologic model this is important because calibrating a hydrological model that does not explicitly simulate lakes in a lake dominated watershed is likely to compensate for this lack of storage in erroneous ways and model performance at ungauged locations is likely to degrade as a result explicitly and correctly modeling lake inflows and outflows is becoming more important with the coming remote sensed lake level observations from the nasa swot satellite mission see https swot jpl nasa gov which promises to measure lake levels at resolutions temporal spatial and vertical that have the potential to be used to calibrate hydrological models in watersheds that would have previously been considered ungauged for example lakes down to 0 06 km2 in surface area can have monitored lake levels https swot jpl nasa gov resources 135 swot the lake surveyor to properly use such remotely sensed lake water level measurements requires correctly modelling the lake water balance which depends on explicitly defining the lake s contributing area and geometric position within the watershed drainage network basinmaker is therefore a tool enabling the hydrological modelling community to take full advantage of swot monitored lake levels around the world declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements support for basinmaker the north american routing product development and the ontario routing product came from multiple sources primary graduate student support for basinmaker contributors was provided by nrcan canadian forest service g c grants 129677 and 129816 and dr tolson s nserc discovery grant secondary support during preliminary basinmaker software development for first author ming han was provided by the canada first research excellence fund provided to global water futures lake futures project some additional secondary support was also provided via the canarie research software program grant rs3 124 to co author juliane mai support for the ontario routing product development was provided by the ontario ministry of northern development mines natural resources and forestry appendix a table a1 the included attributes in the lake river routing products and networks produced by basinmaker v2 0 and v3 0 the attributes marked with exist only in the ontario lake and river routing network generated with basinmaker v3 0 moreover the attribute marked with i e fral is generated outside of basinmaker v3 0 and only exists in the ontario lake and river routing network table a1 name unit type description and specific values subid integer subbasin catchment id dowsubid integer id of subbasin catchment downstream of current subbasin rivslope m m float slope of river channel in the subbasin it is limited to a range of 0 000001 1 it is 1 2345 for non connected lake subbasin and headwater subbasins since these have no river channels rivlength m float length of river channel in the subbasin it is 1 2345 for non connected lake subbasin and headwater subbasins since these have no river channels basslope degree float averaged slope over the subbasin it is limited to a range of 0 60 basaspect degree float averaged aspect over the subbasin it is counterclockwise from east 90 is north 180 is west 270 is south 360 is east basarea m2 float area of subbasin bkfwidth m float bankfull width of river channel in the subbasin width of river when full if this catchment is a lake catchment and the lake is not a reservoir this should be used as an initial estimate of lake s effective crest width it is limited to a range of 0 1 3000 0 bkfdepth m float bankfull depth river channel in the subbasin depth of river when full it is limited to a range of 0 1 10 0 lake cat integer lake category 0 subbasin without lake 1 subbasin with lake that is directly connected to a downstream river channel 2 subbasin with lake that is not directly connected to a downstream river channel hylakeid integer the lake id in hydrolakes database 0 for subbasins without lakes lakevol m3 float volume of lake in hydrolakes database 0 0 for subbasins without lake lakearea m2 float area of lake in hydrolakes database 0 0 for subbasins without lake laketype integer type of lake assigned by hydrolakes 0 subbasin without lake 1 subbasin with natural lake 2 subbasin with lake that is reservoir 3 subbasin with lake that is natural with regulation has poi integer subbasin has point of interest poi at outlet 0 subbasin has no poi 1 subbasin has poi which comes from the following sources streamflow and or water level gauge station and or water quality station and or subbasin outlets extracted from the quaternary ontario watershed boundaries meanelev m float subbasin average elevation floodp n s m1 3 float averaged flood plain manning s coefficient based on landcover along river channel it is limited to a range of ch n to 1 5 it is 1 2345 for non connected lake subbasin and head water subbasins since these have no river channels and thus no delineated flood plain q mean m3 s float subbasin outlet bankfull discharge estimated it is limited to a range of 0 00001 150000 0 ch n s m1 3 float river channel manning s coefficient it is limited to a range of 0 025 0 15 it is 1 2345 for non connected lake subbasin and head water subbasins since these have no river channels drainarea m2 float drainage area of subbasin outlet strahler integer the strahler order of river channel seg id integer the unique river channel id the subbasin belongs to each river channel can include multiple river channels only useful as an internal variable for basinmaker seg order integer the sequence of the river channel denoting the direction of water flow in the corresponding river channel only useful as an internal variable for basinmaker max dem m float the maximum elevation along the river channel min dem m float the minimum elevation along the river channel da obs km2 float subbasin drainage area reported by institution providing point of interest 0 0 for subbasins without point of interest or if no value is provided da error float relative ratio between drainage area based on routing network drainarea and basin drainage area reported by institution providing point of interest da obs a value of 1 0 indicates drainarea equals da obs null for subbasins with da obs 0 obs nm string the point of interest id as reported by agency providing the location null for subbasins with has poi 0 src obs string source of the point of interest poi hydat poi from hydat database pwqmn poi from pwqmn water quality monitoring site quat poi from subbasin outlets extracted from the quaternary ontario watershed boundaries null for subbasins with has poi 0 da chn l m float for the entire upstream drainage area the longest flow path to the subbasin outlet it is 1 2345 for non connected lake subbasin and headwater subbasins since these have no river channels note that this attribute and the other da attributes below can be used in various empirical time of concentration formulas da slope degree float for the entire upstream drainage area the averaged slope over the drainage area it is limited to a range of 0 60 computed as the area weighted average of all subbasins in the drainage area da chn slp m m float for the entire upstream drainage area the slope of river channel along the longest flow path in the drainage area computed as the area weighted average of all subbasins in the drainage area it is limited to a range of 0 000001 1 0 it is 1 2345 for non connected lake subbasin and headwater subbasins since these have no river channels fral float it is the index of flood attenuation by reservoirs and lakes ranging from 0 to 1 note attribute name should have been farl not fral the lake attenuation effect increases as farl decreases from 1 0 to 0 0 farl is an index described in the institute of hydrology 1999 outletlng deg w float subbasin outlet longitude outletlat deg n float subbasin outlet latitude centroid x deg w float subbasin centroid longitude centroid y deg n float subbasin centroid latitude appendix b b1 routing network delineation methodology once the spatial extent of the project is defined basinmaker network delineation mode requires four steps to produce a hydrological routing network from a hydrologically consistent digital elevation model dem these steps are functions nd2 through nd5 in fig 3 given a user defined dem function nd2 in fig 3 will create an initial routing network and routing structure without considering lakes using grass gis terrain analysis functions r stream basins and r watershed the r watershed function in grass gis uses a least cost search algorithm to calculate the flow accumulations ehlschlaeger c 1989 which is designed to minimize the impact of dem data errors the r watershed function will determine each dem cell s flow direction fig 6a and flow accumulation optionally instead of processing the dem users can input the flow direction grid directly then function r stream basins will identify rivers and subbasins fig 6a from the flow accumulation map based upon a user provided flow accumulation threshold note that function nd2 leaves headwater drainage areas a subbasin with a headwater river segment to be delineated into subbasins only after lakes are considered in the next step generally function nd2 will generate a coarser routing network and fewer subbasins when using a larger flow accumulation threshold value the outlet of each subbasin in fig 6a is identified as control points to be used in later steps given a set of lake polygons and an optional set of points of interest function nd3 in fig 3 will further process the outputs from function nd2 fig 6a shows such an example output as well as a lake relative to that output layer it will add lake inlets outlets the initiation points of select headwater river segments and points of interest as new control points new subbasin outlets into the intermediate routing network a point of interest is a location where the user wants to define a subbasin outlet this function will ensure the delineated routing structure of the network meets the following criteria a the water within a lake will move to the lake outlet b any lake outlets are subbasin outlets 3 any lake inlets are subbasin outlets basinmaker only adds a new control point to the initiation point of a headwater river segment if the headwater river segment is not at all covered by a lake see fig 6a which shows two such headwater river segments partially covered by a lake and one that is not the lake covered headwater river segment initiation points in fig 6b are not new control points since each of these would create an extra tiny subbasin between the headwater river segment initiation point and the lake fig 6b such tiny subbasins are unnecessary for hydrological modeling and would increase the computational burden function nd3 will first add lake inlets and outlets into the initial routing structure it distinguishes user provided lake polygons as connected lakes or non connected lakes and only retains lakes with a lake area larger than the user provided lake area threshold basinmaker uses the retained lake polygons to identify lake outlets and inlets and regards them as additional control points a lake outlet is determined as the cell having the highest flow accumulation value within each lake polygon lake inlets are determined from the intersection points among the lake boundary cells and routing network cells basinmaker identifies one lake outlet only for each lake while the number of lake inlets may vary basinmaker will directly use the points of interest input e g a map of point locations of streamflow gauges as control points note that only points of interest within a user specified snapping distance threshold of the delineated river network are automatically included as a control point using the r stream snap function in grass gis afterward an updated routing structure including subbasin polygons is generated based on the updated control points and the flow direction map from function nd2 in fig 3 as shown in fig 6b second function nd3 will identify lakes for which part of the lake area did not belong to the intermediate lake outlet contributing area fig 6b in fig 6b the left part of the lake belongs to a subbasin that does not drain to the lake outlet the right part of the lake is covered by three subbasins green filled subbasins in fig 6b flowing to the lake outlet we define these three subbasins that partially cover the lake and flow to the lake outlet as the partial lake subbasins of this lake third function nd3 will correct the flow direction of cells within the identified lakes as shown in fig 6b the correction needed is to ensure the water in the left part of the lake flows to the lake outlet the correction is achieved with a minimum of processing time by modifying the flow direction of only the lake boundary cells for example any of the cells red boundary cells in fig 6c with their flow directions not pointing towards the three right partial lake subbasins referred to as the un converged boundary cells ubcs are corrected the flow directions of ubcs are corrected in two steps a the flow directions of the two ubc cells that are at the three lake subbasin boundaries cells shown in fig 6c with the yellow color are manipulated to make the water at each ubc flow to one of the adjacent three partial lake subbasins three right lake subbasins in fig 6b b change the flow direction of the remaining ubcs grid cells with the green color in fig 6c to make water in these cells move to the closest yellow cells in fig 6c with this modified flow direction dataset all lake cells now drain to the lake outlet and an updated routing structure will be generated fig 6c next function nd4 in fig 3 estimates physical attributes for each subbasin river channel lake and point of interest as described in the manuscript then function nd5 in fig 3 merges the three partial lake subbasins in fig 6c and generates one lake subbasin completely containing the lake figure b1d when merging partial lake subbasins into one lake subbasin note that the resulting lake subbasin has no river channels subbasin attributes of the partial lake subbasins are carefully merged into a single attribute value for the lake subbasin for example lake subbasin properties like average slope and aspect are computed with an area weighted average of the corresponding partial lake subbasins while lake subbasin area is computed as the sum of the corresponding partial lake subbasins area these four steps ensure that 1 each lake polygon is fully contained by a lake subbasin representing its local contributing area 2 each lake outlet is the outlet of a lake subbasin 3 each lake inlet is represented by a control point of an upstream subbasin thus enabling lake inflow processes to be explicitly represented in a hydrologic model b2 routing network post processing methodology this section introduces the methodologies behind the two functions used for network simplification 1 remove small lakes and 2 decrease river network resolution comparing the intermediate network inputs fig 5a and outputs fig 5c of function remove small lakes demonstrates that the routing network is generally the same but the outputs retain only one lake this function uses the following steps to merge simplify subbasins when a minimum lake area threshold is specified 1 identify lakes in the input routing network with lake area smaller than the user provided lake area thresholds 2 change the lake related attributes appendix a to zero for lakes being removed 3 from upstream to downstream merge subbasins of the intermediate hydrological routing networks fig 5a when the subbasins meet all the following conditions a subbasins are directly connected b they have the same lake attributes they are either lake subbasins overlapping the same lake or non lake subbasins alternately the most downstream subbasin in the group contains a non connected lake and c subbasins all drain to the same river channel 4 update the attribute values of merged subbasins like the function pp5 described in section 2 2 2 attributes of merged subbasins are carefully merged into a single attribute value for the various attribute groups for example select river channel properties are merged using channel length weighted averaging and select subbasins attributes are merged using subbasin area weighted averaging 5 the remove small lakes function will internally call the function combine subbasins covered by the same lake to generate the final hydrological routing networks fig 5d comparing the input fig 5a and output fig 5e routing networks from function decrease river network resolution it is seen that the same lakes are retained but the outputs include only three river channels this function uses the following steps to merge simplify subbasins given minimum subbasin drainage area threshold 1 identify input subbasins fig 5a with a drainage area smaller than the user provided minimum drainage area threshold 2 remove river channels of the identified subbasins 3 change all lakes upstream of the removed subbasin outlet to non connected 4 from upstream to downstream merge subbasin polygons in the intermediate routing networks fig 5a when the subbasins meet all of the following conditions a subbasins are directly connected b they have the same lake attributes they are either lake subbasins overlapping the same lake or non lake subbasins alternately the most downstream subbasin in the group contains a non connected lake c subbasins drain to the same river channel and d upstream subbasins do not include a point of interest 5 update the attribute table of merged subbasins attributes are updated using channel length weighted averaging and subbasin area weighted averaging as described above 6 the decrease river network resolution function will remove river channels from the input intermediate hydrological routing networks by these steps fig 5e and then internally call the function combine subbasins covered by the same lake to generate the final hydrological routing network fig 5f appendix c c1 development of the north american lake river routing product v2 1 we use basinmaker v2 0 to develop version 2 1 of the north american lake river routing product han et al 2021 the input data for this na routing product includes the merit dem yamazaki et al 2017 hydrolakes messager et al 2016 databases for lake polygons points of interest comprising of 7791 streamflow gauges from water survey of canada wsc and 28 164 streamflow gauge from united states geological survey usgs and the global bankfull width depth and discharge estimate for each channel of the finest scale hydrosheds river system andreadis et al 2013 the merit dem is a high accuracy global dem at 3 arcsecond resolution 90 m at the equator developed by eliminating major error components from existing spaceborne dems yamazaki et al 2017 hydrolakes is a database that includes shoreline polygons of all global lakes with a surface area of at least 10 ha messager et al 2016 compared with datasets used to develop the pan canadian lake river routing network version 1 0 han et al 2020 dem resolution is substantially finer now 3 arcseconds instead of 15 arcseconds the na routing product was generated using 1 a constant flow accumulation threshold 2000 cells each 90 m 90 m in size corresponding to 16 km2 to define the initial river network and subbasins 2 a zero lake area threshold to include all lakes from the hydrolakes database with this setup the average subbasin size in the resulting na routing product is roughly 10 km2 in comparison the average subbasin size in the pan canadian lake river routing network is 60 km2 han et al 2020 the channel profile coefficients map for the drainage region above the 60 n latitude from han et al 2020 is used here for the drainage region below the 60 n latitude we inversely calculated the spatial distribution of channel profile coefficients using the global bankfull width depth and discharge product from andreadis et al 2013 the modis land cover type product mcd12q1 friedl and sulla menashe 2015 landuse map is utilized for floodplain roughness estimation along with flood plain manning s coefficients from chow 1959 corresponding to each landuse type the na routing product generated by basinmaker v2 0 does not include additional subbasin control points at the beginning of the headwater river channels defined by the flow accumulation threshold if it is not overlaid by a lake this key difference between v2 0 and v3 0 of basinmaker is depicted in figure c1 also the na routing product generated by basinmaker v2 0 does not include the following attributes that are produced by v3 0 1 the longest flow path of the subbasin s entire upstream drainage area 2 the average slope of the subbasin s entire upstream drainage area 3 the slope of river channel along the longest flow path and 4 the subbasin outlet longitude and latitude fig c1 example vector based routing networks from a basinmaker v3 0 and b basinmaker v2 0 the hydrological routing networks from basinmaker v2 0 do not include additional subbasin control points at the beginning of any of the headwater river channels defined by the flow accumulation threshold while v3 0 does include some of these as described previously in section 2 2 the example watershed includes again a connected lake cl and a non connected lake ncl fig c1 c2 development of the ontario lake and river routing product v1 0 we use basinmaker v3 0 to develop version 1 0 of the ontario lake and river routing product han et al 2022 the ontario routing product covering the province of ontario canada delineates an area of approximately 1 million km2 at a finer spatial resolution than the na routing product the inputs of this application include the enhanced dem and the enhanced flow direction fdr raster layers 30 m 30 m from the ontario integrated hydrology oih datasets hydrolakes messager et al 2016 databases points of interest including 1100 ontario observation gauges from the water survey of canada wsc database 439 active water quality monitoring gauges from the ontario provincial water quality monitoring network pwqmn and 1981 subbasin outlets extracted from the quaternary ontario watershed boundaries parameters provided to basinmaker include the following 1 a constant flow accumulation threshold 5000 cells each 30 m 30m in size corresponding to 4 5 km2 to define the initial river network and subbasins 2 zero as the lake area threshold to include all lakes from the hydrolakes database bankfull width and depth attribute estimation procedures for the ontario routing product are the same as those used in the na routing product see section b1 
25406,hydrological modelling efforts tend to ignore the impacts of lakes or explicitly simulate the behavior of only the largest lakes in a watershed as deriving information required to explicitly represent thousands of lakes is difficult we introduce an open source gis toolbox basinmaker that can efficiently build vector based hydrological routing networks including an arbitrary number of rivers and lakes with attributes e g network topology subbasin and lake geometry channel characteristics that provide the inputs required for hydrological routing models basinmaker functionality is demonstrated to build two high resolution vector based lake river routing products each defining a collection of routing networks across large regions the north american lake river routing and ontario lake river routing products each includes all lakes over 10 ha identified in the hydrolakes dataset basinmaker is unique in terms of lake representation and is especially helpful for modelers who need to explicitly represent numerous lakes in their watershed simulation models keywords lakes rivers routing network vector based routing gis toolbox network delineation software and data availability software name basinmaker developer hydrology research group at the university of waterloo program language python 3 year first available 2020 license open source under the gnu general public license v3 0 software availability http hydrology uwaterloo ca basinmaker index html data availability north american lake river routing product v2 1 basinmaker application example https zenodo org record 4728185 http hydrology uwaterloo ca basinmaker index html ontario lake river routing product v1 basinmaker application example https doi org 10 5281 zenodo 6536085 https lake river routing products uwaterloo hub arcgis com 1 introduction lakes play a critical role in the regional scale water balance first they can retain runoff from the upstream catchment and release water later to the downstream rivers which influences the timing and volume of runoff that is transmitted downstream bowling and lettenmaier 2010 huziy and sushama 2017 spence 2000 woo and mielko 2007 zajac et al 2017 further the active contributing area or the flow connectivity of the watershed is often controlled by lakes costa et al 2020 woo and mielko 2007 moreover the interactions between lakes and groundwater are key components in hydrological processes that can affect both water quantity and quality hughes et al 2014 spence 2006 therefore considering lakes as a component in hydrological models can increase the model s capacity to simulate critical processes and potentially improve model performance golden et al 2016 karan et al 2014 shaw et al 2013 lakes are defined here consistent with hydrolakes messager et al 2016 where they include water bodies that are lakes reservoirs and natural lakes with regulation structure lakes also play a crucial role in biogeochemical processes cheng and basu 2017 analyzed data across 600 sites to show the importance of small water bodies including lakes for retaining nitrogen n and phosphorus p thus regulating their movement to downstream waters harrison et al 2009 reports that small lakes account for 65 of the n removal in all lakes downing 2010 reports that organic carbon burial rates in small lakes exceed those of larger lakes by more than an order of magnitude to build an environmental simulation model of any watershed that explicitly represents lakes modelers need to define the lake characteristics and build the lake river routing structure evenson et al 2015 2016 han et al 2020 the lake river routing structure defines the connectivity between the lake river and its surrounding landscape the hydrological model requires the lake characteristics to calculate lake outflow evaporation and seepage at a given lake level necessary lake characteristics typically include the location and extent of the lake the crest height of the lake outlet and a relationship that defines the lake surface area and volume as a function of lake depth a detailed lake characterization may require lake bathymetry which is unavailable for most lakes several global lake databases such as hydrolakes messager et al 2016 and globathy khazaei et al 2022 provide the estimated location and extent of the lake lake area lake volume and averaged depth for lakes with a lake area larger than 10 ha 0 1 km2 with information from these databases missing lake characteristics such as crest height may be estimated with some assumptions for example han et al 2020 assume that lakes are prismatic which means that the lake area does not change with lake depth and the lake crest elevation relative to the lake bottom is the same as the lake averaged depth in hydrological models lakes are represented as a part of the routing structure lehner and grill 2013 the routing structure is generally used to formally represent water flow pathways from land to rivers lakes and eventually converging to the watershed outlet it is a topologically consistent system of river networks and local contributing areas of lakes and river channels e g subbasins and the connections between them typically in the form of a hierarchical network here a watershed routing structure only defines how the subbasins river channels and lakes in the watershed are connected hydrological routing simulation e g the muskingum or diffusive wave approach using such a routing structure also requires routing relevant parameters e g main channel and floodplain manning s coefficient and attributes e g lake area and river channel slope throughout this paper we consider a hydrological routing network to be the combination of routing structure topology plus the corresponding routing attributes of the lakes channels and subbasins necessary for running a hydrological routing model e g a subcomponent of spatially distributed hydrologic models we also define a collection of routing networks covering a large geographic region and delineated from the same source dem and lake polygons as a routing product as used here the term subbasin is consistent for instance with the definition of a catchment in the waterml2 data model blodgett et al 2021 in that catchments can fully partition the landscape each subbasin may contain a main flowpath and subbasins may be aggregated into a drainage basin watershed the entire contributing area for any single subbasin outlet a routing structure and hence a hydrological routing network can be represented in a gridded approach gaborit et al 2017 miller et al 1994 or vector based approach lin et al 2018 mizukami et al 2016 thober et al 2019 yamazaki et al 2013 when we derive the routing network with the gridded and vector based approaches using the same dem the vector based routing approach can accurately represent the drainage area boundaries and location of hydrological features with fewer computational objects with fewer computational objects the vector based approach is computationally more efficient lin et al 2018 yamazaki et al 2013 gharari et al 2020 also call for land surface modelers to adopt flexible vector based spatial configurations to better model at the scale of interest hydrological components in a vector based routing structure can be well represented by gis based features such as polygons and polylines and the topology relationships between these gis features gis features for a vector based routing structure without lakes can be readily generated from a digital elevation model dem using various gis tools such as arcswat winchell et al 2007 archydro maidment 2002 tatoo mitterer 2022 taudem tarboton 2015 or other hydrology toolsets within gis platforms esri 2012 neteler et al 2012 in the resultant stream only routing structures water is transmitted from one subbasin to another and the flow pathways are simulated as converging from land to rivers and then flowing to the outlet however it is more complicated to produce a vector based routing structure explicitly including lake geometry a lake river routing structure because we need to delineate the lake s upstream contributing area and build the topology between lakes and river channels in the network evenson et al 2015 2016 han et al 2020 wang et al 2021 generally the approaches to generating a lake river vector based routing structure can be classified into three categories first the lake could be simply represented as a node on the delineated routing network whose geometry exists outside of the network li et al 2022 vanderkelen et al 2022 this approach introduces errors in the network due to the fact it does not distinguish the local lake drainage area along the shoreline of the lake from the drainage areas of the upstream delineated river channels defining an inlet for the lake without explicitly incorporating the lake geometry into the delineated routing network the local lake drainage area lake subbasin is unknown this error could be significant in upstream subbasins with lakes and the error will increase as the resolution of the routing network is decreased less lake inflow from upstream river channels more inflow from the local lake subbasin moreover this approach is unable to represent lakes that are not located on the delineated river channels as such the number of unique lakes that can be independently represented in the routing network is reduced when the resolution of the routing network is reduced second a lake river vector based routing structure can be generated by collectively overlaying the lake shoreline polygon onto the subbasin polygons of a vector based routing structure without lakes and then updating the properties of the newly derived polygons and the new network topology molina navarro et al 2018 as noted by the molina navarro et al 2018 the limitation of this approach includes 1 many small polygons around the lakes would be generated after the overlay operation and these small polygons are data redundancies rather than true subbasins molina navarro et al 2018 2 the drainage area of the lake is not accurately represented lehner and grill 2013 third a vector based lake river routing structure can be alternatively generated by directly modifying the flow direction dataset the flow direction dataset is a raster dataset derived from a digital elevation model dem indicating the flow direction of each dem grid cell in this approach the flow direction dataset is manipulated to ensure that water in grid cells that are covered by the lake will drain to the closest derived river channels under the lake maidment 2002 the result of this manipulation can divide a lake into multiple separate subbasins based on unreliable dem elevations under the lake potentially leading to more than one lake outflow river channel han et al 2020 avoided the problem by modifying flow direction grids such that water flows to the designated hydrolakes lake subbasin outlet however the approach in han et al 2020 cannot be applied to the high resolution dem due to the excessive computational burden in addition the implementation of han et al 2020 relied on proprietary gis software this paper introduces a new open source gis toolbox basinmaker 3 0 which can efficiently and correctly delineate and conveniently simplify via post processing a vector based hydrological routing network topology plus the corresponding routing attributes in lake dominated watersheds from any hydrologically consistent dem the basinmaker toolbox was initially developed to encapsulate the delineation and attribute estimation methods presented for the pan canadian lake river routing products developed by han et al 2020 but has since evolved to include new and revised delineation methods and greater functionality and flexibility this paper also reports on some of these new and revised delineation methods behind the toolbox results show the flexibility and utility of the toolbox for example basinmaker can alternatively delineate directly from either a user provided dem or flow direction grid can optionally delineate lakes that are not explicitly connected to the delineated river channels and provides unique post processing functions that can simplify without further terrain analysis pre existing higher resolution lake river routing products to our knowledge such a collection of efficient post processing tools for generating and simplifying lake river routing networks has not been reported elsewhere the remainder of this paper is structured as follows section 2 presents the methodology overview how we define a lake river routing network an overview of the basinmaker software to produce such a routing network and some additional detail on methods behind the software section 3 describes the results of the experiments designed to showcase the software utility and flexibility production of large scale of routing products and reports on some comparisons and validation with existing watershed boundary delineations section 4 is the discussion and limitations while section 5 is the conclusions 2 material and methods 2 1 definition of subbasins and lakes subbasins subdivide a watershed into distinct non overlapping units that are topologically connected and indicate how surface water moves from within the watershed to the watershed outlet this basic watershed discretization strategy is common and is used for example in spatially distributed hydrological models such as swat winchell et al 2007 hype lindström et al 2010 and raven craig et al 2020 basinmaker subbasins are specifically defined as below 1 a subbasin is not a watershed drainage basin and instead can be considered a local drainage basin that excludes the upstream watersheds of all subbasin inlet points it could alternatively be defined as the intersection two or more drainage basins 2 the single subbasin outlet defines the point where all water from the subbasin exits including all internally generated runoff and baseflow and flow received from upstream subbasins 3 headwater subbasins have no upstream subbasin 4 endorheic subbasins are terminal and do not drain to a downstream subbasin 5 each lake subbasin contains one and only one lake and can have an unlimited number of inlet points 6 a non lake subbasin includes a single river channel typical or no river channel e g for a headwater subbasins and can have at most only one inlet point often this inlet is at the confluence of two upstream river channels each of which is contained within their own subbasin 7 all subbasins have a single outlet the outlet connects it to a downstream subbasin except in the cases of a endorheic subbasin small headwater subbasins on the coast draining directly to an external water body e g an ocean or the primary outlet of the watershed of interest 8 a subbasin water body either the lake or river channel connects all upstream subbasins to the downstream subbasin a lake is defined as depression storage compartment on the landscape with the spatial extent defined by a single contiguous polygon which only naturally transmits surface water downstream when some threshold storage volume depth is exceeded in contrast a river channel is a flowpath moving water through a subbasin and it has no permanent storage capacity like a lake basinmaker requires users to distinguish lake water bodies from river water bodies and this is done by users providing lake polygons as an input in order to discretize the lake river routing network and thus merge lake polygons with a routing network basinmaker assumes lake polygons represent the maximum areal extent of the lake body the treatment of lakes with temporally variable surface area can be readily handled in hydrological models using a basinmaker routing network if the maximum lake spatial extent is the input to basinmaker as lake contraction will not impact the geometry of a lake s contributing area how to parameterize lakes with highly variable surface areas appropriately within hydrologic models is beyond the scope of this manuscript reservoirs with managed outflows and natural lakes can both be represented as a lake in basinmaker generated lake river routing networks but when such networks are used as the basis of a hydrologic routing model users will want to carefully determine and then characterize how the case study reservoir levels and outflows are managed likewise hydrologic routing model applications should consider if the prismatic lake area assumption should be replaced with a site specific stage storage relationship 2 2 overview of the lake river routing networks produced by basinmaker in this section we introduce the differences between a routing network without considering lakes and a routing network with lakes defined by basinmaker differences in routing networks are reflected in the routing structure defining how subbasins river channels and lakes are connected an example vector based routing structure without considering lakes is shown in fig 1 a subbasins in such a routing structure are only determined by river channels and subbasin outlets are defined by control points control point locations can be based on river channel junctions points of interest e g monitoring locations or flow accumulation thresholds an example of a routing structure with lakes is shown in fig 1b for the same watershed depicted in fig 1a a lake subbasin is defined as in han et al 2020 by the following rules 1 the extent of a lake subbasin will fully cover the lake 2 a lake outlet defines the outlet of the lake subbasin 3 each of a lake s significant inlets i e those associated with rivers above a specified flow accumulation threshold is defined by an upstream subbasin outlet in this way both inflow and outflow of each lake can be explicitly simulated by hydrological routing models lakes are divided into two categories by basinmaker 1 connected lakes cl which are lakes with an outlet that is explicitly connected to a downstream non zero length river channel in the routing structure and 2 non connected lakes ncl which denote lakes that are defined to drain to the downstream routing network but do not flow through any explicit river channel see small yellow subbasin in fig 1b by default both cl and ncl within a watershed are considered to be contributing areas of the watershed though this assumption can be relaxed i e the area can be disconnected from the rest of the network for ncls when appropriate the only difference between cl and ncl is that a cl always drains into an explicitly represented river channel that is connected to the lake outlet while a ncl does not ncls can exist because the user defined minimum drainage area flow accumulation threshold for defining starting a new river channel can sometimes be larger than the drainage area of smaller lakes particularly for headwater lakes in this way basinmaker separately distinguishes ncls and enables users to optionally define them separately from the cls relative to the lake river routing product defined in han et al 2020 the use of non connected lake subbasins in basinmaker is new the control points shown in fig 1 define the location of a subbasin outlet and for any non lake subbasin the basinmaker delineation strategy described in section 2 2 determines the control point locations in a way that favors the creation of fewer subbasins the proposed routing structure generated with basinmaker describes the watershed routing structure connectedness and flow directions of the component subbasins river channels and lakes the watershed routing structure defines how routing components in the watershed are connected in order to simulate hydrological routing through such a structure additional component attributes physical characteristics and parameters are required these are estimated and output by basinmaker the additional component attributes produced by basinmaker can be categorized into four groups the attributes of the lake attributes of the river channel the attributes of subbasins and the attributes of points of interest the points of interest define the desired locations of subbasin outlets in the delineated routing network and would generally include various monitoring locations and potentially other case study dependent locations lakes in basinmaker are assumed by default to be prismatic with a broad crested weir as the outlet structure and as such basinmaker reports the lake area lake depth and crest width for the lake outlet basinmaker also includes an attribute indicating whether the subbasin has one cl one ncl or no lake with a maximum of one river channel per subbasin basinmaker derives the river channel length cross section profile slope and both the main channel and floodplain manning s coefficients for the subbasin attributes basinmaker includes the subbasin area and the average subbasin slope the methodology to calculate each of these attributes is described in section 2 2 see function nd5 the relation between these various geometric entities as used within the basinmaker toolkit are depicted in the feature diagram of fig 2 2 3 overview of basinmaker functions basinmaker is a gis toolbox written in the python 3 programming language basinmaker functions all rely on geoprocessing functions from the gis environment it is configured to use users can apply basinmaker post processing mode functions in either arcgis pro or google colab which uses geopandas gis environment created by jordahl et al 2020 environments basinmaker delineation mode functions rely on both qgis and grass gis environments although not precisely the same in each gis environment basinmaker light post processing mode uses equivalent geoprocessing functions from geopandas jordahl et al 2020 qgis or arcgis pro 2 3 1 overview of basinmaker functions in network delineation mode fig 3 describes the workflow associated with the five key network delineation functions in basinmaker applied sequentially to generate a routing network from a hydrologically conditioned dem and optional inputs including lake polygons flow direction raster datasets landuse raster datasets bankfull width and depth of channel polyline datasets and a dataset of additional points of interest the define project spatial extent function generates the spatial extent to be processed of the current project basinmaker will clip all input data with this spatial extent and delineate lake river routing network s within this spatial extent the define project spatial extent function includes several options which allow the user to directly specify the spatial extent of the input dem as the spatial extent of the project or alternatively determine the spatial extent of the project from a watershed outlet latitude and longitude the delineation initial subbasins without lakes function generates an initial routing structure without considering lakes fig 1a the add new subbasin outlet points function can be used to add lake inlets lake outlets or other points of interest as new subbasin outlets into the initial routing structure see fig 4 for an example result the generate hydrologic routing attributes function updates geometry attributes and estimates hydrological parameters for the subbasins lakes and rivers generated previously from the add new subbasin outlet points function appendix a lists all attributes generated by the generate hydrologic routing attributes function the output of the generate hydrologic routing attributes function is only an intermediate hydrological routing network since it still represents some lakes being located within multiple subbasins fig 4 the function combine subbasins covered by the same lake generates the finalized hydrological routing network fig 1b note that depending on the user specified option for the first define project spatial extent function the resulting routing network may not be a single watershed and instead may include a number of subbasins that drain to a location external to the chosen dem extents the post processing mode function select subregion of routing structure pp3 in table 1 below can be applied to focus the delineated network on a single watershed and or strip away small watersheds along the boundary of the original spatial extent we refer to section 2 3 1 and appendix b1 for more details of the above briefly explained functions the finalized hydrological routing network output of basinmaker network delineation mode includes the following esri standard shapefiles a lake polygons fig 1b b points of interest c subbasin polygons and river polylines for intermediate hydrological routing network output of the function generate hydrologic routing attributes shown in fig 4 and d subbasin polygons and river polylines for the finalized hydrological routing network fig 1b we will refer to gis files from a to d as basinmaker delineation mode outputs in the following section the intermediate hydrological routing network outputs of the function generate hydrologic routing attributes are included in basinmaker delineation mode outputs because they are the required inputs of basinmaker post processing functions that simplify a basinmaker delineated hydrological routing network section 2 2 2 any basinmaker produced hydrological routing network in either network delineation mode or network post processing mode is defined by the above four esri standard shapefiles note that the attributes of all four attribute groups are all assigned to subbasin polygon shapefile of the hydrological routing network 2 3 2 overview of basinmaker functions in network post processing mode given a pre existing hydrological routing network delineated by basinmaker users can apply network post processing functions pp1 pp2 and pp3 in table 1 independently to customize and simplify an existing hydrological routing network e g remove detail although the core basinmaker generated routing network data are model agnostic users can apply functions pp4 and pp5 in table 1 sequentially to export routing relevant input files for the raven hydrological modelling framework craig et al 2020 functions pp1 and pp2 are described here in detail while the basinmaker website http hydrology uwaterloo ca basinmaker index html describes functions pp3 pp4 and pp5 the input for functions pp1 and pp2 in table 1 is the intermediate hydrological routing network fig 5 a in basinmaker delineation mode outputs note the intermediate routing network gis files not the finalized network gis files are the required inputs here they include the information required to properly simplify and thus recalculate attributes for a lower resolution routing network the output of simplifying function pp1 fig 5c and d and function pp2 fig 5e and f for the simplified post processed network are equivalent to basinmaker delineation mode outputs defined in section 2 2 1 the remove small lakes function removes lakes from the input network based on lake area thresholds for connected lakes and non connected lakes fig 5c and d demonstrate the outputs from the function remove small lakes using the routing network shown in fig 5a as input the decrease river network resolution function increases the size of subbasins and reduces the routing network s resolution from the input when subjected to a larger drainage area threshold subbasins with their drainage area smaller than this threshold are merged with their downstream subbasins fig 5e and f demonstrate the outputs from the function decrease river network resolution using the routing network shown in fig 5a as input we refer to appendix b for more details on the briefly introduced functions that can be used in network post processing mode 2 4 summary of methods for basinmaker functions this section is a high level summary of the methods encapsulated in key basinmaker functions detailed description of the methods for basinmaker functions are found in 2 4 and appendix b 2 4 1 basinmaker network delineation methods once the spatial extent of the project is defined basinmaker network delineation mode requires four steps to produce a hydrological routing network from a hydrologically consistent digital dem these steps are functions nd2 through nd5 in fig 3 we will summarize the methods applied by function nd3 and nd4 in the manuscript while the detailed method applied within other functions are detailed further in appendix b1 function nd2 in fig 3 will create an initial routing network and routing structure without considering lakes using input dem or flow direction the result of function nd2 were shown in fig 6 a given a set of lake polygons and an optional set of points of interest function nd3 in fig 3 will add lake inlets outlets the initiation points of select headwater river segments and points of interest as new control points new subbasin outlets into the intermediate routing network an updated routing structure including subbasin polygons will be generated based on the updated control points and the flow direction map from function nd2 in fig 3 as shown in fig 6b after that function nd3 will identify lakes for which part of the lake area did not belong to the intermediate lake outlet contributing area fig 6b and will correct the flow direction of cells within the identified lakes as shown in fig 6b the correction needed is to ensure the water in the left part of the lake flows to the lake outlet the correction is achieved with a minimum of processing time by modifying the flow direction of only the lake boundary cells for example any of the cells red boundary cells in fig 6c with their flow directions not pointing towards the three right partial lake subbasins referred to as the un converged boundary cells ubcs are corrected next function nd4 in fig 3 estimates physical attributes for each subbasin river reach lake and point of interest shown in figure b1c appendix a lists all attributes added by this function most of these attributes are estimated using the methods described in han et al 2020 except for bankfull width w in m bankfull depth d in m and bankfull discharge q in m3 s the rest of the attributes are estimated using the same methods as described in han et al 2020 andreadis et al 2013 use the following two equations to calculate the w and d of each subbasin w 7 2q 0 5 and d 0 27q 0 39 respectively they estimated q using a power law relationship between q and the drainage area da in km2 q k da c thus to calculate the w and d of each subbasin we need to assign k and c for each subbasin which will be referred to as the channel profile coefficients two options are available the user can provide constant channel profile coefficients for all subbasins or the user can provide a channel profile coefficient polygon gis layer each channel profile coefficient zone in the channel profile coefficient layer has a unique pair of k and c values this function will use the k and c values from the channel profile coefficient zone with the largest intersecting area with each subbasin if a sufficiently large sample of local measurements bankfull width bankfull depth and discharge are available the user could use these local measurements to estimate k and c when local measurements are unavailable users could utilize the bankfull width bankfull depth and discharge provided in andreadis et al 2013 to determine k and c for a specific watershed finally function nd5 in fig 3 merges the three partial lake subbasins in fig 6c and generates one lake subbasin completely containing the lake fig 6d 2 4 2 basinmaker post processing methods although basinmaker provides five post processing gis functions table 1 this section introduces the summary of methodologies behind the two functions used for network simplification 1 remove small lakes and 2 decrease river network resolution we focus on these functions because these are unique in basinmaker compared to other software tools both post processing functions are used to modify the intermediate hydrological routing networks generated by basinmaker in delineation mode fig 5a in general both network simplification functions will efficiently create a new routing network as showed in figs 5c and 4d by applying the following steps more detailed description is in appendix b2 1 identify the subbasins that needs to be merged based on user provided threshold 2 from upstream to downstream merge subbasins of the intermediate hydrological routing networks fig 5a when the subbasins meet certain conditions 3 update the attribute values of merged subbasins like the function nd5 described in section 2 2 1 attributes of merged subbasins are carefully merged into a single attribute value for the various attribute groups for example select river channel properties are merged using channel length weighted averaging and select subbasins attributes are merged using subbasin area weighted averaging 4 assign existing subbasin id values to the merged subbasins merged subbasins are all defined by subbasin outlet locations that were pre existing in the input hydrological routing network as such the subbasin id for a merged subbasin is assigned as the subbasin id from the input network with the same subbasin outlet location this means that once basinmaker initially assigns a subbasin id in network delineation mode e g the subbasin id whose outlet is associated with a streamflow gauging station for all simplified network resolutions derived from the original most detailed delineation subbasins draining to the same outlet location e g a streamflow gauge are always assigned the same subbasin id value 2 5 summary of basinmaker delineation experiments and analyses in order to demonstrate and test the basinmaker software we designed a number of experiments and analyses these function to demonstrate the flexibility and inherent efficiency of the delineation software as well as validate the delineated networks against pre existing delineated watersheds these are reported on in the subsequent results section in the order they are defined here basinmaker application 1 delineates a large scale lake river routing product covering north america the core input data for producing the north american lake river routing product v2 1 han et al 2021 includes the merit dem yamazaki et al 2017 which is 3 arcsecond resolution 90 m at the equator hydrolakes messager et al 2016 databases for lake polygons and points of interest comprising of 7791 streamflow gauges from water survey of canada wsc and 28 164 streamflow gauge from united states geological survey usgs secondary input data and delineation settings for the north american product are detailed in appendix c an example watershed is used to demonstrate select basinmaker post processing functions basinmaker application 2 delineates a large scale lake river routing product covering the province of ontario canada an area of approximately 1 million km2 the core input data for producing the ontario lake and river routing product v1 0 han et al 2022 includes the enhanced flow direction fdr raster layer 30 m 30 m from the ontario integrated hydrology oih dataset hydrolakes messager et al 2016 databases and points of interest including 1539 streamflow plus water quality gauge locations and 1981 subbasin outlets extracted from the quaternary ontario watershed boundaries in the oih dataset provincial mapping unit mapping and information resources branch 2022 unlike the north american product application this application delineates the drainage network from an enhanced fdr enhanced to reflect known watercourse drainage patterns instead of a dem and includes additional points of interest beyond streamflow gauging stations secondary input data and delineation settings for the ontario product are detailed in appendix c the above delineated networks above are used to assess and validate the basinmaker generated watershed boundaries relative to streamflow gauge drainage areas and datasets reported by streamflow monitoring agencies the area comparison is completed for all gauges in the routing products watershed drainage boundary comparisons are assessed for two locations 3 result and discussions 3 1 basinmaker applications basinmaker software was utilized to build the hydrological routing networks for the great lakes region in the mai et al 2022 model intercomparison project where the raven hydrological modelling framework craig et al 2020 in routing only mode was utilized to route distributed runoff fluxes from six different hydrological simulation models through the network basinmaker has successfully produced vector based lake river routing networks for dem flow direction datasets with the following resolutions 500 m hydroshed dem dataset delineating networks as large as 1 788 586 km2 90 m merit dem delineating networks as large as 324 372 km2 30 m oih flow direction delineating networks as large as 100 228 km2 10 m nhd plus flow direction delineating two networks up to 3159 km2 and a 5 m lidar dem delineating a 0 57 km2 watershed however basinmaker software memory limitations might be exceeded when delineating some networks e g a large watershed with a high resolution dem in such a case users need to break up their region watershed into smaller subregions before proceeding with the delineation we have applied basinmaker across a range of applications to successfully produce vector based lake river routing networks for dem flow direction datasets with the following resolutions 500 m hydroshed dem dataset delineating networks as large as 1 788 586 km2 90 m merit dem delineating networks as large as 324 372 km2 30 m oih flow direction delineating networks as large as 100 228 km2 10 m nhd plus flow direction delineating two networks up to 3159 km2 and a 5 m lidar dem delineating a 0 57 km2 watershed however basinmaker software memory limitations might be exceeded when delineating some networks e g a large watershed with a high resolution dem in such a case users need to break up their region watershed into smaller subregions before proceeding with the delineation in this section we will summarize two large scale basinmaker applications the development of the north american lake river routing product v2 1 section 3 1 1 and the development of the ontario lake river routing product v1 0 section 3 1 2 only the high level summary is provided here and the more detailed development documentation for the two routing products can be found in appendix c 3 1 1 the north american lake river routing product v2 1 the north american lake river routing product na routing product v2 1 dataset is available in han et al 2021 and includes 650 971 lakes in total with areas ranging from 0 1 km2 to 117 326 km2 the na routing product v2 1 does not include coastal watersheds with a drainage area smaller than 8500 km2 attribute values were checked to ensure physical plausibility and the allowable ranges for various attributes are provided in table a1 attribute values that could be measured in the field such as river reach length and slope were not evaluated against field data and hence should be taken as an initial starting point in any analysis or modelling of the delineated network in total the na routing product successfully includes 19 391 points of interest which is 53 of the initially specified points of interest 28 164 usgs and 7791 wsc gauges the 47 that could not be included in the na routing product include 4648 streamflow gauges from wsc and 11 916 streamflow gauges from usgs reasons for non inclusion include gauges in coastal watersheds not included in the product gauges falling within a lake polygon and gauges whose co ordinates were deemed too far from the nearest delineated river channel a map based portal to access gauges or regions of the na routing product in han et al 2021 is found at http hydrology uwaterloo ca basinmaker fig 7 demonstrates an example routing network from the na routing product v2 1 and multiple post processed simplified routing networks produced with basinmaker v3 0 for the approximately 4000 km2 petawawa river watershed in ontario canada 3 1 2 the ontario lake and river routing product v1 0 the ontario lake river routing product has an average subbasin size around 4 5 km2 and the dataset is available in han et al 2022 the product includes in total 82 001 lakes with areas ranging from 0 1 km2 to 4506 km2 the ontario routing product does not include coastal watersheds with a drainage area smaller than 250 km2 attribute values were checked to ensure physical plausibility and the allowable ranges for various attributes are provided in table a1 attribute values were not evaluated against field data and hence should be taken as an initial starting point in any analysis or modelling of the delineated network in total the ontario routing product successfully includes 3226 points of interest 94 of the specified points of interest for inclusion made up of 991 ontario observation gauges from the water survey of canada wsc database 457 active water quality monitoring gauges from the ontario provincial water quality monitoring network pwqmn and 1878 subbasin outlets extracted from the quaternary ontario watershed boundaries unlike the north american product the points of interest inclusion success rate is high for the ontario product because there was a substantial manual effort to try and correctly include these points on the delineated network a map based portal to access gauges or regions of the ontario routing product in han et al 2022 is found at https lake river routing products uwaterloo hub arcgis com 3 2 validation of basinmaker generated watershed boundaries the accuracy of the delineated watershed boundary from basinmaker depends on the accuracy of the input flow direction or digital elevation model grids basinmaker uses to delineate the watershed we evaluated the accuracy of select basinmaker generated watershed boundaries from the merit e g what is used in the north american lake river routing product and oih e g what is used in the ontario lake river routing product datasets from two perspectives first we assessed the accuracy by comparing the drainage area of usgs streamflow gauges in the routing networks generated by basinmaker using each dataset and the usgs reported drainage areas buto and anderson 2020 second we compared the watershed boundary from usgs nhd buto and anderson 2020 dataset taken as accurate with the watershed boundary generated by basinmaker with each flow direction dataset 3 2 1 validation of basinmaker generated watershed boundary using merit dataset in this section we compared the drainage area of usgs streamflow gauges in the north american lake river routing product generated by basinmaker da bm and the drainage area from usgs reports da usgs which we assume is correct the north american routing products include 16 248 usgs streamflow gauges and all of them have a usgs reported drainage area the number of usgs gauges with absolute drainage error smaller than 15 is 15 010 the absolute drainage error of each gauge is defined as 100 da bm da usgs da usgs thus the watershed drainage areas generated by basinmaker using merit datasets yamazaki et al 2019 are consistent with the vast majority of usgs streamflow gauges note that this drainage error metric is an attribute available in the north american product for each of the included 16 428 usgs gauges second we compared the watershed boundaries from the usgs nhd dataset taken as accurate with two basinmaker delineated watersheds a basinmaker using merit flow direction dataset e g what is used in the north american lake river routing product and b basinmaker using the usgs nhd plus h flow direction dataset buto and anderson 2020 these comparisons are made for the mill creek watershed in indiana fig 8 the hu10 code of the mill creek watershed a unique number for each watershed in nhd plus h is 0512010607 the two watershed boundaries generated by basinmaker using two different raster datasets are compared with the watershed boundary polygon of mill creek provided in nhd plus h datasets fig 8a demonstrates the watershed boundary generated by basinmaker using merit dem and the watershed boundary polygon provided in nhd plus h dataset buto and anderson 2020 for the mill creek watershed a large discrepancy is evident between the two watershed boundaries especially in the upstream region of the watershed the discrepancy is due to the errors in the merit datasets the blue grids represent the routing network from the merit datasets yamazaki et al 2019 it was created by coloring all grids with the number of upstream drainage pixels larger than 100 in the number of upstream drainage pixels raster of merit datasets yamazaki et al 2019 some river channels of merit dataset do not drain to the mill creek watershed outlet fig 8a fig 8b demonstrates the watershed boundary generated by basinmaker using flow direction grids of nhd plus h and the watershed boundary polygon provided in nhd plus h dataset for the mill creek watershed only a small discrepancy was observed between the two watershed boundaries in some parts of the watershed fig 8b the discrepancy is due to the input of the flow direction grids in nhd plus h the blue line represents the river polyline from the nhd plus h datasets it is the nhdplusburnlineevent polyline data in nhd plus h datasets some river channels of nhd plus h dataset do not drain to the mill creek watershed outlet fig 8b 3 2 2 validation of basinmaker generated watershed boundary using oih dataset we compared the drainage area of usgs streamflow gauges in the ontario lake river routing product generated by basinmaker da bm and the drainage area from usgs reports da usgs the ontario lake river routing product includes 34 usgs streamflow gauges five of them do not have a reported drainage area in the usgs the number of gauges with absolute drainage error smaller than 15 is 23 out of 29 thus the watershed drainage areas generated by basinmaker using oih datasets are consistent with most usgs streamflow gauges fig 9 demonstrates the watershed boundary generated by basinmaker using oih flow direction dataset and the watershed boundary polygon provided in nhd plus h dataset for the watershed drainage to usgs streamflow gauge 05127000 the watershed boundaries from basinmaker and nhd plus h datasets are very consistent with each other an even better alignment of the wdbs is achieved when basinmaker used the flow direction dataset from nhd plus h results not shown the oih dataset is the authoritative source of watersheds and sub watersheds for the province of ontario provincial mapping unit mapping and information resources branch 2022 the polygons defining various oih secondary to quaternary watersheds were all derived from the same 30 m oih enhanced flow direction grid we used to produce the ontario routing product as such the wdb for any oih quaternary watershed point of interest included in the ontario routing product completely corresponds with the quaternary watershed wdb available from the oih dataset provincial mapping unit mapping and information resources branch 2022 in summary basinmaker could generate the lake river routing network represented in the input dem or flow direction dataset the resulting watershed boundaries from basinmaker are very consistent with watershed boundaries produced by other agencies when the same core flow direction grids are utilized for each delineation exercise however basinmaker could not correct the dem or flow direction errors related to watershed boundaries or routing networks and so basinmaker delineated watershed drainage boundaries wdbs can deviate from existing wdbs that have been delineated using a different core flow direction grid or dem dataset 4 discussion and limitations currently basinmaker cannot correctly handle the non contributing areas present in the source dem unless the source dem is specially pre processed such non contributing areas are prevalent for example in the prairie pothole region of north america in reality the runoff generated in a non contributing area does not drain to any downstream routing network however the default delineated routing network from basinmaker will connect subbasins in the non contributing area to the downstream routing network because of this limitation non contributing areas are not represented in either the ontario lake river routing product or the na routing product a special version of the na routing product is available for the nelson river drainage watershed covering a large part of the prairie pothole region that correctly delineates non contributing areas e g the sink subbasin for each non contributing area is simply assigned a downstream subbasin id of 1 this special version is available upon request unlike for permanent non contributing areas identified as a sink in the dem basinmaker is currently unable to properly include non permanent non contributing areas in the routing network but work is ongoing to do so in one of the next versions of basinmaker at the same time basinmaker can include non riparian wetlands in the routing network in addition to lakes provided the set of water polygons input to basinmaker includes both lake and wetland polygons however basinmaker currently assigns the same waterbody outlet structure characteristics to a wetland and lake and thus if users include wetland waterbodies for inclusion in their basinmaker delineated network they will very carefully need to assess the validity of the wetland outflow assumption a broad crested weir with a crest width derived as a function of wetland drainage area work is ongoing to determine how to uniquely parameterize wetland outflows many routing product attributes generated by basinmaker e g manning s n bankfull width bankfull depth in the north american routing product and the ontario product were not evaluated against field data therefore attributes such as these in current basinmaker routing products as well as attributes estimated in subsequent basinmaker delineated hydrological routing networks should be viewed as initial starting points in any analysis utilizing the routing network where available local data for such attributes will likely prove useful to incorporate in the routing network 5 conclusions an open source gis toolbox called basinmaker that can automatically and efficiently build vector based hydrological routing networks including an arbitrary number of lakes is introduced here basinmaker has two main functions 1 delineate the lake river routing network from a dem and 2 post process an existing lake river routing network generated by basinmaker the routing network from basinmaker includes all the necessary hydrological routing model inputs and can be used to build semi distributed hydrological models discretizing raven craig et al 2020 semi distributed hydrological models or routing only models is particularly efficient given basinmaker also has a post processing function to write raven model specific input files describing the watershed discretization key basinmaker post processing operations on an existing hydrological routing network include a extract a subregion of interest b simplify the lake river routing network by decreasing the resolution of the routing network and thus increasing the size of subbasins or by removing the lakes from it c define hrus to subdivide each subbasin and generate model setup files for the raven hydrological modelling framework craig et al 2020 based on user provided landuse and soil layer polygons these post processing tools are helpful for users to utilize a routing network of interest already delineated by basinmaker such as those networks included in the north american lake river routing product v2 1 available at http hydrology uwaterloo ca basinmaker index html and the ontario lake river routing product v1 0 available at https lake river routing products uwaterloo hub arcgis com with the basinmaker toolbox the hydrological community can now easily create a lake river routing network and explicitly model each lake in their spatially distributed hydrologic model this is important because calibrating a hydrological model that does not explicitly simulate lakes in a lake dominated watershed is likely to compensate for this lack of storage in erroneous ways and model performance at ungauged locations is likely to degrade as a result explicitly and correctly modeling lake inflows and outflows is becoming more important with the coming remote sensed lake level observations from the nasa swot satellite mission see https swot jpl nasa gov which promises to measure lake levels at resolutions temporal spatial and vertical that have the potential to be used to calibrate hydrological models in watersheds that would have previously been considered ungauged for example lakes down to 0 06 km2 in surface area can have monitored lake levels https swot jpl nasa gov resources 135 swot the lake surveyor to properly use such remotely sensed lake water level measurements requires correctly modelling the lake water balance which depends on explicitly defining the lake s contributing area and geometric position within the watershed drainage network basinmaker is therefore a tool enabling the hydrological modelling community to take full advantage of swot monitored lake levels around the world declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements support for basinmaker the north american routing product development and the ontario routing product came from multiple sources primary graduate student support for basinmaker contributors was provided by nrcan canadian forest service g c grants 129677 and 129816 and dr tolson s nserc discovery grant secondary support during preliminary basinmaker software development for first author ming han was provided by the canada first research excellence fund provided to global water futures lake futures project some additional secondary support was also provided via the canarie research software program grant rs3 124 to co author juliane mai support for the ontario routing product development was provided by the ontario ministry of northern development mines natural resources and forestry appendix a table a1 the included attributes in the lake river routing products and networks produced by basinmaker v2 0 and v3 0 the attributes marked with exist only in the ontario lake and river routing network generated with basinmaker v3 0 moreover the attribute marked with i e fral is generated outside of basinmaker v3 0 and only exists in the ontario lake and river routing network table a1 name unit type description and specific values subid integer subbasin catchment id dowsubid integer id of subbasin catchment downstream of current subbasin rivslope m m float slope of river channel in the subbasin it is limited to a range of 0 000001 1 it is 1 2345 for non connected lake subbasin and headwater subbasins since these have no river channels rivlength m float length of river channel in the subbasin it is 1 2345 for non connected lake subbasin and headwater subbasins since these have no river channels basslope degree float averaged slope over the subbasin it is limited to a range of 0 60 basaspect degree float averaged aspect over the subbasin it is counterclockwise from east 90 is north 180 is west 270 is south 360 is east basarea m2 float area of subbasin bkfwidth m float bankfull width of river channel in the subbasin width of river when full if this catchment is a lake catchment and the lake is not a reservoir this should be used as an initial estimate of lake s effective crest width it is limited to a range of 0 1 3000 0 bkfdepth m float bankfull depth river channel in the subbasin depth of river when full it is limited to a range of 0 1 10 0 lake cat integer lake category 0 subbasin without lake 1 subbasin with lake that is directly connected to a downstream river channel 2 subbasin with lake that is not directly connected to a downstream river channel hylakeid integer the lake id in hydrolakes database 0 for subbasins without lakes lakevol m3 float volume of lake in hydrolakes database 0 0 for subbasins without lake lakearea m2 float area of lake in hydrolakes database 0 0 for subbasins without lake laketype integer type of lake assigned by hydrolakes 0 subbasin without lake 1 subbasin with natural lake 2 subbasin with lake that is reservoir 3 subbasin with lake that is natural with regulation has poi integer subbasin has point of interest poi at outlet 0 subbasin has no poi 1 subbasin has poi which comes from the following sources streamflow and or water level gauge station and or water quality station and or subbasin outlets extracted from the quaternary ontario watershed boundaries meanelev m float subbasin average elevation floodp n s m1 3 float averaged flood plain manning s coefficient based on landcover along river channel it is limited to a range of ch n to 1 5 it is 1 2345 for non connected lake subbasin and head water subbasins since these have no river channels and thus no delineated flood plain q mean m3 s float subbasin outlet bankfull discharge estimated it is limited to a range of 0 00001 150000 0 ch n s m1 3 float river channel manning s coefficient it is limited to a range of 0 025 0 15 it is 1 2345 for non connected lake subbasin and head water subbasins since these have no river channels drainarea m2 float drainage area of subbasin outlet strahler integer the strahler order of river channel seg id integer the unique river channel id the subbasin belongs to each river channel can include multiple river channels only useful as an internal variable for basinmaker seg order integer the sequence of the river channel denoting the direction of water flow in the corresponding river channel only useful as an internal variable for basinmaker max dem m float the maximum elevation along the river channel min dem m float the minimum elevation along the river channel da obs km2 float subbasin drainage area reported by institution providing point of interest 0 0 for subbasins without point of interest or if no value is provided da error float relative ratio between drainage area based on routing network drainarea and basin drainage area reported by institution providing point of interest da obs a value of 1 0 indicates drainarea equals da obs null for subbasins with da obs 0 obs nm string the point of interest id as reported by agency providing the location null for subbasins with has poi 0 src obs string source of the point of interest poi hydat poi from hydat database pwqmn poi from pwqmn water quality monitoring site quat poi from subbasin outlets extracted from the quaternary ontario watershed boundaries null for subbasins with has poi 0 da chn l m float for the entire upstream drainage area the longest flow path to the subbasin outlet it is 1 2345 for non connected lake subbasin and headwater subbasins since these have no river channels note that this attribute and the other da attributes below can be used in various empirical time of concentration formulas da slope degree float for the entire upstream drainage area the averaged slope over the drainage area it is limited to a range of 0 60 computed as the area weighted average of all subbasins in the drainage area da chn slp m m float for the entire upstream drainage area the slope of river channel along the longest flow path in the drainage area computed as the area weighted average of all subbasins in the drainage area it is limited to a range of 0 000001 1 0 it is 1 2345 for non connected lake subbasin and headwater subbasins since these have no river channels fral float it is the index of flood attenuation by reservoirs and lakes ranging from 0 to 1 note attribute name should have been farl not fral the lake attenuation effect increases as farl decreases from 1 0 to 0 0 farl is an index described in the institute of hydrology 1999 outletlng deg w float subbasin outlet longitude outletlat deg n float subbasin outlet latitude centroid x deg w float subbasin centroid longitude centroid y deg n float subbasin centroid latitude appendix b b1 routing network delineation methodology once the spatial extent of the project is defined basinmaker network delineation mode requires four steps to produce a hydrological routing network from a hydrologically consistent digital elevation model dem these steps are functions nd2 through nd5 in fig 3 given a user defined dem function nd2 in fig 3 will create an initial routing network and routing structure without considering lakes using grass gis terrain analysis functions r stream basins and r watershed the r watershed function in grass gis uses a least cost search algorithm to calculate the flow accumulations ehlschlaeger c 1989 which is designed to minimize the impact of dem data errors the r watershed function will determine each dem cell s flow direction fig 6a and flow accumulation optionally instead of processing the dem users can input the flow direction grid directly then function r stream basins will identify rivers and subbasins fig 6a from the flow accumulation map based upon a user provided flow accumulation threshold note that function nd2 leaves headwater drainage areas a subbasin with a headwater river segment to be delineated into subbasins only after lakes are considered in the next step generally function nd2 will generate a coarser routing network and fewer subbasins when using a larger flow accumulation threshold value the outlet of each subbasin in fig 6a is identified as control points to be used in later steps given a set of lake polygons and an optional set of points of interest function nd3 in fig 3 will further process the outputs from function nd2 fig 6a shows such an example output as well as a lake relative to that output layer it will add lake inlets outlets the initiation points of select headwater river segments and points of interest as new control points new subbasin outlets into the intermediate routing network a point of interest is a location where the user wants to define a subbasin outlet this function will ensure the delineated routing structure of the network meets the following criteria a the water within a lake will move to the lake outlet b any lake outlets are subbasin outlets 3 any lake inlets are subbasin outlets basinmaker only adds a new control point to the initiation point of a headwater river segment if the headwater river segment is not at all covered by a lake see fig 6a which shows two such headwater river segments partially covered by a lake and one that is not the lake covered headwater river segment initiation points in fig 6b are not new control points since each of these would create an extra tiny subbasin between the headwater river segment initiation point and the lake fig 6b such tiny subbasins are unnecessary for hydrological modeling and would increase the computational burden function nd3 will first add lake inlets and outlets into the initial routing structure it distinguishes user provided lake polygons as connected lakes or non connected lakes and only retains lakes with a lake area larger than the user provided lake area threshold basinmaker uses the retained lake polygons to identify lake outlets and inlets and regards them as additional control points a lake outlet is determined as the cell having the highest flow accumulation value within each lake polygon lake inlets are determined from the intersection points among the lake boundary cells and routing network cells basinmaker identifies one lake outlet only for each lake while the number of lake inlets may vary basinmaker will directly use the points of interest input e g a map of point locations of streamflow gauges as control points note that only points of interest within a user specified snapping distance threshold of the delineated river network are automatically included as a control point using the r stream snap function in grass gis afterward an updated routing structure including subbasin polygons is generated based on the updated control points and the flow direction map from function nd2 in fig 3 as shown in fig 6b second function nd3 will identify lakes for which part of the lake area did not belong to the intermediate lake outlet contributing area fig 6b in fig 6b the left part of the lake belongs to a subbasin that does not drain to the lake outlet the right part of the lake is covered by three subbasins green filled subbasins in fig 6b flowing to the lake outlet we define these three subbasins that partially cover the lake and flow to the lake outlet as the partial lake subbasins of this lake third function nd3 will correct the flow direction of cells within the identified lakes as shown in fig 6b the correction needed is to ensure the water in the left part of the lake flows to the lake outlet the correction is achieved with a minimum of processing time by modifying the flow direction of only the lake boundary cells for example any of the cells red boundary cells in fig 6c with their flow directions not pointing towards the three right partial lake subbasins referred to as the un converged boundary cells ubcs are corrected the flow directions of ubcs are corrected in two steps a the flow directions of the two ubc cells that are at the three lake subbasin boundaries cells shown in fig 6c with the yellow color are manipulated to make the water at each ubc flow to one of the adjacent three partial lake subbasins three right lake subbasins in fig 6b b change the flow direction of the remaining ubcs grid cells with the green color in fig 6c to make water in these cells move to the closest yellow cells in fig 6c with this modified flow direction dataset all lake cells now drain to the lake outlet and an updated routing structure will be generated fig 6c next function nd4 in fig 3 estimates physical attributes for each subbasin river channel lake and point of interest as described in the manuscript then function nd5 in fig 3 merges the three partial lake subbasins in fig 6c and generates one lake subbasin completely containing the lake figure b1d when merging partial lake subbasins into one lake subbasin note that the resulting lake subbasin has no river channels subbasin attributes of the partial lake subbasins are carefully merged into a single attribute value for the lake subbasin for example lake subbasin properties like average slope and aspect are computed with an area weighted average of the corresponding partial lake subbasins while lake subbasin area is computed as the sum of the corresponding partial lake subbasins area these four steps ensure that 1 each lake polygon is fully contained by a lake subbasin representing its local contributing area 2 each lake outlet is the outlet of a lake subbasin 3 each lake inlet is represented by a control point of an upstream subbasin thus enabling lake inflow processes to be explicitly represented in a hydrologic model b2 routing network post processing methodology this section introduces the methodologies behind the two functions used for network simplification 1 remove small lakes and 2 decrease river network resolution comparing the intermediate network inputs fig 5a and outputs fig 5c of function remove small lakes demonstrates that the routing network is generally the same but the outputs retain only one lake this function uses the following steps to merge simplify subbasins when a minimum lake area threshold is specified 1 identify lakes in the input routing network with lake area smaller than the user provided lake area thresholds 2 change the lake related attributes appendix a to zero for lakes being removed 3 from upstream to downstream merge subbasins of the intermediate hydrological routing networks fig 5a when the subbasins meet all the following conditions a subbasins are directly connected b they have the same lake attributes they are either lake subbasins overlapping the same lake or non lake subbasins alternately the most downstream subbasin in the group contains a non connected lake and c subbasins all drain to the same river channel 4 update the attribute values of merged subbasins like the function pp5 described in section 2 2 2 attributes of merged subbasins are carefully merged into a single attribute value for the various attribute groups for example select river channel properties are merged using channel length weighted averaging and select subbasins attributes are merged using subbasin area weighted averaging 5 the remove small lakes function will internally call the function combine subbasins covered by the same lake to generate the final hydrological routing networks fig 5d comparing the input fig 5a and output fig 5e routing networks from function decrease river network resolution it is seen that the same lakes are retained but the outputs include only three river channels this function uses the following steps to merge simplify subbasins given minimum subbasin drainage area threshold 1 identify input subbasins fig 5a with a drainage area smaller than the user provided minimum drainage area threshold 2 remove river channels of the identified subbasins 3 change all lakes upstream of the removed subbasin outlet to non connected 4 from upstream to downstream merge subbasin polygons in the intermediate routing networks fig 5a when the subbasins meet all of the following conditions a subbasins are directly connected b they have the same lake attributes they are either lake subbasins overlapping the same lake or non lake subbasins alternately the most downstream subbasin in the group contains a non connected lake c subbasins drain to the same river channel and d upstream subbasins do not include a point of interest 5 update the attribute table of merged subbasins attributes are updated using channel length weighted averaging and subbasin area weighted averaging as described above 6 the decrease river network resolution function will remove river channels from the input intermediate hydrological routing networks by these steps fig 5e and then internally call the function combine subbasins covered by the same lake to generate the final hydrological routing network fig 5f appendix c c1 development of the north american lake river routing product v2 1 we use basinmaker v2 0 to develop version 2 1 of the north american lake river routing product han et al 2021 the input data for this na routing product includes the merit dem yamazaki et al 2017 hydrolakes messager et al 2016 databases for lake polygons points of interest comprising of 7791 streamflow gauges from water survey of canada wsc and 28 164 streamflow gauge from united states geological survey usgs and the global bankfull width depth and discharge estimate for each channel of the finest scale hydrosheds river system andreadis et al 2013 the merit dem is a high accuracy global dem at 3 arcsecond resolution 90 m at the equator developed by eliminating major error components from existing spaceborne dems yamazaki et al 2017 hydrolakes is a database that includes shoreline polygons of all global lakes with a surface area of at least 10 ha messager et al 2016 compared with datasets used to develop the pan canadian lake river routing network version 1 0 han et al 2020 dem resolution is substantially finer now 3 arcseconds instead of 15 arcseconds the na routing product was generated using 1 a constant flow accumulation threshold 2000 cells each 90 m 90 m in size corresponding to 16 km2 to define the initial river network and subbasins 2 a zero lake area threshold to include all lakes from the hydrolakes database with this setup the average subbasin size in the resulting na routing product is roughly 10 km2 in comparison the average subbasin size in the pan canadian lake river routing network is 60 km2 han et al 2020 the channel profile coefficients map for the drainage region above the 60 n latitude from han et al 2020 is used here for the drainage region below the 60 n latitude we inversely calculated the spatial distribution of channel profile coefficients using the global bankfull width depth and discharge product from andreadis et al 2013 the modis land cover type product mcd12q1 friedl and sulla menashe 2015 landuse map is utilized for floodplain roughness estimation along with flood plain manning s coefficients from chow 1959 corresponding to each landuse type the na routing product generated by basinmaker v2 0 does not include additional subbasin control points at the beginning of the headwater river channels defined by the flow accumulation threshold if it is not overlaid by a lake this key difference between v2 0 and v3 0 of basinmaker is depicted in figure c1 also the na routing product generated by basinmaker v2 0 does not include the following attributes that are produced by v3 0 1 the longest flow path of the subbasin s entire upstream drainage area 2 the average slope of the subbasin s entire upstream drainage area 3 the slope of river channel along the longest flow path and 4 the subbasin outlet longitude and latitude fig c1 example vector based routing networks from a basinmaker v3 0 and b basinmaker v2 0 the hydrological routing networks from basinmaker v2 0 do not include additional subbasin control points at the beginning of any of the headwater river channels defined by the flow accumulation threshold while v3 0 does include some of these as described previously in section 2 2 the example watershed includes again a connected lake cl and a non connected lake ncl fig c1 c2 development of the ontario lake and river routing product v1 0 we use basinmaker v3 0 to develop version 1 0 of the ontario lake and river routing product han et al 2022 the ontario routing product covering the province of ontario canada delineates an area of approximately 1 million km2 at a finer spatial resolution than the na routing product the inputs of this application include the enhanced dem and the enhanced flow direction fdr raster layers 30 m 30 m from the ontario integrated hydrology oih datasets hydrolakes messager et al 2016 databases points of interest including 1100 ontario observation gauges from the water survey of canada wsc database 439 active water quality monitoring gauges from the ontario provincial water quality monitoring network pwqmn and 1981 subbasin outlets extracted from the quaternary ontario watershed boundaries parameters provided to basinmaker include the following 1 a constant flow accumulation threshold 5000 cells each 30 m 30m in size corresponding to 4 5 km2 to define the initial river network and subbasins 2 zero as the lake area threshold to include all lakes from the hydrolakes database bankfull width and depth attribute estimation procedures for the ontario routing product are the same as those used in the na routing product see section b1 
25407,this study developed a new process based forest module for the soil and water assessment tool swat based on the physiological process in predicting growth 3 pg model swat 3pg the new model allows for improved biomass assimilation partitioning stem foliage root and losses root turnover foliage loss mortality evaluation at field scale showed that swat 3pg can replicate the different forest biomass components for evergreen forests well testing for deciduous and mixed forests sites using remote sensed data showed that the model can simulate leaf area index lai net primary productivity npp and actual evapotranspiration aet reasonably well and can be used to constrain swat 3pg when lack of field data sensitivity analysis of swat 3pg showed its potential in evaluating the impacts of management and climate on forested ecosystems swat 3pg can also be of importance to forest managers as it can estimate variables such as plant height diameter at breast height dbh and basal area keywords swat 3pg forest simulation biomass carbon modis data availability data will be made available on request software availability name of the software swat 3pg developer ritesh karki and junyu qi contact information junyuqi umd edu year first available 2023 program language fortran 90 cost free software availability https github com riteshkarki swat664 3pg program size 4 05 mb 1 introduction forests play an integral role in hydrologic nutrient and carbon cycling at global regional as well as basin watershed scale forests dominate terrestrial water dynamics as it has significant influence on transpiration rainfall interception percolation and soil moisture dynamics adams et al 1991 dunkerley 2015 lathuillière et al 2012 teuling et al 2019 as a result forests also play a critical role in runoff generation groundwater recharge and the delivery of water from land to riverine and lake systems studies have shown that loss of forest cover can lead to an increase in watershed water yield due to reduction in actual evapotranspiration aet and as a result also increase streamflow jones and post 2004 perry and jones 2017 forests are an integral part of the global carbon cycle as it dominates carbon exchange between the atmosphere and terrestrial biosphere and accounts for about 80 of the global aboveground biomass agb dixon et al 1994 with the increase in atmospheric co2 concentrations and resulting climate change a key global issue forests are an important component of global carbon cycle investigations curtis and gough 2018 detwiler and hall 1988 as it can contribute to carbon sequestration and storage in standing biomass as well as stored wood products the global climate observing system gcos identified agb as one of the 54 essential climate variables because of its role in the global carbon cycle santoro et al 2021 quantifying forest biomass and sequestration potential are also critical from a policy perspective as information on national regional forest biomass is critical to decision making for climate mitigation policies hurtt et al 2019 forests also impart major influence in aquatic ecosystems through the delivery of water sediments nutrients and carbon butman et al 2016 kreutzweiser et al 2008 lee et al 2018 martin et al 2000 prepas et al 2001 as such changes to forest ecosystem including deforestation wildfire pests etc can have significant impact on aquatic carbon fluxes kreutzweiser et al 2008 prepas et al 2001 as forests account for up to 28 and 38 of ecosystem n and p hart et al 2003 respectively it also plays an important role in terrestrial and riverine nutrient balances through uptake and subsequent return to soil through litter and riverine nutrient export accurate representation of soil organic carbon soc and transport from terrestrial to aquatic system to quantify aquatic carbon also necessitates accurate simulation of biomass sequestration storage as well as losses from forested systems butman et al 2016 hydrologic models help us understand the complex relationships between atmospheric terrestrial and aquatic systems for water sediment nutrient and carbon fluxes under different land management land use change climate change etc as a result models are important tools that can aid in policy decision making for water resource management molina navarro et al 2016 d zhang et al 2018 nutrient reduction douglas mankin et al 2010 endale et al 2014 karki et al 2018 as well as understanding and reducing climate impacts diaz and rosenberg 2008 j lee et al 2019 it is however critical to capture the important processes of the different ecosystems in the model wang et al 2020a 2020b the soil and water assessment tool swat is a watershed scale semi distributed process based model neitsch et al 2011 that has been widely applied across the world in varying climate and topographic regions for evaluating the impacts of land management land use change as well as climate haro monteagudo et al 2020 samimi et al 2020 tan et al 2019 2020 upadhyay et al 2022 model improvement effort in recent years has helped further expand the use of swat in additional fields such as soil organic carbon simulation zhang 2018 natural salinity tirabadi et al 2021 as well as soil n2o emissions gao et al 2019 the model is a key component of the u s environmental protection agency usepa hydrologic and water quality systems hawqs hawqs 2020 and u s department of agriculture usda conservation effect assessment project ceap projects and has also been incorporated into the usepa better assessment science integrating point and nonpoint sources basins multipurpose environmental analysis tool us epa 2019 although swat has been widely used with more than 5000 scientific publications and evaluated in over 90 countries https www card iastate edu swat articles the model has had limited applications in forest dominated watersheds due to its limitations in simulating forest biomass and water and nutrient fluxes haas et al 2022 yang and zhang 2016 as the swat model was initially developed with focus on agriculture dominated watersheds the plant growth module in swat is based on a simplified version of the environmental policy integrated climate epic model that was developed for simulating physio chemical processes under agricultural crops and was adjusted minimally for forest growth in the swat model neitsch et al 2011 williams et al 1989 as a result current forest growth module in the swat model has key issues that needs to be addressed for improving forest simulation this can be vital to increasing applicability of the model in forest dominated watersheds along with reducing uncertainty of hydrologic nutrient and carbon simulation from forested regions even in agriculture dominated watersheds as forests make a significant portion of the land use in most watersheds jin et al 2013 there have been studies in recent years that aimed to improve forest simulation in swat but have been often limited to improving forest simulation by improving forest parameterization haas et al 2022 yang and zhang 2016 and any attempts to modify the underlying forest growth algorithms in swat have been mostly focused on modifying how leaf area index lai is simulated alemayehu et al 2017 lai et al 2020 ma et al 2019 strauch and volk 2013 which fails to address key issues in the current module that includes the lack of daily partitioning of assimilated biomass into root stems and leaves lai is solely a function of heat units with no relation to foliage biomass same litterfall routine for evergreen and deciduous forest types lack of accurate forest initialization and no tree mortality there is a wide range of models currently available for forest simulation that are based on empirical regression relationships process based or a combination of both fontes et al 2010 although it is ideally desirable to use comprehensive process based carbon balance models such as biome bgc thornton and running 2000 white et al 2000 or pnet aber and federer 1992 for simulating forest growth the models are very data intensive which makes its usability and compatibility difficult the physiological process in predicting growth 3 pg is a forest growth model that is process based but also uses empirical relationships that greatly simplifies calculations landsberg and waring 1997 the model has been successfully used to simulate forest stand growth for multiple tree species and in a range of climatic conditions amichev et al 2010 2016 gonzalez benecke et al 2014 2016 hn palma et al 2021 the simplicity of 3 pg in model development yet its robustness in simulating forest growth makes it an ideal model to be incorporated into the swat model for improving forest simulation in this study we present an enhanced version of swat model with a new forest module that is developed based on 3 pg herein from referred to as swat 3pg with the goal of better representing forest ecosystems and improving simulation of biomass assimilation partitioning and losses from forests the main objectives are 1 to incorporate the process based 3 pg forest growth model into swat and 2 to test model performance of swat 3pg on simulation of forest biomass and fluxes for different forest types using in situ measurements and remoted sensed data we also discuss the advantages as well as limitations of the new forest module for new model applications we expect the new swat 3pg model to also help reduce uncertainty associated with the estimation of hydrologic nutrient and carbon fluxes from forest systems along with improved estimation of net primary productivity npp net ecosystem productivity nep and soil carbon 2 materials and methods 2 1 model development 2 1 1 forest growth in the default swat model forest growth module in swat is based on a simplified version of the epic model neitsch et al 2011 the model calculates daily intercepted radiation by the leaf area eq s1 which is then used to calculate the potential maximum biomass that can be assimilated each day under optimal conditions using radiation use efficiency rue eq s2 the potential maximum biomass is then constrained to actual growth using constraints based on water temperature and nutrient stresses eqs s3 s7 biomass assimilation in swat within a single year is limited to a fixed amount based on the current age as well as user defined number of years for the tree to reach maturity and tree maximum biomass eq s8 tree growth in a given year is stopped until the next year once the biomass accumulation for the year reaches the maximum limit calculated similarly lai calculation in forest is a function of the potential heat unit accumulated each day along with the current age of tree and age to maturity eq s9 but has no relation to foliage biomass accumulation calculation of canopy height for forests is also based on a user defined maximum tree height along with the current age and age to maturity eq s10 the assimilated biomass for each day is allocated to agb and root the fraction of biomass allocated to root is based on the user defined root to shoot ratio at seeding and maturity along with the potential heat units accumulated to that day for each year eq s11 a fraction of assimilated agb is lost as litterfall each year at the onset of dormancy based on a user defined fraction bio leaf plant dat irrespective of the forest type eq s12 2 1 2 forest growth in 3 pg model 3 pg is a process based stand alone model for forest growth that calculates gross primary productivity gpp based on intercepted utilizable photosynthetically active radiation and a canopy quantum coefficient α c x intercepted photosynthetically active radiation is reduced to utilizable amount using environmental modifiers such as vapor pressure deficit soil water temperature nutrition and atmospheric co2 concentration as well as a decline in growth efficiency due to age the model then uses a constant npp gpp ratio to calculate npp eqs s13 s14 the assimilated biomass is allocated to roots based on growing conditions as well as age and the allocated fraction increases if the site nutrition or the available water is low the remaining assimilated biomass is then partitioned to stem branches herein from referred together as stem and foliage that varies with growing conditions tree age and size and is based on allometric relationships the model uses a sub model derived from the 3 2 power law to calculate density dependent tree mortality self thinning a density independent tree mortality sub routine is also included mortality due to age 3 pg has separate litterfall for deciduous and evergreen forests in that deciduous forests lose all leaves at the onset of dormancy evergreen forests on the other hand have litterfall throughout the year but do not lose all foliage at the onset of dormancy lai calculation is based on assimilated foliage biomass and specific leaf area the model also estimates variables that are important to forest managers including tree height basal area and diameter at breast height dbh initial biomass number of trees per hectare and age are required for the 3 pg model the readers are referred to landsberg and waring 1997 and sands 2010 for the details regarding 3 pg forest growth module calculation of the different growth modifiers biomass partitioning stem mortality litterfall and lai 2 1 3 development of the swat 3pg model the new forest module in swat based on 3 pg consists of three separate sub routines for the simulation of evergreen deciduous and mixed forest systems separately the sub routine for deciduous forest is different from evergreen in that all foliage biomass is lost at the onset of dormancy and biomass assimilation at the beginning of growing season end of dormancy is allocated all to foliage biomass until the total foliage biomass lost at the onset of dormancy the previous year is recovered a limitation of swat using hydrologic response units hrus as the basis of all calculations is that it allows for only one plant type to be growing at any time in an hru this prohibits a realistic simulation of mixed forest systems that requires multiple tree species to be growing simultaneously as a result mixed forest system is simulated by making a slight modification to the deciduous forest module in which only a portion of the total foliage biomass is lost at the onset of dormancy which could be a proxy to the foliage loss only from deciduous trees in a mixed forest ecosystem although only a single tree species is simulated the proposed module for mixed forest system can replicate the average biomass foliage and losses of a mixed forest system biomass assimilation in the new forest module is calculated using equations based on the 3 pg model rather than the swat default module so that already calibrated optimal canopy quantum efficiency α c x eqs s13 s14 for different tree species with 3 pg can be directly incorporated into swat 3pg potential maximum biomass assimilation under optimal conditions in swat 3pg is therefore calculated using α c x rather than rue the growth modifiers constraints were slightly modified in that nutrient stresses nitrogen and phosphorus stresses were used instead of the user defined site fertility rating index used in the 3 pg model frost modifier was removed as swat already prevents biomass accumulation when the average daily temperature is below the user defined minimum temperature below which biomass assimilation does not occur the new equation for calculating assimilated biomass each day is presented in eqs 1 3 eq 1 b i o o p t 0 47 α c x h p h o s y n 10 where b i o o p t is the biomass assimilation under optimal conditions h p h o s y n is the amount of intercepted photosynthetically active radiation and α c x is the optimal canopy quantum efficiency eq 2 f p h y s min f v p d f s w f a g e where f p h y s is the total modifier f v p d is the vapor pressure deficit modifier f s w is soil water modifier and f a g e is age related modifier equations for calculating f v p d f s w and f a g e are provided in eqs s15 s17 eq 3 b i o a c t b i o o p t t s t r s min w s t r s n s t r s p s t r s f p h y s where b i o a c t is the actual biomass assimilation t s t r s is the temperature stress for the given day eq s5 w s t r s is the water stress for the given day eq s4 n s t r s is the nitrogen stress for the given day eq s6 p s t r s is the phosphorus stress for the given day eq s7 and f p h y s is the total modifier assimilated biomass for each day is first partitioned to root fraction based on environmental conditions as well as user defined minimum and maximum npp fraction to roots eqs 4 and 5 the remaining biomass is then partitioned to stem and foliage eqs 6 8 eq 4 n r p r x p r n p r n p r x p r n f p h y s m r o o t where n r is the root fraction for the day p r x and p r n are the maximum and minimum fraction of assimilated biomass to roots f p h y s is the total modifier and m r o o t is calculated using eq 5 eq 5 m r o o t m 0 1 m 0 min w s t r s n s t r s p s t r s where m 0 is the root fraction under poor growing conditions eq 6 n s 1 n r 1 p f s where n s is the stem fraction for the day n r is the root fraction for the day and p f s is calculated using eq 7 eq 7 p f s a d b h t r e e b where a and b are coefficients calculated based on user defined foliage stem partitioning ratio at plant diameter at breast height of 2 cm and 20 cm respectively and d b h t r e e is the current diameter at breast height calculated using allometric relationships eq 8 n f 1 n r n s where n f n r and n s are the foliage root and stem fraction of assimilated biomass for each day the exception for assimilated biomass partitioning for deciduous and mixed forest systems is at the beginning of growing season when all assimilated biomass is assigned to foliage until the total foliage biomass lost at the onset of dormancy is recovered calculations for root turnover foliage loss mortality due to self thinning and age along with the associated biomass losses were incorporated directly from 3 pg modifications were also made to the source code such that forest biomass and age could be initialized for each forest hru using the mgt file which is a requirement with 3 pg the initialized total biomass is partitioned to the three biomass pools for stem roots and leaves based on poorter et al 2012 the dormancy module was modified such that evergreen forests would lose a certain amount of foliage biomass as litterfall during each day of the simulation period based on age while deciduous forests in addition to daily losses lost all foliage at the onset of dormancy a new input file swat3pg 3pg was created to read all 3 pg related input parameters activation key was incorporated in the swat3pg 3pg input file that allowed the user to activate or deactivate tree loss due to mortality as well as self thinning it is important to note that 3 pg runs at a monthly time scale and hence it was necessary to convert all of the parameters with monthly time units to daily when incorporating into the swat3pg 3pg parameter input file the source code was also modified to generate a new output file that lists all the forest outputs 2 2 study sites and data five field sites were selected in the u s to test the new forest module for simulation of evergreen deciduous and mixed forest systems fig 1 evergreen fl 1 evergreen fl 2 and evergreen ga are long term forest productivity evaluation sites and had annual measurements of total stem coarse root and foliage biomass lai and litter fall for loblolly pine pinus taeda evergreen forest type gonzalez benecke et al 2014 2016 all evergreen forest sites were in the appalachian highlands but still varied in climatic and soil conditions table 1 similar long term field measurements were not available for deciduous and mixed forest systems as a result a site in wisconsin wi deciduous wi was identified that was dominated by aspen forests using the u s forest service usfs national forest type dataset and was used to evaluate the deciduous forest module as field measured data were not available the forest module for simulating deciduous forest systems was calibrated and validated for remote sensed npp carbon npp c aet and lai data products and evaluated for simulating forest biomass and its components similarly a mixed forest site mixed va was identified and selected for evaluation in virginia va remote sensed data products lai aet and npp c for the deciduous and mixed forest sites were acquired from moderate resolution imaging spectroradiometer modis datasets justice et al 2002 running et al 2017 table 1 provides the important information for each site 2 3 model setup and evaluation field scale swat models karki et al 2020 were set up for each of the forest sites datasets utilized for setting up the field scale swat models are provided in table 2 field scale swat models for the evergreen sites were developed from the year of initial year of planting to 2020 table 1 even though the observed datasets did not extend to the year 2020 this allowed for comparison of model simulated forest biomass against observed values along with the evaluation of long term forest simulation with the new forest module the forest module for evergreen forest simulation was calibrated at evergreen fl 1 site and validated at the remaining two evergreen sites evergreen fl 2 and evergreen ga fig 2 model validation was performed by transferring only the calibrated forest parameters to the two validation sites and no additional adjustments were made for model simulation field scale models for the deciduous deciduous wi and mixed forest mixed va sites were developed from 2000 to 2020 with the model calibrated using remote sensed data from 2001 to 2010 and validated from 2011 to 2020 at the same sites fig 2 as the deciduous wi and mixed va had existing forests the initial forest age and biomass required for model initialization was acquired from williams et al 2020 model performance evaluation for the simulation of different field measured data in the evergreen sites were limited to graphical comparison between simulated and observed data due to temporal data limitations of one data point for each year fig 2 the deciduous and mixed sites were evaluated using graphical as well as statistical methods as monthly remote sensed data was available from 2001 to 2020 statistical evaluation was performed by calculating the coefficient of determination r2 nash sutcliffe efficiency nse and percent bias pbias which are commonly used measures for evaluating model performance in hydrologic and water quality modeling moriasi et al 2015 fig 2 details the model calibration and validation approach for the simulation of the three forest types using swat 3pg 2 4 swat 3pg model sensitivity analysis multiple scenario runs were performed to evaluate how the new swat 3pg forest module responded for forest simulation under different growing conditions as well as forest initialization the different scenarios simulated and evaluated to test swat 3pg sensitivity are listed in table 3 all the scenario runs were performed at the evergreen fl 1 site and the calibrated model run was used as the baseline against which all scenarios were evaluated fig 2 the scenario runs are important to understand if the new module can provide a response in forest growth under varying growth stresses and initial forest growth conditions which will be critical in using the swat 3pg model for forest management climate and land use change scenario evaluations 3 results 3 1 swat 3pg simulation of evergreen forests initial model parameters for swat 3pg simulation of the evergreen forest site with loblolly pine evergreen fl 1 were acquired from literature bryars et al 2013 gonzalez benecke et al 2016 subedi et al 2015 after which a sensitivity analysis for parameters was performed using r swat nguyen et al 2022 and model calibration was performed using automated r swat as well as manual approaches calibrated model parameters for swat 3pg for the simulation of evergreen forest at evergreen fl 1 site is presented in table s1 graphical comparison between swat 3pg simulated forest biomass components for the evergreen fl 1 site after calibration and observed data is presented in fig 3 it can be seen from the figure that the new forest module successfully replicates the observed trend as well as magnitude in the simulation of total stem and foliage biomass along with foliage loss and lai fig 3 the new forest module was able to capture very well the observed trend of biomass assimilation in total and stem biomass from planting to the juvenile years fig 3a and b swat 3pg was also able to capture the trend of reduction stabilization in total biomass assimilation after the year 2004 with the activation of mortality due to self thinning that can be simulated by the new forest module fig 3a the forest module also replicated well the quick accumulation of foliage biomass in the first few years after planning and the stabilization as well as slight reduction in foliage biomass afterwards fig 3c annual foliage loss was also captured well by the new forest module fig 3e foliage loss in evergreen forests with the new forest module happens throughout the year from the foliage biomass pool unlike the default forest module in swat which makes a proxy foliage biomass loss by removing a certain user defined fraction of biomass from the agb pool at the onset of dormancy the new forest module was also able to show temporal variability in the accumulation of foliage biomass the effect of which can be observed in the slight variability in the assimilation of annual biomass with accurate simulation of foliage biomass and losses the new forest module also matched the observed lai well fig 3f root biomass was the only forest biomass component that the new module was not able to replicate closely fig 3d comparison against observed data shows that the model was able to capture the trend of observed root biomass well in that the accumulation of coarse root biomass was linear in the early years which stabilized later fig 3d the model however over simulated the magnitude of coarse root biomass assimilated in the early years and the assimilation of coarse root biomass also stabilized earlier than observed it is important to note that the observed root biomass only included coarse root biomass and fine root biomass was not accounted for this could have resulted in the underreporting of observed total root biomass and potentially explains the over simulation of root biomass by the new forest module in the early years comparison of model simulated biomass components at the two validation sites evergreen ga and evergreen fl 2 against field measured observed dataset shows that the model performed reasonably well at both sites fig 4 fig 5 although the model tended to overestimate the forest biomass components at the evergreen ga site fig 4 and underestimate at the evergreen fl 2 site fig 5 the model simulated forest biomass closely matched the field measured values and also followed the observed trend well at both validation sites it is worth noting that the calibration and validation sites had different climatic and soil conditions table 1 and the validation run was performed by incorporating only the calibrated swat 3pg forest parameters from the calibration site this can help increase confidence in the new forest module to simulate forest biomass over a large spatial domain with varying climate and soil conditions as the new forest module performed well by only incorporating the calibrated forest parameters from the calibration site comparison of the observed dataset especially foliage biomass lai and foliage loss at the two validation sites showed that the observed datasets at evergreen fl 2 site fig 5 had more variability when compared to evergreen ga site fig 4 which was captured well by the new forest module simulated foliage biomass lai and foliage loss at the evergreen fl 2 site fig 5c e and 5f had high temporal variability while the evergreen ga fig 4c e and 4f showed minimal variability consistent with the observed datasets the variability in the simulation of different forest biomass components in the calibration and validation sites also helps reinforce that the model can respond to varying forest growing conditions which will be critical when using the swat 3pg at larger spatial domain with varying climatic and soil conditions 3 2 swat 3pg simulation of deciduous and mixed forest as the deciduous wi site was dominated by aspen populus tremuloides initial parameters for swat 3pg were derived from amichev et al 2010 which estimated 3 pg parameters for hybrid poplar tree species calibrated swat 3pg parameters for the simulation of deciduous wi site is presented in table s1 comparison of swat 3pg simulated lai aet and npp c against modis estimated data for the deciduous forest site deciduous wi during calibration and validation is presented in fig 6 it can be seen from the figure that swat 3pg does a good job of capturing the observed variability for all three variables during the calibration 2011 2010 and validation period 2011 2020 r2 and nse of greater than 0 5 for the simulation of all three variables lai aet and npp c during both calibration and validation table 4 indicates that swat 3pg can simulate the three variables well for deciduous forest systems pbias for lai and aet was less than 20 for both calibration and validation indicating a good model fit pbias for npp c was however 26 9 during model validation period table 4 indicating to the model having slight difficulty in replicating the remote sensed estimated npp c an additional model run performed for the deciduous wi site using calibrated swat 3pg parameters but forest initialized at planting rather than already growing as initialized during calibration to evaluate how the calibrated model parameters would simulate forest from planting model results evaluation showed that swat 3pg was able to simulate realistic forest biomass components including total stem and foliage biomass as well as lai fig 7 with the calibrated parameters this shows that the use of remote sensed data products lai aet npp c can be important and helpful datasets for constraining swat 3pg parameters for forest simulation especially when performing model simulations at regional watershed scale and there is a lack of field observed data for different forest types as the mixed forest site had multiple forest species initial parameters used for the deciduous wi site were also used as the initial parameters for this site comparison of model simulated monthly lai aet and npp c against modis estimated for the mixed forest site is shown in fig 8 the new module simulated lai and aet well with a close match between simulated and remote sensed variables fig 8 r2 and nse was greater than 0 5 and pbias was less than 10 during both calibration and validation periods table 5 a close match between simulated and observed lai and aet shows that the module for simulation of mixed forests which represents mixed forest systems using a single plant type due to the limitation in swat model can still replicate the spatial as well as temporal variability in lai and aet of the mixed forest system as a whole evaluation of npp c however showed that the mixed forest module had difficulty in replicating the remote sensed estimated npp c swat 3pg was able to capture the observed trend in the assimilation of npp c well fig 8 but the model was not able to capture the observed magnitude in npp c assimilation with the model under simulating npp c during both calibration and validation periods nse 0 1 and pbias 40 it should be noted that swat 3pg simulates mixed forest system similar to a deciduous forest with a single forest type but loses only a portion of foliage biomass as litterfall to replicate the loss of foliage from deciduous trees this limit in swat 3pg does not allow for biomass assimilation of different tree species in a mixed forest system to be simulated separately but is estimated by calibrating for biomass assimilation that is representative of the mixed forest system which can lead to difficulty in the model replicating the observed npp c a better starting parameters by identifying the major forest types in the mixed forest site could also have potentially helped in better npp c simulation evaluation of modis estimated npp c for the mixed site also showed npp c assimilation in the winter months possibly from forest understory which was not captured by the model with no npp c simulation in the winter months this also contributed to the poor performances measure for npp c simulation for the mixed va site it is also equally important to acknowledge the inconsistencies and uncertainties associated with npp c datasets when evaluating against model simulated values xie et al 2020 zhao et al 2006 an additional model run using the calibrated parameters was also performed for the mixed va site but with forest initialized as planting to evaluate the impact of the calibrated parameters on forest simulation the new forest module for mixed forest simulation in swat 3pg was also able to simulate the important components of a mixed forest system including temporal variability in lai foliage biomass loss and total stem and root biomass assimilation fig 9 this shows that remote sensed data products can be successfully used to constrain parameters for the simulation of mixed forest systems in swat 3pg 3 3 swat 3pg model sensitivity for forest simulation sensitivity of the swat 3pg model to forest biomass simulation due to changes in rainfall nitrate forest initialization and self thinning age mortality is presented in fig 10 model scenario runs with rainfall showed that swat 3pg is sensitive to changes in rainfall a reduction in daily rainfall by 30 led to a reduction in total biomass assimilation by an average of 8 when compared to baseline the reduction in biomass assimilation at the end of the simulation period was close to 12 mg ha 4 fig 10a evaluation of the whole simulation period showed that the reduction in biomass assimilation in the middle years from 1999 to 2006 when the forest is juvenile and actively assimilating biomass was much higher 13 increasing daily rainfall by 30 showed only a slight increase in annual biomass assimilation with an average of 2 the maximum increase in biomass assimilation was again observed in the middle years 5 but the difference at the end of the simulation period was only 2 4 84 mg ha fig 10a scenario runs involving changes to nitrogen availability demonstrated the model s sensitivity to change in nutrient levels a slight increase in assimilated forest biomass was observed when doubling the atmospheric deposition of nitrate fig 10b application of nitrate as fertilizer each year 200 kg ha however led to substantial increase in forest biomass assimilation the increase in biomass at the end of the simulation period was close to 12 35 mg ha but the increase was much higher in the middle years at close to 18 40 mg ha a scenario run with already growing forest of 10 years and initial biomass of 130 mg ha showed that the model stabilizes the forest biomass assimilation much faster than when initializing the model at planting due to earlier self thinning fig 10c this shows that the swat 3pg model can vary forest biomass assimilation based on the initialized age and biomass the importance of incorporating mortality due to age as well as self thinning for realistic assimilation of biomass especially when the forest reaches towards maturity can be visualized with the model scenario run that turned off mortality due to age as well as self thinning fig 10d turning off mortality due to age alone had minimal change in forest biomass assimilation which could be potentially attributed to the model run of only 34 years turning off mortality due to self thinning however led to a somewhat linear increase in biomass till the end of the simulation period which can be considered unrealistic fig 10d the reduction stabilization in biomass assimilation after 2006 is due to self thinning sub routine in the swat 3pg model which reduces the number of trees ha based on a 3 2 power rule when the average stem biomass becomes higher than a user defined threshold this also shows the importance of making sure to activate the self thinning sub routine when simulating forest biomass with swat 3pg 4 discussion 4 1 swat 3pg for forest simulation swat 3pg improves on the default swat forest module by incorporating important processes for forest simulation that were missing in the default module swat 3pg partitions assimilated biomass into stem root and foliage based on growth conditions and age which with the default model was partitioned only into above ground and root biomass lai as a result in swat 3pg is a function of assimilated foliage biomass which was calculated only based on accumulated heat units with the default model evaluation of swat simulated biomass and lai with the default forestry module and default parameters in the evergreen fl 1 site fig s1 shows that the model simulated the lai well but poorly simulated total biomass which could potentially be attributed to the lack of check and balance between biomass assimilation and lai simulation with the default model calibration of the default swat model parameters for forest simulation could better match the observed total biomass in the evergreen fl 1 site but model would not be well constrained as it does not simulate and provide output for variables such as foliage biomass stem biomass tree mortality evaluation of swat 3pg at the evergreen deciduous and mixed forest sites shows that the new forestry module can accurately simulate the different forest biomass components by incorporating important processes that were missing in the default swat it is important to note that the default swat module only tracks agb and does not partition agb to stem and foliage biomass similarly the default swat model also does not report root biomass swat 3pg also improves litterfall simulation with realistic representation of litterfall from evergreen forests when compared to the default module fig s1b and also shows temporal variability in lai simulation which was not captured by the default module fig s1b swat 3pg also simulates tree mortality due to age density independent and self thinning density dependent for all forest types which is a more realistic way of constraining biomass than the default swat forestry module which uses a user defined maximum biomass along with the current age of tree and tree age to maturity to limit maximum biomass growth for each year of the simulation period this will contribute to a more realistic approach of biomass stabilization in mature tree forests as well as help in improving the simulation of soc along with lateral carbon and nutrient fluxes to the inland aquatic environment from forest ecosystems as swat 3pg was able to show good variability in the temporal simulation of lai the model should also benefit with improved simulation of hydrology due to improvement in aet simulation comparison against field observations as well as modis npp c shows that the forest module can replicate observed biomass assimilation from forest systems and as the forest module was able to simulate realistic stabilization in biomass assimilation at maturity even with different forest initialization swat 3pg can be an important tool for answering carbon sequestration and storage questions from forest systems the sensitivity of swat 3pg forest growth to climate nutrient and forest initialization also demonstrates its value in evaluating the impacts of climate and management scenarios in forest systems as swat 3pg provides additional outputs such as stock density dbh height basal area stem biomass etc which were not available with the default model swat 3pg can be a tool for forest management evaluation even for commercial purposes with improved simulation of forest ecosystems swat 3pg can be helpful in reducing hydrologic and water quality simulation uncertainties from forested regions which can be important in accurately understanding the physical and chemical processes along with the impacts of multiple scenarios in agriculture dominated watersheds evaluation of different forest age and biomass initialization shows that biomass assimilation is sensitive to forest initialization hence it is important to utilize available resources such as williams et al 2020 and make sure that forest age and biomass is initialized accurately when using swat 3pg an advantage of swat 3pg is that forest parameterization already performed in many 3 pg studies can be leveraged as initial parameters into swat 3pg by evaluating for the dominant forest species in the watershed of interest with accurate forest initialization of age biomass and tree species along with the use of existing 3 pg studies for initial parameterization and leveraging remote sensed data products such as modis for calibration swat 3pg can be an important tool for estimating current and near future carbon stock in forested systems at a regional watershed scale which can be valuable information to planners and policymakers for climate mitigation 4 2 swat 3pg limitations and future development although swat 3pg improved on the simulation of forest ecosystems and provided important additional model outputs when compared to the default forest module in swat there are important limitations that should be considered when using the enhanced model as evident from the simulation results of mixed forest sites it needs to be understood that swat 3pg simulates mixed forest systems using a single plant type due to the limitations in the swat hru approach for model simulation in addition it is important to note that swat 3pg cannot simulate forest succession as such care should be taken when using the swat 3pg for multi century model runs as npp c assimilation in swat 3pg decreases when forest reaches maturity when in reality a forest succession due to natural anthropogenic events is more likely although swat 3pg improves on the litterfall simulation of the default model foliage loss at the onset of dormancy for deciduous and mixed forest systems is simulated on a single day while it happens gradually and could last for multiple weeks in reality it was also observed that swat 3pg simulation of root biomass can be improved as it was not able to accurately represent the observed temporal variability in root biomass an important consideration when using swat 3pg is the initialization of forest age and biomass as swat 3pg requires initialization of forest age and biomass care should be taken when initializing forests especially in watersheds with mature forests as evident from the swat 3pg sensitivity runs biomass assimilation is sensitive to forest age and biomass and it is important to get accurate forest age estimates when initializing the model without which it could be difficult to replicate the estimated biomass and npp c using remote sensed data products incorporating forest succession into the swat 3pg model would be beneficial for using the model for multi century model runs and evaluating long term climate impacts similarly improving the litterfall routine for deciduous forests could be helpful in evaluating the model in a finer temporal scale a new mixed forest subroutine that allows for multiple forest species to grow with competition between the species can be important to improve swat 3pg simulation in mixed forest systems as swat 3pg provides model outputs that are also beneficial for forest managers incorporating forest management practices into the model that are used by forest managers can be beneficial in expanding the model usability to other sectors an important advantage of swat 3pg is that the new innovations in the 3 pg model can be easily incorporated into the model as a result any advancement in 3 pg will also be beneficial in improving swat 3pg it will be important to evaluate the swat 3pg s performance for forest simulation over a large spatial domain with multiple forest species climate and soil to improve confidence in the new forest module which will be critical for using the tool in policy decision making the impact of improved swat 3pg forest simulation on hydrology and water quality including carbon over a large spatial domain also needs to be evaluated with the advancement in the data types and spatial resolution of remote sensed datasets modification to swat 3pg to incorporate remote sensed datasets directly into the model for leveraging these datasets can be an important next step in improving model simulation 5 conclusions this study developed and tested a new forest module for the swat model that is based on 3 pg swat 3pg and improves on the default forest module with improved biomass assimilation partitioning and losses for evergreen deciduous and mixed forest systems the new forest module partitions assimilated biomass into stem foliage and root biomass and also simulates tree mortality due to age as well as self thinning which was not possible with the default module lai in the new forest module is related to foliage biomass rather than heat units as simulated in the default module which allows for improved temporal variability in biomass assimilation modification to the litterfall routine with separate litterfall for evergreen deciduous and mixed forest systems allows for improved litterfall simulation from forest ecosystems evaluation at site scale for evergreen forest using field measured data showed that the new forest module can adequately simulate stem foliage and coarse root biomass along with lai and foliage loss deciduous and mixed forest sites were calibrated against remote sensed lai npp c and aet datasets due to the lack of field data for the two forest types evaluation of the new forest module for deciduous and mixed forest sites shows that the new forest module could adequately replicate lai npp c and aet at deciduous forest site but the new module had slight difficulty in replicating npp c at mixed forest site the difficulty in accurately simulating npp c at mixed forest site could potentially be attributed to swat s inability to simulate multiple plant types in a single hru but improvement in npp c simulation should be achieved with improved initial parameterization for simulation of mixed forest sites assessment of model simulated stem foliage and root biomass as well as lai with calibrated swat 3pg parameters constrained using remote sensed products shows that the remote sensed products could be a valuable asset for constraining parameters for forest simulation with swat 3pg over a large spatial domain when field measured datasets are not available this along with the ability to initialize already existing forests in swat 3pg should allow the new model for improved estimation of carbon stock in forested ecosystems over regional domain as swat 3pg also estimates variables such as dbh height and stand basal area the new model can also be of importance to forest managers and growers model sensitivity analysis of swat 3pg showed that the model can also be a useful tool for evaluating climate and management impacts in forested ecosystems as well as at a watershed scale with reduced uncertainty when compared to the default model declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgment the funding support for this study was provided by the u s department of agriculture the national institute of food and agriculture 2021 67019 33684 and 2023 67019 39221 and the national aeronautics and space administration nnx17ae66g and 80nssc20k0060 this research was supported in part by the u s department of agriculture agricultural research service usda is an equal opportunity provider and employer mention of trade names or commercial products in this publication is solely for the purpose of providing specific information and does not imply recommendation or endorsement by the u s department of agriculture appendix a supplementary data the following is the supplementary data to this article multimedia component 1 multimedia component 1 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2023 105705 
25407,this study developed a new process based forest module for the soil and water assessment tool swat based on the physiological process in predicting growth 3 pg model swat 3pg the new model allows for improved biomass assimilation partitioning stem foliage root and losses root turnover foliage loss mortality evaluation at field scale showed that swat 3pg can replicate the different forest biomass components for evergreen forests well testing for deciduous and mixed forests sites using remote sensed data showed that the model can simulate leaf area index lai net primary productivity npp and actual evapotranspiration aet reasonably well and can be used to constrain swat 3pg when lack of field data sensitivity analysis of swat 3pg showed its potential in evaluating the impacts of management and climate on forested ecosystems swat 3pg can also be of importance to forest managers as it can estimate variables such as plant height diameter at breast height dbh and basal area keywords swat 3pg forest simulation biomass carbon modis data availability data will be made available on request software availability name of the software swat 3pg developer ritesh karki and junyu qi contact information junyuqi umd edu year first available 2023 program language fortran 90 cost free software availability https github com riteshkarki swat664 3pg program size 4 05 mb 1 introduction forests play an integral role in hydrologic nutrient and carbon cycling at global regional as well as basin watershed scale forests dominate terrestrial water dynamics as it has significant influence on transpiration rainfall interception percolation and soil moisture dynamics adams et al 1991 dunkerley 2015 lathuillière et al 2012 teuling et al 2019 as a result forests also play a critical role in runoff generation groundwater recharge and the delivery of water from land to riverine and lake systems studies have shown that loss of forest cover can lead to an increase in watershed water yield due to reduction in actual evapotranspiration aet and as a result also increase streamflow jones and post 2004 perry and jones 2017 forests are an integral part of the global carbon cycle as it dominates carbon exchange between the atmosphere and terrestrial biosphere and accounts for about 80 of the global aboveground biomass agb dixon et al 1994 with the increase in atmospheric co2 concentrations and resulting climate change a key global issue forests are an important component of global carbon cycle investigations curtis and gough 2018 detwiler and hall 1988 as it can contribute to carbon sequestration and storage in standing biomass as well as stored wood products the global climate observing system gcos identified agb as one of the 54 essential climate variables because of its role in the global carbon cycle santoro et al 2021 quantifying forest biomass and sequestration potential are also critical from a policy perspective as information on national regional forest biomass is critical to decision making for climate mitigation policies hurtt et al 2019 forests also impart major influence in aquatic ecosystems through the delivery of water sediments nutrients and carbon butman et al 2016 kreutzweiser et al 2008 lee et al 2018 martin et al 2000 prepas et al 2001 as such changes to forest ecosystem including deforestation wildfire pests etc can have significant impact on aquatic carbon fluxes kreutzweiser et al 2008 prepas et al 2001 as forests account for up to 28 and 38 of ecosystem n and p hart et al 2003 respectively it also plays an important role in terrestrial and riverine nutrient balances through uptake and subsequent return to soil through litter and riverine nutrient export accurate representation of soil organic carbon soc and transport from terrestrial to aquatic system to quantify aquatic carbon also necessitates accurate simulation of biomass sequestration storage as well as losses from forested systems butman et al 2016 hydrologic models help us understand the complex relationships between atmospheric terrestrial and aquatic systems for water sediment nutrient and carbon fluxes under different land management land use change climate change etc as a result models are important tools that can aid in policy decision making for water resource management molina navarro et al 2016 d zhang et al 2018 nutrient reduction douglas mankin et al 2010 endale et al 2014 karki et al 2018 as well as understanding and reducing climate impacts diaz and rosenberg 2008 j lee et al 2019 it is however critical to capture the important processes of the different ecosystems in the model wang et al 2020a 2020b the soil and water assessment tool swat is a watershed scale semi distributed process based model neitsch et al 2011 that has been widely applied across the world in varying climate and topographic regions for evaluating the impacts of land management land use change as well as climate haro monteagudo et al 2020 samimi et al 2020 tan et al 2019 2020 upadhyay et al 2022 model improvement effort in recent years has helped further expand the use of swat in additional fields such as soil organic carbon simulation zhang 2018 natural salinity tirabadi et al 2021 as well as soil n2o emissions gao et al 2019 the model is a key component of the u s environmental protection agency usepa hydrologic and water quality systems hawqs hawqs 2020 and u s department of agriculture usda conservation effect assessment project ceap projects and has also been incorporated into the usepa better assessment science integrating point and nonpoint sources basins multipurpose environmental analysis tool us epa 2019 although swat has been widely used with more than 5000 scientific publications and evaluated in over 90 countries https www card iastate edu swat articles the model has had limited applications in forest dominated watersheds due to its limitations in simulating forest biomass and water and nutrient fluxes haas et al 2022 yang and zhang 2016 as the swat model was initially developed with focus on agriculture dominated watersheds the plant growth module in swat is based on a simplified version of the environmental policy integrated climate epic model that was developed for simulating physio chemical processes under agricultural crops and was adjusted minimally for forest growth in the swat model neitsch et al 2011 williams et al 1989 as a result current forest growth module in the swat model has key issues that needs to be addressed for improving forest simulation this can be vital to increasing applicability of the model in forest dominated watersheds along with reducing uncertainty of hydrologic nutrient and carbon simulation from forested regions even in agriculture dominated watersheds as forests make a significant portion of the land use in most watersheds jin et al 2013 there have been studies in recent years that aimed to improve forest simulation in swat but have been often limited to improving forest simulation by improving forest parameterization haas et al 2022 yang and zhang 2016 and any attempts to modify the underlying forest growth algorithms in swat have been mostly focused on modifying how leaf area index lai is simulated alemayehu et al 2017 lai et al 2020 ma et al 2019 strauch and volk 2013 which fails to address key issues in the current module that includes the lack of daily partitioning of assimilated biomass into root stems and leaves lai is solely a function of heat units with no relation to foliage biomass same litterfall routine for evergreen and deciduous forest types lack of accurate forest initialization and no tree mortality there is a wide range of models currently available for forest simulation that are based on empirical regression relationships process based or a combination of both fontes et al 2010 although it is ideally desirable to use comprehensive process based carbon balance models such as biome bgc thornton and running 2000 white et al 2000 or pnet aber and federer 1992 for simulating forest growth the models are very data intensive which makes its usability and compatibility difficult the physiological process in predicting growth 3 pg is a forest growth model that is process based but also uses empirical relationships that greatly simplifies calculations landsberg and waring 1997 the model has been successfully used to simulate forest stand growth for multiple tree species and in a range of climatic conditions amichev et al 2010 2016 gonzalez benecke et al 2014 2016 hn palma et al 2021 the simplicity of 3 pg in model development yet its robustness in simulating forest growth makes it an ideal model to be incorporated into the swat model for improving forest simulation in this study we present an enhanced version of swat model with a new forest module that is developed based on 3 pg herein from referred to as swat 3pg with the goal of better representing forest ecosystems and improving simulation of biomass assimilation partitioning and losses from forests the main objectives are 1 to incorporate the process based 3 pg forest growth model into swat and 2 to test model performance of swat 3pg on simulation of forest biomass and fluxes for different forest types using in situ measurements and remoted sensed data we also discuss the advantages as well as limitations of the new forest module for new model applications we expect the new swat 3pg model to also help reduce uncertainty associated with the estimation of hydrologic nutrient and carbon fluxes from forest systems along with improved estimation of net primary productivity npp net ecosystem productivity nep and soil carbon 2 materials and methods 2 1 model development 2 1 1 forest growth in the default swat model forest growth module in swat is based on a simplified version of the epic model neitsch et al 2011 the model calculates daily intercepted radiation by the leaf area eq s1 which is then used to calculate the potential maximum biomass that can be assimilated each day under optimal conditions using radiation use efficiency rue eq s2 the potential maximum biomass is then constrained to actual growth using constraints based on water temperature and nutrient stresses eqs s3 s7 biomass assimilation in swat within a single year is limited to a fixed amount based on the current age as well as user defined number of years for the tree to reach maturity and tree maximum biomass eq s8 tree growth in a given year is stopped until the next year once the biomass accumulation for the year reaches the maximum limit calculated similarly lai calculation in forest is a function of the potential heat unit accumulated each day along with the current age of tree and age to maturity eq s9 but has no relation to foliage biomass accumulation calculation of canopy height for forests is also based on a user defined maximum tree height along with the current age and age to maturity eq s10 the assimilated biomass for each day is allocated to agb and root the fraction of biomass allocated to root is based on the user defined root to shoot ratio at seeding and maturity along with the potential heat units accumulated to that day for each year eq s11 a fraction of assimilated agb is lost as litterfall each year at the onset of dormancy based on a user defined fraction bio leaf plant dat irrespective of the forest type eq s12 2 1 2 forest growth in 3 pg model 3 pg is a process based stand alone model for forest growth that calculates gross primary productivity gpp based on intercepted utilizable photosynthetically active radiation and a canopy quantum coefficient α c x intercepted photosynthetically active radiation is reduced to utilizable amount using environmental modifiers such as vapor pressure deficit soil water temperature nutrition and atmospheric co2 concentration as well as a decline in growth efficiency due to age the model then uses a constant npp gpp ratio to calculate npp eqs s13 s14 the assimilated biomass is allocated to roots based on growing conditions as well as age and the allocated fraction increases if the site nutrition or the available water is low the remaining assimilated biomass is then partitioned to stem branches herein from referred together as stem and foliage that varies with growing conditions tree age and size and is based on allometric relationships the model uses a sub model derived from the 3 2 power law to calculate density dependent tree mortality self thinning a density independent tree mortality sub routine is also included mortality due to age 3 pg has separate litterfall for deciduous and evergreen forests in that deciduous forests lose all leaves at the onset of dormancy evergreen forests on the other hand have litterfall throughout the year but do not lose all foliage at the onset of dormancy lai calculation is based on assimilated foliage biomass and specific leaf area the model also estimates variables that are important to forest managers including tree height basal area and diameter at breast height dbh initial biomass number of trees per hectare and age are required for the 3 pg model the readers are referred to landsberg and waring 1997 and sands 2010 for the details regarding 3 pg forest growth module calculation of the different growth modifiers biomass partitioning stem mortality litterfall and lai 2 1 3 development of the swat 3pg model the new forest module in swat based on 3 pg consists of three separate sub routines for the simulation of evergreen deciduous and mixed forest systems separately the sub routine for deciduous forest is different from evergreen in that all foliage biomass is lost at the onset of dormancy and biomass assimilation at the beginning of growing season end of dormancy is allocated all to foliage biomass until the total foliage biomass lost at the onset of dormancy the previous year is recovered a limitation of swat using hydrologic response units hrus as the basis of all calculations is that it allows for only one plant type to be growing at any time in an hru this prohibits a realistic simulation of mixed forest systems that requires multiple tree species to be growing simultaneously as a result mixed forest system is simulated by making a slight modification to the deciduous forest module in which only a portion of the total foliage biomass is lost at the onset of dormancy which could be a proxy to the foliage loss only from deciduous trees in a mixed forest ecosystem although only a single tree species is simulated the proposed module for mixed forest system can replicate the average biomass foliage and losses of a mixed forest system biomass assimilation in the new forest module is calculated using equations based on the 3 pg model rather than the swat default module so that already calibrated optimal canopy quantum efficiency α c x eqs s13 s14 for different tree species with 3 pg can be directly incorporated into swat 3pg potential maximum biomass assimilation under optimal conditions in swat 3pg is therefore calculated using α c x rather than rue the growth modifiers constraints were slightly modified in that nutrient stresses nitrogen and phosphorus stresses were used instead of the user defined site fertility rating index used in the 3 pg model frost modifier was removed as swat already prevents biomass accumulation when the average daily temperature is below the user defined minimum temperature below which biomass assimilation does not occur the new equation for calculating assimilated biomass each day is presented in eqs 1 3 eq 1 b i o o p t 0 47 α c x h p h o s y n 10 where b i o o p t is the biomass assimilation under optimal conditions h p h o s y n is the amount of intercepted photosynthetically active radiation and α c x is the optimal canopy quantum efficiency eq 2 f p h y s min f v p d f s w f a g e where f p h y s is the total modifier f v p d is the vapor pressure deficit modifier f s w is soil water modifier and f a g e is age related modifier equations for calculating f v p d f s w and f a g e are provided in eqs s15 s17 eq 3 b i o a c t b i o o p t t s t r s min w s t r s n s t r s p s t r s f p h y s where b i o a c t is the actual biomass assimilation t s t r s is the temperature stress for the given day eq s5 w s t r s is the water stress for the given day eq s4 n s t r s is the nitrogen stress for the given day eq s6 p s t r s is the phosphorus stress for the given day eq s7 and f p h y s is the total modifier assimilated biomass for each day is first partitioned to root fraction based on environmental conditions as well as user defined minimum and maximum npp fraction to roots eqs 4 and 5 the remaining biomass is then partitioned to stem and foliage eqs 6 8 eq 4 n r p r x p r n p r n p r x p r n f p h y s m r o o t where n r is the root fraction for the day p r x and p r n are the maximum and minimum fraction of assimilated biomass to roots f p h y s is the total modifier and m r o o t is calculated using eq 5 eq 5 m r o o t m 0 1 m 0 min w s t r s n s t r s p s t r s where m 0 is the root fraction under poor growing conditions eq 6 n s 1 n r 1 p f s where n s is the stem fraction for the day n r is the root fraction for the day and p f s is calculated using eq 7 eq 7 p f s a d b h t r e e b where a and b are coefficients calculated based on user defined foliage stem partitioning ratio at plant diameter at breast height of 2 cm and 20 cm respectively and d b h t r e e is the current diameter at breast height calculated using allometric relationships eq 8 n f 1 n r n s where n f n r and n s are the foliage root and stem fraction of assimilated biomass for each day the exception for assimilated biomass partitioning for deciduous and mixed forest systems is at the beginning of growing season when all assimilated biomass is assigned to foliage until the total foliage biomass lost at the onset of dormancy is recovered calculations for root turnover foliage loss mortality due to self thinning and age along with the associated biomass losses were incorporated directly from 3 pg modifications were also made to the source code such that forest biomass and age could be initialized for each forest hru using the mgt file which is a requirement with 3 pg the initialized total biomass is partitioned to the three biomass pools for stem roots and leaves based on poorter et al 2012 the dormancy module was modified such that evergreen forests would lose a certain amount of foliage biomass as litterfall during each day of the simulation period based on age while deciduous forests in addition to daily losses lost all foliage at the onset of dormancy a new input file swat3pg 3pg was created to read all 3 pg related input parameters activation key was incorporated in the swat3pg 3pg input file that allowed the user to activate or deactivate tree loss due to mortality as well as self thinning it is important to note that 3 pg runs at a monthly time scale and hence it was necessary to convert all of the parameters with monthly time units to daily when incorporating into the swat3pg 3pg parameter input file the source code was also modified to generate a new output file that lists all the forest outputs 2 2 study sites and data five field sites were selected in the u s to test the new forest module for simulation of evergreen deciduous and mixed forest systems fig 1 evergreen fl 1 evergreen fl 2 and evergreen ga are long term forest productivity evaluation sites and had annual measurements of total stem coarse root and foliage biomass lai and litter fall for loblolly pine pinus taeda evergreen forest type gonzalez benecke et al 2014 2016 all evergreen forest sites were in the appalachian highlands but still varied in climatic and soil conditions table 1 similar long term field measurements were not available for deciduous and mixed forest systems as a result a site in wisconsin wi deciduous wi was identified that was dominated by aspen forests using the u s forest service usfs national forest type dataset and was used to evaluate the deciduous forest module as field measured data were not available the forest module for simulating deciduous forest systems was calibrated and validated for remote sensed npp carbon npp c aet and lai data products and evaluated for simulating forest biomass and its components similarly a mixed forest site mixed va was identified and selected for evaluation in virginia va remote sensed data products lai aet and npp c for the deciduous and mixed forest sites were acquired from moderate resolution imaging spectroradiometer modis datasets justice et al 2002 running et al 2017 table 1 provides the important information for each site 2 3 model setup and evaluation field scale swat models karki et al 2020 were set up for each of the forest sites datasets utilized for setting up the field scale swat models are provided in table 2 field scale swat models for the evergreen sites were developed from the year of initial year of planting to 2020 table 1 even though the observed datasets did not extend to the year 2020 this allowed for comparison of model simulated forest biomass against observed values along with the evaluation of long term forest simulation with the new forest module the forest module for evergreen forest simulation was calibrated at evergreen fl 1 site and validated at the remaining two evergreen sites evergreen fl 2 and evergreen ga fig 2 model validation was performed by transferring only the calibrated forest parameters to the two validation sites and no additional adjustments were made for model simulation field scale models for the deciduous deciduous wi and mixed forest mixed va sites were developed from 2000 to 2020 with the model calibrated using remote sensed data from 2001 to 2010 and validated from 2011 to 2020 at the same sites fig 2 as the deciduous wi and mixed va had existing forests the initial forest age and biomass required for model initialization was acquired from williams et al 2020 model performance evaluation for the simulation of different field measured data in the evergreen sites were limited to graphical comparison between simulated and observed data due to temporal data limitations of one data point for each year fig 2 the deciduous and mixed sites were evaluated using graphical as well as statistical methods as monthly remote sensed data was available from 2001 to 2020 statistical evaluation was performed by calculating the coefficient of determination r2 nash sutcliffe efficiency nse and percent bias pbias which are commonly used measures for evaluating model performance in hydrologic and water quality modeling moriasi et al 2015 fig 2 details the model calibration and validation approach for the simulation of the three forest types using swat 3pg 2 4 swat 3pg model sensitivity analysis multiple scenario runs were performed to evaluate how the new swat 3pg forest module responded for forest simulation under different growing conditions as well as forest initialization the different scenarios simulated and evaluated to test swat 3pg sensitivity are listed in table 3 all the scenario runs were performed at the evergreen fl 1 site and the calibrated model run was used as the baseline against which all scenarios were evaluated fig 2 the scenario runs are important to understand if the new module can provide a response in forest growth under varying growth stresses and initial forest growth conditions which will be critical in using the swat 3pg model for forest management climate and land use change scenario evaluations 3 results 3 1 swat 3pg simulation of evergreen forests initial model parameters for swat 3pg simulation of the evergreen forest site with loblolly pine evergreen fl 1 were acquired from literature bryars et al 2013 gonzalez benecke et al 2016 subedi et al 2015 after which a sensitivity analysis for parameters was performed using r swat nguyen et al 2022 and model calibration was performed using automated r swat as well as manual approaches calibrated model parameters for swat 3pg for the simulation of evergreen forest at evergreen fl 1 site is presented in table s1 graphical comparison between swat 3pg simulated forest biomass components for the evergreen fl 1 site after calibration and observed data is presented in fig 3 it can be seen from the figure that the new forest module successfully replicates the observed trend as well as magnitude in the simulation of total stem and foliage biomass along with foliage loss and lai fig 3 the new forest module was able to capture very well the observed trend of biomass assimilation in total and stem biomass from planting to the juvenile years fig 3a and b swat 3pg was also able to capture the trend of reduction stabilization in total biomass assimilation after the year 2004 with the activation of mortality due to self thinning that can be simulated by the new forest module fig 3a the forest module also replicated well the quick accumulation of foliage biomass in the first few years after planning and the stabilization as well as slight reduction in foliage biomass afterwards fig 3c annual foliage loss was also captured well by the new forest module fig 3e foliage loss in evergreen forests with the new forest module happens throughout the year from the foliage biomass pool unlike the default forest module in swat which makes a proxy foliage biomass loss by removing a certain user defined fraction of biomass from the agb pool at the onset of dormancy the new forest module was also able to show temporal variability in the accumulation of foliage biomass the effect of which can be observed in the slight variability in the assimilation of annual biomass with accurate simulation of foliage biomass and losses the new forest module also matched the observed lai well fig 3f root biomass was the only forest biomass component that the new module was not able to replicate closely fig 3d comparison against observed data shows that the model was able to capture the trend of observed root biomass well in that the accumulation of coarse root biomass was linear in the early years which stabilized later fig 3d the model however over simulated the magnitude of coarse root biomass assimilated in the early years and the assimilation of coarse root biomass also stabilized earlier than observed it is important to note that the observed root biomass only included coarse root biomass and fine root biomass was not accounted for this could have resulted in the underreporting of observed total root biomass and potentially explains the over simulation of root biomass by the new forest module in the early years comparison of model simulated biomass components at the two validation sites evergreen ga and evergreen fl 2 against field measured observed dataset shows that the model performed reasonably well at both sites fig 4 fig 5 although the model tended to overestimate the forest biomass components at the evergreen ga site fig 4 and underestimate at the evergreen fl 2 site fig 5 the model simulated forest biomass closely matched the field measured values and also followed the observed trend well at both validation sites it is worth noting that the calibration and validation sites had different climatic and soil conditions table 1 and the validation run was performed by incorporating only the calibrated swat 3pg forest parameters from the calibration site this can help increase confidence in the new forest module to simulate forest biomass over a large spatial domain with varying climate and soil conditions as the new forest module performed well by only incorporating the calibrated forest parameters from the calibration site comparison of the observed dataset especially foliage biomass lai and foliage loss at the two validation sites showed that the observed datasets at evergreen fl 2 site fig 5 had more variability when compared to evergreen ga site fig 4 which was captured well by the new forest module simulated foliage biomass lai and foliage loss at the evergreen fl 2 site fig 5c e and 5f had high temporal variability while the evergreen ga fig 4c e and 4f showed minimal variability consistent with the observed datasets the variability in the simulation of different forest biomass components in the calibration and validation sites also helps reinforce that the model can respond to varying forest growing conditions which will be critical when using the swat 3pg at larger spatial domain with varying climatic and soil conditions 3 2 swat 3pg simulation of deciduous and mixed forest as the deciduous wi site was dominated by aspen populus tremuloides initial parameters for swat 3pg were derived from amichev et al 2010 which estimated 3 pg parameters for hybrid poplar tree species calibrated swat 3pg parameters for the simulation of deciduous wi site is presented in table s1 comparison of swat 3pg simulated lai aet and npp c against modis estimated data for the deciduous forest site deciduous wi during calibration and validation is presented in fig 6 it can be seen from the figure that swat 3pg does a good job of capturing the observed variability for all three variables during the calibration 2011 2010 and validation period 2011 2020 r2 and nse of greater than 0 5 for the simulation of all three variables lai aet and npp c during both calibration and validation table 4 indicates that swat 3pg can simulate the three variables well for deciduous forest systems pbias for lai and aet was less than 20 for both calibration and validation indicating a good model fit pbias for npp c was however 26 9 during model validation period table 4 indicating to the model having slight difficulty in replicating the remote sensed estimated npp c an additional model run performed for the deciduous wi site using calibrated swat 3pg parameters but forest initialized at planting rather than already growing as initialized during calibration to evaluate how the calibrated model parameters would simulate forest from planting model results evaluation showed that swat 3pg was able to simulate realistic forest biomass components including total stem and foliage biomass as well as lai fig 7 with the calibrated parameters this shows that the use of remote sensed data products lai aet npp c can be important and helpful datasets for constraining swat 3pg parameters for forest simulation especially when performing model simulations at regional watershed scale and there is a lack of field observed data for different forest types as the mixed forest site had multiple forest species initial parameters used for the deciduous wi site were also used as the initial parameters for this site comparison of model simulated monthly lai aet and npp c against modis estimated for the mixed forest site is shown in fig 8 the new module simulated lai and aet well with a close match between simulated and remote sensed variables fig 8 r2 and nse was greater than 0 5 and pbias was less than 10 during both calibration and validation periods table 5 a close match between simulated and observed lai and aet shows that the module for simulation of mixed forests which represents mixed forest systems using a single plant type due to the limitation in swat model can still replicate the spatial as well as temporal variability in lai and aet of the mixed forest system as a whole evaluation of npp c however showed that the mixed forest module had difficulty in replicating the remote sensed estimated npp c swat 3pg was able to capture the observed trend in the assimilation of npp c well fig 8 but the model was not able to capture the observed magnitude in npp c assimilation with the model under simulating npp c during both calibration and validation periods nse 0 1 and pbias 40 it should be noted that swat 3pg simulates mixed forest system similar to a deciduous forest with a single forest type but loses only a portion of foliage biomass as litterfall to replicate the loss of foliage from deciduous trees this limit in swat 3pg does not allow for biomass assimilation of different tree species in a mixed forest system to be simulated separately but is estimated by calibrating for biomass assimilation that is representative of the mixed forest system which can lead to difficulty in the model replicating the observed npp c a better starting parameters by identifying the major forest types in the mixed forest site could also have potentially helped in better npp c simulation evaluation of modis estimated npp c for the mixed site also showed npp c assimilation in the winter months possibly from forest understory which was not captured by the model with no npp c simulation in the winter months this also contributed to the poor performances measure for npp c simulation for the mixed va site it is also equally important to acknowledge the inconsistencies and uncertainties associated with npp c datasets when evaluating against model simulated values xie et al 2020 zhao et al 2006 an additional model run using the calibrated parameters was also performed for the mixed va site but with forest initialized as planting to evaluate the impact of the calibrated parameters on forest simulation the new forest module for mixed forest simulation in swat 3pg was also able to simulate the important components of a mixed forest system including temporal variability in lai foliage biomass loss and total stem and root biomass assimilation fig 9 this shows that remote sensed data products can be successfully used to constrain parameters for the simulation of mixed forest systems in swat 3pg 3 3 swat 3pg model sensitivity for forest simulation sensitivity of the swat 3pg model to forest biomass simulation due to changes in rainfall nitrate forest initialization and self thinning age mortality is presented in fig 10 model scenario runs with rainfall showed that swat 3pg is sensitive to changes in rainfall a reduction in daily rainfall by 30 led to a reduction in total biomass assimilation by an average of 8 when compared to baseline the reduction in biomass assimilation at the end of the simulation period was close to 12 mg ha 4 fig 10a evaluation of the whole simulation period showed that the reduction in biomass assimilation in the middle years from 1999 to 2006 when the forest is juvenile and actively assimilating biomass was much higher 13 increasing daily rainfall by 30 showed only a slight increase in annual biomass assimilation with an average of 2 the maximum increase in biomass assimilation was again observed in the middle years 5 but the difference at the end of the simulation period was only 2 4 84 mg ha fig 10a scenario runs involving changes to nitrogen availability demonstrated the model s sensitivity to change in nutrient levels a slight increase in assimilated forest biomass was observed when doubling the atmospheric deposition of nitrate fig 10b application of nitrate as fertilizer each year 200 kg ha however led to substantial increase in forest biomass assimilation the increase in biomass at the end of the simulation period was close to 12 35 mg ha but the increase was much higher in the middle years at close to 18 40 mg ha a scenario run with already growing forest of 10 years and initial biomass of 130 mg ha showed that the model stabilizes the forest biomass assimilation much faster than when initializing the model at planting due to earlier self thinning fig 10c this shows that the swat 3pg model can vary forest biomass assimilation based on the initialized age and biomass the importance of incorporating mortality due to age as well as self thinning for realistic assimilation of biomass especially when the forest reaches towards maturity can be visualized with the model scenario run that turned off mortality due to age as well as self thinning fig 10d turning off mortality due to age alone had minimal change in forest biomass assimilation which could be potentially attributed to the model run of only 34 years turning off mortality due to self thinning however led to a somewhat linear increase in biomass till the end of the simulation period which can be considered unrealistic fig 10d the reduction stabilization in biomass assimilation after 2006 is due to self thinning sub routine in the swat 3pg model which reduces the number of trees ha based on a 3 2 power rule when the average stem biomass becomes higher than a user defined threshold this also shows the importance of making sure to activate the self thinning sub routine when simulating forest biomass with swat 3pg 4 discussion 4 1 swat 3pg for forest simulation swat 3pg improves on the default swat forest module by incorporating important processes for forest simulation that were missing in the default module swat 3pg partitions assimilated biomass into stem root and foliage based on growth conditions and age which with the default model was partitioned only into above ground and root biomass lai as a result in swat 3pg is a function of assimilated foliage biomass which was calculated only based on accumulated heat units with the default model evaluation of swat simulated biomass and lai with the default forestry module and default parameters in the evergreen fl 1 site fig s1 shows that the model simulated the lai well but poorly simulated total biomass which could potentially be attributed to the lack of check and balance between biomass assimilation and lai simulation with the default model calibration of the default swat model parameters for forest simulation could better match the observed total biomass in the evergreen fl 1 site but model would not be well constrained as it does not simulate and provide output for variables such as foliage biomass stem biomass tree mortality evaluation of swat 3pg at the evergreen deciduous and mixed forest sites shows that the new forestry module can accurately simulate the different forest biomass components by incorporating important processes that were missing in the default swat it is important to note that the default swat module only tracks agb and does not partition agb to stem and foliage biomass similarly the default swat model also does not report root biomass swat 3pg also improves litterfall simulation with realistic representation of litterfall from evergreen forests when compared to the default module fig s1b and also shows temporal variability in lai simulation which was not captured by the default module fig s1b swat 3pg also simulates tree mortality due to age density independent and self thinning density dependent for all forest types which is a more realistic way of constraining biomass than the default swat forestry module which uses a user defined maximum biomass along with the current age of tree and tree age to maturity to limit maximum biomass growth for each year of the simulation period this will contribute to a more realistic approach of biomass stabilization in mature tree forests as well as help in improving the simulation of soc along with lateral carbon and nutrient fluxes to the inland aquatic environment from forest ecosystems as swat 3pg was able to show good variability in the temporal simulation of lai the model should also benefit with improved simulation of hydrology due to improvement in aet simulation comparison against field observations as well as modis npp c shows that the forest module can replicate observed biomass assimilation from forest systems and as the forest module was able to simulate realistic stabilization in biomass assimilation at maturity even with different forest initialization swat 3pg can be an important tool for answering carbon sequestration and storage questions from forest systems the sensitivity of swat 3pg forest growth to climate nutrient and forest initialization also demonstrates its value in evaluating the impacts of climate and management scenarios in forest systems as swat 3pg provides additional outputs such as stock density dbh height basal area stem biomass etc which were not available with the default model swat 3pg can be a tool for forest management evaluation even for commercial purposes with improved simulation of forest ecosystems swat 3pg can be helpful in reducing hydrologic and water quality simulation uncertainties from forested regions which can be important in accurately understanding the physical and chemical processes along with the impacts of multiple scenarios in agriculture dominated watersheds evaluation of different forest age and biomass initialization shows that biomass assimilation is sensitive to forest initialization hence it is important to utilize available resources such as williams et al 2020 and make sure that forest age and biomass is initialized accurately when using swat 3pg an advantage of swat 3pg is that forest parameterization already performed in many 3 pg studies can be leveraged as initial parameters into swat 3pg by evaluating for the dominant forest species in the watershed of interest with accurate forest initialization of age biomass and tree species along with the use of existing 3 pg studies for initial parameterization and leveraging remote sensed data products such as modis for calibration swat 3pg can be an important tool for estimating current and near future carbon stock in forested systems at a regional watershed scale which can be valuable information to planners and policymakers for climate mitigation 4 2 swat 3pg limitations and future development although swat 3pg improved on the simulation of forest ecosystems and provided important additional model outputs when compared to the default forest module in swat there are important limitations that should be considered when using the enhanced model as evident from the simulation results of mixed forest sites it needs to be understood that swat 3pg simulates mixed forest systems using a single plant type due to the limitations in the swat hru approach for model simulation in addition it is important to note that swat 3pg cannot simulate forest succession as such care should be taken when using the swat 3pg for multi century model runs as npp c assimilation in swat 3pg decreases when forest reaches maturity when in reality a forest succession due to natural anthropogenic events is more likely although swat 3pg improves on the litterfall simulation of the default model foliage loss at the onset of dormancy for deciduous and mixed forest systems is simulated on a single day while it happens gradually and could last for multiple weeks in reality it was also observed that swat 3pg simulation of root biomass can be improved as it was not able to accurately represent the observed temporal variability in root biomass an important consideration when using swat 3pg is the initialization of forest age and biomass as swat 3pg requires initialization of forest age and biomass care should be taken when initializing forests especially in watersheds with mature forests as evident from the swat 3pg sensitivity runs biomass assimilation is sensitive to forest age and biomass and it is important to get accurate forest age estimates when initializing the model without which it could be difficult to replicate the estimated biomass and npp c using remote sensed data products incorporating forest succession into the swat 3pg model would be beneficial for using the model for multi century model runs and evaluating long term climate impacts similarly improving the litterfall routine for deciduous forests could be helpful in evaluating the model in a finer temporal scale a new mixed forest subroutine that allows for multiple forest species to grow with competition between the species can be important to improve swat 3pg simulation in mixed forest systems as swat 3pg provides model outputs that are also beneficial for forest managers incorporating forest management practices into the model that are used by forest managers can be beneficial in expanding the model usability to other sectors an important advantage of swat 3pg is that the new innovations in the 3 pg model can be easily incorporated into the model as a result any advancement in 3 pg will also be beneficial in improving swat 3pg it will be important to evaluate the swat 3pg s performance for forest simulation over a large spatial domain with multiple forest species climate and soil to improve confidence in the new forest module which will be critical for using the tool in policy decision making the impact of improved swat 3pg forest simulation on hydrology and water quality including carbon over a large spatial domain also needs to be evaluated with the advancement in the data types and spatial resolution of remote sensed datasets modification to swat 3pg to incorporate remote sensed datasets directly into the model for leveraging these datasets can be an important next step in improving model simulation 5 conclusions this study developed and tested a new forest module for the swat model that is based on 3 pg swat 3pg and improves on the default forest module with improved biomass assimilation partitioning and losses for evergreen deciduous and mixed forest systems the new forest module partitions assimilated biomass into stem foliage and root biomass and also simulates tree mortality due to age as well as self thinning which was not possible with the default module lai in the new forest module is related to foliage biomass rather than heat units as simulated in the default module which allows for improved temporal variability in biomass assimilation modification to the litterfall routine with separate litterfall for evergreen deciduous and mixed forest systems allows for improved litterfall simulation from forest ecosystems evaluation at site scale for evergreen forest using field measured data showed that the new forest module can adequately simulate stem foliage and coarse root biomass along with lai and foliage loss deciduous and mixed forest sites were calibrated against remote sensed lai npp c and aet datasets due to the lack of field data for the two forest types evaluation of the new forest module for deciduous and mixed forest sites shows that the new forest module could adequately replicate lai npp c and aet at deciduous forest site but the new module had slight difficulty in replicating npp c at mixed forest site the difficulty in accurately simulating npp c at mixed forest site could potentially be attributed to swat s inability to simulate multiple plant types in a single hru but improvement in npp c simulation should be achieved with improved initial parameterization for simulation of mixed forest sites assessment of model simulated stem foliage and root biomass as well as lai with calibrated swat 3pg parameters constrained using remote sensed products shows that the remote sensed products could be a valuable asset for constraining parameters for forest simulation with swat 3pg over a large spatial domain when field measured datasets are not available this along with the ability to initialize already existing forests in swat 3pg should allow the new model for improved estimation of carbon stock in forested ecosystems over regional domain as swat 3pg also estimates variables such as dbh height and stand basal area the new model can also be of importance to forest managers and growers model sensitivity analysis of swat 3pg showed that the model can also be a useful tool for evaluating climate and management impacts in forested ecosystems as well as at a watershed scale with reduced uncertainty when compared to the default model declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgment the funding support for this study was provided by the u s department of agriculture the national institute of food and agriculture 2021 67019 33684 and 2023 67019 39221 and the national aeronautics and space administration nnx17ae66g and 80nssc20k0060 this research was supported in part by the u s department of agriculture agricultural research service usda is an equal opportunity provider and employer mention of trade names or commercial products in this publication is solely for the purpose of providing specific information and does not imply recommendation or endorsement by the u s department of agriculture appendix a supplementary data the following is the supplementary data to this article multimedia component 1 multimedia component 1 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2023 105705 
25408,building cyberinfrastructure for the reuse and reproducibility of complex hydrologic modeling studies iman maghami a ashley van beusekom b lauren hay b zhiyu li c andrew bennett d youngdon choi a bart nijssen b shaowen wang c david tarboton e jonathan l goodall a a department of engineering systems environment university of virginia charlottesville va usa department of engineering systems environment university of virginia charlottesville va usa department of engineering systems environment university of virginia charlottesville virginia usa b department of civil environmental engineering university of washington seattle wa usa department of civil environmental engineering university of washington seattle wa usa department of civil environmental engineering university of washington seattle washington c department of geography geographic information science university of illinois at urbana champaign il usa department of geography geographic information science university of illinois at urbana champaign il usa department of geography geographic information science university of illinois at urbana champaign il usa d department of hydrology and atmospheric sciences university of arizona tucson az usa department of hydrology and atmospheric sciences university of arizona tucson az usa department of hydrology and atmospheric sciences university of arizona tucson az usa e department of civil and environmental engineering utah water research laboratory utah state university logan ut usa department of civil and environmental engineering utah water research laboratory utah state university logan ut usa department of civil and environmental engineering utah water research laboratory utah state university logan utah usa corresponding author university of virginia department of engineering system and environment university of virginia 151 engineers way p o box 400747 charlottesville va 22904 usa university of virginia department of engineering system and environment university of virginia 151 engineers way p o box 400747 charlottesville va 22904 usa building cyberinfrastructure for the reuse and reproducibility of large scale hydrologic modeling studies requires overcoming a number of data management and software architecture challenges the objective of this research is to advance the cyberinfrastructure needed to overcome some of these challenges to make such computational hydrologic studies easier to reuse and reproduce we present novel cyberinfrastructure capable of integrating hydroshare an online data repository cybergis jupyter for water and high performance computing hpc resources computational environments and the structure for unifying multiple modeling alternatives summa hydrologic modeling framework through its application programming interface for orchestrating model runs the cyberinfrastructure is demonstrated for a complex computational modeling study on a contiguous united states dataset we present and discuss key capabilities of the cyberinfrastructure including 1 containerization for portability across compute environments 2 globus for large data transfers 3 a jupyter gateway to hpc environments and 4 jupyter notebooks for capturing the modeling workflows keywords reproducibility computational hydrology jupyter hpc containerization data availability the data and jupyter notebooks used in this study were published on hydroshare as a collection resource with persistent dois 1 introduction reproducibility the ability to duplicate and verify previous findings is a foundational principle in scientific research in computational hydrology melsen et al 2017 highlighted two contrasting definitions of model reproducibility 1 bit reproducibility which is defined as exact replication of a study including the exact same numbers forming the results and 2 conclusion reproducibility which focuses on reproducibility of the conclusions of a study as the conclusions are expected to hold if the same experimental approach is applied they argue that conclusion reproducibility replicating a study s conclusions may be more important than bit reproducibility exactly replicating model runs because hydrological theories need to be tested beyond bit reproducibility by investigating conditions under which theories can be confirmed or falsified even so conclusion reproducibility itself goes beyond the simple sharing of code and data as open source and online resources typically touted for achieving reproducibility the code and data must be accompanied by well documented workﬂows with readable and reusable code chen et al 2020 mullendore et al 2021 simmonds et al 2022 reusable code requires providing open source computational environments in which the code can be executed ensuring this reuse and reproducibility is a non trivial task it requires not only adopting new capabilities for handling complex software and big data it also requires careful software engineering practices to integrate these new capabilities into well designed and built cyberinfrastructure merkel 2014 a growing body of researchers have been discussing and proposing guidelines and strategies for reproducible computational modeling e g bush et al 2021 choi et al 2021 knoben et al 2022 mullendore et al 2021 simmonds et al 2022 in recent work knoben et el 2022 presented a novel approach for creating a hydrologic model at any location or scale local to global by separating model agnostic and model specific configuration steps within cyberinfrastructure workflows choi et al 2021 described a general strategy for creating modern cyberinfrastructure to support open and reproducible hydrologic modeling as the integration of three components 1 online data repositories 2 computational environments leveraging containerization and self documented computational notebooks and 3 application programming interfaces apis that provide programmatic control of complex computational models as an example of this general approach choi et al 2021 also presented an implementation that used 1 hydroshare as the online repository 2 two different jupyter instances one hosted by the consortium of universities for the advancement of hydrologic science inc cuahsi and a second hosted by cybergis jupyter for water as the computational environments and 3 pysumma a python wrapper for manipulating running managing and analyzing of summa structure for unifying multiple modeling alternatives as the model api while choi et al 2021 focused mainly on the system design and demonstrated their approach with a fairly simple modeling use case reproducibility in computational hydrology can present some difficult challenges when dealing with large scale hydrologic studies hutton et al 2016 these challenges mostly pertain to the use of big data and computationally expensive and time consuming resources needed for reproducibility of complex hydrologic modeling studies hutton et al 2016 notes that in these cases new techniques are needed to ensure scientific rigor in this paper we provide an example of the overall system design outlined by choi et al 2021 as applied to a complex hydrologic study by van beusekom et al 2022 hereafter referred to as the vb study we develop the necessary cyberinfrastructure to reproduce this study for selected sub domains and discuss the challenges and opportunities in ensuring conclusion reproducibility for complex hydrologic studies the vb study evaluated the effect of the temporal resolution of surface meteorological inputs or forcings on modeled hydrological fluxes and states for 671 basins across the contiguous united states conus it quantified the difference in hydrologic outcomes based on daily or sub daily forcings for multiple model configurations and parameter values reproducibility of the vb study if one was given only the input data and model code would be challenging because it requires the installation and configuration of the modeling framework summa clark et al 2015a 2015b the data volumes are very large and the model runs require high performance computing hpc resources the complete vb study consisted of 704 6 year model runs for each of the 671 basins or 2 8 million model years summa was implemented with a single hydrologic response unit for each basin resulting in a single output time series for each basin for each model configuration for every model run the output consisted of 14 hydrological variables which required 6 mb per model simulation or 2 834 tb for the entire study while few researchers may be interested in reproducing the entire vb study the more common use case and the focus of this study would be to repeat or extend the vb study for a subset of the basins we want to enable others to reproduce the vb study for subsets of the original domain as a basis for doing additional research enabling conclusion reproducibility rather than the bit reproducibility for such an approach to be effective it is not sufficient to provide the open source summa code and model input data one must also provide the additional components described by choi et al 2021 i e computational environments models exposed through apis and documented model workflows to create a cyberinfrastructure that lowers the barrier to reuse and reproducibility this research contributes to the growing literature advancing cyberinfrastructure for hydrology and other geoscience fields yang et al 2010 illustrated the importance of using hpc in computationally intensive geospatial sciences and hydrologic modeling essawy et al 2016 demonstrated server side workflows for large scale hydrologic data processing although they did not make use of hpc in their application lyu et al 2019 used containerization and combined computational environments including hpc and high throughput computing htc cyberinfrastructure to directly run the models using jupyter notebooks gan et al 2020 integrated a hydrologic data and modeling web service with hydroshare as a data sharing system to show how this integration leads to a findable and reproducible modeling framework gichamo et al 2020 used web based data services to prepare input data for hydrologic models kurtz et al 2017 introduced a cloud based real time data assimilation and modeling framework and showed how parallel processing can be used for complex hydrologic models in the cloud however unlike the vb study none of lyu et al 2019 gan et al 2020 gichamo et al 2020 and kurtz et al 2017 applied their methods on a computationally extensive complex hydrologic use case therefore the challenges and opportunities of using cyberinfrastructure for reproducibility of complex large scale hydrologic modeling for which hpc and big data approaches are required remain largely unexplored to address this research gap we designed and implemented cyberinfrastructure to enable intuitive access to hpc computational environments and to support data transfers into and out of the hpc environment additionally we provide a workflow that allows users to replicate parts of the study within their own computing environments we also perform a workflow run time performance analysis that compares different model scenarios by varying the size of simulations across different computing environments providing users with a guide towards selection of the computing environment depending on the size of their simulations the cyberinfrastructure provides a starting point for users to modify the hydrologic model setups thus going beyond reproducibility i e the ability to duplicate and verify previous findings into replication where one modeling methodology can be used to answer the same scientific research question but with new input data as highlighted by essawy et al 2020 the cyberinfrastructure may also serve as an educational resource by providing an intuitive way for students to perform complex hydrologic modeling studies the data and cyberinfrastructure are provided through hydroshare to run on any basin for which we provide a summa setup to assist the modeler in analyzing basins individually the remainder of this paper is organized as follows in section 2 we provide a brief overview of the vb study the cyberinfrastructure the model workflows and the model scenarios used for a science use case subsetted from the vb study as well as the model workflows run time performance analysis section 3 provides results and discussion the results focus on the modeling use case and an analysis of the workflow run time performance for different computing environments the discussion focuses on opportunities and challenges learned from our experience designing and building the cyberinfrastructure to support our modeling workflows finally our conclusions and recommendations are provided in section 4 2 methods 2 1 overview of the vb study the vb study used 671 basins to study the effects of the temporal resolution of the meteorological forcings on hydrologic model simulations across the conus the basins are part of the camels dataset catchment attributes and meteorology for large sample studies newman et al 2015b a large sample hydrometeorological dataset across the conus consisting of input forcings basin attributes and relevant historical streamflow records the vb study used summa clark et al 2015b to configure multiple model instances for each basin representing eight different model configurations and 11 different sets of model parameter values in addition eight forcing datasets were constructed in each of these forcing datasets one of the meteorological inputs was modified so that the diurnal cycle was replaced by the mean value over that day the vb study performed 704 8 11 8 704 6 year model runs for each camels basin consisting of one year of model initialization and five years of actual simulation model outputs for 14 simulated variables were stored to evaluate the sensitivity of the simulations to changes in model forcings model configurations and model parameters figure a1 and table a1 the vb study results demonstrated that 1 the effect of each forcing input on each model output varies by model output and model location 2 the use of a particular parameter set may not be critical in determining the most and least influential forcing variables and 3 the choice of model physics i e using different model configurations could change the relative effect of each forcing input on model outputs the vb study was run with scripts on the cheyenne supercomputer a 5 34 petaflops high performance computer built for the national center for atmospheric research computational and information systems laboratory 2017 and it took a few days to complete the runs for each basin the output size for a single 6 year run was 6 mb thus reproducing the entire study is computationally expensive and also requires large amounts of storage 704 runs 671 basins 6 mb 2 834 tb however the cyberinfrastructure allows individual basins to be run independently here we focus on a use case in which a researcher wishes to reproduce a subset of the vb study by analyzing one or a few basins within a cloud cyberinfrastructure environment to reach conclusion reproducibility the conclusion reproducibility that we aimed in this study is solely a qualitative one and if the presented cyberinfrastructure can be successfully applied to studies differing from the original study i e the vb study the conclusion reproducibility is achieved 2 2 cyberinfrastructure design and implementation following the approach described in choi et al 2021 we designed and implemented cyberinfrastructure fig 1 to replicate the vb study by integrating 1 the hydroshare online data repository 2 cybergis jupyter for water computing gateway cjw cg and high performance computational environments and 3 a model api that can be utilized in scripts using jupyter notebooks here the pysumma api each of these three components is further explained in the following subsections 2 2 1 online data repositories we used hydroshare an online collaboration environment as the online data repository horsburgh et al 2016 tarboton et al 2014 a collection resource in hydroshare which can be found at choi et al 2023a contains three resources holding the data computational environment and models fig 1 the hydroshare resource holding the data mizukami and wood 2023 contains the forcing dataset for the 671 camels basins the forcings are based on the hourly nldas 2 north american land data assimilation system nldas 2 2014 nldas 2 is hereafter referred to as nldas the original nldas hourly forcing data were created on a 0 125 0 125 grid to create hourly summa model forcings nldas outputs were spatially averaged over each of the 671 camels basins and merged into one netcdf file with this format an opendap server opendap 2021 can extract data for selected basins on the server so that the user does not have to download the entire conus dataset to a local computer hydroshare offers this capability via its thredds data server tarboton and calloway 2021 2 2 2 computational environments the developed computational environments provide a consistent software environment that is independent of each user s own operating system and software libraries making it possible to study a computationally expensive research problem fig 2 shows each computational environments component and the interoperability between the computational environments and hydroshare one computational environment was implemented on the cjw cg cloud service for studies with limited computational demand e g a study of only a few basins or as an instructional tool or for model debugging a second computational environment was developed on an hpc resource to reproduce a problem more representative of challenges posed by the use of big data in the vb study the hpc environment also allows the user to study a particular basin in greater detail in this study the cjw cg computational environment is used to provide 1 the model execution environments configured as docker images to enable execution of the summa model for studies with limited computational demand i e those need to use cjw cg workflow and 2 cyberinfrastructure for preprocessing postprocessing and data storage for both studies with limited computational demand need to use cjw cg workflow and with high computational demand i e those need to use hpc workflow fig 2 the hpc computational environment is only used for providing model execution environments configured as singularity containers to enable execution of the summa model for studies with higher computational demand more details on each computational environment are provided in the rest of this section cjw cg is a cloud computing environment interoperable with hydroshare it is an instance of cybergisx yin et al 2017 that serves the data and computation intensive needs of the water and environmental communities we used cjw cg because it is publicly available is interoperable with advanced cyberinfrastructure resources such as the hpc resource used in this study and has been serving the water and environmental communities to support their modeling needs reproducibility was facilitated by using containerization of the summa model and the pysumma api with docker merkel 2014 in the case of the cjw cg environment or singularity in the case of the hpc environment kurtzer et al 2017 along with a computational gateway interface to jupyter notebooks pysumma and the notebooks are described in a later section fig 2 although using docker is a common approach to containerize the model dependencies we used singularity in the hpc environment because it is designed to work seamlessly with existing batch job systems to support hpc applications the containerization and interface are hosted on the cjw scientific cloud service hosted on jetstream cloud hancock et al 2021 stewart et al 2015 towns et al 2014 the dockerfile is hosted on a github repository li 2021 with pre built docker images being shared on a docker hub repository singularity container used by the hpc environment is hosted on cybergis compute service a middleware platform allowing seamless access to hpc resources via python based software development kit and core middleware services cybergis compute service 2021 li et al 2022 the singularity container was created through docker images conversion cybergis compute service also handles submitting jobs to hpc as well as large data transfer from hpc through globus will be discussed in section 2 2 4 the conda software package was used to manage the project specific computational environment on cjw allowing the user to build a python environment with the summa model pysumma api and other computational dependencies this was done by providing a kernel version for the project cybergis center hydroshare development team 2022 using this stable kernel which captures all the required dependencies with their specific versions ensures careful software version control 2 2 3 model application programming interface api the model api pysumma was chosen to be part of the interactive tool the pysumma api choi et al 2021 wraps the summa hydrologic modeling framework clark et al 2015a and allows the user to script the use of the summa model using python it facilitates model configuration and allows for local execution of the model by either using a docker container or a locally compiled summa executable choi et al 2021 with pysumma a user can modify summa input files and run summa inside a python script as well as automatically parallelize runs and visualize output in the simplest case the pysumma simulation object wraps a single instance of a summa simulation for users who choose to analyze multiple basins at a time in the cjw cg environment instead of the hpc environment the notebook automatically will configure a pysumma distributed object which provides an interface to spatially distributed simulations and handles parallelism and job management under the hood in this study multiple summa simulations are run in each basin so a pysumma ensemble object is used to manage multiple runs with different configurations in the hpc computational environment a custom backend was written to handle parallelism using message passing interface mpi reducing the need for users to customize the configuration based on the type of job that they are running a high level description of pysumma is presented in fig 1 the simulation py enables the execution of the summa model and along with file manager py decisions py force file list and output control py allows for manipulating summa configuration files the distributed py enables the parallel execution of summa 2 2 4 data management and transfer the input data for this study consists of the summa configuration files and the forcing data for the 671 camels basins the configuration files e g geometries information for the 671 camels basins along with their attributes such as hru id are shared within each of the two hydroshare resources holding the jupyter notebooks the forcing data are provided in a hydroshare resource mizukami and wood 2023 the output files resulting from running the notebooks using the cjw cg and hpc computational environments are 1 netcdf output files generated by the summa simulations 2 a netcdf file recording the model performance for each basin as measured by the kling gupta efficiency kge gupta et al 2009 and 3 additional files created by the notebooks such as the figures that visualize the model results in the case of the cjw cg environment after running the notebooks all files are saved in the cjw cg and are directly accessible to the user in the case of the hpc environment the kge results and other files created by the notebooks e g figures are automatically transferred to the cjw cg but the netcdf output files remain within the hpc environment to avoid transferring large volumes of model output as a reminder the size of the model output for the entire vb study was 2 834 tb however if the user of the hpc environment wishes to transfer selected summa netcdf output files from the hpc to be directly accessible for further analysis and long term storage then the cybergis compute service li et al 2022 can be used for reliable high performance large file transfers through the globus service chard et al 2016 foster 2011 as shown in fig 2 data is transferred from hpc to the cjw using globus without going through the job submission server globus is a software as a service that enables the transfer of datasets of any size between different storage options personal computers hpc etc without users being required to be constantly logged in and monitoring the data transfer chard et al 2016 technically the cybergis gis compute acts as a globus app client holding a community globus account that has access to both data endpoints on the jupyter and target hpc when data transfer is needed cybergis compute initiates a globus task between the two endpoints and monitors the progress users are updated with data transfer status in the notebooks environment during the entire process 2 3 model workflows as jupyter notebooks as mentioned earlier the model workflows allow the user to reproduce all or subsets of the vb study using either the cjw cg computational resources referred to later as cjw cg or the hpc and cjw cg computational resources referred to later as hpc the cjw cg and hpc hydroshare resources can be found at choi et al 2023b and choi et al 2023c respectively the model workflows are documented in three for cjw cg or four for hpc jupyter notebooks table 1 shows the summary of the steps taken in each notebook while figure a2 a5 show more detailed information for notebooks 1 4 the first three notebooks for both the cjw cg and hpc environments focus on 1 selecting the study basins simulation period and model input forcings 2 running the summa model and 3 exploring outputs to analyze the effect of each forcing variable in each basin the hpc computational resource uses a fourth notebook to transfer large unprocessed output data from the hpc to cjw using globus notebooks 1 and 3 are very similar between the two hydroshare resources and both cjw cg and hpc hydroshare resources use cjw cg computational resources to run these two notebooks the second notebook differs for the two environments and the difference is explained in section 2 3 2 these notebooks assist a modeler in analyzing camels basins individually providing information on forcings and output variables that are the most least sensitive in their basin with some additional work the cjw cg computational environment can also be hosted on other non cjw cloud services but the hpc environment is more tailored to interact with the cjw cloud service used here to use the hpc computational resource the user must obtain access to the hpc by issuing a request through hydroshare to use cjw once this access is granted users are automatically given free access to two alternative hpc resources 1 the virtual roger resourcing open geospatial education and research hpc administered by the school of earth society and environment at university of illinois urbana champaign uiuc which is integrated with the keeling compute cluster at uiuc virtual roger user guide 2022 and 2 the expanse hpc a much larger nsf xsede resource operated and managed by san diego supercomputer center sdsc expanse system architecture 2022 in theory the cybergis compute service can support other hpcs as well but we did not test other hpcs in this study among the provided hpc options we only used expanse to demonstrate the cyberinfrastructure in our initial experiments expanse hpc performed faster than virtual roger and the goal here was to show how a hpc can scale up a study by speeding up the modeling process compared to a non hpc environment rather than an inter comparison between different hpcs users who do not wish to use hpc computational resources can use cjw cg computational resources directly to run smaller modeling jobs the hardware specifications of the cjw cg and the expanse hpc are compared in table 2 the cjw cg has only three compute nodes each of which has eight cpus with 1 996 ghz clock speed and 30 gb dram each user can only use up to six cpus and the cpus can be shared among users this means the maximum degree of parallelism for simulations using this computational resource is six thus in case of running one basin from the vb study 704 runs and using all the six available cpus each cpu will need to run 117 33 simulations some of them 117 and others 118 simulations the expanse hpc has 728 amd rome standard compute nodes each of which is equipped with 256 gb dram and 128 2 25 ghz cpus expanse user guide 2022 the expanse hpc allows the user to only use up to two nodes at a time i e 256 cpus or the maximum degree of parallelism for simulations thus if a user is running one basin from the vb study 704 runs and using all the available 256 cpus then each cpu will need to run 2 75 simulations some of them two and others three this shows how the hpc resource can scale up the model runs offering a high performance tool more details about the run time performance of the notebooks are discussed in the results and discussion section the following subsections discuss the general purpose of each notebook used to reproduce parts of the vb study for specific coding details refer to the notebooks in the hydroshare resources at choi et al 2023b and choi et al 2023c 2 3 1 data processing notebook the first notebook jn 1 preprocessing processes the original camels summa files and the input forcing datasets table a2 the user can select one or more camels basins 1 671 basins but by selecting a higher number of basins the computational time and expense increases notebook 1 subsets the original camels summa files producing summa attributes parameters initial conditions and hourly nldas forcing files for the selected basin s then additional forcing datasets for the hydrologic model sensitivity study are developed from the nldas data files forcings box in figure a1 as discussed below for each summa model setup variations in 14 summa generated outputs described in table a1 are examined with respect to variations in seven input forcings air pressure prs air temperature tmp long wave radiation lwr precipitation rate ppt specific humidity hum shortwave radiation swr and wind speed wnd under different model parameterizations and configurations the summa outputs generated with the 1 h nldas forcing dataset are considered the benchmark nldas dataset 1 forcings box in figure a1 the rest of datasets ppt to prs datasets forcings box in figure a1 are developed holding each of the individual forcing variables constant over a 24 h period while the other six forcing variables contain the original hourly nldas values figure a2 shows the steps taken in the first notebook this notebook is the same for the cjw cg and hpc environments except that the simulation time period and basins to be explored are pre populated differently the user can change these setups in the third step of this notebook step 1 3 in the last step of this notebook users can visualize the individual forcing variables held constant over a 24 h period against the original hourly nldas values using hourly and cumulative plots 2 3 2 summa execution notebook the second notebook jn 2 running summa executes the summa model using the input data from the first notebook for four different sets of summa basin runs outlined in figure a1 runs box and described in detail in the vb study the first set of basin runs default 8 summa runs per basin runs box uses the eight forcing datasets forcings box combined with default parameters and a default summa configuration the summa default configuration is set in the resource model decision file the second set of basin runs lhs 88 summa runs per basin runs box in figure a1 uses the eight forcing datasets combined with 11 parameter sets and a default summa configuration the 11 parameter sets consist of the default parameter set and 10 additional parameter sets with 15 commonly calibrated parameters table a2 as detailed in the vb study the parameters are sampled using latin hypercube sampling lhs over their defined range the pydoe lhs function lee 2014 is used to create unique 10 15 lhs sampling matrices for the selected basin then the lhs matrices are used to produce 10 parameter sets of the 15 parameters while considering the parameter constraints listed in table 2 the choice of a different seed value will lead to different lhs sets and these sets will be different from the ones used by the vb study the third set of basin runs config 64 summa runs per basin runs box in figure a1 uses the eight forcing datasets combined with the default parameter set and eight summa configurations the eight summa configurations outlined in the configurations box in figure a1 test three model decisions stomatal resistance stomresist choice of snow interception parameterization snowincept and choice of canopy wind profile windprfile with two options for each decision note the default configuration for this study is shown in bold in the configurations box in figure a1 ballberry lightsnow and logbelowcanopy the fourth set of basin runs comprehensive 704 summa runs per basin runs box in figure a1 includes the default lhs and config basin runs and is the only set that needs to be run to replicate a single basin sensitivity study following the vb study method six years of simulation must be run for replication for testing purposes sets 1 3 can also be run by themselves the 10 parameter set files for the basin from the lhs sampling plus the default parameters 11 parameter sets are run each with eight summa configurations configurations box in figure a1 figure a3 shows the steps taken in the second notebook the first two steps in this notebook are the same for the cjw cg and hpc environments but the rest of the workflow differs in the cjw cg notebook the user can define the simulations by selecting the simulation period model configuration and or parameter values depending on which run complexity choice i e default lhs config comprehensive in the runs box in figure a1 is selected the notebook executes a specific set of code cells using a conditional statement logic e g if user selects config prob 1 step 2 7 is run which leads to config runs as shown in the runs box in figure a1 users need to carefully consider the number of basins and the length of the simulation period as the cjw cg environment is not powerful enough to run large simulations in a reasonable time in the hpc notebook we only provided the user with the option to run the most complex problem i e lhs config prob as the hpc is powerful enough to run the full problem making it unnecessary to allow for simpler problems the user can still change the simulation period in step 2 3 of the workflow in figure a3 the other main difference between the cjw cg and hpc notebooks is that the codes calculating kge values for the hpc notebook are executed on the hpc step 2 8 in hpc branch in figure a3 while for the cjw cg environment the kge values are calculated locally on cjw cg step 2 9 in cjw cg branch in figure a3 in the hpc environment the kge values are calculated on the hpc resource to prevent having to transfer large data volumes from the hpc to the cjw cg with the sole purpose of calculating performance metrics users can use globus to transfer selected output files from hpc to the cjw cg for additional analysis notebook 4 which exists only in the hpc environment was developed for this purpose and is discussed in section 2 3 4 a modified and scaled range between 1 and 1 version of the kge was used as an indicator of model output sensitivity to a change in input forcing based on the work of clark et al 2021 and mathevet et al 2006 and is described in the vb study the kge test compares hourly model outputs generated with the benchmark forcing dataset nldas dataset 1 table a2 with outputs generated with the forcing datasets with one forcing held constant cnst datasets 2 8 table a2 kge values are ranked from low to high to determine relative order of forcing influence on model outputs with highest rankings associated with least influence of change to 24 h constant forcing 2 3 3 post processing notebook the third notebook jn 3 post processing produces visualizations of the sensitivity of summa model output to the temporal resolution of the model forcing figure a4 shows the steps taken in the third notebook the notebooks for cjw cg and hpc environments are the same for the selected basin s eight plots are generated with notebook 3 that follow the analysis in the vb study the reader is referred to the supplementary materials and the vb study for a detailed explanation of each of the eight plots in this paper we only present the second figure generated by notebook 3 i e kge values for each output variable for all 8 default model runs 2 3 4 model output transfer the fourth notebook jn 4 use globus is only included in the hpc resource figure a5 to transfer summa output files from hpc to cjw on hydroshare to retrieve the data from the hpc this notebook needs a job id submitted to the hpc and created in notebook 2 while this notebook is running users can see the live status of the file transfer managed by the cybergis compute service once running of this notebook is successfully finished the user will be able to see the location of the transferred file on cjw 2 4 performance analysis we tested the performance of the cyberinfrastructure using a number of model scenarios using six years of simulation to be consistent with the vb study and varying the number of studied basins for each computational environment described in table 3 for the cjw cg environment we tested the performance of notebooks 1 3 for three scenarios table 3 rows 1 3 1 one basin a total of six years of simulations 2 four basins a total of 24 years of simulations and 3 six basins a total of 36 years of simulations we decided not to test the cjw cg environment for more basins as the cjw cg runs were slow and the hpc resource was available for larger simulations for the hpc environment we used expense hpc and tested the performance of notebooks 1 3 for 12 scenarios table 3 rows 4 15 in these scenarios we varied the number of allocated cpus 128 or 256 for parallelism and the total number of basins ranging from one basin a total of six years of simulations to 20 basins a total of 120 years of simulations which equals about three percent of the total simulation years for the whole vb study to test the performance of notebook 4 transferring output files from hpc to the cjw we only used scenarios hpc 256 1 to hpc 256 6 rows 4 9 in table 3 and repeated each transfer 5 times to obtain a range of run time for each of the scenarios 3 results and discussion in this section we first briefly present results of the modeling case study that served as a motivating use case for the cyberinfrastructure then we present results of the performance analysis focusing on contrasting the cjw cg and hpc notebooks using a variety of model setups then we summarize the resulting resources from this study that are shared on hydroshare finally we discuss the resulting system including opportunities and challenges identified through this research that can be the focus of future research 3 1 results of the modeling case study four camels basins with diverse characteristics table 4 were chosen as examples of the effect of basin characteristics on model results we specifically selected these four basins for this modeling case study because we found that they all show different patterns for the four selected basins fig 3 shows the kge values for each summa output variable using the default bil configurations box in figure a1 model configuration runs the runs consist of one reference simulation in which all forcing variables vary on an hourly basis nldas dataset 1 forcings box in figure a1 and seven simulations in which one forcing variable is held constant at the mean daily value throughout each day the seven datasets ppt to prs forcings box in figure a1 kge values were calculated relative to the reference simulation for each of the seven simulations using five years of hourly model output from 10 1 1991 9 30 1996 fig 3 demonstrates the variability in model output sensitivity to the temporal resolution of the forcing variables the first three basins gages 01632900 02212600 and 09378630 show a strong ppt temporal aggregation influence using default whereas gage 11264500 is more influenced by tmp hum and swr temporal aggregation in other words a higher temporal resolution is necessary for the aforementioned forcing variables in the given basins to capture the sub daily hydrologic response shown by the reference simulation the weaker influence of ppt temporal aggregation on the gage 11264500 compared to other gages can be attributed to its high fraction of precipitation falling as snow 0 91 as opposed to 0 1 0 01 0 5 table 4 also in fig 3 we see varying ranges in kge values for particular output variables as an example surfacerunoff is affected by constant hourly values of ppt for gages 01632900 and 09378630 ppt and hum for gage 02212600 and tmp hum swr wnd ppt and prs most to least dominant for gage 11264500 this shows the forcing variables in each basin that need to have a higher temporal resolution to reproduce the surfacerunoff output in the reference simulation in this section we only presented one example of an inter basin comparison to illustrate how different the results can be across different basins researchers can further explore the differences between individual basins using other plots that can be made using the interactive jupyter notebooks and also reproduce the results from the original vb study 3 2 results from performance analysis fig 4 shows the run time for the data processing notebook notebook 1 and the post processing notebook notebook 3 for the 15 scenarios listed in table 3 notebooks 1 and 3 are very similar between cjw cg and hpc computational environments notebooks 1 and 3 do not take a significant time to run because they are only preprocessing and output analysis notebooks and no simulations are run for scenarios with fewer than 30 simulation years notebook 1 takes longer than notebook 3 but this changes for scenarios with more simulation years as the rate of run time increase with simulation years is much higher with notebook 3 than with notebook 1 for the cjw cg environment the average time to run notebooks 1 and 3 across the tested scenarios only takes 0 6 of the entire time needed to run all notebooks 1 2 and 3 this means the time required to run data processing and post processing notebooks is not a limiting factor for running the simulations for the hpc environment this ratio increases to 8 5 and 11 3 when using 128 and 256 cpus respectively this dramatic increase in the ratio is due to the significant decrease in run time of notebook 2 when using hpc the run time for the summa execution notebook notebook 2 for the 15 model scenarios using different computation environments is shown in fig 5 the high rate of run time increase with increasing simulation years for the cjw cg environment emphasizes that while the cjw cg environment is technically able to simulate smaller models it might not be fast enough to run larger simulations in the case of running six basins for six years the hpc was 3 6 and 2 6 times faster than the cjw cg when using 256 and 128 cpus respectively hpc with 256 cpus scenario hpc 256 6 could finish the simulations for 120 years 3 percent of the vb study in 2nullh and 10nullmin while hpc with 128 cpus scenario hpc 128 6 could run the same problem in 1 48 times of the time need by hpc 256 6 using the hpc with 256 cpus assuming a conservative linear extrapolation the summa simulations from notebook 2 are expected to be done in about 75 hours for the entire vb study in summary hpc provides considerably faster simulations making them ideal to run for larger studies when using the hpc resource and in the case of 120 years of simulation dividing the number of the allocated cpus by two led to about a 50 increase in the run time and not 100 as one might expect this non linear scaling can be mainly attributed to 1 communication overhead in the computational resource that reduces scaling and 2 the fact that some parts of the codes in notebook 2 did not utilize parallelism for example kge values were only calculated after they were exported as netcdf files instead of being calculated directly from the raw summa output files the rate of run time increase for hpc with 128 cpus is higher compared to that for hpc with 256 cpus this may be attributed to the communication overhead because each cpu in the case of the hpc with 128 cpus needs to run twice as many simulations compared to hpc with 256 cpus the run time for transferring the summa output files from expanse hpc to cjw on hydroshare using the globus service integrated by cybergis compute service is shown in fig 6 each transfer was repeated 5 times to obtain a range of run time for each of the model simulations with a different total number of simulation years the range of the transfer time for each total number of simulation years is small indicating a consistent data transfer for 120 years of simulation it took 14 5nullmin on average to transfer 118 gb of data from hpc to cjw highlighting that the data transfer approach from hpc to cjw is fast and stable the transfer rate gb min is independent of data size fig 6 3 3 data organization in hydroshare the data for this study was pre processed and the output post processed by using existing python packages the study demonstrates the potential for using the online repository of hydroshare to not only store data and modeling code but to also store computational environments api version documentation and container installation hydroshare as a hydrology based repository service facilitated this by allowing all the parts of the problem to be stored together as one resource furthermore parts of the resource can be extracted and made into a new version of the resource updated revised or modified to promote collaboration to this point a hydroshare collection resource was created that contained three composite resources these resources are published and have digital object identifier doi which makes them immutable and findable fig 7 shows the landing page for the hydroshare collection resource that groups the three composite resources the three composite resources that are contained by this collection resource are shown in dialogue box 1 the related resources in box 2 refers to this paper and box 3 shows the information on how to cite this resource fig 8 shows the landing page for the hydroshare composite resource holding the hpc notebooks box 1 shows the contents of the resource most importantly the four jupyter notebooks and the readme md file the readme md file box 2 provides the user with the instructions on how to run the notebooks box 3 shows the information on how to cite this hydroshare resource 3 4 opportunities and challenges this study demonstrated a real world working implementation application of strategies for reproducible hydrologic modeling presented by choi et al 2021 to a large scale hydrologic study the vb study this section discusses the opportunities and challenges of this implementation if one needs to adopt this cyberinfrastructure for studies significantly differing from the vb study considerable changes or extra steps might be needed for instance 1 if exploring non camels basins then extra steps to prepare the inputs might be needed or 2 if using hydrologic models other than summa then containerization of the model might be needed despite the plausible challenges when making these non trivial extra steps the intended main opportunity here is that the modeling community can learn from the presented open cyberinfrastructure considering the commonalities among the hydrologic models with regard to the input data preprocessing processing and postprocessing steps needed by them knoben et al 2022 minimal changes in the notebooks are required to use the presented cyberinfrastructure to rerun parts or all of the vb study or to extend the experiments performed in that study for selected camels basins with these minimal changes a user could use 1 different camels basins 2 different parameters in the lhs set 3 different simulation periods e g a drought period 4 more than 10 lhs sets e g a more thorough exploration of the parameter space and 5 additional summa model configurations the last two changes i e using a larger number of lhs sets and different model configuration decisions highlights a major challenge in reproducing a computationally complex study here the limit on manageable data size was pushed even when running a few basins hpc computational power was required to run the full six years of simulation expanding the parameter exploration space or adding model decisions would compound the data size thus while this work is advancing cyberinfrastructure used for big data in hydrology challenges remain the second major challenge that is encountered is implementing version control what if users need to run the jupyter notebooks presented in this study in their own computational environment not deployed on cjw or they need to install a newer version of a model api how can they make sure they have a reproducible framework that is robust enough to tackle the version control problem because there are many individual pieces of software it was challenging at times for the study team to keep all the software versions synchronized we propose that future research should tackle the version control challenge by making the computational environment all documented and installable via a python environment file the pysumma code which is used for hydrology modeling was installed via conda just as the rest of the infrastructure in the future python package updates will break compatibility but compatibility can be preserved by installing the older versions as documented in the environment file or the user understanding the updates in order to manually work around the updated package incompatibility if a researcher wants to use a newer future version of pysumma then they may need to debug some parts of the jupyter notebooks that are affected by the changes while this is not an ideal way to handle version updates at least the researcher has options of a working albeit older computational environment from which to begin reproducing the study before updating to newer software the specifics of the environment can be placed in a python environment yml file that can be shared as part of the online model and data repositories and can be installed with an installation notebook inside the repository this can use best practice for transparency about what dependencies the computational gateway interface notebooks need to run the specifics of each dependency can be described in the installation notebook so that if in the future there are issues with the availability of that dependency then a suitable substitute can be found version control issues can be thus addressed through this methodology albeit an imperfect solution depending on possible user troubleshooting in addition to the two major challenges described above there are two additional challenges related to the use of the hpc environments 1 large data transfers between computational environments online data repositories and a user s personal computer and 2 allowing users to execute their workflows on different hpc environments based on their use case and access to hpc environments there may be cases for example where users does not want to utilize hpc resources due to financial cost concerns and need to transfer a large amount of model outputs from an hpc environment s temporary scratch directory to a jupyter compute environment to further analyze the data using the jupyter compute environment transferring large datasets e g the entire output from vb study or even the four selected basins study explored in this paper would be slow and unreliable using standard data transfer approaches i e compress data into a big package and then transfer it in this study we used globus to do this data transfer which can transfer multiple individual files in parallel without a need to compress data a big package and other related cyberinfrastructures that do not currently use globus or a related technology could benefit from doing so globus is not limited to data transfers between the hpc environment and the jupyter compute environments cjw in the case of this study however in fact it is possible that the full or a large portion of the model output can be stored on an online data repository or even on a user s own personal computer in either case the online data repository or the user s personal computer the outputs could be downloaded using globus if globus is installed and they become a globus server making a user s personal computer a globus server may be the case that the user prefers to back up a model run not in an online data repository but at some other location in this case globus could be used to connect directly with the hpc environment thereby bypassing both any jupyter compute environments cjw in the case of this study as well as online data repositories hydroshare in the case of this study as an intermediate storage location if the large data takes much of the space in the user s personal computer user may consider transferring it to external hard drives that offer larger capacity to allow users to execute their workflows on different hpc environments users would need to set up their own job submission service and configure the jupyter environment e g cjw to the specific hpc environment that they have access to although the job submission software used in this study is open source it is customized for the uiuc hpc used in the study so it cannot be directly used for other hpcs future work could be for cjw to act as a connector to user supplied hpc environments in this case cjw would ask users to provide their own credentials and to their own hpc rather than only using the uiuc hpc service while not a simple task standardization of job submission approaches across hpc environments makes this functionality possible generalizing the approach through future research could benefit users to access their own institutional hpcs and other hpcs at the national level that the user has access to 4 conclusions the importance of reproducibility is broadly recognized across different scientific disciplines when it comes to computational hydrology this can be a significant challenge this research shows how an architecture that integrates the 1 online data repositories 2 computational environments and 3 model api can facilitate reproduction of the components of modern and complex hydrologic studies for this purpose we used a recently published large scale hydrologic study vb study as an example we designed and built cyberinfrastructure that utilized software components to enable intuitive and online access to computational environments this approach was used to remove the potential software inconsistencies from users differing personal software editions as well as to make implementation easier with pre compiled software with the added complication of a computationally expensive research problem instead of a case study this approach gave the user the option to use either the cjw cg or hpc computational environments depending on how much they need to reproduce a problem more representative of the big data problem using hydroshare as the data repository and containerization of the pysumma api with docker or singularity in the case of the hpc environment along with a computational gateway interface of jupyter notebooks both hosted on the cjw made this possible three jupyter notebooks for the cjw cg environment and four jupyter notebooks for hpc environment were developed notebooks 1 3 for both cjw cg and hpc environments enable 1 preparing the forcing data simulation period and study camels basins 2 executing summa hydrologic model and 3 visualization of the results notebook 4 only developed for the hpc environment enables transferring large data from hpc to the scientific cloud service i e cjw using globus service integrated by cybergis compute in a reliable high performance and fast way we presented a modeling case study subset from the vb study that served as a motivating use case for the cyberinfrastructure the case study showed how four individual basins with different characteristics can lead to different patterns of temporal aggregation for each of the forcing variables given the same model setup the case study served to show that the developed cyberinfrastructure enables others to reproduce the vb study for subsets of the original domain as a basis for doing additional research enabling conclusion reproducibility beyond bit reproducibility we analyzed performance of the notebooks focusing on contrasting hpc and cjw cg notebooks using a variety of model scenarios the hpc environments could perform significantly faster simulations compared to cjw cg enabling users to explore a large number of basins and simulation periods this clearly showed how the use of hpc from a jupyter gateway could advance the reproducibility of modern and complex hydrologic studies the run time performance analysis for the big data transfer notebook for the hpc environment showed that the method used was stable reliable and fast therefore similar studies could easily benefit from the same approach for transferring large data between scientific cloud services with the focus of this research was on conclusion reproducibility over bit reproducibility of the vb study users can easily modify the notebooks to test different situations by varying the study basins and periods parameterizations and model configurations these situations highlighted two major challenges first the complexity of the big data problem eventually became large enough that it needed to be run using the hpc computation environment which presented other smaller challenges of data transfer and portability of the hpc environment second implementation of a version control system was needed e g when a user needs to install a newer version of a model api or when a user needs to run these codes on their local machine rather than the used cloud based computational environment sharing the dependencies of the computational environments as a python environment yml file and an installation notebook that installs them was discussed as a future solution to tackle the version control issue finally as a broader impact the vb study methodology replicated with interactive codes could also serve as a valuable educational resource allowing educators to present sophisticated modeling experiments for use within classrooms through online python notebooks likewise the basic approach could be extended to enable new water decision support systems that take advantage of the summa framework and hpc yet remain easy to interact with through notebooks this can help to for example evaluate forcing sensitivity to a water resources management objective or explore the parameter and model uncertainties of summa using different algorithms such as markov chain monte carlo mcmc and bayesian model averaging bma samadi et al 2020 in a systematic manner with more work to harden and improve the usability of the system presented here these additional use cases can be possible resource description reference original nldas forcings for the camels basins can be obtained as a netcdf file mizukami and wood 2023 summa simulations using camels datasets on cybergis jupyter for water choi et al 2023b summa simulations using camels datasets for hpc use with cybergis jupyter for water choi et al 2023c the data from the camels dataset newman et al 2015a was consolidated into one netcdf file taking advantage of opendap data services supported by the hydroshare thredds server and web application connector tarboton and calloway 2021 the summa setup for the camels basins can be obtained from the summa camels folder of the hydroshare resources list of relevant urls cybergis jupyter for water https go illinois edu cybergis jupyter water docker https www docker com hydroshare rest api https www hydroshare org hsapi numpy https www numpy org pandas https pandas pydata org pysumma https github com uw hydro pysumma releases tag v3 0 3 seaborn https seaborn pydata org singularity https sylabs io summa https github com ch earth summa releases tag v3 0 3 xarray http xarray pydata org xsede https www xsede org declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments this work was supported by the national science foundation nsf under collaborative grants 1664061 1664119 and 1664018 for the development of hydroshare http www hydroshare org 1928369 for the integration of reproducibility methods into hydroshare the work also was supported by the institute for geospatial understanding through an integrative discovery environment i guide that is funded by nsf under award no 2118329 any opinions findings and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of the nsf appendix this section provides supplemental material to support our methods and results the figures and tables are referred to in the main text fig a1 an overview of the forcing datasets forcings yellow box parameter sets parameters blue box and model configurations configurations green box used in the 704 summa model runs runs pink box performed for each of the 671 camels basins note the pink numbers that follow each forcing parameter and configuration refers to the summa model run set as numbered in the pink runs box e g the default parameter set in the parameters box is used with summa model runs 1 2 3 and 4 in the runs box source modified from van beusekom et al 2022 fig a1 fig a2 the preprocessing notebook jn1 diagram fig a2 fig a3 running summa notebook jn2 diagram fig a3 fig a4 post processing notebook jn3 diagram fig a4 fig a5 hpc data transfer notebook jn4 diagram fig a5 table a1 summa output variables chosen for analysis source van beusekom et al 2022 table a1 variable type summa variable name description units 1 liquid water fluxes for the soil domain surfacerunoff surface runoff m s 1 2 aquiferbaseflow baseflow from the aquifer m s 1 3 infiltration infiltration of water into the soil profile m s 1 4 rainplusmelt rain plus melt m s 1 5 soildrainage drainage from the bottom of the soil profile m s 1 6 turbulent heat transfer latheattotal latent heat from the canopy air space to the atmosphere w m 2 7 senheattotal sensible heat from the canopy air space to the atmosphere w m 2 8 snowsublimation snow sublimation frost below canopy or non vegetated kg m 2 s 1 9 snow swe snow water equivalent kg m 2 10 vegetation canopywat mass of total water on the vegetation canopy kg m 2 11 derived netradiation net radiation w m 2 12 totalet total evapotranspiration kg m 2 s 1 13 totalrunoff total runoff m s 1 14 totalsoilwat total mass of water in the soil kg m 2 table a2 parameters chosen for latin hypercube sampling source van beusekom et al 2022 table a2 parameter name minimum maximum default constraints k macropore 1 0d 7 0 1 0 0001 k soil 1 0d 7 1 0d 5 variable theta sat 0 3 0 6 variable critsoiltranspire fieldcapacity theta res aquiferbaseflowexp 1 10 2 0 aquiferbaseflowrate 0 0 1 0 1 qsurfscale 1 100 50 summerlai 0 01 10 3 frozenprecipmultip 0 5 1 5 1 heightcanopytop 0 05 100 variable heightcanopybottom heightcanopybottom 0 5 variable routinggammashape 2 3 2 5 routinggammascale 1 100000 20000 albedorefresh 1 10 1 0 tempcritrain 272 16 274 16 273 16 windreductionparam 0 1 0 28 the eight plots generated by notebook 3 are described as follows 1 location of the selected camels basin 2 kge values for each cnst forcing dataset datasets 2 8 table a2 by output variable using the default model runs this is a subset of figure 9a from van beusekom et al 2022 3 boxplots depicting the range in the kge values for each set of model runs default lhs config and comprehensive table a1 by output variable note boxplots only appear for the model runs selected in notebook 2 this is a subset of figure 9b from van beusekom et al 2022 4 boxplots depicting the range in the kge values for each set of model runs default lhs config comprehensive table a1 by cnst forcing dataset datasets 2 8 table a2 note boxplots only appear for the model runs executed in notebook 2 this is a subset of figure 9c from van beusekom et al 2022 5 ranks 1 7 stacked barplots depicting the relative basin kge rank counts by cnst forcing dataset datasets 2 8 table a2 for the 14 summa output variables note bars on this plot will only appear if the comprehensive basin runs are executed in notebook 2 this is a subset of fig 8 from van beusekom et al 2022 6 ranks 1 7 stacked barplots depicting the relative basin kge rank counts by cnst forcing dataset datasets 2 8 table a2 for the eight summa configurations note the complete figure will only appear if the comprehensive basin runs are executed in notebook 2 a stacked bar for the default configuration bll will be plotted if the lhs basin runs are executed in notebook 2 this is a subset of fig 8 from van beusekom et al 2022 7 boxplots for each output variable depicting the range in the seven summed kge values from cnst forcing datasets 2 8 for the eight summa configurations or for the default configuration if only the default configuration was run default or lhs basin runs in notebook 2 this is a subset of fig 6 from van beusekom et al 2022 8 boxplots depicting the range in the summed summa hourly output variables over the period of record produced using the benchmark nldas forcing dataset for the eight summa configuration or for the default configuration if only the default configuration was run default or lhs basin runs in notebook 2 note a point will appear instead of a boxplot if only the default parameter set was run default or config basin runs in notebook 2 this analysis is not in van beusekom et al 2022 it is included in the interactive tool to supply users with potential summa output variable ranges for their selected basin to reproduce the modeling case study presented in the current paper the selected four basins need to be specified in notebook 1 figure a2 step 1 3 2 select basins and simulation period and then notebook 3 can be used to reproduce fig 3 kge values using the default model runs for each cnst dataset datasets 2 8 table a2 grouped by summa output variable 
25408,building cyberinfrastructure for the reuse and reproducibility of complex hydrologic modeling studies iman maghami a ashley van beusekom b lauren hay b zhiyu li c andrew bennett d youngdon choi a bart nijssen b shaowen wang c david tarboton e jonathan l goodall a a department of engineering systems environment university of virginia charlottesville va usa department of engineering systems environment university of virginia charlottesville va usa department of engineering systems environment university of virginia charlottesville virginia usa b department of civil environmental engineering university of washington seattle wa usa department of civil environmental engineering university of washington seattle wa usa department of civil environmental engineering university of washington seattle washington c department of geography geographic information science university of illinois at urbana champaign il usa department of geography geographic information science university of illinois at urbana champaign il usa department of geography geographic information science university of illinois at urbana champaign il usa d department of hydrology and atmospheric sciences university of arizona tucson az usa department of hydrology and atmospheric sciences university of arizona tucson az usa department of hydrology and atmospheric sciences university of arizona tucson az usa e department of civil and environmental engineering utah water research laboratory utah state university logan ut usa department of civil and environmental engineering utah water research laboratory utah state university logan ut usa department of civil and environmental engineering utah water research laboratory utah state university logan utah usa corresponding author university of virginia department of engineering system and environment university of virginia 151 engineers way p o box 400747 charlottesville va 22904 usa university of virginia department of engineering system and environment university of virginia 151 engineers way p o box 400747 charlottesville va 22904 usa building cyberinfrastructure for the reuse and reproducibility of large scale hydrologic modeling studies requires overcoming a number of data management and software architecture challenges the objective of this research is to advance the cyberinfrastructure needed to overcome some of these challenges to make such computational hydrologic studies easier to reuse and reproduce we present novel cyberinfrastructure capable of integrating hydroshare an online data repository cybergis jupyter for water and high performance computing hpc resources computational environments and the structure for unifying multiple modeling alternatives summa hydrologic modeling framework through its application programming interface for orchestrating model runs the cyberinfrastructure is demonstrated for a complex computational modeling study on a contiguous united states dataset we present and discuss key capabilities of the cyberinfrastructure including 1 containerization for portability across compute environments 2 globus for large data transfers 3 a jupyter gateway to hpc environments and 4 jupyter notebooks for capturing the modeling workflows keywords reproducibility computational hydrology jupyter hpc containerization data availability the data and jupyter notebooks used in this study were published on hydroshare as a collection resource with persistent dois 1 introduction reproducibility the ability to duplicate and verify previous findings is a foundational principle in scientific research in computational hydrology melsen et al 2017 highlighted two contrasting definitions of model reproducibility 1 bit reproducibility which is defined as exact replication of a study including the exact same numbers forming the results and 2 conclusion reproducibility which focuses on reproducibility of the conclusions of a study as the conclusions are expected to hold if the same experimental approach is applied they argue that conclusion reproducibility replicating a study s conclusions may be more important than bit reproducibility exactly replicating model runs because hydrological theories need to be tested beyond bit reproducibility by investigating conditions under which theories can be confirmed or falsified even so conclusion reproducibility itself goes beyond the simple sharing of code and data as open source and online resources typically touted for achieving reproducibility the code and data must be accompanied by well documented workﬂows with readable and reusable code chen et al 2020 mullendore et al 2021 simmonds et al 2022 reusable code requires providing open source computational environments in which the code can be executed ensuring this reuse and reproducibility is a non trivial task it requires not only adopting new capabilities for handling complex software and big data it also requires careful software engineering practices to integrate these new capabilities into well designed and built cyberinfrastructure merkel 2014 a growing body of researchers have been discussing and proposing guidelines and strategies for reproducible computational modeling e g bush et al 2021 choi et al 2021 knoben et al 2022 mullendore et al 2021 simmonds et al 2022 in recent work knoben et el 2022 presented a novel approach for creating a hydrologic model at any location or scale local to global by separating model agnostic and model specific configuration steps within cyberinfrastructure workflows choi et al 2021 described a general strategy for creating modern cyberinfrastructure to support open and reproducible hydrologic modeling as the integration of three components 1 online data repositories 2 computational environments leveraging containerization and self documented computational notebooks and 3 application programming interfaces apis that provide programmatic control of complex computational models as an example of this general approach choi et al 2021 also presented an implementation that used 1 hydroshare as the online repository 2 two different jupyter instances one hosted by the consortium of universities for the advancement of hydrologic science inc cuahsi and a second hosted by cybergis jupyter for water as the computational environments and 3 pysumma a python wrapper for manipulating running managing and analyzing of summa structure for unifying multiple modeling alternatives as the model api while choi et al 2021 focused mainly on the system design and demonstrated their approach with a fairly simple modeling use case reproducibility in computational hydrology can present some difficult challenges when dealing with large scale hydrologic studies hutton et al 2016 these challenges mostly pertain to the use of big data and computationally expensive and time consuming resources needed for reproducibility of complex hydrologic modeling studies hutton et al 2016 notes that in these cases new techniques are needed to ensure scientific rigor in this paper we provide an example of the overall system design outlined by choi et al 2021 as applied to a complex hydrologic study by van beusekom et al 2022 hereafter referred to as the vb study we develop the necessary cyberinfrastructure to reproduce this study for selected sub domains and discuss the challenges and opportunities in ensuring conclusion reproducibility for complex hydrologic studies the vb study evaluated the effect of the temporal resolution of surface meteorological inputs or forcings on modeled hydrological fluxes and states for 671 basins across the contiguous united states conus it quantified the difference in hydrologic outcomes based on daily or sub daily forcings for multiple model configurations and parameter values reproducibility of the vb study if one was given only the input data and model code would be challenging because it requires the installation and configuration of the modeling framework summa clark et al 2015a 2015b the data volumes are very large and the model runs require high performance computing hpc resources the complete vb study consisted of 704 6 year model runs for each of the 671 basins or 2 8 million model years summa was implemented with a single hydrologic response unit for each basin resulting in a single output time series for each basin for each model configuration for every model run the output consisted of 14 hydrological variables which required 6 mb per model simulation or 2 834 tb for the entire study while few researchers may be interested in reproducing the entire vb study the more common use case and the focus of this study would be to repeat or extend the vb study for a subset of the basins we want to enable others to reproduce the vb study for subsets of the original domain as a basis for doing additional research enabling conclusion reproducibility rather than the bit reproducibility for such an approach to be effective it is not sufficient to provide the open source summa code and model input data one must also provide the additional components described by choi et al 2021 i e computational environments models exposed through apis and documented model workflows to create a cyberinfrastructure that lowers the barrier to reuse and reproducibility this research contributes to the growing literature advancing cyberinfrastructure for hydrology and other geoscience fields yang et al 2010 illustrated the importance of using hpc in computationally intensive geospatial sciences and hydrologic modeling essawy et al 2016 demonstrated server side workflows for large scale hydrologic data processing although they did not make use of hpc in their application lyu et al 2019 used containerization and combined computational environments including hpc and high throughput computing htc cyberinfrastructure to directly run the models using jupyter notebooks gan et al 2020 integrated a hydrologic data and modeling web service with hydroshare as a data sharing system to show how this integration leads to a findable and reproducible modeling framework gichamo et al 2020 used web based data services to prepare input data for hydrologic models kurtz et al 2017 introduced a cloud based real time data assimilation and modeling framework and showed how parallel processing can be used for complex hydrologic models in the cloud however unlike the vb study none of lyu et al 2019 gan et al 2020 gichamo et al 2020 and kurtz et al 2017 applied their methods on a computationally extensive complex hydrologic use case therefore the challenges and opportunities of using cyberinfrastructure for reproducibility of complex large scale hydrologic modeling for which hpc and big data approaches are required remain largely unexplored to address this research gap we designed and implemented cyberinfrastructure to enable intuitive access to hpc computational environments and to support data transfers into and out of the hpc environment additionally we provide a workflow that allows users to replicate parts of the study within their own computing environments we also perform a workflow run time performance analysis that compares different model scenarios by varying the size of simulations across different computing environments providing users with a guide towards selection of the computing environment depending on the size of their simulations the cyberinfrastructure provides a starting point for users to modify the hydrologic model setups thus going beyond reproducibility i e the ability to duplicate and verify previous findings into replication where one modeling methodology can be used to answer the same scientific research question but with new input data as highlighted by essawy et al 2020 the cyberinfrastructure may also serve as an educational resource by providing an intuitive way for students to perform complex hydrologic modeling studies the data and cyberinfrastructure are provided through hydroshare to run on any basin for which we provide a summa setup to assist the modeler in analyzing basins individually the remainder of this paper is organized as follows in section 2 we provide a brief overview of the vb study the cyberinfrastructure the model workflows and the model scenarios used for a science use case subsetted from the vb study as well as the model workflows run time performance analysis section 3 provides results and discussion the results focus on the modeling use case and an analysis of the workflow run time performance for different computing environments the discussion focuses on opportunities and challenges learned from our experience designing and building the cyberinfrastructure to support our modeling workflows finally our conclusions and recommendations are provided in section 4 2 methods 2 1 overview of the vb study the vb study used 671 basins to study the effects of the temporal resolution of the meteorological forcings on hydrologic model simulations across the conus the basins are part of the camels dataset catchment attributes and meteorology for large sample studies newman et al 2015b a large sample hydrometeorological dataset across the conus consisting of input forcings basin attributes and relevant historical streamflow records the vb study used summa clark et al 2015b to configure multiple model instances for each basin representing eight different model configurations and 11 different sets of model parameter values in addition eight forcing datasets were constructed in each of these forcing datasets one of the meteorological inputs was modified so that the diurnal cycle was replaced by the mean value over that day the vb study performed 704 8 11 8 704 6 year model runs for each camels basin consisting of one year of model initialization and five years of actual simulation model outputs for 14 simulated variables were stored to evaluate the sensitivity of the simulations to changes in model forcings model configurations and model parameters figure a1 and table a1 the vb study results demonstrated that 1 the effect of each forcing input on each model output varies by model output and model location 2 the use of a particular parameter set may not be critical in determining the most and least influential forcing variables and 3 the choice of model physics i e using different model configurations could change the relative effect of each forcing input on model outputs the vb study was run with scripts on the cheyenne supercomputer a 5 34 petaflops high performance computer built for the national center for atmospheric research computational and information systems laboratory 2017 and it took a few days to complete the runs for each basin the output size for a single 6 year run was 6 mb thus reproducing the entire study is computationally expensive and also requires large amounts of storage 704 runs 671 basins 6 mb 2 834 tb however the cyberinfrastructure allows individual basins to be run independently here we focus on a use case in which a researcher wishes to reproduce a subset of the vb study by analyzing one or a few basins within a cloud cyberinfrastructure environment to reach conclusion reproducibility the conclusion reproducibility that we aimed in this study is solely a qualitative one and if the presented cyberinfrastructure can be successfully applied to studies differing from the original study i e the vb study the conclusion reproducibility is achieved 2 2 cyberinfrastructure design and implementation following the approach described in choi et al 2021 we designed and implemented cyberinfrastructure fig 1 to replicate the vb study by integrating 1 the hydroshare online data repository 2 cybergis jupyter for water computing gateway cjw cg and high performance computational environments and 3 a model api that can be utilized in scripts using jupyter notebooks here the pysumma api each of these three components is further explained in the following subsections 2 2 1 online data repositories we used hydroshare an online collaboration environment as the online data repository horsburgh et al 2016 tarboton et al 2014 a collection resource in hydroshare which can be found at choi et al 2023a contains three resources holding the data computational environment and models fig 1 the hydroshare resource holding the data mizukami and wood 2023 contains the forcing dataset for the 671 camels basins the forcings are based on the hourly nldas 2 north american land data assimilation system nldas 2 2014 nldas 2 is hereafter referred to as nldas the original nldas hourly forcing data were created on a 0 125 0 125 grid to create hourly summa model forcings nldas outputs were spatially averaged over each of the 671 camels basins and merged into one netcdf file with this format an opendap server opendap 2021 can extract data for selected basins on the server so that the user does not have to download the entire conus dataset to a local computer hydroshare offers this capability via its thredds data server tarboton and calloway 2021 2 2 2 computational environments the developed computational environments provide a consistent software environment that is independent of each user s own operating system and software libraries making it possible to study a computationally expensive research problem fig 2 shows each computational environments component and the interoperability between the computational environments and hydroshare one computational environment was implemented on the cjw cg cloud service for studies with limited computational demand e g a study of only a few basins or as an instructional tool or for model debugging a second computational environment was developed on an hpc resource to reproduce a problem more representative of challenges posed by the use of big data in the vb study the hpc environment also allows the user to study a particular basin in greater detail in this study the cjw cg computational environment is used to provide 1 the model execution environments configured as docker images to enable execution of the summa model for studies with limited computational demand i e those need to use cjw cg workflow and 2 cyberinfrastructure for preprocessing postprocessing and data storage for both studies with limited computational demand need to use cjw cg workflow and with high computational demand i e those need to use hpc workflow fig 2 the hpc computational environment is only used for providing model execution environments configured as singularity containers to enable execution of the summa model for studies with higher computational demand more details on each computational environment are provided in the rest of this section cjw cg is a cloud computing environment interoperable with hydroshare it is an instance of cybergisx yin et al 2017 that serves the data and computation intensive needs of the water and environmental communities we used cjw cg because it is publicly available is interoperable with advanced cyberinfrastructure resources such as the hpc resource used in this study and has been serving the water and environmental communities to support their modeling needs reproducibility was facilitated by using containerization of the summa model and the pysumma api with docker merkel 2014 in the case of the cjw cg environment or singularity in the case of the hpc environment kurtzer et al 2017 along with a computational gateway interface to jupyter notebooks pysumma and the notebooks are described in a later section fig 2 although using docker is a common approach to containerize the model dependencies we used singularity in the hpc environment because it is designed to work seamlessly with existing batch job systems to support hpc applications the containerization and interface are hosted on the cjw scientific cloud service hosted on jetstream cloud hancock et al 2021 stewart et al 2015 towns et al 2014 the dockerfile is hosted on a github repository li 2021 with pre built docker images being shared on a docker hub repository singularity container used by the hpc environment is hosted on cybergis compute service a middleware platform allowing seamless access to hpc resources via python based software development kit and core middleware services cybergis compute service 2021 li et al 2022 the singularity container was created through docker images conversion cybergis compute service also handles submitting jobs to hpc as well as large data transfer from hpc through globus will be discussed in section 2 2 4 the conda software package was used to manage the project specific computational environment on cjw allowing the user to build a python environment with the summa model pysumma api and other computational dependencies this was done by providing a kernel version for the project cybergis center hydroshare development team 2022 using this stable kernel which captures all the required dependencies with their specific versions ensures careful software version control 2 2 3 model application programming interface api the model api pysumma was chosen to be part of the interactive tool the pysumma api choi et al 2021 wraps the summa hydrologic modeling framework clark et al 2015a and allows the user to script the use of the summa model using python it facilitates model configuration and allows for local execution of the model by either using a docker container or a locally compiled summa executable choi et al 2021 with pysumma a user can modify summa input files and run summa inside a python script as well as automatically parallelize runs and visualize output in the simplest case the pysumma simulation object wraps a single instance of a summa simulation for users who choose to analyze multiple basins at a time in the cjw cg environment instead of the hpc environment the notebook automatically will configure a pysumma distributed object which provides an interface to spatially distributed simulations and handles parallelism and job management under the hood in this study multiple summa simulations are run in each basin so a pysumma ensemble object is used to manage multiple runs with different configurations in the hpc computational environment a custom backend was written to handle parallelism using message passing interface mpi reducing the need for users to customize the configuration based on the type of job that they are running a high level description of pysumma is presented in fig 1 the simulation py enables the execution of the summa model and along with file manager py decisions py force file list and output control py allows for manipulating summa configuration files the distributed py enables the parallel execution of summa 2 2 4 data management and transfer the input data for this study consists of the summa configuration files and the forcing data for the 671 camels basins the configuration files e g geometries information for the 671 camels basins along with their attributes such as hru id are shared within each of the two hydroshare resources holding the jupyter notebooks the forcing data are provided in a hydroshare resource mizukami and wood 2023 the output files resulting from running the notebooks using the cjw cg and hpc computational environments are 1 netcdf output files generated by the summa simulations 2 a netcdf file recording the model performance for each basin as measured by the kling gupta efficiency kge gupta et al 2009 and 3 additional files created by the notebooks such as the figures that visualize the model results in the case of the cjw cg environment after running the notebooks all files are saved in the cjw cg and are directly accessible to the user in the case of the hpc environment the kge results and other files created by the notebooks e g figures are automatically transferred to the cjw cg but the netcdf output files remain within the hpc environment to avoid transferring large volumes of model output as a reminder the size of the model output for the entire vb study was 2 834 tb however if the user of the hpc environment wishes to transfer selected summa netcdf output files from the hpc to be directly accessible for further analysis and long term storage then the cybergis compute service li et al 2022 can be used for reliable high performance large file transfers through the globus service chard et al 2016 foster 2011 as shown in fig 2 data is transferred from hpc to the cjw using globus without going through the job submission server globus is a software as a service that enables the transfer of datasets of any size between different storage options personal computers hpc etc without users being required to be constantly logged in and monitoring the data transfer chard et al 2016 technically the cybergis gis compute acts as a globus app client holding a community globus account that has access to both data endpoints on the jupyter and target hpc when data transfer is needed cybergis compute initiates a globus task between the two endpoints and monitors the progress users are updated with data transfer status in the notebooks environment during the entire process 2 3 model workflows as jupyter notebooks as mentioned earlier the model workflows allow the user to reproduce all or subsets of the vb study using either the cjw cg computational resources referred to later as cjw cg or the hpc and cjw cg computational resources referred to later as hpc the cjw cg and hpc hydroshare resources can be found at choi et al 2023b and choi et al 2023c respectively the model workflows are documented in three for cjw cg or four for hpc jupyter notebooks table 1 shows the summary of the steps taken in each notebook while figure a2 a5 show more detailed information for notebooks 1 4 the first three notebooks for both the cjw cg and hpc environments focus on 1 selecting the study basins simulation period and model input forcings 2 running the summa model and 3 exploring outputs to analyze the effect of each forcing variable in each basin the hpc computational resource uses a fourth notebook to transfer large unprocessed output data from the hpc to cjw using globus notebooks 1 and 3 are very similar between the two hydroshare resources and both cjw cg and hpc hydroshare resources use cjw cg computational resources to run these two notebooks the second notebook differs for the two environments and the difference is explained in section 2 3 2 these notebooks assist a modeler in analyzing camels basins individually providing information on forcings and output variables that are the most least sensitive in their basin with some additional work the cjw cg computational environment can also be hosted on other non cjw cloud services but the hpc environment is more tailored to interact with the cjw cloud service used here to use the hpc computational resource the user must obtain access to the hpc by issuing a request through hydroshare to use cjw once this access is granted users are automatically given free access to two alternative hpc resources 1 the virtual roger resourcing open geospatial education and research hpc administered by the school of earth society and environment at university of illinois urbana champaign uiuc which is integrated with the keeling compute cluster at uiuc virtual roger user guide 2022 and 2 the expanse hpc a much larger nsf xsede resource operated and managed by san diego supercomputer center sdsc expanse system architecture 2022 in theory the cybergis compute service can support other hpcs as well but we did not test other hpcs in this study among the provided hpc options we only used expanse to demonstrate the cyberinfrastructure in our initial experiments expanse hpc performed faster than virtual roger and the goal here was to show how a hpc can scale up a study by speeding up the modeling process compared to a non hpc environment rather than an inter comparison between different hpcs users who do not wish to use hpc computational resources can use cjw cg computational resources directly to run smaller modeling jobs the hardware specifications of the cjw cg and the expanse hpc are compared in table 2 the cjw cg has only three compute nodes each of which has eight cpus with 1 996 ghz clock speed and 30 gb dram each user can only use up to six cpus and the cpus can be shared among users this means the maximum degree of parallelism for simulations using this computational resource is six thus in case of running one basin from the vb study 704 runs and using all the six available cpus each cpu will need to run 117 33 simulations some of them 117 and others 118 simulations the expanse hpc has 728 amd rome standard compute nodes each of which is equipped with 256 gb dram and 128 2 25 ghz cpus expanse user guide 2022 the expanse hpc allows the user to only use up to two nodes at a time i e 256 cpus or the maximum degree of parallelism for simulations thus if a user is running one basin from the vb study 704 runs and using all the available 256 cpus then each cpu will need to run 2 75 simulations some of them two and others three this shows how the hpc resource can scale up the model runs offering a high performance tool more details about the run time performance of the notebooks are discussed in the results and discussion section the following subsections discuss the general purpose of each notebook used to reproduce parts of the vb study for specific coding details refer to the notebooks in the hydroshare resources at choi et al 2023b and choi et al 2023c 2 3 1 data processing notebook the first notebook jn 1 preprocessing processes the original camels summa files and the input forcing datasets table a2 the user can select one or more camels basins 1 671 basins but by selecting a higher number of basins the computational time and expense increases notebook 1 subsets the original camels summa files producing summa attributes parameters initial conditions and hourly nldas forcing files for the selected basin s then additional forcing datasets for the hydrologic model sensitivity study are developed from the nldas data files forcings box in figure a1 as discussed below for each summa model setup variations in 14 summa generated outputs described in table a1 are examined with respect to variations in seven input forcings air pressure prs air temperature tmp long wave radiation lwr precipitation rate ppt specific humidity hum shortwave radiation swr and wind speed wnd under different model parameterizations and configurations the summa outputs generated with the 1 h nldas forcing dataset are considered the benchmark nldas dataset 1 forcings box in figure a1 the rest of datasets ppt to prs datasets forcings box in figure a1 are developed holding each of the individual forcing variables constant over a 24 h period while the other six forcing variables contain the original hourly nldas values figure a2 shows the steps taken in the first notebook this notebook is the same for the cjw cg and hpc environments except that the simulation time period and basins to be explored are pre populated differently the user can change these setups in the third step of this notebook step 1 3 in the last step of this notebook users can visualize the individual forcing variables held constant over a 24 h period against the original hourly nldas values using hourly and cumulative plots 2 3 2 summa execution notebook the second notebook jn 2 running summa executes the summa model using the input data from the first notebook for four different sets of summa basin runs outlined in figure a1 runs box and described in detail in the vb study the first set of basin runs default 8 summa runs per basin runs box uses the eight forcing datasets forcings box combined with default parameters and a default summa configuration the summa default configuration is set in the resource model decision file the second set of basin runs lhs 88 summa runs per basin runs box in figure a1 uses the eight forcing datasets combined with 11 parameter sets and a default summa configuration the 11 parameter sets consist of the default parameter set and 10 additional parameter sets with 15 commonly calibrated parameters table a2 as detailed in the vb study the parameters are sampled using latin hypercube sampling lhs over their defined range the pydoe lhs function lee 2014 is used to create unique 10 15 lhs sampling matrices for the selected basin then the lhs matrices are used to produce 10 parameter sets of the 15 parameters while considering the parameter constraints listed in table 2 the choice of a different seed value will lead to different lhs sets and these sets will be different from the ones used by the vb study the third set of basin runs config 64 summa runs per basin runs box in figure a1 uses the eight forcing datasets combined with the default parameter set and eight summa configurations the eight summa configurations outlined in the configurations box in figure a1 test three model decisions stomatal resistance stomresist choice of snow interception parameterization snowincept and choice of canopy wind profile windprfile with two options for each decision note the default configuration for this study is shown in bold in the configurations box in figure a1 ballberry lightsnow and logbelowcanopy the fourth set of basin runs comprehensive 704 summa runs per basin runs box in figure a1 includes the default lhs and config basin runs and is the only set that needs to be run to replicate a single basin sensitivity study following the vb study method six years of simulation must be run for replication for testing purposes sets 1 3 can also be run by themselves the 10 parameter set files for the basin from the lhs sampling plus the default parameters 11 parameter sets are run each with eight summa configurations configurations box in figure a1 figure a3 shows the steps taken in the second notebook the first two steps in this notebook are the same for the cjw cg and hpc environments but the rest of the workflow differs in the cjw cg notebook the user can define the simulations by selecting the simulation period model configuration and or parameter values depending on which run complexity choice i e default lhs config comprehensive in the runs box in figure a1 is selected the notebook executes a specific set of code cells using a conditional statement logic e g if user selects config prob 1 step 2 7 is run which leads to config runs as shown in the runs box in figure a1 users need to carefully consider the number of basins and the length of the simulation period as the cjw cg environment is not powerful enough to run large simulations in a reasonable time in the hpc notebook we only provided the user with the option to run the most complex problem i e lhs config prob as the hpc is powerful enough to run the full problem making it unnecessary to allow for simpler problems the user can still change the simulation period in step 2 3 of the workflow in figure a3 the other main difference between the cjw cg and hpc notebooks is that the codes calculating kge values for the hpc notebook are executed on the hpc step 2 8 in hpc branch in figure a3 while for the cjw cg environment the kge values are calculated locally on cjw cg step 2 9 in cjw cg branch in figure a3 in the hpc environment the kge values are calculated on the hpc resource to prevent having to transfer large data volumes from the hpc to the cjw cg with the sole purpose of calculating performance metrics users can use globus to transfer selected output files from hpc to the cjw cg for additional analysis notebook 4 which exists only in the hpc environment was developed for this purpose and is discussed in section 2 3 4 a modified and scaled range between 1 and 1 version of the kge was used as an indicator of model output sensitivity to a change in input forcing based on the work of clark et al 2021 and mathevet et al 2006 and is described in the vb study the kge test compares hourly model outputs generated with the benchmark forcing dataset nldas dataset 1 table a2 with outputs generated with the forcing datasets with one forcing held constant cnst datasets 2 8 table a2 kge values are ranked from low to high to determine relative order of forcing influence on model outputs with highest rankings associated with least influence of change to 24 h constant forcing 2 3 3 post processing notebook the third notebook jn 3 post processing produces visualizations of the sensitivity of summa model output to the temporal resolution of the model forcing figure a4 shows the steps taken in the third notebook the notebooks for cjw cg and hpc environments are the same for the selected basin s eight plots are generated with notebook 3 that follow the analysis in the vb study the reader is referred to the supplementary materials and the vb study for a detailed explanation of each of the eight plots in this paper we only present the second figure generated by notebook 3 i e kge values for each output variable for all 8 default model runs 2 3 4 model output transfer the fourth notebook jn 4 use globus is only included in the hpc resource figure a5 to transfer summa output files from hpc to cjw on hydroshare to retrieve the data from the hpc this notebook needs a job id submitted to the hpc and created in notebook 2 while this notebook is running users can see the live status of the file transfer managed by the cybergis compute service once running of this notebook is successfully finished the user will be able to see the location of the transferred file on cjw 2 4 performance analysis we tested the performance of the cyberinfrastructure using a number of model scenarios using six years of simulation to be consistent with the vb study and varying the number of studied basins for each computational environment described in table 3 for the cjw cg environment we tested the performance of notebooks 1 3 for three scenarios table 3 rows 1 3 1 one basin a total of six years of simulations 2 four basins a total of 24 years of simulations and 3 six basins a total of 36 years of simulations we decided not to test the cjw cg environment for more basins as the cjw cg runs were slow and the hpc resource was available for larger simulations for the hpc environment we used expense hpc and tested the performance of notebooks 1 3 for 12 scenarios table 3 rows 4 15 in these scenarios we varied the number of allocated cpus 128 or 256 for parallelism and the total number of basins ranging from one basin a total of six years of simulations to 20 basins a total of 120 years of simulations which equals about three percent of the total simulation years for the whole vb study to test the performance of notebook 4 transferring output files from hpc to the cjw we only used scenarios hpc 256 1 to hpc 256 6 rows 4 9 in table 3 and repeated each transfer 5 times to obtain a range of run time for each of the scenarios 3 results and discussion in this section we first briefly present results of the modeling case study that served as a motivating use case for the cyberinfrastructure then we present results of the performance analysis focusing on contrasting the cjw cg and hpc notebooks using a variety of model setups then we summarize the resulting resources from this study that are shared on hydroshare finally we discuss the resulting system including opportunities and challenges identified through this research that can be the focus of future research 3 1 results of the modeling case study four camels basins with diverse characteristics table 4 were chosen as examples of the effect of basin characteristics on model results we specifically selected these four basins for this modeling case study because we found that they all show different patterns for the four selected basins fig 3 shows the kge values for each summa output variable using the default bil configurations box in figure a1 model configuration runs the runs consist of one reference simulation in which all forcing variables vary on an hourly basis nldas dataset 1 forcings box in figure a1 and seven simulations in which one forcing variable is held constant at the mean daily value throughout each day the seven datasets ppt to prs forcings box in figure a1 kge values were calculated relative to the reference simulation for each of the seven simulations using five years of hourly model output from 10 1 1991 9 30 1996 fig 3 demonstrates the variability in model output sensitivity to the temporal resolution of the forcing variables the first three basins gages 01632900 02212600 and 09378630 show a strong ppt temporal aggregation influence using default whereas gage 11264500 is more influenced by tmp hum and swr temporal aggregation in other words a higher temporal resolution is necessary for the aforementioned forcing variables in the given basins to capture the sub daily hydrologic response shown by the reference simulation the weaker influence of ppt temporal aggregation on the gage 11264500 compared to other gages can be attributed to its high fraction of precipitation falling as snow 0 91 as opposed to 0 1 0 01 0 5 table 4 also in fig 3 we see varying ranges in kge values for particular output variables as an example surfacerunoff is affected by constant hourly values of ppt for gages 01632900 and 09378630 ppt and hum for gage 02212600 and tmp hum swr wnd ppt and prs most to least dominant for gage 11264500 this shows the forcing variables in each basin that need to have a higher temporal resolution to reproduce the surfacerunoff output in the reference simulation in this section we only presented one example of an inter basin comparison to illustrate how different the results can be across different basins researchers can further explore the differences between individual basins using other plots that can be made using the interactive jupyter notebooks and also reproduce the results from the original vb study 3 2 results from performance analysis fig 4 shows the run time for the data processing notebook notebook 1 and the post processing notebook notebook 3 for the 15 scenarios listed in table 3 notebooks 1 and 3 are very similar between cjw cg and hpc computational environments notebooks 1 and 3 do not take a significant time to run because they are only preprocessing and output analysis notebooks and no simulations are run for scenarios with fewer than 30 simulation years notebook 1 takes longer than notebook 3 but this changes for scenarios with more simulation years as the rate of run time increase with simulation years is much higher with notebook 3 than with notebook 1 for the cjw cg environment the average time to run notebooks 1 and 3 across the tested scenarios only takes 0 6 of the entire time needed to run all notebooks 1 2 and 3 this means the time required to run data processing and post processing notebooks is not a limiting factor for running the simulations for the hpc environment this ratio increases to 8 5 and 11 3 when using 128 and 256 cpus respectively this dramatic increase in the ratio is due to the significant decrease in run time of notebook 2 when using hpc the run time for the summa execution notebook notebook 2 for the 15 model scenarios using different computation environments is shown in fig 5 the high rate of run time increase with increasing simulation years for the cjw cg environment emphasizes that while the cjw cg environment is technically able to simulate smaller models it might not be fast enough to run larger simulations in the case of running six basins for six years the hpc was 3 6 and 2 6 times faster than the cjw cg when using 256 and 128 cpus respectively hpc with 256 cpus scenario hpc 256 6 could finish the simulations for 120 years 3 percent of the vb study in 2nullh and 10nullmin while hpc with 128 cpus scenario hpc 128 6 could run the same problem in 1 48 times of the time need by hpc 256 6 using the hpc with 256 cpus assuming a conservative linear extrapolation the summa simulations from notebook 2 are expected to be done in about 75 hours for the entire vb study in summary hpc provides considerably faster simulations making them ideal to run for larger studies when using the hpc resource and in the case of 120 years of simulation dividing the number of the allocated cpus by two led to about a 50 increase in the run time and not 100 as one might expect this non linear scaling can be mainly attributed to 1 communication overhead in the computational resource that reduces scaling and 2 the fact that some parts of the codes in notebook 2 did not utilize parallelism for example kge values were only calculated after they were exported as netcdf files instead of being calculated directly from the raw summa output files the rate of run time increase for hpc with 128 cpus is higher compared to that for hpc with 256 cpus this may be attributed to the communication overhead because each cpu in the case of the hpc with 128 cpus needs to run twice as many simulations compared to hpc with 256 cpus the run time for transferring the summa output files from expanse hpc to cjw on hydroshare using the globus service integrated by cybergis compute service is shown in fig 6 each transfer was repeated 5 times to obtain a range of run time for each of the model simulations with a different total number of simulation years the range of the transfer time for each total number of simulation years is small indicating a consistent data transfer for 120 years of simulation it took 14 5nullmin on average to transfer 118 gb of data from hpc to cjw highlighting that the data transfer approach from hpc to cjw is fast and stable the transfer rate gb min is independent of data size fig 6 3 3 data organization in hydroshare the data for this study was pre processed and the output post processed by using existing python packages the study demonstrates the potential for using the online repository of hydroshare to not only store data and modeling code but to also store computational environments api version documentation and container installation hydroshare as a hydrology based repository service facilitated this by allowing all the parts of the problem to be stored together as one resource furthermore parts of the resource can be extracted and made into a new version of the resource updated revised or modified to promote collaboration to this point a hydroshare collection resource was created that contained three composite resources these resources are published and have digital object identifier doi which makes them immutable and findable fig 7 shows the landing page for the hydroshare collection resource that groups the three composite resources the three composite resources that are contained by this collection resource are shown in dialogue box 1 the related resources in box 2 refers to this paper and box 3 shows the information on how to cite this resource fig 8 shows the landing page for the hydroshare composite resource holding the hpc notebooks box 1 shows the contents of the resource most importantly the four jupyter notebooks and the readme md file the readme md file box 2 provides the user with the instructions on how to run the notebooks box 3 shows the information on how to cite this hydroshare resource 3 4 opportunities and challenges this study demonstrated a real world working implementation application of strategies for reproducible hydrologic modeling presented by choi et al 2021 to a large scale hydrologic study the vb study this section discusses the opportunities and challenges of this implementation if one needs to adopt this cyberinfrastructure for studies significantly differing from the vb study considerable changes or extra steps might be needed for instance 1 if exploring non camels basins then extra steps to prepare the inputs might be needed or 2 if using hydrologic models other than summa then containerization of the model might be needed despite the plausible challenges when making these non trivial extra steps the intended main opportunity here is that the modeling community can learn from the presented open cyberinfrastructure considering the commonalities among the hydrologic models with regard to the input data preprocessing processing and postprocessing steps needed by them knoben et al 2022 minimal changes in the notebooks are required to use the presented cyberinfrastructure to rerun parts or all of the vb study or to extend the experiments performed in that study for selected camels basins with these minimal changes a user could use 1 different camels basins 2 different parameters in the lhs set 3 different simulation periods e g a drought period 4 more than 10 lhs sets e g a more thorough exploration of the parameter space and 5 additional summa model configurations the last two changes i e using a larger number of lhs sets and different model configuration decisions highlights a major challenge in reproducing a computationally complex study here the limit on manageable data size was pushed even when running a few basins hpc computational power was required to run the full six years of simulation expanding the parameter exploration space or adding model decisions would compound the data size thus while this work is advancing cyberinfrastructure used for big data in hydrology challenges remain the second major challenge that is encountered is implementing version control what if users need to run the jupyter notebooks presented in this study in their own computational environment not deployed on cjw or they need to install a newer version of a model api how can they make sure they have a reproducible framework that is robust enough to tackle the version control problem because there are many individual pieces of software it was challenging at times for the study team to keep all the software versions synchronized we propose that future research should tackle the version control challenge by making the computational environment all documented and installable via a python environment file the pysumma code which is used for hydrology modeling was installed via conda just as the rest of the infrastructure in the future python package updates will break compatibility but compatibility can be preserved by installing the older versions as documented in the environment file or the user understanding the updates in order to manually work around the updated package incompatibility if a researcher wants to use a newer future version of pysumma then they may need to debug some parts of the jupyter notebooks that are affected by the changes while this is not an ideal way to handle version updates at least the researcher has options of a working albeit older computational environment from which to begin reproducing the study before updating to newer software the specifics of the environment can be placed in a python environment yml file that can be shared as part of the online model and data repositories and can be installed with an installation notebook inside the repository this can use best practice for transparency about what dependencies the computational gateway interface notebooks need to run the specifics of each dependency can be described in the installation notebook so that if in the future there are issues with the availability of that dependency then a suitable substitute can be found version control issues can be thus addressed through this methodology albeit an imperfect solution depending on possible user troubleshooting in addition to the two major challenges described above there are two additional challenges related to the use of the hpc environments 1 large data transfers between computational environments online data repositories and a user s personal computer and 2 allowing users to execute their workflows on different hpc environments based on their use case and access to hpc environments there may be cases for example where users does not want to utilize hpc resources due to financial cost concerns and need to transfer a large amount of model outputs from an hpc environment s temporary scratch directory to a jupyter compute environment to further analyze the data using the jupyter compute environment transferring large datasets e g the entire output from vb study or even the four selected basins study explored in this paper would be slow and unreliable using standard data transfer approaches i e compress data into a big package and then transfer it in this study we used globus to do this data transfer which can transfer multiple individual files in parallel without a need to compress data a big package and other related cyberinfrastructures that do not currently use globus or a related technology could benefit from doing so globus is not limited to data transfers between the hpc environment and the jupyter compute environments cjw in the case of this study however in fact it is possible that the full or a large portion of the model output can be stored on an online data repository or even on a user s own personal computer in either case the online data repository or the user s personal computer the outputs could be downloaded using globus if globus is installed and they become a globus server making a user s personal computer a globus server may be the case that the user prefers to back up a model run not in an online data repository but at some other location in this case globus could be used to connect directly with the hpc environment thereby bypassing both any jupyter compute environments cjw in the case of this study as well as online data repositories hydroshare in the case of this study as an intermediate storage location if the large data takes much of the space in the user s personal computer user may consider transferring it to external hard drives that offer larger capacity to allow users to execute their workflows on different hpc environments users would need to set up their own job submission service and configure the jupyter environment e g cjw to the specific hpc environment that they have access to although the job submission software used in this study is open source it is customized for the uiuc hpc used in the study so it cannot be directly used for other hpcs future work could be for cjw to act as a connector to user supplied hpc environments in this case cjw would ask users to provide their own credentials and to their own hpc rather than only using the uiuc hpc service while not a simple task standardization of job submission approaches across hpc environments makes this functionality possible generalizing the approach through future research could benefit users to access their own institutional hpcs and other hpcs at the national level that the user has access to 4 conclusions the importance of reproducibility is broadly recognized across different scientific disciplines when it comes to computational hydrology this can be a significant challenge this research shows how an architecture that integrates the 1 online data repositories 2 computational environments and 3 model api can facilitate reproduction of the components of modern and complex hydrologic studies for this purpose we used a recently published large scale hydrologic study vb study as an example we designed and built cyberinfrastructure that utilized software components to enable intuitive and online access to computational environments this approach was used to remove the potential software inconsistencies from users differing personal software editions as well as to make implementation easier with pre compiled software with the added complication of a computationally expensive research problem instead of a case study this approach gave the user the option to use either the cjw cg or hpc computational environments depending on how much they need to reproduce a problem more representative of the big data problem using hydroshare as the data repository and containerization of the pysumma api with docker or singularity in the case of the hpc environment along with a computational gateway interface of jupyter notebooks both hosted on the cjw made this possible three jupyter notebooks for the cjw cg environment and four jupyter notebooks for hpc environment were developed notebooks 1 3 for both cjw cg and hpc environments enable 1 preparing the forcing data simulation period and study camels basins 2 executing summa hydrologic model and 3 visualization of the results notebook 4 only developed for the hpc environment enables transferring large data from hpc to the scientific cloud service i e cjw using globus service integrated by cybergis compute in a reliable high performance and fast way we presented a modeling case study subset from the vb study that served as a motivating use case for the cyberinfrastructure the case study showed how four individual basins with different characteristics can lead to different patterns of temporal aggregation for each of the forcing variables given the same model setup the case study served to show that the developed cyberinfrastructure enables others to reproduce the vb study for subsets of the original domain as a basis for doing additional research enabling conclusion reproducibility beyond bit reproducibility we analyzed performance of the notebooks focusing on contrasting hpc and cjw cg notebooks using a variety of model scenarios the hpc environments could perform significantly faster simulations compared to cjw cg enabling users to explore a large number of basins and simulation periods this clearly showed how the use of hpc from a jupyter gateway could advance the reproducibility of modern and complex hydrologic studies the run time performance analysis for the big data transfer notebook for the hpc environment showed that the method used was stable reliable and fast therefore similar studies could easily benefit from the same approach for transferring large data between scientific cloud services with the focus of this research was on conclusion reproducibility over bit reproducibility of the vb study users can easily modify the notebooks to test different situations by varying the study basins and periods parameterizations and model configurations these situations highlighted two major challenges first the complexity of the big data problem eventually became large enough that it needed to be run using the hpc computation environment which presented other smaller challenges of data transfer and portability of the hpc environment second implementation of a version control system was needed e g when a user needs to install a newer version of a model api or when a user needs to run these codes on their local machine rather than the used cloud based computational environment sharing the dependencies of the computational environments as a python environment yml file and an installation notebook that installs them was discussed as a future solution to tackle the version control issue finally as a broader impact the vb study methodology replicated with interactive codes could also serve as a valuable educational resource allowing educators to present sophisticated modeling experiments for use within classrooms through online python notebooks likewise the basic approach could be extended to enable new water decision support systems that take advantage of the summa framework and hpc yet remain easy to interact with through notebooks this can help to for example evaluate forcing sensitivity to a water resources management objective or explore the parameter and model uncertainties of summa using different algorithms such as markov chain monte carlo mcmc and bayesian model averaging bma samadi et al 2020 in a systematic manner with more work to harden and improve the usability of the system presented here these additional use cases can be possible resource description reference original nldas forcings for the camels basins can be obtained as a netcdf file mizukami and wood 2023 summa simulations using camels datasets on cybergis jupyter for water choi et al 2023b summa simulations using camels datasets for hpc use with cybergis jupyter for water choi et al 2023c the data from the camels dataset newman et al 2015a was consolidated into one netcdf file taking advantage of opendap data services supported by the hydroshare thredds server and web application connector tarboton and calloway 2021 the summa setup for the camels basins can be obtained from the summa camels folder of the hydroshare resources list of relevant urls cybergis jupyter for water https go illinois edu cybergis jupyter water docker https www docker com hydroshare rest api https www hydroshare org hsapi numpy https www numpy org pandas https pandas pydata org pysumma https github com uw hydro pysumma releases tag v3 0 3 seaborn https seaborn pydata org singularity https sylabs io summa https github com ch earth summa releases tag v3 0 3 xarray http xarray pydata org xsede https www xsede org declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments this work was supported by the national science foundation nsf under collaborative grants 1664061 1664119 and 1664018 for the development of hydroshare http www hydroshare org 1928369 for the integration of reproducibility methods into hydroshare the work also was supported by the institute for geospatial understanding through an integrative discovery environment i guide that is funded by nsf under award no 2118329 any opinions findings and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of the nsf appendix this section provides supplemental material to support our methods and results the figures and tables are referred to in the main text fig a1 an overview of the forcing datasets forcings yellow box parameter sets parameters blue box and model configurations configurations green box used in the 704 summa model runs runs pink box performed for each of the 671 camels basins note the pink numbers that follow each forcing parameter and configuration refers to the summa model run set as numbered in the pink runs box e g the default parameter set in the parameters box is used with summa model runs 1 2 3 and 4 in the runs box source modified from van beusekom et al 2022 fig a1 fig a2 the preprocessing notebook jn1 diagram fig a2 fig a3 running summa notebook jn2 diagram fig a3 fig a4 post processing notebook jn3 diagram fig a4 fig a5 hpc data transfer notebook jn4 diagram fig a5 table a1 summa output variables chosen for analysis source van beusekom et al 2022 table a1 variable type summa variable name description units 1 liquid water fluxes for the soil domain surfacerunoff surface runoff m s 1 2 aquiferbaseflow baseflow from the aquifer m s 1 3 infiltration infiltration of water into the soil profile m s 1 4 rainplusmelt rain plus melt m s 1 5 soildrainage drainage from the bottom of the soil profile m s 1 6 turbulent heat transfer latheattotal latent heat from the canopy air space to the atmosphere w m 2 7 senheattotal sensible heat from the canopy air space to the atmosphere w m 2 8 snowsublimation snow sublimation frost below canopy or non vegetated kg m 2 s 1 9 snow swe snow water equivalent kg m 2 10 vegetation canopywat mass of total water on the vegetation canopy kg m 2 11 derived netradiation net radiation w m 2 12 totalet total evapotranspiration kg m 2 s 1 13 totalrunoff total runoff m s 1 14 totalsoilwat total mass of water in the soil kg m 2 table a2 parameters chosen for latin hypercube sampling source van beusekom et al 2022 table a2 parameter name minimum maximum default constraints k macropore 1 0d 7 0 1 0 0001 k soil 1 0d 7 1 0d 5 variable theta sat 0 3 0 6 variable critsoiltranspire fieldcapacity theta res aquiferbaseflowexp 1 10 2 0 aquiferbaseflowrate 0 0 1 0 1 qsurfscale 1 100 50 summerlai 0 01 10 3 frozenprecipmultip 0 5 1 5 1 heightcanopytop 0 05 100 variable heightcanopybottom heightcanopybottom 0 5 variable routinggammashape 2 3 2 5 routinggammascale 1 100000 20000 albedorefresh 1 10 1 0 tempcritrain 272 16 274 16 273 16 windreductionparam 0 1 0 28 the eight plots generated by notebook 3 are described as follows 1 location of the selected camels basin 2 kge values for each cnst forcing dataset datasets 2 8 table a2 by output variable using the default model runs this is a subset of figure 9a from van beusekom et al 2022 3 boxplots depicting the range in the kge values for each set of model runs default lhs config and comprehensive table a1 by output variable note boxplots only appear for the model runs selected in notebook 2 this is a subset of figure 9b from van beusekom et al 2022 4 boxplots depicting the range in the kge values for each set of model runs default lhs config comprehensive table a1 by cnst forcing dataset datasets 2 8 table a2 note boxplots only appear for the model runs executed in notebook 2 this is a subset of figure 9c from van beusekom et al 2022 5 ranks 1 7 stacked barplots depicting the relative basin kge rank counts by cnst forcing dataset datasets 2 8 table a2 for the 14 summa output variables note bars on this plot will only appear if the comprehensive basin runs are executed in notebook 2 this is a subset of fig 8 from van beusekom et al 2022 6 ranks 1 7 stacked barplots depicting the relative basin kge rank counts by cnst forcing dataset datasets 2 8 table a2 for the eight summa configurations note the complete figure will only appear if the comprehensive basin runs are executed in notebook 2 a stacked bar for the default configuration bll will be plotted if the lhs basin runs are executed in notebook 2 this is a subset of fig 8 from van beusekom et al 2022 7 boxplots for each output variable depicting the range in the seven summed kge values from cnst forcing datasets 2 8 for the eight summa configurations or for the default configuration if only the default configuration was run default or lhs basin runs in notebook 2 this is a subset of fig 6 from van beusekom et al 2022 8 boxplots depicting the range in the summed summa hourly output variables over the period of record produced using the benchmark nldas forcing dataset for the eight summa configuration or for the default configuration if only the default configuration was run default or lhs basin runs in notebook 2 note a point will appear instead of a boxplot if only the default parameter set was run default or config basin runs in notebook 2 this analysis is not in van beusekom et al 2022 it is included in the interactive tool to supply users with potential summa output variable ranges for their selected basin to reproduce the modeling case study presented in the current paper the selected four basins need to be specified in notebook 1 figure a2 step 1 3 2 select basins and simulation period and then notebook 3 can be used to reproduce fig 3 kge values using the default model runs for each cnst dataset datasets 2 8 table a2 grouped by summa output variable 
25409,a research gap in calibrating distributed watershed hydrologic models lies in the development of calibration frameworks adaptable to increasing complexity of hydrologic models parallel computing is a promising approach to address this gap however parallel calibration approaches should be fault tolerant portable and easy to implement with minimum communication overhead for fast knowledge sharing between parallel nodes accordingly we developed a knowledge sharing parallel calibration approach using chapel programming language with which we implemented the parallel dynamically dimensioned search dds algorithm by adopting multiple perturbation factors and parallel dynamic searching strategies to keep a balance between exploration and exploitation of the search space our results showed that this approach achieved super linear speedup and parallel efficiency above 75 in addition our approach has a low communication overhead along with the positive impact of knowledge sharing in the convergence behavior of the parallel dds algorithm keywords distributed hydrologic model parallel calibration approach parallel dds chapel data availability data will be made available on request 1 introduction due to the extensive spatiotemporal heterogeneity of predominant hydrological processes distributed watershed hydrologic models require numerous model parameters and input datasets singh and frevert 2003 khakbaz et al 2012 refsgaard 1997 one distributed model simulation accounts for the most time consuming single computation in watershed modelling with a runtime varying from minutes to days zhang et al 2009a b razavi et al 2010 subsequently applying global meta heuristic optimization algorithms imposes high computational requirements on calibrating these hydrologic models xu et al 2013 the calibration requires an abundant number of model simulations for thorough exploration and exploitation of their hyper dimensional parameter search space this substantial computational burden can also get exacerbated by using high resolution datasets and or working on larger watersheds budamala and baburao mahindrakar 2021 thus not only the calibration of distributed watershed hydrologic models is a computationally expensive process but also its complexity and size can potentially grow parallel computing is the simultaneous execution of independent and parallelizable components of a large computing application this approach is promising to provide speedup the ratio of execution times of a parallel application to its sequential version and scalability the ratio of actual speedup to the ideal speedup with a certain number of processors for watershed hydrologic model calibration with any size or complexity asgari et al 2022 quinn 1994 sub model and model level parallelization are two popular parallelization strategies at the sub model level parallelization takes place for the sub units of a hydrologic model to speed up a single model simulation however it comes with challenges of model reconstruction and complexities associated with parallel task management for component communications lin and zhang 2021 the model level parallelization is to parallelize integrated model simulations to speed up the entire model calibration process which has shown much ease in achieving parallel computing objectives for instance xia et al 2021 developed parallel optimization with dynamic coordinate search using surrogates pods to not only decrease the number of model simulations but also increase the calibration speed the implemented parallelization strategy was at the model level and the authors reported speedy automatic calibration in their study open multi processing openmp huo et al 2018 kan et al 2016 2018 2020 and message passing interface mpi li et al 2015 vrugt et al 2006 kim and ryu 2019 are two popular parallelization paradigms for model level parallelization strategy in the calibration of watershed hydrologic models however the current research gap in the parallel calibration of hydrologic models challenges the applicability of these paradigms population based metaheuristic algorithms such as artificial bee colony abc huo et al 2018 bayesian optimization bo ma et al 2022 shuffled complex evolution sce ua kan et al 2018 particle swarm algorithm pso ma et al 2021 niu et al 2021 and genetic algorithm d zhang et al 2016 have been extensively implemented in parallel calibration of hydrologic models contrary to single solution algorithms these algorithms search the parameter space with multiple initial evolved solutions this behavior empowers population based metaheuristic algorithms to investigate different zones simultaneously and eliminate the bias in the initial solution or sampling strategy prügel bennett 2010 hence they are inherently parallelizable as the initial population can be split into subpopulations to get evolved independently on parallel computing units however algorithms such as abc and pso are swarm based algorithms the evaluation of the population in these algorithms has a cooperation phase in which individuals transfer the information they have gained to lead each other to the global optimum ab wahab et al 2015 beheshti and shamsuddin 2013 although splitting the populations into subpopulations increases the diversity and lowers the possibility of getting trapped in a local optimum it eliminates the cooperation phase and impacts the convergence speed in addition hadka and reed 2015 showed that running distinct simulations with independent populations on each parallel node decreased the communication cost however the periodic migration events in which the most optimum populations from parallel nodes were exchanged led to producing the highest quality results thus not only the cooperation phase should be preserved in parallel population based optimization algorithms but also it is beneficial to provide it for parallel single solution optimization algorithms the reason is that single solution optimization algorithms are not equipped with strategies of avoidance of local optimum by appropriate cluster wide node coordination and broader exploration majeed and rao 2017 hence knowledge sharing becomes vital for both parallel single solution and population based optimization algorithms for adaptive and purposeful parallel exploitation and exploration of parameter search space knowledge sharing comes with two conditions asynchronicity and low communication overhead zamani et al 2021 parallelized dynamically the dimensioned search dds algorithm for parallel calibration of the swat model they provided knowledge sharing to some degree for the algorithm but in a synchronous architecture their results showed negative impacts of knowledge sharing on parallel efficiency in parallel calibration of hydrologic models this degradation of parallel efficiency defined as the speedup gain relative to the scale of the parallel system asgari et al 2022 was due to different configurations of computing units such as various disk access times for reading writing databases and potential failures throughout simulations on each node in a synchronous architecture besides communication overhead in knowledge sharing parallel systems should be minimized to ensure that adding the knowledge sharing feature does not impact the overall parallel efficiency of these systems considering these two conditions openmp and mpi parallelization paradigms are evaluated further openmp adopts fork join fj parallelism framework for shared memory parallelization in the fj framework task decomposition parallel computation and result combination are carried out repeatedly which is suitable for applications that can be broken down recursively into several subproblems peng et al 2017 this framework is more appropriate for fine grained parallelization to avoid long latencies the time elapse between requesting a memory space and receiving a response silc et al 1998 in the result combination step derived from unequal processing speeds of parallel computing units synchronization in the fj framework exists in the result combination where all computing units must finish processing their assigned subproblems before proceeding with further unprocessed ones by implementing fine grained parallelization which focuses on decomposing sequential problems to the maximum number of small and fast to finish parallelizable components latencies in the result combination of fj can be minimized corbellini et al 2018 lea 2000 another limitation of the openmp paradigm lies in its incapability of supplying horizontal scaling horizontal scaling is the capability of increasing involved computing units in the execution of the parallel application for more complex problems in openmp based systems the number of computing units and the size of the shared memory bound the horizontal scaling kirk and wen mei 2016 in fact long latencies in accessing the shared memory in these systems degrade theoretical parallel performance and consequently limit scalability the limitation of openmp based systems paved the way for using the mpi parallelization technique to develop distributed memory parallel systems for calibrating hydrologic models bomhof et al 2019 in mpi based systems data transfer among parallel tasks are explicitly managed by sending receiving messages these explicit send and receive messages which are i o requests ultimately lead to saturated i o activities long latencies due to multiple disk accesses and parallel computing performance degradation ou et al 2006 another associated problem with the mpi parallelization technique is its low parallel productivity niu et al 2021 which calls for more studies on parallel systems that can be easy to implement parallel productivity defines as the ability to develop high performance computing hpc systems that are easier to program which reduces software development cost and time however this ease should not decrease the overall system s parallel performance tolson and shoemaker 2007 studies have referred to the low parallel productivity of mpi based systems by mentioning the tight coupling between hydrologic models and these systems which requires model reconstruction lin and zhang 2021 zhang et al 2021 her et al 2015 pointed out that significant modifications of sequential optimization algorithms and or hydrologic model source codes were expected in developing mpi based parallel calibration systems niu et al 2021 encouraged hydrologists to adopt parallel computing in calibration of hydrologic models which led to increased use of distributed hydrologic models and their enhancement however implementation of parallel systems should be relatively easy with minimum requirements for pre installation frameworks libraries packages the challenges in mpi based systems along with their limitations in handling parallel task node failures motivated researchers to implement hadoop based systems lin and zhang 2021 zhang et al 2021 ma et al 2022 however several studies reported that the shared memory of hadoop based systems and the i o operations for node communications along with simultaneous disk access in computing units resulted in high i o overheads and long latencies in these systems zamani et al 2021 ma et al 2022 zhang et al 2016 2021 a zhang et al 2016 lin and zhang 2021 and ma et al 2021 pointed out that the spark computing framework can reduce disk i o overhead in hadoop these studies showed that increasing parallel tasks decreased the parallel efficiency in transferring big data blocks due to the distribution and operation of tasks in spark ma et al 2021 similarly due to spark task management and data transfer spark might not be suitable for calibrating lightweight hydrological models with intensive communications lin and zhang 2021 therefore there is a research gap in developing fault tolerant portable and easy to implement knowledge sharing based parallel calibration algorithms that require minimum communication overhead a new generation of parallel computing models called partitioned global address space pgas has emerged to improve parallel productivity and performance unlike openmp based systems where only global memory is available and mpi based systems where parallel computing units access merely local memory each computing unit in the pgas model has its local memory while part of it is shared with other computing units global memory in the pgas model computing nodes can read write not only their local memories but also the remote shared memory of each computing node synchronously or asynchronously de wael et al 2015 dinan et al 2010 pang et al 2022 showed that the asynchronous hydrologic model calibration has a 40 70 improvement in computational time compared to the synchronous version parallel calibration of hydrologic models has an asynchronous nature due to different configurations of computing units such as various disk access times for reading writing databases and potential failures throughout simulations on each node hence in nodal communications only one side is aware of the communication event which is called one sided communication this communication is a type of network communication in which one computing unit is responsible for identifying the sender receiver of the data knowledge and the size of data knowledge the other computing unit in network communication is only either the sender or receiver of data knowledge this communication is indispensable for asynchronous communication nishtala et al 2011 since the mpi parallelization paradigm requires matching send and receive messages to allow data transfer which is called two sided communication it is inconvenient for knowledge sharing parallel calibration of hydrologic models on the other hand pgas languages are attractive for the parallel calibration of hydrologic models as they support one sided communication chapel is a parallel programming language based on the pgas model which supports the implementation of productive and general purpose hpc systems through its high level abstractions for data and task parallelism gmys et al 2020 studied high performance programming languages chapel julia and python for developing parallel metaheuristic optimization algorithms by considering c openmp as the reference for their study they concluded that chapel was the only programming language with a higher level of parallel productivity than c openmp in programming the metaheuristic optimization algorithms the authors showed that chapel had the smallest gap between serial and parallel relative performance which proves its high scalability besides julia and python do not have mature multi threading support for programming parallel metaheuristics as chapel does in this study we proposed a knowledge sharing parallel calibration approach as a general parallel framework for calibrating distributed hydrologic models using the chapel programming language our parallelization approach stands out from all previously designed implemented approaches frameworks due to its unique features 1 parallel dynamically dimensioned search dds algorithm with different size perturbation and divided perturbation zones 2 advanced database management 3 asynchronous multithreading feature for fast and reliable knowledge sharing and 4 parallel node failure handling capability by addressing chapel s q threads tasking layer challenge for running hydrologic models we implemented our proposed parallel calibration approach to calibrate the integrated modelling for the watershed evaluation of bmps imwebs fully distributed hydrologic model liu et al 2018 for the catfish creek watershed of ontario in canada we evaluated the capability of our proposed approach in providing reliable model calibration results model performance along with parallel speed up and parallel efficiency our proposed approach achieved super linear speedup and parallel efficiency above 75 in addition we proved that our approach has a low communication overheard along with the positive impact of knowledge sharing in the convergence behaviour of the parallel dds algorithm the rest of the paper is organized as follows we started section 2 by presenting the conceptual framework for our proposed approach including an introduction to chapel programming language in section 3 we described the sequential dds algorithm and focused on discussing our proposed parallel dds algorithm in sections 4 5 and 6 we discussed in detail several unique features of our approach and ended up with the implementation procedure based on chapel programming concepts in section 7 in section 8 we described the study area the distributed hydrologic model and the parallel computing clusters systems that we adopted in our study in section 9 we presented our findings both from hydrologic model calibration and parallel computing perspectives we provided our conclusions in section 10 2 a conceptual framework for parallel calibration of distributed watershed hydrologic models we proposed a knowledge sharing parallelization approach as a general framework for calibrating distributed watershed hydrologic models and implemented it using chapel parallel programming language our study focused on rainfall runoff models for streamflow prediction at the watershed outlet s arsenault 2017 optimization algorithms are used in the model calibration process to simulate the behaviour of the watershed under study these algorithms iteratively 1 modify the values of model parameters 2 run model simulations and 3 calculate objective function s using streamflow measurement data and simulated results to guide the exploration and exploitation of the parameter search space pechlivanidis et al 2011 sitterson et al 2018 the calibration process should only include the representative parameters of dominant water cycle processes in the watershed under study to lessen computational burdens associated with model calibration we call these parameters calibration parametersthroughout this paper nonetheless pruning the model parameters to limit them to only calibration parameters is not a sufficient approach to address the computational burdens thoroughly hence parallel computing has been adopted to speed up the calibration of distributed hydrologic models and provide scalability as the complexity and size of the calibration applications grow 2 1 chapel programming language for parallel calibration of distributed hydrologic models parallel computing technologies should not be accompanied by steep learning curves for users with different levels of programming skills in addition there should be a minimum gap between parallel and sequential programming languages that most programmers are accustomed thus parallel programming languages should have abstractions for concepts such as parallelism data distribution synchronization and communication that are inherent elements of parallelization chamberlain et al 2007 chapel is a pgas based parallel programming language with a high level of programmability which results in a shorter time less cost of parallel application development dun and taura 2012 chapel is fast its code reading writing is as attractive as python code it competes with c and c in portability and flexibility respectively it provides the performance of fortran and its scalability challenges the scalability of mpi chamberlain et al 2018 in parallel calibration of hydrologic models chapel has not been used to our best knowledge therefore in this study we investigated it to develop our knowledge sharing parallel optimization algorithm for calibration of distributed hydrologic models in chapel locale indicates one of the parallel nodes on which the program runs all chapel programs start running from locale 0 which could be considered the master node and other locales as slave nodes based on the master slave model shao et al 2000 our proposed approach throughout the paper consists of two main components locale 0 s and other locales processes although locale 0 could participate in model calibration we decided to exempt it to avoid its memory draining which could be caused by iterative hydrologic model simulations 2 2 knowledge sharing approach for parallel calibration of hydrologic models in calibrating hydrologic models using single objective optimization the ultimate goal is to find optimal values for calibration parameters hence optimization s best solution is considered the best parameter set in this paper locale and locale 0 best parameter sets are parameter sets with the most optimum minimum or maximum objective function value s among all parameter sets used for model simulations on that specific locale and on all locales respectively fig 1 a shows graphically the independent blue and black dashed lines and knowledge sharing red dashed and solid lines parallelization approaches for simulation based parallelization of distributed watershed hydrologic model calibration using a single solution optimization algorithm fig 1 b translates fig 1 a in a stepwise approach in fig 1 b each locale communicates with locale 0 to receive its best parameter set the communication form style is described in section 5 in the absence of locale 0 s best parameter set each locale uses its local backup best parameter set which is the best parameter set obtained by that specific locale afterwards using the best parameter set either local or locale 0 best parameter set the current parameter set is perturbed using the dds optimization algorithm in the independent parallelization approach each locale works on its best parameter set independently from other locales this approach focuses on a wide exploration of the parameter space and diversifying the solutions however in distributed watershed hydrologic models with computationally expensive model simulations it is necessary to limit the exploration of parameter space and avoid searching neighborhoods containing less optimal parameter sets therefore in the knowledge sharing parallelization approach locales also communicate to share optimization knowledge and guide each other not to search neighbourhoods that have been already investigated we call this knowledge investigated neighborhood locations therefore in the knowledge sharing parallelization approach locales share two pieces of optimization knowledge called locale 0 best parameter set and investigated neighborhood locations these two pieces of information help locales guide each other toward the potential global optimum and have a more effective search of the parameter space working on the locale 0 best parameter set on all locales provides thorough exploitation of one neighborhood with high optimum potential instead of partial exploitations of multiple neighborhoods as exploitation of one neighborhood on all locales may lead to redundant simulations the same parameter sets may be investigated by more than one locale investigated neighborhood locations provides avoidance of redundant exploration and focus on new neighborhoods to increase the effectiveness of search as shown in fig 1 b the strategy on each locale is that the perturbation of the current parameter set on each locale gets repeated until the purebred parameter set is not among investigated neighborhood locations then the perturbed parameter set is added to locale s investigated neighborhood locations and this updated knowledge is shared with other locales in addition the hydrologic model with the new parameter set initializes a new model simulation and calculates the objective function value afterwards the simulation results are stored in the locale databases and a copy is sent to the locale 0 to be stored in the centralized databases the copy is sent through shared memory locations programming variable since all locales have access to the shared memory locations pgas only locale 0 reads and updates its centralized database with locales shared data our proposed knowledge sharing parallelization approach for calibrating hydrologic models has four distinctive features which are 1 parallel dds with different size perturbation and divided perturbation zones 2 advanced database management 3 asynchronous multithreading for fast and reliable knowledge sharing and 4 parallel node locale fault tolerant capability we further elaborated on these features in the following sections 3 heuristic optimization algorithms parallel dynamically dimensioned search dds algorithm a single objective optimization problem is defined as finding either the minimum or maximum value of an objective function f x in an n dimensional search space n 1 where each dimension is bounded with an upper and lower value l i x i k i 1 i n in equation 1 s denotes the bounded search space x is called a candidate solution and x is an optimal solution song et al 2019 more information can be found in tasgetiren and suganthan s 2006 study f x f x 1 x 2 x n f s r n equation1 f x f x x s r n heuristic optimization algorithms iteratively seek the optimal solutions in their given parameter search space until they meet pre defined constraint s during the exploration and exploitation of the parameter search space the solution with the minimum maximum objective function value depending on the nature of the optimization problem is labelled as the current best solution s the current best solution keeps changing improving until it is labelled as the optimal solution yang et al 2014 3 1 dynamically dimensioned search dds algorithm dynamically dimensioned search dds algorithm is a single solution algorithm that starts with exploring the search space via diversifying the candidate solutions and as the number of iterations increases it switches to an intensified search of the current best solution s neighborhood or exploitation this adjustment is carried out by a dynamic reduction of dimensions in the search space tolson and shoemaker 2007 fig 2 shows the logics of the dds algorithm for a 5 dimensional search space as its shown the candidate solution for each next iteration is generated based on three components which are current solution best solution and perturbation values in the dds algorithm the best solution is the solution which has achieved the minimum objective function value in minimization problems in all carried out iterations the value of the current solution for each dimension is called a decision variable e g x 1 x 2 in fig 2 and at each iteration dds randomly selects a subset of decision variables to perturb e g x 1 x 3 x 4 in fig 2 the perturbation value for ith decision variable is calculated using equation 2 in this equation n i is a random number in 0 1 range r is perturbation parameter which defines the random perturbation size as a fraction of the range of decision variable and its default value is 0 2 tolson and shoemaker 2007 and k i l i is the range of ith decision variable in the search space equation2 p i n i r k i l i the main motivations for utilizing this optimization algorithm in our study were 1 no need for computationally intensive task of fine tuning the algorithm setting and its parameter values dds has one parameter to set which is perturbation parameter r 2 dds support of both exploration and exploitation of the search space and 3 few dds parallelization studies in the calibration of hydrologic models tolson and shoemaker 2007 claimed that the dds algorithm is ideally suited for computationally expensive optimization problems such as distributed watershed model calibration bomhof et al 2019 showed that distributed hydrological model calibration results with the dds algorithm and the pa dds pareto archive dds algorithm the multi objective version of dds have the same quality in addition mai et al 2016 showed that the dds calibration results were located close to the pareto fronts of the pa dds results thus in the case of the dds and pa dds algorithms the multi objective calibrations are preferable when multiple equivalent solutions are required 3 2 parallel dds algorithm with different size perturbation and divided perturbation zones tolson et al 2014 developed two parallel implementations of the dds algorithm called the embarrassingly parallel dds ep dds and the parallel dds p dds their results showed that these implementations achieved higher quality solutions more efficiently compared to the serial dds algorithm however the ep dds algorithm did not have any communications between parallel nodes and the p dds algorithm was a synchronous implementation our parallel implementation of dds is not only asynchronous but also equipped with knowledge sharing communications among parallel computing nodes we developed our parallel version of the dds algorithm by proposing and implementing different size perturbation zones and divided perturbation zones algorithm1 shows the pseudocode of the only parameter perturbation phase of our sequential dds algorithm algorithm1 parameter perturbation phase of the dds algorithm image 1 matott et al 2013 showed that for dds algorithm the telescoping strategy or dynamic range reduction improves its efficiency and effectiveness in fact if searching parameter bounds are contracted step by step the dds algorithm achieves more efficient results in this study we defined multiple lenses of telescope from narrow to wide and assigned each computer to only search the parameter space through its own assigned lens fig 3 a shows permitted gray box and potential white oval change zones of calibration parameters around each parameter set blue circle in a 2 dimensional parameter space perturbed parameter sets can fall in any location inside the potential change zone algorithm1 since parameter ranges along with perturbation factor in the dds algorithm are constant values the controlling factor in location determination is the vector of normal values n n is a random vector generated for each model simulation with values in the range of 1 1 since n is a random vector there is no guarantee to have a good spread of the locations of perturbed parameter sets across the potential change zone to address the probable poor spread of parameters sets across the potential change zone we proposed and implemented the idea of divided perturbation zones parallel computing allows us to divide the 1 1 range into sub ranges and assign exploration and exploitation of each of the sub ranges to a parallel node thus this approach can reduce the probability of having concentrated locations for perturbed parameter sets while there are locations with low visiting rates yen et al s 2016 study concluded that the change of perturbation factor might lead to variation in convergence speed or having more optimum parameter sets as fig 3 c shows the values of the perturbation factor determine how wide the potential change zone can be the higher the perturbation factors the wider the perturbation zone and the higher chance of thorough exploration and exploitation of the permitted parameter change zone accordingly besides the approach of divided perturbation zones we proposed and implemented the approach of different size perturbation zones by assigning different perturbation factors to different parallel nodes in our study we used 0 1 0 2 and 0 3 r values and assigned them to an equal number of parallel nodes therefore by implementing both divided perturbation zones and different size perturbation ideas we were able to not only increase the diversity of parameter sets but also explore a higher range of parameter space fig 3 d shows the final potential change zone after applying both divided perturbation zones and different size perturbation zoneideas as mentioned earlier in our knowledge sharing parallelization approach chapel locales share investigated neighborhood locations that we call the forbidden n list as shown in algorithm1 a perturbed parameter set is generated by adding the perturbation vector p to the best parameter set vector p has two elements which are c b e s t and n c b e s t is the constant vector as we have shown in algorithm1 thus to differentiate neighbor locations from each other in the dds algorithm we need to know the associated n to each location so a list of n vectors each locale has examined to generate perturbed parameter sets defines the neighborhood that it has already investigated and other locales are forbidden to search those neighborhoods again this explains why we call the investigated neighborhood locations knowledge as forbidden n list two important adjustments need to be applied to the forbidden n list whenever the best parameter set on each locale gets changed and it intends to share the list with other locales forbidden n list contains investigated locations in the parameter space relative to the best parameter set therefore whenever the best parameter set gets changed all n s in the forbidden n list must get updated to reflect investigated locations relative to the new best parameter set as shown in fig 4 this process can be accomplished by deduction of n b e s t from all n s in forbidden n list n b e s t is the associated n to the newer best parameter set besides whenever each locale intends to read the shared forbidden n list from other locales it must check remote locales best parameter set to which forbidden n list locations on those locales are related if those best parameter sets are different from its own it should calculate the n b e s t the vector for achieving the locale s best parameter set from other locales best parameter sets and update the received forbidden n list only for itself another update should be applied to the forbidden n list when locales use various values for perturbation factor we came up with the idea of forbidden n list by considering the r value constant and the same 0 2 on all locales although the value is constant it is varied on different locales therefore we need to take out that variation in r values and integrate it to n since the n is only varying element in p this process is shown in detail in algorithm2 0 2 r n equals to s h a r e d m o d i f i e d n f o r b i d d e n in fig 4 algorithm2 modified forbidden n list image 2 4 advanced database management for the chapel based framework simultaneous model simulations on chapel locales require an independent parameter set with unique values and hydrologic model input files on each locale model calibration on each begins with receiving general information for calibration parameters e g names and calibration ranges of parameters in addition specific information data that the hydrologic model needs to run simulations are provided for the model calibration application in addition to enable post processing of the model output s on each locale information data such as observation data performance metrics calibration objective s etc should be provided for all locales separately after post processing stage model output s simulation information start end and total runtime etc performance evaluation results and used parameter set for each model simulation should be stored not only on local databases on locales but also in the centralized database on local 0 databases are stored on each locale s hard drive and they are independent from each other local databases are responsible for storing the results of model simulations carried out on each specific locale while the centralized database stores the results of model simulations carried out on all locales to have better data transfer minimize data read write mistakes avoid data redundancy and have fast data access we decided to use sqlite relational database to provide a reliable storage platform for chapel locales as chapel programming language lacks database management api libraries we utilized entity framework ef core library which is a database access library for net applications with c programming language to develop all essential database management functions for our knowledge sharing parallelization approach ef core an open access cross platform and well supported tool helps reduce development time and cost by enabling developers to program fast data access without using raw sql queries after developing the c database management scripts we built a collaboration between these scripts and the chapel program using 1 the chapel subprocess module and 2 net command line interface cli chapel subprocess module supports launching and interacting with other programs in our chapel program and net cli is a cross platform set of commands to develop and work with net applications projects smith 2021 thus whenever databases in our chapel program should be read updated or inserted c scripts are accessed run directly in chapel subprocesses using net cli commands 5 asynchronous multithreading for fast and reliable knowledge sharing in parallel calibration of hydrologic models to improve parallel computing performance by avoiding data knowledge transfer and or explicit synchronization between data knowledge senders and receivers pgas based programming languages provide global access to shared memory locations to all parallel computing units however this unbound global access can potentially lead to a data race condition if the parallelization approach lacks proper synchronization between reads and writes from to the global shared memories data races in general is when multiple parallel tasks access the shared memory without sufficient protections which leads to unpredictable behavior such as program crashing banerjee et al 2006 read write synchronization with which a parallel task blocks the access of other parallel tasks to shared memory locations when it attempts to read or write data knowledge is a popular strategy to address the race condition in pgas based systems nevertheless this strategy not only leads to deterioration in parallel performance to imposing over synchronization nishtala et al 2011 but also causes access hindrance to shared memory locations for the entire calibration time according to our observation in this study it is highly possible that a hydrologic model calibration crashes in the simulation stage when a parallel node accesses locks shared memory before moving onto the memory unblocking stage this leads to cutting off all parallel computing units from knowledge sharing throughout the model calibration in parallel calibration of hydrologic models model simulations on parallel computing units are asynchronous put simply it is common that on computing unit a the hydrologic model simulation is halfway through while on computing unit b it is finished the locale has shared knowledge and it intends to start a new model simulation asynchronous multithreading enables fast bare of synchronization and reliable bare of data race conditions knowledge sharing approach among parallel nodes asynchronous programming approach allows the program to execute a long running task and be responsive to any internal or external events at the same time syme et al 2011 in this study we addressed the race condition issue in parallel calibration of hydrologic models by asynchronous multithreading programming in chapel programming launguage we achieved this innovation by using chapel s high level statements which enables sequencing a program execution weiland 2007 accordingly reliable sharing of knowledge among chapel locales required the development of two asynchronous threads which we called them 1 listener thread fig 5 a and 2 investigated neighborhood thread fig 5 b the listener thread is a real time recipient of the best parameter set on local 0 the investigated neighborhood thread is not only a reliable recipient but also a provider of information about investigated neighborhood locations in the parameter space on each locale 1 listener thread by updating local databases each locale tracks the local improvement of the performance metric s as we see in the steps of locale 0 section of fig 5 a for any observed local improvement and with the assumption that it is also a global improvement an improvement considering the recorded performance of all model simulations carried out on all parallel computing units the corresponding locale sends a request to locale 0 to update its shared best parameter set upon receiving the request locale 0 locks the shared mutex clears the shared best parameter set memory containers queries the centralized database and fills the memory containers with the updated best parameter set the share mutex is shared memory space that is used read write by all locales to protect shared data from being simultaneously accessed the shared best parameter set memory location on locale 0 is partitioned into several sub locations equal to the number of locales and locale 0 fills all these sub locations with a copy of the best parameter set this approach was applied to address the data race issue when locales try to access the shared best parameter set memory location on locale 0 at the same time it should be mentioned that partitioning shared memory location into sub locations also enables hydrologists to share 1 different sub populations in population based optimization algorithms and or 2 different single parameter sets in single solution optimization algorithms with locales sharing different single parameter sets increases the exploration of the parameter space whenever necessary the shared mutex is a synchronized variable constantly being tracked by all locales listener thread fig 5 a s listener thread shows that when shared mutex is locked locales are not allowed to access the shared memory containers since they are being modified as soon as the shared mutex gets unlocked locales read their allocated sub location of the shared best parameter set memory location 2 investigated neighborhood thread as shown in the investigated neighborhood t hread section of fig 5 b this thread does 1 read and 2 write to the shared investigated neighborhood memory locations regarding the write behavior whenever a local intends to write into its memory sub location it sets its exclusive mutex to prevent other locales from reading its container while being modified since the size of shared knowledge should be bounded to avoid latency and long communication overheads the locale empties its memory container if it has reached the limit before updating writing into it the higher the number of model simulations the wider the investigated neighborhood on each locale in distributed systems with hundreds of locales reading shared knowledge of all locales that each has shared unbounded knowledge leads to longer one to one communication time and consequently the latency in the knowledge sharing step in addition the thread on each locale is responsible for checking the current best parameter set on locales from which it intends to read the shared knowledge if their best parameter sets differ from locale s it modifies the shared knowledge to make it applicable to its own best parameter set same applies when locale 0 s best parameter set gets updated and shared with all locales when a local intends to read the shared knowledge from other locales it checks their exclusive mutex if they are unlocked locks it and reads the shared knowledge otherwise it records the locale id and attempts it in the next simulations it should be mentioned that investigated neighborhood thread was not developed as listener type of thread since asynchronous threads should be alive and listening throughout the program execution the higher number of locales the higher number of alive asynchronous threads thus this synchronism might lead to performance degradation in general 6 parallel node locale fault tolerant capability the challenges of chapel q threads tasking layer for running hydrologic models faults failure caused by hardware and or software defections or programming bugs are categorized as transient due to the result of external disruptions intermittent due to unstable computer operation or permanent due to physical damages nelson 1990 the first two categories occur more often which calls for the development of fault tolerance feature for the parallel system fault tolerance feature of a system is its ability to perform its ultimate intended job despite the existence of faults ma et al 2022 investigated the capability of the hadoop based systems to handle parallel node failures for calibrating hydrologic models they claimed that such systems are fault tolerant thus partial failures do not impact the ongoing model calibration processes regarding calibration of distributed hydrologic models main faults can be caused by the inability of the parallel node in successful executions of the model simulations due to memory and hard disk issues for example memory issues can cause model simulations to either progress too slowly or stop altogether on the other hand hard disk issues that happen more frequently for models that read write databases can interrupt opening closing the input and output files or signal the model falsely that required files cannot be found the list of memory and disk issues can considerably increase when working with parallel systems with restricted allocated resources in this case segmentation fault becomes a common problem when the program hydrologic model frequently tries to read write illegal non allocated memory locations this fault causes calibration programs to crash chapel based parallel programs do not need to be tightly coupled with hydrologic models these models can be called and run as external programs e g executable or binary files directly in chapel subprocesses which are independent operating system os processes created inside the chapel program the first advantage of running hydrologic models in subprocesses is the capability of tracking the progress of model simulation to terminate it and all its associated subprocesses in cases where memory issues have already ceased and or slowed them in addition if the segmentation fault or disk issues arise chapel programs can be informed by reading subprocesses error pipes and they can terminate model simulations thus if hydrologic model calibrations fail or get terminated for some reason only their processes will be closed affected and the main process on which the parallel calibration algorithm is running will not get impacted and it can continue its ultimate job which fits into the definition of fault tolerant systems to maximize parallel performance the number of parallel tasks should be matched by available hardware in a parallel system which is carried out by parallel runtime qthreads is a general purpose parallel runtime that supports programming lightweight threads and various parallel synchronization methods in chapel the tasking layer which is based on qthreads is a fundamental layer for implementing threads and defining parallelization mechanisms the default value for the number of threads that the qthreads tasking layer can create in chapel is equal to the physical cpus on the locales to provide optimal affinity and high performance this pinning of threads to cpus is calledqthreads cpu pinning however running hydrologic models in chapel subprocesses faces an unusual and long running time due to the default setting of qthreads cpu pinning in chapel based programs in our research we learned that this default setting reserves all cpus on the machine only for chapel threads to workaround this barrier in the development of chapel based parallel calibration algorithms for hydrologic models we need to first limit the number of cpus that the chapel programs can use for creating their threads this can lead to performance degradation in running chapel threads if those threads are cpu intensive however in the calibration of hydrologic models running the hydrologic model itself is a cpu intensive task therefore limiting cpus on locales would not impact the overall performance of chapel programs in addition we need to disable qthreads cpu pinning which in theory allows the chapel threads and the subprocess threads to migrate away from one another when the hydrologic model is running in a subprocess chapel qthreads migrate to idle cpus if necessary to achieve the aforementioned goal we need to set qt affinity no and chpl rt num threads per locale which are chapel environment variables in our research setting these two variables led to superb performance improvement in running the imwebs model the runtime of a single imwebs model simulation for an 8 year calibration period decreased from 65 min to 13 min on a locale with 12 cpu cores this issue has already been flagged when running openmp threads with chapel threads rolinger et al 2018 we verified this issue in the calibration of hydrologic models as well and would like to suggest setting these variables in chapel based parallel calibration programs for hydrologic models 7 implementation of the knowledge sharing parallel calibration approach at the beginning of the calibration process data transfer to all locales should be accomplished outside the chapel based parallelization framework the hydrologic model binary file along with its input data calibration specific files containing essential information about performance metrics watershed gauge ids calibration parameter names etc c ef core project folder and locales local databases should be transferred across the cluster of locales afterwards the execution of the chapel based calibration program starts on locale 0 the responsible locale for data or and task parallelization this locale possesses a different workflow from others fig 6 and as the first step in its unique workflow it calls the database update function simultaneously on all locales this function fills local centralized databases with calibration specific information transferred to locales beforehand then to enable broad initial exploration of the parameter space locale 0 generates parameter sets with diverse parameter values for all locales thereafter declaring and initializing shared variables for enabling knowledge sharing among locales and task parallelization are carried out by calling coforall statement to ease task parallelization in chapel the coforall loop coding for the main parallelization in the calibration of hydrologic models is as the same as popular for loop coding but with a different keyword the main difference is that in the for loop the block of tasks in the body of the for loop is executed sequentially but in the coforall loop it is executed parallelly on locales after task parallelization locale 0 waits until either knowledge sharing requests or centralized database filling requests are sent from locales this locale also constantly tracks the improvement of performance metrics if the performance thresholds are met job termination orders are sent out to all other locales the workflow of all other locales starts with two simultaneous and independent threads which are alive throughout the calibration process the first thread calledthe main thread consists of sequential phases that are 1 parameter set and shared knowledge update 2 hydrologic model simulation 3 knowledge sharing 4 model post processing and database update and 5 termination check the second thread called the listener thread has only one responsibility which is to constantly checks shared locale 0 s best parameter set and reads it immediately when local 0 updates it as we explained the listener thread in previous subsections the focus here is to elaborate on the main thread and its sequential phases in the parameter set and shared knowledge update phase four tasks are carried out 1 reading the simulation id variable on locale 0 2 updating initializing locale s best parameter set 3 modifying locale s investigated neighborhood locations to reflect any update in the best parameter set and 4 running parallel dds to perturb the current parameter set based on the initialized or updated best parameter set as some locales face regular faults or slow executions due to limited computational resources to provide the load balance feature simulation ids should be instantiated on locales by reading the simulation id variable on locale 0 the value of this variable starts from 1 and is limited to the total defined of simulations tds and locales are responsible for incrementing it by a unit upon each read as soon as the simulation id value equals the tds the calibration terminates on all locales the next step is to either initialize generated initial parameter sets for locales on locale 0 or update best parameter set found on all locales and stored in locale 0 centralized database the best parameter set on each locale then using parallel dds algorithm and locale 0 s shared best parameter set the current parameter set on locale gets perturbed to generate a new parameter set for hydrologic model simulation this generated parameter set is fed to the hydrologic model in the hydrologic model simulation phase concurrent with the execution of the new model simulation the model progress check thread continuously checks 1 the log file of the model simulation to evaluate the progress rate and 2 potential memory issues if the progress rate is oddly slow or constant or the memory faults occur the model simulation gets terminated otherwise it keeps tracking until the model simulation is completed upon successful execution of the model simulation the current parameter set is added to the locale s investigated neighborhood locations and the associated thread not only shares the locale s updated knowledge but also reads other locales updated shared knowledge thereupon in the model post processing and database update phase parallel dds objective function s is calculated and locale s database gets updated in addition if the current parameter set has achieved a more optimum objective function value a request for updating best parameter set on locale 0 is sent as the last step in the termination check phase stopping criteria on both locale 0 and locale are checked to decide whether the model calibration must be ceased or continued 8 an application of knowledge sharing parallelization approach for calibrating distributed hydrologic models we applied our proposed parallelization approach as a general framework for parallel calibration of distributed hydrologic models to evaluate its efficiency in providing fast and reliable model calibration processes in this section we discussed our study area the distributed hydrologic model and the computing clusters along with performance metrics that we worked with in this research 8 1 the catfish creek watershed the catfish creek watershed located in southwest ontario fig 7 is a representative watershed within canada s great lakes ecoregion this watershed has an 490 k m 2 drainage area in elgin and oxford counties various climatic conditions along with terrain and altitude variations in southern ontario have brought a high level of biodiversity for this watershed the catfish creek watershed located at the north shore of lake erie has a temperate climate compared to other parts of southern ontario it has moderate and even precipitation all over the year with hot and humid summers and mild winters lake erie source protection region technical team 2008 the weather pattern shows that the winter days in this watershed begin in december and last until late february or early march with temperatures lower 0 o c spring lasts two months and june to september is considered the summer season which is followed by two months of the fall season sanderson 1998 the mean annual temperature in this watershed is about 7 5 8 5 o c with potential extreme temperatures in january 32 o c and july 38 o c this an agriculture dominated watershed with the city of st thomas and the town of aylmer as its main urban areas the catfish creek watershed is part of great lakes region the largest freshwater ecosystem in the world and it is in the carolinian canada life zone which is one of ontario s most threatened ecological regions besides this region supports approximately 25 of the total canadian agricultural productivity lake erie source protection region technical team 2008 the significant economic environmental and ecological importance of this region motivated us to select this representative watershed as our study area 8 2 integrated modelling for watershed evaluation of bmps imwebs model the integrated modelling system for watershed evaluation of bmps imwebs is a fully distributed process based hydrologic model designed by the guelph watershed evaluation group for best management practice bmp assessment 1 1 https www weg uoguelph ca weg model development this model is developed based upon the water and energy transfer between soil plant and atmosphere wetspa model wang et al 1996 agricultural policy environmental extender apex wang et al 2012 riparian ecosystem management model remm lowrance et al 2000 and soil water assessment tool swat model arnold et al 2012 three essential input datasets including dem soil and landuse for the imwebs model are geospatial data to delineate watershed generate parameters set up the model along with climate data such as precipitation minimum and maximum temperature solar radiation wind speed and direction and land management data for assessing various bmps scenarios using an object oriented and modular based approach for dynamic watershed modelling the imwebs model simulates hydrologic processes such as snowmelt climate water balance surface runoff groundwater plant growth soil erosion and nutrient cycles time series and spatial distribution are outputs of the imwebs model time series output includes sediment flow and nutrients at user defined locations and time ranges spatial distribution output contains the distribution of hydrologic variables at user defined scales and time ranges watershed evaluation group 2020 fig 8 shows the runtime of a single 5 year imwebs model simulation for the catfish creek watershed on different numbers of cpu cores the figure shows that as the number of cpu cores increases the imwebs runtime decreases this behavior indicates that the imwebs model inherently uses the multi threading approach to exploit as many cpu cores as available for a single model simulation besides fig 8 shows that the runtime has a behavior similar to the exponential decay behavior this behavior indicates that as the number of cpus increases the decrease rate in run time gets smaller as we observe in a regular personal computer with cpu cores varying between 5 and 10 the imwebs single simulation runtime is 7 10 min although the multi threading feature of the imwebs model provides faster sequential model simulation it imposes challenges in utilizing cpu cores for running concurrent model simulations on a single computer therefore the multi core execution of single model simulation in the imwebs model hinders the multi core execution of multiple model simulations due to limited computational resources on a single computer this limitation calls for adopting a multi computer parallelization approach for parallel calibration of the imwebs model in this study we calibrated the imwebs model to daily stream flow data at the watershed outlet for the period of 2005 2009 this period covers both wet and dry precipitation years in the catfish watershed fig 9 a shows the monthly average and fig 9 b shows the annual average of the precipitation in 2005 2009 using the split sample method the 2010 2015 period was selected as the model validation period for our model the calibration parameters along with the adopted ranges for each have been shown in table 1 8 3 model performance metrics the model performance fordaily flow simulation has been analyzed and reported using the nash sutcliffe nse coefficient nash and sutcliffe 1970 equation 4 mccuen et al 2006 suggested accompanying the nse value with the model bias value since the nse value is influenced by biased model predictions therefore we need to calculate the model bias to assure that the bias level is in an acceptable range in this study we used equation 5 to evaluate model bias equation 4 n s e 1 i 1 n o i s i 2 i 1 n o i o 2 equation 5 b i a s 1 i 1 n s i i 1 n o i o i and s i are the daily observed and simulated values of flow for the ith day of calibration validation period o is the mean of the observed data and n is the total number days in calibration validation period the nse metric shows the variance of the observation data explained by the simulated output this metric has been widely used in the literature on hydrologic model calibration and mostly emphasizes extreme events rather than average flows the nse ranges from to 1 1 indicates a perfect matching of simulated flow to the observed data 0 indicates that the model simulates as accurately as the mean of the observed data and less than zero suggests that the average of the observed data is a better predictor unduche et al 2018 in our optimization problem which is a minimization problem we used 1 n s e as our objective function model bias shows the average tendency of the simulated flow values to be greater or smaller than the of observation data therefore a model bias equal to 0 is the optimal value a positive bias shows an underestimation in simulated values and a negative value indicates an overestimation in simulated values van liew et al 2005 8 4 high performance computing systems graham and cedar clusters compute canada is a cluster computing network across canada consisting of regional cluster of servers and it provides the access to hpc computing resources for research in canada in fact by integrating high performance computers data resources tools and research support team it facilitates the parallel computing research around the country baldwin 2012 in this research we used two clusters named graham and cedar for implementing our knowledge sharing based parallel calibration framework graham and cedar are both heterogeneous linux based clusters graham is located at the university of waterloo and cedar is located at simon fraser university one of the differences that these two clusters have is their different cpu clock speed the number of cycles that a cpu executes per second each computer on graham cluster has intel r xeon r cpu e5 2667 v4 3 20 ghz cpu with 4 gb memory on the other hand each computer on cedar cluster has intel r xeon r cpu e5 2683 v4 2 10 ghz cpu similarly with 4 gb memory another important difference lies in their different high performance communication architecture graham benefits from the infiniband architecture which provides low latency several microseconds for small messages and high bandwidth 700 900 mbytes s liu et al 2005 cedar benefits from omni path architecture owned by intel it provides low communication latency and high throughput birrittella et al 2016 in this research we implemented our proposed framework on both clusters to explore how the interconnection architecture and the cpu clock speed can impact the chapel based parallel calibration framework in this study we considered the computational budget equivalent to 900 hydrologic model simulations optimization function evaluations on a parallel system with seven locales one locale 0 and 6 contributing locales in the calibration process we ran two 900 simulation replicates on both cedar and graham clusters to evaluate the convergence behavior and the communication overhead of our proposed knowledge sharing framework each locale had 15 cpu cores in total while 11 cpu cores were allocated to run the imwebs model and 4 cpu cores were allocated to the chapel program on each locale to evaluate the parallel speedup and parallel efficiency of our proposed framework we ran nine 900 simulation replicates with 2 3 4 5 6 7 8 and 9 locales on both cedar and graham with the same cpu core configurations 9 results and discussion we implemented our knowledge sharing parallel calibration framework for calibrating the imwebs model using stream flow data collected for the period of 2005 2009 in the catfish creek watershed the validation of our developed imwebs model was carried out using stream flow data collected in the 2010 2015 period this framework was implemented using seven parallel computing nodes six locales plus locale 0 on both cedar and graham clusters model performance results along with the assessment of the parallel framework e g parallel speedup and efficiency assessments etc are discussed in the following subsections 9 1 hydrologic model performance calibration and validation of the imwebs model by running 900 model simulations we evaluated the model performance by calculating daily and monthly nse and model bias values a 5 year calibration period was selected for the imwebs model based on previous studies on the calibration of the swat model for example carlos mendoza et al 2021 reported one to eight years of data collection could be sufficient to calibrate the model to the robust parameter values in addition the instability of imwebs runtime on both graham and cedar clusters hindered increasing the calibration period beyond five years due to a large number of model simulations as will be discussed in the next subsection the runtime of the imwebs model can increase to 3 times of its expected runtime throughout the calibration this increase can result in substantial inferior parallel performance for long calibration periods for complex models such as imwebs and large watersheds such as the catfish creek watershed although a 5 year period can be a sufficient calibration period carlos mendoza et al 2021 xu 2021 pointed out that for watersheds with higher rates of change in climate and physical characteristics eight to ten years of data could be considered necessary for model calibration with either daily or monthly time steps considering the model runtime instability challenge we calibrated our model for 2005 2009 in the first 900 model simulations then we switched to a 15 year calibration period 1995 2009 for the last 100 simulations to show the impact of the calibration length on model performance in our study we listed the statistical results of the calibrated model with a 5 year data and with 15 year data table 2 daily and monthly nse and model bias values for both calibration periods fall in the satisfactory 0 36 nse 0 60 and good 0 60 nse 0 75 categories carlos mendoza et al 2021 moriasi et al 2007 table 3 shows the calibrated values for calibration parameters in both calibration periods statistical measures in table 2 clearly show that extending the calibration period has improved both calibration nes and model bias values this indicates that climate and land use variabilities are influential factors to be considered in hydrologic model development for the catfish creek watershed in addition it also indicates that the low accuracy of input data could have played a destructive role for model performance in the shorter calibration period which was alleviated by extending the calibration period xu 2021 the improvement trend is also observable to some degree due to smaller extension compared to the calibration period extension in the longer validation period generally by falling in the satisfactory and good categories carlos mendoza et al 2021 moriasi et al 2007 the statistical results of model validation show the capability of the developed model for flow simulation outside the calibration periods and also there is not any parameter over fitting to the calibration period condition in the developed models gupta et al 2009 the observed and simulated daily flow hydrographs for two calibration and validation periods are shown in fig 10 the simulation hydrographs show that the developed imwebs models for both calibration periods can simulate the trend of the measured hydrograph however there is a higher fit for simulated to measured mapping for the longer calibration and validation periods in addition these hydrographs reveal that although the developed imwebs models can simulate low and intermediate flow values well they are relatively poor predictors for high flow values in some specific months such as january march and april examples are shown in light brown dashed boxes our investigation in precipitation data for these three months in 2005 2009 calibration period revealed that there were no significant precipitation events in these months hence the high flow values can be attributed to snowmelt process in the watershed the inability of the model in simulating this process properly can be explained by several reasons the first important reason suggested by zare et al 2022 is that the accuracy of precipitation and temperature data in snowmelt season substantially impacts the model performance in simulating high flow values according to wu et al s 2017 stdy proper parameter range selection highly impacts the model calibration performance the selected range for one parameter might impact the calibration of other parameters due to existing correlation of parameters in distributed hydrologic models in addition different parameter ranges can lead to changes in parameter sensitivity in watershed thus the correlation between parameters along with insensitive parameter ranges could influence the model ability in simulating the associated hydrological processes in addition butts et al 2004 points out that decision on how parameters change in the model calibration also impacts the overall model performance information on negative or positive correlation between parameters can be incorporated into the model calibration to achieve higher model performance among other reasons we can refer to model performance sensitivity to the selected calibration timescales massmann 2020 the impact of precipitation gauge networks in humid regions on the performance of the distributed hydrologic models xu et al 2013 etc 9 2 the impact of parallel dds knowledge sharing on the convergence behavior to evaluate the impact of sharing knowledge on the convergence behavior of the parallel dds across the cluster of computers we designed and tested two scenarios calledknowledge sharing and without knowledge sharing scenarios these scenarios both start with the same initial parameter sets on all locales to eliminate the effects of initial parameter sets on the path of convergence in the calibration process the only difference between these two scenarios is that the first scenario benefits from sharing knowledge such as the best parameter set from locale 0 and the forbidden n lists from all locales while in the second scenario we have independent locales without any communication with each other both scenarios were implemented on the graham cluster with seven chapel locales and for approximately 900 950 simulations their convergence behavior is shown in fig 11 in the knowledge sharing scenario the high and diversified values of the objective function 1 nse at the beginning of the optimization show that the different locales had started the calibration process by exploring various locations in the parameter space as the result of communications and knowledge sharing all locales were guided to the neighborhood of the best parameter set after 70 simulations in the calibration process and they started the thorough exploitation of this neighborhood after around 125 simulations on each locale due to low standard deviation of objective function values small improvements the parallel dds begun the limited explorations while continuing the exploitation of the parameter space this strategy ended up at improvement of the objective function value at around 140 simulations the trend throughout the calibration process shows that locales start and continue the exploration of the parameter space till they find and share the best parameter set with each other afterwards the intense exploitation by considering the forbidden n lists shared from all locales begins while the parallel dds also considers applying limited explorations whenever the improvement of the objective function appears to be small as shown in fig 11 the convergence behavior of the without knowledge sharing scenario and the knowledge sharing scenario was very different the without knowledge sharing scenario still had a high variation in objective function values after 70 simulations on locales we call this pattern the partial simultaneous exploration and exploitation of multiple neighborhoods condition this condition in simple words is when the optimization algorithm focuses on multiple neighborhoods instead of the one with a high probability of containing the most optimum solution to increase the exploration of the entire parameter space however due to the limited number of simulations allocated to each neighborhood the algorithm is unable to conduct sufficient exploitation of those neighborhoods as the result of this condition we see that the algorithm was unable to achieve a better or equal objective function value compared to the first scenario these two convergence behaviors show that the knowledge sharing among parallel nodes is an effective feature to lead parallel optimization algorithms to locations in the parameter space with more optimum objective function values 9 3 communication overhead and imwebs parallel execution runtime on cedar and graham clusters in parallel computing systems the communication time for sharing receiving data among parallel nodes is considered overhead low communication overhead is an important requirement for knowledge sharing among parallel nodes which is offered by chapel however helbecque et al 2022 mentioned that the communication overhead in chapel can be impacted by the network latencies in our study the cedar cluster provides the omni path network with latencies of a few microseconds and the graham cluster provides the infiniband network with latencies of less than 1 μs to evaluate the communication overhead in both clusters we measured the total runtime of every single simulation ts along with the imwebs model runtime ti in its associated simulation to obtain the portion of the total runtime of each simulation that can be a representative of the communication overheard we subtracted ti from ts and called it simulation runtime excluding imwebs runtime tsei most of this time is devoted to running the parallel dds algorithm database querying updating and communicating with other locales although this time does not merely show communication overhead it can help us evaluate its approximation fig 12 shows tsei on seven locales excluding locale 0 in cedar and graham clusters fig 12 shows that tsei is small on both clusters and the fact that the communication overhead could be lower and more consistent in the graham cluster than the cedar cluster this behavior is consistent with findings in helbecque et al s 2022 study and was expected due to the lower latency in infiniband network in the graham cluster tsei or communication overheard on the graham cluster varies around 37 40 s and on the cedar cluster around 40 60 s another obvious pattern in fig 12 is that some simulations show high and unusual tsei values we believe these unusual tsei values are not because of an increase in communication overhead in those simulations one of the main challenges of running the imwebs model on shared public clusters was the high rates of segmentation fault and hard disk issues reading writing from to with input and output database although these issues can occur at any point when the imwebs model is running we have tried to detect and terminate the affected model simulations after 3 min in most cases by implementing the model progress check thread in our program when an affected model simulation is terminated unsuccessfully after 3 min these wasted 3 min are automatically added to the next simulation s total runtime we have shown the 3 min threshold with a black dashed line in fig 12 if we exclude these wasted 3 min from those high and unusual tsei values tsei values fall in the 37 40 s range for the graham cluster red dashed line and in the 40 60 s range for the cedar cluster blue dashed line this proves that these high values are due to segmentation faults and hard disk issues occurring in the imwebs model simulations not increase in communications table 4 shows the average communication overhead as a function of imwebs runtime in 6 locales on both the cedar and graham cluster we averaged the ratio of the communication overhead to imwebs runtime for all simulations on each locale another issue that we faced in running the imwebs model was the high variation increase in the total runtime of a single model simulation as shown in fig 13 first on the graham cluster the common imweb runtimes were 7 8 and 9 min while on the cedar cluster they were 9 10 and 11 min this difference can be explained by the different cpu clock speeds that computers on graham and cedar clusters have second the runtime can increase from 7 9 min 21 min 3 times higher which considerably impacts the parallel speedup in addition it can impose challenges in extending the calibration period or working on larger watersheds imwebs is a distributed hydrologic model which requires various high resolution datasets to be loaded to the cache memory in each single model simulation in parallel systems with limited available memory e g public clusters this behavior could be explained by high volume of cache request to read hard disk and the exhaustion of the available memory therefore we believe this behavior is not imwebs specific and could be observed in the parallelization of distributed hydrologic models on restricted resource parallel systems 9 4 parallel speedup and parallel efficiency in a parallel computing system with n number of parallel units speedup is defined as the ratio of the elapsed time of running the program on a single computing unit to the execution time of running the program on n computing units eager et al 1989 in our study the elapsed time of running the program on a single computing unit is considered as running the calibration program for 900 simulations on a two locale parallel system on this system one locale was locale 0 which did not contribute to the calibration process and the other locale was locale 1 the only locale running the calibration process then we gradually put in more locales in the calibration process re ran the calibration program and measured the execution time of the system fig 14 a shows speedup gain for our proposed knowledge sharing parallelization approach for calibrating hydrologic models the number of locales on this figure is the number of locales contributing to the calibration process the total execution time of a parallel program is bounded by the parallel node with the lowest execution speed due to the variation and increase in execution time decrease in execution speed of the imwebs model fig 14 there is a low probability that a parallel system with two parallel nodes achieves a linear speedup 1 53 was achieved as we see the impact of imwebs run time variation decreases as the number of parallel nodes increases a sublinear speedup for a parallel system with two locales linear speedups for parallel systems with three and four locales 3 02 and 3 92 respectively and super linear speedups for systems with 4 locales 5 71 6 4 9 07 and 9 45 for 5 6 7 and 8 locales respectively achieved for our proposed parallelization approach the speedup trend was ascending which showed that the communication overhead in our system was also low fig 14 b shows the parallel efficiency for our proposed parallel framework using different number of locales two factors that determines either the degradation or improvement of the parallel efficiency in a parallel system are uneven parallel jobs allocation load unbalance and communication latency tang et al 2012 the optimal value of the parallel efficiency is 1 corresponds to linear speedup the closer to one the better the parallel efficiency in our study we achieved parallel efficiently 0 75 and for parallel systems with 4 locales the parallel efficiency surpassed one this proves that our parallel calibration framework has the load balance feature as discussed earlier and the communication latencies are low 10 conclusion in this study we designed and implemented a knowledge sharing parallel calibration approach for calibrating distributed hydrologic models using pgas parallelization concepts in chapel parallel programming language by adopting the asynchronous multi threading approach for developing the listener thread we enabled a fast and reliable knowledge sharing capability for parallel nodes in addition we used entity framework ef core library in c programming language to implement all necessary database management functions for our parallel chapel based calibration program we proposed and implemented the parallel dynamically dimensioned search dds algorithm equipped with divided perturbation zones and different size perturbation zones to increase the diversity of parameter sets and exploration range of the parameter space our proposed framework is also a fault tolerant framework that provides a reliable parallel calibration process for distributed hydrologic models to provide not only a fault tolerate feature for our parallelization approach but also a loose coupling feature between our parallel calibration program and hydrologic models we addressed the low performance challenge that the chapel q thread tasking layer imposes on running external applications in chapel parallel programs we implemented our parallel calibration framework on the graham and cedar cluster of computers on compute canada cluster computing network to calibrate the distributed imwebs model using the catfish creek watershed in ontario canada as the study area our parallel calibration framework achieved satisfactory to good model performance results in addition we showed that the knowledge sharing approach in parallelization of single solution optimization algorithms could improve the optimal solution by intensifying the exploitation within the initial set of simulations and further conducting limited exploration whenever the rate of improvement of the optimization problem becomes small based on the evaluation of our parallel calibration approach on two clusters of computers with different interconnection networks we proved that the interconnection network the type of network which connects parallel computing units to create a unified cluster was an influential factor in communication overhead in parallel systems in addition we showed that our proposed approach has a low communication overhead despite being equipped with the knowledge sharing feature the assessment of the parallel speedup and parallel efficiency showed that our parallel calibration framework could achieve super linear speedup with parallel efficiency above 75 these assessments proved that our proposed framework has the load balance feature and low communication overhead as a future research direction our proposed framework can be adapted with more sophisticated calibration algorithms and approaches that consider the inherent uncertainty of parameters associated with hydrologic models as well as the difficulty in characterizing the heterogeneity of hydrologic systems our approach is a general parallel calibration approach that researchers can adopt and alter the optimization algorithm to algorithms which consider the uncertainty of the calibration parameters in perturbing them in addition researchers can add global sensitivity analysis methods that empower the calibration process regarding its time and performance software and data availability the codes that were developed for parallel calibration of imwebs model using chapel parallel programming language can be found in github https github com marjanasgari chapelparalleldds git this repository was created by marjan asgari e mail masgari uoguelph ca in 2022 and has program codes author experimental environments are explained in detail in section 8 4 the authors do not have a permission to share data declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgement this work was supported by food from thought program at the university of guelph funded by the canada first research excellence fund the authors are grateful to chapel community for their constant support on the chapel s discourse page and compute canada support team for their fast and persistent troubleshooting and support special thanks to bradford chamberlain hewlett packard enterprise seattle usa for his contribution in brainstorming on debugging our chapel calibration program we would like to sincerely thank two anonymous reviewers for their valuable comments and feedback which helped us to improve our paper 
25409,a research gap in calibrating distributed watershed hydrologic models lies in the development of calibration frameworks adaptable to increasing complexity of hydrologic models parallel computing is a promising approach to address this gap however parallel calibration approaches should be fault tolerant portable and easy to implement with minimum communication overhead for fast knowledge sharing between parallel nodes accordingly we developed a knowledge sharing parallel calibration approach using chapel programming language with which we implemented the parallel dynamically dimensioned search dds algorithm by adopting multiple perturbation factors and parallel dynamic searching strategies to keep a balance between exploration and exploitation of the search space our results showed that this approach achieved super linear speedup and parallel efficiency above 75 in addition our approach has a low communication overhead along with the positive impact of knowledge sharing in the convergence behavior of the parallel dds algorithm keywords distributed hydrologic model parallel calibration approach parallel dds chapel data availability data will be made available on request 1 introduction due to the extensive spatiotemporal heterogeneity of predominant hydrological processes distributed watershed hydrologic models require numerous model parameters and input datasets singh and frevert 2003 khakbaz et al 2012 refsgaard 1997 one distributed model simulation accounts for the most time consuming single computation in watershed modelling with a runtime varying from minutes to days zhang et al 2009a b razavi et al 2010 subsequently applying global meta heuristic optimization algorithms imposes high computational requirements on calibrating these hydrologic models xu et al 2013 the calibration requires an abundant number of model simulations for thorough exploration and exploitation of their hyper dimensional parameter search space this substantial computational burden can also get exacerbated by using high resolution datasets and or working on larger watersheds budamala and baburao mahindrakar 2021 thus not only the calibration of distributed watershed hydrologic models is a computationally expensive process but also its complexity and size can potentially grow parallel computing is the simultaneous execution of independent and parallelizable components of a large computing application this approach is promising to provide speedup the ratio of execution times of a parallel application to its sequential version and scalability the ratio of actual speedup to the ideal speedup with a certain number of processors for watershed hydrologic model calibration with any size or complexity asgari et al 2022 quinn 1994 sub model and model level parallelization are two popular parallelization strategies at the sub model level parallelization takes place for the sub units of a hydrologic model to speed up a single model simulation however it comes with challenges of model reconstruction and complexities associated with parallel task management for component communications lin and zhang 2021 the model level parallelization is to parallelize integrated model simulations to speed up the entire model calibration process which has shown much ease in achieving parallel computing objectives for instance xia et al 2021 developed parallel optimization with dynamic coordinate search using surrogates pods to not only decrease the number of model simulations but also increase the calibration speed the implemented parallelization strategy was at the model level and the authors reported speedy automatic calibration in their study open multi processing openmp huo et al 2018 kan et al 2016 2018 2020 and message passing interface mpi li et al 2015 vrugt et al 2006 kim and ryu 2019 are two popular parallelization paradigms for model level parallelization strategy in the calibration of watershed hydrologic models however the current research gap in the parallel calibration of hydrologic models challenges the applicability of these paradigms population based metaheuristic algorithms such as artificial bee colony abc huo et al 2018 bayesian optimization bo ma et al 2022 shuffled complex evolution sce ua kan et al 2018 particle swarm algorithm pso ma et al 2021 niu et al 2021 and genetic algorithm d zhang et al 2016 have been extensively implemented in parallel calibration of hydrologic models contrary to single solution algorithms these algorithms search the parameter space with multiple initial evolved solutions this behavior empowers population based metaheuristic algorithms to investigate different zones simultaneously and eliminate the bias in the initial solution or sampling strategy prügel bennett 2010 hence they are inherently parallelizable as the initial population can be split into subpopulations to get evolved independently on parallel computing units however algorithms such as abc and pso are swarm based algorithms the evaluation of the population in these algorithms has a cooperation phase in which individuals transfer the information they have gained to lead each other to the global optimum ab wahab et al 2015 beheshti and shamsuddin 2013 although splitting the populations into subpopulations increases the diversity and lowers the possibility of getting trapped in a local optimum it eliminates the cooperation phase and impacts the convergence speed in addition hadka and reed 2015 showed that running distinct simulations with independent populations on each parallel node decreased the communication cost however the periodic migration events in which the most optimum populations from parallel nodes were exchanged led to producing the highest quality results thus not only the cooperation phase should be preserved in parallel population based optimization algorithms but also it is beneficial to provide it for parallel single solution optimization algorithms the reason is that single solution optimization algorithms are not equipped with strategies of avoidance of local optimum by appropriate cluster wide node coordination and broader exploration majeed and rao 2017 hence knowledge sharing becomes vital for both parallel single solution and population based optimization algorithms for adaptive and purposeful parallel exploitation and exploration of parameter search space knowledge sharing comes with two conditions asynchronicity and low communication overhead zamani et al 2021 parallelized dynamically the dimensioned search dds algorithm for parallel calibration of the swat model they provided knowledge sharing to some degree for the algorithm but in a synchronous architecture their results showed negative impacts of knowledge sharing on parallel efficiency in parallel calibration of hydrologic models this degradation of parallel efficiency defined as the speedup gain relative to the scale of the parallel system asgari et al 2022 was due to different configurations of computing units such as various disk access times for reading writing databases and potential failures throughout simulations on each node in a synchronous architecture besides communication overhead in knowledge sharing parallel systems should be minimized to ensure that adding the knowledge sharing feature does not impact the overall parallel efficiency of these systems considering these two conditions openmp and mpi parallelization paradigms are evaluated further openmp adopts fork join fj parallelism framework for shared memory parallelization in the fj framework task decomposition parallel computation and result combination are carried out repeatedly which is suitable for applications that can be broken down recursively into several subproblems peng et al 2017 this framework is more appropriate for fine grained parallelization to avoid long latencies the time elapse between requesting a memory space and receiving a response silc et al 1998 in the result combination step derived from unequal processing speeds of parallel computing units synchronization in the fj framework exists in the result combination where all computing units must finish processing their assigned subproblems before proceeding with further unprocessed ones by implementing fine grained parallelization which focuses on decomposing sequential problems to the maximum number of small and fast to finish parallelizable components latencies in the result combination of fj can be minimized corbellini et al 2018 lea 2000 another limitation of the openmp paradigm lies in its incapability of supplying horizontal scaling horizontal scaling is the capability of increasing involved computing units in the execution of the parallel application for more complex problems in openmp based systems the number of computing units and the size of the shared memory bound the horizontal scaling kirk and wen mei 2016 in fact long latencies in accessing the shared memory in these systems degrade theoretical parallel performance and consequently limit scalability the limitation of openmp based systems paved the way for using the mpi parallelization technique to develop distributed memory parallel systems for calibrating hydrologic models bomhof et al 2019 in mpi based systems data transfer among parallel tasks are explicitly managed by sending receiving messages these explicit send and receive messages which are i o requests ultimately lead to saturated i o activities long latencies due to multiple disk accesses and parallel computing performance degradation ou et al 2006 another associated problem with the mpi parallelization technique is its low parallel productivity niu et al 2021 which calls for more studies on parallel systems that can be easy to implement parallel productivity defines as the ability to develop high performance computing hpc systems that are easier to program which reduces software development cost and time however this ease should not decrease the overall system s parallel performance tolson and shoemaker 2007 studies have referred to the low parallel productivity of mpi based systems by mentioning the tight coupling between hydrologic models and these systems which requires model reconstruction lin and zhang 2021 zhang et al 2021 her et al 2015 pointed out that significant modifications of sequential optimization algorithms and or hydrologic model source codes were expected in developing mpi based parallel calibration systems niu et al 2021 encouraged hydrologists to adopt parallel computing in calibration of hydrologic models which led to increased use of distributed hydrologic models and their enhancement however implementation of parallel systems should be relatively easy with minimum requirements for pre installation frameworks libraries packages the challenges in mpi based systems along with their limitations in handling parallel task node failures motivated researchers to implement hadoop based systems lin and zhang 2021 zhang et al 2021 ma et al 2022 however several studies reported that the shared memory of hadoop based systems and the i o operations for node communications along with simultaneous disk access in computing units resulted in high i o overheads and long latencies in these systems zamani et al 2021 ma et al 2022 zhang et al 2016 2021 a zhang et al 2016 lin and zhang 2021 and ma et al 2021 pointed out that the spark computing framework can reduce disk i o overhead in hadoop these studies showed that increasing parallel tasks decreased the parallel efficiency in transferring big data blocks due to the distribution and operation of tasks in spark ma et al 2021 similarly due to spark task management and data transfer spark might not be suitable for calibrating lightweight hydrological models with intensive communications lin and zhang 2021 therefore there is a research gap in developing fault tolerant portable and easy to implement knowledge sharing based parallel calibration algorithms that require minimum communication overhead a new generation of parallel computing models called partitioned global address space pgas has emerged to improve parallel productivity and performance unlike openmp based systems where only global memory is available and mpi based systems where parallel computing units access merely local memory each computing unit in the pgas model has its local memory while part of it is shared with other computing units global memory in the pgas model computing nodes can read write not only their local memories but also the remote shared memory of each computing node synchronously or asynchronously de wael et al 2015 dinan et al 2010 pang et al 2022 showed that the asynchronous hydrologic model calibration has a 40 70 improvement in computational time compared to the synchronous version parallel calibration of hydrologic models has an asynchronous nature due to different configurations of computing units such as various disk access times for reading writing databases and potential failures throughout simulations on each node hence in nodal communications only one side is aware of the communication event which is called one sided communication this communication is a type of network communication in which one computing unit is responsible for identifying the sender receiver of the data knowledge and the size of data knowledge the other computing unit in network communication is only either the sender or receiver of data knowledge this communication is indispensable for asynchronous communication nishtala et al 2011 since the mpi parallelization paradigm requires matching send and receive messages to allow data transfer which is called two sided communication it is inconvenient for knowledge sharing parallel calibration of hydrologic models on the other hand pgas languages are attractive for the parallel calibration of hydrologic models as they support one sided communication chapel is a parallel programming language based on the pgas model which supports the implementation of productive and general purpose hpc systems through its high level abstractions for data and task parallelism gmys et al 2020 studied high performance programming languages chapel julia and python for developing parallel metaheuristic optimization algorithms by considering c openmp as the reference for their study they concluded that chapel was the only programming language with a higher level of parallel productivity than c openmp in programming the metaheuristic optimization algorithms the authors showed that chapel had the smallest gap between serial and parallel relative performance which proves its high scalability besides julia and python do not have mature multi threading support for programming parallel metaheuristics as chapel does in this study we proposed a knowledge sharing parallel calibration approach as a general parallel framework for calibrating distributed hydrologic models using the chapel programming language our parallelization approach stands out from all previously designed implemented approaches frameworks due to its unique features 1 parallel dynamically dimensioned search dds algorithm with different size perturbation and divided perturbation zones 2 advanced database management 3 asynchronous multithreading feature for fast and reliable knowledge sharing and 4 parallel node failure handling capability by addressing chapel s q threads tasking layer challenge for running hydrologic models we implemented our proposed parallel calibration approach to calibrate the integrated modelling for the watershed evaluation of bmps imwebs fully distributed hydrologic model liu et al 2018 for the catfish creek watershed of ontario in canada we evaluated the capability of our proposed approach in providing reliable model calibration results model performance along with parallel speed up and parallel efficiency our proposed approach achieved super linear speedup and parallel efficiency above 75 in addition we proved that our approach has a low communication overheard along with the positive impact of knowledge sharing in the convergence behaviour of the parallel dds algorithm the rest of the paper is organized as follows we started section 2 by presenting the conceptual framework for our proposed approach including an introduction to chapel programming language in section 3 we described the sequential dds algorithm and focused on discussing our proposed parallel dds algorithm in sections 4 5 and 6 we discussed in detail several unique features of our approach and ended up with the implementation procedure based on chapel programming concepts in section 7 in section 8 we described the study area the distributed hydrologic model and the parallel computing clusters systems that we adopted in our study in section 9 we presented our findings both from hydrologic model calibration and parallel computing perspectives we provided our conclusions in section 10 2 a conceptual framework for parallel calibration of distributed watershed hydrologic models we proposed a knowledge sharing parallelization approach as a general framework for calibrating distributed watershed hydrologic models and implemented it using chapel parallel programming language our study focused on rainfall runoff models for streamflow prediction at the watershed outlet s arsenault 2017 optimization algorithms are used in the model calibration process to simulate the behaviour of the watershed under study these algorithms iteratively 1 modify the values of model parameters 2 run model simulations and 3 calculate objective function s using streamflow measurement data and simulated results to guide the exploration and exploitation of the parameter search space pechlivanidis et al 2011 sitterson et al 2018 the calibration process should only include the representative parameters of dominant water cycle processes in the watershed under study to lessen computational burdens associated with model calibration we call these parameters calibration parametersthroughout this paper nonetheless pruning the model parameters to limit them to only calibration parameters is not a sufficient approach to address the computational burdens thoroughly hence parallel computing has been adopted to speed up the calibration of distributed hydrologic models and provide scalability as the complexity and size of the calibration applications grow 2 1 chapel programming language for parallel calibration of distributed hydrologic models parallel computing technologies should not be accompanied by steep learning curves for users with different levels of programming skills in addition there should be a minimum gap between parallel and sequential programming languages that most programmers are accustomed thus parallel programming languages should have abstractions for concepts such as parallelism data distribution synchronization and communication that are inherent elements of parallelization chamberlain et al 2007 chapel is a pgas based parallel programming language with a high level of programmability which results in a shorter time less cost of parallel application development dun and taura 2012 chapel is fast its code reading writing is as attractive as python code it competes with c and c in portability and flexibility respectively it provides the performance of fortran and its scalability challenges the scalability of mpi chamberlain et al 2018 in parallel calibration of hydrologic models chapel has not been used to our best knowledge therefore in this study we investigated it to develop our knowledge sharing parallel optimization algorithm for calibration of distributed hydrologic models in chapel locale indicates one of the parallel nodes on which the program runs all chapel programs start running from locale 0 which could be considered the master node and other locales as slave nodes based on the master slave model shao et al 2000 our proposed approach throughout the paper consists of two main components locale 0 s and other locales processes although locale 0 could participate in model calibration we decided to exempt it to avoid its memory draining which could be caused by iterative hydrologic model simulations 2 2 knowledge sharing approach for parallel calibration of hydrologic models in calibrating hydrologic models using single objective optimization the ultimate goal is to find optimal values for calibration parameters hence optimization s best solution is considered the best parameter set in this paper locale and locale 0 best parameter sets are parameter sets with the most optimum minimum or maximum objective function value s among all parameter sets used for model simulations on that specific locale and on all locales respectively fig 1 a shows graphically the independent blue and black dashed lines and knowledge sharing red dashed and solid lines parallelization approaches for simulation based parallelization of distributed watershed hydrologic model calibration using a single solution optimization algorithm fig 1 b translates fig 1 a in a stepwise approach in fig 1 b each locale communicates with locale 0 to receive its best parameter set the communication form style is described in section 5 in the absence of locale 0 s best parameter set each locale uses its local backup best parameter set which is the best parameter set obtained by that specific locale afterwards using the best parameter set either local or locale 0 best parameter set the current parameter set is perturbed using the dds optimization algorithm in the independent parallelization approach each locale works on its best parameter set independently from other locales this approach focuses on a wide exploration of the parameter space and diversifying the solutions however in distributed watershed hydrologic models with computationally expensive model simulations it is necessary to limit the exploration of parameter space and avoid searching neighborhoods containing less optimal parameter sets therefore in the knowledge sharing parallelization approach locales also communicate to share optimization knowledge and guide each other not to search neighbourhoods that have been already investigated we call this knowledge investigated neighborhood locations therefore in the knowledge sharing parallelization approach locales share two pieces of optimization knowledge called locale 0 best parameter set and investigated neighborhood locations these two pieces of information help locales guide each other toward the potential global optimum and have a more effective search of the parameter space working on the locale 0 best parameter set on all locales provides thorough exploitation of one neighborhood with high optimum potential instead of partial exploitations of multiple neighborhoods as exploitation of one neighborhood on all locales may lead to redundant simulations the same parameter sets may be investigated by more than one locale investigated neighborhood locations provides avoidance of redundant exploration and focus on new neighborhoods to increase the effectiveness of search as shown in fig 1 b the strategy on each locale is that the perturbation of the current parameter set on each locale gets repeated until the purebred parameter set is not among investigated neighborhood locations then the perturbed parameter set is added to locale s investigated neighborhood locations and this updated knowledge is shared with other locales in addition the hydrologic model with the new parameter set initializes a new model simulation and calculates the objective function value afterwards the simulation results are stored in the locale databases and a copy is sent to the locale 0 to be stored in the centralized databases the copy is sent through shared memory locations programming variable since all locales have access to the shared memory locations pgas only locale 0 reads and updates its centralized database with locales shared data our proposed knowledge sharing parallelization approach for calibrating hydrologic models has four distinctive features which are 1 parallel dds with different size perturbation and divided perturbation zones 2 advanced database management 3 asynchronous multithreading for fast and reliable knowledge sharing and 4 parallel node locale fault tolerant capability we further elaborated on these features in the following sections 3 heuristic optimization algorithms parallel dynamically dimensioned search dds algorithm a single objective optimization problem is defined as finding either the minimum or maximum value of an objective function f x in an n dimensional search space n 1 where each dimension is bounded with an upper and lower value l i x i k i 1 i n in equation 1 s denotes the bounded search space x is called a candidate solution and x is an optimal solution song et al 2019 more information can be found in tasgetiren and suganthan s 2006 study f x f x 1 x 2 x n f s r n equation1 f x f x x s r n heuristic optimization algorithms iteratively seek the optimal solutions in their given parameter search space until they meet pre defined constraint s during the exploration and exploitation of the parameter search space the solution with the minimum maximum objective function value depending on the nature of the optimization problem is labelled as the current best solution s the current best solution keeps changing improving until it is labelled as the optimal solution yang et al 2014 3 1 dynamically dimensioned search dds algorithm dynamically dimensioned search dds algorithm is a single solution algorithm that starts with exploring the search space via diversifying the candidate solutions and as the number of iterations increases it switches to an intensified search of the current best solution s neighborhood or exploitation this adjustment is carried out by a dynamic reduction of dimensions in the search space tolson and shoemaker 2007 fig 2 shows the logics of the dds algorithm for a 5 dimensional search space as its shown the candidate solution for each next iteration is generated based on three components which are current solution best solution and perturbation values in the dds algorithm the best solution is the solution which has achieved the minimum objective function value in minimization problems in all carried out iterations the value of the current solution for each dimension is called a decision variable e g x 1 x 2 in fig 2 and at each iteration dds randomly selects a subset of decision variables to perturb e g x 1 x 3 x 4 in fig 2 the perturbation value for ith decision variable is calculated using equation 2 in this equation n i is a random number in 0 1 range r is perturbation parameter which defines the random perturbation size as a fraction of the range of decision variable and its default value is 0 2 tolson and shoemaker 2007 and k i l i is the range of ith decision variable in the search space equation2 p i n i r k i l i the main motivations for utilizing this optimization algorithm in our study were 1 no need for computationally intensive task of fine tuning the algorithm setting and its parameter values dds has one parameter to set which is perturbation parameter r 2 dds support of both exploration and exploitation of the search space and 3 few dds parallelization studies in the calibration of hydrologic models tolson and shoemaker 2007 claimed that the dds algorithm is ideally suited for computationally expensive optimization problems such as distributed watershed model calibration bomhof et al 2019 showed that distributed hydrological model calibration results with the dds algorithm and the pa dds pareto archive dds algorithm the multi objective version of dds have the same quality in addition mai et al 2016 showed that the dds calibration results were located close to the pareto fronts of the pa dds results thus in the case of the dds and pa dds algorithms the multi objective calibrations are preferable when multiple equivalent solutions are required 3 2 parallel dds algorithm with different size perturbation and divided perturbation zones tolson et al 2014 developed two parallel implementations of the dds algorithm called the embarrassingly parallel dds ep dds and the parallel dds p dds their results showed that these implementations achieved higher quality solutions more efficiently compared to the serial dds algorithm however the ep dds algorithm did not have any communications between parallel nodes and the p dds algorithm was a synchronous implementation our parallel implementation of dds is not only asynchronous but also equipped with knowledge sharing communications among parallel computing nodes we developed our parallel version of the dds algorithm by proposing and implementing different size perturbation zones and divided perturbation zones algorithm1 shows the pseudocode of the only parameter perturbation phase of our sequential dds algorithm algorithm1 parameter perturbation phase of the dds algorithm image 1 matott et al 2013 showed that for dds algorithm the telescoping strategy or dynamic range reduction improves its efficiency and effectiveness in fact if searching parameter bounds are contracted step by step the dds algorithm achieves more efficient results in this study we defined multiple lenses of telescope from narrow to wide and assigned each computer to only search the parameter space through its own assigned lens fig 3 a shows permitted gray box and potential white oval change zones of calibration parameters around each parameter set blue circle in a 2 dimensional parameter space perturbed parameter sets can fall in any location inside the potential change zone algorithm1 since parameter ranges along with perturbation factor in the dds algorithm are constant values the controlling factor in location determination is the vector of normal values n n is a random vector generated for each model simulation with values in the range of 1 1 since n is a random vector there is no guarantee to have a good spread of the locations of perturbed parameter sets across the potential change zone to address the probable poor spread of parameters sets across the potential change zone we proposed and implemented the idea of divided perturbation zones parallel computing allows us to divide the 1 1 range into sub ranges and assign exploration and exploitation of each of the sub ranges to a parallel node thus this approach can reduce the probability of having concentrated locations for perturbed parameter sets while there are locations with low visiting rates yen et al s 2016 study concluded that the change of perturbation factor might lead to variation in convergence speed or having more optimum parameter sets as fig 3 c shows the values of the perturbation factor determine how wide the potential change zone can be the higher the perturbation factors the wider the perturbation zone and the higher chance of thorough exploration and exploitation of the permitted parameter change zone accordingly besides the approach of divided perturbation zones we proposed and implemented the approach of different size perturbation zones by assigning different perturbation factors to different parallel nodes in our study we used 0 1 0 2 and 0 3 r values and assigned them to an equal number of parallel nodes therefore by implementing both divided perturbation zones and different size perturbation ideas we were able to not only increase the diversity of parameter sets but also explore a higher range of parameter space fig 3 d shows the final potential change zone after applying both divided perturbation zones and different size perturbation zoneideas as mentioned earlier in our knowledge sharing parallelization approach chapel locales share investigated neighborhood locations that we call the forbidden n list as shown in algorithm1 a perturbed parameter set is generated by adding the perturbation vector p to the best parameter set vector p has two elements which are c b e s t and n c b e s t is the constant vector as we have shown in algorithm1 thus to differentiate neighbor locations from each other in the dds algorithm we need to know the associated n to each location so a list of n vectors each locale has examined to generate perturbed parameter sets defines the neighborhood that it has already investigated and other locales are forbidden to search those neighborhoods again this explains why we call the investigated neighborhood locations knowledge as forbidden n list two important adjustments need to be applied to the forbidden n list whenever the best parameter set on each locale gets changed and it intends to share the list with other locales forbidden n list contains investigated locations in the parameter space relative to the best parameter set therefore whenever the best parameter set gets changed all n s in the forbidden n list must get updated to reflect investigated locations relative to the new best parameter set as shown in fig 4 this process can be accomplished by deduction of n b e s t from all n s in forbidden n list n b e s t is the associated n to the newer best parameter set besides whenever each locale intends to read the shared forbidden n list from other locales it must check remote locales best parameter set to which forbidden n list locations on those locales are related if those best parameter sets are different from its own it should calculate the n b e s t the vector for achieving the locale s best parameter set from other locales best parameter sets and update the received forbidden n list only for itself another update should be applied to the forbidden n list when locales use various values for perturbation factor we came up with the idea of forbidden n list by considering the r value constant and the same 0 2 on all locales although the value is constant it is varied on different locales therefore we need to take out that variation in r values and integrate it to n since the n is only varying element in p this process is shown in detail in algorithm2 0 2 r n equals to s h a r e d m o d i f i e d n f o r b i d d e n in fig 4 algorithm2 modified forbidden n list image 2 4 advanced database management for the chapel based framework simultaneous model simulations on chapel locales require an independent parameter set with unique values and hydrologic model input files on each locale model calibration on each begins with receiving general information for calibration parameters e g names and calibration ranges of parameters in addition specific information data that the hydrologic model needs to run simulations are provided for the model calibration application in addition to enable post processing of the model output s on each locale information data such as observation data performance metrics calibration objective s etc should be provided for all locales separately after post processing stage model output s simulation information start end and total runtime etc performance evaluation results and used parameter set for each model simulation should be stored not only on local databases on locales but also in the centralized database on local 0 databases are stored on each locale s hard drive and they are independent from each other local databases are responsible for storing the results of model simulations carried out on each specific locale while the centralized database stores the results of model simulations carried out on all locales to have better data transfer minimize data read write mistakes avoid data redundancy and have fast data access we decided to use sqlite relational database to provide a reliable storage platform for chapel locales as chapel programming language lacks database management api libraries we utilized entity framework ef core library which is a database access library for net applications with c programming language to develop all essential database management functions for our knowledge sharing parallelization approach ef core an open access cross platform and well supported tool helps reduce development time and cost by enabling developers to program fast data access without using raw sql queries after developing the c database management scripts we built a collaboration between these scripts and the chapel program using 1 the chapel subprocess module and 2 net command line interface cli chapel subprocess module supports launching and interacting with other programs in our chapel program and net cli is a cross platform set of commands to develop and work with net applications projects smith 2021 thus whenever databases in our chapel program should be read updated or inserted c scripts are accessed run directly in chapel subprocesses using net cli commands 5 asynchronous multithreading for fast and reliable knowledge sharing in parallel calibration of hydrologic models to improve parallel computing performance by avoiding data knowledge transfer and or explicit synchronization between data knowledge senders and receivers pgas based programming languages provide global access to shared memory locations to all parallel computing units however this unbound global access can potentially lead to a data race condition if the parallelization approach lacks proper synchronization between reads and writes from to the global shared memories data races in general is when multiple parallel tasks access the shared memory without sufficient protections which leads to unpredictable behavior such as program crashing banerjee et al 2006 read write synchronization with which a parallel task blocks the access of other parallel tasks to shared memory locations when it attempts to read or write data knowledge is a popular strategy to address the race condition in pgas based systems nevertheless this strategy not only leads to deterioration in parallel performance to imposing over synchronization nishtala et al 2011 but also causes access hindrance to shared memory locations for the entire calibration time according to our observation in this study it is highly possible that a hydrologic model calibration crashes in the simulation stage when a parallel node accesses locks shared memory before moving onto the memory unblocking stage this leads to cutting off all parallel computing units from knowledge sharing throughout the model calibration in parallel calibration of hydrologic models model simulations on parallel computing units are asynchronous put simply it is common that on computing unit a the hydrologic model simulation is halfway through while on computing unit b it is finished the locale has shared knowledge and it intends to start a new model simulation asynchronous multithreading enables fast bare of synchronization and reliable bare of data race conditions knowledge sharing approach among parallel nodes asynchronous programming approach allows the program to execute a long running task and be responsive to any internal or external events at the same time syme et al 2011 in this study we addressed the race condition issue in parallel calibration of hydrologic models by asynchronous multithreading programming in chapel programming launguage we achieved this innovation by using chapel s high level statements which enables sequencing a program execution weiland 2007 accordingly reliable sharing of knowledge among chapel locales required the development of two asynchronous threads which we called them 1 listener thread fig 5 a and 2 investigated neighborhood thread fig 5 b the listener thread is a real time recipient of the best parameter set on local 0 the investigated neighborhood thread is not only a reliable recipient but also a provider of information about investigated neighborhood locations in the parameter space on each locale 1 listener thread by updating local databases each locale tracks the local improvement of the performance metric s as we see in the steps of locale 0 section of fig 5 a for any observed local improvement and with the assumption that it is also a global improvement an improvement considering the recorded performance of all model simulations carried out on all parallel computing units the corresponding locale sends a request to locale 0 to update its shared best parameter set upon receiving the request locale 0 locks the shared mutex clears the shared best parameter set memory containers queries the centralized database and fills the memory containers with the updated best parameter set the share mutex is shared memory space that is used read write by all locales to protect shared data from being simultaneously accessed the shared best parameter set memory location on locale 0 is partitioned into several sub locations equal to the number of locales and locale 0 fills all these sub locations with a copy of the best parameter set this approach was applied to address the data race issue when locales try to access the shared best parameter set memory location on locale 0 at the same time it should be mentioned that partitioning shared memory location into sub locations also enables hydrologists to share 1 different sub populations in population based optimization algorithms and or 2 different single parameter sets in single solution optimization algorithms with locales sharing different single parameter sets increases the exploration of the parameter space whenever necessary the shared mutex is a synchronized variable constantly being tracked by all locales listener thread fig 5 a s listener thread shows that when shared mutex is locked locales are not allowed to access the shared memory containers since they are being modified as soon as the shared mutex gets unlocked locales read their allocated sub location of the shared best parameter set memory location 2 investigated neighborhood thread as shown in the investigated neighborhood t hread section of fig 5 b this thread does 1 read and 2 write to the shared investigated neighborhood memory locations regarding the write behavior whenever a local intends to write into its memory sub location it sets its exclusive mutex to prevent other locales from reading its container while being modified since the size of shared knowledge should be bounded to avoid latency and long communication overheads the locale empties its memory container if it has reached the limit before updating writing into it the higher the number of model simulations the wider the investigated neighborhood on each locale in distributed systems with hundreds of locales reading shared knowledge of all locales that each has shared unbounded knowledge leads to longer one to one communication time and consequently the latency in the knowledge sharing step in addition the thread on each locale is responsible for checking the current best parameter set on locales from which it intends to read the shared knowledge if their best parameter sets differ from locale s it modifies the shared knowledge to make it applicable to its own best parameter set same applies when locale 0 s best parameter set gets updated and shared with all locales when a local intends to read the shared knowledge from other locales it checks their exclusive mutex if they are unlocked locks it and reads the shared knowledge otherwise it records the locale id and attempts it in the next simulations it should be mentioned that investigated neighborhood thread was not developed as listener type of thread since asynchronous threads should be alive and listening throughout the program execution the higher number of locales the higher number of alive asynchronous threads thus this synchronism might lead to performance degradation in general 6 parallel node locale fault tolerant capability the challenges of chapel q threads tasking layer for running hydrologic models faults failure caused by hardware and or software defections or programming bugs are categorized as transient due to the result of external disruptions intermittent due to unstable computer operation or permanent due to physical damages nelson 1990 the first two categories occur more often which calls for the development of fault tolerance feature for the parallel system fault tolerance feature of a system is its ability to perform its ultimate intended job despite the existence of faults ma et al 2022 investigated the capability of the hadoop based systems to handle parallel node failures for calibrating hydrologic models they claimed that such systems are fault tolerant thus partial failures do not impact the ongoing model calibration processes regarding calibration of distributed hydrologic models main faults can be caused by the inability of the parallel node in successful executions of the model simulations due to memory and hard disk issues for example memory issues can cause model simulations to either progress too slowly or stop altogether on the other hand hard disk issues that happen more frequently for models that read write databases can interrupt opening closing the input and output files or signal the model falsely that required files cannot be found the list of memory and disk issues can considerably increase when working with parallel systems with restricted allocated resources in this case segmentation fault becomes a common problem when the program hydrologic model frequently tries to read write illegal non allocated memory locations this fault causes calibration programs to crash chapel based parallel programs do not need to be tightly coupled with hydrologic models these models can be called and run as external programs e g executable or binary files directly in chapel subprocesses which are independent operating system os processes created inside the chapel program the first advantage of running hydrologic models in subprocesses is the capability of tracking the progress of model simulation to terminate it and all its associated subprocesses in cases where memory issues have already ceased and or slowed them in addition if the segmentation fault or disk issues arise chapel programs can be informed by reading subprocesses error pipes and they can terminate model simulations thus if hydrologic model calibrations fail or get terminated for some reason only their processes will be closed affected and the main process on which the parallel calibration algorithm is running will not get impacted and it can continue its ultimate job which fits into the definition of fault tolerant systems to maximize parallel performance the number of parallel tasks should be matched by available hardware in a parallel system which is carried out by parallel runtime qthreads is a general purpose parallel runtime that supports programming lightweight threads and various parallel synchronization methods in chapel the tasking layer which is based on qthreads is a fundamental layer for implementing threads and defining parallelization mechanisms the default value for the number of threads that the qthreads tasking layer can create in chapel is equal to the physical cpus on the locales to provide optimal affinity and high performance this pinning of threads to cpus is calledqthreads cpu pinning however running hydrologic models in chapel subprocesses faces an unusual and long running time due to the default setting of qthreads cpu pinning in chapel based programs in our research we learned that this default setting reserves all cpus on the machine only for chapel threads to workaround this barrier in the development of chapel based parallel calibration algorithms for hydrologic models we need to first limit the number of cpus that the chapel programs can use for creating their threads this can lead to performance degradation in running chapel threads if those threads are cpu intensive however in the calibration of hydrologic models running the hydrologic model itself is a cpu intensive task therefore limiting cpus on locales would not impact the overall performance of chapel programs in addition we need to disable qthreads cpu pinning which in theory allows the chapel threads and the subprocess threads to migrate away from one another when the hydrologic model is running in a subprocess chapel qthreads migrate to idle cpus if necessary to achieve the aforementioned goal we need to set qt affinity no and chpl rt num threads per locale which are chapel environment variables in our research setting these two variables led to superb performance improvement in running the imwebs model the runtime of a single imwebs model simulation for an 8 year calibration period decreased from 65 min to 13 min on a locale with 12 cpu cores this issue has already been flagged when running openmp threads with chapel threads rolinger et al 2018 we verified this issue in the calibration of hydrologic models as well and would like to suggest setting these variables in chapel based parallel calibration programs for hydrologic models 7 implementation of the knowledge sharing parallel calibration approach at the beginning of the calibration process data transfer to all locales should be accomplished outside the chapel based parallelization framework the hydrologic model binary file along with its input data calibration specific files containing essential information about performance metrics watershed gauge ids calibration parameter names etc c ef core project folder and locales local databases should be transferred across the cluster of locales afterwards the execution of the chapel based calibration program starts on locale 0 the responsible locale for data or and task parallelization this locale possesses a different workflow from others fig 6 and as the first step in its unique workflow it calls the database update function simultaneously on all locales this function fills local centralized databases with calibration specific information transferred to locales beforehand then to enable broad initial exploration of the parameter space locale 0 generates parameter sets with diverse parameter values for all locales thereafter declaring and initializing shared variables for enabling knowledge sharing among locales and task parallelization are carried out by calling coforall statement to ease task parallelization in chapel the coforall loop coding for the main parallelization in the calibration of hydrologic models is as the same as popular for loop coding but with a different keyword the main difference is that in the for loop the block of tasks in the body of the for loop is executed sequentially but in the coforall loop it is executed parallelly on locales after task parallelization locale 0 waits until either knowledge sharing requests or centralized database filling requests are sent from locales this locale also constantly tracks the improvement of performance metrics if the performance thresholds are met job termination orders are sent out to all other locales the workflow of all other locales starts with two simultaneous and independent threads which are alive throughout the calibration process the first thread calledthe main thread consists of sequential phases that are 1 parameter set and shared knowledge update 2 hydrologic model simulation 3 knowledge sharing 4 model post processing and database update and 5 termination check the second thread called the listener thread has only one responsibility which is to constantly checks shared locale 0 s best parameter set and reads it immediately when local 0 updates it as we explained the listener thread in previous subsections the focus here is to elaborate on the main thread and its sequential phases in the parameter set and shared knowledge update phase four tasks are carried out 1 reading the simulation id variable on locale 0 2 updating initializing locale s best parameter set 3 modifying locale s investigated neighborhood locations to reflect any update in the best parameter set and 4 running parallel dds to perturb the current parameter set based on the initialized or updated best parameter set as some locales face regular faults or slow executions due to limited computational resources to provide the load balance feature simulation ids should be instantiated on locales by reading the simulation id variable on locale 0 the value of this variable starts from 1 and is limited to the total defined of simulations tds and locales are responsible for incrementing it by a unit upon each read as soon as the simulation id value equals the tds the calibration terminates on all locales the next step is to either initialize generated initial parameter sets for locales on locale 0 or update best parameter set found on all locales and stored in locale 0 centralized database the best parameter set on each locale then using parallel dds algorithm and locale 0 s shared best parameter set the current parameter set on locale gets perturbed to generate a new parameter set for hydrologic model simulation this generated parameter set is fed to the hydrologic model in the hydrologic model simulation phase concurrent with the execution of the new model simulation the model progress check thread continuously checks 1 the log file of the model simulation to evaluate the progress rate and 2 potential memory issues if the progress rate is oddly slow or constant or the memory faults occur the model simulation gets terminated otherwise it keeps tracking until the model simulation is completed upon successful execution of the model simulation the current parameter set is added to the locale s investigated neighborhood locations and the associated thread not only shares the locale s updated knowledge but also reads other locales updated shared knowledge thereupon in the model post processing and database update phase parallel dds objective function s is calculated and locale s database gets updated in addition if the current parameter set has achieved a more optimum objective function value a request for updating best parameter set on locale 0 is sent as the last step in the termination check phase stopping criteria on both locale 0 and locale are checked to decide whether the model calibration must be ceased or continued 8 an application of knowledge sharing parallelization approach for calibrating distributed hydrologic models we applied our proposed parallelization approach as a general framework for parallel calibration of distributed hydrologic models to evaluate its efficiency in providing fast and reliable model calibration processes in this section we discussed our study area the distributed hydrologic model and the computing clusters along with performance metrics that we worked with in this research 8 1 the catfish creek watershed the catfish creek watershed located in southwest ontario fig 7 is a representative watershed within canada s great lakes ecoregion this watershed has an 490 k m 2 drainage area in elgin and oxford counties various climatic conditions along with terrain and altitude variations in southern ontario have brought a high level of biodiversity for this watershed the catfish creek watershed located at the north shore of lake erie has a temperate climate compared to other parts of southern ontario it has moderate and even precipitation all over the year with hot and humid summers and mild winters lake erie source protection region technical team 2008 the weather pattern shows that the winter days in this watershed begin in december and last until late february or early march with temperatures lower 0 o c spring lasts two months and june to september is considered the summer season which is followed by two months of the fall season sanderson 1998 the mean annual temperature in this watershed is about 7 5 8 5 o c with potential extreme temperatures in january 32 o c and july 38 o c this an agriculture dominated watershed with the city of st thomas and the town of aylmer as its main urban areas the catfish creek watershed is part of great lakes region the largest freshwater ecosystem in the world and it is in the carolinian canada life zone which is one of ontario s most threatened ecological regions besides this region supports approximately 25 of the total canadian agricultural productivity lake erie source protection region technical team 2008 the significant economic environmental and ecological importance of this region motivated us to select this representative watershed as our study area 8 2 integrated modelling for watershed evaluation of bmps imwebs model the integrated modelling system for watershed evaluation of bmps imwebs is a fully distributed process based hydrologic model designed by the guelph watershed evaluation group for best management practice bmp assessment 1 1 https www weg uoguelph ca weg model development this model is developed based upon the water and energy transfer between soil plant and atmosphere wetspa model wang et al 1996 agricultural policy environmental extender apex wang et al 2012 riparian ecosystem management model remm lowrance et al 2000 and soil water assessment tool swat model arnold et al 2012 three essential input datasets including dem soil and landuse for the imwebs model are geospatial data to delineate watershed generate parameters set up the model along with climate data such as precipitation minimum and maximum temperature solar radiation wind speed and direction and land management data for assessing various bmps scenarios using an object oriented and modular based approach for dynamic watershed modelling the imwebs model simulates hydrologic processes such as snowmelt climate water balance surface runoff groundwater plant growth soil erosion and nutrient cycles time series and spatial distribution are outputs of the imwebs model time series output includes sediment flow and nutrients at user defined locations and time ranges spatial distribution output contains the distribution of hydrologic variables at user defined scales and time ranges watershed evaluation group 2020 fig 8 shows the runtime of a single 5 year imwebs model simulation for the catfish creek watershed on different numbers of cpu cores the figure shows that as the number of cpu cores increases the imwebs runtime decreases this behavior indicates that the imwebs model inherently uses the multi threading approach to exploit as many cpu cores as available for a single model simulation besides fig 8 shows that the runtime has a behavior similar to the exponential decay behavior this behavior indicates that as the number of cpus increases the decrease rate in run time gets smaller as we observe in a regular personal computer with cpu cores varying between 5 and 10 the imwebs single simulation runtime is 7 10 min although the multi threading feature of the imwebs model provides faster sequential model simulation it imposes challenges in utilizing cpu cores for running concurrent model simulations on a single computer therefore the multi core execution of single model simulation in the imwebs model hinders the multi core execution of multiple model simulations due to limited computational resources on a single computer this limitation calls for adopting a multi computer parallelization approach for parallel calibration of the imwebs model in this study we calibrated the imwebs model to daily stream flow data at the watershed outlet for the period of 2005 2009 this period covers both wet and dry precipitation years in the catfish watershed fig 9 a shows the monthly average and fig 9 b shows the annual average of the precipitation in 2005 2009 using the split sample method the 2010 2015 period was selected as the model validation period for our model the calibration parameters along with the adopted ranges for each have been shown in table 1 8 3 model performance metrics the model performance fordaily flow simulation has been analyzed and reported using the nash sutcliffe nse coefficient nash and sutcliffe 1970 equation 4 mccuen et al 2006 suggested accompanying the nse value with the model bias value since the nse value is influenced by biased model predictions therefore we need to calculate the model bias to assure that the bias level is in an acceptable range in this study we used equation 5 to evaluate model bias equation 4 n s e 1 i 1 n o i s i 2 i 1 n o i o 2 equation 5 b i a s 1 i 1 n s i i 1 n o i o i and s i are the daily observed and simulated values of flow for the ith day of calibration validation period o is the mean of the observed data and n is the total number days in calibration validation period the nse metric shows the variance of the observation data explained by the simulated output this metric has been widely used in the literature on hydrologic model calibration and mostly emphasizes extreme events rather than average flows the nse ranges from to 1 1 indicates a perfect matching of simulated flow to the observed data 0 indicates that the model simulates as accurately as the mean of the observed data and less than zero suggests that the average of the observed data is a better predictor unduche et al 2018 in our optimization problem which is a minimization problem we used 1 n s e as our objective function model bias shows the average tendency of the simulated flow values to be greater or smaller than the of observation data therefore a model bias equal to 0 is the optimal value a positive bias shows an underestimation in simulated values and a negative value indicates an overestimation in simulated values van liew et al 2005 8 4 high performance computing systems graham and cedar clusters compute canada is a cluster computing network across canada consisting of regional cluster of servers and it provides the access to hpc computing resources for research in canada in fact by integrating high performance computers data resources tools and research support team it facilitates the parallel computing research around the country baldwin 2012 in this research we used two clusters named graham and cedar for implementing our knowledge sharing based parallel calibration framework graham and cedar are both heterogeneous linux based clusters graham is located at the university of waterloo and cedar is located at simon fraser university one of the differences that these two clusters have is their different cpu clock speed the number of cycles that a cpu executes per second each computer on graham cluster has intel r xeon r cpu e5 2667 v4 3 20 ghz cpu with 4 gb memory on the other hand each computer on cedar cluster has intel r xeon r cpu e5 2683 v4 2 10 ghz cpu similarly with 4 gb memory another important difference lies in their different high performance communication architecture graham benefits from the infiniband architecture which provides low latency several microseconds for small messages and high bandwidth 700 900 mbytes s liu et al 2005 cedar benefits from omni path architecture owned by intel it provides low communication latency and high throughput birrittella et al 2016 in this research we implemented our proposed framework on both clusters to explore how the interconnection architecture and the cpu clock speed can impact the chapel based parallel calibration framework in this study we considered the computational budget equivalent to 900 hydrologic model simulations optimization function evaluations on a parallel system with seven locales one locale 0 and 6 contributing locales in the calibration process we ran two 900 simulation replicates on both cedar and graham clusters to evaluate the convergence behavior and the communication overhead of our proposed knowledge sharing framework each locale had 15 cpu cores in total while 11 cpu cores were allocated to run the imwebs model and 4 cpu cores were allocated to the chapel program on each locale to evaluate the parallel speedup and parallel efficiency of our proposed framework we ran nine 900 simulation replicates with 2 3 4 5 6 7 8 and 9 locales on both cedar and graham with the same cpu core configurations 9 results and discussion we implemented our knowledge sharing parallel calibration framework for calibrating the imwebs model using stream flow data collected for the period of 2005 2009 in the catfish creek watershed the validation of our developed imwebs model was carried out using stream flow data collected in the 2010 2015 period this framework was implemented using seven parallel computing nodes six locales plus locale 0 on both cedar and graham clusters model performance results along with the assessment of the parallel framework e g parallel speedup and efficiency assessments etc are discussed in the following subsections 9 1 hydrologic model performance calibration and validation of the imwebs model by running 900 model simulations we evaluated the model performance by calculating daily and monthly nse and model bias values a 5 year calibration period was selected for the imwebs model based on previous studies on the calibration of the swat model for example carlos mendoza et al 2021 reported one to eight years of data collection could be sufficient to calibrate the model to the robust parameter values in addition the instability of imwebs runtime on both graham and cedar clusters hindered increasing the calibration period beyond five years due to a large number of model simulations as will be discussed in the next subsection the runtime of the imwebs model can increase to 3 times of its expected runtime throughout the calibration this increase can result in substantial inferior parallel performance for long calibration periods for complex models such as imwebs and large watersheds such as the catfish creek watershed although a 5 year period can be a sufficient calibration period carlos mendoza et al 2021 xu 2021 pointed out that for watersheds with higher rates of change in climate and physical characteristics eight to ten years of data could be considered necessary for model calibration with either daily or monthly time steps considering the model runtime instability challenge we calibrated our model for 2005 2009 in the first 900 model simulations then we switched to a 15 year calibration period 1995 2009 for the last 100 simulations to show the impact of the calibration length on model performance in our study we listed the statistical results of the calibrated model with a 5 year data and with 15 year data table 2 daily and monthly nse and model bias values for both calibration periods fall in the satisfactory 0 36 nse 0 60 and good 0 60 nse 0 75 categories carlos mendoza et al 2021 moriasi et al 2007 table 3 shows the calibrated values for calibration parameters in both calibration periods statistical measures in table 2 clearly show that extending the calibration period has improved both calibration nes and model bias values this indicates that climate and land use variabilities are influential factors to be considered in hydrologic model development for the catfish creek watershed in addition it also indicates that the low accuracy of input data could have played a destructive role for model performance in the shorter calibration period which was alleviated by extending the calibration period xu 2021 the improvement trend is also observable to some degree due to smaller extension compared to the calibration period extension in the longer validation period generally by falling in the satisfactory and good categories carlos mendoza et al 2021 moriasi et al 2007 the statistical results of model validation show the capability of the developed model for flow simulation outside the calibration periods and also there is not any parameter over fitting to the calibration period condition in the developed models gupta et al 2009 the observed and simulated daily flow hydrographs for two calibration and validation periods are shown in fig 10 the simulation hydrographs show that the developed imwebs models for both calibration periods can simulate the trend of the measured hydrograph however there is a higher fit for simulated to measured mapping for the longer calibration and validation periods in addition these hydrographs reveal that although the developed imwebs models can simulate low and intermediate flow values well they are relatively poor predictors for high flow values in some specific months such as january march and april examples are shown in light brown dashed boxes our investigation in precipitation data for these three months in 2005 2009 calibration period revealed that there were no significant precipitation events in these months hence the high flow values can be attributed to snowmelt process in the watershed the inability of the model in simulating this process properly can be explained by several reasons the first important reason suggested by zare et al 2022 is that the accuracy of precipitation and temperature data in snowmelt season substantially impacts the model performance in simulating high flow values according to wu et al s 2017 stdy proper parameter range selection highly impacts the model calibration performance the selected range for one parameter might impact the calibration of other parameters due to existing correlation of parameters in distributed hydrologic models in addition different parameter ranges can lead to changes in parameter sensitivity in watershed thus the correlation between parameters along with insensitive parameter ranges could influence the model ability in simulating the associated hydrological processes in addition butts et al 2004 points out that decision on how parameters change in the model calibration also impacts the overall model performance information on negative or positive correlation between parameters can be incorporated into the model calibration to achieve higher model performance among other reasons we can refer to model performance sensitivity to the selected calibration timescales massmann 2020 the impact of precipitation gauge networks in humid regions on the performance of the distributed hydrologic models xu et al 2013 etc 9 2 the impact of parallel dds knowledge sharing on the convergence behavior to evaluate the impact of sharing knowledge on the convergence behavior of the parallel dds across the cluster of computers we designed and tested two scenarios calledknowledge sharing and without knowledge sharing scenarios these scenarios both start with the same initial parameter sets on all locales to eliminate the effects of initial parameter sets on the path of convergence in the calibration process the only difference between these two scenarios is that the first scenario benefits from sharing knowledge such as the best parameter set from locale 0 and the forbidden n lists from all locales while in the second scenario we have independent locales without any communication with each other both scenarios were implemented on the graham cluster with seven chapel locales and for approximately 900 950 simulations their convergence behavior is shown in fig 11 in the knowledge sharing scenario the high and diversified values of the objective function 1 nse at the beginning of the optimization show that the different locales had started the calibration process by exploring various locations in the parameter space as the result of communications and knowledge sharing all locales were guided to the neighborhood of the best parameter set after 70 simulations in the calibration process and they started the thorough exploitation of this neighborhood after around 125 simulations on each locale due to low standard deviation of objective function values small improvements the parallel dds begun the limited explorations while continuing the exploitation of the parameter space this strategy ended up at improvement of the objective function value at around 140 simulations the trend throughout the calibration process shows that locales start and continue the exploration of the parameter space till they find and share the best parameter set with each other afterwards the intense exploitation by considering the forbidden n lists shared from all locales begins while the parallel dds also considers applying limited explorations whenever the improvement of the objective function appears to be small as shown in fig 11 the convergence behavior of the without knowledge sharing scenario and the knowledge sharing scenario was very different the without knowledge sharing scenario still had a high variation in objective function values after 70 simulations on locales we call this pattern the partial simultaneous exploration and exploitation of multiple neighborhoods condition this condition in simple words is when the optimization algorithm focuses on multiple neighborhoods instead of the one with a high probability of containing the most optimum solution to increase the exploration of the entire parameter space however due to the limited number of simulations allocated to each neighborhood the algorithm is unable to conduct sufficient exploitation of those neighborhoods as the result of this condition we see that the algorithm was unable to achieve a better or equal objective function value compared to the first scenario these two convergence behaviors show that the knowledge sharing among parallel nodes is an effective feature to lead parallel optimization algorithms to locations in the parameter space with more optimum objective function values 9 3 communication overhead and imwebs parallel execution runtime on cedar and graham clusters in parallel computing systems the communication time for sharing receiving data among parallel nodes is considered overhead low communication overhead is an important requirement for knowledge sharing among parallel nodes which is offered by chapel however helbecque et al 2022 mentioned that the communication overhead in chapel can be impacted by the network latencies in our study the cedar cluster provides the omni path network with latencies of a few microseconds and the graham cluster provides the infiniband network with latencies of less than 1 μs to evaluate the communication overhead in both clusters we measured the total runtime of every single simulation ts along with the imwebs model runtime ti in its associated simulation to obtain the portion of the total runtime of each simulation that can be a representative of the communication overheard we subtracted ti from ts and called it simulation runtime excluding imwebs runtime tsei most of this time is devoted to running the parallel dds algorithm database querying updating and communicating with other locales although this time does not merely show communication overhead it can help us evaluate its approximation fig 12 shows tsei on seven locales excluding locale 0 in cedar and graham clusters fig 12 shows that tsei is small on both clusters and the fact that the communication overhead could be lower and more consistent in the graham cluster than the cedar cluster this behavior is consistent with findings in helbecque et al s 2022 study and was expected due to the lower latency in infiniband network in the graham cluster tsei or communication overheard on the graham cluster varies around 37 40 s and on the cedar cluster around 40 60 s another obvious pattern in fig 12 is that some simulations show high and unusual tsei values we believe these unusual tsei values are not because of an increase in communication overhead in those simulations one of the main challenges of running the imwebs model on shared public clusters was the high rates of segmentation fault and hard disk issues reading writing from to with input and output database although these issues can occur at any point when the imwebs model is running we have tried to detect and terminate the affected model simulations after 3 min in most cases by implementing the model progress check thread in our program when an affected model simulation is terminated unsuccessfully after 3 min these wasted 3 min are automatically added to the next simulation s total runtime we have shown the 3 min threshold with a black dashed line in fig 12 if we exclude these wasted 3 min from those high and unusual tsei values tsei values fall in the 37 40 s range for the graham cluster red dashed line and in the 40 60 s range for the cedar cluster blue dashed line this proves that these high values are due to segmentation faults and hard disk issues occurring in the imwebs model simulations not increase in communications table 4 shows the average communication overhead as a function of imwebs runtime in 6 locales on both the cedar and graham cluster we averaged the ratio of the communication overhead to imwebs runtime for all simulations on each locale another issue that we faced in running the imwebs model was the high variation increase in the total runtime of a single model simulation as shown in fig 13 first on the graham cluster the common imweb runtimes were 7 8 and 9 min while on the cedar cluster they were 9 10 and 11 min this difference can be explained by the different cpu clock speeds that computers on graham and cedar clusters have second the runtime can increase from 7 9 min 21 min 3 times higher which considerably impacts the parallel speedup in addition it can impose challenges in extending the calibration period or working on larger watersheds imwebs is a distributed hydrologic model which requires various high resolution datasets to be loaded to the cache memory in each single model simulation in parallel systems with limited available memory e g public clusters this behavior could be explained by high volume of cache request to read hard disk and the exhaustion of the available memory therefore we believe this behavior is not imwebs specific and could be observed in the parallelization of distributed hydrologic models on restricted resource parallel systems 9 4 parallel speedup and parallel efficiency in a parallel computing system with n number of parallel units speedup is defined as the ratio of the elapsed time of running the program on a single computing unit to the execution time of running the program on n computing units eager et al 1989 in our study the elapsed time of running the program on a single computing unit is considered as running the calibration program for 900 simulations on a two locale parallel system on this system one locale was locale 0 which did not contribute to the calibration process and the other locale was locale 1 the only locale running the calibration process then we gradually put in more locales in the calibration process re ran the calibration program and measured the execution time of the system fig 14 a shows speedup gain for our proposed knowledge sharing parallelization approach for calibrating hydrologic models the number of locales on this figure is the number of locales contributing to the calibration process the total execution time of a parallel program is bounded by the parallel node with the lowest execution speed due to the variation and increase in execution time decrease in execution speed of the imwebs model fig 14 there is a low probability that a parallel system with two parallel nodes achieves a linear speedup 1 53 was achieved as we see the impact of imwebs run time variation decreases as the number of parallel nodes increases a sublinear speedup for a parallel system with two locales linear speedups for parallel systems with three and four locales 3 02 and 3 92 respectively and super linear speedups for systems with 4 locales 5 71 6 4 9 07 and 9 45 for 5 6 7 and 8 locales respectively achieved for our proposed parallelization approach the speedup trend was ascending which showed that the communication overhead in our system was also low fig 14 b shows the parallel efficiency for our proposed parallel framework using different number of locales two factors that determines either the degradation or improvement of the parallel efficiency in a parallel system are uneven parallel jobs allocation load unbalance and communication latency tang et al 2012 the optimal value of the parallel efficiency is 1 corresponds to linear speedup the closer to one the better the parallel efficiency in our study we achieved parallel efficiently 0 75 and for parallel systems with 4 locales the parallel efficiency surpassed one this proves that our parallel calibration framework has the load balance feature as discussed earlier and the communication latencies are low 10 conclusion in this study we designed and implemented a knowledge sharing parallel calibration approach for calibrating distributed hydrologic models using pgas parallelization concepts in chapel parallel programming language by adopting the asynchronous multi threading approach for developing the listener thread we enabled a fast and reliable knowledge sharing capability for parallel nodes in addition we used entity framework ef core library in c programming language to implement all necessary database management functions for our parallel chapel based calibration program we proposed and implemented the parallel dynamically dimensioned search dds algorithm equipped with divided perturbation zones and different size perturbation zones to increase the diversity of parameter sets and exploration range of the parameter space our proposed framework is also a fault tolerant framework that provides a reliable parallel calibration process for distributed hydrologic models to provide not only a fault tolerate feature for our parallelization approach but also a loose coupling feature between our parallel calibration program and hydrologic models we addressed the low performance challenge that the chapel q thread tasking layer imposes on running external applications in chapel parallel programs we implemented our parallel calibration framework on the graham and cedar cluster of computers on compute canada cluster computing network to calibrate the distributed imwebs model using the catfish creek watershed in ontario canada as the study area our parallel calibration framework achieved satisfactory to good model performance results in addition we showed that the knowledge sharing approach in parallelization of single solution optimization algorithms could improve the optimal solution by intensifying the exploitation within the initial set of simulations and further conducting limited exploration whenever the rate of improvement of the optimization problem becomes small based on the evaluation of our parallel calibration approach on two clusters of computers with different interconnection networks we proved that the interconnection network the type of network which connects parallel computing units to create a unified cluster was an influential factor in communication overhead in parallel systems in addition we showed that our proposed approach has a low communication overhead despite being equipped with the knowledge sharing feature the assessment of the parallel speedup and parallel efficiency showed that our parallel calibration framework could achieve super linear speedup with parallel efficiency above 75 these assessments proved that our proposed framework has the load balance feature and low communication overhead as a future research direction our proposed framework can be adapted with more sophisticated calibration algorithms and approaches that consider the inherent uncertainty of parameters associated with hydrologic models as well as the difficulty in characterizing the heterogeneity of hydrologic systems our approach is a general parallel calibration approach that researchers can adopt and alter the optimization algorithm to algorithms which consider the uncertainty of the calibration parameters in perturbing them in addition researchers can add global sensitivity analysis methods that empower the calibration process regarding its time and performance software and data availability the codes that were developed for parallel calibration of imwebs model using chapel parallel programming language can be found in github https github com marjanasgari chapelparalleldds git this repository was created by marjan asgari e mail masgari uoguelph ca in 2022 and has program codes author experimental environments are explained in detail in section 8 4 the authors do not have a permission to share data declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgement this work was supported by food from thought program at the university of guelph funded by the canada first research excellence fund the authors are grateful to chapel community for their constant support on the chapel s discourse page and compute canada support team for their fast and persistent troubleshooting and support special thanks to bradford chamberlain hewlett packard enterprise seattle usa for his contribution in brainstorming on debugging our chapel calibration program we would like to sincerely thank two anonymous reviewers for their valuable comments and feedback which helped us to improve our paper 
