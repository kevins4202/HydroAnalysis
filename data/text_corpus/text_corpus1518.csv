index,text
7590,rainfall is one of the most crucial processes in hydrology and the direct and indirect rainfall measurement methods are constantly being updated and improved the standard instrument used to measure rainfall rate and accumulation is the rain gauge which provides direct observations though the small dimension of the orifice allows rain gauges to be installed anywhere it also causes errors due to the splash and wind effects to investigate the role of the orifice dimension this study for the first time introduces and demonstrates an apparatus for observing rainfall called a giant rain gauge that is characterised by a collecting surface of 100m2 to discuss the new instrument and its technical details a preliminary analysis of 26 rainfall events is provided the results suggest that there are significant differences between the standard and proposed rain gauges specifically major discrepancies are evident for low time aggregation scale 5 10 and 15min and for high rainfall intensity values keywords rainfall precipitation rainfall instrument rain gauge giant rain gauge 1 introduction precipitation is a key input in a variety of environmental studies especially in hydrology where it is pivotal for achieving appropriate modelling and simulation e g larson and peck 1974 vaze et al 2011 of floods the total amount of liquid precipitation in mm is traditionally measured using rain gauges e g weighting type and tipping bucket type and these direct observations are considered the true amount at ground level the small orifice size usually in the range 100 300cm2 makes this instrument enough versatile to allow dense observation networks but the limited dimension is in contrast with the high space time variability of precipitation indeed rain gauges provide point measurements from which it is challenging to determine the spatial rainfall distribution or extend the estimated amount at the catchment scale moreover assuming that rain gauges are carefully maintained to avoid clogging of the gauge funnel and that they are periodically re calibrated some sources of error are possible due to the wind effects e g kurtyka 1953 robinson and rodda 1969 sevruk 1989 hughes et al 1993 hanna 1995 wetting losses e g sevruk 1989 sevruk and klemm 1989 evaporation e g sevruk 1986 and splashing effects e g rodda 1967 among them wind triggers the most important systematic error which occurs in the range of 2 10 nešpor and sevruk 1999 in contrast measurements obtained from remote sensing devices such as satellite borne airborne sensors and ground based radar can provide rainfall estimation over significantly larger areas thousands of km2 weather radar on the ground can be useful for monitoring a precipitation event and predicting its short term evolution because it offers the advantage of gaining insights into the characteristics of ongoing precipitation processes however ground weather radar provides indirect observations that are quantified by converting radar power or phase measurements into a rainfall rate using conversion relations quantitative precipitation estimations from ground based weather radar can be affected by many sources of errors e g brandes et al 1999 villarini and krajewski 2010 sebastianelli et al 2013 which can be related to errors in radar measurements i e radar mis calibration and attenuation effects or in the conversion of the radar output into the rainfall rate at the ground i e the vertical variability of the precipitation system and the assumption concerning the drop size distribution for applying rainfall algorithms adirosi et al 2014 furthermore comparing the weather radar and rain gauge measurements is arduous because of temporal and spatial sampling uncertainties villarini et al 2008 the spatial sampling error occurs because the estimation of the areal amounts use point measurements the accuracy of quantitative precipitation estimation from radar measurements is determined by comparing the radar rainfall output with the true rain gauge measurement however the large difference between rain gauges and the radar resolution area around nine orders of magnitude leads to large differences in the spatial sampling properties austin 1987 noted that even if rain gauge and radar measurements are not affected by errors discrepancies would remain between the devices due to the different sampling areas ciach and krajewski 1999 identified a lack of accurate area averaged rainfall data as one of the major issues in radar rainfall estimates in the literature many authors such as zawadzki 1973 rodrìguez iturbe and mejìa 1974 seed and austin 1990 kitchen and blackall 1992 ciach and krajewski 1999 habib et al 2001 habib and krajewski 2002 ciach 2003 wood et al 2000 jensen and pedersen 2005 villarini et al 2008 have studied the effects of different sampling areas in the radar rain gauge comparison and the correlated errors habib and krajewski 2002 for example found that in florida the spatial sampling error ranges from 40 to 80 of the total discrepancy between the radar and rain gauge measurements whereas jensen and pedersen 2005 found high variability approximately 100 of the rainfall accumulation measured by nine rain gauges within a single radar pixel 500m 500m despite the variety of literature regarding the error in the rain gauge radar comparison due to the huge difference in their sampling area to date the consequence of this issue on the estimation validation of the rainfall rate from remote sensing measurements is not fully documented or understood ciach and krajewski 1999 though it is a key topic in several disciplines following these premises the limited dimensions of the rain gauge orifice and the sampling discrepancy with the radar stimulated us to propose a special apparatus with an unprecedented sampling area 10m 10m called a giant rain gauge which has been designed built and installed at tuscia university in italy to the best of the authors knowledge there is no similar device currently at one s disposal and the only remotely similar approach was developed by de jong et al 2011 we believe that the analysis of the data collected by the giant rain gauge can provide information useful to better understand the sources of rain gauge errors for high precipitation intensities this new apparatus is expected to reduce typical rain gauge errors splashing and wind effect and to better understand the role of the rain gauge orifice size with respect to the spatial drop distribution compared with radar advantages are related to the reduced difference between the sampling areas of the two devices the giant rain gauge decreases the latter difference by almost three orders of magnitude in this study we provide a description of the proposed apparatus section 2 and details on its calibration and practical use section 3 moreover we present some preliminary results illustrating the measurement comparison among the giant and four benchmark standard rain gauges during 26 rainfall events focusing on high precipitation intensities section 4 a comparison with radar data and a specific investigation of the possible reduction of error due to splashing and wind effects will be the subjects of a future study 2 giant rain gauge apparatus description in this section the technical details of the proposed apparatus are provided the installation is located in viterbo province in central italy on the experimental farm of tuscia university in a flat and open area with an elevation of 300m a a s l fig 1 the general setup figs 1 and 2 consists of a 100m2 square surface that collects rainfall in a small cylindrical tank the four vertexes of the square surface are 302 5m a a s l and four standard rain gauges are installed in their proximity 1m from each vertex and at 303 5m a a s l fig 2a to provide a benchmark for the precipitation estimated using the giant rain gauge rainfall is collected first in two separate stainless steel corrugated sheets mounted on small wood pillars the sheets 10m 5m each are surrounded on the external sides by a 20 cm vertical border to prevent water losses due to the splash effect and they have a 20 slope to allow the water to move toward the centre line of the 100m2 surface the water flowing in the corrugated sheets is accumulated in a triangular stainless steel channel characterised by a 90 angle at the bottom two 15cm sides and 1 longitudinal slope the elevations of the triangular channel are respectively 301 5m and 301 4m a a s l in the upper and in the lower extremities the triangular channel is designed to allow water flow up to 10l s i e equivalent to a precipitation of approximately 350mm h intensity and to collect it in a vertical plastic pipeline with a diameter of 100mm that pours water into a cylindrical tank fig 2b and c the cylindrical tank at the end of the triangular section channel is on ground level it is characterised by a 50cm diameter and 1m height and it has a weir 30cm from the bottom fig 2d the weir has a lower triangular section with a 53 angle which becomes rectangular at a height of 4cm and has a constant 4cm width reaching to the upper part of the tank the weir mixed section has been adopted to obtain a significant water level for small precipitation amounts a water level of 5mm corresponds to 0 1mm h rainfall intensity at the bottom of the tank a high resolution water level sensor is present and allows for discharge flow measurement outside the tank using an appropriately calibrated weir equation see section 3 once the discharge is estimated it is possible to quantify the rainfall occurring on the giant rain gauge by dividing it by the 100m2 orifice area in section 3 details of the apparatus optimal time resolution are provided the instruments installed in the giant rain gauge are as follows 4 standard tipping bucket rain gauges sbs 500 campbell scientific the collector area is equal to 500cm2 the overall height is 440mm and the tip sensitivity is 0 20mm of rain the four rain gauges are installed as a benchmark for the rainfall estimated by the proposed apparatus 1 high resolution water level sensor sts atm 1st n this is built of stainless steel and was specifically designed for a water column with a height less than 1m and an accuracy of 0 3 1 ss2 pt100 standard air thermometer 1 vector instruments a100h anemometer and 1 li 200sa li cor pyranometer sensor that were designed to obtain field measurements of global solar radiation are present as ancillary instruments to complete the proposed apparatus all instruments are powered by a photovoltaic panel and are linked to a campbell scientific cr10x data logger to store the data at user defined timescales averaging the values that are recorded by the single instruments once per second during the first tests of the giant rain gauges two particular issues were faced mainly during high rainfall intensity a the water flowing from the two sheets toward the triangular channel was characterised by high velocities which resulted in overtopping of the channel sides and falling directly to the ground without reaching the tank and b the water surface inside the tank was characterised by high turbulence which resulted in a water level sensor signal with excessive noise the turbulence was present though the plastic pipeline ended at the bottom of the tank and 30cm of water was available to reduce the flow kinetic energy to overcome the first issue the triangular central channel was modified by introducing a vertical longitudinal sheet fig 2b and c in the middle of the section with the aim of absorbing the impact of the water flowing from the sheets and letting it fall entirely inside the channel without dispersion on the ground the second issue was solved by employing a second vertical pipeline 200mm diameter that was concentric with the first 100mm diameter plastic pipeline this second pipeline was placed on the bottom of the tank and had enough small holes to dissipate the water kinetic energy where the water sensor level was installed with this layout the water turbulence was limited to the area between two pipes and did not affect the water level sensor located outside the two pipelines moreover with the aim of reducing the noise signal the water level sensor was not placed at the bottom of the tank but was fixed within a 20 cm high concrete cylinder that was open on top 3 giant rain gauge calibration and optimal time resolution evaluation two sets of tests were performed the first one for quantifying the flow dynamic in the entire apparatus and the optimal time resolution to adopt for rainfall estimation and the second one for calibrating the weir stage discharge relationship the first test was conducted to quantify the time lag between the precipitation falling on the proposed apparatus and the water transit through the weir and consequently to evaluate the optimal time resolution of the giant rain gauge the structure of the proposed apparatus causes three types of time lag due to the water movement on the corrugated sheets in the triangular channel and within the tank where a certain difference in the water level can be found between the two concentric pipelines and the weir water height theoretically for high intensity rainfall the total time lag of the structure should be limited because the water moves with a relatively high velocity whereas for a low intensity rainfall the maximum time lag should be reached the general layout of the test is described as follows a pipeline with an 80mm diameter connected to a 5m3 open reservoir filled by the experimental farm well which is able to discharge up to 10l s was adopted the water present in the reservoir was pumped in the pipeline where a mag 1100 electromagnetic flow meter equipped with a signal converter mag 5000 accuracy 0 5 was installed and connected to the data logger an instantaneous rectangular impulse was generated in the farthest point of the proposed apparatus with respect to the weir once the fixed discharge coming from the pipeline pump system was stabilised at a constant value the pipeline was instantaneously located in the established point on the corrugated sheet kept in this position for a couple of minutes and then instantaneously removed from the corrugated sheets this operation was repeated four times re establishing the same initial condition with different discharge values that ranged from 1l s to 5l s fig 3 shows the obtained results comparing the rectangular input discharge and the weir answer there is a clear time shift between the two hydrographs shown in fig 3 this time shift is proportional to the input discharge and equal to 10 20 35 and 40s for input discharges equal to 1 1 2 1 3 35 and 4 8l s respectively the reason for this behaviour can be attributed to the increasing turbulence of the water flow toward the triangular channel and to the backwater effect when impacting on the vertical sheet this behaviour is due to the input flow concentration given by the pipeline in practice we assume that all of the precipitation is falling on 10cm2 of the surface so this time shift should be considered the ideal upper threshold because this effect should be significantly reduced with real uniformly distributed rainfall more evident from the results of fig 3 is the recession limb of the hydrographs where a conspicuous time lag is found between the end of the input discharge and the end of the output discharge this time lag which represents the concentration time of the proposed apparatus is almost constant in the four tests with values of 185 200 205 and 195s results suggest that the time lag does not depend on the input discharge indeed by visual inspection during tests it was clear that water spends most of the time in the triangular channel and specifically in its emptiness since for shallow water levels the flow velocity is very low these tests support the decision to fix the giant rain gauge time resolution to 5min the second test consists of pouring flow with a known discharge and recording the weir height related to that specific discharge using the same set up as the previous test pipeline reservoir electromagnetic flow meter the pipeline was poured directly into the triangular channel of the proposed apparatus and 10 tests with different discharges water levels were conducted by varying the discharge in the range 65 6500l h corresponding to rainfall intensities from 0 65mm h to 65mm h for each test the water sensor level and the electromagnetic flow meter values were stored in the data logger once per second and each test lasted 5min the stage discharge equation was estimated based on the discharge and water level values averaged over the recorded 300s fig 4 reports the results of this first test the left shows the whole stage discharge relationship and the right shows 300 data points from one test fig 4 indicates that the water level is affected by noise due to the residual turbulence inside the tank however this noise is present only when the sensor records every second calibrating the stage discharge equation with data averaged at a 5 min time resolution the coefficient of determination r 2 was equal to 0 9999 the equation is expressed through two formulas that are used for water levels lower than 4cm triangular section weir or higher than 4cm mixed rectangular triangular section weir 1 q 7 15 h 3 32 72 h 2 11 43 h h 4 cm q 51 04 h 4 2 657 5 h 4 1026 8 h 4 cm where q is the discharge l h and h is the water level cm the two previous formulas were implemented in the data logger programming code which allowed the calculation of the discharge from the water level value measured every second at a 5 min time resolution the average values for discharge which were estimated every second are calculated and stored on the data logger finally as aforementioned the rainfall depth in the selected time interval is estimated by dividing the average discharge by the 100m2 square surface of the proposed apparatus and its value is compared with the values provided by the four rain gauges 4 preliminary results in this section a preliminary comparison between the rainfall amount estimated using the giant rain gauge and the four benchmark standard rain gauges is illustrated the giant rain gauge has been operating since november 2012 and after a period for testing the functionality we started to record and collect rainfall data in the present work the comparison focuses on the precipitation intensity values at different time resolution as mentioned in the introduction one of the aims of this apparatus is to verify if short duration precipitations are influenced by the rain gauge orifice dimensions we analysed 26 rainfall events for which the main characteristics are illustrated in table 1 the magnitude of the selected storms is modest the 30 min intensity is an approximately 1 year return period in agreement with the viterbo intensity duration frequency curves the selected events cover a wide range of precipitation types and occurred in different seasons and with different wind and temperature conditions wind is observed at a 10 min resolution and table 1 shows the average and maximum value during the rainfall event the temperature is observed at a 1 h resolution and the average event value is listed in the last column of table 1 to evaluate the observations provided by the giant rain gauge and by the four benchmark rain gauges the intensity data collected during the 26 events are jointly visualised at different time resolutions 5min 10min 30min in a matrix plot which allows for a comparison of each benchmark rain gauge and the averaged values figs 5 7 a first visual comparison supports two conclusions the four rain gauges are in good agreement although they are located 10m from each other the scattering is small at a 5 min resolution the giant rain gauge vs benchmark rain gauge comparison displays a higher dispersion than does the benchmark rain gauge vs benchmark rain gauge this dispersion decreases with increasing temporal aggregation and is asymmetrical for high values at 5 and 10min of duration to preliminary quantify the difference between the giant rain gauge and the four benchmark rain gauges we chose the relative mean difference index that offers a direct quantification of the discrepancy 2 rmd j 1 n i gr r j r j where n is the number of values i represents the selected pairs gr is the value related to the giant rain gauge and rj is the value of each benchmark rain gauge with j 1 2 3 4 because the giant rain gauge was designed to investigate the behaviour of high rainfall intensity we fixed a threshold and compared only those pairs that were above this threshold specifically the empirical 95 quantile of observed giant rain gauge values is adopted for each time resolution the obtained thresholds are 1 08mm 1 47mm 1 96mm 3 53mm and 6 13mm for 5 10 15 30 and 60min respectively for the resulting pairs for which the giant rain gauge values are higher than the fixed threshold the rmd was estimated by comparing the giant rain gauge to each benchmark rain gauge fig 8 left illustrates the results and indicates that until 15min of duration there are significant differences from 40 at 5min to 10 at 15min with increasing duration the difference tends to zero for a complete overview and to confirm that the differences are related to the rainfall intensity two other quantiles were assumed as thresholds 75 and 85 and all data without a threshold were analysed fig 8 right summarises the obtained results and the threshold values are listed in table 2 in fig 8 right each line represents the average of the comparison with the four benchmarks reducing the quantiles reduces the differences and negative rmd values appear giant rain gauge values lower than the benchmark values negative rmd values are frequent for very low intensity rainfall 2 3mm h for which the apparatus time lag is greater than 5min and evaporation or losses in general could impact the data moreover fig 8 right suggests that 15min represents a duration threshold for which smaller dimensions of the collecting surface could impact the rainfall observation 5 conclusions and future work in this work an original apparatus called the giant rain gauge for rainfall observation is introduced and illustrated the innovation of this apparatus is the unprecedented dimensions of the collecting surface which are equal to 100m2 the goal is to provide a new benchmark for investigating the typical source of errors in common rain gauges splash and wind effects and the role of the limited orifice size and possibly to improve weather radar estimation and validation the proposed giant rain gauge is installed in an open experimental area of tuscia university in italy and is surrounded by four standard rain gauges useful as benchmarks for this first period of analysis calibration tests indicated that the optimal time resolution of the apparatus is 5min to provide an initial assessment of the proposed device 26 rainfall events were selected and their intensities were compared with the four common rain gauge values the events were not extreme because the return period was minimal 1year and covered a large range of possible storms in terms of season wind and temperature the comparison was performed by varying the time resolution 5 10 15 30 and 60min and estimating an index to quantify the differences for intensity values greater than differing thresholds as expected we found that for a time resolution lower than 15min there are significant differences 40 at 5min for rainfall intensity greater than 1mm 5min between the giant and standard rain gauges these differences tend toward zero with increasing resolution time and decreasing rainfall intensity future work is planned to develop two different analyses first events with specific climatic characteristics will be selected and compared using different instruments specifically intensity values related to the maximum temperature maximum wind or specific climatic properties will be analysed to support hypotheses on possible sources of discrepancy with common rain gauges high return period storms will be studied because we expect maximum differences among recording instruments for these events second a comparison with weather radar will be provided for major storms to evaluate the possible benefit of the proposed instrumentation acknowledgments we are grateful to witold krajewski rolf hut and an anonymous reviewer for their useful comments which contributed to improve the paper moreover we thank elisa adirosi francesco napolitano and fabio russo for their comments on the giant rain gauge design we are also immensely grateful to roberto rapiti giuliano cipollari and paolo valerio ciorba for the prototype fabrication 
7590,rainfall is one of the most crucial processes in hydrology and the direct and indirect rainfall measurement methods are constantly being updated and improved the standard instrument used to measure rainfall rate and accumulation is the rain gauge which provides direct observations though the small dimension of the orifice allows rain gauges to be installed anywhere it also causes errors due to the splash and wind effects to investigate the role of the orifice dimension this study for the first time introduces and demonstrates an apparatus for observing rainfall called a giant rain gauge that is characterised by a collecting surface of 100m2 to discuss the new instrument and its technical details a preliminary analysis of 26 rainfall events is provided the results suggest that there are significant differences between the standard and proposed rain gauges specifically major discrepancies are evident for low time aggregation scale 5 10 and 15min and for high rainfall intensity values keywords rainfall precipitation rainfall instrument rain gauge giant rain gauge 1 introduction precipitation is a key input in a variety of environmental studies especially in hydrology where it is pivotal for achieving appropriate modelling and simulation e g larson and peck 1974 vaze et al 2011 of floods the total amount of liquid precipitation in mm is traditionally measured using rain gauges e g weighting type and tipping bucket type and these direct observations are considered the true amount at ground level the small orifice size usually in the range 100 300cm2 makes this instrument enough versatile to allow dense observation networks but the limited dimension is in contrast with the high space time variability of precipitation indeed rain gauges provide point measurements from which it is challenging to determine the spatial rainfall distribution or extend the estimated amount at the catchment scale moreover assuming that rain gauges are carefully maintained to avoid clogging of the gauge funnel and that they are periodically re calibrated some sources of error are possible due to the wind effects e g kurtyka 1953 robinson and rodda 1969 sevruk 1989 hughes et al 1993 hanna 1995 wetting losses e g sevruk 1989 sevruk and klemm 1989 evaporation e g sevruk 1986 and splashing effects e g rodda 1967 among them wind triggers the most important systematic error which occurs in the range of 2 10 nešpor and sevruk 1999 in contrast measurements obtained from remote sensing devices such as satellite borne airborne sensors and ground based radar can provide rainfall estimation over significantly larger areas thousands of km2 weather radar on the ground can be useful for monitoring a precipitation event and predicting its short term evolution because it offers the advantage of gaining insights into the characteristics of ongoing precipitation processes however ground weather radar provides indirect observations that are quantified by converting radar power or phase measurements into a rainfall rate using conversion relations quantitative precipitation estimations from ground based weather radar can be affected by many sources of errors e g brandes et al 1999 villarini and krajewski 2010 sebastianelli et al 2013 which can be related to errors in radar measurements i e radar mis calibration and attenuation effects or in the conversion of the radar output into the rainfall rate at the ground i e the vertical variability of the precipitation system and the assumption concerning the drop size distribution for applying rainfall algorithms adirosi et al 2014 furthermore comparing the weather radar and rain gauge measurements is arduous because of temporal and spatial sampling uncertainties villarini et al 2008 the spatial sampling error occurs because the estimation of the areal amounts use point measurements the accuracy of quantitative precipitation estimation from radar measurements is determined by comparing the radar rainfall output with the true rain gauge measurement however the large difference between rain gauges and the radar resolution area around nine orders of magnitude leads to large differences in the spatial sampling properties austin 1987 noted that even if rain gauge and radar measurements are not affected by errors discrepancies would remain between the devices due to the different sampling areas ciach and krajewski 1999 identified a lack of accurate area averaged rainfall data as one of the major issues in radar rainfall estimates in the literature many authors such as zawadzki 1973 rodrìguez iturbe and mejìa 1974 seed and austin 1990 kitchen and blackall 1992 ciach and krajewski 1999 habib et al 2001 habib and krajewski 2002 ciach 2003 wood et al 2000 jensen and pedersen 2005 villarini et al 2008 have studied the effects of different sampling areas in the radar rain gauge comparison and the correlated errors habib and krajewski 2002 for example found that in florida the spatial sampling error ranges from 40 to 80 of the total discrepancy between the radar and rain gauge measurements whereas jensen and pedersen 2005 found high variability approximately 100 of the rainfall accumulation measured by nine rain gauges within a single radar pixel 500m 500m despite the variety of literature regarding the error in the rain gauge radar comparison due to the huge difference in their sampling area to date the consequence of this issue on the estimation validation of the rainfall rate from remote sensing measurements is not fully documented or understood ciach and krajewski 1999 though it is a key topic in several disciplines following these premises the limited dimensions of the rain gauge orifice and the sampling discrepancy with the radar stimulated us to propose a special apparatus with an unprecedented sampling area 10m 10m called a giant rain gauge which has been designed built and installed at tuscia university in italy to the best of the authors knowledge there is no similar device currently at one s disposal and the only remotely similar approach was developed by de jong et al 2011 we believe that the analysis of the data collected by the giant rain gauge can provide information useful to better understand the sources of rain gauge errors for high precipitation intensities this new apparatus is expected to reduce typical rain gauge errors splashing and wind effect and to better understand the role of the rain gauge orifice size with respect to the spatial drop distribution compared with radar advantages are related to the reduced difference between the sampling areas of the two devices the giant rain gauge decreases the latter difference by almost three orders of magnitude in this study we provide a description of the proposed apparatus section 2 and details on its calibration and practical use section 3 moreover we present some preliminary results illustrating the measurement comparison among the giant and four benchmark standard rain gauges during 26 rainfall events focusing on high precipitation intensities section 4 a comparison with radar data and a specific investigation of the possible reduction of error due to splashing and wind effects will be the subjects of a future study 2 giant rain gauge apparatus description in this section the technical details of the proposed apparatus are provided the installation is located in viterbo province in central italy on the experimental farm of tuscia university in a flat and open area with an elevation of 300m a a s l fig 1 the general setup figs 1 and 2 consists of a 100m2 square surface that collects rainfall in a small cylindrical tank the four vertexes of the square surface are 302 5m a a s l and four standard rain gauges are installed in their proximity 1m from each vertex and at 303 5m a a s l fig 2a to provide a benchmark for the precipitation estimated using the giant rain gauge rainfall is collected first in two separate stainless steel corrugated sheets mounted on small wood pillars the sheets 10m 5m each are surrounded on the external sides by a 20 cm vertical border to prevent water losses due to the splash effect and they have a 20 slope to allow the water to move toward the centre line of the 100m2 surface the water flowing in the corrugated sheets is accumulated in a triangular stainless steel channel characterised by a 90 angle at the bottom two 15cm sides and 1 longitudinal slope the elevations of the triangular channel are respectively 301 5m and 301 4m a a s l in the upper and in the lower extremities the triangular channel is designed to allow water flow up to 10l s i e equivalent to a precipitation of approximately 350mm h intensity and to collect it in a vertical plastic pipeline with a diameter of 100mm that pours water into a cylindrical tank fig 2b and c the cylindrical tank at the end of the triangular section channel is on ground level it is characterised by a 50cm diameter and 1m height and it has a weir 30cm from the bottom fig 2d the weir has a lower triangular section with a 53 angle which becomes rectangular at a height of 4cm and has a constant 4cm width reaching to the upper part of the tank the weir mixed section has been adopted to obtain a significant water level for small precipitation amounts a water level of 5mm corresponds to 0 1mm h rainfall intensity at the bottom of the tank a high resolution water level sensor is present and allows for discharge flow measurement outside the tank using an appropriately calibrated weir equation see section 3 once the discharge is estimated it is possible to quantify the rainfall occurring on the giant rain gauge by dividing it by the 100m2 orifice area in section 3 details of the apparatus optimal time resolution are provided the instruments installed in the giant rain gauge are as follows 4 standard tipping bucket rain gauges sbs 500 campbell scientific the collector area is equal to 500cm2 the overall height is 440mm and the tip sensitivity is 0 20mm of rain the four rain gauges are installed as a benchmark for the rainfall estimated by the proposed apparatus 1 high resolution water level sensor sts atm 1st n this is built of stainless steel and was specifically designed for a water column with a height less than 1m and an accuracy of 0 3 1 ss2 pt100 standard air thermometer 1 vector instruments a100h anemometer and 1 li 200sa li cor pyranometer sensor that were designed to obtain field measurements of global solar radiation are present as ancillary instruments to complete the proposed apparatus all instruments are powered by a photovoltaic panel and are linked to a campbell scientific cr10x data logger to store the data at user defined timescales averaging the values that are recorded by the single instruments once per second during the first tests of the giant rain gauges two particular issues were faced mainly during high rainfall intensity a the water flowing from the two sheets toward the triangular channel was characterised by high velocities which resulted in overtopping of the channel sides and falling directly to the ground without reaching the tank and b the water surface inside the tank was characterised by high turbulence which resulted in a water level sensor signal with excessive noise the turbulence was present though the plastic pipeline ended at the bottom of the tank and 30cm of water was available to reduce the flow kinetic energy to overcome the first issue the triangular central channel was modified by introducing a vertical longitudinal sheet fig 2b and c in the middle of the section with the aim of absorbing the impact of the water flowing from the sheets and letting it fall entirely inside the channel without dispersion on the ground the second issue was solved by employing a second vertical pipeline 200mm diameter that was concentric with the first 100mm diameter plastic pipeline this second pipeline was placed on the bottom of the tank and had enough small holes to dissipate the water kinetic energy where the water sensor level was installed with this layout the water turbulence was limited to the area between two pipes and did not affect the water level sensor located outside the two pipelines moreover with the aim of reducing the noise signal the water level sensor was not placed at the bottom of the tank but was fixed within a 20 cm high concrete cylinder that was open on top 3 giant rain gauge calibration and optimal time resolution evaluation two sets of tests were performed the first one for quantifying the flow dynamic in the entire apparatus and the optimal time resolution to adopt for rainfall estimation and the second one for calibrating the weir stage discharge relationship the first test was conducted to quantify the time lag between the precipitation falling on the proposed apparatus and the water transit through the weir and consequently to evaluate the optimal time resolution of the giant rain gauge the structure of the proposed apparatus causes three types of time lag due to the water movement on the corrugated sheets in the triangular channel and within the tank where a certain difference in the water level can be found between the two concentric pipelines and the weir water height theoretically for high intensity rainfall the total time lag of the structure should be limited because the water moves with a relatively high velocity whereas for a low intensity rainfall the maximum time lag should be reached the general layout of the test is described as follows a pipeline with an 80mm diameter connected to a 5m3 open reservoir filled by the experimental farm well which is able to discharge up to 10l s was adopted the water present in the reservoir was pumped in the pipeline where a mag 1100 electromagnetic flow meter equipped with a signal converter mag 5000 accuracy 0 5 was installed and connected to the data logger an instantaneous rectangular impulse was generated in the farthest point of the proposed apparatus with respect to the weir once the fixed discharge coming from the pipeline pump system was stabilised at a constant value the pipeline was instantaneously located in the established point on the corrugated sheet kept in this position for a couple of minutes and then instantaneously removed from the corrugated sheets this operation was repeated four times re establishing the same initial condition with different discharge values that ranged from 1l s to 5l s fig 3 shows the obtained results comparing the rectangular input discharge and the weir answer there is a clear time shift between the two hydrographs shown in fig 3 this time shift is proportional to the input discharge and equal to 10 20 35 and 40s for input discharges equal to 1 1 2 1 3 35 and 4 8l s respectively the reason for this behaviour can be attributed to the increasing turbulence of the water flow toward the triangular channel and to the backwater effect when impacting on the vertical sheet this behaviour is due to the input flow concentration given by the pipeline in practice we assume that all of the precipitation is falling on 10cm2 of the surface so this time shift should be considered the ideal upper threshold because this effect should be significantly reduced with real uniformly distributed rainfall more evident from the results of fig 3 is the recession limb of the hydrographs where a conspicuous time lag is found between the end of the input discharge and the end of the output discharge this time lag which represents the concentration time of the proposed apparatus is almost constant in the four tests with values of 185 200 205 and 195s results suggest that the time lag does not depend on the input discharge indeed by visual inspection during tests it was clear that water spends most of the time in the triangular channel and specifically in its emptiness since for shallow water levels the flow velocity is very low these tests support the decision to fix the giant rain gauge time resolution to 5min the second test consists of pouring flow with a known discharge and recording the weir height related to that specific discharge using the same set up as the previous test pipeline reservoir electromagnetic flow meter the pipeline was poured directly into the triangular channel of the proposed apparatus and 10 tests with different discharges water levels were conducted by varying the discharge in the range 65 6500l h corresponding to rainfall intensities from 0 65mm h to 65mm h for each test the water sensor level and the electromagnetic flow meter values were stored in the data logger once per second and each test lasted 5min the stage discharge equation was estimated based on the discharge and water level values averaged over the recorded 300s fig 4 reports the results of this first test the left shows the whole stage discharge relationship and the right shows 300 data points from one test fig 4 indicates that the water level is affected by noise due to the residual turbulence inside the tank however this noise is present only when the sensor records every second calibrating the stage discharge equation with data averaged at a 5 min time resolution the coefficient of determination r 2 was equal to 0 9999 the equation is expressed through two formulas that are used for water levels lower than 4cm triangular section weir or higher than 4cm mixed rectangular triangular section weir 1 q 7 15 h 3 32 72 h 2 11 43 h h 4 cm q 51 04 h 4 2 657 5 h 4 1026 8 h 4 cm where q is the discharge l h and h is the water level cm the two previous formulas were implemented in the data logger programming code which allowed the calculation of the discharge from the water level value measured every second at a 5 min time resolution the average values for discharge which were estimated every second are calculated and stored on the data logger finally as aforementioned the rainfall depth in the selected time interval is estimated by dividing the average discharge by the 100m2 square surface of the proposed apparatus and its value is compared with the values provided by the four rain gauges 4 preliminary results in this section a preliminary comparison between the rainfall amount estimated using the giant rain gauge and the four benchmark standard rain gauges is illustrated the giant rain gauge has been operating since november 2012 and after a period for testing the functionality we started to record and collect rainfall data in the present work the comparison focuses on the precipitation intensity values at different time resolution as mentioned in the introduction one of the aims of this apparatus is to verify if short duration precipitations are influenced by the rain gauge orifice dimensions we analysed 26 rainfall events for which the main characteristics are illustrated in table 1 the magnitude of the selected storms is modest the 30 min intensity is an approximately 1 year return period in agreement with the viterbo intensity duration frequency curves the selected events cover a wide range of precipitation types and occurred in different seasons and with different wind and temperature conditions wind is observed at a 10 min resolution and table 1 shows the average and maximum value during the rainfall event the temperature is observed at a 1 h resolution and the average event value is listed in the last column of table 1 to evaluate the observations provided by the giant rain gauge and by the four benchmark rain gauges the intensity data collected during the 26 events are jointly visualised at different time resolutions 5min 10min 30min in a matrix plot which allows for a comparison of each benchmark rain gauge and the averaged values figs 5 7 a first visual comparison supports two conclusions the four rain gauges are in good agreement although they are located 10m from each other the scattering is small at a 5 min resolution the giant rain gauge vs benchmark rain gauge comparison displays a higher dispersion than does the benchmark rain gauge vs benchmark rain gauge this dispersion decreases with increasing temporal aggregation and is asymmetrical for high values at 5 and 10min of duration to preliminary quantify the difference between the giant rain gauge and the four benchmark rain gauges we chose the relative mean difference index that offers a direct quantification of the discrepancy 2 rmd j 1 n i gr r j r j where n is the number of values i represents the selected pairs gr is the value related to the giant rain gauge and rj is the value of each benchmark rain gauge with j 1 2 3 4 because the giant rain gauge was designed to investigate the behaviour of high rainfall intensity we fixed a threshold and compared only those pairs that were above this threshold specifically the empirical 95 quantile of observed giant rain gauge values is adopted for each time resolution the obtained thresholds are 1 08mm 1 47mm 1 96mm 3 53mm and 6 13mm for 5 10 15 30 and 60min respectively for the resulting pairs for which the giant rain gauge values are higher than the fixed threshold the rmd was estimated by comparing the giant rain gauge to each benchmark rain gauge fig 8 left illustrates the results and indicates that until 15min of duration there are significant differences from 40 at 5min to 10 at 15min with increasing duration the difference tends to zero for a complete overview and to confirm that the differences are related to the rainfall intensity two other quantiles were assumed as thresholds 75 and 85 and all data without a threshold were analysed fig 8 right summarises the obtained results and the threshold values are listed in table 2 in fig 8 right each line represents the average of the comparison with the four benchmarks reducing the quantiles reduces the differences and negative rmd values appear giant rain gauge values lower than the benchmark values negative rmd values are frequent for very low intensity rainfall 2 3mm h for which the apparatus time lag is greater than 5min and evaporation or losses in general could impact the data moreover fig 8 right suggests that 15min represents a duration threshold for which smaller dimensions of the collecting surface could impact the rainfall observation 5 conclusions and future work in this work an original apparatus called the giant rain gauge for rainfall observation is introduced and illustrated the innovation of this apparatus is the unprecedented dimensions of the collecting surface which are equal to 100m2 the goal is to provide a new benchmark for investigating the typical source of errors in common rain gauges splash and wind effects and the role of the limited orifice size and possibly to improve weather radar estimation and validation the proposed giant rain gauge is installed in an open experimental area of tuscia university in italy and is surrounded by four standard rain gauges useful as benchmarks for this first period of analysis calibration tests indicated that the optimal time resolution of the apparatus is 5min to provide an initial assessment of the proposed device 26 rainfall events were selected and their intensities were compared with the four common rain gauge values the events were not extreme because the return period was minimal 1year and covered a large range of possible storms in terms of season wind and temperature the comparison was performed by varying the time resolution 5 10 15 30 and 60min and estimating an index to quantify the differences for intensity values greater than differing thresholds as expected we found that for a time resolution lower than 15min there are significant differences 40 at 5min for rainfall intensity greater than 1mm 5min between the giant and standard rain gauges these differences tend toward zero with increasing resolution time and decreasing rainfall intensity future work is planned to develop two different analyses first events with specific climatic characteristics will be selected and compared using different instruments specifically intensity values related to the maximum temperature maximum wind or specific climatic properties will be analysed to support hypotheses on possible sources of discrepancy with common rain gauges high return period storms will be studied because we expect maximum differences among recording instruments for these events second a comparison with weather radar will be provided for major storms to evaluate the possible benefit of the proposed instrumentation acknowledgments we are grateful to witold krajewski rolf hut and an anonymous reviewer for their useful comments which contributed to improve the paper moreover we thank elisa adirosi francesco napolitano and fabio russo for their comments on the giant rain gauge design we are also immensely grateful to roberto rapiti giuliano cipollari and paolo valerio ciorba for the prototype fabrication 
7591,we present the application of a parametric stochastic weather generator within a nonstationary context enabling simulations of weather sequences conditioned on interannual and multi decadal trends the generalized linear model framework of the weather generator allows any number of covariates to be included such as large scale climate indices local climate information seasonal precipitation and temperature among others here we focus on the salado a basin of the argentine pampas as a case study but the methodology is portable to any region we include domain averaged e g areal seasonal total precipitation and mean maximum and minimum temperatures as covariates for conditional simulation areal covariates are motivated by a principal component analysis that indicates the seasonal spatial average is the dominant mode of variability across the domain we find this modification to be effective in capturing the nonstationarity prevalent in interseasonal precipitation and temperature data we further illustrate the ability of this weather generator to act as a spatiotemporal downscaler of seasonal forecasts and multidecadal projections both of which are generally of coarse resolution keywords generalized linear models stochastic weather generator conditional simulation downscaling seasonal forecasts daily precipitation daily temperature 1 introduction scientific and technological advances together with awareness of the importance of climate on human endeavors are creating increased worldwide demand for climate information fortunately our ability to monitor and predict variations in climate has increased substantially barnston et al 2010 stockdale et al 2010 a number of groups now forecast climate conditions a few seasons ahead goddard et al 2003 saha et al 2006 emerging developments may enable climate projections 10 20years into the future a scale intermediate between seasonal forecasts and manmade climate change projections haines et al 2009 hurrell et al 2009 meehl et al 2009 these advances however must be matched by a better understanding of how science can inform climate resilient planning and development stainforth et al 2007 to support public and private adaptation and mitigation responses climate information must be credible legitimate and especially salient e g relevant to the needs of decision makers cash et al 2003 needs include not only predictions or projections 1 following bray and von storch 2009 prediction conveys a sense of certainty whereas projection is associated more with the possibility of something happening given a certain set of plausible but not necessarily probable circumstances a prediction can be used to design specific response strategies while a projection or more precisely a series of projections provides a range on which to consider a range of response strategies 1 bray and von storch 2009 of regional climate potential outcomes of adaptation actions are probably more relevant to stakeholders than raw climate information thus an enhanced capacity is needed to translate climate information into distributions of outcomes for risk assessment and management hansen et al 2006 process models e g crop biophysical models hydrological models can be useful tools to assess likely impacts on climate sensitive sectors of society and to evaluate the outcomes of alternative adaptive actions ferreyra et al 2001 berger 2001 berger et al 2006 happe et al 2008 freeman et al 2009 schreinemachers and berger 2011 bert et al 2006 2007 2014 these models however typically require daily weather data although historical daily weather can be used getting long term daily weather is laborious and costly at best and in some cases impossible typically historical observations have missing data that are not accepted by impact models similarly point measurements may not represent the true spatial variability of a nonstationary natural process e g daily precipitation most importantly observed sequences provide a solution based on only one realization of the weather process richardson 1981 the use of seasonal forecasts of regional climate and its impacts can help decision makers to lessen the adverse effects of unfavorable conditions or alternatively to capitalize on favorable conditions nevertheless a major obstacle to broader use of seasonal climate forecasts is their coarse spatial and temporal resolution similarly 10 20year projections of regional climate conditions have been identified as important to infrastructure planners water resource managers and many others hurrell et al 2009 unfortunately projections of regional monthly precipitation and temperature from climate models not only are coarse in space and time as seasonal forecasts but also involve considerable uncertainty which requires exploration of the impacts of alternative plausible trajectories stochastic weather generators have long been used for risk assessment and adaptation as they can provide a rich variety of plausible climatic scenarios moreover weather generators can produce spatially consistent series that can be used to downscale larger scale scenarios traditional weather generators stemming from richardson 1981 model precipitation occurrence as a chain dependent process katz 1977 and thus are capable of generating physically realistic prolonged wet and dry spells the remaining weather variables e g precipitation intensity and temperature are parameterized using probability distributions for precipitation intensity and linear time series models for temperature which capture historical climatological variability and linear relationships between variables but fail to capture extremes e g extreme drought or flooding in order to capture the variability of weather attributes in any specific season the simulations need to be conditioned on appropriate covariates one approach is to estimate the parameters of the generator conditionally by considering enso el niño southern oscillation trenberth and stepaniak 2001 phase or any other teleconnection to a region s climate which enables simulation of skillful sequences grondona et al 2000 ferreyra et al 2001 wilby et al 2002 meza 2005 katz et al 2002 wilks 2008 illustrated the capability of interpolating weather generator parameters to arbitrary locations e g on a grid using local weighted regressions wilks 2009 subsequently offered a method to synchronize gridded synthetic weather series on observed weather data approaches to producing weather sequences that deviate from climatology have included the implementation of seasonal correction factors perturbation of parameters or input data and spectral approaches caron et al 2008 kilsby et al 2007 hansen and mavromatis 2001 schoof et al 2005 qian et al 2010 nonparametric weather generators have an improved ability to capture nonlinearities between variables and sites included in this subclass are the k nearest neighbor k nn bootstrap resampling method brandsma and buishand 1998 rajagopalan and lall 1999 buishand and brandsma 2001 beersma and buishand 2003 yates et al 2003 sharif and burn 2007 and kernel density based estimators rajagopalan et al 1997 harrold et al 2003 mehrotra and sharma 2007 caraway et al 2014 first applied a clustering algorithm to identify regions of similar climatology before applying the k nn approach which has shown good performance in regions of complex terrain apipattanavis et al 2010 modified the k nn approach to create a semi parametric weather generator that better captures the duration of wet and dry spells via markov chain modeling modifications of the k nn based weather generator to incorporate seasonal precipitation forecasts apipattanavis et al 2010 and multi decadal projections podestá et al 2009 have also been proposed in these situations the resampling is weighted to reflect the projected distribution of regional climate conditions these methods are simple and powerful however their main drawback is that they cannot generate values outside the range of historical data more importantly it is not easy to generate weather sequences at locations other than those with historical observations pioneered by stern and coe 1984 generalized linear models glms are able to straightforwardly model non normal data through a suite of link functions relevant to this research glms can be used to model and simulate daily weather sequences and have paved the way for generating space time weather sequences at any desired location kleiber et al 2012 2013 furrer and katz 2007 kim et al 2012 yan et al 2002 yang et al 2005 chandler 2005 verdin et al 2015 recently verdin et al 2015 incorporated these developments into a robust space time weather generator and demonstrated its capability to generate realistic weather sequences at arbitrary locations in the pampas of argentina also the region targeted by this paper the glm framework offers several advantages mainly they reduce the effort in modeling non normal variables and are parsimonious mccullagh and nelder 1989 especially for discrete and skewed variables e g precipitation occurrence and intensity respectively coupled with spatial processes glms can generate sequences at any spatial resolution which is important for resource management furthermore covariates such as enso information seasonal climate forecasts and annual cycles can easily be incorporated in the glms to refine or narrow the distribution of expected values e g chandler and wheater 2002 wheater et al 2005 furrer and katz 2007 kim et al 2012 as motivated earlier in this section skillful and realistic sequences of daily weather in any given season are essential for efficient planning and management of agricultural resources one method of obtaining such sequences requires generating space time weather sequences that are consistent with and conditioned on coarse climate information from seasonal to decadal time scales to this end here we propose a modification to the stochastic weather generator presented in verdin et al 2015 to include the coarse scale information as covariates we refer to the weather generator of verdin et al 2015 as original that of this research will be called the modified weather generator the paper is organized as follows the study region and data are described in section 2 section 3 contains a brief summary of the modified methodology in section 4 we discuss the results and in section 5 we conclude with a summary of the research and future work 2 study region and data application of this methodology is focused on a network of seventeen weather stations located in and around the salado a basin of the pampas of argentina see fig 1 the salado is part of the large río de la plata basin herzer 2003 note the study region differs from that of verdin et al 2015 the a basin is an agriculturally productive sub basin within the salado river basin where maize soybean and wheat are grown the salado basin has very flat topography and a poorly developed and disintegrated drainage system the western basin salado a includes mega parabolic dunes separated by depressions that constrain evacuation of surface water aragón et al 2010 viglizzo et al 2009 1997 since colonial times the salado has shown alternating floods and droughts that displace populations and disrupt productive activities and livelihoods for extended periods floods were frequent during the late 19th and early 20th centuries a relatively wet epoch in contrast extensive droughts were more frequent during the drier 1930s 1950s herzer 2003 seager et al 2010 partly in response to rain increases since the 1970s severe floods have occurred in the salado basin in 1980 1991 93 and 2000 01 herzer 2003 floods in the western half of the pampas between 1997 and 2003 left 27 of the landscape under water halved grain production damaged infrastructure and soil quality and transformed the few remaining natural areas viglizzo et al 2009 in contrast an almost unprecedented drought in 2008 skansi et al 2009 decreased soybean and wheat production in the region by about 30 and 50 respectively we apply the proposed methodology on a sub basin scale to illustrate its ability in downscaling coarse seasonal multi decadal forecasts projections to local daily weather patterns while maintaining physically realistic climatic characteristics as agriculture in the pampas is entirely rainfed it is of interest to provide a robust risk assessment for crop yields in this region during the last half of the 20th century the study region experienced one of the most significant positive trends in annual precipitation amounts in the world giorgi 2002 this overall increase in precipitation partly contributed to immense agricultural expansion to the semi arid regions of the western pampas bert et al 2014 since the turn of the 21st century however observed conditions suggest a significant decrease in regional annual precipitation which begs the question are the existing agricultural production systems viable in a drier future analysis of a system s response to an ensemble of possible futures that exhibit significant fluctuations in annual precipitation on the multi decadal scale is of utmost importance for production risk analysis in climatically marginal regions such as the western pampas daily time series of precipitation minimum temperature and maximum temperature are available for a network of 17 weather stations from 1 january 1961 to near present in this research we use data up to 31 december 2013 this data was collected and organized by associates at the servicio meteorológico nacional national meteorological service of buenos aires argentina and extensive quality control was carried out to ensure its validity while there is a significant longitudinal gradient in precipitation and temperature 800mm year precipitation and 24 c maximum temperature in the west 1000mm year precipitation and 20 c maximum temperature in east the climatic tendencies e g trends are similar between all weather stations thus the a sub basin serves as an optimal test bed for this methodology 3 methodology 3 1 model structure we follow the model structure defined in verdin et al 2015 a summary of which is provided below in describing this methodology we also develop modifications to improve flexibility by producing conditional weather sequences driven by seasonal forecasts multi decadal projections climate drivers or variables or any other relevant information introduced as time series of covariates it should be noted that in equations 2 4 7 and 8 the ellipses denote any number of relevant covariates the user wishes to include such as seasonal characteristics e g mean temperature or total precipitation large scale climate modes e g el niño southern oscillation pacific decadal oscillation atlantic multidecadal oscillation or any other climatic variables here we propose to use seasonal spatial average precipitation and temperatures as covariates these additional covariates are calculated from the gauge data it is acknowledged that a possible scale mismatch exists between the domain average calculated from 17 stations and the true domain average however the network of stations is evenly spaced throughout the domain thus it is fair to assume the stations adequately represent the true domain average in the weather generator described here we define two explicit components of daily weather patterns local climate and daily variability as suggested by kleiber et al 2013 local climate represents the expected value of a given meteorological process largely due to seasonal cycle daily variability provides perturbations to local climate due to weather precipitation is considered the primary variable in that occurrence of precipitation tends to modify maximum and minimum temperatures on that day e g due to cloud cover and latent heat transfer minimum and maximum temperatures are therefore conditional on precipitation occurrence precipitation intensities are modeled and simulated independently from occurrence in this research precipitation occurrence and intensity e g amounts and minimum and maximum temperatures at location s r 2 for day t 1 t where t is the number of days in the observational record are denoted as o s t a s t zn s t and zx s t respectively as in verdin et al 2015 occurrence is modeled as a probit process driven by a latent gaussian process wo s t via 1 o s t l w o s t 0 if wo s t is positive this is indicative of rain on day t at location s and is assigned the value 1 if the latent gaussian process is negative or equal to zero day t at location s is dry and is assigned the value 0 the mean function of the latent gaussian process is simply a regression on covariates that are appropriate for the domain of interest similar to verdin et al 2015 this regression has covariates 2 x o s t 1 o s t 1 cos 2 π t 365 sin 2 π t 365 st t which are the intercept term the previous day s occurrence two harmonic terms to account for seasonality and the domain averaged seasonal total precipitation the key modification to this regression is the seasonal total covariate denoted by st t in practice this covariate is divided into four distinct covariates relating to each season covariates are set to zero for times not included in their respective season to maintain spatial correlations of precipitation occurrence in the domain an explicit correlation function is defined for wo s t a correlation function is used instead of a covariance function because probit regression has variance unity by definition precipitation amounts at any individual location are modeled as a gamma random variable as in kleiber et al 2012 as follows 3 a s t g s t 1 φ w a s t where g s t 1 is the quantile function e g inverse cumulative distribution function of the gamma distribution at location s and day t and φ is the cumulative distribution function of a standard normal the simulated rainfall values maintain spatial correlation by applying a spatially varying copula function to the zero mean gaussian process wa s t with correlation function ca h t chilés and delfiner 1999 the shape parameter varies with space such that each location has its own distinct value the scale parameter varies with both space and time its time dependence is based on the seasonal characteristics of precipitation which are generally captured by annual harmonics similar to the occurrence process the gamma model parameters are informed by a set of covariates including the areal seasonal total precipitation covariates as in the occurrence model as follows 4 x a s t 1 cos 2 π t 365 sin 2 π t 365 st t following verdin et al 2015 the minimum and maximum temperatures zn s t and zx s t respectively at location s and day t are decomposed as follows 5 z n s t β n s x n s t w n s t 6 z x s t β x s x x s t w x s t in each equation the product on the right side of the equality is a regression on some covariates xn s t and xx s t for minimum and maximum temperatures respectively these products represent the average behavior of temperatures over the observational period in this the key modification to the weather generator of verdin et al 2015 is the inclusion of areal seasonal mean minimum smn t and maximum smx t temperature covariates which are included in both temperature models as follows 7 x n s t 1 cos 2 π t 365 sin 2 π t 365 r t z n s t 1 z x s t 1 o s t smn t smx t 8 x x s t 1 cos 2 π t 365 sin 2 π t 365 r t z n s t 1 z x s t 1 o s t smn t smx t which are the intercept term two harmonic terms to account for seasonality r t which is a linear drift ranging from 1 to 1 to account for temperature trends over the observational period the previous day s minimum and maximum temperatures the current day s precipitation occurrence and the seasonal mean minimum and mean maximum temperatures respectively daily variability is denoted as wn s t and wx s t for minimum and maximum temperatures respectively and maintains spatial correlation by realizations from a mean zero gaussian process with an empirical covariance structure defined by the residuals of the local regressions kleiber et al 2013 found that the gaussian assumption for temperature models was appropriate the above are glms and are fitted hierarchically we refer the reader to verdin et al 2015 and kleiber et al 2012 2013 for details on implementation it should be noted that the additional covariates are applied only to the local climate component and not the daily weather component the daily weather component is by definition random temporally independent noise see kleiber et al 2012 for validation of this assumption thus the daily weather component is not conditional on the additional covariates rather it is conditioned only by the calendar date there are distinct correlation and covariance matrices for each month 3 2 significance testing the inclusion of seasonal covariates could lead to a reduction in the significance of the harmonic covariates for all 17 stations the seasonal covariates are highly significant indicated by the respective p values of their regression coefficients for many of the stations both cosine and sine covariates remain highly significant however at few stations the sine covariate loses significance the akaike information criterion aic of the modified models at each location for each climate variable are consistently lower than those for the original models that do not contain the seasonal covariates implying that the modified weather generator more adequately describes the modeled processes table 1 reports the change in aic value original minus modified models positivity implies a decrease for all 17 stations for the four variables that make up the weather generators 4 results from application in the salado a basin 4 1 covariate selection we apply the methodology as described in the previous section to the network of 17 stations in and around the salado a basin of the argentine pampas see fig 1 given the relative homogeneity of the basin area and scale at which seasonal climate forecasts are available we propose three domain averaged covariates seasonal total precipitation seasonal mean minimum temperature and seasonal mean maximum temperature the growing season for summer crops in the salado a basin begins in october with harvest coming in late march or april therefore we focus on the ond season we define the seasons as january march jfm april june amj july september jas and october december ond the first principal component of ond seasonal total precipitation at each of the 17 stations explains 47 of the total variance those of ond seasonal average minimum and maximum temperatures explain 71 and 77 of the total variance respectively the magnitudes of these first principal components are nearly constant across space which further justifies the use of domain averaged information fig 2 shows the first principal component of the three variables along with the domain averaged time series the behavior of which are well described by their first principal components thus the four glms as described in the previous section were fitted with the additional covariates described above these covariates were found to be highly significant at all the locations e g regression assigns all additional covariates p values 0 001 other seasons show similar results not shown 4 2 validation to assess the efficacy of the additional covariates we employ both the original and modified weather generators in spatial and temporal validations described in the following subsections 4 2 1 spatial validation to assess the spatial performance of the modified weather generator three stations were withheld from the model fitting process these withheld stations are identified in fig 1 spatial process models were used to estimate the model parameters at the withheld locations and 100 realizations over the 53 year observational period were produced using the estimated parameters fig 3 shows the relationship between the observed and ensemble mean ond probability of occurrence seasonal total rainfall mean maximum temperature and mean minimum temperature for each of the three stations as produced by the original top row and modified bottom row weather generators simulations from the original weather generator show no relationship with the observations this is to be expected as only harmonic and autoregressive covariates are considered conversely simulations from the modified generator capture the observations strongly due to the inclusion of the domain averaged seasonal covariates similar results were seen for other seasons figures not shown 4 2 2 temporal validation it is also worthwhile to investigate the temporal performance of the weather generator to validate its use for seasonal forecasts multidecadal projections and climate change scenarios to this end we fitted the original and modified weather generators on historic data for the calibration period 1 january 1961 31 december 2000 then 100 realizations were generated for the validation period 1 january 2001 31 december 2013 fig 4 shows the difference between observed and ensemble mean simulated domain averaged seasonal total precipitation mean maximum temperature and mean minimum temperature a perfect fit would show a horizontal line of ordinate zero root mean square error rmse is calculated between simulated and observed seasonal values for all 100 realizations the rmse is greatly reduced by including the domain averaged seasonal covariates in the validation period for seasonal total precipitation the rmse is reduced from 77 2 4 mm to 21 3 6 mm for seasonal mean maximum temperature the rmse is reduced from 1 05 0 05 c to 0 37 0 05 c and for seasonal mean minimum temperature the rmse is reduced from 0 99 0 04 c to 0 37 0 04 c 4 3 seasonal forecasts often times seasonal climate forecasts are provided as probabilities of precipitation and temperature being within different ranges e g terciles for a large region this is a common format for presenting uncertain climate information among other agencies around the world the international research institute for climate and society iri www iri columbia edu provides seasonal three month probabilistic forecasts with one to four months lead time the iri presents these forecasts in terms of a n b probabilities where a is above normal n is near normal and b is below normal the three categories span an equal range and are defined with respect to climatological terciles e g 33rd and 67th percentiles for example a 15 35 50 precipitation forecast implies there is a 15 chance of experiencing above normal conditions a 35 chance of experiencing near normal conditions and a 50 chance of experiencing below normal precipitation in the upcoming season agricultural decisions in the salado a basin are typically made before the beginning of the summer growing season 1 october every year thus we focus on ond seasonal forecasts the ond season is also a critical period in terms of crop yield generation and has shown tendencies towards skillful climate predictions in part due to significant enso signals grimm et al 1998 2000 montecinos et al 2000 ropelewski and halpert 1987 barros and silvestri 2002 boulanger et al 2005 ropelewski and bell 2008 grimm 2011 barreiro 2010 we select iri forecasts for ond 2010 a dry and hot forecast e g 15 35 50 for precipitation 40 35 25 for temperature and ond 2012 a wet and hot forecast e g 40 35 25 for both precipitation and temperature as case studies for this methodology issued on 1 september 2010 and 1 september 2012 respectively to generate space time weather sequences for the two ond seasons from the modified generator ensembles of domain averaged seasonal precipitation and temperature are needed to use as covariates to this end 100 observed ond domain averaged values of precipitation and maximum and minimum temperature are sampled with replacement this is accomplished by first categorizing the observed domain averaged seasonal weather as above near or below normal based on the empirical terciles then assigning the categorical forecasts as probabilities e g 15 35 50 and 40 35 25 for precipitation and temperature respectively to the values in each category and resampling with these assigned weights as the probability metric for instance there is a 15 chance of sampling an above normal precipitation value there are 35 and 50 chances of sampling near normal and below normal precipitation values respectively the result of this resampling scheme is 100 values that are used as covariates to drive the modified weather generator 100 separate times the output of these 100 independent runs is essentially a downscaled ensemble of weather patterns that exhibit the traits of the seasonal forecasts the top row of fig 5 shows the probability density functions pdfs of domain averaged ond precipitation and temperatures from the original and modified weather generators the pdf of ond climatology and the observed values of ond 2010 and 2012 the precipitation pdfs from the modified generator have shifted towards the observed values in both 2010 and 2012 this shift is indicative not only of forecast skill but also the effectiveness of the modified generator in simulating scenarios representative of the forecasts mean maximum minimum temperature during ond 2010 2012 was observed to be above normal and the pdf from the modified generator gives greater probability to above average temperatures than that of the original generator ond 2012 2010 experienced near normal maximum minimum temperatures so the original generator gives highest probability to observed conditions however the range of possible scenarios offered by the original generator is limited and will give near zero probability to above and below normal conditions which for planning purposes can be misleading the domain averaged seasonal totals of precipitation and seasonal averages of temperatures that are generated from the two weather generators are plotted with the observed in the bottom row of fig 5 table 2 reports p values from kolmogorov smirnov tests comparing the distributions of original and modified generator output the differences between the original and modified distributions for ond 2010 weather scenarios and ond 2012 precipitation are significant at the 95 level maximum and minimum temperature scenarios for ond 2012 are not significantly different indicating the covariate values sampled from the iri probabilistic forecast thus the scenarios produced by the modified weather generator do not deviate significantly from climatology weather simulations on a regular grid are of particular interest as they are used to drive hydrologic and agriculture models for agricultural planning to mitigate crop failure to simulate daily weather on a grid the β coefficients for each covariate of the weather generator models are estimated in space from their respective spatial models to the desired spatial resolution 5km 5km these gridded coefficients are then used to obtain the mean function and the daily weather processes are simulated via mean zero gaussian random fields fig 6 shows the difference between the ensemble mean of gridded seasonal total precipitation mean maximum temperature and mean minimum temperature for ond 2010 from the original and modified weather generator the modified generator simulates a drier and hotter domain than the original generator which is consistent with the seasonal forecast notably the modified weather generator simulates a cooling for mean minimum temperature in the southern part of the basin which is inconsistent with the seasonal forecast the differences between the 95 ensemble spread 97 5th percentile minus 2 5th percentile produced by the original and modified weather generators are shown in fig 7 as can be seen the ensemble spread difference for seasonal total precipitation is mostly red and yellow while those for mean maximum and minimum temperature are mostly blue and yellow which illustrates that the modified generator produced wider ensemble spreads than the original generator the uncertainty in the probabilistic seasonal climate forecast is propagated to the modified weather generator resulting in a wider distribution than that of the original generator similar findings can be seen for ond 2012 figures not shown 4 4 multi decadal projections multi decadal projections are useful in a number of applications including environmental impact studies agricultural decision making and water resources management to name a few in agriculture multi decadal projections help in making informed investment decisions e g whether or not to buy a farm in a climatically marginal area invest in irrigation etc specifically the climate of the pampas has shown significant decadal variability and since the 1970s has exhibited a steady increase in both annual and extreme precipitation this trend in precipitation has in part promoted significant expansion of agricultural area to climatically marginal regions of the pampas given the uncertainty of future climate coupled with a known decadal variability it is unclear if existing agricultural systems may remain viable if climate reverts to a drier epoch specific to this research future climate scenarios can be used to drive hydrologic and crop simulation models thus providing an assessment of the viability of existing agricultural production systems in climatically marginal regions of the salado a basin however future climate projections from climate models are generally of coarse spatial e g on a grid and temporal e g monthly resolutions and therefore cannot provide reliable projections of weather at the local scale these monthly and consequently seasonal projections can be incorporated into the modified generator and thus enable the generation of daily weather sequences conditioned on the projections at any desired location both monitored and unmonitored in the study region to this end we explored the ability of the modified generator to downscale medium term projections in the salado a basin a regional climate model projection experiment rcp8 5 was obtained for the period 1 january 2015 to 31 december 2050 a 36 year projection produced using the cordex cmip5 regional climate model ec earth consortium 2014 and bias corrected mcginnis et al 2015 using the claris lpb dataset penalba et al 2014 this projection focuses on south america and is gridded at 0 44 no notable long term trends in annual precipitation totals are projected but the magnitudes are significantly lower than seen in the historic record both maximum and minimum annual average temperatures show positive trends and are projected to increase by approximately 1 c by the year 2050 seasonal values of areal precipitation and temperature for the salado a basin were computed to use as covariates to drive the modified weather generator only the grids that cover the salado a basin and the 17 station data are considered when computing domain averaged precipitation totals and temperature means 100 realizations of daily weather sequences were simulated using both the original and modified weather generators fig 8 shows seasonal residuals projected minus simulated of the ensemble mean of the original and modified weather generator simulations for the 36 year projection period as was seen previously the original generator shows much larger and more variable residuals than does the modified generator consistent with the temporal validation rmse is greatly reduced by including the domain averaged seasonal covariates in the models rmse for seasonal total precipitation is reduced from 90mm to 14mm rmse for seasonal mean maximum temperature is reduced from 1 09 c to 0 48 c rmse for seasonal mean minimum temperature is reduced from 1 43 c to 0 23 c there is a slight warm bias in seasonal mean maximum temperature simulated by the modified generator as compared to that projected by the rcm to illustrate the spatial ability of the weather generators we simulated 100 realizations of daily weather sequences for the period 2015 2050 conditioned on the projected seasonal characteristics fig 9 shows the difference in ensemble mean ond total precipitation maximum temperature and minimum temperature as simulated by the original and modified generators consistent with the climate model trends the modified generator simulates a drier and hotter future across the domain 5 summary and future work we have proposed and validated the use of a parametric stochastic weather generator in a nonstationary context such as climate change impact studies with application in the salado a basin of the argentine pampas this region was selected due to its status as one of the most productive agricultural regions in south america and its strong climatic variability that is experienced at multiple time scales agriculture in the pampas is predominantly rainfed thus high quality seasonal forecast information could greatly impact the outcome e g crop yield risk of failure of a growing season the modified weather generator presented in this research has flexibility in its glm framework such that any number of covariates can be included in the model fit effectively conditioning the weather generator to produce downscaled weather sequences for example in this research we used areal average seasonal total precipitation and mean minimum and maximum temperatures as additional covariates which were shown to be highly significant in the model fit the use of areal averages was justified via principal component analysis for non homogeneous or mountainous regions consider site specific averages or a clustering algorithm the coarse information provided by these additional covariates successfully trickled from seasonal regional down to daily local scales such that wet dry days are more prevalent during seasons with above normal below normal seasonal total precipitation it is with the conditioned output of the weather generator that research teams may provide a more robust estimate of production risk for a region by running the daily weather sequences through process based e g crop simulation hydrologic models the output of process based models may be interpreted and provided to a farmer or decision maker who then will have seasonal forecast information that is relevant to the decisions they must make e g probability of not meeting a crop yield goal where and when to plant a certain crop as opposed to spatially coarse probabilistic statements as are typically reported similarly multidecadal projection information can be used to generate conditional weather sequences to assist in assessing the viability of existing agricultural infrastructure in climatically marginal regions in this a regional climatic trend may be extracted and used to produce conditional weather sequences which may be used to drive any relevant process based models the output of the modified weather generator presented in this manuscript has been validated by direct comparison to the original weather generator of verdin et al 2015 it has been shown that using simple covariates such as domain averaged seasonal total mean precipitation temperatures improves the skill of the generator in producing daily weather sequences that exhibit the traits and trends of a seasonal forecast or multi decadal projection in representing domain averaged behavior for the validation period 2001 2013 this modification to the weather generator reduced rmse values from 77mm to 21mm for precipitation 1 05 c to 0 37 c for maximum temperature and 0 99 c to 0 37 c for minimum temperature similarly the modified generator faithfully reproduced the trends and variability of historic precipitation and temperature at individual sites while the original generator replicates the expected behavior of e g climatology of each season with little to no interannual variability in generating sequences consistent with a seasonal forecast the kolmogorov smirnov tests suggest the output from original and modified weather generators exhibit significantly different traits with 95 confidence unless the seasonal forecast is similar to climatology the modified weather generator was shown to produce pdfs that better represent the range of possible futures while the pdfs from the original weather generator give near zero probability to the upper and lower terciles e g wet hot and dry cold conditions on the multi decadal scale the modified weather generator is flexible in its ability to capture the considerable interannual and decadal variability prevalent in the projected precipitation totals as well as the increase in both minimum and maximum temperatures application of this methodology to other areas is called for however careful attention must be paid to the spatial and temporal climatic variability in the region of interest local climate regional teleconnections and global climate drivers should be identified for optimal skill in downscaling seasonal forecasts and multi decadal projections principal component analysis on seasonal attributes such as seasonal total precipitation and mean temperatures can help decide if domain averaged clustered or site specific covariates should be considered however the model setup as defined in verdin et al 2015 should be considered a baseline model for use in any basin the additional covariates as described in this manuscript need be fine tuned to successfully generate skillful weather scenarios one shortcoming to the weather generator presented in this research is that it uses only precipitation and temperature covariates to condition the weather generator there has been great progress in the identification of teleconnections and climate drivers for regions around the world the pampas are no exception while it was mentioned in this manuscript that such teleconnections could be used as covariates to condition the weather generator output this approach was not investigated a second shortcoming to this methodology is that the uncertainties associated with the parameters of the weather generator are not propagated to the simulations as the maximum likelihood estimates of the parameters are kept fixed as a result the variability of simulations can be underestimated bayesian methods that explicitly quantify the parameter uncertainties are attractive options the methodology of this weather generator is inherently hierarchical thus considering the use of a bayesian hierarchical framework is a natural extension to this problem the authors are currently exploring this approach in a bayesian context the parameters are treated as random variables and are sampled from appropriate distributions typically via markov chain monte carlo based on likelihood acceptance criteria which results in posterior distributions of all model parameters these posterior distributions better represent the uncertainty involved in traditional parameter estimation techniques and when used in a weather generation framework will provide a more realistic range of uncertainty in synthetic weather sequences acknowledgments this research is funded by national science foundation award 1211613 the authors would like to thank the met service of argentina for providing the weather data the authors are grateful for drs linda means and seth mcginnis of the national center for atmospheric research ncar for providing the cmip5 regional climate model projections this work utilized the janus supercomputer which is supported by the national science foundation award number cns 0821794 and the university of colorado boulder the janus supercomputer is a joint effort of the university of colorado boulder the university of colorado denver and the national center for atmospheric research we thank the two anonymous reviewers for their insightful comments which significantly improved the manuscript 
7591,we present the application of a parametric stochastic weather generator within a nonstationary context enabling simulations of weather sequences conditioned on interannual and multi decadal trends the generalized linear model framework of the weather generator allows any number of covariates to be included such as large scale climate indices local climate information seasonal precipitation and temperature among others here we focus on the salado a basin of the argentine pampas as a case study but the methodology is portable to any region we include domain averaged e g areal seasonal total precipitation and mean maximum and minimum temperatures as covariates for conditional simulation areal covariates are motivated by a principal component analysis that indicates the seasonal spatial average is the dominant mode of variability across the domain we find this modification to be effective in capturing the nonstationarity prevalent in interseasonal precipitation and temperature data we further illustrate the ability of this weather generator to act as a spatiotemporal downscaler of seasonal forecasts and multidecadal projections both of which are generally of coarse resolution keywords generalized linear models stochastic weather generator conditional simulation downscaling seasonal forecasts daily precipitation daily temperature 1 introduction scientific and technological advances together with awareness of the importance of climate on human endeavors are creating increased worldwide demand for climate information fortunately our ability to monitor and predict variations in climate has increased substantially barnston et al 2010 stockdale et al 2010 a number of groups now forecast climate conditions a few seasons ahead goddard et al 2003 saha et al 2006 emerging developments may enable climate projections 10 20years into the future a scale intermediate between seasonal forecasts and manmade climate change projections haines et al 2009 hurrell et al 2009 meehl et al 2009 these advances however must be matched by a better understanding of how science can inform climate resilient planning and development stainforth et al 2007 to support public and private adaptation and mitigation responses climate information must be credible legitimate and especially salient e g relevant to the needs of decision makers cash et al 2003 needs include not only predictions or projections 1 following bray and von storch 2009 prediction conveys a sense of certainty whereas projection is associated more with the possibility of something happening given a certain set of plausible but not necessarily probable circumstances a prediction can be used to design specific response strategies while a projection or more precisely a series of projections provides a range on which to consider a range of response strategies 1 bray and von storch 2009 of regional climate potential outcomes of adaptation actions are probably more relevant to stakeholders than raw climate information thus an enhanced capacity is needed to translate climate information into distributions of outcomes for risk assessment and management hansen et al 2006 process models e g crop biophysical models hydrological models can be useful tools to assess likely impacts on climate sensitive sectors of society and to evaluate the outcomes of alternative adaptive actions ferreyra et al 2001 berger 2001 berger et al 2006 happe et al 2008 freeman et al 2009 schreinemachers and berger 2011 bert et al 2006 2007 2014 these models however typically require daily weather data although historical daily weather can be used getting long term daily weather is laborious and costly at best and in some cases impossible typically historical observations have missing data that are not accepted by impact models similarly point measurements may not represent the true spatial variability of a nonstationary natural process e g daily precipitation most importantly observed sequences provide a solution based on only one realization of the weather process richardson 1981 the use of seasonal forecasts of regional climate and its impacts can help decision makers to lessen the adverse effects of unfavorable conditions or alternatively to capitalize on favorable conditions nevertheless a major obstacle to broader use of seasonal climate forecasts is their coarse spatial and temporal resolution similarly 10 20year projections of regional climate conditions have been identified as important to infrastructure planners water resource managers and many others hurrell et al 2009 unfortunately projections of regional monthly precipitation and temperature from climate models not only are coarse in space and time as seasonal forecasts but also involve considerable uncertainty which requires exploration of the impacts of alternative plausible trajectories stochastic weather generators have long been used for risk assessment and adaptation as they can provide a rich variety of plausible climatic scenarios moreover weather generators can produce spatially consistent series that can be used to downscale larger scale scenarios traditional weather generators stemming from richardson 1981 model precipitation occurrence as a chain dependent process katz 1977 and thus are capable of generating physically realistic prolonged wet and dry spells the remaining weather variables e g precipitation intensity and temperature are parameterized using probability distributions for precipitation intensity and linear time series models for temperature which capture historical climatological variability and linear relationships between variables but fail to capture extremes e g extreme drought or flooding in order to capture the variability of weather attributes in any specific season the simulations need to be conditioned on appropriate covariates one approach is to estimate the parameters of the generator conditionally by considering enso el niño southern oscillation trenberth and stepaniak 2001 phase or any other teleconnection to a region s climate which enables simulation of skillful sequences grondona et al 2000 ferreyra et al 2001 wilby et al 2002 meza 2005 katz et al 2002 wilks 2008 illustrated the capability of interpolating weather generator parameters to arbitrary locations e g on a grid using local weighted regressions wilks 2009 subsequently offered a method to synchronize gridded synthetic weather series on observed weather data approaches to producing weather sequences that deviate from climatology have included the implementation of seasonal correction factors perturbation of parameters or input data and spectral approaches caron et al 2008 kilsby et al 2007 hansen and mavromatis 2001 schoof et al 2005 qian et al 2010 nonparametric weather generators have an improved ability to capture nonlinearities between variables and sites included in this subclass are the k nearest neighbor k nn bootstrap resampling method brandsma and buishand 1998 rajagopalan and lall 1999 buishand and brandsma 2001 beersma and buishand 2003 yates et al 2003 sharif and burn 2007 and kernel density based estimators rajagopalan et al 1997 harrold et al 2003 mehrotra and sharma 2007 caraway et al 2014 first applied a clustering algorithm to identify regions of similar climatology before applying the k nn approach which has shown good performance in regions of complex terrain apipattanavis et al 2010 modified the k nn approach to create a semi parametric weather generator that better captures the duration of wet and dry spells via markov chain modeling modifications of the k nn based weather generator to incorporate seasonal precipitation forecasts apipattanavis et al 2010 and multi decadal projections podestá et al 2009 have also been proposed in these situations the resampling is weighted to reflect the projected distribution of regional climate conditions these methods are simple and powerful however their main drawback is that they cannot generate values outside the range of historical data more importantly it is not easy to generate weather sequences at locations other than those with historical observations pioneered by stern and coe 1984 generalized linear models glms are able to straightforwardly model non normal data through a suite of link functions relevant to this research glms can be used to model and simulate daily weather sequences and have paved the way for generating space time weather sequences at any desired location kleiber et al 2012 2013 furrer and katz 2007 kim et al 2012 yan et al 2002 yang et al 2005 chandler 2005 verdin et al 2015 recently verdin et al 2015 incorporated these developments into a robust space time weather generator and demonstrated its capability to generate realistic weather sequences at arbitrary locations in the pampas of argentina also the region targeted by this paper the glm framework offers several advantages mainly they reduce the effort in modeling non normal variables and are parsimonious mccullagh and nelder 1989 especially for discrete and skewed variables e g precipitation occurrence and intensity respectively coupled with spatial processes glms can generate sequences at any spatial resolution which is important for resource management furthermore covariates such as enso information seasonal climate forecasts and annual cycles can easily be incorporated in the glms to refine or narrow the distribution of expected values e g chandler and wheater 2002 wheater et al 2005 furrer and katz 2007 kim et al 2012 as motivated earlier in this section skillful and realistic sequences of daily weather in any given season are essential for efficient planning and management of agricultural resources one method of obtaining such sequences requires generating space time weather sequences that are consistent with and conditioned on coarse climate information from seasonal to decadal time scales to this end here we propose a modification to the stochastic weather generator presented in verdin et al 2015 to include the coarse scale information as covariates we refer to the weather generator of verdin et al 2015 as original that of this research will be called the modified weather generator the paper is organized as follows the study region and data are described in section 2 section 3 contains a brief summary of the modified methodology in section 4 we discuss the results and in section 5 we conclude with a summary of the research and future work 2 study region and data application of this methodology is focused on a network of seventeen weather stations located in and around the salado a basin of the pampas of argentina see fig 1 the salado is part of the large río de la plata basin herzer 2003 note the study region differs from that of verdin et al 2015 the a basin is an agriculturally productive sub basin within the salado river basin where maize soybean and wheat are grown the salado basin has very flat topography and a poorly developed and disintegrated drainage system the western basin salado a includes mega parabolic dunes separated by depressions that constrain evacuation of surface water aragón et al 2010 viglizzo et al 2009 1997 since colonial times the salado has shown alternating floods and droughts that displace populations and disrupt productive activities and livelihoods for extended periods floods were frequent during the late 19th and early 20th centuries a relatively wet epoch in contrast extensive droughts were more frequent during the drier 1930s 1950s herzer 2003 seager et al 2010 partly in response to rain increases since the 1970s severe floods have occurred in the salado basin in 1980 1991 93 and 2000 01 herzer 2003 floods in the western half of the pampas between 1997 and 2003 left 27 of the landscape under water halved grain production damaged infrastructure and soil quality and transformed the few remaining natural areas viglizzo et al 2009 in contrast an almost unprecedented drought in 2008 skansi et al 2009 decreased soybean and wheat production in the region by about 30 and 50 respectively we apply the proposed methodology on a sub basin scale to illustrate its ability in downscaling coarse seasonal multi decadal forecasts projections to local daily weather patterns while maintaining physically realistic climatic characteristics as agriculture in the pampas is entirely rainfed it is of interest to provide a robust risk assessment for crop yields in this region during the last half of the 20th century the study region experienced one of the most significant positive trends in annual precipitation amounts in the world giorgi 2002 this overall increase in precipitation partly contributed to immense agricultural expansion to the semi arid regions of the western pampas bert et al 2014 since the turn of the 21st century however observed conditions suggest a significant decrease in regional annual precipitation which begs the question are the existing agricultural production systems viable in a drier future analysis of a system s response to an ensemble of possible futures that exhibit significant fluctuations in annual precipitation on the multi decadal scale is of utmost importance for production risk analysis in climatically marginal regions such as the western pampas daily time series of precipitation minimum temperature and maximum temperature are available for a network of 17 weather stations from 1 january 1961 to near present in this research we use data up to 31 december 2013 this data was collected and organized by associates at the servicio meteorológico nacional national meteorological service of buenos aires argentina and extensive quality control was carried out to ensure its validity while there is a significant longitudinal gradient in precipitation and temperature 800mm year precipitation and 24 c maximum temperature in the west 1000mm year precipitation and 20 c maximum temperature in east the climatic tendencies e g trends are similar between all weather stations thus the a sub basin serves as an optimal test bed for this methodology 3 methodology 3 1 model structure we follow the model structure defined in verdin et al 2015 a summary of which is provided below in describing this methodology we also develop modifications to improve flexibility by producing conditional weather sequences driven by seasonal forecasts multi decadal projections climate drivers or variables or any other relevant information introduced as time series of covariates it should be noted that in equations 2 4 7 and 8 the ellipses denote any number of relevant covariates the user wishes to include such as seasonal characteristics e g mean temperature or total precipitation large scale climate modes e g el niño southern oscillation pacific decadal oscillation atlantic multidecadal oscillation or any other climatic variables here we propose to use seasonal spatial average precipitation and temperatures as covariates these additional covariates are calculated from the gauge data it is acknowledged that a possible scale mismatch exists between the domain average calculated from 17 stations and the true domain average however the network of stations is evenly spaced throughout the domain thus it is fair to assume the stations adequately represent the true domain average in the weather generator described here we define two explicit components of daily weather patterns local climate and daily variability as suggested by kleiber et al 2013 local climate represents the expected value of a given meteorological process largely due to seasonal cycle daily variability provides perturbations to local climate due to weather precipitation is considered the primary variable in that occurrence of precipitation tends to modify maximum and minimum temperatures on that day e g due to cloud cover and latent heat transfer minimum and maximum temperatures are therefore conditional on precipitation occurrence precipitation intensities are modeled and simulated independently from occurrence in this research precipitation occurrence and intensity e g amounts and minimum and maximum temperatures at location s r 2 for day t 1 t where t is the number of days in the observational record are denoted as o s t a s t zn s t and zx s t respectively as in verdin et al 2015 occurrence is modeled as a probit process driven by a latent gaussian process wo s t via 1 o s t l w o s t 0 if wo s t is positive this is indicative of rain on day t at location s and is assigned the value 1 if the latent gaussian process is negative or equal to zero day t at location s is dry and is assigned the value 0 the mean function of the latent gaussian process is simply a regression on covariates that are appropriate for the domain of interest similar to verdin et al 2015 this regression has covariates 2 x o s t 1 o s t 1 cos 2 π t 365 sin 2 π t 365 st t which are the intercept term the previous day s occurrence two harmonic terms to account for seasonality and the domain averaged seasonal total precipitation the key modification to this regression is the seasonal total covariate denoted by st t in practice this covariate is divided into four distinct covariates relating to each season covariates are set to zero for times not included in their respective season to maintain spatial correlations of precipitation occurrence in the domain an explicit correlation function is defined for wo s t a correlation function is used instead of a covariance function because probit regression has variance unity by definition precipitation amounts at any individual location are modeled as a gamma random variable as in kleiber et al 2012 as follows 3 a s t g s t 1 φ w a s t where g s t 1 is the quantile function e g inverse cumulative distribution function of the gamma distribution at location s and day t and φ is the cumulative distribution function of a standard normal the simulated rainfall values maintain spatial correlation by applying a spatially varying copula function to the zero mean gaussian process wa s t with correlation function ca h t chilés and delfiner 1999 the shape parameter varies with space such that each location has its own distinct value the scale parameter varies with both space and time its time dependence is based on the seasonal characteristics of precipitation which are generally captured by annual harmonics similar to the occurrence process the gamma model parameters are informed by a set of covariates including the areal seasonal total precipitation covariates as in the occurrence model as follows 4 x a s t 1 cos 2 π t 365 sin 2 π t 365 st t following verdin et al 2015 the minimum and maximum temperatures zn s t and zx s t respectively at location s and day t are decomposed as follows 5 z n s t β n s x n s t w n s t 6 z x s t β x s x x s t w x s t in each equation the product on the right side of the equality is a regression on some covariates xn s t and xx s t for minimum and maximum temperatures respectively these products represent the average behavior of temperatures over the observational period in this the key modification to the weather generator of verdin et al 2015 is the inclusion of areal seasonal mean minimum smn t and maximum smx t temperature covariates which are included in both temperature models as follows 7 x n s t 1 cos 2 π t 365 sin 2 π t 365 r t z n s t 1 z x s t 1 o s t smn t smx t 8 x x s t 1 cos 2 π t 365 sin 2 π t 365 r t z n s t 1 z x s t 1 o s t smn t smx t which are the intercept term two harmonic terms to account for seasonality r t which is a linear drift ranging from 1 to 1 to account for temperature trends over the observational period the previous day s minimum and maximum temperatures the current day s precipitation occurrence and the seasonal mean minimum and mean maximum temperatures respectively daily variability is denoted as wn s t and wx s t for minimum and maximum temperatures respectively and maintains spatial correlation by realizations from a mean zero gaussian process with an empirical covariance structure defined by the residuals of the local regressions kleiber et al 2013 found that the gaussian assumption for temperature models was appropriate the above are glms and are fitted hierarchically we refer the reader to verdin et al 2015 and kleiber et al 2012 2013 for details on implementation it should be noted that the additional covariates are applied only to the local climate component and not the daily weather component the daily weather component is by definition random temporally independent noise see kleiber et al 2012 for validation of this assumption thus the daily weather component is not conditional on the additional covariates rather it is conditioned only by the calendar date there are distinct correlation and covariance matrices for each month 3 2 significance testing the inclusion of seasonal covariates could lead to a reduction in the significance of the harmonic covariates for all 17 stations the seasonal covariates are highly significant indicated by the respective p values of their regression coefficients for many of the stations both cosine and sine covariates remain highly significant however at few stations the sine covariate loses significance the akaike information criterion aic of the modified models at each location for each climate variable are consistently lower than those for the original models that do not contain the seasonal covariates implying that the modified weather generator more adequately describes the modeled processes table 1 reports the change in aic value original minus modified models positivity implies a decrease for all 17 stations for the four variables that make up the weather generators 4 results from application in the salado a basin 4 1 covariate selection we apply the methodology as described in the previous section to the network of 17 stations in and around the salado a basin of the argentine pampas see fig 1 given the relative homogeneity of the basin area and scale at which seasonal climate forecasts are available we propose three domain averaged covariates seasonal total precipitation seasonal mean minimum temperature and seasonal mean maximum temperature the growing season for summer crops in the salado a basin begins in october with harvest coming in late march or april therefore we focus on the ond season we define the seasons as january march jfm april june amj july september jas and october december ond the first principal component of ond seasonal total precipitation at each of the 17 stations explains 47 of the total variance those of ond seasonal average minimum and maximum temperatures explain 71 and 77 of the total variance respectively the magnitudes of these first principal components are nearly constant across space which further justifies the use of domain averaged information fig 2 shows the first principal component of the three variables along with the domain averaged time series the behavior of which are well described by their first principal components thus the four glms as described in the previous section were fitted with the additional covariates described above these covariates were found to be highly significant at all the locations e g regression assigns all additional covariates p values 0 001 other seasons show similar results not shown 4 2 validation to assess the efficacy of the additional covariates we employ both the original and modified weather generators in spatial and temporal validations described in the following subsections 4 2 1 spatial validation to assess the spatial performance of the modified weather generator three stations were withheld from the model fitting process these withheld stations are identified in fig 1 spatial process models were used to estimate the model parameters at the withheld locations and 100 realizations over the 53 year observational period were produced using the estimated parameters fig 3 shows the relationship between the observed and ensemble mean ond probability of occurrence seasonal total rainfall mean maximum temperature and mean minimum temperature for each of the three stations as produced by the original top row and modified bottom row weather generators simulations from the original weather generator show no relationship with the observations this is to be expected as only harmonic and autoregressive covariates are considered conversely simulations from the modified generator capture the observations strongly due to the inclusion of the domain averaged seasonal covariates similar results were seen for other seasons figures not shown 4 2 2 temporal validation it is also worthwhile to investigate the temporal performance of the weather generator to validate its use for seasonal forecasts multidecadal projections and climate change scenarios to this end we fitted the original and modified weather generators on historic data for the calibration period 1 january 1961 31 december 2000 then 100 realizations were generated for the validation period 1 january 2001 31 december 2013 fig 4 shows the difference between observed and ensemble mean simulated domain averaged seasonal total precipitation mean maximum temperature and mean minimum temperature a perfect fit would show a horizontal line of ordinate zero root mean square error rmse is calculated between simulated and observed seasonal values for all 100 realizations the rmse is greatly reduced by including the domain averaged seasonal covariates in the validation period for seasonal total precipitation the rmse is reduced from 77 2 4 mm to 21 3 6 mm for seasonal mean maximum temperature the rmse is reduced from 1 05 0 05 c to 0 37 0 05 c and for seasonal mean minimum temperature the rmse is reduced from 0 99 0 04 c to 0 37 0 04 c 4 3 seasonal forecasts often times seasonal climate forecasts are provided as probabilities of precipitation and temperature being within different ranges e g terciles for a large region this is a common format for presenting uncertain climate information among other agencies around the world the international research institute for climate and society iri www iri columbia edu provides seasonal three month probabilistic forecasts with one to four months lead time the iri presents these forecasts in terms of a n b probabilities where a is above normal n is near normal and b is below normal the three categories span an equal range and are defined with respect to climatological terciles e g 33rd and 67th percentiles for example a 15 35 50 precipitation forecast implies there is a 15 chance of experiencing above normal conditions a 35 chance of experiencing near normal conditions and a 50 chance of experiencing below normal precipitation in the upcoming season agricultural decisions in the salado a basin are typically made before the beginning of the summer growing season 1 october every year thus we focus on ond seasonal forecasts the ond season is also a critical period in terms of crop yield generation and has shown tendencies towards skillful climate predictions in part due to significant enso signals grimm et al 1998 2000 montecinos et al 2000 ropelewski and halpert 1987 barros and silvestri 2002 boulanger et al 2005 ropelewski and bell 2008 grimm 2011 barreiro 2010 we select iri forecasts for ond 2010 a dry and hot forecast e g 15 35 50 for precipitation 40 35 25 for temperature and ond 2012 a wet and hot forecast e g 40 35 25 for both precipitation and temperature as case studies for this methodology issued on 1 september 2010 and 1 september 2012 respectively to generate space time weather sequences for the two ond seasons from the modified generator ensembles of domain averaged seasonal precipitation and temperature are needed to use as covariates to this end 100 observed ond domain averaged values of precipitation and maximum and minimum temperature are sampled with replacement this is accomplished by first categorizing the observed domain averaged seasonal weather as above near or below normal based on the empirical terciles then assigning the categorical forecasts as probabilities e g 15 35 50 and 40 35 25 for precipitation and temperature respectively to the values in each category and resampling with these assigned weights as the probability metric for instance there is a 15 chance of sampling an above normal precipitation value there are 35 and 50 chances of sampling near normal and below normal precipitation values respectively the result of this resampling scheme is 100 values that are used as covariates to drive the modified weather generator 100 separate times the output of these 100 independent runs is essentially a downscaled ensemble of weather patterns that exhibit the traits of the seasonal forecasts the top row of fig 5 shows the probability density functions pdfs of domain averaged ond precipitation and temperatures from the original and modified weather generators the pdf of ond climatology and the observed values of ond 2010 and 2012 the precipitation pdfs from the modified generator have shifted towards the observed values in both 2010 and 2012 this shift is indicative not only of forecast skill but also the effectiveness of the modified generator in simulating scenarios representative of the forecasts mean maximum minimum temperature during ond 2010 2012 was observed to be above normal and the pdf from the modified generator gives greater probability to above average temperatures than that of the original generator ond 2012 2010 experienced near normal maximum minimum temperatures so the original generator gives highest probability to observed conditions however the range of possible scenarios offered by the original generator is limited and will give near zero probability to above and below normal conditions which for planning purposes can be misleading the domain averaged seasonal totals of precipitation and seasonal averages of temperatures that are generated from the two weather generators are plotted with the observed in the bottom row of fig 5 table 2 reports p values from kolmogorov smirnov tests comparing the distributions of original and modified generator output the differences between the original and modified distributions for ond 2010 weather scenarios and ond 2012 precipitation are significant at the 95 level maximum and minimum temperature scenarios for ond 2012 are not significantly different indicating the covariate values sampled from the iri probabilistic forecast thus the scenarios produced by the modified weather generator do not deviate significantly from climatology weather simulations on a regular grid are of particular interest as they are used to drive hydrologic and agriculture models for agricultural planning to mitigate crop failure to simulate daily weather on a grid the β coefficients for each covariate of the weather generator models are estimated in space from their respective spatial models to the desired spatial resolution 5km 5km these gridded coefficients are then used to obtain the mean function and the daily weather processes are simulated via mean zero gaussian random fields fig 6 shows the difference between the ensemble mean of gridded seasonal total precipitation mean maximum temperature and mean minimum temperature for ond 2010 from the original and modified weather generator the modified generator simulates a drier and hotter domain than the original generator which is consistent with the seasonal forecast notably the modified weather generator simulates a cooling for mean minimum temperature in the southern part of the basin which is inconsistent with the seasonal forecast the differences between the 95 ensemble spread 97 5th percentile minus 2 5th percentile produced by the original and modified weather generators are shown in fig 7 as can be seen the ensemble spread difference for seasonal total precipitation is mostly red and yellow while those for mean maximum and minimum temperature are mostly blue and yellow which illustrates that the modified generator produced wider ensemble spreads than the original generator the uncertainty in the probabilistic seasonal climate forecast is propagated to the modified weather generator resulting in a wider distribution than that of the original generator similar findings can be seen for ond 2012 figures not shown 4 4 multi decadal projections multi decadal projections are useful in a number of applications including environmental impact studies agricultural decision making and water resources management to name a few in agriculture multi decadal projections help in making informed investment decisions e g whether or not to buy a farm in a climatically marginal area invest in irrigation etc specifically the climate of the pampas has shown significant decadal variability and since the 1970s has exhibited a steady increase in both annual and extreme precipitation this trend in precipitation has in part promoted significant expansion of agricultural area to climatically marginal regions of the pampas given the uncertainty of future climate coupled with a known decadal variability it is unclear if existing agricultural systems may remain viable if climate reverts to a drier epoch specific to this research future climate scenarios can be used to drive hydrologic and crop simulation models thus providing an assessment of the viability of existing agricultural production systems in climatically marginal regions of the salado a basin however future climate projections from climate models are generally of coarse spatial e g on a grid and temporal e g monthly resolutions and therefore cannot provide reliable projections of weather at the local scale these monthly and consequently seasonal projections can be incorporated into the modified generator and thus enable the generation of daily weather sequences conditioned on the projections at any desired location both monitored and unmonitored in the study region to this end we explored the ability of the modified generator to downscale medium term projections in the salado a basin a regional climate model projection experiment rcp8 5 was obtained for the period 1 january 2015 to 31 december 2050 a 36 year projection produced using the cordex cmip5 regional climate model ec earth consortium 2014 and bias corrected mcginnis et al 2015 using the claris lpb dataset penalba et al 2014 this projection focuses on south america and is gridded at 0 44 no notable long term trends in annual precipitation totals are projected but the magnitudes are significantly lower than seen in the historic record both maximum and minimum annual average temperatures show positive trends and are projected to increase by approximately 1 c by the year 2050 seasonal values of areal precipitation and temperature for the salado a basin were computed to use as covariates to drive the modified weather generator only the grids that cover the salado a basin and the 17 station data are considered when computing domain averaged precipitation totals and temperature means 100 realizations of daily weather sequences were simulated using both the original and modified weather generators fig 8 shows seasonal residuals projected minus simulated of the ensemble mean of the original and modified weather generator simulations for the 36 year projection period as was seen previously the original generator shows much larger and more variable residuals than does the modified generator consistent with the temporal validation rmse is greatly reduced by including the domain averaged seasonal covariates in the models rmse for seasonal total precipitation is reduced from 90mm to 14mm rmse for seasonal mean maximum temperature is reduced from 1 09 c to 0 48 c rmse for seasonal mean minimum temperature is reduced from 1 43 c to 0 23 c there is a slight warm bias in seasonal mean maximum temperature simulated by the modified generator as compared to that projected by the rcm to illustrate the spatial ability of the weather generators we simulated 100 realizations of daily weather sequences for the period 2015 2050 conditioned on the projected seasonal characteristics fig 9 shows the difference in ensemble mean ond total precipitation maximum temperature and minimum temperature as simulated by the original and modified generators consistent with the climate model trends the modified generator simulates a drier and hotter future across the domain 5 summary and future work we have proposed and validated the use of a parametric stochastic weather generator in a nonstationary context such as climate change impact studies with application in the salado a basin of the argentine pampas this region was selected due to its status as one of the most productive agricultural regions in south america and its strong climatic variability that is experienced at multiple time scales agriculture in the pampas is predominantly rainfed thus high quality seasonal forecast information could greatly impact the outcome e g crop yield risk of failure of a growing season the modified weather generator presented in this research has flexibility in its glm framework such that any number of covariates can be included in the model fit effectively conditioning the weather generator to produce downscaled weather sequences for example in this research we used areal average seasonal total precipitation and mean minimum and maximum temperatures as additional covariates which were shown to be highly significant in the model fit the use of areal averages was justified via principal component analysis for non homogeneous or mountainous regions consider site specific averages or a clustering algorithm the coarse information provided by these additional covariates successfully trickled from seasonal regional down to daily local scales such that wet dry days are more prevalent during seasons with above normal below normal seasonal total precipitation it is with the conditioned output of the weather generator that research teams may provide a more robust estimate of production risk for a region by running the daily weather sequences through process based e g crop simulation hydrologic models the output of process based models may be interpreted and provided to a farmer or decision maker who then will have seasonal forecast information that is relevant to the decisions they must make e g probability of not meeting a crop yield goal where and when to plant a certain crop as opposed to spatially coarse probabilistic statements as are typically reported similarly multidecadal projection information can be used to generate conditional weather sequences to assist in assessing the viability of existing agricultural infrastructure in climatically marginal regions in this a regional climatic trend may be extracted and used to produce conditional weather sequences which may be used to drive any relevant process based models the output of the modified weather generator presented in this manuscript has been validated by direct comparison to the original weather generator of verdin et al 2015 it has been shown that using simple covariates such as domain averaged seasonal total mean precipitation temperatures improves the skill of the generator in producing daily weather sequences that exhibit the traits and trends of a seasonal forecast or multi decadal projection in representing domain averaged behavior for the validation period 2001 2013 this modification to the weather generator reduced rmse values from 77mm to 21mm for precipitation 1 05 c to 0 37 c for maximum temperature and 0 99 c to 0 37 c for minimum temperature similarly the modified generator faithfully reproduced the trends and variability of historic precipitation and temperature at individual sites while the original generator replicates the expected behavior of e g climatology of each season with little to no interannual variability in generating sequences consistent with a seasonal forecast the kolmogorov smirnov tests suggest the output from original and modified weather generators exhibit significantly different traits with 95 confidence unless the seasonal forecast is similar to climatology the modified weather generator was shown to produce pdfs that better represent the range of possible futures while the pdfs from the original weather generator give near zero probability to the upper and lower terciles e g wet hot and dry cold conditions on the multi decadal scale the modified weather generator is flexible in its ability to capture the considerable interannual and decadal variability prevalent in the projected precipitation totals as well as the increase in both minimum and maximum temperatures application of this methodology to other areas is called for however careful attention must be paid to the spatial and temporal climatic variability in the region of interest local climate regional teleconnections and global climate drivers should be identified for optimal skill in downscaling seasonal forecasts and multi decadal projections principal component analysis on seasonal attributes such as seasonal total precipitation and mean temperatures can help decide if domain averaged clustered or site specific covariates should be considered however the model setup as defined in verdin et al 2015 should be considered a baseline model for use in any basin the additional covariates as described in this manuscript need be fine tuned to successfully generate skillful weather scenarios one shortcoming to the weather generator presented in this research is that it uses only precipitation and temperature covariates to condition the weather generator there has been great progress in the identification of teleconnections and climate drivers for regions around the world the pampas are no exception while it was mentioned in this manuscript that such teleconnections could be used as covariates to condition the weather generator output this approach was not investigated a second shortcoming to this methodology is that the uncertainties associated with the parameters of the weather generator are not propagated to the simulations as the maximum likelihood estimates of the parameters are kept fixed as a result the variability of simulations can be underestimated bayesian methods that explicitly quantify the parameter uncertainties are attractive options the methodology of this weather generator is inherently hierarchical thus considering the use of a bayesian hierarchical framework is a natural extension to this problem the authors are currently exploring this approach in a bayesian context the parameters are treated as random variables and are sampled from appropriate distributions typically via markov chain monte carlo based on likelihood acceptance criteria which results in posterior distributions of all model parameters these posterior distributions better represent the uncertainty involved in traditional parameter estimation techniques and when used in a weather generation framework will provide a more realistic range of uncertainty in synthetic weather sequences acknowledgments this research is funded by national science foundation award 1211613 the authors would like to thank the met service of argentina for providing the weather data the authors are grateful for drs linda means and seth mcginnis of the national center for atmospheric research ncar for providing the cmip5 regional climate model projections this work utilized the janus supercomputer which is supported by the national science foundation award number cns 0821794 and the university of colorado boulder the janus supercomputer is a joint effort of the university of colorado boulder the university of colorado denver and the national center for atmospheric research we thank the two anonymous reviewers for their insightful comments which significantly improved the manuscript 
7592,rainfall time series of high temporal resolution and spatial density are crucial for urban hydrology the multiplicative random cascade model can be used for temporal rainfall disaggregation of daily data to generate such time series here the uniform splitting approach with a branching number of 3 in the first disaggregation step is applied to achieve a final resolution of 5min subsequent steps after disaggregation are necessary three modifications at different disaggregation levels are tested in this investigation uniform splitting at δt 15min linear interpolation at δt 7 5min and δt 3 75min results are compared both with observations and an often used approach based on the assumption that a time steps with δt 5 625min as resulting if a branching number of 2 is applied throughout can be replaced with δt 5min called the 1280min approach spatial consistence is implemented in the disaggregated time series using a resampling algorithm in total 24 recording stations in lower saxony northern germany with a 5min resolution have been used for the validation of the disaggregation procedure the urban hydrological suitability is tested with an artificial combined sewer system of about 170hectares the results show that all three variations outperform the 1280min approach regarding reproduction of wet spell duration average intensity fraction of dry intervals and lag 1 autocorrelation extreme values with durations of 5min are also better represented for durations of 1h all approaches show only slight deviations from the observed extremes the applied resampling algorithm is capable to achieve sufficient spatial consistence the effects on the urban hydrological simulations are significant without spatial consistence flood volumes of manholes and combined sewer overflow are strongly underestimated after resampling results using disaggregated time series as input are in the range of those using observed time series best overall performance regarding rainfall statistics are obtained by the method in which the disaggregation process ends at time steps with 7 5min duration deriving the 5min time steps by linear interpolation with subsequent resampling this method leads to a good representation of manhole flooding and combined sewer overflow volume in terms of hydrological simulations and outperforms the 1280min approach keywords precipitation rainfall disaggregation cascade model spatial consistence urban hydrology swmm 1 introduction rainfall time series of high temporal resolution and sufficient station density are crucial for urban hydrology schilling 1991 bruni et al 2015 analyzed the influence of spatial and temporal resolution of rainfall on intensities and simulated runoff with decreasing resolution the variability of rainfall intensities was reduced and the runoff behavior changed regarding maximum water depth and runoff peaks emmanuel et al 2012 extracted different types of rainfall from radar data in western france with 5min resolution and analyzed their spatial extension with variograms they found that for an adequate spatial representation rainfall gauges should have a maximum distance of 6 5km for light rain events and 2 5km for showers berne et al 2004 found similar values by an investigation of intensive mediterranean rainfall events they recommended temporal resolutions of 3 6min and a station density of 2 4km for urban catchments with an area of 1 10km2 ochoa rodriguez et al 2015 analyzed the effect of different combinations of temporal 1 10min and spatial 100 3000m resolutions for different catchments up to drainage areas of 8 7km2 they found that for drainage areas greater than 100ha spatial resolution of 1km is sufficient if the temporal resolution is fine enough 5min radar measured rainfall data would meet these requirements unfortunately the direct measurement with a radar device is not possible only reflected energy from hydrometeors at a certain height above the ground can be measured thus radar data can be affected by different sources of errors e g variations in the relationship between reflected energy and rainfall intensity depending on rainfall type changes in the precipitation particles before reaching the ground anomalous beam propagation and attenuation wilson and brandes 1979 hence it could be expected that the use of uncorrected radar data is not acceptable for many hydrological applications however after correction radar data can be an useful input for e g urban hydrological applications ochoa rodriguez et al 2015 on the other hand direct rainfall measurement is possible with rain gauges the measurement is also affected by errors e g wind induced errors losses from surface wetting or evaporation from collectors but these can be quantified and corrected richter 1995 sevruk 2005 however the availability of observed time series for rain gauges meeting the aforementioned requirements concerning temporal and spatial resolution is rare on the contrary time series with lower temporal resolution e g daily measurements exist for much longer periods and denser networks disaggregation of the time series from these non recording stations using information from the recording stations is a possible solution to this data sparseness problem several methods are available that can be used for this disaggregation e g method of fragments e g wójcik and buishand 2003 poisson cluster models e g onof et al 2000 cascade models or a combination of different methods e g paschalis et al 2014 for a theoretical introduction to cascade models and the underlying theory of scale invariance the reader is referred to serinaldi 2010 and the reviews of veneziano et al 2006 veneziano and langousis 2010 and schertzer and lovejoy 2011 an advantage of micro canonical cascade models is their exact conservation of rainfall volume of the coarse time series at each disaggregation step olsson 1998 the total rainfall amount of each coarse time step is distributed on the number of finer time steps whereby the number of resulting wet time steps and their rainfall amount as fraction of the total rainfall amount of the coarser time step depends on the cascade generator an aggregation of the disaggregated time series results in exactly the same time series that was used as a starting point for the disaggregation accordingly all required parameters for the disaggregation process can also be estimated from the aggregation of the recording time series carsteanu and foufoula georgiou 1996 and can then be applied to the disaggregation of time series from surrounding non recording stations koutsoyiannis et al 2003 cascade models are widely applied in urban hydrology one structural element of the cascade models is the branching number b which determines the number of finer time steps generated from one coarser time step in most cases b 2 which means that a starting length of 24h will result in an inapplicable temporal resolution of 11 25 or 5 625min so a direct disaggregation from daily values to 5 or 10 min values is not possible one established solution is to preserve the rainfall amount of a day but the length is reduced to 1280min instead of 1440min molnar and burlando 2005 2008 paschalis et al 2014 licznar et al 2011a b 2015 under this assumption temporal resolutions of 5 or 10 min can be achieved however this is a very coarse assumption and missing time steps have to be infilled with dry intervals for applications such as the continuous modeling of sewer systems if b is changed during the disaggregation process other temporal resolutions can be achieved lisniak et al 2013 introduce a cascade model with b 3 to reach the first and b 2 to reach all further disaggregation levels which leads to time steps with 1h duration a parameter sparse version of their model is applied in müller and haberlandt 2015 the application of cascade models for generation of high resolution time series is recommended by several authors segond et al 2007 suggest a cascade model for the disaggregation of hourly time series although other methods were also tested in their investigation onof et al 2005 disaggregated hourly values with b 2 to 3 75min intervals and transformed them into 5min intervals by a linear interpolation similar to the diversion process to achieve hourly resolutions described in güntner et al 2001 hingray and ben haha 2005 tested several models for disaggregation from 1h to 10min micro canonical models have shown best performance regarding rainfall statistics but also for modeled discharge results considering these previous studies the testing of different cascade model variations for the disaggregation of daily values is the first novelty of this investigation the impact on time series characteristics like average wet spell duration and amount dry spell duration fraction of dry intervals average intensity autocorrelation function and also extreme values will be analyzed furthermore the impact on overflow occurrence and volume within an artificial sewer network will be analyzed although rainfall time series are often disaggregated for this purpose analyzes of the impact on runoff in sewer systems are rare an artificial sewer system in combination with real recording rainfall gauges will be used for this study as time series of different stations are disaggregated without taking into account time series of surrounding stations this per station or single point procedure results in unrealistic spatial patterns of rainfall since the spatial resolution is also important for high resolution rainfall berne et al 2004 emmanuel et al 2012 the spatial distribution of rainfall has to be considered müller and haberlandt 2015 introduced a resampling algorithm to implement spatial consistence into time series after disaggregation for hourly resolutions a similar procedure will be used in this study for the first time to the author s knowledge for 5min values also it will be analyzed if there is a necessity for spatial heterogeneous rainfall resulting from more than one station to represent extreme events in such a small catchment or if uniform rainfall resulting from one station is sufficient the paper is organized as follows in the data section the investigation area the rainfall stations and the artificial sewer system are described in the next section the applied methods are discussed in three parts in the first part different possibilities for the rainfall disaggregation are explained the second part concerns the implementation of spatial consistence in the disaggregated time series the implementation in the sewer system and the analysis of the model results are described in the third part in the results and discussion section the results for all three parts are shown and discussed a summary and outlook are given in the final section 2 data 2 1 rainfall data the stations used for this investigation are located in and around lower saxony germany 47614km2 see fig 1 the investigation area can be divided into three different regions the harz middle mountains with altitudes up to 1141m in the south the coastal area around the north sea in the north and the flatland around the lüneburger heide in between some areas of the harz mountains have average annual precipitation greater than 1400mm furthermore the study area can be divided climatologically according to the köppen geiger classification into a temperate climate in the north and a cold climate in the mountainous region both climates exhibit hot summers but no dry season peel et al 2007 in fig 1 24 recording stations from the german weather service dwd with long term time series 9 20years are shown these stations are used for the validation of the cascade model variants concerning the representation of different rainfall characteristics these are overall characteristics like average intensity and fraction of wet hours but also event characteristics like dry spell duration wet spell duration and wet spell amount as well as extreme values events are defined as having a minimum of one dry time step before and after the rainfall occurrence after dunkerley 2008 a dry time step is defined with a rainfall intensity of 0mm 5min the measurement devices are either tipping buckets drop counters or weighting gauges with accuracies of 0 1mm or 0 01mm time series of single stations from different data bases with different temporal resolutions 1 and 5min have been combined to extend their time series length so the resulting temporal resolution is 5min this enables also comparisons with previous microcanonical cascade studies see discussion in licznar et al 2015 these characteristics and further information of the rainfall stations are given in table 1 additionally five recording stations from the city of brunswick are used see table 1 station i v these stations are not shown in fig 1 due to their proximity with distances to each other less than 5km however the city of brunswick is shown the time series lengths of these stations are shorter 02 01 2000 24 12 2006 so that they have not been used for the validation of the cascade model variants due to their proximity these stations are used for the estimation of the bivariate characteristics for the validation of the resampling algorithm and as input for the validation within an urban hydrological model the measurement devices are tipping buckets with accuracies of 0 1mm and temporal resolutions of 1min however to enable comparisons to the aforementioned stations the time series are aggregated to 5min 2 2 combined sewer system an artificial combined sewer system was constructed in order to compare the different disaggregation approaches fig 2 the application of artificial systems is a common approach for the validation of synthetic rainfall kim and olivera 2012 the sewer system consists of 22 sub catchments with a mean area size of 7 6ha ranging from 1 1ha to 16ha with a standard deviation of 4 6ha and a cumulative total of 168 1ha each catchment has a fraction imperviousness of 65 a uniform slope of 0 25 is used for all pipes a tank before the outfall of the sewer system was implemented it has a storage capacity of 2184m3 equal to 20m3 per hectare of impervious area and is a typical value after imhoff and imhoff 2007 three rain gauges are implemented they have been arranged with the same distances to each other as three rain gauges in the city brunswick station i ii and iv this allows a direct comparison between simulated discharges and flood volumes resulting from observed and disaggregated time series with a maximum distance of about 3km for these three stations the rainfall data meets the requirements suggested by berne et al 2004 and emmanuel et al 2012 additionally each of the rain gauges influences similar fractions of the sewer system station i 32 7 station ii 31 2 station iv 36 0 whereby to each subcatchment only the one rain gauge is assigned which is closest to the centroid of the subcatchment see fig 2 if one of the rain gauges would be situated more central than the other two it would affect a higher fraction of the area due to a higher distance of other stations if so the rainfall input would be uniform for also a higher fraction of the area so due to the central position of all three rain gauges the effect of the spatial consistence will be emphasized 3 methods in this section the cascade model for the disaggregation of the rainfall time series will be described first afterwards the implementation of the spatial consistence and the application to an artificial urban hydrological case are explained 3 1 cascade model the principle of a multiplicative micro canonocal cascade model as it was introduced by olsson 1998 for temporal rainfall disaggregation is illustrated in fig 3 one coarse time step is split into b finer time steps of equal duration where b is the branching number with b 2 in fig 3 for the splitting the weights w1 and w2 are used to determine the rainfall volume in the two finer time steps the sum of w1 and w2 is 1 in each split so that the rainfall volume is conserved exactly an aggregation of the disaggregated rainfall would result in the same time series that has been used for the disaggregation possible combinations of w1 and w2 are given in 1 the so called cascade generator 1 w 1 w 2 0 and 1 with p 0 1 1 and 0 with p 1 0 x and 1 x with p x 1 x 0 x 1 where p is the probability of each combination of weights the probability p 0 1 denotes a splitting with no rainfall volume assigned to the first time step w1 and 100 of the rainfall volume w2 1 w1 in the second time step the probability p 1 0 causes a vice versa result the rainfall volume could also be distributed over both time steps with a x 1 x splitting the relative fraction x of rainfall assigned to the first time step is defined as 0 x 1 considering x as a random variable for all disaggregation steps a probability density function f x with the empirical probabilities for each value of x is estimated theoretical density functions are not fitted it has been shown by several authors that the parameters are volume and position dependent olsson 1998 güntner et al 2001 rupp et al 2009 for the cascade model in general four different positions in the time series starting enclosed isolated ending and two volume classes for each position are used güntner et al 2001 analyzed different thresholds for the differentiation of the two volume classes they proved that the mean rainfall intensity of all rainfall intensities of the actual cascade level for one position is an acceptable threshold for this differentiation it has therefore also been applied in this study the probabilities are shown exemplary for rain gauge göttingen in appendix a for aggregation steps from 5min to 1280min marshak et al 1994 analyzed estimated parameters based on different aggregation levels and compared them differences between the parameter sets are significant and should be taken into account especially if the disaggregation process is carried out over a high number of cascade levels furthermore it can be distinguished between unbounded and bounded cascade models in unbounded cascade model the parameters are assumed to be scale independent which means that the same parameter set is applied over all disaggregation levels however it was found that the cascade generator exhibits a scale dependency see serinaldi 2010 and references therein so for each disaggregation step a different parameter set has to be applied it should be mentioned that rupp et al 2009 found only minor improvements using scale dependent parameter sets by an application over a small range of time scales daily to hourly although unbounded cascade models are still used jebari et al 2012 bounded cascade models can be found more frequently in the recent literature rupp et al 2009 licznar et al 2011a b lombardo et al 2012 lisniak et al 2013 this increases the amount of parameters with each disaggregation level the similarity of p 0 1 for starting boxes and p 1 0 for ending boxes and vice versa as well as p 0 1 p 1 0 for enclosed and isolated boxes güntner et al 2001 could be used to reduce the parameter count in order to keep the model as parameter parsimonious as possible however to isolate the effects of the approaches described below this was not applied in this study for the sake of completeness it should be mentioned that other cascade models less parameters exist e g universal multifractals with three parameters valid on various successive cascade steps the interested reader is referred to the review of schertzer and lovejoy 2011 however a multiplicative micro canonical bounded cascade model is applied for this study in most cases disaggregation starts with daily values from time series of non recording stations since the network density is higher and time series are longer for these types of stations in comparison to recording stations however with the method explained above a final resolution of 5min cannot be achieved directly one day lasts per definition 24h which is 1440min in total using a branching number of b 2 throughout the whole disaggregation process results in temporal resolutions of 5 625min after eight disaggregation steps this is not a very useful resolution if these time series are to be used for further applications such as urban water management models different alternatives to reach a 5min resolution are explained and discussed in the following two subsections method a 1280min approach one possibility is to assume that the complete daily rainfall amount occurs in only 1280min using this assumption a final resolution of 5min can be achieved by a complete conservation of the rainfall amount this assumption is common for generating rainfall for urban applications licznar et al 2011a b 2015 molnar and burlando 2005 serinaldi 2010 paschalis et al 2014 however the final time series length is reduced due to this underlying assumption there exist different possibilities how to avoid this reduction e g inserting missing time steps as dry time steps in each day or only between two successive dry days however each of these methods would directly influence the time series characteristics for further processing the disaggregated shortened time series was used without any changes another alternative would be a disaggregation down to a very fine scale and then aggregate time steps to the temporal resolution most similar to 5min possible disaggregation levels would be 42 2s 11 disaggregation steps respectively 5 3s 14 disaggregation steps which can be aggregated to 4 922min respectively 5 01min however the scale dependent parameters cannot be estimated from observations for this high resolution so this alternative could not be applied in the investigation method b uniform splitting approach the uniform splitting approach was introduced by müller and haberlandt 2015 and uses a branching number b 3 only in the first disaggregation step resulting in three 8h intervals one two or all three of these 8h intervals can be wet the probabilities for the number of wet intervals can also be estimated from observations see table 2 the threshold for the volume classes in this first step is a quantile q chosen with respect to very high daily rainfall intensities q 0 998 as in müller and haberlandt 2015 the probabilities in table 2 show a clear dependency on the volume class while for the lower volume class the probability is highest for one wet 8h interval for the upper volume class it is for three wet 8h intervals due to the chosen threshold only a small number of wet days are used for the parameter estimation for the upper volume class for the majority of those days all three 8h intervals are wet only a minority if at all shows just one or two wet 8h intervals this confirms the findings of müller and haberlandt 2015 and underlines the importance of the implementation of an upper volume class the application of b 3 in the first disaggregation step has been done before by lisniak et al 2013 to achieve a target resolution of 1h which is a suitable temporal resolution as input for rainfall runoff models for rural catchments the daily rainfall volume is distributed uniformly among the number of randomly as wet identified 8h intervals this uniform distribution on the before chosen number of wet 8h intervals is a compromise between the parameter intensive approach proposed by lisniak et al who have used up to 8 empirical distribution functions to represent the splitting behavior from daily to 8h time steps and the quality of the disaggregation results for the second and all following disaggregation steps the branching number is reduced to b 2 it is obvious that to achieve a target resolution of 5min additional modifications have to be introduced the disaggregation levels delivering time steps of δt 15 7 5 or 3 75 min therefore were used to introduce these modifications it should be mentioned that the parameters for the disaggregation steps for resolutions finer than 15min with b 2 cannot be estimated from observations directly since only 5min values are available for disaggregations from 15 to 7 5 and consequently 3 75min the parameter set from the aggregation of 5 to 10min was used throughout method b1 modification for δt 15 min similar to the first disaggregation step a uniform splitting with a branching number b 3 is applied the threshold for the differentiation into two volume classes was chosen as the mean of all rainfall intensities at the 15min level the parameters of the two volume classes differ significantly from each other method b2 modification for δt 7 5 min firstly the rainfall volume of each time step is distributed uniformly on three time steps with 2 5min afterwards two non overlapping time steps are aggregated always method b3 modification for δt 3 75 min the applied method is similar to b2 and was used by onof et al 2005 and onof and arnbjerg nielsen 2009 for the disaggregation from 1h down to 5min the rainfall volume is distributed uniformly on three finer time steps with 1 25min duration followed by an aggregation of four non overlapping time steps the disaggregation is a random process which leads to different results depending on the initialization of the random number generator this random behavior is covered by a certain number of disaggregation runs it was found that after 30 disaggregation runs the average values of the main characteristics see section 4 1 temporal disaggregation did not change significantly by an increasing number of disaggregation runs accordingly 30 disaggregations were carried out for each method a comparison of the rainfall characteristics rc of disaggregated time series dis with the observations obs is carried out regarding the relative error re the objective criterion is calculated for each station i over all realizations n of the disaggregation and averaged afterwards over all stations 2 re 1 n i 1 n rc dis i rc obs i rc obs i for the investigation of the rainfall extremes resulting from the different methods partial duration series also known as peaks over threshold are extracted for all time series the threshold is chosen in a way to obtain two values per year on average from each time series dwa a 531 2012 based on 30 realisations for each method the median of the extreme values was determined for further processing since the time series lengths of the stations differ also elements in the partial duration series and hence return periods are different in order to include all stations in an objective comparison of extreme values an exponential distribution was fitted to the medians the exponential distribution which is a standard distribution function in germany for partial duration series of rainfall dwa a 531 2012 was chosen for this purpose the deviations of the fitted extreme values between disaggregated and observed rainfall are calculated using the relative root mean square error rrmse the deviations between disaggregated and observed rainfall intensities i for single return periods t of the fitted distribution function for each station i are calculated and averaged afterwards over all stations n 3 rrmse t 1 n i 1 n i t dis i i t obs i 2 i t obs i with t 0 5 1 2 5 10 years this criterion was used for the evaluation of rainfall disaggregation products before by güntner et al 2001 for single stations they evaluate deviations of up to 10 as accurately generated up to 15 as well reproduced and higher than 40 as overestimated respectively more than 200 as severe overestimated these values are based on the disaggregation results in their manuscript and are provided here to give a general feeling for the criterion as well as for the re whose absolute values are comparable to rrmse 3 2 spatial consistence the disaggregation of the rainfall time series is a pointwise procedure this yields unrealistic spatial patterns of rainfall a resampling procedure introduced by müller and haberlandt 2015 is applied to implement spatial consistence into the disaggregated time series z three bivariate characteristics are assumed to represent spatial consistence namely probability of occurrence coefficient of correlation and continuity ratio wilks 1998 1 probability of occurrence the probability of occurrence pk l describes the probability of rainfall occurrence at two stations k and l at the same time 4 p k l z k 0 z l 0 n 11 n where n is the total number of non missing observation hours at both stations k and l and n11 represents the number of simultaneous rainfall occurrence at both stations a differentiation for convective and stratiform events as in cowpertwait 1995 was not applied 2 pearson s coefficient of correlation the pearson s coefficient of correlation is used to describe the relationship between simultaneously occurring rainfall at two stations k and l it is a measure of the linear relation between both rainfall time series eq 4 this coefficient was used previously for multisite rainfall generation by breinl et al 2013 2014 5 ρ k l cov z k z l var z k xvar z l z k 0 z 1 0 3 continuity measure the continuity measure compares the expected rainfall amount at one station for cases with and without rain at the neighboring station e is the expectation operator 6 c k l e z k z k 0 z l 0 e z k z k 0 z l 0 it is possible to estimate the prescribed values of these characteristics as functions of the separation distance between two stations from observed data see fig 4 for the estimation all stations from the city brunswick are used stations i v from table 1 the results for stations 1 24 are not shown here because the spatial scale with distances up to 350km is not useful for the aim of urban hydrologic modeling the resampling procedure involves a bivariate objective function ok l that has to be minimized 7 o k l w 1 p k l p k l w 2 ρ k l ρ k l w 3 c k l c k l the parameters indicated by are the prescribed values for two stations resulting from the regression lines in fig 4 and the other parameters are the actual values the fit of the regression lines to the observations are also validated with pearson s coefficient of correlation to avoid confusion with the spatial characteristic here the symbol r 2 is used for the goodness of fit while for the probability of occurrence and the continuity ratio only a small r 2 value is achieved due to the variation of these parameters for the coefficient of correlation a r 2 value of 0 8462 is determined however the general behavior for all three characteristics increasing or decreasing with increasing distance can be represented by the regression lines the weights w1 w2 and w3 are necessary to adjust the scale of the rainfall characteristics and to consider their importance the regression lines are necessary to determine prescribed values for distances not available from the recording rain gauges for the minimization a resampling algorithm namely simulated annealing is implemented kirkpatrick et al 1983 aarts and korst 1965 the algorithm has been used for rainfall generation processes before bardossy 1998 haberlandt et al 2008 for the resampling two conditions are considered the structure of the disaggregated time series combination of position and volume classes should not be changed also the rainfall amount of each day should not be changed therefore only the relative diurnal cycles of the disaggregated time series are resampled preserving the structure of the disaggregated time series described by position and volume class of the daily time step one disaggregated time series is chosen randomly as a reference time series the relative diurnal cycles of a second randomly chosen time series will be resampled until the objective function is minimized so it cannot be improved for a certain number of attempts swaps are only possible between days with the same position and volume class at the beginning of the algorithm also bad swaps worsening of the objective function are accepted with a certain probability π the parameter π decreases with the run time of the resampling algorithm this allows to leave a local optimum to find the global minimum after the resampling the time series serves as a new reference station along with the first one further time series have to be resampled with respect to all reference time series and will be added afterwards to the set of reference time series for detailed information on the applied resampling algorithm the reader is referred to müller and haberlandt 2015 since the cascade model is based on the scaling theory it could be questioned if there is a discrepancy in the temporal dimension of the disaggregation process due to the spatial dimension of the resampling algorithm therefore scaling behavior of the disaggregated time series is analyzed before and after the resampling with the relation described in eq 8 8 m q λ k q with moments m moments order q the moments scaling exponent k q and the scale ratio λ analyzing the scaling behavior with log log plots of mq and λ indicating different durations is a common method in the field of rainfall disaggregation with cascade models see e g over and gupta 1994 svensson et al 1996 burlando and rosso 1996 serinaldi 2010 the scale ratio represents a dimensionless ratio of two temporal resolutions of one time series dry time steps are neglected for the scaling analyzes for the moments estimation probability weighted moments pwm as in yu et al 2014 and ding et al 2015 are applied an advantage of the pwm is their relative robustness against large rainfall intensities kumar et al 1994 hosking and wallis 1997 according to kumar et al 2014 and lombardo et al 2014 the investigation is limited to 1 q 3 the pwm of different temporal resolutions will be compared before and after the resampling process 3 3 urbanhydrological modeling 3 3 1 model swmm the sewer system has been constructed in the epa storm water management model 5 1 swmm rossman 2010 with swmm dynamic rainfall runoff simulations can be carried out continuously or event based to determine runoff quantity and quality from urban areas a constructed sewer system in swmm is split horizontally into subcatchments each subcatchment is characterized by a number of parameters e g total area fraction of impervious area and slope connecting main pipes of the subcatchments are represented by links furthermore the sewer system can be complemented by storage treatment devices pumps and regulators the application of a semi distributed model like swmm limits the ability to reflect spatial variability due to the fact that for each subcatchment the rainfall is assumed to be uniform for its area however since disaggregated time series from rain gauges are used in this study there is no degradation of spatial variability nevertheless it could be a negative aspect especially when high resolution spatial rainfall data is used gires et al 2015 for each subcatchment flow is generated from both dry weather flow representing domestic and industrial wastewater flow and rainfall runoff from pervious and impervious areas rainfall is assumed to be uniform for each subcatchment for infiltration the equations of modified horton green ampt and curve number are selectable here the curve number equation was used the dynamic wave equation as approximation of the st venant equations is used for the calculation of the flows through the sewer system as forcing main equation for the dynamic wave hazen williams equation has been chosen pondage of flooded nodes can be allowed or not here it was permitted overland flow does not occur 3 3 2 influence of number of recording stations it could be questioned whether one station is enough to represent rainfall for such a small catchment 168 1ha so the impact of one or more implemented stations we apply three stations on the combined sewer system runoff has to be analyzed for this analysis the following procedure was applied using observed data only for three stations the partial duration series of extreme values were derived as described in section 4 1 for the extremes of the other stations in lower saxony this results in 14 total extreme values for each station using the time period 01 01 2000 24 12 2006 that is available for the stations prinzenweg bürgerpark und weststadt the return periods tk of these extreme values can be calculated with the weibull plotting position comparisons are carried out for extreme values with 30min duration and for return periods of tk 4 4years and tk 0 9years respectively these are representative return periods for the dimensioning of sewer system elements dwa a 118 2006 din en 752 2 1996 if only one station is used the extreme value is considered to be uniform throughout the whole catchment this procedure is carried out for the extremes of each station so in total three extreme values are analyzed for each return period if three stations are used the time steps of an extreme value of one station this station is the so called master station and the simultaneous time steps from the other two stations are used as spatial heterogeneous input again the procedure is carried out for each station as a master station in total three events are analyzed here events are defined as an extreme value at the master station and simultaneous time steps at the other two stations 3 3 3 influence of disaggregation method the aim of the second investigation based on the sewer system is to investigate if there is a need for the implementation of spatial consistence using more than one station in addition to the choice of the disaggregation method therefore three rainfall stations are implemented throughout for each disaggregation method the time series of all 30 realizations are resampled res to implement spatial consistence for each method a a res b1 b1 res b2 b2 res b3 b3 res and each realization 1 30 the partial duration series of extreme values were derived again for one event the time steps of an extreme value of one station and the simultaneous time steps from the other two stations are used as spatial heterogeneous input as carried out for three stations before see section 3 3 2 in total 90 events for each return period based on the 30 realizations of each method are used for simulation 4 results and discussion the results section is organized as follows the univariate rainfall characteristics of the disaggregation will be discussed in section 4 1 while the multivariate characteristics will be analyzed in section 4 2 all results of the urban hydrological modeling will be discussed in section 4 3 4 1 temporal disaggregation in total four different variations of the micro canonical cascade model were tested to disaggregate time series from daily to 5 min values seven basic rainfall characteristics were chosen for analyzes wet spell duration and amount average intensity dry spell duration fraction of dry intervals autocorrelation and extreme values a wet period is defined as the duration with rainfall volume continuously greater than 0mm in each time step the rainfall characteristics are illustrated in figs 5 8 and in table 3 as observations vs disaggregations for each station and as averages resulting from eq 2 molnar and burlando 2005 and müller and haberlandt 2015 identified a high fraction of rainfall intensities in the disaggregated time series smaller than the accuracy of the measuring instrument and hence the minimum resolution in the observed time series which have been used for the parameter estimation these time steps are from a hydrological point of view negligible to reduce the impact on the results due to these small time steps an additional analyzes of the rainfall characteristics with a minimum intensity of higher than 0 1mm as threshold was introduced this threshold value has additionally the advantage to exclude single tips from the measurement device the results are shown in fig 6 for stations with wet spell durations wsd shorter than 17 5 min fig 5 method b2 and b3 show acceptable over underestimations the results are similar for b2 for wet spell durations shorter than 14min if a threshold is taken into account fig 6 for stations with longer wsd all methods show underestimations respectively an underestimation of the wsd was identified before by olsson 1998 this can be explained by the definition of a wet period every dry spell regardless of its length terminates a wet spell so the reproduction of wsd becomes more and more complicated with an increasing length of observed wsd because a single dry time step would divide a long wet spell into two shorter wet spells the generation of dry intervals depends on the probabilities of p 1 0 and p 0 1 these probabilities are significant lower for all positions for the higher volume class in comparison to the lower corresponding volume class see appendix a the influence of the rainfall intensity on the generation of dry intervals has been identified before by rupp et al 2009 it can also be confirmed that the variation of the probabilities is higher between the volume classes in comparison to the variation between different scales the results for the average intensity also show a clear structure and trend if no threshold is taken into account all methods overestimate the intensities with the largest deviations shown for method a 63 on average one reason for this lies in the reduction of the day duration by 160min 2 7h 1440 1280min if a threshold is introduced deviations are smaller for all methods method b2 shows the smallest deviations for the average intensity in both analyzes the results for the wet spell amount wsa do not show a clear trend for all methods like that for wsd and average intensity fig 5 the underestimation of wsd and the overestimation of average intensities compensate each other and lead to a deceptively good fit for method a with only slightly underestimations of wsa method b1 underestimates the observations more strongly while method b2 and b3 overestimate wsa for most of the stations if time steps with small rainfall intensities are neglected different results can be identified fig 6 method a and b3 show strong underestimations while b1 and b2 show acceptable agreements with 3 and 1 respectively it should be noticed that the absolute values of wsd are slightly decreasing with an introduction of the threshold while wsa and average intensity are strongly increasing for the dry spell duration dsd methods b2 and b3 show similar results if all values are taken into account both overestimate the observations by 8 and 5 respectively while method a and b1 underestimate the observations by 13 and 10 respectively with the introduction of the threshold single tips of the measurement device are ignored which increases the dsd significantly all methods lead to underestimations with the worst representation by method a the underestimation of both durations wsd and dsd by method a is a systematic error due to the shortening of the total length of the time series the underestimation of dsd could be reduced by inserting dry intervals for the missing time steps of this method however the fraction of dry intervals is overestimated by method a as well as by the other methods if all values are included the differences between the methods are much smaller in comparison to wsd see table 3 since all dry days which are not influenced by the chosen disaggregation method are also taken into account with the threshold introduction the fraction of dry intervals is underestimated by all methods however the deviations between the methods themselves as well as in comparison to the observations are very small 1 and can be neglected furthermore the autocorrelation function is shown in fig 7 for the observed time series for station harzgerode and as median of the autocorrelation functions resulting from 30 realisations for each method taking into account all values of the disaggregated time series the autocorrelation is underestimated by all methods the relative errors are determined as average of all stations for lag 1 and lag 12 representing temporal shifts of 5min and 1h respectively see table 3 the lag 1 autocorrelation is underestimated by method a with 50 while method b1 3 and b2 4 show smaller underestimations for lag 12 all methods show significant underestimation of the autocorrelation function of approximately 50 the autocorrelation function for b1 shows peaks periodically in a 3 lags distance due to the applied uniform splitting approach for disaggregation of 15min to 5min the underestimation of the autocorrelation function is a well known problem and has been identified before by e g olsson 1998 güntner et al 2001 pui et al 2012 and paschalis et al 2012 2014 a comparison of the observed empirical extreme values with the range the 0 05 and 95 quantileand the median of the 30 disaggregations for station osnabrück is given in fig 8 the median and both quantiles represent typical results for all stations for the illustration the weibull plotting position was used the medians of all 30 realisations for b2 and b3 show a good fit to the observed values while the median for a tends to overestimate the observations the range and quantiles of b2 and b3 are similar while a shows strong overestimations and b1 underestimations respectively for the highest return period t 35years overestimations of the observed values can be identified by a factor of 6 for method a if the range is taken into account for other stations overestimations for method b1 can be identified from the range as well not shown here the results shown regarding median and both quantiles are representative for most of the stations for comparisons exponential distributions were fitted to the median of all realizations for each station rainfall intensities are analyzed for the return periods of 0 5 twice a year 1 2 5 and 10years and for durations of 5min and 1h fig 9 shows the relative errors as box whisker plots for rainfall intensities of 5min duration with return periods of 1 and 5 years for each method method a leads to the highest overestimation for both return periods hence the shortening of the day duration to 1280min also affects the extremes with 5min duration extreme values are also overestimated by b3 due to the disaggregation down to a very fine temporal resolution of 3 75min the splitting of the daily rainfall amount can potentially be reduced onto only a small number of fine time steps this leads to an overestimation of the extreme values concerning the medians b2 overestimates the observations slightly while b1 underestimates them in b1 rainfall at a temporal resolution of 15min is split with one disaggregation step into three finer final time steps while in b2 two disaggregation steps follow this causes a higher intensity of rainfall similar to the overestimation of b3 however the range of rainfall quantiles with 5year return periods resulting from b2 is much higher than for b1 for longer durations the differences between the methods decrease see fig 10 for b1 b2 and b3 the results for 1h are similar because the disaggregation process is exactly the same until this duration minor differences are only caused by adjacent time steps of extreme events for a return period of 1year the median of a b1 b2 and b3 show the same slight overestimation of the observed values for a 5year return period a also leads to a slight overestimation while b1 b2 and b3 underestimate the extreme values of the observations it should be noted that the deviations for 1h rainfall duration are much smaller than for 5min for all methods however the smallest ranges for both return periods result from a which delivers the best performance for an hourly target time step the relative root mean square errors rrmse for all rainfall quantiles and 5min time step are given in table 4 the rrmse for all return periods is highest for method a followed by b3 b2 is slightly higher than b1 for all return periods except 0 5years from a practical point of view using the medians of b2 as design values would lead to a dimensioning on the safe side 4 2 spatial consistence spatial consistence was assumed to be represented by matching three bivariate characteristics namely probability of occurrence pearson s coefficient of correlation and continuity ratio in fig 11 values for these characteristics resulting from the observations after the disaggregation without resampling and after resampling are shown for the coefficient of correlation values similar to the observations could be achieved although before resampling an underestimation independent from station distances could be identified for the continuity ratio the results are more complex to interpret due to the definition of this characteristic eq 6 for each pair of stations two different values exist depending on which station is defined as k and l during the resampling only the time series combinations of the actual station k and the reference stations l are taken into account not vice versa the resulting values are the three well fitted values from the resampled dataset for each distance in fig 11 for higher distances values comparable to those from observations could be achieved for a distance of 1 4km a slight worsening can be identified continuity ratios with values ck l 0 8 represent the combinations that have not been considered during the resampling process these values are worsening by the resampling for all distances an omitting of continuity ratio would cause a worsening of all values for this characteristic müller and haberlandt 2015 so the continuity ratio remained included in the objective function the probability of occurrence was underestimated for the disaggregated non resampled time series for all distances after the resampling all values could be improved significantly however values resulting from the observations could not be reached and are still underestimated due to the shortness of the time series only a limited number of relative diurnal cycles is available the number is too small to find matching relative diurnal cycles especially for a temporal resolution of 5min with 288 time steps each day although a strong weighting of this characteristic in the objective function w 1 0 899 in comparison to w 2 0 002 for pearson s coefficient of correlation and w 3 0 099 for continuity ratio a better fit was not possible however it can be assumed that this characteristic improves with increasing time series length it is further analyzed if the resampling algorithm influences the scaling behavior of the disaggregated time series therefore the first three moments have been calculated for all realizations before and after the resampling the means are shown in fig 12 the first probably weighted moment represents the mean value of the time series it is not changed by the resampling algorithm since the total rainfall amount and the number of wet time steps are not changed however the second and the third moment show slight increases indicating minor changes of the standard deviation and the skewness of the average intensity the increases are stronger for finer temporal resolutions and can be identified for all methods however the deviations from the not resampled time series are smaller than 5 and hence accepted 4 3 urban hydrological modeling for the urban model two investigations are carried out in the first part the necessity for the implementation of more than one rainfall station will be analyzed this investigation is carried out using observed time series only secondly the benefit for the implementation of spatial consistence using more than one station in addition to the choice of the disaggregation method is analyzed for both investigations two criteria will be used for the validation the flood volume represents the water volume that leaves the sewage system temporary through manholes the combined sewer overflow volume is the cumulative volume of the sewage system which is released from the tank to the receiving water and not to the treatment plant it should be mentioned that a variation of the swmm model parameter would influence the simulation results as sensitive parameters have been identified the imperviousness and the surface depression storage barco et al 2008 goldstein et al 2010 also slope and the capacity of the main pipes would affect the resulting flood volumes and combined sewer overflow volume a different tank volume would affect combined sewer overflow volume for more information about the parameter sensitivity the reader is referred to krebs et al 2014 4 3 1 influence of number of recording stations for this investigation only observed time series are used in fig 13 the resulting flood volumes from using one uniform rainfall or three stations heterogeneous rainfall are shown for an observed extreme event at the master station with 30min duration and a return period of tk 4 4years the heterogeneous case is assumed to represent the reference with small differences between results based on different extreme events however there are high deviations using uniform rainfall as input in comparison to spatial heterogeneous rainfall the flood volume is overestimated by 143 on average if only one station is used as input in comparison to using all three stations representing the reference through its spatial coverage however using only one station can lead to larger overestimations of 384 station bürgerpark but also to underestimations station weststadt by 15 the same investigation was carried out for events with a return period of tk 0 9years for all three events no flooding occurs using three stations as input not shown here however using one station leads to flooding for the station weststadt so again an overestimation of flooding volume occurs using spatial uniform rainfall the results for the combined sewer overflow volume the overflow of the tank are similar it is overestimated by using only one station as input results are the same for both return periods fig 14 it can be concluded that one station is not sufficient to represent the rainfall behavior adequately although only the effects for a small catchment 168 1ha are analyzed this confirms the results of e g schilling 1991 berne et al 2004 emmanuel et al 2012 gires et al 2015 bruni et al 2015 and ochoa rodriguez et al 2015 that for the representation of spatial variability of rainfall a high station network density is crucial these results are conformable to the theory of areal reduction factors which should be mentioned in this context the basic idea of this concept is that extreme point values cannot be used uniformly for applications requiring spatial rainfall therefore the point values have to be reduced with an areal reduction factor this is indicated by the results since without a reduction of the point rainfall values overstimations of 67 tk 4 4a respectively 71 tk 0 9a of the combined sewer overflow volume occur comparing the average results of using 1 and 3 stations sivapalan and blöschl 1998 found that these factors depend on the return period of the events and the applied catchment area veneziano and langousis 2005 investigate the areal reduction factors in context of a multifractal analysis for a critical review of areal reduction factors and methods to estimate them the reader is referred to wright et al 2013 for further investigations spatial heterogeneous rainfall is applied throughout the study 4 3 2 influence of disaggregation methods and spatial consistence for this investigation observed time series are used as reference for the validation of the disaggregated time series for the comparison of the disaggregation methods before and after the resampling the flood volumes of all nodes in the sewer system fig 15 and the corresponding combined sewer overflow fig 16 are analyzed event based see section 3 3 3 for the event selection in fig 15 the total flood volume of each method is higher after resampling than before first the results for events with tk 4 4years are discussed after the disaggregation but before the resampling wet time steps are located randomly within the days of the time series after resampling the probability of simultaneously rainfall occurrence is much higher which results in higher areal rainfall amounts and accordingly an increase in flood volume without resampling the total flood volume is underestimated by all methods so if more than one station is used in the urban hydrological model the disaggregated time series have to be resampled without resampling unrealistic spatial patterns occur and the simulated flood volumes are not representable for method a an overestimation of the average rainfall intensity and the extreme values has been identified before also the total flood volume is overestimated after resampling by about 240 the overestimation of b1 and underestimation of b3 seem to be contrary to the findings of the extreme values underestimation of b1 and overestimation of b3 in b1 rainfall is split uniformly from time steps with 15min duration to 5min duration this leads to an underestimation of the extreme values with 5min duration but results in continuous rainfall events in b3 extreme values are overestimated but rainfall events may be interrupted by dry intervals concerning the total flood volume the continuity of extreme rainfall events seems to have a greater influence than short extreme values the resampled time series of b2 show the best fit to the observations with an underestimation of 20 but only slightly better than b3 for the smaller events tk 0 9years using observed rainfall does not lead to any flooding for all simulations also for all non resampled time series no nodes are flooded however this should not be interpreted as being a good fit of all disaggregated time series without the subsequent resampling an additional investigation of the combined sewer overflow volume is carried out to analyze this possible interpretation nevertheless it should be noted that nodes are flooded after the resampling the highest total volume occurs for a the lowest for b2 a higher total flood volume of b3 in comparison to b2 can be identified for the smaller event and vice versa for the higher event this is consistent with the relationship of the extreme values for short durations see fig 9 in fig 16 the corresponding combined sewer overflow volumes during the events with return periods of tk 4 4years and tk 0 9years are shown for the non resampled time series almost no overflow occurs which results in high underestimations in comparison to the observed time series concerning the resampled time series for the event with tk 4 4years the results are quite similar to the total flood volumes of the nodes a and b1 lead to overestimations while b2 and b3 show underestimations the best fit can be identified with a slight underestimation of 3 4 for b2 for the smaller event tk 0 9years all methods show underestimations again combined sewer overflow volume cannot be reproduced by the time series without spatial consistence after the resampling a leads to the strongest underestimation while the results for b1 b2 and b3 are similar about 21 underestimation it should be noted that although no nodes were flooded by these events an overflow volume of the tank occurs which is underestimated by all disaggregation methods this can be explained by fraction of dry time steps during the event table 5 for the three observed extreme events no dry time steps occur at the master station while for the resampled time series the fraction ranges between 18 b1 and 31 for the other two stations the fraction of dry time steps are even stronger overestimated with 50 66 in comparison to 6 for the observations so although the total rainfall amount of an extreme event is well represented the distribution of the rainfall amount in the event can have a significant influence on the modeling results hence the validation of applicability of extreme values from generated time series cannot be carried out without urban hydrological modeling the higher rainfall intensity of single time steps due to a higher fraction dry time steps in an extreme event leads to an overestimation of the flood volume for all disaggregation methods for the observations the water elevation in the system remains completely below the manhole cover level hence no delayed flows occur and the whole water volume reaches the tank immediately causing the combined sewer overflow 5 conclusions and outlook in this study two different investigations were presented to generate 5min rainfall time series and apply them for urban hydrologic modeling first three modifications of the multiplicative random cascade model are introduced all three modifications are further developments of the uniform splitting approach introduced by müller and haberlandt 2015 a branching number b 3 is applied in the first disaggregation step while for all disaggregation steps b 2 is applied to achieve a final resolution of 5min different methods are tested implemented at the disaggregation level 6 called b1 7 b2 and 8 b3 which represent 15 7 5 and 3 75min intervals respectively in b1 a uniform splitting is applied while for the other two the final resolution is achieved by linear interpolation the performance of the model was compared with observed values from lower saxony germany and an existing modification of the cascade model the so called 1280min approach called a different criteria regarding time series statistics and extreme values were taken into account for the evaluation molnar and burlando 2008 analyzed the influence for parameter estimation using 1440min starting and ending of each day were discarded or continuous usage of 1280min and could find no large difference however it could be shown that for the disaggregated time series differences are crucial the investigations have been carried out twice once taking all values of the time series into account and once including a threshold to neglect the influence of single tips in the observed time series and too small rainfall intensities in the disaggregated time series the following conclusions can be drawn 1 method a is outperformed by b1 b2 and b3 regarding wet and dry spell duration average intensity lag 1 autocorrelation and fraction of dry intervals for wet spell amount a and b3 show better results than b1 and b2 taking into account all time steps with the threshold cleaned time series best results are provided by method b1 and b2 2 extreme values of 5min duration are highly overestimated by a and b3 b1 shows slight underestimations while b2 tends to overestimate the observations slightly for extreme values of 1h duration only minor differences can be identified 3 altogether b2 shows the best performance most of the characteristics are only slightly overestimated wet spell amount 8 dry spell duration 8 average intensity 11 and fraction of dry intervals 1 wet spell durations are slightly underestimated 3 the relative root mean square error for extreme values of 5min duration is rrmse 16 the overestimation for extreme values with 5min duration can also be found by onof et al 2005 for b3 molnar and burlando 2005 show also overestimations resulting from a while in licznar et al 2011a extreme values of 5min duration are underestimated the autocorrelation of the time series is underestimated by all methods lombardo et al 2012 proved that the autocorrelation cannot be reproduced by the micro canonical cascade model an underestimation of the autocorrelation was identified before by e g olsson 1998 güntner et al 2001 pui et al 2012 and paschalis et al 2012 2014 lisniak et al 2013 show a good representation of the autocorrelation function for a validation period while for the calibration period underestimations occur for all lags rupp et al 2009 analyzed four different kinds of cascade models depending on the model choice autocorrelation function was under or overestimated one possibility to improve the results of method a could be a dressed cascade model see schertzer et al 2002 paulson and baxter 2007 which includes a continuation of the disaggregation process to very fine time scales and subsequently aggregation to the scale of interest another possibility would be to implement an additional disaggregation step from 5 625min to 2 8125min and subsequently an averaged weighting to achieve a final resolution of 5min since the disaggregation process is carried out station based spatial consistence is missing in between the disaggregated time series the implementation of spatial consistence for 5min rainfall is the second novelty of this study a resampling algorithm is applied for the implementation of spatial consistence müller and haberlandt 2015 defined by the distance dependent bivariate characteristics probability of occurrence pearson s coefficient of correlation and continuity ratio wilks 1998 these characteristics have been analyzed before and after the resampling procedure 4 the resampling algorithm is capable of implementing spatial consistence for time series with 5min resolution 5 the probability of occurrence and pearson s coefficient of correlation could be improved significantly continuity ratio shows a slight worsening the disaggregated time series with and without spatial consistence as well as observed time series have been used as input for an artificial urban hydrological system the main findings are 6 using spatial uniform rainfall one station as input does not ensure an adequate representation of node flooding and tank overflow therefore spatial heterogeneous rainfall three stations has been applied for all further simulations 7 the resampled time series lead to comparable results to those from the observations without resampling unrealistic results regarding the volume of flooded nodes or combined sewer overflow volume occur 8 the resampled version of b2 leads to the best fit to observations method a results in the worst representation among all resampled versions the overall performance of b2 was better in comparison to a although the latter one can be seen as the standard disaggregation variant of the cascade model for urban hydrological investigations licznar et al 2011a b 2015 molnar and burlando 2005 serinaldi 2010 paschalis et al 2014 the potential of the resampling algorithm has been proven for time series with 5min resolution however the reproduction of bivariate characteristics can be improved further investigations of the proposed methods should be carried out for other regions with different rainfall characteristics and climate acknowledgements first of all the associated editor and two anonymous reviewers are gratefully acknowledged their suggestions and comments helped to improve the manuscript significantly the authors thank the students jonas legler for calibration of the simulated annealing parameters and bartosz gierszewski for testing some validation methods and figure preparation thanks also to ana callau poduje and ross pidoto for useful comments on an early draft of the manuscript a special thank to bastian heinrich for figure preparation and technical support during the investigation we are also thankful for the permission to use the data of the german national weather service deutscher wetterdienst dwd and stadtentwässerung braunschweig gmbh this study is part of the project synopse funded by the german federal ministry of education and research bmbf funding number 033w002a appendix a see table a1 
7592,rainfall time series of high temporal resolution and spatial density are crucial for urban hydrology the multiplicative random cascade model can be used for temporal rainfall disaggregation of daily data to generate such time series here the uniform splitting approach with a branching number of 3 in the first disaggregation step is applied to achieve a final resolution of 5min subsequent steps after disaggregation are necessary three modifications at different disaggregation levels are tested in this investigation uniform splitting at δt 15min linear interpolation at δt 7 5min and δt 3 75min results are compared both with observations and an often used approach based on the assumption that a time steps with δt 5 625min as resulting if a branching number of 2 is applied throughout can be replaced with δt 5min called the 1280min approach spatial consistence is implemented in the disaggregated time series using a resampling algorithm in total 24 recording stations in lower saxony northern germany with a 5min resolution have been used for the validation of the disaggregation procedure the urban hydrological suitability is tested with an artificial combined sewer system of about 170hectares the results show that all three variations outperform the 1280min approach regarding reproduction of wet spell duration average intensity fraction of dry intervals and lag 1 autocorrelation extreme values with durations of 5min are also better represented for durations of 1h all approaches show only slight deviations from the observed extremes the applied resampling algorithm is capable to achieve sufficient spatial consistence the effects on the urban hydrological simulations are significant without spatial consistence flood volumes of manholes and combined sewer overflow are strongly underestimated after resampling results using disaggregated time series as input are in the range of those using observed time series best overall performance regarding rainfall statistics are obtained by the method in which the disaggregation process ends at time steps with 7 5min duration deriving the 5min time steps by linear interpolation with subsequent resampling this method leads to a good representation of manhole flooding and combined sewer overflow volume in terms of hydrological simulations and outperforms the 1280min approach keywords precipitation rainfall disaggregation cascade model spatial consistence urban hydrology swmm 1 introduction rainfall time series of high temporal resolution and sufficient station density are crucial for urban hydrology schilling 1991 bruni et al 2015 analyzed the influence of spatial and temporal resolution of rainfall on intensities and simulated runoff with decreasing resolution the variability of rainfall intensities was reduced and the runoff behavior changed regarding maximum water depth and runoff peaks emmanuel et al 2012 extracted different types of rainfall from radar data in western france with 5min resolution and analyzed their spatial extension with variograms they found that for an adequate spatial representation rainfall gauges should have a maximum distance of 6 5km for light rain events and 2 5km for showers berne et al 2004 found similar values by an investigation of intensive mediterranean rainfall events they recommended temporal resolutions of 3 6min and a station density of 2 4km for urban catchments with an area of 1 10km2 ochoa rodriguez et al 2015 analyzed the effect of different combinations of temporal 1 10min and spatial 100 3000m resolutions for different catchments up to drainage areas of 8 7km2 they found that for drainage areas greater than 100ha spatial resolution of 1km is sufficient if the temporal resolution is fine enough 5min radar measured rainfall data would meet these requirements unfortunately the direct measurement with a radar device is not possible only reflected energy from hydrometeors at a certain height above the ground can be measured thus radar data can be affected by different sources of errors e g variations in the relationship between reflected energy and rainfall intensity depending on rainfall type changes in the precipitation particles before reaching the ground anomalous beam propagation and attenuation wilson and brandes 1979 hence it could be expected that the use of uncorrected radar data is not acceptable for many hydrological applications however after correction radar data can be an useful input for e g urban hydrological applications ochoa rodriguez et al 2015 on the other hand direct rainfall measurement is possible with rain gauges the measurement is also affected by errors e g wind induced errors losses from surface wetting or evaporation from collectors but these can be quantified and corrected richter 1995 sevruk 2005 however the availability of observed time series for rain gauges meeting the aforementioned requirements concerning temporal and spatial resolution is rare on the contrary time series with lower temporal resolution e g daily measurements exist for much longer periods and denser networks disaggregation of the time series from these non recording stations using information from the recording stations is a possible solution to this data sparseness problem several methods are available that can be used for this disaggregation e g method of fragments e g wójcik and buishand 2003 poisson cluster models e g onof et al 2000 cascade models or a combination of different methods e g paschalis et al 2014 for a theoretical introduction to cascade models and the underlying theory of scale invariance the reader is referred to serinaldi 2010 and the reviews of veneziano et al 2006 veneziano and langousis 2010 and schertzer and lovejoy 2011 an advantage of micro canonical cascade models is their exact conservation of rainfall volume of the coarse time series at each disaggregation step olsson 1998 the total rainfall amount of each coarse time step is distributed on the number of finer time steps whereby the number of resulting wet time steps and their rainfall amount as fraction of the total rainfall amount of the coarser time step depends on the cascade generator an aggregation of the disaggregated time series results in exactly the same time series that was used as a starting point for the disaggregation accordingly all required parameters for the disaggregation process can also be estimated from the aggregation of the recording time series carsteanu and foufoula georgiou 1996 and can then be applied to the disaggregation of time series from surrounding non recording stations koutsoyiannis et al 2003 cascade models are widely applied in urban hydrology one structural element of the cascade models is the branching number b which determines the number of finer time steps generated from one coarser time step in most cases b 2 which means that a starting length of 24h will result in an inapplicable temporal resolution of 11 25 or 5 625min so a direct disaggregation from daily values to 5 or 10 min values is not possible one established solution is to preserve the rainfall amount of a day but the length is reduced to 1280min instead of 1440min molnar and burlando 2005 2008 paschalis et al 2014 licznar et al 2011a b 2015 under this assumption temporal resolutions of 5 or 10 min can be achieved however this is a very coarse assumption and missing time steps have to be infilled with dry intervals for applications such as the continuous modeling of sewer systems if b is changed during the disaggregation process other temporal resolutions can be achieved lisniak et al 2013 introduce a cascade model with b 3 to reach the first and b 2 to reach all further disaggregation levels which leads to time steps with 1h duration a parameter sparse version of their model is applied in müller and haberlandt 2015 the application of cascade models for generation of high resolution time series is recommended by several authors segond et al 2007 suggest a cascade model for the disaggregation of hourly time series although other methods were also tested in their investigation onof et al 2005 disaggregated hourly values with b 2 to 3 75min intervals and transformed them into 5min intervals by a linear interpolation similar to the diversion process to achieve hourly resolutions described in güntner et al 2001 hingray and ben haha 2005 tested several models for disaggregation from 1h to 10min micro canonical models have shown best performance regarding rainfall statistics but also for modeled discharge results considering these previous studies the testing of different cascade model variations for the disaggregation of daily values is the first novelty of this investigation the impact on time series characteristics like average wet spell duration and amount dry spell duration fraction of dry intervals average intensity autocorrelation function and also extreme values will be analyzed furthermore the impact on overflow occurrence and volume within an artificial sewer network will be analyzed although rainfall time series are often disaggregated for this purpose analyzes of the impact on runoff in sewer systems are rare an artificial sewer system in combination with real recording rainfall gauges will be used for this study as time series of different stations are disaggregated without taking into account time series of surrounding stations this per station or single point procedure results in unrealistic spatial patterns of rainfall since the spatial resolution is also important for high resolution rainfall berne et al 2004 emmanuel et al 2012 the spatial distribution of rainfall has to be considered müller and haberlandt 2015 introduced a resampling algorithm to implement spatial consistence into time series after disaggregation for hourly resolutions a similar procedure will be used in this study for the first time to the author s knowledge for 5min values also it will be analyzed if there is a necessity for spatial heterogeneous rainfall resulting from more than one station to represent extreme events in such a small catchment or if uniform rainfall resulting from one station is sufficient the paper is organized as follows in the data section the investigation area the rainfall stations and the artificial sewer system are described in the next section the applied methods are discussed in three parts in the first part different possibilities for the rainfall disaggregation are explained the second part concerns the implementation of spatial consistence in the disaggregated time series the implementation in the sewer system and the analysis of the model results are described in the third part in the results and discussion section the results for all three parts are shown and discussed a summary and outlook are given in the final section 2 data 2 1 rainfall data the stations used for this investigation are located in and around lower saxony germany 47614km2 see fig 1 the investigation area can be divided into three different regions the harz middle mountains with altitudes up to 1141m in the south the coastal area around the north sea in the north and the flatland around the lüneburger heide in between some areas of the harz mountains have average annual precipitation greater than 1400mm furthermore the study area can be divided climatologically according to the köppen geiger classification into a temperate climate in the north and a cold climate in the mountainous region both climates exhibit hot summers but no dry season peel et al 2007 in fig 1 24 recording stations from the german weather service dwd with long term time series 9 20years are shown these stations are used for the validation of the cascade model variants concerning the representation of different rainfall characteristics these are overall characteristics like average intensity and fraction of wet hours but also event characteristics like dry spell duration wet spell duration and wet spell amount as well as extreme values events are defined as having a minimum of one dry time step before and after the rainfall occurrence after dunkerley 2008 a dry time step is defined with a rainfall intensity of 0mm 5min the measurement devices are either tipping buckets drop counters or weighting gauges with accuracies of 0 1mm or 0 01mm time series of single stations from different data bases with different temporal resolutions 1 and 5min have been combined to extend their time series length so the resulting temporal resolution is 5min this enables also comparisons with previous microcanonical cascade studies see discussion in licznar et al 2015 these characteristics and further information of the rainfall stations are given in table 1 additionally five recording stations from the city of brunswick are used see table 1 station i v these stations are not shown in fig 1 due to their proximity with distances to each other less than 5km however the city of brunswick is shown the time series lengths of these stations are shorter 02 01 2000 24 12 2006 so that they have not been used for the validation of the cascade model variants due to their proximity these stations are used for the estimation of the bivariate characteristics for the validation of the resampling algorithm and as input for the validation within an urban hydrological model the measurement devices are tipping buckets with accuracies of 0 1mm and temporal resolutions of 1min however to enable comparisons to the aforementioned stations the time series are aggregated to 5min 2 2 combined sewer system an artificial combined sewer system was constructed in order to compare the different disaggregation approaches fig 2 the application of artificial systems is a common approach for the validation of synthetic rainfall kim and olivera 2012 the sewer system consists of 22 sub catchments with a mean area size of 7 6ha ranging from 1 1ha to 16ha with a standard deviation of 4 6ha and a cumulative total of 168 1ha each catchment has a fraction imperviousness of 65 a uniform slope of 0 25 is used for all pipes a tank before the outfall of the sewer system was implemented it has a storage capacity of 2184m3 equal to 20m3 per hectare of impervious area and is a typical value after imhoff and imhoff 2007 three rain gauges are implemented they have been arranged with the same distances to each other as three rain gauges in the city brunswick station i ii and iv this allows a direct comparison between simulated discharges and flood volumes resulting from observed and disaggregated time series with a maximum distance of about 3km for these three stations the rainfall data meets the requirements suggested by berne et al 2004 and emmanuel et al 2012 additionally each of the rain gauges influences similar fractions of the sewer system station i 32 7 station ii 31 2 station iv 36 0 whereby to each subcatchment only the one rain gauge is assigned which is closest to the centroid of the subcatchment see fig 2 if one of the rain gauges would be situated more central than the other two it would affect a higher fraction of the area due to a higher distance of other stations if so the rainfall input would be uniform for also a higher fraction of the area so due to the central position of all three rain gauges the effect of the spatial consistence will be emphasized 3 methods in this section the cascade model for the disaggregation of the rainfall time series will be described first afterwards the implementation of the spatial consistence and the application to an artificial urban hydrological case are explained 3 1 cascade model the principle of a multiplicative micro canonocal cascade model as it was introduced by olsson 1998 for temporal rainfall disaggregation is illustrated in fig 3 one coarse time step is split into b finer time steps of equal duration where b is the branching number with b 2 in fig 3 for the splitting the weights w1 and w2 are used to determine the rainfall volume in the two finer time steps the sum of w1 and w2 is 1 in each split so that the rainfall volume is conserved exactly an aggregation of the disaggregated rainfall would result in the same time series that has been used for the disaggregation possible combinations of w1 and w2 are given in 1 the so called cascade generator 1 w 1 w 2 0 and 1 with p 0 1 1 and 0 with p 1 0 x and 1 x with p x 1 x 0 x 1 where p is the probability of each combination of weights the probability p 0 1 denotes a splitting with no rainfall volume assigned to the first time step w1 and 100 of the rainfall volume w2 1 w1 in the second time step the probability p 1 0 causes a vice versa result the rainfall volume could also be distributed over both time steps with a x 1 x splitting the relative fraction x of rainfall assigned to the first time step is defined as 0 x 1 considering x as a random variable for all disaggregation steps a probability density function f x with the empirical probabilities for each value of x is estimated theoretical density functions are not fitted it has been shown by several authors that the parameters are volume and position dependent olsson 1998 güntner et al 2001 rupp et al 2009 for the cascade model in general four different positions in the time series starting enclosed isolated ending and two volume classes for each position are used güntner et al 2001 analyzed different thresholds for the differentiation of the two volume classes they proved that the mean rainfall intensity of all rainfall intensities of the actual cascade level for one position is an acceptable threshold for this differentiation it has therefore also been applied in this study the probabilities are shown exemplary for rain gauge göttingen in appendix a for aggregation steps from 5min to 1280min marshak et al 1994 analyzed estimated parameters based on different aggregation levels and compared them differences between the parameter sets are significant and should be taken into account especially if the disaggregation process is carried out over a high number of cascade levels furthermore it can be distinguished between unbounded and bounded cascade models in unbounded cascade model the parameters are assumed to be scale independent which means that the same parameter set is applied over all disaggregation levels however it was found that the cascade generator exhibits a scale dependency see serinaldi 2010 and references therein so for each disaggregation step a different parameter set has to be applied it should be mentioned that rupp et al 2009 found only minor improvements using scale dependent parameter sets by an application over a small range of time scales daily to hourly although unbounded cascade models are still used jebari et al 2012 bounded cascade models can be found more frequently in the recent literature rupp et al 2009 licznar et al 2011a b lombardo et al 2012 lisniak et al 2013 this increases the amount of parameters with each disaggregation level the similarity of p 0 1 for starting boxes and p 1 0 for ending boxes and vice versa as well as p 0 1 p 1 0 for enclosed and isolated boxes güntner et al 2001 could be used to reduce the parameter count in order to keep the model as parameter parsimonious as possible however to isolate the effects of the approaches described below this was not applied in this study for the sake of completeness it should be mentioned that other cascade models less parameters exist e g universal multifractals with three parameters valid on various successive cascade steps the interested reader is referred to the review of schertzer and lovejoy 2011 however a multiplicative micro canonical bounded cascade model is applied for this study in most cases disaggregation starts with daily values from time series of non recording stations since the network density is higher and time series are longer for these types of stations in comparison to recording stations however with the method explained above a final resolution of 5min cannot be achieved directly one day lasts per definition 24h which is 1440min in total using a branching number of b 2 throughout the whole disaggregation process results in temporal resolutions of 5 625min after eight disaggregation steps this is not a very useful resolution if these time series are to be used for further applications such as urban water management models different alternatives to reach a 5min resolution are explained and discussed in the following two subsections method a 1280min approach one possibility is to assume that the complete daily rainfall amount occurs in only 1280min using this assumption a final resolution of 5min can be achieved by a complete conservation of the rainfall amount this assumption is common for generating rainfall for urban applications licznar et al 2011a b 2015 molnar and burlando 2005 serinaldi 2010 paschalis et al 2014 however the final time series length is reduced due to this underlying assumption there exist different possibilities how to avoid this reduction e g inserting missing time steps as dry time steps in each day or only between two successive dry days however each of these methods would directly influence the time series characteristics for further processing the disaggregated shortened time series was used without any changes another alternative would be a disaggregation down to a very fine scale and then aggregate time steps to the temporal resolution most similar to 5min possible disaggregation levels would be 42 2s 11 disaggregation steps respectively 5 3s 14 disaggregation steps which can be aggregated to 4 922min respectively 5 01min however the scale dependent parameters cannot be estimated from observations for this high resolution so this alternative could not be applied in the investigation method b uniform splitting approach the uniform splitting approach was introduced by müller and haberlandt 2015 and uses a branching number b 3 only in the first disaggregation step resulting in three 8h intervals one two or all three of these 8h intervals can be wet the probabilities for the number of wet intervals can also be estimated from observations see table 2 the threshold for the volume classes in this first step is a quantile q chosen with respect to very high daily rainfall intensities q 0 998 as in müller and haberlandt 2015 the probabilities in table 2 show a clear dependency on the volume class while for the lower volume class the probability is highest for one wet 8h interval for the upper volume class it is for three wet 8h intervals due to the chosen threshold only a small number of wet days are used for the parameter estimation for the upper volume class for the majority of those days all three 8h intervals are wet only a minority if at all shows just one or two wet 8h intervals this confirms the findings of müller and haberlandt 2015 and underlines the importance of the implementation of an upper volume class the application of b 3 in the first disaggregation step has been done before by lisniak et al 2013 to achieve a target resolution of 1h which is a suitable temporal resolution as input for rainfall runoff models for rural catchments the daily rainfall volume is distributed uniformly among the number of randomly as wet identified 8h intervals this uniform distribution on the before chosen number of wet 8h intervals is a compromise between the parameter intensive approach proposed by lisniak et al who have used up to 8 empirical distribution functions to represent the splitting behavior from daily to 8h time steps and the quality of the disaggregation results for the second and all following disaggregation steps the branching number is reduced to b 2 it is obvious that to achieve a target resolution of 5min additional modifications have to be introduced the disaggregation levels delivering time steps of δt 15 7 5 or 3 75 min therefore were used to introduce these modifications it should be mentioned that the parameters for the disaggregation steps for resolutions finer than 15min with b 2 cannot be estimated from observations directly since only 5min values are available for disaggregations from 15 to 7 5 and consequently 3 75min the parameter set from the aggregation of 5 to 10min was used throughout method b1 modification for δt 15 min similar to the first disaggregation step a uniform splitting with a branching number b 3 is applied the threshold for the differentiation into two volume classes was chosen as the mean of all rainfall intensities at the 15min level the parameters of the two volume classes differ significantly from each other method b2 modification for δt 7 5 min firstly the rainfall volume of each time step is distributed uniformly on three time steps with 2 5min afterwards two non overlapping time steps are aggregated always method b3 modification for δt 3 75 min the applied method is similar to b2 and was used by onof et al 2005 and onof and arnbjerg nielsen 2009 for the disaggregation from 1h down to 5min the rainfall volume is distributed uniformly on three finer time steps with 1 25min duration followed by an aggregation of four non overlapping time steps the disaggregation is a random process which leads to different results depending on the initialization of the random number generator this random behavior is covered by a certain number of disaggregation runs it was found that after 30 disaggregation runs the average values of the main characteristics see section 4 1 temporal disaggregation did not change significantly by an increasing number of disaggregation runs accordingly 30 disaggregations were carried out for each method a comparison of the rainfall characteristics rc of disaggregated time series dis with the observations obs is carried out regarding the relative error re the objective criterion is calculated for each station i over all realizations n of the disaggregation and averaged afterwards over all stations 2 re 1 n i 1 n rc dis i rc obs i rc obs i for the investigation of the rainfall extremes resulting from the different methods partial duration series also known as peaks over threshold are extracted for all time series the threshold is chosen in a way to obtain two values per year on average from each time series dwa a 531 2012 based on 30 realisations for each method the median of the extreme values was determined for further processing since the time series lengths of the stations differ also elements in the partial duration series and hence return periods are different in order to include all stations in an objective comparison of extreme values an exponential distribution was fitted to the medians the exponential distribution which is a standard distribution function in germany for partial duration series of rainfall dwa a 531 2012 was chosen for this purpose the deviations of the fitted extreme values between disaggregated and observed rainfall are calculated using the relative root mean square error rrmse the deviations between disaggregated and observed rainfall intensities i for single return periods t of the fitted distribution function for each station i are calculated and averaged afterwards over all stations n 3 rrmse t 1 n i 1 n i t dis i i t obs i 2 i t obs i with t 0 5 1 2 5 10 years this criterion was used for the evaluation of rainfall disaggregation products before by güntner et al 2001 for single stations they evaluate deviations of up to 10 as accurately generated up to 15 as well reproduced and higher than 40 as overestimated respectively more than 200 as severe overestimated these values are based on the disaggregation results in their manuscript and are provided here to give a general feeling for the criterion as well as for the re whose absolute values are comparable to rrmse 3 2 spatial consistence the disaggregation of the rainfall time series is a pointwise procedure this yields unrealistic spatial patterns of rainfall a resampling procedure introduced by müller and haberlandt 2015 is applied to implement spatial consistence into the disaggregated time series z three bivariate characteristics are assumed to represent spatial consistence namely probability of occurrence coefficient of correlation and continuity ratio wilks 1998 1 probability of occurrence the probability of occurrence pk l describes the probability of rainfall occurrence at two stations k and l at the same time 4 p k l z k 0 z l 0 n 11 n where n is the total number of non missing observation hours at both stations k and l and n11 represents the number of simultaneous rainfall occurrence at both stations a differentiation for convective and stratiform events as in cowpertwait 1995 was not applied 2 pearson s coefficient of correlation the pearson s coefficient of correlation is used to describe the relationship between simultaneously occurring rainfall at two stations k and l it is a measure of the linear relation between both rainfall time series eq 4 this coefficient was used previously for multisite rainfall generation by breinl et al 2013 2014 5 ρ k l cov z k z l var z k xvar z l z k 0 z 1 0 3 continuity measure the continuity measure compares the expected rainfall amount at one station for cases with and without rain at the neighboring station e is the expectation operator 6 c k l e z k z k 0 z l 0 e z k z k 0 z l 0 it is possible to estimate the prescribed values of these characteristics as functions of the separation distance between two stations from observed data see fig 4 for the estimation all stations from the city brunswick are used stations i v from table 1 the results for stations 1 24 are not shown here because the spatial scale with distances up to 350km is not useful for the aim of urban hydrologic modeling the resampling procedure involves a bivariate objective function ok l that has to be minimized 7 o k l w 1 p k l p k l w 2 ρ k l ρ k l w 3 c k l c k l the parameters indicated by are the prescribed values for two stations resulting from the regression lines in fig 4 and the other parameters are the actual values the fit of the regression lines to the observations are also validated with pearson s coefficient of correlation to avoid confusion with the spatial characteristic here the symbol r 2 is used for the goodness of fit while for the probability of occurrence and the continuity ratio only a small r 2 value is achieved due to the variation of these parameters for the coefficient of correlation a r 2 value of 0 8462 is determined however the general behavior for all three characteristics increasing or decreasing with increasing distance can be represented by the regression lines the weights w1 w2 and w3 are necessary to adjust the scale of the rainfall characteristics and to consider their importance the regression lines are necessary to determine prescribed values for distances not available from the recording rain gauges for the minimization a resampling algorithm namely simulated annealing is implemented kirkpatrick et al 1983 aarts and korst 1965 the algorithm has been used for rainfall generation processes before bardossy 1998 haberlandt et al 2008 for the resampling two conditions are considered the structure of the disaggregated time series combination of position and volume classes should not be changed also the rainfall amount of each day should not be changed therefore only the relative diurnal cycles of the disaggregated time series are resampled preserving the structure of the disaggregated time series described by position and volume class of the daily time step one disaggregated time series is chosen randomly as a reference time series the relative diurnal cycles of a second randomly chosen time series will be resampled until the objective function is minimized so it cannot be improved for a certain number of attempts swaps are only possible between days with the same position and volume class at the beginning of the algorithm also bad swaps worsening of the objective function are accepted with a certain probability π the parameter π decreases with the run time of the resampling algorithm this allows to leave a local optimum to find the global minimum after the resampling the time series serves as a new reference station along with the first one further time series have to be resampled with respect to all reference time series and will be added afterwards to the set of reference time series for detailed information on the applied resampling algorithm the reader is referred to müller and haberlandt 2015 since the cascade model is based on the scaling theory it could be questioned if there is a discrepancy in the temporal dimension of the disaggregation process due to the spatial dimension of the resampling algorithm therefore scaling behavior of the disaggregated time series is analyzed before and after the resampling with the relation described in eq 8 8 m q λ k q with moments m moments order q the moments scaling exponent k q and the scale ratio λ analyzing the scaling behavior with log log plots of mq and λ indicating different durations is a common method in the field of rainfall disaggregation with cascade models see e g over and gupta 1994 svensson et al 1996 burlando and rosso 1996 serinaldi 2010 the scale ratio represents a dimensionless ratio of two temporal resolutions of one time series dry time steps are neglected for the scaling analyzes for the moments estimation probability weighted moments pwm as in yu et al 2014 and ding et al 2015 are applied an advantage of the pwm is their relative robustness against large rainfall intensities kumar et al 1994 hosking and wallis 1997 according to kumar et al 2014 and lombardo et al 2014 the investigation is limited to 1 q 3 the pwm of different temporal resolutions will be compared before and after the resampling process 3 3 urbanhydrological modeling 3 3 1 model swmm the sewer system has been constructed in the epa storm water management model 5 1 swmm rossman 2010 with swmm dynamic rainfall runoff simulations can be carried out continuously or event based to determine runoff quantity and quality from urban areas a constructed sewer system in swmm is split horizontally into subcatchments each subcatchment is characterized by a number of parameters e g total area fraction of impervious area and slope connecting main pipes of the subcatchments are represented by links furthermore the sewer system can be complemented by storage treatment devices pumps and regulators the application of a semi distributed model like swmm limits the ability to reflect spatial variability due to the fact that for each subcatchment the rainfall is assumed to be uniform for its area however since disaggregated time series from rain gauges are used in this study there is no degradation of spatial variability nevertheless it could be a negative aspect especially when high resolution spatial rainfall data is used gires et al 2015 for each subcatchment flow is generated from both dry weather flow representing domestic and industrial wastewater flow and rainfall runoff from pervious and impervious areas rainfall is assumed to be uniform for each subcatchment for infiltration the equations of modified horton green ampt and curve number are selectable here the curve number equation was used the dynamic wave equation as approximation of the st venant equations is used for the calculation of the flows through the sewer system as forcing main equation for the dynamic wave hazen williams equation has been chosen pondage of flooded nodes can be allowed or not here it was permitted overland flow does not occur 3 3 2 influence of number of recording stations it could be questioned whether one station is enough to represent rainfall for such a small catchment 168 1ha so the impact of one or more implemented stations we apply three stations on the combined sewer system runoff has to be analyzed for this analysis the following procedure was applied using observed data only for three stations the partial duration series of extreme values were derived as described in section 4 1 for the extremes of the other stations in lower saxony this results in 14 total extreme values for each station using the time period 01 01 2000 24 12 2006 that is available for the stations prinzenweg bürgerpark und weststadt the return periods tk of these extreme values can be calculated with the weibull plotting position comparisons are carried out for extreme values with 30min duration and for return periods of tk 4 4years and tk 0 9years respectively these are representative return periods for the dimensioning of sewer system elements dwa a 118 2006 din en 752 2 1996 if only one station is used the extreme value is considered to be uniform throughout the whole catchment this procedure is carried out for the extremes of each station so in total three extreme values are analyzed for each return period if three stations are used the time steps of an extreme value of one station this station is the so called master station and the simultaneous time steps from the other two stations are used as spatial heterogeneous input again the procedure is carried out for each station as a master station in total three events are analyzed here events are defined as an extreme value at the master station and simultaneous time steps at the other two stations 3 3 3 influence of disaggregation method the aim of the second investigation based on the sewer system is to investigate if there is a need for the implementation of spatial consistence using more than one station in addition to the choice of the disaggregation method therefore three rainfall stations are implemented throughout for each disaggregation method the time series of all 30 realizations are resampled res to implement spatial consistence for each method a a res b1 b1 res b2 b2 res b3 b3 res and each realization 1 30 the partial duration series of extreme values were derived again for one event the time steps of an extreme value of one station and the simultaneous time steps from the other two stations are used as spatial heterogeneous input as carried out for three stations before see section 3 3 2 in total 90 events for each return period based on the 30 realizations of each method are used for simulation 4 results and discussion the results section is organized as follows the univariate rainfall characteristics of the disaggregation will be discussed in section 4 1 while the multivariate characteristics will be analyzed in section 4 2 all results of the urban hydrological modeling will be discussed in section 4 3 4 1 temporal disaggregation in total four different variations of the micro canonical cascade model were tested to disaggregate time series from daily to 5 min values seven basic rainfall characteristics were chosen for analyzes wet spell duration and amount average intensity dry spell duration fraction of dry intervals autocorrelation and extreme values a wet period is defined as the duration with rainfall volume continuously greater than 0mm in each time step the rainfall characteristics are illustrated in figs 5 8 and in table 3 as observations vs disaggregations for each station and as averages resulting from eq 2 molnar and burlando 2005 and müller and haberlandt 2015 identified a high fraction of rainfall intensities in the disaggregated time series smaller than the accuracy of the measuring instrument and hence the minimum resolution in the observed time series which have been used for the parameter estimation these time steps are from a hydrological point of view negligible to reduce the impact on the results due to these small time steps an additional analyzes of the rainfall characteristics with a minimum intensity of higher than 0 1mm as threshold was introduced this threshold value has additionally the advantage to exclude single tips from the measurement device the results are shown in fig 6 for stations with wet spell durations wsd shorter than 17 5 min fig 5 method b2 and b3 show acceptable over underestimations the results are similar for b2 for wet spell durations shorter than 14min if a threshold is taken into account fig 6 for stations with longer wsd all methods show underestimations respectively an underestimation of the wsd was identified before by olsson 1998 this can be explained by the definition of a wet period every dry spell regardless of its length terminates a wet spell so the reproduction of wsd becomes more and more complicated with an increasing length of observed wsd because a single dry time step would divide a long wet spell into two shorter wet spells the generation of dry intervals depends on the probabilities of p 1 0 and p 0 1 these probabilities are significant lower for all positions for the higher volume class in comparison to the lower corresponding volume class see appendix a the influence of the rainfall intensity on the generation of dry intervals has been identified before by rupp et al 2009 it can also be confirmed that the variation of the probabilities is higher between the volume classes in comparison to the variation between different scales the results for the average intensity also show a clear structure and trend if no threshold is taken into account all methods overestimate the intensities with the largest deviations shown for method a 63 on average one reason for this lies in the reduction of the day duration by 160min 2 7h 1440 1280min if a threshold is introduced deviations are smaller for all methods method b2 shows the smallest deviations for the average intensity in both analyzes the results for the wet spell amount wsa do not show a clear trend for all methods like that for wsd and average intensity fig 5 the underestimation of wsd and the overestimation of average intensities compensate each other and lead to a deceptively good fit for method a with only slightly underestimations of wsa method b1 underestimates the observations more strongly while method b2 and b3 overestimate wsa for most of the stations if time steps with small rainfall intensities are neglected different results can be identified fig 6 method a and b3 show strong underestimations while b1 and b2 show acceptable agreements with 3 and 1 respectively it should be noticed that the absolute values of wsd are slightly decreasing with an introduction of the threshold while wsa and average intensity are strongly increasing for the dry spell duration dsd methods b2 and b3 show similar results if all values are taken into account both overestimate the observations by 8 and 5 respectively while method a and b1 underestimate the observations by 13 and 10 respectively with the introduction of the threshold single tips of the measurement device are ignored which increases the dsd significantly all methods lead to underestimations with the worst representation by method a the underestimation of both durations wsd and dsd by method a is a systematic error due to the shortening of the total length of the time series the underestimation of dsd could be reduced by inserting dry intervals for the missing time steps of this method however the fraction of dry intervals is overestimated by method a as well as by the other methods if all values are included the differences between the methods are much smaller in comparison to wsd see table 3 since all dry days which are not influenced by the chosen disaggregation method are also taken into account with the threshold introduction the fraction of dry intervals is underestimated by all methods however the deviations between the methods themselves as well as in comparison to the observations are very small 1 and can be neglected furthermore the autocorrelation function is shown in fig 7 for the observed time series for station harzgerode and as median of the autocorrelation functions resulting from 30 realisations for each method taking into account all values of the disaggregated time series the autocorrelation is underestimated by all methods the relative errors are determined as average of all stations for lag 1 and lag 12 representing temporal shifts of 5min and 1h respectively see table 3 the lag 1 autocorrelation is underestimated by method a with 50 while method b1 3 and b2 4 show smaller underestimations for lag 12 all methods show significant underestimation of the autocorrelation function of approximately 50 the autocorrelation function for b1 shows peaks periodically in a 3 lags distance due to the applied uniform splitting approach for disaggregation of 15min to 5min the underestimation of the autocorrelation function is a well known problem and has been identified before by e g olsson 1998 güntner et al 2001 pui et al 2012 and paschalis et al 2012 2014 a comparison of the observed empirical extreme values with the range the 0 05 and 95 quantileand the median of the 30 disaggregations for station osnabrück is given in fig 8 the median and both quantiles represent typical results for all stations for the illustration the weibull plotting position was used the medians of all 30 realisations for b2 and b3 show a good fit to the observed values while the median for a tends to overestimate the observations the range and quantiles of b2 and b3 are similar while a shows strong overestimations and b1 underestimations respectively for the highest return period t 35years overestimations of the observed values can be identified by a factor of 6 for method a if the range is taken into account for other stations overestimations for method b1 can be identified from the range as well not shown here the results shown regarding median and both quantiles are representative for most of the stations for comparisons exponential distributions were fitted to the median of all realizations for each station rainfall intensities are analyzed for the return periods of 0 5 twice a year 1 2 5 and 10years and for durations of 5min and 1h fig 9 shows the relative errors as box whisker plots for rainfall intensities of 5min duration with return periods of 1 and 5 years for each method method a leads to the highest overestimation for both return periods hence the shortening of the day duration to 1280min also affects the extremes with 5min duration extreme values are also overestimated by b3 due to the disaggregation down to a very fine temporal resolution of 3 75min the splitting of the daily rainfall amount can potentially be reduced onto only a small number of fine time steps this leads to an overestimation of the extreme values concerning the medians b2 overestimates the observations slightly while b1 underestimates them in b1 rainfall at a temporal resolution of 15min is split with one disaggregation step into three finer final time steps while in b2 two disaggregation steps follow this causes a higher intensity of rainfall similar to the overestimation of b3 however the range of rainfall quantiles with 5year return periods resulting from b2 is much higher than for b1 for longer durations the differences between the methods decrease see fig 10 for b1 b2 and b3 the results for 1h are similar because the disaggregation process is exactly the same until this duration minor differences are only caused by adjacent time steps of extreme events for a return period of 1year the median of a b1 b2 and b3 show the same slight overestimation of the observed values for a 5year return period a also leads to a slight overestimation while b1 b2 and b3 underestimate the extreme values of the observations it should be noted that the deviations for 1h rainfall duration are much smaller than for 5min for all methods however the smallest ranges for both return periods result from a which delivers the best performance for an hourly target time step the relative root mean square errors rrmse for all rainfall quantiles and 5min time step are given in table 4 the rrmse for all return periods is highest for method a followed by b3 b2 is slightly higher than b1 for all return periods except 0 5years from a practical point of view using the medians of b2 as design values would lead to a dimensioning on the safe side 4 2 spatial consistence spatial consistence was assumed to be represented by matching three bivariate characteristics namely probability of occurrence pearson s coefficient of correlation and continuity ratio in fig 11 values for these characteristics resulting from the observations after the disaggregation without resampling and after resampling are shown for the coefficient of correlation values similar to the observations could be achieved although before resampling an underestimation independent from station distances could be identified for the continuity ratio the results are more complex to interpret due to the definition of this characteristic eq 6 for each pair of stations two different values exist depending on which station is defined as k and l during the resampling only the time series combinations of the actual station k and the reference stations l are taken into account not vice versa the resulting values are the three well fitted values from the resampled dataset for each distance in fig 11 for higher distances values comparable to those from observations could be achieved for a distance of 1 4km a slight worsening can be identified continuity ratios with values ck l 0 8 represent the combinations that have not been considered during the resampling process these values are worsening by the resampling for all distances an omitting of continuity ratio would cause a worsening of all values for this characteristic müller and haberlandt 2015 so the continuity ratio remained included in the objective function the probability of occurrence was underestimated for the disaggregated non resampled time series for all distances after the resampling all values could be improved significantly however values resulting from the observations could not be reached and are still underestimated due to the shortness of the time series only a limited number of relative diurnal cycles is available the number is too small to find matching relative diurnal cycles especially for a temporal resolution of 5min with 288 time steps each day although a strong weighting of this characteristic in the objective function w 1 0 899 in comparison to w 2 0 002 for pearson s coefficient of correlation and w 3 0 099 for continuity ratio a better fit was not possible however it can be assumed that this characteristic improves with increasing time series length it is further analyzed if the resampling algorithm influences the scaling behavior of the disaggregated time series therefore the first three moments have been calculated for all realizations before and after the resampling the means are shown in fig 12 the first probably weighted moment represents the mean value of the time series it is not changed by the resampling algorithm since the total rainfall amount and the number of wet time steps are not changed however the second and the third moment show slight increases indicating minor changes of the standard deviation and the skewness of the average intensity the increases are stronger for finer temporal resolutions and can be identified for all methods however the deviations from the not resampled time series are smaller than 5 and hence accepted 4 3 urban hydrological modeling for the urban model two investigations are carried out in the first part the necessity for the implementation of more than one rainfall station will be analyzed this investigation is carried out using observed time series only secondly the benefit for the implementation of spatial consistence using more than one station in addition to the choice of the disaggregation method is analyzed for both investigations two criteria will be used for the validation the flood volume represents the water volume that leaves the sewage system temporary through manholes the combined sewer overflow volume is the cumulative volume of the sewage system which is released from the tank to the receiving water and not to the treatment plant it should be mentioned that a variation of the swmm model parameter would influence the simulation results as sensitive parameters have been identified the imperviousness and the surface depression storage barco et al 2008 goldstein et al 2010 also slope and the capacity of the main pipes would affect the resulting flood volumes and combined sewer overflow volume a different tank volume would affect combined sewer overflow volume for more information about the parameter sensitivity the reader is referred to krebs et al 2014 4 3 1 influence of number of recording stations for this investigation only observed time series are used in fig 13 the resulting flood volumes from using one uniform rainfall or three stations heterogeneous rainfall are shown for an observed extreme event at the master station with 30min duration and a return period of tk 4 4years the heterogeneous case is assumed to represent the reference with small differences between results based on different extreme events however there are high deviations using uniform rainfall as input in comparison to spatial heterogeneous rainfall the flood volume is overestimated by 143 on average if only one station is used as input in comparison to using all three stations representing the reference through its spatial coverage however using only one station can lead to larger overestimations of 384 station bürgerpark but also to underestimations station weststadt by 15 the same investigation was carried out for events with a return period of tk 0 9years for all three events no flooding occurs using three stations as input not shown here however using one station leads to flooding for the station weststadt so again an overestimation of flooding volume occurs using spatial uniform rainfall the results for the combined sewer overflow volume the overflow of the tank are similar it is overestimated by using only one station as input results are the same for both return periods fig 14 it can be concluded that one station is not sufficient to represent the rainfall behavior adequately although only the effects for a small catchment 168 1ha are analyzed this confirms the results of e g schilling 1991 berne et al 2004 emmanuel et al 2012 gires et al 2015 bruni et al 2015 and ochoa rodriguez et al 2015 that for the representation of spatial variability of rainfall a high station network density is crucial these results are conformable to the theory of areal reduction factors which should be mentioned in this context the basic idea of this concept is that extreme point values cannot be used uniformly for applications requiring spatial rainfall therefore the point values have to be reduced with an areal reduction factor this is indicated by the results since without a reduction of the point rainfall values overstimations of 67 tk 4 4a respectively 71 tk 0 9a of the combined sewer overflow volume occur comparing the average results of using 1 and 3 stations sivapalan and blöschl 1998 found that these factors depend on the return period of the events and the applied catchment area veneziano and langousis 2005 investigate the areal reduction factors in context of a multifractal analysis for a critical review of areal reduction factors and methods to estimate them the reader is referred to wright et al 2013 for further investigations spatial heterogeneous rainfall is applied throughout the study 4 3 2 influence of disaggregation methods and spatial consistence for this investigation observed time series are used as reference for the validation of the disaggregated time series for the comparison of the disaggregation methods before and after the resampling the flood volumes of all nodes in the sewer system fig 15 and the corresponding combined sewer overflow fig 16 are analyzed event based see section 3 3 3 for the event selection in fig 15 the total flood volume of each method is higher after resampling than before first the results for events with tk 4 4years are discussed after the disaggregation but before the resampling wet time steps are located randomly within the days of the time series after resampling the probability of simultaneously rainfall occurrence is much higher which results in higher areal rainfall amounts and accordingly an increase in flood volume without resampling the total flood volume is underestimated by all methods so if more than one station is used in the urban hydrological model the disaggregated time series have to be resampled without resampling unrealistic spatial patterns occur and the simulated flood volumes are not representable for method a an overestimation of the average rainfall intensity and the extreme values has been identified before also the total flood volume is overestimated after resampling by about 240 the overestimation of b1 and underestimation of b3 seem to be contrary to the findings of the extreme values underestimation of b1 and overestimation of b3 in b1 rainfall is split uniformly from time steps with 15min duration to 5min duration this leads to an underestimation of the extreme values with 5min duration but results in continuous rainfall events in b3 extreme values are overestimated but rainfall events may be interrupted by dry intervals concerning the total flood volume the continuity of extreme rainfall events seems to have a greater influence than short extreme values the resampled time series of b2 show the best fit to the observations with an underestimation of 20 but only slightly better than b3 for the smaller events tk 0 9years using observed rainfall does not lead to any flooding for all simulations also for all non resampled time series no nodes are flooded however this should not be interpreted as being a good fit of all disaggregated time series without the subsequent resampling an additional investigation of the combined sewer overflow volume is carried out to analyze this possible interpretation nevertheless it should be noted that nodes are flooded after the resampling the highest total volume occurs for a the lowest for b2 a higher total flood volume of b3 in comparison to b2 can be identified for the smaller event and vice versa for the higher event this is consistent with the relationship of the extreme values for short durations see fig 9 in fig 16 the corresponding combined sewer overflow volumes during the events with return periods of tk 4 4years and tk 0 9years are shown for the non resampled time series almost no overflow occurs which results in high underestimations in comparison to the observed time series concerning the resampled time series for the event with tk 4 4years the results are quite similar to the total flood volumes of the nodes a and b1 lead to overestimations while b2 and b3 show underestimations the best fit can be identified with a slight underestimation of 3 4 for b2 for the smaller event tk 0 9years all methods show underestimations again combined sewer overflow volume cannot be reproduced by the time series without spatial consistence after the resampling a leads to the strongest underestimation while the results for b1 b2 and b3 are similar about 21 underestimation it should be noted that although no nodes were flooded by these events an overflow volume of the tank occurs which is underestimated by all disaggregation methods this can be explained by fraction of dry time steps during the event table 5 for the three observed extreme events no dry time steps occur at the master station while for the resampled time series the fraction ranges between 18 b1 and 31 for the other two stations the fraction of dry time steps are even stronger overestimated with 50 66 in comparison to 6 for the observations so although the total rainfall amount of an extreme event is well represented the distribution of the rainfall amount in the event can have a significant influence on the modeling results hence the validation of applicability of extreme values from generated time series cannot be carried out without urban hydrological modeling the higher rainfall intensity of single time steps due to a higher fraction dry time steps in an extreme event leads to an overestimation of the flood volume for all disaggregation methods for the observations the water elevation in the system remains completely below the manhole cover level hence no delayed flows occur and the whole water volume reaches the tank immediately causing the combined sewer overflow 5 conclusions and outlook in this study two different investigations were presented to generate 5min rainfall time series and apply them for urban hydrologic modeling first three modifications of the multiplicative random cascade model are introduced all three modifications are further developments of the uniform splitting approach introduced by müller and haberlandt 2015 a branching number b 3 is applied in the first disaggregation step while for all disaggregation steps b 2 is applied to achieve a final resolution of 5min different methods are tested implemented at the disaggregation level 6 called b1 7 b2 and 8 b3 which represent 15 7 5 and 3 75min intervals respectively in b1 a uniform splitting is applied while for the other two the final resolution is achieved by linear interpolation the performance of the model was compared with observed values from lower saxony germany and an existing modification of the cascade model the so called 1280min approach called a different criteria regarding time series statistics and extreme values were taken into account for the evaluation molnar and burlando 2008 analyzed the influence for parameter estimation using 1440min starting and ending of each day were discarded or continuous usage of 1280min and could find no large difference however it could be shown that for the disaggregated time series differences are crucial the investigations have been carried out twice once taking all values of the time series into account and once including a threshold to neglect the influence of single tips in the observed time series and too small rainfall intensities in the disaggregated time series the following conclusions can be drawn 1 method a is outperformed by b1 b2 and b3 regarding wet and dry spell duration average intensity lag 1 autocorrelation and fraction of dry intervals for wet spell amount a and b3 show better results than b1 and b2 taking into account all time steps with the threshold cleaned time series best results are provided by method b1 and b2 2 extreme values of 5min duration are highly overestimated by a and b3 b1 shows slight underestimations while b2 tends to overestimate the observations slightly for extreme values of 1h duration only minor differences can be identified 3 altogether b2 shows the best performance most of the characteristics are only slightly overestimated wet spell amount 8 dry spell duration 8 average intensity 11 and fraction of dry intervals 1 wet spell durations are slightly underestimated 3 the relative root mean square error for extreme values of 5min duration is rrmse 16 the overestimation for extreme values with 5min duration can also be found by onof et al 2005 for b3 molnar and burlando 2005 show also overestimations resulting from a while in licznar et al 2011a extreme values of 5min duration are underestimated the autocorrelation of the time series is underestimated by all methods lombardo et al 2012 proved that the autocorrelation cannot be reproduced by the micro canonical cascade model an underestimation of the autocorrelation was identified before by e g olsson 1998 güntner et al 2001 pui et al 2012 and paschalis et al 2012 2014 lisniak et al 2013 show a good representation of the autocorrelation function for a validation period while for the calibration period underestimations occur for all lags rupp et al 2009 analyzed four different kinds of cascade models depending on the model choice autocorrelation function was under or overestimated one possibility to improve the results of method a could be a dressed cascade model see schertzer et al 2002 paulson and baxter 2007 which includes a continuation of the disaggregation process to very fine time scales and subsequently aggregation to the scale of interest another possibility would be to implement an additional disaggregation step from 5 625min to 2 8125min and subsequently an averaged weighting to achieve a final resolution of 5min since the disaggregation process is carried out station based spatial consistence is missing in between the disaggregated time series the implementation of spatial consistence for 5min rainfall is the second novelty of this study a resampling algorithm is applied for the implementation of spatial consistence müller and haberlandt 2015 defined by the distance dependent bivariate characteristics probability of occurrence pearson s coefficient of correlation and continuity ratio wilks 1998 these characteristics have been analyzed before and after the resampling procedure 4 the resampling algorithm is capable of implementing spatial consistence for time series with 5min resolution 5 the probability of occurrence and pearson s coefficient of correlation could be improved significantly continuity ratio shows a slight worsening the disaggregated time series with and without spatial consistence as well as observed time series have been used as input for an artificial urban hydrological system the main findings are 6 using spatial uniform rainfall one station as input does not ensure an adequate representation of node flooding and tank overflow therefore spatial heterogeneous rainfall three stations has been applied for all further simulations 7 the resampled time series lead to comparable results to those from the observations without resampling unrealistic results regarding the volume of flooded nodes or combined sewer overflow volume occur 8 the resampled version of b2 leads to the best fit to observations method a results in the worst representation among all resampled versions the overall performance of b2 was better in comparison to a although the latter one can be seen as the standard disaggregation variant of the cascade model for urban hydrological investigations licznar et al 2011a b 2015 molnar and burlando 2005 serinaldi 2010 paschalis et al 2014 the potential of the resampling algorithm has been proven for time series with 5min resolution however the reproduction of bivariate characteristics can be improved further investigations of the proposed methods should be carried out for other regions with different rainfall characteristics and climate acknowledgements first of all the associated editor and two anonymous reviewers are gratefully acknowledged their suggestions and comments helped to improve the manuscript significantly the authors thank the students jonas legler for calibration of the simulated annealing parameters and bartosz gierszewski for testing some validation methods and figure preparation thanks also to ana callau poduje and ross pidoto for useful comments on an early draft of the manuscript a special thank to bastian heinrich for figure preparation and technical support during the investigation we are also thankful for the permission to use the data of the german national weather service deutscher wetterdienst dwd and stadtentwässerung braunschweig gmbh this study is part of the project synopse funded by the german federal ministry of education and research bmbf funding number 033w002a appendix a see table a1 
7593,scenario neutral approaches are being used increasingly for climate impact assessments as they allow water resource system performance to be evaluated independently of climate change projections an important element of these approaches is the generation of perturbed series of hydrometeorological variables that form the inputs to hydrologic and water resource assessment models with most scenario neutral studies to date considering only shifts in the average and a limited number of other statistics of each climate variable in this study a stochastic generation approach is used to perturb not only the average of the relevant hydrometeorological variables but also attributes such as the intermittency and extremes an optimization based inverse approach is developed to obtain hydrometeorological time series with uniform coverage across the possible ranges of rainfall attributes referred to as the exposure space the approach is demonstrated on a widely used rainfall generator wgen for a case study at adelaide australia and is shown to be capable of producing evenly distributed samples over the exposure space the inverse approach expands the applicability of the scenario neutral approach in evaluating a water resource system s sensitivity to a wider range of plausible climate change scenarios keywords scenario neutral climate impact study stochastic generator wgen exposure space inverse approach optimization 1 introduction scenario neutral approaches are being used increasingly to assess the possible impact of climate change on the performance of water resources systems brown et al 2012 brown and wilby 2012 dessai and hulme 2004 nazemi and wheater 2014 as well as social and ecological systems gao et al 2016 poff et al 2015 the information generated from these approaches can be used to assess system vulnerability under alternative climate change scenarios and to calculate climatic thresholds at which system performance begins to change abruptly brown et al 2011 poff et al 2015 scenario neutral approaches can also accommodate changes in climate projections without the need for additional analysis prudhomme et al 2010 and can help to identify the important hydrometeorological variables or particularly critical states of these variables that affect the system under consideration the latter feature is particularly useful for selecting 1 climate models 2 strategies to generate future rainfall conditions from gcm based projections known as statistical downscaling and 3 alternative lines of evidence e g expert opinion and data from the paleo climatic record that can provide useful information about these variables ultimately this allows for the development of a more complete set of projections that describe how these variables might change in a greenhouse gas enhanced climate nazemi et al 2013 singh et al 2014 steinschneider and brown 2013 vano et al 2015 central to the scenario neutral approach is the analysis of system sensitivity to a range of hydrometeorological conditions such analyses involve exposing the system to perturbed hydrometeorological forcing data that reflect various hydrometeorological conditions that the system may confront in the future referred to as the exposure space to this end it is important to consider the possible variations not only in the average states of the relevant hydrometeorological variables such as annual average rainfall and potential evapotranspiration see kay et al 2014 prudhomme et al 2013 but also their other attributes including extremes seasonality and interannual variability meselhe et al 2009 moody and brown 2013 prudhomme et al 2010 steinschneider and brown 2013 indeed assessments of historical and or future changes to rainfall as a result of climate change have already indicated different changes to the averages collins et al 2013 extremes ajami et al 2007 alexander et al 2006 westra et al 2013 2014 temporal distribution rajah et al 2014 and low frequency variability e g johnson et al 2011 of rainfall throughout the world similarly complex changes to other relevant hydrometeorological variables might also be expected including potential evapotranspiration and snowfall and melt one approach to generating perturbed hydrometeorological forcing data is by applying scaling factors to historical records of each of the relevant hydrometeorological variables these factors can be applied at annual or monthly scales kay et al 2014 paton et al 2013 prudhomme et al 2013 2010 singh et al 2014 or different factors that can be applied across different quantiles in the entire distribution nazemi et al 2013 although such approaches might be viable for perturbing a small number of hydrometeorological variables and their attributes i e low dimensional exposure spaces the capacity of these to represent the potentially complex variations in a wider range of variables and attributes i e high dimensional exposure spaces is likely to be limited consequently when using scaling factors to perturb historical data for climate impact assessments the resultant projections may not show the full range of variability that can be expected in a greenhouse gas enhanced climate prudhomme et al 2013 2010 steinschneider and brown 2013 the use of stochastic generators has been proposed as an alternative to scaling factors to generate hydrometeorological data in a way that can account for a wider range of possible changes whateley et al 2014 some recent advances include the use of a multi site weather generator that is capable of producing realistic time series of meteorological variables with shifts to the mean standard deviation extremes daily scale markov transition probabilities and low frequency interannual variability for examples see steinschneider and brown 2013 wilby et al 2014 yates et al 2015 this is achieved through the perturbation of stochastic model parameters including the transition probabilities and the autocorrelation coefficient and the subsequent application of quantile correction which in combination can be used to generate the high dimensional exposure space a challenge with this approach however is that it is difficult to assess a priori which parameters of the stochastic generator should be modified to produce time series at pre specified points in the exposure space potentially leading to insufficient exploration of the exposure space this challenge arises both as a result of the non linear mapping between the parameters of a stochastic generator and the statistics of the hydrometeorological variables as well as due to the stochastic nature of the model which means that a single parameter set will produce hydrometeorological data that span multiple points on the exposure space steinschneider and brown 2013 in order to address the shortcomings of existing approaches in generating hydrometeorological data to form the exposure space we introduce the concept and framework for an inverse approach with demonstration on a case study the proposed inverse approach enables stochastic generators to be used to generate time series that uniformly span the desired range of the hydrometeorological variables and attributes of interest and thus provides uniform coverage of the exposure space to serve the needs of scenario neutral climate impact assessments although generally applicable to any parametric weather generator this paper focuses on applying the method to rainfall time series for the following reasons 1 although stochastic generators have been used to generate a range of weather variables including temperature humidity and wind e g racsko et al 1991 semenov and brooks 1999 the majority of applications have focused on the generation of rainfall data due to their importance as an input to many water resource assessments e g chiew and mcmahon 2002b jones and thornton 1993 2 at daily or shorter timescales rainfall is intermittent highly skewed with rainfall series typically exhibiting a large number of moderate rainfall days and a small number of very heavy rainfall days and exhibits variability at seasonal interannual and longer time scales bastola et al 2011 dubrovský et al 2000 as a result rainfall is often regarded as a particularly challenging variable to simulate stochastically 3 there has been a substantial amount of work on developing stochastic generation models to both generate replicates of historical rainfall data beven 1987 boughton and droop 2003 chen and brissette 2014 clark and slater 2006 frost 2004 furrer and katz 2008 langousis and kaleris 2014 langousis et al 2015 as well as downscaling gcm based climate projections allen and pruitt 1986 bastola et al 2011 fowler et al 2007 jones et al 2011 kay and jones 2012 wilby et al 2014 yates et al 2015 the remainder of this paper is structured as follows in section 2 we illustrate the alternative approaches that are currently available for generating an exposure space including the historical scaling forward and inverse approaches this section also provides details of the proposed inverse approach section 3 introduces a case study and two stochastic generators that are used to illustrate both the proposed approach as well as a simple forward approach that is used as a basis of comparison the results are given in section 4 followed by conclusions in section 5 2 proposed inverse approach to exposure space generation 2 1 rationale for an inverse approach to perturbing stochastic model parameters as described in the introduction a central feature of scenario neutral approaches is the exploration of a water resource system s response to a range of different hydrometeorological conditions this range of hydrological variables e g rainfall temperature evapotranspiration and the set of attributes of these variables e g annual average variance seasonal differences extremes are collectively referred to as an exposure space and represent the range of conditions of interest that a system may be exposed to under a future climate for example if a scenario neutral approach was to be used to evaluate system sensitivity to changes in the average variability and extremes of rainfall then this would require generating a three dimensional exposure space with each dimension representing one of the rainfall attributes fig 1 illustrates the conceptual approaches that could be used to generate an exposure space e which consists of the plausible future changes represented as the gray shaded region with the origin corresponding to no change in various rainfall attributes of interest represented by two axes a 1 and a 2 which refer to two generic rainfall attributes or groups of attributes two techniques are involved in the perturbation approaches namely scaling of rainfall time series and stochastic rainfall generation as shown in the two green squares we use the term scaling in the figure to collectively refer to perturbations that are directly applied to rainfall time series through the use of change factors at annual monthly or other time scales kay et al 2014 prudhomme et al 2010 prudhomme and williamson 2013 or more complex methods such as quantile mapping as used in steinschneider and brown 2013 consequently the scaling technique can only modify rainfall intensity on wet days the term stochastic generation in the figure refers to indirect modification of the rainfall time series through changing the parameters of stochastic generators as used in dubrovský et al 2000 jones and page 2001 steinschneider and brown 2013 the parameter space θ consists of two axes of θ 1 and θ 2 which refer to two generic parameters or groups of parameters the plausible ranges for all parameters are represented by the gray shaded region while the origin represents the set of parameters corresponding to the historical rainfall condition the first approach historical scaling as shown in the top right corner of fig 1 is analogous to the approach used by prudhomme et al 2010 2013 and kay et al 2014 in which additive and or multiplicative scaling factors are applied directly to historical hydrometeorological time series to obtain the desired changes in the relevant variables usually rainfall and potential evapotranspiration although conceptually simple this approach is not capable of representing variations in the rainfall intermittency such as the frequency and persistence of dry wet day occurrence furthermore it is difficult to apply this approach to higher dimensional exposure spaces since it becomes difficult to develop an approach to scale each attribute independently of the other attributes consequently it can be difficult to sample some regions of the exposure space the remaining approaches use stochastic weather generators to obtain perturbed rainfall time series the forward approach as illustrated in the middle of fig 1 involves perturbing the parameters of stochastic generators over some pre defined parameter space to yield an exposure space dubrovský et al 2000 jones and page 2001 however the non linear mapping between the parameters of a stochastic generator and the attributes of the hydrometeorological variables means that it is unlikely that the full range of the desired exposure space will be covered conversely some perturbations may lead to rainfall attributes with levels out of the defined plausible ranges of the exposure space consequently further scaling may still be necessary after application of the forward approach steinschneider and brown 2013 used this combined forward plus scaling approach by firstly perturbing the parameters of a stochastic generator including markov chain transition probabilities and the autoregressive model for low frequency variability to obtain stochastic sequences without changing the historical rainfall intensity the wet day rainfall intensity in the stochastic sequences was subsequently quantile mapped to yield a set of target daily rainfall series with desired levels of rainfall attributes although this approach is likely to provide a much better coverage of the exposure space some portions of the exposure space may still remain poorly represented because of the difficulty in finding parameters that will result in all combinations of the hydrometeorological attributes of interest the limitations of both the historical scaling and forward approaches motivate the inverse approach proposed in this paper bottom of fig 1 here the desired values of the attributes of interest in the exposure space are the starting point for the analysis followed by an optimization step to identify the stochastic generator parameters that produce stochastic sequences with these attributes this approach provides control over the level of coverage of the exposure space as required for the implementation of scenario neutral approaches to climate impact assessments 2 2 overview of the inverse approach to generate hydrometeorological time series with a range plausible attribute levels the inverse approach is proposed as follows which involves two primary steps 1 identify a set of target levels for each attribute included in the exposure space in order to achieve an even coverage of the exposure space we first select the desired levels we would like to sample for each attribute included in the exposure space referred to as target levels a number of different approaches can be used to select and combine the target levels which produce individual target locations in the exposure space including sampling on a regular grid or using more computationally efficient sampling methods such as latin hypercube sampling stein 1987 or hammersley sampling halton 1960 hammersley 1960 2 generate hydrometeorological time series that satisfy each target set of attributes for each target location of the exposure space we combine stochastic weather generation with a formal optimization approach to identify the best fit parameter set for the stochastic generator this parameter set should be capable of producing hydrometeorological time series with the levels of attributes corresponding to that particular target location as detailed below during the optimization process the decision variables are the parameters of the stochastic generator the objective is to identify the parameters of the stochastic rainfall generator that minimize the difference between the values of the hydrometeorological attributes that correspond to the target location and those of the corresponding simulated values the following objective function is proposed for minimization 1 f obj i k 1 k p s i k p his k p his k p i k p his k p his k 100 2 where i 1 2 n for n target locations in the exposure space for the kth attribute of the hydrometeorological variable of interest p k p s k represents the target level and p k represents the simulated level from the stochastic generator since different attributes are likely to consist of different magnitudes the difference between a target level and the simulated level is represented as a percentage change relative to its long term averaged historical value p his k to ensure consistent scales across different attributes the optimization problem can be solved using a variety of optimization algorithms such as genetic algorithms holland 1975 or shuffled complex evolution duan et al 1993 the optimization procedure proceeds as follows 1 values of the parameters of the stochastic generator are perturbed based on the searching strategy of the selected optimization algorithm 2 the corresponding time series of the desired hydrometeorological variables are generated 3 the values of the attributes of interest are calculated and 4 the objective function is calculated in accordance with eq 1 which drives the algorithm s searching behavior steps 1 4 are then repeated until the specified stopping criterion has been met such as the completion of a pre specified number of iterations or until the objective function value is sufficiently small maier et al 2014 it is important to note that as part of the inverse approach the random seed of the stochastic generator should be held constant to ensure that the optimization proceeds as efficiently as possible as discussed further in the following section 2 3 random sampling issues of stochastic generators and the implications on the inverse approach the stochastic component of the rainfall generator can produce substantial variations in the simulation of rainfall attributes even with a single parameter set this randomness can affect the efficiency of the optimization process used in the inverse approach essentially every iteration of the optimization involves a comparison among multiple parameter sets in terms of their ability to generate the target locations in the exposure space however as a result of stochastic generation a single parameter set can lead to multiple potential locations on the exposure space fig 2 this can then mislead the comparison and affect optimization efficiency as changes made to parameters by the optimization algorithm in order to lead the search in one particular direction might actually have the opposite effect to illustrate this issue consider a simple optimization problem to find the best fit parameters of a gaussian distribution with the objective of getting a target sample mean of x 3 suppose that for one iteration the optimizer attempts to compare samples drawn from a simple gaussian random generator x n μ σ where the parameter μ is changed from 4 0 to 4 5 while holding σ at a constant value of 1 in the upper panel of fig 3 we show 50 random values drawn with each parameter set for this set of random values the sample mean from x n 4 0 1 is 4 2 compared with the sample mean from x n 4 5 1 which is 4 0 therefore the resulted sample mean from n 4 0 1 is actually further away from the target sample mean of x 3 compared with n 4 5 1 so that the search direction of the optimizer may be misled although this variance can be reduced with a larger sample size or a longer simulation period it can never be completely eliminated to overcome this problem during optimization the random number seed is held constant when producing the stochastic replicates this ensures that any changes made to the parameters during the optimization process will lead the search in the desired direction using the same example in the lower panel of fig 3 we show 50 samples drawn from both x n 4 0 1 and x n 4 5 1 with the same random seed used for each pair of samples resulting in samples means of 3 9 and 4 4 respectively thereby indicating that n 4 0 1 is better at producing a target sample mean of x 3 in this way the stochastic generator is able to search through the correct directions on the parameter space to find parameters that converge toward the target rainfall attributes as discussed in section 2 2 it is important to emphasize that the objective of the approach is to generate samples of hydrometeorological time series with specific levels for each attribute rather than to identify the parameter sets that will produce those parameters in a population sense returning to the above example the objective is to find a stochastic replicate with sample mean x 3 regardless of the values of the parameters μ σ used to achieve this value consequently once this goal has been met the search can stop and the parameter values that were used to produce the stochastic time series corresponding to each target location can be discarded 3 case study the proposed inverse approach is illustrated on rainfall data from a catchment in south australia using two stochastic rainfall generators the richardson model and the wgen model to provide a benchmark for the proposed inverse approach its performance is also compared with that of a forward approach the rainfall data stochastic rainfall generators and the specific implementation of the forward and inverse approaches are described in this section 3 1 data we used a rainfall time series from a gauge in the southern mount lofty ranges close to adelaide south australia as a case study the climate in this region is temperate with most rainfall occurring in winter and spring may to october the mean annual rainfall was 913mm for the study period from 1989 to 2004 the daily rainfall data over this period have been used to represent the baseline historical rainfall conditions we used four rainfall attributes as the dimensions of the exposure space with definitions and baseline values provided in table 1 these attributes represent key features of rainfall patterns namely the average daily rainfall pd the wet day frequency wd a measure of the rainfall intermittency cdd and a measure of extreme rainfall pex99 these attributes have been commonly used to assess the performance of rainfall generators chen and brissette 2014 fowler et al 2007 hashmi et al 2011 kilsby et al 2007 semenov 2007 and are also closely related to several of the indices used for the detection and attribution of climate change as described by the expert team on climate change detection and indices etccdi klein tank et al 2009 for each rainfall attribute we defined a plausible range for sampling which defined the range of each dimension within the exposure space of between 50 and 150 of the corresponding historical value these bounds were wider than would be expected from most climate change projections e g csiro and bureau of meteorology 2015 stocker et al 2013 to encompass a large range of climate projections for example from climate models in the exposure space 3 2 stochastic rainfall generators two versions of the richardson type stochastic rainfall generator with different levels of complexity were used to generate the exposure space we started with a simplified four parameter model which assumes uniform rainfall characteristics over the year the advantage of this model is that it is possible to analytically derive the parameters that correspond to each target location in the exposure space however this simplified model uses a single value for each parameter throughout the year and thus is unable to capture seasonal scale variability to highlight some practical issues with rainfall sampling we then considered a more complex and widely used model namely the wgen richardson and wright 1984 3 2 1 the four parameter richardson model the simplified richardson type rainfall generator uses the following four parameters the two parameters of the 1st order two state markov chain used for representing the transition probabilities of rainfall occurrence pdd dry dry probability and pwd wet dry probability and the two parameters of a gamma distribution for representing the rainfall intensity on wet days α scale and β shape an approximate analytical expression relating two of the four output rainfall attributes pd and wd to the model parameters is given in dubrovský et al 2000 as 2 pd α β wd 365 25 3 wd 365 25 1 p dd 1 p dd p wd these analytical expressions have been used when exploring the implications of random sampling issues on the inverse generation approach section 4 1 3 3 2 2 the wgen model the wgen model richardson and wright 1984 has the same structure as the simplified richardson model except that it uses a unique set of the four parameters for each month of the year leading to a total of 48 parameters this model has been used widely for climate impact studies and is generally shown to capture most of the key features of daily rainfall series bastola et al 2011 katz 2002 kim et al 2007 since the proposed inverse approach involves optimization of the parameter values a search space with low dimension i e consisting of a small number of parameters as decision variables is desired to reduce the size of the parameter space in the inverse approach the number of decision variables to be considered was reduced from 48 to eight by fitting harmonic functions to describe the seasonal variations of each parameter prudhomme et al 2013 the harmonic function takes the form of 4 β t β 0 a cos 2 π t t φ where β t represents one of the four parameters during month t 1 t with t 12 β 0 represents the arithmetic mean of the parameter a represents the amplitude and φ corresponds to the month where the maximum occurs it is worth mentioning that although parameter φ can be varied as part of the optimization the four attribute exposure space in this case study was not designed to focus on shifts in rainfall seasonality section 3 1 so that φ was held constant at its historically optimal value to determine the value of φ we obtained the monthly estimates of pdd pwd α and β based on the method in richardson 1981 using the historical rainfall data and fitted a harmonic function to each parameter fig 4 the corresponding values of φ were thus identified to be 2 1 8 and 1 i e february january august and january for the four parameters respectively as a result the optimization was performed on the mean β 0 and amplitude a of each of the four model parameters leading to an eight dimensional search space 3 3 sampling approach as illustrated in fig 1 application of the forward approach involves sampling the parameter space prior to using the stochastic model similarly application of the inverse sampling approach involves the identification of target locations in the exposure space as the basis for optimization one approach to sampling both the parameter space in the forward approach and exposure space in the inverse approach is to define a grid of evenly spaced points over the entire space however this can be inefficient particularly for high dimensional problems for a large number of parameters attributes in the exposure space in the forward inverse approach for example if one wished to sample on a grid of width 10 for the parameter space of the four parameter richardson model then it would be necessary to evaluate a total of 104 10 000 separate parameter sets this issue is particularly pertinent for the inverse approach since optimization is required to find a parameter set that corresponds to each point in the exposure space therefore to provide even coverage of the parameter or exposure space while keeping the sample sizes low two structured sampling techniques have been employed namely latin hypercube sampling lhs and improved distributed hypercube sampling ihs the objective of the analysis in this paper is to illustrate the inverse approach by comparing its performance with the forward approach therefore for consistency the objective of the sampling approach was to obtain 100 samples within the exposure space for the forward approach it is not known a priori whether a particular parameter set in the parameter space will yield a sample in our exposure space i e within the plausible range of 50 150 for each rainfall attribute as defined in section 3 1 so that the number of samples that need to be drawn from the parameter space is not known to determine the total number of samples in a computationally efficient manner we used the latin hypercube sampling lhs method which allows starting with a small sample size and adding new samples while keeping the previously generated ones the lhs method involves sampling m variables with a desired sample size n by dividing the range of each variable into n equally probable intervals n samples are then drawn so that any interval for each variable is only sampled once stein 1987 to add n new sample points the existing design is re divided into n n intervals the n old samples are kept which occupy n intervals and then n new samples are drawn to fill the remaining n intervals unlike the lhs method the ihs method manteufel 2001 beachkofski and grandhi 2002 requires that the number of samples be specified a priori but ensures more even coverage of the sampling space this latter feature is attractive when sparsely sampling potentially high dimensional spaces and is therefore recommended to determine the target locations in the exposure space for the inverse sampling approach the ihs method is similar to the lhs method with two additional objectives 1 the average minimum distance between sample points equals the optimal distance dopt that is if the span of each output variable is normalized to 1 so that the entire sample space is a hypercube of volume 1 then each sample point should cover an equal hypervolume with dimension of m within the entire space this gives the optimal distance between sample points i e d opt 1 n sample m 2 the coefficient of variance cov of all minima between each pair of sample points is close to zero 3 4 implementation of forward approach as mentioned previously the forward approach has been used to provide a benchmark against which the utility of the proposed inverse approach can be assessed the approach involves the following steps 1 the parameter space is constructed by selecting appropriate ranges of the parameters for the stochastic generators 2 parameter sets are drawn from the parameter space using a sampling strategy such as the lhs method described in section 3 3 3 the sampled parameter sets are used to generate time series of the hydrometeorological variables of interest in this case rainfall and 4 the values of the attributes that define the exposure space are calculated for each of the generated hydrometeorological time series for the simple four parameter richardson model the transition probability parameters pdd and pwd both vary between 0 and 1 the two rainfall intensity parameters α and β are for the gamma distribution and should be greater than 0 note that their values are mostly between 0 and 1 when calibrated to historical data see richardson and wright 1984 from a preliminary analysis for our case study α and β values of 0 56 and 0 10 were obtained respectively yielding rainfall time series with attributes that are close to the historical data therefore although α and β do not physically have upper bounds and can take any value above 0 their ranges were set to be between 0 and 1 in the forward approach based on their historical values the use of such a small range ensures that the parameter space surrounding the historical levels of the parameters is sufficiently sampled for wgen the parameter ranges were defined in a similar way so that the bounds of both the transition probability and rainfall intensity parameters were set to 0 and 1 for all months as mentioned previously for both stochastic models lhs was used to sample the parameter space an initial latin hypercube sample size of 100 parameter sets was used and this was incremented until 100 rainfall time series were generated with attributes within the plausible bounds of the exposure space 3 5 implementation of proposed inverse approach a general description of the inverse approach was provided in section 2 2 the ihs method section 3 3 was used to determine the target locations for the optimization which consist of 100 sets of combined levels of the four rainfall attributes that uniformly cover the exposure space for each target location the best fit parameter sets for both the four parameter richardson model and the wgen model were identified using optimization the shuffled complex evolution algorithm duan et al 1993 was used as the optimization engine due to its proven ability for solving complex optimization problems in hydrological studies gupta et al 1999 thyer et al 1999 wang et al 2010 based on the general formulation in eq 1 the objective function to be minimized for both stochastic models was 5 f obj i pd s i pd his pd his pd i pd his pd his 100 2 wd s i wd his wd his wd i wd his wd his 100 2 cdd s i cdd his cdd his cdd i cdd his cdd his 100 2 pex 99 s i pex 99 his pex 99 his pex 99 i pex 99 his pex 99 his 100 2 the constraints of the optimization consist of the plausible ranges of the parameters for both models as mentioned in section 3 4 the plausible range for the probability parameters pdd s and pwd s is between 0 and 1 for the intensity parameters α s and β s which do not have a physical upper limit we defined the range to be between 0 to 104 which was wider than the range used for the forward approach section 3 4 to enable more extensive searching within the defined range for the wgen model since a harmonic function has been fitted to the monthly values of each of the probability and intensity parameters section 3 2 2 the actual decision variables for the optimization were the parameters of the harmonic functions i e β 0 and a which represent the mean and amplitude respectively as in eq 4 to ensure that the probability parameters were always within 0 and 1 while the intensity parameters were always within 0 and 104 during the optimization process the values of the mean and amplitude for each of these parameters have been optimized sequentially in the first step the mean value of each parameter has been optimized with the amplitude kept as zero once the mean has been determined a second optimization was conducted to estimate the amplitude for example if the mean of pdd is found by the optimizer to be 0 3 in the first step its amplitude must be constrained between 0 and 0 3 in the second step to avoid values of pdd going beyond 0 and 1 it should be noted that in determining the target locations the ihs only checks the multi dimensional uniformity of the overall distribution without considering the physical interpretation for each individual target location therefore it is important to ensure that each target location selected is physically plausible for example pd should always be less than pex99 and wd should never exceed 365days for this study these constraints were automatically satisfied because a relatively small plausible range of 50 150 was selected for each attribute if the rainfall samples are required to show larger variances it may be necessary to impose additional constraints in the optimization procedure to ensure the resultant samples remain physically plausible 4 results 4 1 the four parameter richardson model 4 1 1 forward approach the coverage of the exposure space obtained by applying the forward approach to the four parameter richardson model is shown in fig 5 which shows high variances in some rainfall attributes in particular the generated pd cdd and pex99 can go up to 15 000 6000 and 80 000 of their corresponding historical values respectively fig 5a which are well outside the bounds of the exposure space the sampled wd has lower variance with values up to only 226 of the historical values since a year contains a maximum of 365 or 366days however these values are still above the upper limit of the exposure space of 150 the high variance leads to low sampling efficiency to obtain 100 sets of combined levels of rainfall attributes within our exposure space a total of 7635 lhs samples of parameter sets had to be generated i e 98 7 samples were unacceptable and discarded all 7635 sets of rainfall attributes are plotted in fig 5a with the 100 plausible samples shown in fig 5b in addition to the issue of inefficient sampling based on both a visual inspection of the coverage on the exposure space as well as consideration of the correlation coefficients it is clear that the coverage of the exposure space is uneven fig 5b in particular samples are clustered in small regions of the exposure space for each rainfall attribute with other parts of the space receiving limited or no coverage for example the correlation between pd and pex99 is quite high which results in better coverage over regions closer to the diagonal of the joint distribution of pd and pex99 than other regions the above problems with using the forward approach are most likely due to the non linear translation from parameters to rainfall attributes through the stochastic generator so that large variations in certain regions in parameters space result in small variations in exposure space and vice versa this non linearity will be further illustrated in the next section with the distribution of parameters identified through the inverse approach 4 1 2 inverse approach fig 6 a shows the 100 target locations of desired rainfall attributes that have been determined using the ihs approach section 3 3 as can be seen the ihs approach generates samples that appear to be uniformly distributed across the exposure space with even coverage across each attribute and low cross correlations between attributes the final set of combined levels of attributes corresponding to each of the 100 stochastically generated rainfall time series obtained using the inverse approach is presented in fig 6b as can be seen the optimization based approach is effective in producing the desired levels of rainfall attributes i e target locations with all of the 100 samples falling within the bounds of the exposure space and with relatively even coverage of the exposure space fig 6b therefore the inverse approach delivers much better coverage of the exposure space than the forward approach fig 5 fig 7 shows the values of the 100 parameter sets for the four parameter richardson model identified via application of the inverse approach highlighting the non linear mapping between parameter space and exposure space this is best illustrated with the non uniform distribution of the best fit parameters in contrast to the uniform distribution of the exposure space fig 6 furthermore the parameters have considerably different ranges compared with the a priori 0 1 ranges that were specified for the forward approach for example the values of pdd generally vary within a narrower range of 0 5 0 9 whereas values of α are as high as 10 therefore the ranges of 0 1 defined for the four parameters in the forward approach as detailed in section 3 4 can significantly limit the resultant coverage of the exposure space this also reflects the high degree of non linearity in the mapping between the parameter values and the exposure space as a small change in the exposure space may result in a large shift in parameter space interestingly for the case study considered although the inverse approach had the additional step of parameter optimization the computational time required to obtain 100 samples was 32 6 shorter than for the forward approach this is likely due to the large number of samples that were discarded in the forward approach 4 1 3 implications of random sampling on the inverse approach in the above example we fixed the random seed of the random number generator during the optimization process due to reasons discussed in section 2 3 to illustrate the importance of this aspect of the optimization we use the analytical expressions in eqs 2 and 3 to estimate the model parameters that will yield individual target locations from a grid consisting five evenly spaced levels for each of wd and pd 50 75 100 125 and 150 of their historical values these locations within the exposure space are given as green dots in fig 8 we then generated 100 stochastic replicates from each of these parameter sets with different random seeds which are shown as blue and red scatter about each of the target locations in fig 8 the stochastic nature of the model is clear for all target locations for each parameter set the 100 replicates of wd vary up to 10 around their target level which is similar for all target levels of wd and pd in contrast the 100 replicates of pd are closer to the target level for smaller pd e g up to 10 around where the target level is 50 while for larger pd target levels the spread among replicates increases substantially e g up to 40 around where the target level is 150 compared with the sampling resolution required in this study shown in fig 6a the variability in fig 8 is in fact much higher which can adversely affect the capacity of the optimizer to find parameters that correspond to each target location as discussed in section 2 3 4 2 the wgen model 4 2 1 forward approach the coverage of the exposure space obtained by applying the forward approach to the wgen model is shown in fig 9 similar to the results for the four parameter model section 4 1 1 the forward approach shows low efficiency to obtain 100 sets of rainfall attributes within the range of the exposure space 1453 lhs samples of wgen parameter sets were required fig 9a which means that 93 1 of samples were discarded with the 100 plausible sets in fig 9b the coverage of the exposure space is poor which is also evident through the high pairwise correlations such as between pd and pex99 and between wd and cdd 4 2 2 inverse approach to examine the performance of the inverse approach with wgen the 100 target locations which have been determined using the ihs approach section 3 3 are plotted in fig 10 a the final optimized set of attributes corresponding to each of the 100 stochastically generated rainfall time series is presented in fig 10b the inverse approach is generally effective in evenly covering the exposure space and reproducing these target locations in particular this approach delivers much better coverage of the exposure space than the forward approach fig 9 in the following aspects 1 all of the 100 samples are within the plausible output space defined in table 1 suggesting effective control over the values of individual rainfall attributes and 2 the joint distribution of multiple rainfall attributes is much more uniform across the exposure space and the pairwise correlations between different attributes are reduced as an alternative approach to assessing the relative uniformity of the sampling in the exposure space the minimum distances between sample points in the exposure space are compared for both the forward as orange dots in fig 11 and inverse approaches as blue dots in fig 11 the results show that the inverse sampling approach produces a more uniform coverage as the minima between sample points are closer to the optimal distance dopt 0 32 see section 3 3 furthermore the coefficient of variance cov of these minimum distances is also much lower i e 0 52 for the inverse approach compared with 2 90 for the forward approach it is worth noting that to obtain 100 sample points on the exposure space with the wgen model the overall execution time required for implementing the inverse approach is 73 longer than that for the forward approach this is most likely due to the difficulty in solving optimization problems with a larger number of parameters as a result of the larger search space that has to be explored however the inverse approach ensures uniform coverage of the exposure space with the desired resolution which is the key objective for constructing the exposure space in contrast the forward approach failed to obtain such coverage therefore although associated with a higher computational expense the proposed inverse approach is the only way of achieving the desired coverage of the exposure space 5 discussion this study presented a framework for sampling various rainfall conditions to construct an exposure space for scenario neutral climate impact assessments here we discuss some practical considerations as well as possible future adaptations of the framework 5 1 design of exposure space to represent more complex potential climate changes the four rainfall attributes considered in the exposure space for this study i e pd wd cdd and pex99 are good descriptors of a range of changes of annual precipitation characteristics however there is also a range of other rainfall attributes that might be important when considering the impact of climate change such as changes at seasonal or interannual timescales e g christensen et al 2007 johnson et al 2011 kwon et al 2009 furthermore potential future variations in other climatic features such as temperature solar radiation and evapotranspiration may also have a substantial impact on water resources for examples see chiew and mcmahon 2002a prudhomme and williamson 2013 the inverse approach presented here is sufficiently flexible to cater to all attributes in the exposure space that are of interest provided they can be generated with an appropriate stochastic generator although this comes at the expense of additional computational cost considering the trade off between the flexibility of producing different climate patterns and computational effort it is important to identify key hydrometeorological variables of interest as well as their attributes based on an understanding of the behavior of the system being analyzed depending on the specific hydrometeorological variables involved the format of the objective function may require modification from eq 1 which was designed assuming multiplicative perturbations to attributes e g changes expressed as a percentage of the historical value for example temperature changes are typically represented in an additive way e g increases in temperature by degrees celsius for examples see chiew and mcmahon 2002a kingston et al 2009 and this would require an adjustment to the objective function in eq 1 in this study the boundary of the exposure space was set at 50 150 of the historical values of each attribute which is sufficiently wide to incorporate a large number of possible changes in each of the rainfall attributes while also using the same percentage changes across attributes to facilitate illustration however this framework can be easily adapted to incorporate tailored bounds for the exposure space which should be carefully selected to suit the case study under consideration in particular if the bounds deviate too far from present conditions a significant portion of samples will be unrealistic even when extreme climate change impacts are considered conversely if the bounds are too narrow system response to some plausible climatic changes might not be considered whateley et al 2014 multiple sources of information could be considered in selecting these bounds including gcm based climate projections e g collins et al 2013 of possible future climatic changes and additional lines of evidence on possible changes to key variables such as from long term paleoclimatology reconstructions e g ault et al 2014 hansen and sato 2012 ho et al 2015 in addition it is worth specifying an exposure space with bounds that are wider than the range suggested from all currently available sources of information so that additional climate change projections can be included in the analysis as they are developed steinschneider and brown 2013 finally when determining the target locations consisting of different hydrometeorological attributes on the exposure space it is desirable to ensure the physical realism of each individual location so that corresponding time series can be obtained with the aid of stochastic weather generators this requires not only ensuring that the target levels of individual attribute are realistic such as the constraints for the levels of wd as discussed in section 3 5 but also maintaining physically plausible relationships among multiple attributes for example a target location cannot consist a wd value of 100days with a cdd value of 300days because this combination means that the annual average wet day is 100days while the annual average dry spell length is 300days which is physically unrealistic 5 2 stochastic generation of the exposure space in this study we used a sample size of 100 to represent different levels of changes for each individual attribute considered in the exposure space with fixing the random seed across replicates to facilitate improved convergence during the optimization process in this way however there is likely to be limited variability in between time series corresponding to different points on the exposure space except for variations related to the target statistics this issue can be addressed in at least three ways 1 increase the sample size and thus the coverage resolution in the exposure space increasing the exposure space resolution is likely to be particularly useful when the number of attributes increases as this will lead to a corresponding increase in the dimensionality of the exposure space 2 the length of each sample can be increased currently the length is equal to the length of the historical data series i e 15years however it would be trivial to allow the simulation to run for longer periods of time to obtain greater stochastic variation this will require the same number of optimized parameter sets although because of the use of the same initial seed there will still be significant similarities between individual samples 3 the procedure can be repeated multiple times with different random seeds for each iteration thereby generating multiple replicates this would substantially increase the level of stochastic variability although at the expense of additional computational time the appropriate sample size length of each sample and number of replicates are likely to depend on the case study considered as well as available computational resources 5 3 equi finality in the optimization process when using optimization to search for best fit solutions equi finality issues are likely to arise i e multiple solutions leading to the same results in the objective function therefore they are not distinguishable during optimization see beven and freer 2001 this problem is further complicated within the proposed inverse approach as the values of the objective function for optimization are based on results from stochastic models equi finality issues are likely to be greatest for low dimensional exposure spaces since higher dimensional exposure spaces add constraints to the parameter space for example the chance that two contrasting combinations of pdd and pwd lead to the same combination of wd and cdd is much lower compared to that resulting in the same level of wd in isolation thus increasing the number of attributes considered could have the additional advantage of reducing the number of feasible parameter sets to be considered it is worth noting that although equi finality is likely to occur when the proposed inverse approach is implemented the aim of the approach is to identify time series of outputs from the stochastic generator that result in desired values of the attributes included in the exposure space and not to the identification of the resulting parameters in the stochastic generator as discussed in section 2 3 however when different parameter sets lead to the same combination of attributes on the exposure space the different time series of hydrometeorological variables which they produce can consist of varying degrees of physical realism therefore checking the physical realism of the generated time series can potentially help to eliminate unrealistic parameter sets and thus resolve any equi finality issues 5 4 computational efficiency and execution time in our particular implementation of the proposed inverse approach the computational time required to produce 100 evenly distributed samples is around eight hours using an intel xeon e3 2 60ghz 8 cores processor with 32gb ram for both the four parameter model and the wgen suggesting a relatively high computational demand however in general the computational effort required is dependent on a number of practical specifications including the operating system programming language and algorithm used as the key focus of this study is to introduce and illustrate a new method the above mentioned specifications have not been optimized for computational efficiency we have used the r package lhs carnell 2012 for obtaining samples over the exposure space with the ihs and lhs methods together with the shuffled complex evolution algorithm embedded in the r package hydromad andrews and guillaume 2013 for solving the optimal parameter values for the stochastic generator we have also developed our own r scripts to execute the four parameter richardson and the wgen models it is expected that an improved integration of these different modeling components with the aid of other programming languages such as fortran or c will further increase computational efficiency 6 conclusions generation of exposure spaces for scenario neutral climate impact assessments should consider a range of potential variations in relevant hydrometeorological variables including shifts in the average intermittency variability and extremes the exposure space describes the range of conditions of interest that a system may be exposed to under a future climate and this paper presents and demonstrates an inverse approach to stochastically generating hydrometeorological time series to uniformly cover this exposure space the utility of the proposed inverse approach is benchmarked against a forward approach for rainfall generation for a south australian catchment using two richardson type stochastic rainfall generators of varying complexity the results highlight the highly non linear translation from parameter space to exposure space and thus the need for the proposed inverse approach in order to obtain a relatively uniform coverage of the exposure space for both models the inverse approach demonstrates better control of the sampling range with 100 of samples falling within the exposure space furthermore the uniformity of the coverage of the four dimensional exposure space is substantially improved several potential adaptations for future implementations of the framework have been discussed including 1 the design of the exposure space to represent more complex changes in climate 2 improvements to the way that stochastic samples in the exposure space are generated 3 ways of reducing the effects of equifinality during the optimization process and 4 methods for increasing computational efficiency the flexibility of the proposed inverse approach enables consideration of all climate attributes of interest at the desired resolution thereby expanding the applicability of the scenario neutral approach to evaluating a water resource system s sensitivity to a wide range of plausible changes in climate acknowledgements the authors wish to thank christel prudhomme and an anonymous reviewer for their thoughtful comments on the manuscript seth westra s time was supported by australian research council discovery project dp150100411 appendix a see table a 1 
7593,scenario neutral approaches are being used increasingly for climate impact assessments as they allow water resource system performance to be evaluated independently of climate change projections an important element of these approaches is the generation of perturbed series of hydrometeorological variables that form the inputs to hydrologic and water resource assessment models with most scenario neutral studies to date considering only shifts in the average and a limited number of other statistics of each climate variable in this study a stochastic generation approach is used to perturb not only the average of the relevant hydrometeorological variables but also attributes such as the intermittency and extremes an optimization based inverse approach is developed to obtain hydrometeorological time series with uniform coverage across the possible ranges of rainfall attributes referred to as the exposure space the approach is demonstrated on a widely used rainfall generator wgen for a case study at adelaide australia and is shown to be capable of producing evenly distributed samples over the exposure space the inverse approach expands the applicability of the scenario neutral approach in evaluating a water resource system s sensitivity to a wider range of plausible climate change scenarios keywords scenario neutral climate impact study stochastic generator wgen exposure space inverse approach optimization 1 introduction scenario neutral approaches are being used increasingly to assess the possible impact of climate change on the performance of water resources systems brown et al 2012 brown and wilby 2012 dessai and hulme 2004 nazemi and wheater 2014 as well as social and ecological systems gao et al 2016 poff et al 2015 the information generated from these approaches can be used to assess system vulnerability under alternative climate change scenarios and to calculate climatic thresholds at which system performance begins to change abruptly brown et al 2011 poff et al 2015 scenario neutral approaches can also accommodate changes in climate projections without the need for additional analysis prudhomme et al 2010 and can help to identify the important hydrometeorological variables or particularly critical states of these variables that affect the system under consideration the latter feature is particularly useful for selecting 1 climate models 2 strategies to generate future rainfall conditions from gcm based projections known as statistical downscaling and 3 alternative lines of evidence e g expert opinion and data from the paleo climatic record that can provide useful information about these variables ultimately this allows for the development of a more complete set of projections that describe how these variables might change in a greenhouse gas enhanced climate nazemi et al 2013 singh et al 2014 steinschneider and brown 2013 vano et al 2015 central to the scenario neutral approach is the analysis of system sensitivity to a range of hydrometeorological conditions such analyses involve exposing the system to perturbed hydrometeorological forcing data that reflect various hydrometeorological conditions that the system may confront in the future referred to as the exposure space to this end it is important to consider the possible variations not only in the average states of the relevant hydrometeorological variables such as annual average rainfall and potential evapotranspiration see kay et al 2014 prudhomme et al 2013 but also their other attributes including extremes seasonality and interannual variability meselhe et al 2009 moody and brown 2013 prudhomme et al 2010 steinschneider and brown 2013 indeed assessments of historical and or future changes to rainfall as a result of climate change have already indicated different changes to the averages collins et al 2013 extremes ajami et al 2007 alexander et al 2006 westra et al 2013 2014 temporal distribution rajah et al 2014 and low frequency variability e g johnson et al 2011 of rainfall throughout the world similarly complex changes to other relevant hydrometeorological variables might also be expected including potential evapotranspiration and snowfall and melt one approach to generating perturbed hydrometeorological forcing data is by applying scaling factors to historical records of each of the relevant hydrometeorological variables these factors can be applied at annual or monthly scales kay et al 2014 paton et al 2013 prudhomme et al 2013 2010 singh et al 2014 or different factors that can be applied across different quantiles in the entire distribution nazemi et al 2013 although such approaches might be viable for perturbing a small number of hydrometeorological variables and their attributes i e low dimensional exposure spaces the capacity of these to represent the potentially complex variations in a wider range of variables and attributes i e high dimensional exposure spaces is likely to be limited consequently when using scaling factors to perturb historical data for climate impact assessments the resultant projections may not show the full range of variability that can be expected in a greenhouse gas enhanced climate prudhomme et al 2013 2010 steinschneider and brown 2013 the use of stochastic generators has been proposed as an alternative to scaling factors to generate hydrometeorological data in a way that can account for a wider range of possible changes whateley et al 2014 some recent advances include the use of a multi site weather generator that is capable of producing realistic time series of meteorological variables with shifts to the mean standard deviation extremes daily scale markov transition probabilities and low frequency interannual variability for examples see steinschneider and brown 2013 wilby et al 2014 yates et al 2015 this is achieved through the perturbation of stochastic model parameters including the transition probabilities and the autocorrelation coefficient and the subsequent application of quantile correction which in combination can be used to generate the high dimensional exposure space a challenge with this approach however is that it is difficult to assess a priori which parameters of the stochastic generator should be modified to produce time series at pre specified points in the exposure space potentially leading to insufficient exploration of the exposure space this challenge arises both as a result of the non linear mapping between the parameters of a stochastic generator and the statistics of the hydrometeorological variables as well as due to the stochastic nature of the model which means that a single parameter set will produce hydrometeorological data that span multiple points on the exposure space steinschneider and brown 2013 in order to address the shortcomings of existing approaches in generating hydrometeorological data to form the exposure space we introduce the concept and framework for an inverse approach with demonstration on a case study the proposed inverse approach enables stochastic generators to be used to generate time series that uniformly span the desired range of the hydrometeorological variables and attributes of interest and thus provides uniform coverage of the exposure space to serve the needs of scenario neutral climate impact assessments although generally applicable to any parametric weather generator this paper focuses on applying the method to rainfall time series for the following reasons 1 although stochastic generators have been used to generate a range of weather variables including temperature humidity and wind e g racsko et al 1991 semenov and brooks 1999 the majority of applications have focused on the generation of rainfall data due to their importance as an input to many water resource assessments e g chiew and mcmahon 2002b jones and thornton 1993 2 at daily or shorter timescales rainfall is intermittent highly skewed with rainfall series typically exhibiting a large number of moderate rainfall days and a small number of very heavy rainfall days and exhibits variability at seasonal interannual and longer time scales bastola et al 2011 dubrovský et al 2000 as a result rainfall is often regarded as a particularly challenging variable to simulate stochastically 3 there has been a substantial amount of work on developing stochastic generation models to both generate replicates of historical rainfall data beven 1987 boughton and droop 2003 chen and brissette 2014 clark and slater 2006 frost 2004 furrer and katz 2008 langousis and kaleris 2014 langousis et al 2015 as well as downscaling gcm based climate projections allen and pruitt 1986 bastola et al 2011 fowler et al 2007 jones et al 2011 kay and jones 2012 wilby et al 2014 yates et al 2015 the remainder of this paper is structured as follows in section 2 we illustrate the alternative approaches that are currently available for generating an exposure space including the historical scaling forward and inverse approaches this section also provides details of the proposed inverse approach section 3 introduces a case study and two stochastic generators that are used to illustrate both the proposed approach as well as a simple forward approach that is used as a basis of comparison the results are given in section 4 followed by conclusions in section 5 2 proposed inverse approach to exposure space generation 2 1 rationale for an inverse approach to perturbing stochastic model parameters as described in the introduction a central feature of scenario neutral approaches is the exploration of a water resource system s response to a range of different hydrometeorological conditions this range of hydrological variables e g rainfall temperature evapotranspiration and the set of attributes of these variables e g annual average variance seasonal differences extremes are collectively referred to as an exposure space and represent the range of conditions of interest that a system may be exposed to under a future climate for example if a scenario neutral approach was to be used to evaluate system sensitivity to changes in the average variability and extremes of rainfall then this would require generating a three dimensional exposure space with each dimension representing one of the rainfall attributes fig 1 illustrates the conceptual approaches that could be used to generate an exposure space e which consists of the plausible future changes represented as the gray shaded region with the origin corresponding to no change in various rainfall attributes of interest represented by two axes a 1 and a 2 which refer to two generic rainfall attributes or groups of attributes two techniques are involved in the perturbation approaches namely scaling of rainfall time series and stochastic rainfall generation as shown in the two green squares we use the term scaling in the figure to collectively refer to perturbations that are directly applied to rainfall time series through the use of change factors at annual monthly or other time scales kay et al 2014 prudhomme et al 2010 prudhomme and williamson 2013 or more complex methods such as quantile mapping as used in steinschneider and brown 2013 consequently the scaling technique can only modify rainfall intensity on wet days the term stochastic generation in the figure refers to indirect modification of the rainfall time series through changing the parameters of stochastic generators as used in dubrovský et al 2000 jones and page 2001 steinschneider and brown 2013 the parameter space θ consists of two axes of θ 1 and θ 2 which refer to two generic parameters or groups of parameters the plausible ranges for all parameters are represented by the gray shaded region while the origin represents the set of parameters corresponding to the historical rainfall condition the first approach historical scaling as shown in the top right corner of fig 1 is analogous to the approach used by prudhomme et al 2010 2013 and kay et al 2014 in which additive and or multiplicative scaling factors are applied directly to historical hydrometeorological time series to obtain the desired changes in the relevant variables usually rainfall and potential evapotranspiration although conceptually simple this approach is not capable of representing variations in the rainfall intermittency such as the frequency and persistence of dry wet day occurrence furthermore it is difficult to apply this approach to higher dimensional exposure spaces since it becomes difficult to develop an approach to scale each attribute independently of the other attributes consequently it can be difficult to sample some regions of the exposure space the remaining approaches use stochastic weather generators to obtain perturbed rainfall time series the forward approach as illustrated in the middle of fig 1 involves perturbing the parameters of stochastic generators over some pre defined parameter space to yield an exposure space dubrovský et al 2000 jones and page 2001 however the non linear mapping between the parameters of a stochastic generator and the attributes of the hydrometeorological variables means that it is unlikely that the full range of the desired exposure space will be covered conversely some perturbations may lead to rainfall attributes with levels out of the defined plausible ranges of the exposure space consequently further scaling may still be necessary after application of the forward approach steinschneider and brown 2013 used this combined forward plus scaling approach by firstly perturbing the parameters of a stochastic generator including markov chain transition probabilities and the autoregressive model for low frequency variability to obtain stochastic sequences without changing the historical rainfall intensity the wet day rainfall intensity in the stochastic sequences was subsequently quantile mapped to yield a set of target daily rainfall series with desired levels of rainfall attributes although this approach is likely to provide a much better coverage of the exposure space some portions of the exposure space may still remain poorly represented because of the difficulty in finding parameters that will result in all combinations of the hydrometeorological attributes of interest the limitations of both the historical scaling and forward approaches motivate the inverse approach proposed in this paper bottom of fig 1 here the desired values of the attributes of interest in the exposure space are the starting point for the analysis followed by an optimization step to identify the stochastic generator parameters that produce stochastic sequences with these attributes this approach provides control over the level of coverage of the exposure space as required for the implementation of scenario neutral approaches to climate impact assessments 2 2 overview of the inverse approach to generate hydrometeorological time series with a range plausible attribute levels the inverse approach is proposed as follows which involves two primary steps 1 identify a set of target levels for each attribute included in the exposure space in order to achieve an even coverage of the exposure space we first select the desired levels we would like to sample for each attribute included in the exposure space referred to as target levels a number of different approaches can be used to select and combine the target levels which produce individual target locations in the exposure space including sampling on a regular grid or using more computationally efficient sampling methods such as latin hypercube sampling stein 1987 or hammersley sampling halton 1960 hammersley 1960 2 generate hydrometeorological time series that satisfy each target set of attributes for each target location of the exposure space we combine stochastic weather generation with a formal optimization approach to identify the best fit parameter set for the stochastic generator this parameter set should be capable of producing hydrometeorological time series with the levels of attributes corresponding to that particular target location as detailed below during the optimization process the decision variables are the parameters of the stochastic generator the objective is to identify the parameters of the stochastic rainfall generator that minimize the difference between the values of the hydrometeorological attributes that correspond to the target location and those of the corresponding simulated values the following objective function is proposed for minimization 1 f obj i k 1 k p s i k p his k p his k p i k p his k p his k 100 2 where i 1 2 n for n target locations in the exposure space for the kth attribute of the hydrometeorological variable of interest p k p s k represents the target level and p k represents the simulated level from the stochastic generator since different attributes are likely to consist of different magnitudes the difference between a target level and the simulated level is represented as a percentage change relative to its long term averaged historical value p his k to ensure consistent scales across different attributes the optimization problem can be solved using a variety of optimization algorithms such as genetic algorithms holland 1975 or shuffled complex evolution duan et al 1993 the optimization procedure proceeds as follows 1 values of the parameters of the stochastic generator are perturbed based on the searching strategy of the selected optimization algorithm 2 the corresponding time series of the desired hydrometeorological variables are generated 3 the values of the attributes of interest are calculated and 4 the objective function is calculated in accordance with eq 1 which drives the algorithm s searching behavior steps 1 4 are then repeated until the specified stopping criterion has been met such as the completion of a pre specified number of iterations or until the objective function value is sufficiently small maier et al 2014 it is important to note that as part of the inverse approach the random seed of the stochastic generator should be held constant to ensure that the optimization proceeds as efficiently as possible as discussed further in the following section 2 3 random sampling issues of stochastic generators and the implications on the inverse approach the stochastic component of the rainfall generator can produce substantial variations in the simulation of rainfall attributes even with a single parameter set this randomness can affect the efficiency of the optimization process used in the inverse approach essentially every iteration of the optimization involves a comparison among multiple parameter sets in terms of their ability to generate the target locations in the exposure space however as a result of stochastic generation a single parameter set can lead to multiple potential locations on the exposure space fig 2 this can then mislead the comparison and affect optimization efficiency as changes made to parameters by the optimization algorithm in order to lead the search in one particular direction might actually have the opposite effect to illustrate this issue consider a simple optimization problem to find the best fit parameters of a gaussian distribution with the objective of getting a target sample mean of x 3 suppose that for one iteration the optimizer attempts to compare samples drawn from a simple gaussian random generator x n μ σ where the parameter μ is changed from 4 0 to 4 5 while holding σ at a constant value of 1 in the upper panel of fig 3 we show 50 random values drawn with each parameter set for this set of random values the sample mean from x n 4 0 1 is 4 2 compared with the sample mean from x n 4 5 1 which is 4 0 therefore the resulted sample mean from n 4 0 1 is actually further away from the target sample mean of x 3 compared with n 4 5 1 so that the search direction of the optimizer may be misled although this variance can be reduced with a larger sample size or a longer simulation period it can never be completely eliminated to overcome this problem during optimization the random number seed is held constant when producing the stochastic replicates this ensures that any changes made to the parameters during the optimization process will lead the search in the desired direction using the same example in the lower panel of fig 3 we show 50 samples drawn from both x n 4 0 1 and x n 4 5 1 with the same random seed used for each pair of samples resulting in samples means of 3 9 and 4 4 respectively thereby indicating that n 4 0 1 is better at producing a target sample mean of x 3 in this way the stochastic generator is able to search through the correct directions on the parameter space to find parameters that converge toward the target rainfall attributes as discussed in section 2 2 it is important to emphasize that the objective of the approach is to generate samples of hydrometeorological time series with specific levels for each attribute rather than to identify the parameter sets that will produce those parameters in a population sense returning to the above example the objective is to find a stochastic replicate with sample mean x 3 regardless of the values of the parameters μ σ used to achieve this value consequently once this goal has been met the search can stop and the parameter values that were used to produce the stochastic time series corresponding to each target location can be discarded 3 case study the proposed inverse approach is illustrated on rainfall data from a catchment in south australia using two stochastic rainfall generators the richardson model and the wgen model to provide a benchmark for the proposed inverse approach its performance is also compared with that of a forward approach the rainfall data stochastic rainfall generators and the specific implementation of the forward and inverse approaches are described in this section 3 1 data we used a rainfall time series from a gauge in the southern mount lofty ranges close to adelaide south australia as a case study the climate in this region is temperate with most rainfall occurring in winter and spring may to october the mean annual rainfall was 913mm for the study period from 1989 to 2004 the daily rainfall data over this period have been used to represent the baseline historical rainfall conditions we used four rainfall attributes as the dimensions of the exposure space with definitions and baseline values provided in table 1 these attributes represent key features of rainfall patterns namely the average daily rainfall pd the wet day frequency wd a measure of the rainfall intermittency cdd and a measure of extreme rainfall pex99 these attributes have been commonly used to assess the performance of rainfall generators chen and brissette 2014 fowler et al 2007 hashmi et al 2011 kilsby et al 2007 semenov 2007 and are also closely related to several of the indices used for the detection and attribution of climate change as described by the expert team on climate change detection and indices etccdi klein tank et al 2009 for each rainfall attribute we defined a plausible range for sampling which defined the range of each dimension within the exposure space of between 50 and 150 of the corresponding historical value these bounds were wider than would be expected from most climate change projections e g csiro and bureau of meteorology 2015 stocker et al 2013 to encompass a large range of climate projections for example from climate models in the exposure space 3 2 stochastic rainfall generators two versions of the richardson type stochastic rainfall generator with different levels of complexity were used to generate the exposure space we started with a simplified four parameter model which assumes uniform rainfall characteristics over the year the advantage of this model is that it is possible to analytically derive the parameters that correspond to each target location in the exposure space however this simplified model uses a single value for each parameter throughout the year and thus is unable to capture seasonal scale variability to highlight some practical issues with rainfall sampling we then considered a more complex and widely used model namely the wgen richardson and wright 1984 3 2 1 the four parameter richardson model the simplified richardson type rainfall generator uses the following four parameters the two parameters of the 1st order two state markov chain used for representing the transition probabilities of rainfall occurrence pdd dry dry probability and pwd wet dry probability and the two parameters of a gamma distribution for representing the rainfall intensity on wet days α scale and β shape an approximate analytical expression relating two of the four output rainfall attributes pd and wd to the model parameters is given in dubrovský et al 2000 as 2 pd α β wd 365 25 3 wd 365 25 1 p dd 1 p dd p wd these analytical expressions have been used when exploring the implications of random sampling issues on the inverse generation approach section 4 1 3 3 2 2 the wgen model the wgen model richardson and wright 1984 has the same structure as the simplified richardson model except that it uses a unique set of the four parameters for each month of the year leading to a total of 48 parameters this model has been used widely for climate impact studies and is generally shown to capture most of the key features of daily rainfall series bastola et al 2011 katz 2002 kim et al 2007 since the proposed inverse approach involves optimization of the parameter values a search space with low dimension i e consisting of a small number of parameters as decision variables is desired to reduce the size of the parameter space in the inverse approach the number of decision variables to be considered was reduced from 48 to eight by fitting harmonic functions to describe the seasonal variations of each parameter prudhomme et al 2013 the harmonic function takes the form of 4 β t β 0 a cos 2 π t t φ where β t represents one of the four parameters during month t 1 t with t 12 β 0 represents the arithmetic mean of the parameter a represents the amplitude and φ corresponds to the month where the maximum occurs it is worth mentioning that although parameter φ can be varied as part of the optimization the four attribute exposure space in this case study was not designed to focus on shifts in rainfall seasonality section 3 1 so that φ was held constant at its historically optimal value to determine the value of φ we obtained the monthly estimates of pdd pwd α and β based on the method in richardson 1981 using the historical rainfall data and fitted a harmonic function to each parameter fig 4 the corresponding values of φ were thus identified to be 2 1 8 and 1 i e february january august and january for the four parameters respectively as a result the optimization was performed on the mean β 0 and amplitude a of each of the four model parameters leading to an eight dimensional search space 3 3 sampling approach as illustrated in fig 1 application of the forward approach involves sampling the parameter space prior to using the stochastic model similarly application of the inverse sampling approach involves the identification of target locations in the exposure space as the basis for optimization one approach to sampling both the parameter space in the forward approach and exposure space in the inverse approach is to define a grid of evenly spaced points over the entire space however this can be inefficient particularly for high dimensional problems for a large number of parameters attributes in the exposure space in the forward inverse approach for example if one wished to sample on a grid of width 10 for the parameter space of the four parameter richardson model then it would be necessary to evaluate a total of 104 10 000 separate parameter sets this issue is particularly pertinent for the inverse approach since optimization is required to find a parameter set that corresponds to each point in the exposure space therefore to provide even coverage of the parameter or exposure space while keeping the sample sizes low two structured sampling techniques have been employed namely latin hypercube sampling lhs and improved distributed hypercube sampling ihs the objective of the analysis in this paper is to illustrate the inverse approach by comparing its performance with the forward approach therefore for consistency the objective of the sampling approach was to obtain 100 samples within the exposure space for the forward approach it is not known a priori whether a particular parameter set in the parameter space will yield a sample in our exposure space i e within the plausible range of 50 150 for each rainfall attribute as defined in section 3 1 so that the number of samples that need to be drawn from the parameter space is not known to determine the total number of samples in a computationally efficient manner we used the latin hypercube sampling lhs method which allows starting with a small sample size and adding new samples while keeping the previously generated ones the lhs method involves sampling m variables with a desired sample size n by dividing the range of each variable into n equally probable intervals n samples are then drawn so that any interval for each variable is only sampled once stein 1987 to add n new sample points the existing design is re divided into n n intervals the n old samples are kept which occupy n intervals and then n new samples are drawn to fill the remaining n intervals unlike the lhs method the ihs method manteufel 2001 beachkofski and grandhi 2002 requires that the number of samples be specified a priori but ensures more even coverage of the sampling space this latter feature is attractive when sparsely sampling potentially high dimensional spaces and is therefore recommended to determine the target locations in the exposure space for the inverse sampling approach the ihs method is similar to the lhs method with two additional objectives 1 the average minimum distance between sample points equals the optimal distance dopt that is if the span of each output variable is normalized to 1 so that the entire sample space is a hypercube of volume 1 then each sample point should cover an equal hypervolume with dimension of m within the entire space this gives the optimal distance between sample points i e d opt 1 n sample m 2 the coefficient of variance cov of all minima between each pair of sample points is close to zero 3 4 implementation of forward approach as mentioned previously the forward approach has been used to provide a benchmark against which the utility of the proposed inverse approach can be assessed the approach involves the following steps 1 the parameter space is constructed by selecting appropriate ranges of the parameters for the stochastic generators 2 parameter sets are drawn from the parameter space using a sampling strategy such as the lhs method described in section 3 3 3 the sampled parameter sets are used to generate time series of the hydrometeorological variables of interest in this case rainfall and 4 the values of the attributes that define the exposure space are calculated for each of the generated hydrometeorological time series for the simple four parameter richardson model the transition probability parameters pdd and pwd both vary between 0 and 1 the two rainfall intensity parameters α and β are for the gamma distribution and should be greater than 0 note that their values are mostly between 0 and 1 when calibrated to historical data see richardson and wright 1984 from a preliminary analysis for our case study α and β values of 0 56 and 0 10 were obtained respectively yielding rainfall time series with attributes that are close to the historical data therefore although α and β do not physically have upper bounds and can take any value above 0 their ranges were set to be between 0 and 1 in the forward approach based on their historical values the use of such a small range ensures that the parameter space surrounding the historical levels of the parameters is sufficiently sampled for wgen the parameter ranges were defined in a similar way so that the bounds of both the transition probability and rainfall intensity parameters were set to 0 and 1 for all months as mentioned previously for both stochastic models lhs was used to sample the parameter space an initial latin hypercube sample size of 100 parameter sets was used and this was incremented until 100 rainfall time series were generated with attributes within the plausible bounds of the exposure space 3 5 implementation of proposed inverse approach a general description of the inverse approach was provided in section 2 2 the ihs method section 3 3 was used to determine the target locations for the optimization which consist of 100 sets of combined levels of the four rainfall attributes that uniformly cover the exposure space for each target location the best fit parameter sets for both the four parameter richardson model and the wgen model were identified using optimization the shuffled complex evolution algorithm duan et al 1993 was used as the optimization engine due to its proven ability for solving complex optimization problems in hydrological studies gupta et al 1999 thyer et al 1999 wang et al 2010 based on the general formulation in eq 1 the objective function to be minimized for both stochastic models was 5 f obj i pd s i pd his pd his pd i pd his pd his 100 2 wd s i wd his wd his wd i wd his wd his 100 2 cdd s i cdd his cdd his cdd i cdd his cdd his 100 2 pex 99 s i pex 99 his pex 99 his pex 99 i pex 99 his pex 99 his 100 2 the constraints of the optimization consist of the plausible ranges of the parameters for both models as mentioned in section 3 4 the plausible range for the probability parameters pdd s and pwd s is between 0 and 1 for the intensity parameters α s and β s which do not have a physical upper limit we defined the range to be between 0 to 104 which was wider than the range used for the forward approach section 3 4 to enable more extensive searching within the defined range for the wgen model since a harmonic function has been fitted to the monthly values of each of the probability and intensity parameters section 3 2 2 the actual decision variables for the optimization were the parameters of the harmonic functions i e β 0 and a which represent the mean and amplitude respectively as in eq 4 to ensure that the probability parameters were always within 0 and 1 while the intensity parameters were always within 0 and 104 during the optimization process the values of the mean and amplitude for each of these parameters have been optimized sequentially in the first step the mean value of each parameter has been optimized with the amplitude kept as zero once the mean has been determined a second optimization was conducted to estimate the amplitude for example if the mean of pdd is found by the optimizer to be 0 3 in the first step its amplitude must be constrained between 0 and 0 3 in the second step to avoid values of pdd going beyond 0 and 1 it should be noted that in determining the target locations the ihs only checks the multi dimensional uniformity of the overall distribution without considering the physical interpretation for each individual target location therefore it is important to ensure that each target location selected is physically plausible for example pd should always be less than pex99 and wd should never exceed 365days for this study these constraints were automatically satisfied because a relatively small plausible range of 50 150 was selected for each attribute if the rainfall samples are required to show larger variances it may be necessary to impose additional constraints in the optimization procedure to ensure the resultant samples remain physically plausible 4 results 4 1 the four parameter richardson model 4 1 1 forward approach the coverage of the exposure space obtained by applying the forward approach to the four parameter richardson model is shown in fig 5 which shows high variances in some rainfall attributes in particular the generated pd cdd and pex99 can go up to 15 000 6000 and 80 000 of their corresponding historical values respectively fig 5a which are well outside the bounds of the exposure space the sampled wd has lower variance with values up to only 226 of the historical values since a year contains a maximum of 365 or 366days however these values are still above the upper limit of the exposure space of 150 the high variance leads to low sampling efficiency to obtain 100 sets of combined levels of rainfall attributes within our exposure space a total of 7635 lhs samples of parameter sets had to be generated i e 98 7 samples were unacceptable and discarded all 7635 sets of rainfall attributes are plotted in fig 5a with the 100 plausible samples shown in fig 5b in addition to the issue of inefficient sampling based on both a visual inspection of the coverage on the exposure space as well as consideration of the correlation coefficients it is clear that the coverage of the exposure space is uneven fig 5b in particular samples are clustered in small regions of the exposure space for each rainfall attribute with other parts of the space receiving limited or no coverage for example the correlation between pd and pex99 is quite high which results in better coverage over regions closer to the diagonal of the joint distribution of pd and pex99 than other regions the above problems with using the forward approach are most likely due to the non linear translation from parameters to rainfall attributes through the stochastic generator so that large variations in certain regions in parameters space result in small variations in exposure space and vice versa this non linearity will be further illustrated in the next section with the distribution of parameters identified through the inverse approach 4 1 2 inverse approach fig 6 a shows the 100 target locations of desired rainfall attributes that have been determined using the ihs approach section 3 3 as can be seen the ihs approach generates samples that appear to be uniformly distributed across the exposure space with even coverage across each attribute and low cross correlations between attributes the final set of combined levels of attributes corresponding to each of the 100 stochastically generated rainfall time series obtained using the inverse approach is presented in fig 6b as can be seen the optimization based approach is effective in producing the desired levels of rainfall attributes i e target locations with all of the 100 samples falling within the bounds of the exposure space and with relatively even coverage of the exposure space fig 6b therefore the inverse approach delivers much better coverage of the exposure space than the forward approach fig 5 fig 7 shows the values of the 100 parameter sets for the four parameter richardson model identified via application of the inverse approach highlighting the non linear mapping between parameter space and exposure space this is best illustrated with the non uniform distribution of the best fit parameters in contrast to the uniform distribution of the exposure space fig 6 furthermore the parameters have considerably different ranges compared with the a priori 0 1 ranges that were specified for the forward approach for example the values of pdd generally vary within a narrower range of 0 5 0 9 whereas values of α are as high as 10 therefore the ranges of 0 1 defined for the four parameters in the forward approach as detailed in section 3 4 can significantly limit the resultant coverage of the exposure space this also reflects the high degree of non linearity in the mapping between the parameter values and the exposure space as a small change in the exposure space may result in a large shift in parameter space interestingly for the case study considered although the inverse approach had the additional step of parameter optimization the computational time required to obtain 100 samples was 32 6 shorter than for the forward approach this is likely due to the large number of samples that were discarded in the forward approach 4 1 3 implications of random sampling on the inverse approach in the above example we fixed the random seed of the random number generator during the optimization process due to reasons discussed in section 2 3 to illustrate the importance of this aspect of the optimization we use the analytical expressions in eqs 2 and 3 to estimate the model parameters that will yield individual target locations from a grid consisting five evenly spaced levels for each of wd and pd 50 75 100 125 and 150 of their historical values these locations within the exposure space are given as green dots in fig 8 we then generated 100 stochastic replicates from each of these parameter sets with different random seeds which are shown as blue and red scatter about each of the target locations in fig 8 the stochastic nature of the model is clear for all target locations for each parameter set the 100 replicates of wd vary up to 10 around their target level which is similar for all target levels of wd and pd in contrast the 100 replicates of pd are closer to the target level for smaller pd e g up to 10 around where the target level is 50 while for larger pd target levels the spread among replicates increases substantially e g up to 40 around where the target level is 150 compared with the sampling resolution required in this study shown in fig 6a the variability in fig 8 is in fact much higher which can adversely affect the capacity of the optimizer to find parameters that correspond to each target location as discussed in section 2 3 4 2 the wgen model 4 2 1 forward approach the coverage of the exposure space obtained by applying the forward approach to the wgen model is shown in fig 9 similar to the results for the four parameter model section 4 1 1 the forward approach shows low efficiency to obtain 100 sets of rainfall attributes within the range of the exposure space 1453 lhs samples of wgen parameter sets were required fig 9a which means that 93 1 of samples were discarded with the 100 plausible sets in fig 9b the coverage of the exposure space is poor which is also evident through the high pairwise correlations such as between pd and pex99 and between wd and cdd 4 2 2 inverse approach to examine the performance of the inverse approach with wgen the 100 target locations which have been determined using the ihs approach section 3 3 are plotted in fig 10 a the final optimized set of attributes corresponding to each of the 100 stochastically generated rainfall time series is presented in fig 10b the inverse approach is generally effective in evenly covering the exposure space and reproducing these target locations in particular this approach delivers much better coverage of the exposure space than the forward approach fig 9 in the following aspects 1 all of the 100 samples are within the plausible output space defined in table 1 suggesting effective control over the values of individual rainfall attributes and 2 the joint distribution of multiple rainfall attributes is much more uniform across the exposure space and the pairwise correlations between different attributes are reduced as an alternative approach to assessing the relative uniformity of the sampling in the exposure space the minimum distances between sample points in the exposure space are compared for both the forward as orange dots in fig 11 and inverse approaches as blue dots in fig 11 the results show that the inverse sampling approach produces a more uniform coverage as the minima between sample points are closer to the optimal distance dopt 0 32 see section 3 3 furthermore the coefficient of variance cov of these minimum distances is also much lower i e 0 52 for the inverse approach compared with 2 90 for the forward approach it is worth noting that to obtain 100 sample points on the exposure space with the wgen model the overall execution time required for implementing the inverse approach is 73 longer than that for the forward approach this is most likely due to the difficulty in solving optimization problems with a larger number of parameters as a result of the larger search space that has to be explored however the inverse approach ensures uniform coverage of the exposure space with the desired resolution which is the key objective for constructing the exposure space in contrast the forward approach failed to obtain such coverage therefore although associated with a higher computational expense the proposed inverse approach is the only way of achieving the desired coverage of the exposure space 5 discussion this study presented a framework for sampling various rainfall conditions to construct an exposure space for scenario neutral climate impact assessments here we discuss some practical considerations as well as possible future adaptations of the framework 5 1 design of exposure space to represent more complex potential climate changes the four rainfall attributes considered in the exposure space for this study i e pd wd cdd and pex99 are good descriptors of a range of changes of annual precipitation characteristics however there is also a range of other rainfall attributes that might be important when considering the impact of climate change such as changes at seasonal or interannual timescales e g christensen et al 2007 johnson et al 2011 kwon et al 2009 furthermore potential future variations in other climatic features such as temperature solar radiation and evapotranspiration may also have a substantial impact on water resources for examples see chiew and mcmahon 2002a prudhomme and williamson 2013 the inverse approach presented here is sufficiently flexible to cater to all attributes in the exposure space that are of interest provided they can be generated with an appropriate stochastic generator although this comes at the expense of additional computational cost considering the trade off between the flexibility of producing different climate patterns and computational effort it is important to identify key hydrometeorological variables of interest as well as their attributes based on an understanding of the behavior of the system being analyzed depending on the specific hydrometeorological variables involved the format of the objective function may require modification from eq 1 which was designed assuming multiplicative perturbations to attributes e g changes expressed as a percentage of the historical value for example temperature changes are typically represented in an additive way e g increases in temperature by degrees celsius for examples see chiew and mcmahon 2002a kingston et al 2009 and this would require an adjustment to the objective function in eq 1 in this study the boundary of the exposure space was set at 50 150 of the historical values of each attribute which is sufficiently wide to incorporate a large number of possible changes in each of the rainfall attributes while also using the same percentage changes across attributes to facilitate illustration however this framework can be easily adapted to incorporate tailored bounds for the exposure space which should be carefully selected to suit the case study under consideration in particular if the bounds deviate too far from present conditions a significant portion of samples will be unrealistic even when extreme climate change impacts are considered conversely if the bounds are too narrow system response to some plausible climatic changes might not be considered whateley et al 2014 multiple sources of information could be considered in selecting these bounds including gcm based climate projections e g collins et al 2013 of possible future climatic changes and additional lines of evidence on possible changes to key variables such as from long term paleoclimatology reconstructions e g ault et al 2014 hansen and sato 2012 ho et al 2015 in addition it is worth specifying an exposure space with bounds that are wider than the range suggested from all currently available sources of information so that additional climate change projections can be included in the analysis as they are developed steinschneider and brown 2013 finally when determining the target locations consisting of different hydrometeorological attributes on the exposure space it is desirable to ensure the physical realism of each individual location so that corresponding time series can be obtained with the aid of stochastic weather generators this requires not only ensuring that the target levels of individual attribute are realistic such as the constraints for the levels of wd as discussed in section 3 5 but also maintaining physically plausible relationships among multiple attributes for example a target location cannot consist a wd value of 100days with a cdd value of 300days because this combination means that the annual average wet day is 100days while the annual average dry spell length is 300days which is physically unrealistic 5 2 stochastic generation of the exposure space in this study we used a sample size of 100 to represent different levels of changes for each individual attribute considered in the exposure space with fixing the random seed across replicates to facilitate improved convergence during the optimization process in this way however there is likely to be limited variability in between time series corresponding to different points on the exposure space except for variations related to the target statistics this issue can be addressed in at least three ways 1 increase the sample size and thus the coverage resolution in the exposure space increasing the exposure space resolution is likely to be particularly useful when the number of attributes increases as this will lead to a corresponding increase in the dimensionality of the exposure space 2 the length of each sample can be increased currently the length is equal to the length of the historical data series i e 15years however it would be trivial to allow the simulation to run for longer periods of time to obtain greater stochastic variation this will require the same number of optimized parameter sets although because of the use of the same initial seed there will still be significant similarities between individual samples 3 the procedure can be repeated multiple times with different random seeds for each iteration thereby generating multiple replicates this would substantially increase the level of stochastic variability although at the expense of additional computational time the appropriate sample size length of each sample and number of replicates are likely to depend on the case study considered as well as available computational resources 5 3 equi finality in the optimization process when using optimization to search for best fit solutions equi finality issues are likely to arise i e multiple solutions leading to the same results in the objective function therefore they are not distinguishable during optimization see beven and freer 2001 this problem is further complicated within the proposed inverse approach as the values of the objective function for optimization are based on results from stochastic models equi finality issues are likely to be greatest for low dimensional exposure spaces since higher dimensional exposure spaces add constraints to the parameter space for example the chance that two contrasting combinations of pdd and pwd lead to the same combination of wd and cdd is much lower compared to that resulting in the same level of wd in isolation thus increasing the number of attributes considered could have the additional advantage of reducing the number of feasible parameter sets to be considered it is worth noting that although equi finality is likely to occur when the proposed inverse approach is implemented the aim of the approach is to identify time series of outputs from the stochastic generator that result in desired values of the attributes included in the exposure space and not to the identification of the resulting parameters in the stochastic generator as discussed in section 2 3 however when different parameter sets lead to the same combination of attributes on the exposure space the different time series of hydrometeorological variables which they produce can consist of varying degrees of physical realism therefore checking the physical realism of the generated time series can potentially help to eliminate unrealistic parameter sets and thus resolve any equi finality issues 5 4 computational efficiency and execution time in our particular implementation of the proposed inverse approach the computational time required to produce 100 evenly distributed samples is around eight hours using an intel xeon e3 2 60ghz 8 cores processor with 32gb ram for both the four parameter model and the wgen suggesting a relatively high computational demand however in general the computational effort required is dependent on a number of practical specifications including the operating system programming language and algorithm used as the key focus of this study is to introduce and illustrate a new method the above mentioned specifications have not been optimized for computational efficiency we have used the r package lhs carnell 2012 for obtaining samples over the exposure space with the ihs and lhs methods together with the shuffled complex evolution algorithm embedded in the r package hydromad andrews and guillaume 2013 for solving the optimal parameter values for the stochastic generator we have also developed our own r scripts to execute the four parameter richardson and the wgen models it is expected that an improved integration of these different modeling components with the aid of other programming languages such as fortran or c will further increase computational efficiency 6 conclusions generation of exposure spaces for scenario neutral climate impact assessments should consider a range of potential variations in relevant hydrometeorological variables including shifts in the average intermittency variability and extremes the exposure space describes the range of conditions of interest that a system may be exposed to under a future climate and this paper presents and demonstrates an inverse approach to stochastically generating hydrometeorological time series to uniformly cover this exposure space the utility of the proposed inverse approach is benchmarked against a forward approach for rainfall generation for a south australian catchment using two richardson type stochastic rainfall generators of varying complexity the results highlight the highly non linear translation from parameter space to exposure space and thus the need for the proposed inverse approach in order to obtain a relatively uniform coverage of the exposure space for both models the inverse approach demonstrates better control of the sampling range with 100 of samples falling within the exposure space furthermore the uniformity of the coverage of the four dimensional exposure space is substantially improved several potential adaptations for future implementations of the framework have been discussed including 1 the design of the exposure space to represent more complex changes in climate 2 improvements to the way that stochastic samples in the exposure space are generated 3 ways of reducing the effects of equifinality during the optimization process and 4 methods for increasing computational efficiency the flexibility of the proposed inverse approach enables consideration of all climate attributes of interest at the desired resolution thereby expanding the applicability of the scenario neutral approach to evaluating a water resource system s sensitivity to a wide range of plausible changes in climate acknowledgements the authors wish to thank christel prudhomme and an anonymous reviewer for their thoughtful comments on the manuscript seth westra s time was supported by australian research council discovery project dp150100411 appendix a see table a 1 
7594,long range dependence lrd the so called hurst kolmogorov behaviour is considered to be an intrinsic characteristic of most natural processes this behaviour manifests itself by the prevalence of slowly decaying autocorrelation function and questions the markov assumption often habitually employed in time series analysis herein we investigate the dependence structure of annual rainfall using a large set comprising more than a thousand stations worldwide of length 100years or more as well as a smaller number of paleoclimatic reconstructions covering the last 12 000years our findings suggest weak long term persistence for instrumental data average h 0 59 which becomes stronger with scale i e in the paleoclimatic reconstructions average h 0 75 keywords long range dependence hurst behaviour long term persistence rainfall variability precipitation reconstructions proxy records 1 introduction since hurst 1951 brought long term persistence also known as long range dependence lrd into scientific discourse the interest in this time series behaviour has been rising this is mainly due to its serious implications into the modelling and design processes in various scientific fields and particularly in water resources o connell et al 2015 another fact significantly contributing to its growing popularity is that lrd has been identified in many climatic variables such as temperature pelletier 1998 koutsoyiannis 2003 rainfall fraedrich and larnder 1993 pelletier and turcotte 1997 wind power haslett and raftery 1989 and the north atlantic oscillation index stephenson et al 2000 hurst behaviour also has a strong physical basis as it is derived from the principle of entropy maximization koutsoyiannis 2011a a principle which can be used to determine the theoretical probability distribution model for rainfall papalexiou and koutsoyiannis 2012 more detailed discussion on the history and relevance of the hurst behaviour can be found in the recent review paper by o connell et al 2015 in this analysis we aim to investigate the dependence properties of annual rainfall studies regarding lrd in annual rainfall are usually limited to a specific area and or utilize datasets of relatively short lengths kantelhardt et al 2006 bunde et al 2013 zhai et al 2014 short record lengths can introduce bias into the estimation of long term persistence properties which in general need more than 100years in order to avoid underestimation and in cases of very strong dependence even more than 1000 koutsoyiannis and montanari 2007 a majority of other studies investigate the dependence structure of rainfall at sub annual or even smaller scales papalexiou et al 2011 but in that case the phenomenon gets complicated due to the combined effects of seasonal variation and intermittency on the other hand paleoclimatic reconstructions suggest strong lrd behaviour in multi decadal to centennial time scales pelletier and turcotte 1997 markonis and koutsoyiannis 2016 evidently there are still ample grounds for research on the existence of lrd in annual precipitation herein we have analyzed more than one thousand annual precipitation records of length of a hundred years or more from different areas of the world as well as approximately 70 paleoclimatic records spanning the time from 12 thousand years ago until the present day to quantify lrd we estimated the hurst coefficient by applying two algorithmic versions of the aggregated variance method and employed monte carlo method to identify a common hurst coefficient for all the records additionally we performed a simple test on the autocorrelation structure of the first few lags to examine whether the hypothesis of a markovian autocorrelation structure is justified or not finally we investigated the effect of time scale and record length on lrd estimation using the paleoclimatic series 2 dataset the instrumental data were obtained from the global historical climatology network ghcn daily http www ncdc noaa gov oa climate ghcn daily which contains daily data from more than 50 000 land surface stations around the globe a significant percentage of these records exhibit the typical issues of most datasets available i e missing values short record length and rainfall values of questionable quality such as unrealistic outliers in order to restrict data quality to a significantly high level we filtered the dataset using certain criteria initially we chose to study only the stations satisfying the following conditions a record length over 100years b missing daily values percentage less than 20 of the record length and c suspect values with quality flags less than 0 1 this first quality screening resulted in 3477 stations of daily data with lengths varying from 100years to 173years then in order to construct the annual series we first deleted all daily values assigned quality flags indicating unrealistically large values and then estimated the average daily value per year notably because of the existence of missing values within some records summing up daily values to obtain the annual total would result in smaller estimates than the real ones instead it is more robust to estimate the daily mean values per year as the mean value estimate can be accurate even in the presence of some missing values this is equivalent to estimating the annual total by first infilling the daily missing values of a year with the daily average of the year years having more than 20 missing daily values were considered missing and their annual estimate was not derived via the abovementioned method the 3477 daily stations are aggregated to the annual time step among these stations there are different combinations of record lengths and number of missing yearly values e g 558 stations having 100years in a sequence with no missing values 1474 stations with more than 100 annual values and only eight stations without any missing values we choose to analyze 1265 stations having more than 100 annual values and a missing yearly values percentage less than 15 obviously this choice ensures a higher quality dataset for our analysis on the annual time step paleoclimatic data can also be used to determine the existence of lrd in climatic variables since they cover quite larger scales compared to the instrumental data sets mandelbrot and wallis 1969 koutsoyiannis 2003 bunde et al 2013 markonis and koutsoyiannis 2013 herein we used 68 paleoclimatic records of rainfall reconstructions located mainly in the northern hemisphere to explore time scales reaching up to 12 000years three different data sets were used corresponding to the proxy variable used for the reconstruction process i e tree rings speleothems and other including lake sediments pollen corals and multi proxy reconstructions the 40 time series of tree ring reconstructions are the largest data set and have a mean sample size of 900 values at an annual time scale see table 1 the other two data sets with 16 and 12 time series correspondingly in most cases have fewer values and varying time resolutions ranging between 1 and 100years it must be noted that the speleothem records are proxy records of δ18o a variable which is linearly linked to rainfall 3 analysis and results 3 1 aggregated variance method the method employed herein is based on the study of the variability of the data averaged at different timescales the method is typically referred to as aggregated variance method but what it actually aggregates is the timescale and not the variance specifically let x j be a stationary process in discrete time j referring to years in our case with standard deviation σ and let 1 x j k 1 k l j 1 k 1 jk x l k 1 2 3 denote the averaged process at timescale k with standard deviation σ k in the case of an uncorrelated process the standard deviation of x j k is obtained by σ k σ k in other cases e g if the process is fractional gaussian noise or a hurst kolmogorov process the abovementioned law is invalid instead one obtains the elementary scaling property 2 σ k k h 1 σ where h is the hurst coefficient which for stationary and positively correlated processes varies in the range 0 5 1 beran 1994 the value of h 0 5 denotes time independence while smaller values are indicative of anti persistence the autocorrelation of the aggregated process is independent of the scale of aggregation k and is given as follows 3 ρ j k ρ j 1 2 j 1 2 h j 1 2 h j 2 h j 0 to apply the method to the data we used a graphic tool the climacogram koutsoyiannis 2011b which is the double logarithmic plot of the standard deviation σ k of the aggregated time series at scale k versus the time scale k the h value is estimated as the slope of the fitted line least squares regression in a variant of that method the estimation bias of the standard deviation which depends on the time scale of aggregation is also considered see section 3 2 below each averaged time series is constructed as follows for every scale k the data are divided into n groups the number of which is obtained as the fraction of the data length l versus the scale value k for example in time scale k 4 120years would be divided in 30 non overlapping groups of 4years subsequently the values within each group are averaged according to eq 1 however when missing values are encountered the process of averaging may become problematic depending on the number of missing values if more than a half of the values is missing then the estimate would be quite uncertain markonis 2015 to overcome the issue we use a simple criterion on the number of missing values before estimating the averaged series within each group a for scale k 2 the average value is estimated only when both values exist b for scales k 3 the average value is estimated only when there are at least three values within the group according to the latter rule we estimated the averaged series for all the scales between k min and k max where k min 1 and k max l 10 so that the variance in the maximum scale is estimated from at least 10 values koutsoyiannis 2003 for a 100 year record length this would be the variance of the decadal means the results of the algorithm implementation for the instrumental data are shown in table 2 and fig 1 suggesting some evidence of weak long range dependence more specifically it was found that 85 of the data exhibit h 0 5 yet with notable variation for example only half of the data show h 0 59 i e a more pronounced dependence structure a very strong dependence structure h 0 80 is reported for 2 5 of the records while for 15 of them we observe lack of dependence for the 95 confidence interval h values fluctuate between 0 4 and 0 8 in paleoclimatic data the hurst coefficient shows a tendency for higher estimates as well as an increased range of values fig 2 in order to test the effects of our parametric choices for the value of the minimum and maximum scale we examined how the median and the variance of h estimates vary for different k min and k max as can be seen in fig 3 the variance of the hurst parameter estimate becomes larger as the value of the minimum scale k min increases yet the value of the median in the estimate remains the same therefore our choice of k min 1 is well justified since greater values of k min only amplify the uncertainty in h estimation in addition the observation of the same median strengthens our hypothesis of the lrd structure because in the alternative hypothesis of short term dependence we would notice some change in the climacogram curvature and correspondingly to the logarithmic slope the results for the k max were similar it can be seen in fig 4 that the decrease in the number of values in the last scale increases the variance of the hurst parameter estimate in this case too therefore the choice of n 10 leads to more reliable results compared to using smaller values of n 3 2 least squares based on standard deviation method lssd koutsoyiannis 2003 demonstrated how the use of the classical estimator for the standard deviation can introduce significant negative bias in the estimation of the hurst parameter by the aggregated variance method this is because the hypothesis of independence which is a necessary condition for the use of the estimator is violated in the case of processes with strong lrd behaviour this shortcoming may be overcome by the use of the least squares based on standard deviation method lssd koutsoyiannis 2003 tyralis and koutsoyiannis 2011 which performs a simultaneous estimation of the hurst parameter h and the standard deviation σ using an approximately unbiased estimator for the latter here for simplicity reasons we applied the lssd method tyralis and koutsoyiannis 2011 only to the sample of the 558 44 of the total stations with no missing values and then compared our estimate with the one obtained by the aggregated variance method for the same sample as shown in table 3 and fig 5 the two methods show small deviations from each other overall the value of the bias fluctuates between 1 and 2 with the bias in the estimate of the average being approximately 1 the bias is negligible in this case because the estimated hurst parameter is not very high 3 3 monte carlo testing we also investigated the assumption that the observed distribution of the sample estimates of h results from a single model with a specific true value sometimes referred to as population value of the hurst coefficient in order to produce a theoretical sample of time series exhibiting hurst dynamics we used a simple algorithm that generates fractional gaussian noise based on a multiple timescale fluctuation approach koutsoyiannis 2002 we generated 1265 time series from a gaussian distribution that reproduce the record length the mean and the standard deviation of the empirical sample repeated the same procedure for several theoretical h values and then estimated the empirical ones we should note that there may be some cases where moderate departures from normality are observed for the annual rainfall distribution especially in arid semi arid regions of the world or regions severely affected by the el nino southern oscillation enso still the use of the assumption of normality for the synthetic records albeit simplifying is justifiable as for the majority of the stations the central limit theorem holds therefore normality is a good approximation while in general using monte carlo experiments we were able to find that the estimation of the hurst coefficient is practically insensitive to the underlying distribution subsequently the distribution of the empirical estimates for the synthetic time series was compared to the distribution of the empirical estimates for the historic time series used in the analysis it appears that the value of h 0 58 fig 6 yields the most satisfactory match however it is worth noticing that that 2 5 of the stations exhibiting h 0 8 are outside the range of the theoretical distribution 3 4 autocorrelation analysis the estimated hurst coefficient is not high enough to allow for any certain conclusion on the type of the dependence structure since relatively low hurst coefficients 0 5 0 6 can be estimated when there is short range dependence or no dependence at all due to algorithmic inadequacies sample bias and estimation uncertainty to this end we have employed the autocorrelation function to further examine the dependence properties of rainfall still one should keep in mind that the classical autocorrelation estimator as in the case of standard deviation is biased downwards koutsoyiannis 2003 dimitriadis and koutsoyiannis 2015 however since the estimator is biased downwards any result in favour of ltp would mean that in reality the ltp is even stronger the autocorrelation coefficients of the first three lags for the instrumental data are low table 4 on further investigation we tested whether independence is a plausible scenario for the dependence structure of our data we produced 1265 independent i e uncorrelated time series of the same sample size and estimated the sample autocorrelation coefficients fig 7 it can be seen that for all three lags the value of the median of the historic data is greater than the one estimated from uncorrelated synthetic data this is more obvious in the case of autocorrelation of lag 1 where for confidence interval 95 the values of the independent data fluctuate in the range 0 175 to 0 173 while the historic ones are in the range 0 09 to 0 37 in addition in all three cases the historic samples exhibit significantly fewer negative values than the uncorrelated ones the above results could be typical for a markov process too also known as ar 1 process to address this issue a simple ad hoc test which exploits the distinctive properties of markov processes was designed under the markov hypothesis the theoretical autocorrelation coefficient for lag 2 would be estimated as ρ 2 ρ 1 2 where ρ 1 is the known empirical autocorrelation likewise the markovian autocorrelation coefficient for lag 3 would be given as ρ 3 ρ 1 3 the resulting theoretical estimate is compared to the empirical one for the same lag if the empirical value is higher than the theoretical ar 1 one then the markov hypothesis weakens we applied this comparison to the 52 of the stations for which all the autocorrelation coefficients for lags 1 3 are positive fig 8 it is evident that the empirical estimates are considerably higher than the theoretical ones resulting from an ar 1 structure and therefore the markov assumption becomes less likely in addition the empirical estimates do not follow the exponential convergence to zero of the markovian ones but instead remain approximately stable for lags 2 and 3 this is in agreement with the theoretical behaviour of lrd whose distinctive feature is the existence of slowly decaying autocorrelation function beran 1994 having tested the cases of independence and short range dependence we finally examined whether the autocorrelation structure is consistent with that of a fgn model via a visual comparison of the two in fig 9 the empirical autocorrelation coefficient ρ 1 is plotted against the corresponding empirical hurst coefficient h as obtained from eq 2 the theoretical autocorrelation values of a fgn model as obtained from eq 3 are plotted as well the diagram shows that the autocorrelation structure is consistent with that of a fgn model the deviation between the theoretical and the empirical estimates becomes greater in the region of high values of h still this is justified due to the increased negative bias in the autocorrelation estimation in that case 3 5 paleoclimatic data paleoclimatic data reveal a stronger form of dependence at larger time scales which is in good agreement with other relevant studies bunde et al 2013 franke et al 2013 in general we can divide proxy data into three categories based on their temporal resolution high resolution data with annual time step mainly tree rings medium resolution data with decadal time step mainly speleothems and low resolution data with centennial time increments such as lake sediments or pollen however certain factors should be taken in consideration regarding some fundamental uncertainties about rainfall reconstruction from proxy variables often tree rings represent areal reconstructions of precipitation and in some cases the area covered is rather large e g ce11 as defined in table 1 since the individual time series are cross correlated their aggregation to a single time series might increase h granger 1980 to address this issue a random cross examination of individual tree rings records was performed and it was confirmed that the individual series exhibit the same behaviour as the aggregated records in addition until recently the most common approach to transform the proxy variable tree ring width to the reconstructed one rainfall was through a method that involved detrending and or pre whitening of the original time series this methodology has a severe impact on the low frequency variability briffa et al 1996 helama et al 2004 and thus on h estimation as presented in fig 2 red diagonal lines detrended pre whitened time series have a mean h equal to 0 5 while the rest of the records which are derived using the regional curve standardization briffa et al 1992 or the neural networks ni et al 2002 methods have a mean near 0 72 on the other hand in some cases such as the time series with centennial time resolution the uncertainties in the correct estimation of the precipitation amount are so high that the record is strongly smoothed in order to depict only major shifts of the mean these data sets e g as05 or cc09 as defined in table 1 are of small sample sizes and thus h estimates are unavoidably pushed towards values that reach close to 1 fig 2 green 1 for interpretation of color in fig 2 the reader is referred to the web version of this article 1 diagonal lines even in larger data sets i e speleothems if the smoothing happens to be combined with a strong monotonic trend of the original data then again h values would falsely tend towards 1 fig 2 orange diagonal lines such estimates cannot be included in the estimation of h which finally reaches 0 75 for the paleoclimatic data however they cannot be totally neglected as they provide some qualitative evidence for the long term change in rainfall which includes both long term trends and abrupt shifts in the mean the effect of record length on lrd is further explored by partitioning the reconstructed records to smaller segments which can be achieved by estimating h through a moving time window of variable length i e 50 100 250 and 500years since the tree rings have annual resolution they are suitable for such analysis as the results are directly comparable with the estimates of the instrumental time series to limit any methodological uncertainties the cl02 dataset was used which contains 15 records with average h h equal to 0 75 has record length close to 1000years and is not detrended or pre whitened the results show that if the sample size is equal to the instrumental records 100years then lrd structure fluctuates between white noise h 0 5 and strong hurst behaviour h 0 9 with h 0 71 fig 10 as the record length increases h values are constrained to 0 7 0 8 and h converges to 0 75 the results are reproduced for an equal number of synthetic time series with similar size and lrd properties however the even higher values of hurst coefficient h 0 8 found in other paleoclimatic reconstructions fig 2 cannot be simply attributed to sample size bias their behaviour cannot be reproduced by synthetic time series this suggests that either rainfall presents different dependence structures in sub decadal and above decadal scales markonis and koutsoyiannis 2016 or that the stronger lrd is artificial introduced to the precipitation reconstructions through some intrinsic properties of the proxy variables e g karst transit time in speleothems dee et al 2015 4 discussion and conclusions the analysis of the global instrumental data set shows that there are notable indications of weak lrd in the annual rainfall as the hurst parameter is not very high the aggregated variance method induces only 1 2 negative bias in the hurst coefficient estimation and therefore the best population value of h that has been shown through monte carlo estimation to account for the observed sampling variability h 0 58 may be considered accurately representative for instrumental data the study of the autocorrelation function shows that it is consistent with the autocorrelation of a fgn model even though for a certain percentage of the stations the markov hypothesis could not be falsified specifically the existence of negative correlations in all three lags examined did not permit the application of the abovementioned method in the case of the 48 of the stations some studies using smaller data sets potter 1979 fraedrich and blender 2003 kantelhardt et al 2006 supported the appropriateness of the markov structure but they did not investigate the differences between actual and theoretical auto correlation in larger lags fig 8 these differences might be quite small and thus allow the stochastic modelling of annual rainfall as a markovian process for record lengths below 100years it has been shown though that they might have serious implications when it comes to the estimation of trend significance and as a result the observed changes in rainfall might be considered much rarer than they actually are cohn and lins 2005 lastly it was shown as well that the autocorrelation function significantly departs from the case of independence although the above findings are in favour of the existence of a stronger dependence structure than the one typically assumed in literature potter 1979 fraedrich and blender 2003 kantelhardt et al 2006 it seems that there is a discrepancy between smaller and larger time scales fraedrich and larnder 1993 pelletier and turcotte 1997 poveda 2011 ault et al 2013 to this end the most important source of uncertainty in the determination of lrd which is the record length should not be overlooked koutsoyiannis 2002 koutsoyiannis and montanari 2007 although using stations with relatively high compared to the majority of the existing rainfall data records record length the accurate detection of long range dependence cannot be guaranteed because this behaviour may require even longer record length to be revealed subsequently the low estimates of hurst parameter in instrumental time series could be attributed to the limited record length available in some cases and therefore should be considered characteristic only for this time horizon of approximately 100years this behaviour of lrd is illustrated in the case of paleoclimatic data with annual time step when the sample size of paleoclimatic data is restricted to match the one of instrumental data approximately 100 years the distribution of the estimated hurst coefficient exhibits a lower mean value together with an increased variance compared to the one arising from larger sample sizes however these results could not be reproduced for paleoclimatic series of longer time scales i e above decadal which suggests that the discrepancy in lrd structure i e the difference in the mean value of h between sub decadal annual as in the instrumental series and above decadal as in the paleoclimatic series scales may be nonetheless inherent in precipitation behaviour rather than being solely attributed to the sample size effect this finding is in good agreement with a recently published work by markonis and koutsoyiannis 2016 which emphasizes the influence of time scale when it comes to the analysis of the dependence of a time series in addition due to historical and socio economic reasons the data set does not include enough or any stations at all adequate for our analysis from certain regions of the world such as central africa and south america however the representation of climates according to the köppen classification system remains fairly good since a wide variety of climates is still represented in north america australia and central europe i e the areas which contribute the most to our dataset even so the possibility of a misrepresentation of climates cannot be excluded and this constitutes a source of uncertainty in our results and an area open for research in the future when more stations of larger record lengths will be made publicly available it is also important to consider the uncertainty induced due to measurement errors or false homogenization techniques which may introduce bias to the estimation of lrd steirou 2011 ghcn daily highlights the potential bias provoked by changes in instrumentation over the years and it is possible that this kind of bias could also affect the estimation of h ultimately the high variability of the results is in accordance with the inherent uncertainty of the phenomenon apart from algorithmic or data choices an important conclusion drawn from the analysis is that simplifying assumptions commonly used in practice such as inter annual independence may in cases significantly depart from reality and hence a thorough and careful study of the dependence properties of the dataset as performed here is recommended especially when longer time horizons are of interest 
7594,long range dependence lrd the so called hurst kolmogorov behaviour is considered to be an intrinsic characteristic of most natural processes this behaviour manifests itself by the prevalence of slowly decaying autocorrelation function and questions the markov assumption often habitually employed in time series analysis herein we investigate the dependence structure of annual rainfall using a large set comprising more than a thousand stations worldwide of length 100years or more as well as a smaller number of paleoclimatic reconstructions covering the last 12 000years our findings suggest weak long term persistence for instrumental data average h 0 59 which becomes stronger with scale i e in the paleoclimatic reconstructions average h 0 75 keywords long range dependence hurst behaviour long term persistence rainfall variability precipitation reconstructions proxy records 1 introduction since hurst 1951 brought long term persistence also known as long range dependence lrd into scientific discourse the interest in this time series behaviour has been rising this is mainly due to its serious implications into the modelling and design processes in various scientific fields and particularly in water resources o connell et al 2015 another fact significantly contributing to its growing popularity is that lrd has been identified in many climatic variables such as temperature pelletier 1998 koutsoyiannis 2003 rainfall fraedrich and larnder 1993 pelletier and turcotte 1997 wind power haslett and raftery 1989 and the north atlantic oscillation index stephenson et al 2000 hurst behaviour also has a strong physical basis as it is derived from the principle of entropy maximization koutsoyiannis 2011a a principle which can be used to determine the theoretical probability distribution model for rainfall papalexiou and koutsoyiannis 2012 more detailed discussion on the history and relevance of the hurst behaviour can be found in the recent review paper by o connell et al 2015 in this analysis we aim to investigate the dependence properties of annual rainfall studies regarding lrd in annual rainfall are usually limited to a specific area and or utilize datasets of relatively short lengths kantelhardt et al 2006 bunde et al 2013 zhai et al 2014 short record lengths can introduce bias into the estimation of long term persistence properties which in general need more than 100years in order to avoid underestimation and in cases of very strong dependence even more than 1000 koutsoyiannis and montanari 2007 a majority of other studies investigate the dependence structure of rainfall at sub annual or even smaller scales papalexiou et al 2011 but in that case the phenomenon gets complicated due to the combined effects of seasonal variation and intermittency on the other hand paleoclimatic reconstructions suggest strong lrd behaviour in multi decadal to centennial time scales pelletier and turcotte 1997 markonis and koutsoyiannis 2016 evidently there are still ample grounds for research on the existence of lrd in annual precipitation herein we have analyzed more than one thousand annual precipitation records of length of a hundred years or more from different areas of the world as well as approximately 70 paleoclimatic records spanning the time from 12 thousand years ago until the present day to quantify lrd we estimated the hurst coefficient by applying two algorithmic versions of the aggregated variance method and employed monte carlo method to identify a common hurst coefficient for all the records additionally we performed a simple test on the autocorrelation structure of the first few lags to examine whether the hypothesis of a markovian autocorrelation structure is justified or not finally we investigated the effect of time scale and record length on lrd estimation using the paleoclimatic series 2 dataset the instrumental data were obtained from the global historical climatology network ghcn daily http www ncdc noaa gov oa climate ghcn daily which contains daily data from more than 50 000 land surface stations around the globe a significant percentage of these records exhibit the typical issues of most datasets available i e missing values short record length and rainfall values of questionable quality such as unrealistic outliers in order to restrict data quality to a significantly high level we filtered the dataset using certain criteria initially we chose to study only the stations satisfying the following conditions a record length over 100years b missing daily values percentage less than 20 of the record length and c suspect values with quality flags less than 0 1 this first quality screening resulted in 3477 stations of daily data with lengths varying from 100years to 173years then in order to construct the annual series we first deleted all daily values assigned quality flags indicating unrealistically large values and then estimated the average daily value per year notably because of the existence of missing values within some records summing up daily values to obtain the annual total would result in smaller estimates than the real ones instead it is more robust to estimate the daily mean values per year as the mean value estimate can be accurate even in the presence of some missing values this is equivalent to estimating the annual total by first infilling the daily missing values of a year with the daily average of the year years having more than 20 missing daily values were considered missing and their annual estimate was not derived via the abovementioned method the 3477 daily stations are aggregated to the annual time step among these stations there are different combinations of record lengths and number of missing yearly values e g 558 stations having 100years in a sequence with no missing values 1474 stations with more than 100 annual values and only eight stations without any missing values we choose to analyze 1265 stations having more than 100 annual values and a missing yearly values percentage less than 15 obviously this choice ensures a higher quality dataset for our analysis on the annual time step paleoclimatic data can also be used to determine the existence of lrd in climatic variables since they cover quite larger scales compared to the instrumental data sets mandelbrot and wallis 1969 koutsoyiannis 2003 bunde et al 2013 markonis and koutsoyiannis 2013 herein we used 68 paleoclimatic records of rainfall reconstructions located mainly in the northern hemisphere to explore time scales reaching up to 12 000years three different data sets were used corresponding to the proxy variable used for the reconstruction process i e tree rings speleothems and other including lake sediments pollen corals and multi proxy reconstructions the 40 time series of tree ring reconstructions are the largest data set and have a mean sample size of 900 values at an annual time scale see table 1 the other two data sets with 16 and 12 time series correspondingly in most cases have fewer values and varying time resolutions ranging between 1 and 100years it must be noted that the speleothem records are proxy records of δ18o a variable which is linearly linked to rainfall 3 analysis and results 3 1 aggregated variance method the method employed herein is based on the study of the variability of the data averaged at different timescales the method is typically referred to as aggregated variance method but what it actually aggregates is the timescale and not the variance specifically let x j be a stationary process in discrete time j referring to years in our case with standard deviation σ and let 1 x j k 1 k l j 1 k 1 jk x l k 1 2 3 denote the averaged process at timescale k with standard deviation σ k in the case of an uncorrelated process the standard deviation of x j k is obtained by σ k σ k in other cases e g if the process is fractional gaussian noise or a hurst kolmogorov process the abovementioned law is invalid instead one obtains the elementary scaling property 2 σ k k h 1 σ where h is the hurst coefficient which for stationary and positively correlated processes varies in the range 0 5 1 beran 1994 the value of h 0 5 denotes time independence while smaller values are indicative of anti persistence the autocorrelation of the aggregated process is independent of the scale of aggregation k and is given as follows 3 ρ j k ρ j 1 2 j 1 2 h j 1 2 h j 2 h j 0 to apply the method to the data we used a graphic tool the climacogram koutsoyiannis 2011b which is the double logarithmic plot of the standard deviation σ k of the aggregated time series at scale k versus the time scale k the h value is estimated as the slope of the fitted line least squares regression in a variant of that method the estimation bias of the standard deviation which depends on the time scale of aggregation is also considered see section 3 2 below each averaged time series is constructed as follows for every scale k the data are divided into n groups the number of which is obtained as the fraction of the data length l versus the scale value k for example in time scale k 4 120years would be divided in 30 non overlapping groups of 4years subsequently the values within each group are averaged according to eq 1 however when missing values are encountered the process of averaging may become problematic depending on the number of missing values if more than a half of the values is missing then the estimate would be quite uncertain markonis 2015 to overcome the issue we use a simple criterion on the number of missing values before estimating the averaged series within each group a for scale k 2 the average value is estimated only when both values exist b for scales k 3 the average value is estimated only when there are at least three values within the group according to the latter rule we estimated the averaged series for all the scales between k min and k max where k min 1 and k max l 10 so that the variance in the maximum scale is estimated from at least 10 values koutsoyiannis 2003 for a 100 year record length this would be the variance of the decadal means the results of the algorithm implementation for the instrumental data are shown in table 2 and fig 1 suggesting some evidence of weak long range dependence more specifically it was found that 85 of the data exhibit h 0 5 yet with notable variation for example only half of the data show h 0 59 i e a more pronounced dependence structure a very strong dependence structure h 0 80 is reported for 2 5 of the records while for 15 of them we observe lack of dependence for the 95 confidence interval h values fluctuate between 0 4 and 0 8 in paleoclimatic data the hurst coefficient shows a tendency for higher estimates as well as an increased range of values fig 2 in order to test the effects of our parametric choices for the value of the minimum and maximum scale we examined how the median and the variance of h estimates vary for different k min and k max as can be seen in fig 3 the variance of the hurst parameter estimate becomes larger as the value of the minimum scale k min increases yet the value of the median in the estimate remains the same therefore our choice of k min 1 is well justified since greater values of k min only amplify the uncertainty in h estimation in addition the observation of the same median strengthens our hypothesis of the lrd structure because in the alternative hypothesis of short term dependence we would notice some change in the climacogram curvature and correspondingly to the logarithmic slope the results for the k max were similar it can be seen in fig 4 that the decrease in the number of values in the last scale increases the variance of the hurst parameter estimate in this case too therefore the choice of n 10 leads to more reliable results compared to using smaller values of n 3 2 least squares based on standard deviation method lssd koutsoyiannis 2003 demonstrated how the use of the classical estimator for the standard deviation can introduce significant negative bias in the estimation of the hurst parameter by the aggregated variance method this is because the hypothesis of independence which is a necessary condition for the use of the estimator is violated in the case of processes with strong lrd behaviour this shortcoming may be overcome by the use of the least squares based on standard deviation method lssd koutsoyiannis 2003 tyralis and koutsoyiannis 2011 which performs a simultaneous estimation of the hurst parameter h and the standard deviation σ using an approximately unbiased estimator for the latter here for simplicity reasons we applied the lssd method tyralis and koutsoyiannis 2011 only to the sample of the 558 44 of the total stations with no missing values and then compared our estimate with the one obtained by the aggregated variance method for the same sample as shown in table 3 and fig 5 the two methods show small deviations from each other overall the value of the bias fluctuates between 1 and 2 with the bias in the estimate of the average being approximately 1 the bias is negligible in this case because the estimated hurst parameter is not very high 3 3 monte carlo testing we also investigated the assumption that the observed distribution of the sample estimates of h results from a single model with a specific true value sometimes referred to as population value of the hurst coefficient in order to produce a theoretical sample of time series exhibiting hurst dynamics we used a simple algorithm that generates fractional gaussian noise based on a multiple timescale fluctuation approach koutsoyiannis 2002 we generated 1265 time series from a gaussian distribution that reproduce the record length the mean and the standard deviation of the empirical sample repeated the same procedure for several theoretical h values and then estimated the empirical ones we should note that there may be some cases where moderate departures from normality are observed for the annual rainfall distribution especially in arid semi arid regions of the world or regions severely affected by the el nino southern oscillation enso still the use of the assumption of normality for the synthetic records albeit simplifying is justifiable as for the majority of the stations the central limit theorem holds therefore normality is a good approximation while in general using monte carlo experiments we were able to find that the estimation of the hurst coefficient is practically insensitive to the underlying distribution subsequently the distribution of the empirical estimates for the synthetic time series was compared to the distribution of the empirical estimates for the historic time series used in the analysis it appears that the value of h 0 58 fig 6 yields the most satisfactory match however it is worth noticing that that 2 5 of the stations exhibiting h 0 8 are outside the range of the theoretical distribution 3 4 autocorrelation analysis the estimated hurst coefficient is not high enough to allow for any certain conclusion on the type of the dependence structure since relatively low hurst coefficients 0 5 0 6 can be estimated when there is short range dependence or no dependence at all due to algorithmic inadequacies sample bias and estimation uncertainty to this end we have employed the autocorrelation function to further examine the dependence properties of rainfall still one should keep in mind that the classical autocorrelation estimator as in the case of standard deviation is biased downwards koutsoyiannis 2003 dimitriadis and koutsoyiannis 2015 however since the estimator is biased downwards any result in favour of ltp would mean that in reality the ltp is even stronger the autocorrelation coefficients of the first three lags for the instrumental data are low table 4 on further investigation we tested whether independence is a plausible scenario for the dependence structure of our data we produced 1265 independent i e uncorrelated time series of the same sample size and estimated the sample autocorrelation coefficients fig 7 it can be seen that for all three lags the value of the median of the historic data is greater than the one estimated from uncorrelated synthetic data this is more obvious in the case of autocorrelation of lag 1 where for confidence interval 95 the values of the independent data fluctuate in the range 0 175 to 0 173 while the historic ones are in the range 0 09 to 0 37 in addition in all three cases the historic samples exhibit significantly fewer negative values than the uncorrelated ones the above results could be typical for a markov process too also known as ar 1 process to address this issue a simple ad hoc test which exploits the distinctive properties of markov processes was designed under the markov hypothesis the theoretical autocorrelation coefficient for lag 2 would be estimated as ρ 2 ρ 1 2 where ρ 1 is the known empirical autocorrelation likewise the markovian autocorrelation coefficient for lag 3 would be given as ρ 3 ρ 1 3 the resulting theoretical estimate is compared to the empirical one for the same lag if the empirical value is higher than the theoretical ar 1 one then the markov hypothesis weakens we applied this comparison to the 52 of the stations for which all the autocorrelation coefficients for lags 1 3 are positive fig 8 it is evident that the empirical estimates are considerably higher than the theoretical ones resulting from an ar 1 structure and therefore the markov assumption becomes less likely in addition the empirical estimates do not follow the exponential convergence to zero of the markovian ones but instead remain approximately stable for lags 2 and 3 this is in agreement with the theoretical behaviour of lrd whose distinctive feature is the existence of slowly decaying autocorrelation function beran 1994 having tested the cases of independence and short range dependence we finally examined whether the autocorrelation structure is consistent with that of a fgn model via a visual comparison of the two in fig 9 the empirical autocorrelation coefficient ρ 1 is plotted against the corresponding empirical hurst coefficient h as obtained from eq 2 the theoretical autocorrelation values of a fgn model as obtained from eq 3 are plotted as well the diagram shows that the autocorrelation structure is consistent with that of a fgn model the deviation between the theoretical and the empirical estimates becomes greater in the region of high values of h still this is justified due to the increased negative bias in the autocorrelation estimation in that case 3 5 paleoclimatic data paleoclimatic data reveal a stronger form of dependence at larger time scales which is in good agreement with other relevant studies bunde et al 2013 franke et al 2013 in general we can divide proxy data into three categories based on their temporal resolution high resolution data with annual time step mainly tree rings medium resolution data with decadal time step mainly speleothems and low resolution data with centennial time increments such as lake sediments or pollen however certain factors should be taken in consideration regarding some fundamental uncertainties about rainfall reconstruction from proxy variables often tree rings represent areal reconstructions of precipitation and in some cases the area covered is rather large e g ce11 as defined in table 1 since the individual time series are cross correlated their aggregation to a single time series might increase h granger 1980 to address this issue a random cross examination of individual tree rings records was performed and it was confirmed that the individual series exhibit the same behaviour as the aggregated records in addition until recently the most common approach to transform the proxy variable tree ring width to the reconstructed one rainfall was through a method that involved detrending and or pre whitening of the original time series this methodology has a severe impact on the low frequency variability briffa et al 1996 helama et al 2004 and thus on h estimation as presented in fig 2 red diagonal lines detrended pre whitened time series have a mean h equal to 0 5 while the rest of the records which are derived using the regional curve standardization briffa et al 1992 or the neural networks ni et al 2002 methods have a mean near 0 72 on the other hand in some cases such as the time series with centennial time resolution the uncertainties in the correct estimation of the precipitation amount are so high that the record is strongly smoothed in order to depict only major shifts of the mean these data sets e g as05 or cc09 as defined in table 1 are of small sample sizes and thus h estimates are unavoidably pushed towards values that reach close to 1 fig 2 green 1 for interpretation of color in fig 2 the reader is referred to the web version of this article 1 diagonal lines even in larger data sets i e speleothems if the smoothing happens to be combined with a strong monotonic trend of the original data then again h values would falsely tend towards 1 fig 2 orange diagonal lines such estimates cannot be included in the estimation of h which finally reaches 0 75 for the paleoclimatic data however they cannot be totally neglected as they provide some qualitative evidence for the long term change in rainfall which includes both long term trends and abrupt shifts in the mean the effect of record length on lrd is further explored by partitioning the reconstructed records to smaller segments which can be achieved by estimating h through a moving time window of variable length i e 50 100 250 and 500years since the tree rings have annual resolution they are suitable for such analysis as the results are directly comparable with the estimates of the instrumental time series to limit any methodological uncertainties the cl02 dataset was used which contains 15 records with average h h equal to 0 75 has record length close to 1000years and is not detrended or pre whitened the results show that if the sample size is equal to the instrumental records 100years then lrd structure fluctuates between white noise h 0 5 and strong hurst behaviour h 0 9 with h 0 71 fig 10 as the record length increases h values are constrained to 0 7 0 8 and h converges to 0 75 the results are reproduced for an equal number of synthetic time series with similar size and lrd properties however the even higher values of hurst coefficient h 0 8 found in other paleoclimatic reconstructions fig 2 cannot be simply attributed to sample size bias their behaviour cannot be reproduced by synthetic time series this suggests that either rainfall presents different dependence structures in sub decadal and above decadal scales markonis and koutsoyiannis 2016 or that the stronger lrd is artificial introduced to the precipitation reconstructions through some intrinsic properties of the proxy variables e g karst transit time in speleothems dee et al 2015 4 discussion and conclusions the analysis of the global instrumental data set shows that there are notable indications of weak lrd in the annual rainfall as the hurst parameter is not very high the aggregated variance method induces only 1 2 negative bias in the hurst coefficient estimation and therefore the best population value of h that has been shown through monte carlo estimation to account for the observed sampling variability h 0 58 may be considered accurately representative for instrumental data the study of the autocorrelation function shows that it is consistent with the autocorrelation of a fgn model even though for a certain percentage of the stations the markov hypothesis could not be falsified specifically the existence of negative correlations in all three lags examined did not permit the application of the abovementioned method in the case of the 48 of the stations some studies using smaller data sets potter 1979 fraedrich and blender 2003 kantelhardt et al 2006 supported the appropriateness of the markov structure but they did not investigate the differences between actual and theoretical auto correlation in larger lags fig 8 these differences might be quite small and thus allow the stochastic modelling of annual rainfall as a markovian process for record lengths below 100years it has been shown though that they might have serious implications when it comes to the estimation of trend significance and as a result the observed changes in rainfall might be considered much rarer than they actually are cohn and lins 2005 lastly it was shown as well that the autocorrelation function significantly departs from the case of independence although the above findings are in favour of the existence of a stronger dependence structure than the one typically assumed in literature potter 1979 fraedrich and blender 2003 kantelhardt et al 2006 it seems that there is a discrepancy between smaller and larger time scales fraedrich and larnder 1993 pelletier and turcotte 1997 poveda 2011 ault et al 2013 to this end the most important source of uncertainty in the determination of lrd which is the record length should not be overlooked koutsoyiannis 2002 koutsoyiannis and montanari 2007 although using stations with relatively high compared to the majority of the existing rainfall data records record length the accurate detection of long range dependence cannot be guaranteed because this behaviour may require even longer record length to be revealed subsequently the low estimates of hurst parameter in instrumental time series could be attributed to the limited record length available in some cases and therefore should be considered characteristic only for this time horizon of approximately 100years this behaviour of lrd is illustrated in the case of paleoclimatic data with annual time step when the sample size of paleoclimatic data is restricted to match the one of instrumental data approximately 100 years the distribution of the estimated hurst coefficient exhibits a lower mean value together with an increased variance compared to the one arising from larger sample sizes however these results could not be reproduced for paleoclimatic series of longer time scales i e above decadal which suggests that the discrepancy in lrd structure i e the difference in the mean value of h between sub decadal annual as in the instrumental series and above decadal as in the paleoclimatic series scales may be nonetheless inherent in precipitation behaviour rather than being solely attributed to the sample size effect this finding is in good agreement with a recently published work by markonis and koutsoyiannis 2016 which emphasizes the influence of time scale when it comes to the analysis of the dependence of a time series in addition due to historical and socio economic reasons the data set does not include enough or any stations at all adequate for our analysis from certain regions of the world such as central africa and south america however the representation of climates according to the köppen classification system remains fairly good since a wide variety of climates is still represented in north america australia and central europe i e the areas which contribute the most to our dataset even so the possibility of a misrepresentation of climates cannot be excluded and this constitutes a source of uncertainty in our results and an area open for research in the future when more stations of larger record lengths will be made publicly available it is also important to consider the uncertainty induced due to measurement errors or false homogenization techniques which may introduce bias to the estimation of lrd steirou 2011 ghcn daily highlights the potential bias provoked by changes in instrumentation over the years and it is possible that this kind of bias could also affect the estimation of h ultimately the high variability of the results is in accordance with the inherent uncertainty of the phenomenon apart from algorithmic or data choices an important conclusion drawn from the analysis is that simplifying assumptions commonly used in practice such as inter annual independence may in cases significantly depart from reality and hence a thorough and careful study of the dependence properties of the dataset as performed here is recommended especially when longer time horizons are of interest 
