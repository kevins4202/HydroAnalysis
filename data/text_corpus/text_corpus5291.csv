index,text
26455,environmental managers often do not have sufficient empirical data to inform decisions and instead must rely on expert predictions however the informal methods often used to gather expert opinions are prone to cognitive and motivational biases we developed a structured elicitation protocol where opinions are directly incorporated into bayesian network bbn models the 4 stage protocol includes approaches to minimise biases during pre elicitation workshop facilitation and output analysis and results in a fully functional bbn model we illustrate our protocol using examples from environmental flow management in australia presenting models of vegetation responses to changes in riverine flow regimes the reliance on expert opinion and the contested nature of many environmental management decisions mean that our structured elicitation protocol is potentially of great value for developing robust environmental recommendations this method also lends itself to effective adaptive management because the expert populated ecological response models can be readily updated with field data keywords expert elicitation decision making bayesian networks riparian vegetation environmental flow management bias 1 introduction decision making informed by expert opinion is common in environmental management and can form a basis for urgent management and policy decisions when stakes are too high to postpone such choices krueger et al 2012 martin et al 2012 in complex systems when formal theories and or measured data may be scarce expert opinion can help to assess whether information is consistent and where evidence may be lacking martin et al 2012 informal techniques that are implicit unstructured and undocumented such as roundtable discussions are commonly used for extracting expert knowledge fidler et al 2012 mcbride and burgman 2012 whilst discussion itself is not necessarily problematic the knowledge provided by such unstructured group expert opinion is usually based on subjective judgements prone to cognitive and motivational biases fidler et al 2012 garthwaite et al 2005 o hagan et al 2006 o hagan and oakley 2004 speirs bridge et al 2010 tversky and kahneman 1975 cognitive biases occur as a result of a failure to adequately process aggregate or integrate relevant information due to limitations on human processing ability mcbride and burgman 2012 wilson 1994 overconfidence biases for example undermine expert judgments by underestimating uncertainty soll and klayman 2004 other examples of cognitive biases include the availability bias where familiarity with one particular driving factor may lead an expert to believe it to be more important than it actually is kynn 2008 and anchoring biases where an initial value is used to calculate another value by adjusting it up or down jacowitz and kahneman 1995 speirs bridge et al 2010 motivational biases are conscious or subconscious adjustments in an expert s responses that depend on their particular context personal beliefs and experiences and from what the expert stands to gain or lose personally form a decision garthwaite et al 2005 biases are inherent in heuristic processing the utility of expert knowledge is therefore dependent on the rigour with which it is elicited martin et al 2012 the choice of an expert elicitation method needs to account for and minimise the risk of bias affecting expert judgements and resulting elicited data speirs bridge et al 2010 formal procedures to elicit expert knowledge have been designed to account for such biases in order to increase credibility repeatability and transparency hanea et al 2017 mcbride and burgman 2012 structured elicitation often employs a framework combining foundational elements of decision theory and mathematics with procedures for minimising biases keeney and von winterfeldt 1991 o hagan et al 2006 recently the idea investigate discuss estimate aggregate structured protocol by hanea et al 2017 has combined different elements from established approaches to optimise expert knowledge elicitation the success of expert elicitation depends on facilitation and the way expert judgements are collected and compiled therefore only well managed systematic elicitation protocols can return high quality transparent and repeatable predictions from experts cook et al 2012 hanea et al 2017 knol et al 2010 martin et al 2012 runge et al 2011 one area of environmental management where expert opinion is widely used is the provision of environmental flows water released from storage solely to benefit the environment horne et al 2017b to restore regulated river systems stewardson and webb 2010 restoring more natural flow regimes is a key issue around the world poff et al 1997 with substantial investments from governments in an attempt to manage degraded river systems acreman and ferguson 2010 skinner and langford 2013 effective restoration and management of these systems rely on an understanding of the relationships between the stressors changes in flow regimes and ecological responses poff and zimmerman 2010 webb et al 2013 however while general principles of the ecological effects of changes in flow regime are well accepted poff and zimmerman 2010 there are few quantitative predictions about how different components of degraded ecosystems will respond to flow restoration arthington and pusey 2003 souchon et al 2008 despite this paucity of quantified relationships flow management decisions must be made therefore of the many environmental flow assessment techniques that have been developed tharme 2003 those that make direct predictions of ecological effects have relied to a great degree on expert opinion stewardson and webb 2010 in australia expert opinion is a major determinant of how the approximately 15 billion investment in environmental water under the basin plan and other initiatives will be used commonwealth of australia 2014 it is therefore surprising that systematic approaches to elicit expert knowledge are less common than informal methods mcbride and burgman 2012 the reliance on expert opinion and the contested nature of environmental flows means that structured elicitation should be of great potential value for developing environmental flow recommendations webb et al 2015 bayesian belief networks bbns pearl 2000 is a modelling technique that is commonly used in natural resource management applications mccann et al 2006 and has also been used in environmental flows applications chan et al 2012 horne et al 2017c shenton et al 2011 shenton et al 2014 one of the often cited advantages of bbns is that they can incorporate multiple data types including expert knowledge horne et al 2017a however this advantage also leaves them open to being populated by poorly elicited expert opinion affected by bias and overconfidence the quantitative outputs from these models may give a false sense of security in the results if they are based on poor quality expert derived data this paper presents a 4 stage formal expert elicitation protocol where opinions are directly incorporated into bbn models providing improved rigour in the relationships and subsequent predictions the 4 stages include pre elicitation workshop facilitation output analysis and bbn model building the first 3 stages are based on the idea framework by hanea et al 2017 combining different established approaches to minimise biases of elicited opinions the final stage consists of discretising the elicited probability distributions in order to populate the conditional probability tables in a bbn model this model becomes an ideal vehicle for later updating with empirical data we first present the general principles underpinning the protocol providing a justification of the approaches used at each stage section 2 we then illustrate application of the protocol using two case studies that elicited quantitative predictions of ecological responses to changes in riverine flow regimes and other environmental factors under environmental flow recommendations section 3 the two case studies elicited expert predictions of expected changes in i terrestrial vegetation encroachment into river channels or undesirable vegetation cover hereafter encroachment and ii abundance of native riparian species on river banks or desirable vegetation cover hereafter native banks the paper closes by assessing strengths and weaknesses of the protocol especially as experienced through the case studies 2 expert elicitation protocol general principles our approach to minimising cognitive and motivational biases include i using a question protocol to reduce overconfidence and availability biases during pre elicitation ii requiring experts to answer questions independently and in isolation to minimise motivational biases but also allowing multiple rounds of judgement to ensure transparency and expert comprehension during workshop facilitation iii using mathematical accumulation of opinion and interpolation as analysis tools to avoid expert fatigue in earlier stages and iv incorporation of opinion into bayesian belief networks models to allow direct predictions under different scenarios with current and updated data further details on all the approaches used are provided below for each of the 4 stages and illustrated in fig 1 2 1 stage 1 pre elicitation 2 1 1 conceptual model building expert elicitation should be based on conceptual models for which both the extent of key scientific knowledge and gaps are identified via literature reviews mcbride et al 2012 in our protocol a conceptual model consists of a set of state variables with arrows linking those variables illustrating the hypothesized causal relationships among them e g fig 2 a these relationships form the basis of the elicitation questions such models can be complex and may need to be simplified to intuitively structure a problem into a set of variables for which knowledge can be elicited keeney and von winterfeldt 1991 mcbride and burgman 2012 a simplified conceptual model provides the structure of a bayesian belief network bbn model developed at stage 4 in this protocol 2 1 2 variables their states and scenarios definition to quantify the linkages among variables with a relatively small number of questions our protocol employs discretization of continuous variables for example a discharge volume in a river might be discretised to high 10 000 ml d medium 2000 10 000 or low 2000 any number of states can be chosen to characterize a variable but using fewer states is recommended to prevent complexities in judging the results mittal and kassim 2007 each combination of discretised states for a causal relationship defines a scenario for the elicitation discretization simplifies the types of questions that need to be asked and also translates well into conditional probability tables cpts cpts provide a useful structure to define the effects of different variables on a particular response variable under different scenarios they also reflect the quantitative expression of expert opinion cain 2001 underpinning the bbn model at stage 4 2 1 3 question protocol in order to combat expert bias the 4 step procedure developed by speirs bridge et al 2010 is used to formulate elicitation questions for each scenario the 4 step question procedure consists of asking experts to estimate for each scenario i the lowest expected value ii the highest expected value iii the most likely value and iv the probability that this interval captures the true value this procedure based on the idea protocol is designed to minimise overconfidence soll and klayman 2004 and has proved useful for eliciting expert knowledge in a range of settings james et al 2010 knol et al 2010 martin et al 2012 2 2 stage 2 workshop facilitation 2 2 1 expert selection subject to availability and willingness of the experts to participate pre selection of experts based on their relevant knowledge and experience is key to identifying contributions from individuals within existing time constraints burgman et al 2011 mcbride and burgman 2012 the number of experts that is sufficient in an elicitation process should be appropriate to the scope of the elicitation available time and resources martin et al 2012 mcbride et al 2012 our experience suggests that 6 experts is often an appropriate number to balance workload for the workshop facilitators and optimize dialogue among experts importantly it also allows for a diverse group of experts which is important for minimizing bias burgman 2015 2 2 2 scenario presentation during workshop facilitation environmental scenarios should be presented in a way that can facilitate expert comprehension the facilitator should also encourage experts to ask for clarification to ensure that the scenarios are understood and to resolve any ambiguities mcbride et al 2012 to maximize comprehension we present scenarios pictorially in the form of printed questions and also as questions read aloud by the facilitator 2 2 3 indirect and independent question answering indirect or predictive elicitation approaches can be used to obtain a specific response from the experts without overtly asking for that response this ensures that experts are less aware of the link between their answers and the final destination of the elicited data for model building thereby reducing motivational biases low choy et al 2012 in our protocol although the final goal is to identify the individual environmental variables and combinations that are most important for the ecological response being considered the experts are not asked this directly instead they are asked to predict environmental outcomes under a wide range of scenarios we do not claim that an expert will answer questions with no preconception of what the most important variable s might be merely that by eliciting the effect of the variable s through multiple questions and multiple experts any bias will be reduced in order to minimise representativeness and availability biases kynn 2008 each participant answers the questions independently and in isolation to reduce anchoring bias during elicitation no initial amounts that could potentially be used as starting values are included in the questions and experts are asked not to refer to their own previous answers or look at previous further scenarios while answering a question mcbride and burgman 2012 2 2 4 within workshop feedback and re estimation the four lowest highest most likely confidence individual responses for a given scenario can be translated into probability distributions this is advantageous over a direct point estimate because it provides a range of values with known probability of capturing the true value hand 2008 to achieve this we use minimum cross entropy calculations kraan and bedford 2005 myung et al 1996 as implemented by salomon 2013 the theory behind this method is beyond the scope of this paper and interested readers are directed to salomon 2013 for full details outputs from the method are further illustrated through the example in section 3 minimum cross entropy allows the determination of the summary statistics characterizing an expert s estimates garthwaite et al 2005 individually elicited expert probability distributions can then be mapped and displayed mccarthy 2007 for each scenario providing experts with visual representations of their responses this highlights differences among individual experts answers as well as making them aware of the statistical interpretation of their estimates garthwaite et al 2005 hogarth 1975 kynn 2008 salomon 2013 in our protocol probability distributions derived from the first set of independent estimates are used in facilitated group discussions aimed at verifying and or modifying expert judgments as another way to manage and alleviate individual expert bias and to improve accuracy of estimates kuhnert et al 2010 mcbride et al 2012 wintle et al 2013 following discussions experts are allowed to submit a revised estimate for any scenario but the original estimate is not discarded see below this idea based protocol can be thought of as a modified version of the delphi process burgman 2005 dalkey and helmer 1963 but for which reaching consensus on elicited scores is not the goal it guards against group think but retains the advantages of discussion burgman 2015 2 3 stage 3 analysis of outputs 2 3 1 aggregation mathematical accumulation is a useful approach for combining expert opinion that may otherwise be prone to biases burgman 2005 2015 group aggregated estimates can be calculated as an average of individual expert distributions in our protocol each expert s results independent and revised if applicable are first averaged and then those results are averaged across the group to provide the ensemble distribution of expert opinion including both the initial and revised estimates from each expert means that we guard against group think by retaining the initial estimate but also offer the opportunity to reduce the influence of initial outlying values by including the final should the expert s thinking change during the group discussion in this attempt to encourage participants to resample their own information arbitrary language based disagreements meanings of words and context can be reconciled burgman 2015 schultze et al 2012 2 3 2 interpolation interpolation of elicited outputs allows reduction in the total number of scenarios and questions put to the experts during a workshop hence reducing the possibility of expert fatigue affecting elicitation outputs burgman et al 2011 if one can assume that causal relationships in the conceptual model are independent outputs for some of the scenarios can be estimated using interpolation rather than having to elicit expert opinions for each of the scenarios individually cain 2001 we use the interpolation methods of cain 2001 in our protocol to complete the results for the scenarios not covered during elicitation the interpolation approach is illustrated through the example in section 3 2 4 stage 4 model building the final stage in the protocol is the incorporation of the expert opinions into a bayesian belief network bbn model bayesian networks are graphical models that can be used to predict the state of one or more response variables as determined by the states of the linked driving variables through conditional probability relationships korb and nicholson 2010 they have been widely used in natural resource management applications mccann et al 2006 the structure of the bbn is derived directly from the conceptual model created at stage 1 fig 2 it is an influence diagram outlining the most relevant relationships between dependent and independent variables korb and nicholson 2010 the relationships among state variables in the bbn are determined by the set of cpts in our protocol the aggregated and interpolated outputs are used to populate the cpts to produce what we refer to as elicited probability tables epts this results in a fully operational model that allows rapid assessment of how different scenarios affect outcomes the models can also be readily updated with empirical data when those data become available finally bbns provide an indirect measure to rank variables for model validation and such values can be compared to direct ranking of experts 3 case studies on vegetation responses to environmental flows we illustrate the above protocol using the example of two elicitation projects on encroachment and native banks to quantitatively predict the response of in channel and riparian vegetation to inundation by environmental flows under a number of different scenarios 3 1 application of the protocol 3 1 1 pre elicitation we built two evidence based conceptual models to predict terrestrial vegetation encroachment into river channels miller et al 2013 and abundance of native riparian species on river banks miller et al 2015 both models of ecological response were based on rigorous literature analysis norris et al 2012 recognising that simpler models can be a more useful starting point for expert elicitation mcbride and burgman 2012 we used simplified versions of the models highlighting only the most important causal links fig 2 for the encroachment example these causal links included i increasing the duration and frequency of inundation will reduce encroachment of undesirable terrestrial vegetation into regulated river channels and ii increasing the frequency and duration of inundation will increase abundance of native vegetation on the river banks thresholds among states were required for some variables e g definitions of long medium and short for inundation by base flows table 1 these were based on published environmental flows recommendations where possible e g skm 2002 the causal relationships formed the basis of the elicitation questions defining scenarios about the relationship between riparian vegetation and flow components table 1 the final cpts provided a summary of expected outcomes for each ecological response given 18 different environmental scenarios for encroachment table 2 and 24 scenarios for native banks tables 3 and 4 for each case study the scenarios were organised from worse to best situations to be presented during expert elicitation and to be used as the basis for later interpolation a total of 11 scenarios for encroachment and 16 scenarios for native banks 11 for herbaceous and 5 for woody species were selected for elicitation with the remaining scenarios later filled using the interpolation approaches of cain 2001 the 4 step elicitation questionnaire from speirs bridge et al 2010 was then integrated into a scenario guide that contained the questions see example in fig 3 in all cases the questions referred to the expected vegetation cover expressed as percentage cover of a hypothetical sampling quadrat on an average river bank in victoria australia the measure of confidence that the elicited range between lowest and highest estimates captured the true of vegetation cover was restricted to 50 as a confidence below 50 implies that the expert believes the actual value is more likely to be found outside of their given interval than within it speirs bridge et al 2010 3 1 2 workshops facilitation a diverse group of six experts including fluvial geomorphologists botanists ecologists and river management consultants participated in each of the elicitation workshops on encroachment and native banks each expert received a scenario guide containing background information on the study and a questionnaire see example in fig 3 to ensure that the elicited information was representative of the correct environmental setting the scenario guide detailed the parameters for each cpt scenario and how they should be interpreted across different victorian rivers we also provided time for questions and discussion at this point to ensure that experts had a shared understanding of the factors to be included in the scenarios and what their answers would represent as a way to familiarise the experts with the elicitation process an unrelated practice question was posed and the responses discussed prior to commencing the actual elicitation the scenarios were then presented both in pictorial and written form fig 3 and were also read aloud between scenarios we used an unrelated numerical or maths trivia question to further ensure the elicited answers to different scenarios were cognitively separated after completing all of the elicited questions for all cpt scenarios we translated the expert responses into individual probability distributions which in both case studies described the probability of different levels of vegetation cover under different environmental flow scenarios because we were dealing with responses that are bounded at 0 and 100 cover we used the beta distribution for these examples the individual elicited distributions were displayed for each scenario see example in fig 4 only then were experts allowed to discuss their opinions in light of the discussion the participants were invited but not required to make a second group informed answer for each of the questions in addition at the end of the workshop experts were also asked to provide feedback on the workshop and to directly rank each predictor variable flow duration flow frequency flow season and stock access from most score 4 to least score 1 influential on the response variables direct rankings from experts were used for comparison to indirect rankings calculated from the bbns as a means of evaluating expert coherence 3 1 3 analysis of outputs the initial and revised if provided estimates of vegetation cover under each scenario were mostly right skewed and we used log transformation to reduce this asymmetry this allowed each estimate to be easily transformed from its expert nominated confidence interval to a standard confidence interval 80 transformation would not be necessary for more symmetrically distributed expert data the standardized confidence intervals were then averaged by taking the mean of the two estimates of the upper bound and the mean of the two estimates of the lower bound respectively within experts for each question fig 5 these individual expert distributions were aggregated using the average of all estimates again by averaging the bounds this process is equivalent to simple averaging of probability distributions as described in clemen and winkler 2007 an approach that those authors show to perform approximately as well as other more complex methods for combining probability distributions the averages were then used to populate each of the cpt elicited scenarios by computing the proportion of the probability distribution mass that lay within each state fig 5 the probabilities of vegetation cover for the epts were linearly interpolated to the remaining scenarios from the elicited information in the cpts tables 2 4 linear relationships are the simplest functional form and in the absence of theoretical or empirical evidence regarding likely functional forms among our state variables we chose to use this interpolation factors were calculated from a switch in the state of a predictor variable from having a positive effect to having a negative effect e g presence of stock will negatively affect native riparian species cover due to grazing interested readers are directed to cain 2001 where this approach is detailed fig 5 illustrates how the interpolation factors and interpolated values were calculated for the encroachment case the final epts contained both the elicited and interpolated probabilities of high medium and low vegetation cover under each analysed scenario fig 5 3 1 4 model building and testing based on the final epts we built bbns using the netica program norsys software corp 2010 and we used them to obtain indirect rankings of the predictor variables these were then compared to the direct expert ranking on how predictor variables influenced response variables in netica a variable s sensitivity is quantified as entropy reduction if a predictor variable has a larger value of entropy reduction this indicates that during the elicitation workshop the experts subconsciously attributed importance to this variable when considering the effect of different environmental scenarios 3 2 modelling results expert opinion indicated that long durations of inundation give the lowest probability of both high undesirable vegetation cover encroachment and low desirable vegetation cover native banks conversely stock presence was expected to result in low probabilities of both high levels of encroached vegetation in channels and native vegetation on river banks tables 2 4 for encroachment probability of high vegetation cover 25 100 coverage ranged from 25 to 91 table 2 for native banks probabilities of high cover of herbaceous native species were lower with scenarios having between 8 and 52 probability of high cover table 3 woody native species fared worse again with probabilities of high cover only ranging from 9 to 15 and low covers 0 5 being the most likely outcome for all scenarios table 4 the resulting cpts were sensitive to the combinations of environmental drivers despite differing opinions among the experts appendix a the elicited probabilities of encroached terrestrial vegetation cover differed widely across the different environmental scenarios in the encroachment cpt table 2 however there was less divergence in the elicited probabilities of native riparian vegetation cover for the scenarios in the native banks cpt tables 3 and 4 flow duration was identified by experts as the most important variable for discouraging growth of terrestrial vegetation in the river channel whilst stock access was considered most responsible for reduced abundance of native riparian species on river banks table 5 comparing direct vs indirect ranking of predictor variables it is apparent that the experts are clear both consciously and subconsciously about these two most important environmental variables they both show the highest entropy reduction indirect ranking and are given the highest scores by experts table 5 there is only some disparity between indirect and direct rankings on the importance of season and delivery frequency of base flows for encroachment and on the ranking of flows duration and season for native banks table 5 through the feedback at the end of the workshop the experts indicated a variety of confounding factors that they considered when estimating the highest vegetation expected and lowest vegetation expected bounds for each scenario these included shading effects of canopy bank slope and aspect substrate and texture land use and exotic vegetation competition for encroachment the experts from the native banks workshop also identified that they expected native vegetation especially woody species to respond to longer inundation durations than those included in our scenario guide 4 discussion environmental management requires quantitative predictions of ecological responses to management actions in the absence of field data the use of a formalised expert elicitation protocol can produce transparent methodologically rigorous and defensible estimates of such relationships cook et al 2012 knol et al 2010 martin et al 2012 runge et al 2011 our protocol brings together for the first time three previously published methods for eliciting and synthesizing expert knowledge the four point elicitation method speirs bridge et al 2010 minimum cross entropy for visualizing expert based probability distributions salomon 2013 and bayesian belief networks pearl 2000 this provides standardised elicitation of high quality transparent predictions of environmental responses knol et al 2010 that can be used for environmental management following the idea framework approach hanea et al 2017 the combination of several hitherto separate approaches to minimise expected cognitive biases is an improvement over informal methods for expert elicitation moreover the incorporation of accumulated opinions into bayesian belief networks provides a means for this knowledge to be used in the rapid and direct prediction of ecological responses under different management scenarios resulting in a fully operational model that can be readily updated with field data our findings have a direct application for environmental flow management and other natural resource management applications since the case studies described in this paper were done the protocol has been employed to investigate the likely effectiveness of complementary environmental restoration projects to enhance outcomes from environmental flows cresswell et al 2017 to inform the development of fish passage structure design criteria for non sport fish in the southern hemisphere wilkes et al 2017 and to quantify simple models of ecological response of the type reported in horne et al 2017c for incorporation into optimisation based decision support tools for environmental flows planning expert based predictions elicited here indicate that when delivering flows to achieve ecological outcomes for riparian and channel vegetation delivery of specified durations of base flows high flows and over bank flows are significantly more important than whether these flows are delivered continuously or episodically or the season of the flow is delivered in these are intuitively sensible predictions that can be tested through monitoring within an adaptive management framework the formal protocol provides several advantages over more commonly used informal approaches firstly the 4 step question procedure developed by speirs bridge et al 2010 reduces overconfidence bias second indirect elicitation through the presented scenarios reduces motivational biases as experts are not necessarily aware of the link between their answers and final quantification of models using the elicited data low choy et al 2012 in addition our method shows more transparency than informal round table discussion elicitation methods as experts are required to first answer questions independently of previous questions and without discussion this allows every expert s opinion to hold equal weight kynn 2008 and also provides a full audit trail of results should they be challenged 4 1 practical implementation of the protocol for the workshop to yield useful results from experts it was important to run an informal but well facilitated discussion of the environmental variables during the introduction of the scenarios guide this included a detailed description of the different states of the variables of interest e g we defined long base flow inundation duration as 50 days and any references for such definitions while group discussion was paramount to making sure that the experts understood and agreed about what specifically they were predicting the conversation had to remain solely about the definition and not the influence of the variables as discussion of influence could introduce bias into the subsequent elicitation one example of such clarification was whether we were seeking opinions on either basal or canopy cover for a given scenario our group size was a trade off among i maximizing diversity within the expert group ii finding enough experts available simultaneously for a one day workshop iii optimizing dialogue among experts and iv maintaining an achievable workload for the people running the elicitation however we tried to make the expert pool as diverse as possible in terms of expertise and background to minimise bias hanea et al 2017 mcbride and burgman 2012 we found that experts were often initially uncomfortable with the generality of scenarios during the facilitator introduction and scenario guide phase however once they completed the practice question and began to work through the formal environmental scenario questions they reported feeling more comfortable and even enjoying the elicitation process our use of the minimum cross entropy calculator salomon 2013 in situ allowed us to show the distributions of the elicited intervals in the second half of the workshop this was important for informing the subsequent discussion and provision of revised estimates and it was a major step in helping experts to understand how their answers would contribute to the final conditional probability table the use of interpolation to extrapolate predictions for some scenarios cain 2001 allowed us to maximise the number of variables we could include making our results more ecologically realistic it also reduced the number of questions asked of the experts easing fatigue reducing the number of answers required from experts allowed us to restrict the workshops to one day in length alleviating the logistical challenges of organising longer or additional workshops involving different experts the only negative impact interpolation was the reduced consideration of specific combinations of drivers we had to assume independence of the effects of some drivers and it is possible that expert opinion might not have supported this had these questions been asked in the workshops the direct ranking of the effect of different predictor variables from most to least influential was useful to contrast with indirectly derived results and to assess our protocol it also provided a way to detect the degree of agreement between conscious direct ranking and subconscious indirect elicitation opinions from the experts the results support both the validity of the elicitation protocol employed here and the objectivity of the expert knowledge provided 4 2 lessons learned and potential improvements the two case studies reported here are presented as if exactly the same method were followed each time this is an oversimplification we learned a lot during the encroachment workshop which meant that native banks ran more smoothly when it was undertaken several months later by the same staff but with different experts for example we extended the time allocated for discussion of the scenario guide to ensure that this step was not rushed and that all participants entered the elicitation phase of the workshop with a clear understanding we also provided more specific guidance about the interpretation of the self nominated confidence intervals in the expert estimates while the experts attending the encroachment workshop were content that we had identified important drivers of terrestrial vegetation encroachment there were several instances during the native banks workshop where the experts conceptual understanding of the relationships between native riparian vegetation and our environmental predictors were not aligned experts from the native banks workshop identified that they expected native vegetation to respond to longer inundation durations than those included in our scenario guide this illustrates a need to assess the adequacy of proposed scenarios and model structures prior to the workshop however this can be difficult as experts attending the workshop were deliberately not consulted about questions beforehand to reduce the effects of anchoring on their elicited opinions mcbride and burgman 2012 we did ground truth conceptual model structures with other experts prior to the workshop but this did not always match the opinions of the workshop experts when asked to quantify the relationships currently we are experimenting with directly involving the experts from the workshop in the design of conceptual models and the designation of driver states and thresholds cresswell et al 2017 there is a number of ways in which the sensitivity of the outputs could be tested first there is no specific reason for equally weighting the initial and final estimates of each expert prior to aggregation across the group we did this because we wanted both estimates to contribute to the final distribution reducing the effect of group think but also taking advantage of the collective expertise of the group to allow experts to resample their own information we could assess the sensitivity of estimates to this step by including only initial or final estimates or by altering the weightings similarly the sensitivity of the final distribution to group makeup can be explored by jack knife style iterations of estimates each excluding individual experts from the combined distribution nicol et al 2017 second the sensitivity of estimates to the discretizations chosen could also be tested here we based state thresholds on published environmental flows recommendations as these are relevant to management decisions but these values are mostly based on expert opinion and negotiated settlements and it would be useful to test the extent to which they affect model outputs when we interpolated among elicited scenarios we assumed linear relationships outputs would have been slightly different had we assumed another functional form and this could be investigated however linear relationships are the simplest functional form and we suspect that any study that needs to employ expert elicitation to quantify relationship will not also have sufficient evidence to assume a particular functional form beyond linear 4 3 interpretation of our results our elicitation protocol produced results that were consistent with but extended the literature regarding the importance of inundation for reducing undesirable vegetation cover encroachment and increasing desirable vegetation cover native banks miller et al 2013 found strong evidence in favour of the importance of inundation duration frequency and seasonality for reducing terrestrial vegetation abundance conversely miller et al 2015 found that an increase in the duration of high flows will simultaneously stimulate germination and reduce survival of seedlings and herbaceous vegetation but overall found strong evidence that more frequent overbank and high flows will increase survival and condition of native woody vegetation overall these findings are aligned with the elicitation findings that inundation is an important variable for discouraging growth of terrestrial vegetation in the river channel and encouraging desirable vegetation cover however while these earlier literature analyses had provided qualitative evidence for specific cause effect relationships the expert elicitation allowed us to quantify those linkages 4 4 implications for environmental management our protocol of standardised expert elicitation aims to improve the transparency and repeatability of management decisions when field data are challenging or costly to collect for the case studies presented here the conceptual models and bbns quantified with expert opinion were simplified as they were intended to create expert elicited priors for bayesian hierarchical models based on field data webb et al 2015 however our expert elicitation process is equally useful for any situation requiring more complex conceptual models and bbns that can be used for direct estimation of environmental outcomes to inform management decision making ideally elicited opinions would be used within an adaptive management loop following the implementation of decisions monitoring and evaluation would provide data to improve understanding it is a specific advantage of the bayesian networks employed here that such data can be used to update cpts through the application of bayes rule bayesian methods combine the expert opinion with data rather than replacing one with the other in this way predictions of ecological responses would progressively rely less and less upon expert opinion as more data were collected nevertheless at the start of this process managers may be making decisions in a virtual data vacuum expert opinion may be the only data available and the protocol outlined here maximizes the robustness and transparency of these data acknowledgements funding was provided by the australian research council lp100200170 we thank yacov salomon for his help with the minimum cross entropy calculator and all the experts for their participation in the workshops appendix a supplementary data the following is the supplementary data related to this article supplementary data 1 supplementary data 1 supplementary data 2 supplementary data 2 appendix a supplementary data supplementary data related to this article can be found at https doi org 10 1016 j envsoft 2017 11 020 
26455,environmental managers often do not have sufficient empirical data to inform decisions and instead must rely on expert predictions however the informal methods often used to gather expert opinions are prone to cognitive and motivational biases we developed a structured elicitation protocol where opinions are directly incorporated into bayesian network bbn models the 4 stage protocol includes approaches to minimise biases during pre elicitation workshop facilitation and output analysis and results in a fully functional bbn model we illustrate our protocol using examples from environmental flow management in australia presenting models of vegetation responses to changes in riverine flow regimes the reliance on expert opinion and the contested nature of many environmental management decisions mean that our structured elicitation protocol is potentially of great value for developing robust environmental recommendations this method also lends itself to effective adaptive management because the expert populated ecological response models can be readily updated with field data keywords expert elicitation decision making bayesian networks riparian vegetation environmental flow management bias 1 introduction decision making informed by expert opinion is common in environmental management and can form a basis for urgent management and policy decisions when stakes are too high to postpone such choices krueger et al 2012 martin et al 2012 in complex systems when formal theories and or measured data may be scarce expert opinion can help to assess whether information is consistent and where evidence may be lacking martin et al 2012 informal techniques that are implicit unstructured and undocumented such as roundtable discussions are commonly used for extracting expert knowledge fidler et al 2012 mcbride and burgman 2012 whilst discussion itself is not necessarily problematic the knowledge provided by such unstructured group expert opinion is usually based on subjective judgements prone to cognitive and motivational biases fidler et al 2012 garthwaite et al 2005 o hagan et al 2006 o hagan and oakley 2004 speirs bridge et al 2010 tversky and kahneman 1975 cognitive biases occur as a result of a failure to adequately process aggregate or integrate relevant information due to limitations on human processing ability mcbride and burgman 2012 wilson 1994 overconfidence biases for example undermine expert judgments by underestimating uncertainty soll and klayman 2004 other examples of cognitive biases include the availability bias where familiarity with one particular driving factor may lead an expert to believe it to be more important than it actually is kynn 2008 and anchoring biases where an initial value is used to calculate another value by adjusting it up or down jacowitz and kahneman 1995 speirs bridge et al 2010 motivational biases are conscious or subconscious adjustments in an expert s responses that depend on their particular context personal beliefs and experiences and from what the expert stands to gain or lose personally form a decision garthwaite et al 2005 biases are inherent in heuristic processing the utility of expert knowledge is therefore dependent on the rigour with which it is elicited martin et al 2012 the choice of an expert elicitation method needs to account for and minimise the risk of bias affecting expert judgements and resulting elicited data speirs bridge et al 2010 formal procedures to elicit expert knowledge have been designed to account for such biases in order to increase credibility repeatability and transparency hanea et al 2017 mcbride and burgman 2012 structured elicitation often employs a framework combining foundational elements of decision theory and mathematics with procedures for minimising biases keeney and von winterfeldt 1991 o hagan et al 2006 recently the idea investigate discuss estimate aggregate structured protocol by hanea et al 2017 has combined different elements from established approaches to optimise expert knowledge elicitation the success of expert elicitation depends on facilitation and the way expert judgements are collected and compiled therefore only well managed systematic elicitation protocols can return high quality transparent and repeatable predictions from experts cook et al 2012 hanea et al 2017 knol et al 2010 martin et al 2012 runge et al 2011 one area of environmental management where expert opinion is widely used is the provision of environmental flows water released from storage solely to benefit the environment horne et al 2017b to restore regulated river systems stewardson and webb 2010 restoring more natural flow regimes is a key issue around the world poff et al 1997 with substantial investments from governments in an attempt to manage degraded river systems acreman and ferguson 2010 skinner and langford 2013 effective restoration and management of these systems rely on an understanding of the relationships between the stressors changes in flow regimes and ecological responses poff and zimmerman 2010 webb et al 2013 however while general principles of the ecological effects of changes in flow regime are well accepted poff and zimmerman 2010 there are few quantitative predictions about how different components of degraded ecosystems will respond to flow restoration arthington and pusey 2003 souchon et al 2008 despite this paucity of quantified relationships flow management decisions must be made therefore of the many environmental flow assessment techniques that have been developed tharme 2003 those that make direct predictions of ecological effects have relied to a great degree on expert opinion stewardson and webb 2010 in australia expert opinion is a major determinant of how the approximately 15 billion investment in environmental water under the basin plan and other initiatives will be used commonwealth of australia 2014 it is therefore surprising that systematic approaches to elicit expert knowledge are less common than informal methods mcbride and burgman 2012 the reliance on expert opinion and the contested nature of environmental flows means that structured elicitation should be of great potential value for developing environmental flow recommendations webb et al 2015 bayesian belief networks bbns pearl 2000 is a modelling technique that is commonly used in natural resource management applications mccann et al 2006 and has also been used in environmental flows applications chan et al 2012 horne et al 2017c shenton et al 2011 shenton et al 2014 one of the often cited advantages of bbns is that they can incorporate multiple data types including expert knowledge horne et al 2017a however this advantage also leaves them open to being populated by poorly elicited expert opinion affected by bias and overconfidence the quantitative outputs from these models may give a false sense of security in the results if they are based on poor quality expert derived data this paper presents a 4 stage formal expert elicitation protocol where opinions are directly incorporated into bbn models providing improved rigour in the relationships and subsequent predictions the 4 stages include pre elicitation workshop facilitation output analysis and bbn model building the first 3 stages are based on the idea framework by hanea et al 2017 combining different established approaches to minimise biases of elicited opinions the final stage consists of discretising the elicited probability distributions in order to populate the conditional probability tables in a bbn model this model becomes an ideal vehicle for later updating with empirical data we first present the general principles underpinning the protocol providing a justification of the approaches used at each stage section 2 we then illustrate application of the protocol using two case studies that elicited quantitative predictions of ecological responses to changes in riverine flow regimes and other environmental factors under environmental flow recommendations section 3 the two case studies elicited expert predictions of expected changes in i terrestrial vegetation encroachment into river channels or undesirable vegetation cover hereafter encroachment and ii abundance of native riparian species on river banks or desirable vegetation cover hereafter native banks the paper closes by assessing strengths and weaknesses of the protocol especially as experienced through the case studies 2 expert elicitation protocol general principles our approach to minimising cognitive and motivational biases include i using a question protocol to reduce overconfidence and availability biases during pre elicitation ii requiring experts to answer questions independently and in isolation to minimise motivational biases but also allowing multiple rounds of judgement to ensure transparency and expert comprehension during workshop facilitation iii using mathematical accumulation of opinion and interpolation as analysis tools to avoid expert fatigue in earlier stages and iv incorporation of opinion into bayesian belief networks models to allow direct predictions under different scenarios with current and updated data further details on all the approaches used are provided below for each of the 4 stages and illustrated in fig 1 2 1 stage 1 pre elicitation 2 1 1 conceptual model building expert elicitation should be based on conceptual models for which both the extent of key scientific knowledge and gaps are identified via literature reviews mcbride et al 2012 in our protocol a conceptual model consists of a set of state variables with arrows linking those variables illustrating the hypothesized causal relationships among them e g fig 2 a these relationships form the basis of the elicitation questions such models can be complex and may need to be simplified to intuitively structure a problem into a set of variables for which knowledge can be elicited keeney and von winterfeldt 1991 mcbride and burgman 2012 a simplified conceptual model provides the structure of a bayesian belief network bbn model developed at stage 4 in this protocol 2 1 2 variables their states and scenarios definition to quantify the linkages among variables with a relatively small number of questions our protocol employs discretization of continuous variables for example a discharge volume in a river might be discretised to high 10 000 ml d medium 2000 10 000 or low 2000 any number of states can be chosen to characterize a variable but using fewer states is recommended to prevent complexities in judging the results mittal and kassim 2007 each combination of discretised states for a causal relationship defines a scenario for the elicitation discretization simplifies the types of questions that need to be asked and also translates well into conditional probability tables cpts cpts provide a useful structure to define the effects of different variables on a particular response variable under different scenarios they also reflect the quantitative expression of expert opinion cain 2001 underpinning the bbn model at stage 4 2 1 3 question protocol in order to combat expert bias the 4 step procedure developed by speirs bridge et al 2010 is used to formulate elicitation questions for each scenario the 4 step question procedure consists of asking experts to estimate for each scenario i the lowest expected value ii the highest expected value iii the most likely value and iv the probability that this interval captures the true value this procedure based on the idea protocol is designed to minimise overconfidence soll and klayman 2004 and has proved useful for eliciting expert knowledge in a range of settings james et al 2010 knol et al 2010 martin et al 2012 2 2 stage 2 workshop facilitation 2 2 1 expert selection subject to availability and willingness of the experts to participate pre selection of experts based on their relevant knowledge and experience is key to identifying contributions from individuals within existing time constraints burgman et al 2011 mcbride and burgman 2012 the number of experts that is sufficient in an elicitation process should be appropriate to the scope of the elicitation available time and resources martin et al 2012 mcbride et al 2012 our experience suggests that 6 experts is often an appropriate number to balance workload for the workshop facilitators and optimize dialogue among experts importantly it also allows for a diverse group of experts which is important for minimizing bias burgman 2015 2 2 2 scenario presentation during workshop facilitation environmental scenarios should be presented in a way that can facilitate expert comprehension the facilitator should also encourage experts to ask for clarification to ensure that the scenarios are understood and to resolve any ambiguities mcbride et al 2012 to maximize comprehension we present scenarios pictorially in the form of printed questions and also as questions read aloud by the facilitator 2 2 3 indirect and independent question answering indirect or predictive elicitation approaches can be used to obtain a specific response from the experts without overtly asking for that response this ensures that experts are less aware of the link between their answers and the final destination of the elicited data for model building thereby reducing motivational biases low choy et al 2012 in our protocol although the final goal is to identify the individual environmental variables and combinations that are most important for the ecological response being considered the experts are not asked this directly instead they are asked to predict environmental outcomes under a wide range of scenarios we do not claim that an expert will answer questions with no preconception of what the most important variable s might be merely that by eliciting the effect of the variable s through multiple questions and multiple experts any bias will be reduced in order to minimise representativeness and availability biases kynn 2008 each participant answers the questions independently and in isolation to reduce anchoring bias during elicitation no initial amounts that could potentially be used as starting values are included in the questions and experts are asked not to refer to their own previous answers or look at previous further scenarios while answering a question mcbride and burgman 2012 2 2 4 within workshop feedback and re estimation the four lowest highest most likely confidence individual responses for a given scenario can be translated into probability distributions this is advantageous over a direct point estimate because it provides a range of values with known probability of capturing the true value hand 2008 to achieve this we use minimum cross entropy calculations kraan and bedford 2005 myung et al 1996 as implemented by salomon 2013 the theory behind this method is beyond the scope of this paper and interested readers are directed to salomon 2013 for full details outputs from the method are further illustrated through the example in section 3 minimum cross entropy allows the determination of the summary statistics characterizing an expert s estimates garthwaite et al 2005 individually elicited expert probability distributions can then be mapped and displayed mccarthy 2007 for each scenario providing experts with visual representations of their responses this highlights differences among individual experts answers as well as making them aware of the statistical interpretation of their estimates garthwaite et al 2005 hogarth 1975 kynn 2008 salomon 2013 in our protocol probability distributions derived from the first set of independent estimates are used in facilitated group discussions aimed at verifying and or modifying expert judgments as another way to manage and alleviate individual expert bias and to improve accuracy of estimates kuhnert et al 2010 mcbride et al 2012 wintle et al 2013 following discussions experts are allowed to submit a revised estimate for any scenario but the original estimate is not discarded see below this idea based protocol can be thought of as a modified version of the delphi process burgman 2005 dalkey and helmer 1963 but for which reaching consensus on elicited scores is not the goal it guards against group think but retains the advantages of discussion burgman 2015 2 3 stage 3 analysis of outputs 2 3 1 aggregation mathematical accumulation is a useful approach for combining expert opinion that may otherwise be prone to biases burgman 2005 2015 group aggregated estimates can be calculated as an average of individual expert distributions in our protocol each expert s results independent and revised if applicable are first averaged and then those results are averaged across the group to provide the ensemble distribution of expert opinion including both the initial and revised estimates from each expert means that we guard against group think by retaining the initial estimate but also offer the opportunity to reduce the influence of initial outlying values by including the final should the expert s thinking change during the group discussion in this attempt to encourage participants to resample their own information arbitrary language based disagreements meanings of words and context can be reconciled burgman 2015 schultze et al 2012 2 3 2 interpolation interpolation of elicited outputs allows reduction in the total number of scenarios and questions put to the experts during a workshop hence reducing the possibility of expert fatigue affecting elicitation outputs burgman et al 2011 if one can assume that causal relationships in the conceptual model are independent outputs for some of the scenarios can be estimated using interpolation rather than having to elicit expert opinions for each of the scenarios individually cain 2001 we use the interpolation methods of cain 2001 in our protocol to complete the results for the scenarios not covered during elicitation the interpolation approach is illustrated through the example in section 3 2 4 stage 4 model building the final stage in the protocol is the incorporation of the expert opinions into a bayesian belief network bbn model bayesian networks are graphical models that can be used to predict the state of one or more response variables as determined by the states of the linked driving variables through conditional probability relationships korb and nicholson 2010 they have been widely used in natural resource management applications mccann et al 2006 the structure of the bbn is derived directly from the conceptual model created at stage 1 fig 2 it is an influence diagram outlining the most relevant relationships between dependent and independent variables korb and nicholson 2010 the relationships among state variables in the bbn are determined by the set of cpts in our protocol the aggregated and interpolated outputs are used to populate the cpts to produce what we refer to as elicited probability tables epts this results in a fully operational model that allows rapid assessment of how different scenarios affect outcomes the models can also be readily updated with empirical data when those data become available finally bbns provide an indirect measure to rank variables for model validation and such values can be compared to direct ranking of experts 3 case studies on vegetation responses to environmental flows we illustrate the above protocol using the example of two elicitation projects on encroachment and native banks to quantitatively predict the response of in channel and riparian vegetation to inundation by environmental flows under a number of different scenarios 3 1 application of the protocol 3 1 1 pre elicitation we built two evidence based conceptual models to predict terrestrial vegetation encroachment into river channels miller et al 2013 and abundance of native riparian species on river banks miller et al 2015 both models of ecological response were based on rigorous literature analysis norris et al 2012 recognising that simpler models can be a more useful starting point for expert elicitation mcbride and burgman 2012 we used simplified versions of the models highlighting only the most important causal links fig 2 for the encroachment example these causal links included i increasing the duration and frequency of inundation will reduce encroachment of undesirable terrestrial vegetation into regulated river channels and ii increasing the frequency and duration of inundation will increase abundance of native vegetation on the river banks thresholds among states were required for some variables e g definitions of long medium and short for inundation by base flows table 1 these were based on published environmental flows recommendations where possible e g skm 2002 the causal relationships formed the basis of the elicitation questions defining scenarios about the relationship between riparian vegetation and flow components table 1 the final cpts provided a summary of expected outcomes for each ecological response given 18 different environmental scenarios for encroachment table 2 and 24 scenarios for native banks tables 3 and 4 for each case study the scenarios were organised from worse to best situations to be presented during expert elicitation and to be used as the basis for later interpolation a total of 11 scenarios for encroachment and 16 scenarios for native banks 11 for herbaceous and 5 for woody species were selected for elicitation with the remaining scenarios later filled using the interpolation approaches of cain 2001 the 4 step elicitation questionnaire from speirs bridge et al 2010 was then integrated into a scenario guide that contained the questions see example in fig 3 in all cases the questions referred to the expected vegetation cover expressed as percentage cover of a hypothetical sampling quadrat on an average river bank in victoria australia the measure of confidence that the elicited range between lowest and highest estimates captured the true of vegetation cover was restricted to 50 as a confidence below 50 implies that the expert believes the actual value is more likely to be found outside of their given interval than within it speirs bridge et al 2010 3 1 2 workshops facilitation a diverse group of six experts including fluvial geomorphologists botanists ecologists and river management consultants participated in each of the elicitation workshops on encroachment and native banks each expert received a scenario guide containing background information on the study and a questionnaire see example in fig 3 to ensure that the elicited information was representative of the correct environmental setting the scenario guide detailed the parameters for each cpt scenario and how they should be interpreted across different victorian rivers we also provided time for questions and discussion at this point to ensure that experts had a shared understanding of the factors to be included in the scenarios and what their answers would represent as a way to familiarise the experts with the elicitation process an unrelated practice question was posed and the responses discussed prior to commencing the actual elicitation the scenarios were then presented both in pictorial and written form fig 3 and were also read aloud between scenarios we used an unrelated numerical or maths trivia question to further ensure the elicited answers to different scenarios were cognitively separated after completing all of the elicited questions for all cpt scenarios we translated the expert responses into individual probability distributions which in both case studies described the probability of different levels of vegetation cover under different environmental flow scenarios because we were dealing with responses that are bounded at 0 and 100 cover we used the beta distribution for these examples the individual elicited distributions were displayed for each scenario see example in fig 4 only then were experts allowed to discuss their opinions in light of the discussion the participants were invited but not required to make a second group informed answer for each of the questions in addition at the end of the workshop experts were also asked to provide feedback on the workshop and to directly rank each predictor variable flow duration flow frequency flow season and stock access from most score 4 to least score 1 influential on the response variables direct rankings from experts were used for comparison to indirect rankings calculated from the bbns as a means of evaluating expert coherence 3 1 3 analysis of outputs the initial and revised if provided estimates of vegetation cover under each scenario were mostly right skewed and we used log transformation to reduce this asymmetry this allowed each estimate to be easily transformed from its expert nominated confidence interval to a standard confidence interval 80 transformation would not be necessary for more symmetrically distributed expert data the standardized confidence intervals were then averaged by taking the mean of the two estimates of the upper bound and the mean of the two estimates of the lower bound respectively within experts for each question fig 5 these individual expert distributions were aggregated using the average of all estimates again by averaging the bounds this process is equivalent to simple averaging of probability distributions as described in clemen and winkler 2007 an approach that those authors show to perform approximately as well as other more complex methods for combining probability distributions the averages were then used to populate each of the cpt elicited scenarios by computing the proportion of the probability distribution mass that lay within each state fig 5 the probabilities of vegetation cover for the epts were linearly interpolated to the remaining scenarios from the elicited information in the cpts tables 2 4 linear relationships are the simplest functional form and in the absence of theoretical or empirical evidence regarding likely functional forms among our state variables we chose to use this interpolation factors were calculated from a switch in the state of a predictor variable from having a positive effect to having a negative effect e g presence of stock will negatively affect native riparian species cover due to grazing interested readers are directed to cain 2001 where this approach is detailed fig 5 illustrates how the interpolation factors and interpolated values were calculated for the encroachment case the final epts contained both the elicited and interpolated probabilities of high medium and low vegetation cover under each analysed scenario fig 5 3 1 4 model building and testing based on the final epts we built bbns using the netica program norsys software corp 2010 and we used them to obtain indirect rankings of the predictor variables these were then compared to the direct expert ranking on how predictor variables influenced response variables in netica a variable s sensitivity is quantified as entropy reduction if a predictor variable has a larger value of entropy reduction this indicates that during the elicitation workshop the experts subconsciously attributed importance to this variable when considering the effect of different environmental scenarios 3 2 modelling results expert opinion indicated that long durations of inundation give the lowest probability of both high undesirable vegetation cover encroachment and low desirable vegetation cover native banks conversely stock presence was expected to result in low probabilities of both high levels of encroached vegetation in channels and native vegetation on river banks tables 2 4 for encroachment probability of high vegetation cover 25 100 coverage ranged from 25 to 91 table 2 for native banks probabilities of high cover of herbaceous native species were lower with scenarios having between 8 and 52 probability of high cover table 3 woody native species fared worse again with probabilities of high cover only ranging from 9 to 15 and low covers 0 5 being the most likely outcome for all scenarios table 4 the resulting cpts were sensitive to the combinations of environmental drivers despite differing opinions among the experts appendix a the elicited probabilities of encroached terrestrial vegetation cover differed widely across the different environmental scenarios in the encroachment cpt table 2 however there was less divergence in the elicited probabilities of native riparian vegetation cover for the scenarios in the native banks cpt tables 3 and 4 flow duration was identified by experts as the most important variable for discouraging growth of terrestrial vegetation in the river channel whilst stock access was considered most responsible for reduced abundance of native riparian species on river banks table 5 comparing direct vs indirect ranking of predictor variables it is apparent that the experts are clear both consciously and subconsciously about these two most important environmental variables they both show the highest entropy reduction indirect ranking and are given the highest scores by experts table 5 there is only some disparity between indirect and direct rankings on the importance of season and delivery frequency of base flows for encroachment and on the ranking of flows duration and season for native banks table 5 through the feedback at the end of the workshop the experts indicated a variety of confounding factors that they considered when estimating the highest vegetation expected and lowest vegetation expected bounds for each scenario these included shading effects of canopy bank slope and aspect substrate and texture land use and exotic vegetation competition for encroachment the experts from the native banks workshop also identified that they expected native vegetation especially woody species to respond to longer inundation durations than those included in our scenario guide 4 discussion environmental management requires quantitative predictions of ecological responses to management actions in the absence of field data the use of a formalised expert elicitation protocol can produce transparent methodologically rigorous and defensible estimates of such relationships cook et al 2012 knol et al 2010 martin et al 2012 runge et al 2011 our protocol brings together for the first time three previously published methods for eliciting and synthesizing expert knowledge the four point elicitation method speirs bridge et al 2010 minimum cross entropy for visualizing expert based probability distributions salomon 2013 and bayesian belief networks pearl 2000 this provides standardised elicitation of high quality transparent predictions of environmental responses knol et al 2010 that can be used for environmental management following the idea framework approach hanea et al 2017 the combination of several hitherto separate approaches to minimise expected cognitive biases is an improvement over informal methods for expert elicitation moreover the incorporation of accumulated opinions into bayesian belief networks provides a means for this knowledge to be used in the rapid and direct prediction of ecological responses under different management scenarios resulting in a fully operational model that can be readily updated with field data our findings have a direct application for environmental flow management and other natural resource management applications since the case studies described in this paper were done the protocol has been employed to investigate the likely effectiveness of complementary environmental restoration projects to enhance outcomes from environmental flows cresswell et al 2017 to inform the development of fish passage structure design criteria for non sport fish in the southern hemisphere wilkes et al 2017 and to quantify simple models of ecological response of the type reported in horne et al 2017c for incorporation into optimisation based decision support tools for environmental flows planning expert based predictions elicited here indicate that when delivering flows to achieve ecological outcomes for riparian and channel vegetation delivery of specified durations of base flows high flows and over bank flows are significantly more important than whether these flows are delivered continuously or episodically or the season of the flow is delivered in these are intuitively sensible predictions that can be tested through monitoring within an adaptive management framework the formal protocol provides several advantages over more commonly used informal approaches firstly the 4 step question procedure developed by speirs bridge et al 2010 reduces overconfidence bias second indirect elicitation through the presented scenarios reduces motivational biases as experts are not necessarily aware of the link between their answers and final quantification of models using the elicited data low choy et al 2012 in addition our method shows more transparency than informal round table discussion elicitation methods as experts are required to first answer questions independently of previous questions and without discussion this allows every expert s opinion to hold equal weight kynn 2008 and also provides a full audit trail of results should they be challenged 4 1 practical implementation of the protocol for the workshop to yield useful results from experts it was important to run an informal but well facilitated discussion of the environmental variables during the introduction of the scenarios guide this included a detailed description of the different states of the variables of interest e g we defined long base flow inundation duration as 50 days and any references for such definitions while group discussion was paramount to making sure that the experts understood and agreed about what specifically they were predicting the conversation had to remain solely about the definition and not the influence of the variables as discussion of influence could introduce bias into the subsequent elicitation one example of such clarification was whether we were seeking opinions on either basal or canopy cover for a given scenario our group size was a trade off among i maximizing diversity within the expert group ii finding enough experts available simultaneously for a one day workshop iii optimizing dialogue among experts and iv maintaining an achievable workload for the people running the elicitation however we tried to make the expert pool as diverse as possible in terms of expertise and background to minimise bias hanea et al 2017 mcbride and burgman 2012 we found that experts were often initially uncomfortable with the generality of scenarios during the facilitator introduction and scenario guide phase however once they completed the practice question and began to work through the formal environmental scenario questions they reported feeling more comfortable and even enjoying the elicitation process our use of the minimum cross entropy calculator salomon 2013 in situ allowed us to show the distributions of the elicited intervals in the second half of the workshop this was important for informing the subsequent discussion and provision of revised estimates and it was a major step in helping experts to understand how their answers would contribute to the final conditional probability table the use of interpolation to extrapolate predictions for some scenarios cain 2001 allowed us to maximise the number of variables we could include making our results more ecologically realistic it also reduced the number of questions asked of the experts easing fatigue reducing the number of answers required from experts allowed us to restrict the workshops to one day in length alleviating the logistical challenges of organising longer or additional workshops involving different experts the only negative impact interpolation was the reduced consideration of specific combinations of drivers we had to assume independence of the effects of some drivers and it is possible that expert opinion might not have supported this had these questions been asked in the workshops the direct ranking of the effect of different predictor variables from most to least influential was useful to contrast with indirectly derived results and to assess our protocol it also provided a way to detect the degree of agreement between conscious direct ranking and subconscious indirect elicitation opinions from the experts the results support both the validity of the elicitation protocol employed here and the objectivity of the expert knowledge provided 4 2 lessons learned and potential improvements the two case studies reported here are presented as if exactly the same method were followed each time this is an oversimplification we learned a lot during the encroachment workshop which meant that native banks ran more smoothly when it was undertaken several months later by the same staff but with different experts for example we extended the time allocated for discussion of the scenario guide to ensure that this step was not rushed and that all participants entered the elicitation phase of the workshop with a clear understanding we also provided more specific guidance about the interpretation of the self nominated confidence intervals in the expert estimates while the experts attending the encroachment workshop were content that we had identified important drivers of terrestrial vegetation encroachment there were several instances during the native banks workshop where the experts conceptual understanding of the relationships between native riparian vegetation and our environmental predictors were not aligned experts from the native banks workshop identified that they expected native vegetation to respond to longer inundation durations than those included in our scenario guide this illustrates a need to assess the adequacy of proposed scenarios and model structures prior to the workshop however this can be difficult as experts attending the workshop were deliberately not consulted about questions beforehand to reduce the effects of anchoring on their elicited opinions mcbride and burgman 2012 we did ground truth conceptual model structures with other experts prior to the workshop but this did not always match the opinions of the workshop experts when asked to quantify the relationships currently we are experimenting with directly involving the experts from the workshop in the design of conceptual models and the designation of driver states and thresholds cresswell et al 2017 there is a number of ways in which the sensitivity of the outputs could be tested first there is no specific reason for equally weighting the initial and final estimates of each expert prior to aggregation across the group we did this because we wanted both estimates to contribute to the final distribution reducing the effect of group think but also taking advantage of the collective expertise of the group to allow experts to resample their own information we could assess the sensitivity of estimates to this step by including only initial or final estimates or by altering the weightings similarly the sensitivity of the final distribution to group makeup can be explored by jack knife style iterations of estimates each excluding individual experts from the combined distribution nicol et al 2017 second the sensitivity of estimates to the discretizations chosen could also be tested here we based state thresholds on published environmental flows recommendations as these are relevant to management decisions but these values are mostly based on expert opinion and negotiated settlements and it would be useful to test the extent to which they affect model outputs when we interpolated among elicited scenarios we assumed linear relationships outputs would have been slightly different had we assumed another functional form and this could be investigated however linear relationships are the simplest functional form and we suspect that any study that needs to employ expert elicitation to quantify relationship will not also have sufficient evidence to assume a particular functional form beyond linear 4 3 interpretation of our results our elicitation protocol produced results that were consistent with but extended the literature regarding the importance of inundation for reducing undesirable vegetation cover encroachment and increasing desirable vegetation cover native banks miller et al 2013 found strong evidence in favour of the importance of inundation duration frequency and seasonality for reducing terrestrial vegetation abundance conversely miller et al 2015 found that an increase in the duration of high flows will simultaneously stimulate germination and reduce survival of seedlings and herbaceous vegetation but overall found strong evidence that more frequent overbank and high flows will increase survival and condition of native woody vegetation overall these findings are aligned with the elicitation findings that inundation is an important variable for discouraging growth of terrestrial vegetation in the river channel and encouraging desirable vegetation cover however while these earlier literature analyses had provided qualitative evidence for specific cause effect relationships the expert elicitation allowed us to quantify those linkages 4 4 implications for environmental management our protocol of standardised expert elicitation aims to improve the transparency and repeatability of management decisions when field data are challenging or costly to collect for the case studies presented here the conceptual models and bbns quantified with expert opinion were simplified as they were intended to create expert elicited priors for bayesian hierarchical models based on field data webb et al 2015 however our expert elicitation process is equally useful for any situation requiring more complex conceptual models and bbns that can be used for direct estimation of environmental outcomes to inform management decision making ideally elicited opinions would be used within an adaptive management loop following the implementation of decisions monitoring and evaluation would provide data to improve understanding it is a specific advantage of the bayesian networks employed here that such data can be used to update cpts through the application of bayes rule bayesian methods combine the expert opinion with data rather than replacing one with the other in this way predictions of ecological responses would progressively rely less and less upon expert opinion as more data were collected nevertheless at the start of this process managers may be making decisions in a virtual data vacuum expert opinion may be the only data available and the protocol outlined here maximizes the robustness and transparency of these data acknowledgements funding was provided by the australian research council lp100200170 we thank yacov salomon for his help with the minimum cross entropy calculator and all the experts for their participation in the workshops appendix a supplementary data the following is the supplementary data related to this article supplementary data 1 supplementary data 1 supplementary data 2 supplementary data 2 appendix a supplementary data supplementary data related to this article can be found at https doi org 10 1016 j envsoft 2017 11 020 
26456,redd was initially conceived of as a multi level carbon based payment for environmental services pes it is still often assumed to be a cost effective climate change mitigation strategy but this assumption is mostly based on theoretical studies and static opportunity cost calculations we used spatial and socioeconomic datasets from an amazonian deforestation frontier in brazil to construct a simulation model of redd payments to households that can be used to assess redd interventions our simredd model consists of dynamic optimization and land use cover change allocation submodels built into an agent based model platform the model assumes that households maximize profit under perfect market conditions and calculates the optimal household land use cover configuration at equilibrium under a given redd pes scenario these scenarios include pes based on 1 forest area and 2 carbon stocks insights gained from simulations under different conditions can assist in the design of more effective efficient and equitable redd programs keywords redd payments for environmental services land use cover change agent based modeling farm household optimal control software and or data availability the model described simredd was built in netlogo v 5 3 1 software for agent based modeling wilensky 1999 the first and current version of the model was finalized in october 2016 netlogo and therefore simredd should work on any platform on which java 5 or later is installed computers with 8 gb of ram might run out of memory due to the substantial number of virtual agents simulated in the model simredd script is freely available from the corresponding author after signature of a data use agreement between the requester the corresponding author and the principal investigators responsible for acquisition of the sensitive household level socioeconomic data 1 introduction spatially explicit biophysical and anthropogenic factors correlated with tropical deforestation are well discussed in the literature e g geist and lambin 2002 müller et al 2012 pfaff 1999 and are often key components of land use cover change lucc simulation models eastman 2015 soares filho et al 2002 examples of these factors are land slopes and distances from roads markets and protected areas the observation that the relationships among such factors shed little light on the decision making processes that undergird lucc deadman et al 2004 le et al 2008 parker et al 2003 motivates attempts to elucidate the causal effects of policy interventions on deforestation e g blackman 2013 börner et al 2016 one way to remedy this deficiency is through use of agent based models abm which offer a powerful way to explore coupled human natural complex systems wilensky and rand 2015 abms can be used for example to inform the design of policies or programs to curb deforestation while minimizing negative impacts on local stakeholders iwamura et al 2016 purnomo et al 2013 snilstveit et al 2016 in this paper we describe a hybrid optimization abm that simulates the effects of payments for environmental services pes to settlers on an old amazonian deforestation frontier on lucc and community welfare payments were structured in accordance with redd reducing emissions from deforestation and forest degradation with recognition of the roles of conservation sustainable forest management and enhancement of carbon stocks un redd 2008 2011 practices the model presented here was designed to shed light on issues related to the long run effectiveness efficiency and equity of redd interventions an abm is a complexity theory based computerized simulation snooks 2008 of a number of decision makers agents who interact through prescribed rules in a dynamic environment an 2012 liu et al 2007 the independence of agents represented in an abm and their interactions allow researchers to capture characteristics over time of complex systems like heterogeneity nonlinearity feedbacks and emergent properties an 2012 an et al 2005 liu et al 2007 abms have the potential to assess impacts of policy scenarios on coupled human natural systems as they attempt to mimic the behavior of institutions and stakeholders in a virtual representation of reality e g andersen et al 2014 iwamura et al 2014 purnomo et al 2013 thus development and application of abms represent a reasonable early step in policy making farmer and foley 2009 which is substantially less costly than field based randomized control trials and can provide guidance sooner than quasi experimental evaluations of pilot initiatives sills et al 2017 lucc models based on abm frameworks are often coupled with biophysical information about the landscape that can be used for spatially explicit model validation deadman et al 2004 ngo and see 2012 pontius et al 2011 the spatial component of lucc abm allows for better virtual representation of real landscapes and consideration of both natural and socioeconomic contexts an et al 2005 deadman et al 2004 mena et al 2011 arguably the most important component of a lucc abm is the decision making processes of its virtual agents e g farm households in general those processes are based on decision trees e g deadman et al 2004 salvini et al 2016 or the maximization of utility functions e g andersen et al 2014 monticino et al 2007 maximization problems often include labor income and physical constraints e g evans et al 2001 and are grounded in farm household economics and agricultural production theories angelsen 1999 darwin et al 1996 redd was initially conceived as a system of pes for carbon retention or sequestration un redd 2011 it has garnered substantial international attention as a potentially cost effective approach to climate change mitigation kindermann et al 2008 negotiations at the 21st conference of parties of the united nations framework convention on climate change acknowledged redd as a strategy to be incorporated into future international climate regulations unfccc 2015 yet it is widely accepted that redd will only be effective if payments offset stakeholders opportunity costs which can vary substantially abram et al 2016 borrego and skutsch 2014 torres et al 2013 a recent meta analysis by phan et al 2014 reported opportunity costs for redd range from us 0 05 to 92 mg co2 1 the average cost in latin america was approximately 5 4 mg co2 1 within the 0 2 to 13 mg co2 1 range reported by börner and wunder 2008 for the brazilian amazon ickowitz et al 2017 reported substantially higher opportunity costs to smallholders over a 30 year time horizon with a 9 discount rate ranging from 11 to 77 mg co2 1 across five sites in the brazilian amazon in contrast nepstad et al 2007 use a much lower opportunity estimate of 1 5 mg co2 1 across the entire region multiple reasons explain why opportunity costs range so widely across the tropics but the primary one stems from variation in possible land uses not all tropical forests are located in plantations due to logistical e g inaccessible regions or environmental reasons e g soil quality precipitation regimes and presence of phytopathogens in addition the lack of high quality spatial and economic data to estimate opportunity costs often forces researchers to extrapolate them from one region to others with very different conditions land use options and opportunity costs e g börner et al 2014 nepstad et al 2007 which could result in inaccurate estimates furthermore when opportunity cost estimates are static i e they are not allowed to change with changes in economic conditions over time pana and gheyssens 2016 takasaki 2012 interpretation of results must be made with extra caution given that market conditions can change substantially over time more dynamic analyses are warranted abms can shed light on the extent to which different redd payments are likely to change business as usual behavior through time by simulating lucc decision making by farm households which avoids the potential pitfalls of static opportunity cost analyses andersen et al 2014 purnomo et al 2013 salvini et al 2016 here we describe a hybrid optimization abm e g andersen et al 2014 we developed to evaluate the potential impacts of redd payments on settlers on an amazonian deforestation frontier impacts are evaluated in terms of land use cover change co2 emissions total program expenditures on pes given program enrollment rates and community welfare even though our model was constructed on an abm platform we consider it a hybrid model because agents lucc decisions are strictly based on the solution of the dynamic optimization problem in which each household seeks to maximize farm profits 2 methods 2 1 study region the study region covers approximately 6000 km2 in rondônia state brazil including the municipality of ouro preto do oeste and its five contiguous neighbors vale do paraíso nova união teixeirópolis urupá and mirante da serra all of which are near the major federal highway br 364 fig 1 the area lies within the arc of deforestation in the brazilian amazon caviglia harris 2005 a well documented heavily deforested old frontier where settlers began arriving in the 1960s in response mostly to government sponsored programs these include the national integration program the northwest region integrated development program and operation amazonia caviglia harris 2004 sills and caviglia harris 2008 in the study region the federal land reform agency incra allocated nearly 9000 lots laid out in a regular pattern along roads to settlers arriving from other regions deforestation in the study region has therefore followed a road related fishbone pattern and has been the subject of numerous lucc studies since the early 1990s e g dale et al 1994 2 2 datasets 2 2 1 socioeconomic data socioeconomic household survey data from a systematic random sample stratified by municipality collected from 697 households in 2009 were used to parameterize the model caviglia harris et al 2014 the majority of households in the region virtually all of which immigrated from outside the amazon are small scale producers 98 with an average property size of 65 ha range 2 240 ha while the average for medium and large lots top 2 is 745 ha range 240 3000 ha households initially focused on annual and perennial crops but revenues from these crops declined by almost 70 between 1996 and 2009 and dairy and beef cattle production largely replaced that income milk production increased from an average of approximately 17 000 l per year in 1996 to approximately 30 000 l in 2009 overall annual income per household increased from 4000 to more than 9000 inflation adjusted for the same period and the average number of cattle owned per household increased from approximately 70 to 140 head the settlements experienced high rates of deforestation between 1996 and 2009 with mature forest cover decreasing by more than 50 overall only about 10 of forest cover remained in the study region in 2009 caviglia harris and harris 2011 caviglia harris et al 2015 2 2 2 remote sensing and gis data the hybrid model incorporates a spatially explicit lucc submodel based on five spatial layers 1 the first is a map displaying the location of 8900 settlement properties caviglia harris et al 2015 fig 1 2 the second is a landsat 5 satellite based lucc classification map generated in 1996 the year the first survey was completed showing mature forest secondary forest i e forest that regenerated naturally after abandonment of agricultural land and agricultural use classes fig s1 roberts et al 2002 toomey et al 2013 3 the third map created using the artificial neural network based algorithm shows the ranked suitability i e risk for deforestation of mature forest patches the deforestation suitability map was constructed with the multi layer perceptron mlp an artificial neural network based algorithm available in terrset v 18 11 software eastman 2015 it indicates the forest patches within the study region that are more likely to be cleared in the future fig s2 the map was based on the mapped land use cover changes and persistence between 1986 and 1996 and 11 biophysical maps of the study region the maps included in the mlp were administrative areas at the municipality level macro political regions soil quality elevation timber market zones distances from previously deforested areas distance from major roads distance from major rivers distance from protected areas distance from logging areas and distance from major markets table s1 and fig s3 the calibration accuracy of the mlp was 71 table s2 4 the fourth is a map of ranked suitability of secondary forest clearing based on euclidian distances to agricultural land in 1996 5 the final map provides the ranked suitability of agricultural abandonment based on euclidian distances to forest patches in 1996 additionally the tropical carbon density map developed by baccini et al 2012 was used to estimate per hectare averages of net carbon emissions from lucc figs s4 and s5 2 3 model description the lucc model named simredd is described in accordance to the overview design concepts and details odd protocol a standard guideline for the description of abms grimm et al 2010 simredd was built in netlogo v 5 3 1 wilensky 1999 general data analyses were conducted with r v 3 3 0 statistical software r core team 2016 algebraic transformations were performed in mathematica v 9 0 wolfram research inc 2012 spatial analyses were conducted with r terrset v 18 11 eastman 2015 and arcgis v 10 3 1 esri 2015 software 2 3 1 purpose the model aims to simulate the effects of direct redd payments conditional on forest cover including both retention of mature forest and regeneration of secondary forest the objective of the model is to assist in the design of more effective efficient and equitable redd programs and policies in the long run the first step in the model is to create a baseline lucc scenario at equilibrium without redd payments the second step introduces redd payments as an additional source of annual income for households payments are proportional to the sum of the area in mature forest and the area set aside for secondary forest regrowth on each property given a pes scenario households choose a new optimal land use cover configuration based on estimates of carbon stocks associated with each land use cover class the model quantifies net carbon emissions mg co2 from the lucc activities two types of pes for forest conservation are often discussed in the literature 1 payments based on forest carbon stocks and 2 payments based on forest area the first scheme is associated to carbon market based initiatives e g hamrick and goldstein 2016 while the second approach is often taken by government conservation programs e g arriagada et al 2012 de koning et al 2011 simredd can simulate these two pes schemes redd payments under the carbon market option are applied in accordance with decentralized private pes for avoided deforestation and voluntary carbon market rules that are expected to be followed by national redd programs verified carbon standard 2017a redd payments based on area ha 1 are referred to as the policy option in simredd and are assumed to be annual constant and based on the current area of mature forest cover plus agricultural areas set aside for secondary forest regrowth on each property in practical terms the implementation of pes policy option is easier when compared to the carbon market option described above as it does not require estimation of forest carbon stocks under the carbon market option redd payments are based on the differences in carbon emissions under the baseline and project scenarios mg co2 1 this option also assumes that redd payments for a given ton of co2 either removed or not emitted to the atmosphere i e additional co2 are made only once during a hypothetical carbon project lifetime in accordance with carbon market standards verified carbon standard 2017b the project lifetime considered in our simulations is 20 years constant renewal of the hypothetical carbon project is assumed after expiration because the model is projected into infinity as result a given ton of additional co2 is rewarded multiple times but only once every 20 years therefore a direct implication of changing the hypothetical carbon project lifetime is that if length 20 years a given ton of additional co2 is rewarded more frequently in the long term and hence redd revenues for the household increases the opposite effect is observed if the length of the project is 20 years it is our understanding that these two schemes encompass the majority of currently implemented pes interventions focused on forest conservation and restoration in accordance with the framework of the united nations collaborative program on redd un redd 2015 the model considers but makes no distinctions between payments made for avoided deforestation and promotion of natural regeneration this simplification seems justified because secondary forests present at the beginning of model simulations in 1996 and newly formed forest patches created through the abandonment of agricultural land are both assumed to reach the status of mature forests when projected into infinity when no lucc occurs at model equilibrium see section 2 3 6 and the supplementary material for details the model reports the total annual amount of money spent by the simulated redd scheme on pes at equilibrium the model assumes that enrollment is voluntary and unconstrained and that the amount of land households allocate to redd if any depends on the extent to which the offered pes offset the households opportunity costs the model does not incorporate the total cost of the redd interventions as it does not capture transaction or administrative costs luttrell et al 2017 thompson et al 2013 2 3 2 entities state variables and scales there are two types of entities in the model households or farmer agents and land use cover patches 28 5 m resolution the approximately 6000 km2 of settlements were divided into ten sub regions to cope with the computational limitations of netlogo railsback et al 2006 fig s6 table s3 agricultural patches represent different and unspecified mixes of annual and perennial crops and pasture three variables describe each patch grid cell in the model 1 a land use cover category mature forest secondary forest or agriculture 2 a patch owner identification number and 3 a value related to its suitability of change to a different land use cover class either the risk of deforestation for forest patches or of abandonment for agricultural patches aboveground carbon stocks were estimated in mg co2 ha 1 the standard unit of carbon emission offsets rifai et al 2015 unfccc 2013a based on the weight ratio between co2 and c 44 12 a biomass carbon fraction of 0 47 and below to aboveground biomass ratios of 0 24 and 0 20 for mature and secondary forest respectively ipcc 2006 mokany et al 2006 the averages of aboveground carbon stocks associated with the mature and secondary forest land cover classes were estimated at 537 and 418 mg co2 ha 1 respectively baccini et al 2012 fig s4 for agricultural patches a total carbon stock at equilibrium of 104 mg co2 ha 1 was adopted fearnside 1996 farmer agents are assigned a patch owner identification number that indicates which patches they own and the size of their farms farms are not allowed to change sizes over the course of the simulation i e household members do not split or acquire lots 2 3 3 process overview and scheduling the model represents two processes related to lucc in the settlements the first is quantification of the optimal agricultural area for each farm the second is the spatial allocation of deforestation of forest patches and abandonment of agricultural patches shifts in the optimal agricultural area result from changes in the parameter values displayed on the model interface market price of agricultural outputs redd payments for avoided deforestation and promotion of natural regeneration and wage rates simulations end when the lucc equilibrium state is reached as defined by the individual farmer agents optimal agricultural area under each payment scenario 2 3 4 design concepts basic principles the model is based on the theory of farm households operating in complete markets darwin et al 1996 in which households are assumed to behave as if they maximize profits angelsen 1999 takasaki 2013 these farm households are the farmer agents in the model a cobb douglas production function is embedded into the farmer agent s profit function the literature on farm household production is generally focused on one crop to avoid problems often encountered in multicrop production models barnum and squire 1979 in light of this limitation and to make the lucc model as generalizable as possible the rice equivalent andersen et al 2014 was adopted as a single standard agricultural output annual and perennial crops as well as animal production and their respective prices were converted into their rice equivalent based on energy content and annual productivity tables s4 and s5 fig s7 emergence lucc patterns emerge based on the optimal agricultural area calculation and the areas more suitable for land use cover change and persistence due to the fact that transitions of one patch affect the suitability for transitions of neighboring patches adaptation learning agents do not present adaptive traits i e they do not change behavior over time in response to learning objective the farmer agent s objective is to maximize a profit function through a dynamic optimization framework a cobb douglas production function e g barnum and squire 1979 determines the volume of rice equivalents produced as a function of the household s agricultural area and labor predicted production combined with agricultural output prices determine the households revenue the model assumes that a given agricultural patch maintains the same productivity into infinity which is an average of the production potential of the agricultural land obtained from the panel data through an ordinary least squares regression maintenance costs of agricultural patches deforestation costs and labor costs are subtracted from household revenue in cases that incorporate redd payments the latter are added to household profit prediction sensing households are treated as homogenous agents they are aware of the patches within their farms boundaries the current land use cover class of those patches and the suitability of mature and secondary forest patches for deforestation or of agricultural patches for abandonment through the dynamic optimization exercise the farmer agents become aware of the optimal size of agricultural area and change their land use cover patches to reach that level interaction collectives land use cover patches belong to a farm and a farmer agent farmer agents interact exclusively with their own land use cover patches with no interactions among households nonetheless neighboring patches influence the suitability of lucc of areas that belong to different farmer agents stochasticity no stochastic processes are incorporated in the model the initialization conditions are based on a lucc classification map of 1996 while the dynamic optimization process is based on an internal analytical solution both procedures are deterministic observations changes in areas of forest and agricultural land at the sub region level in the simulation are monitored and compared to the areas in lucc classification maps derived from landsat satellite imagery annual household profits at the equilibrium state of the model run are also monitored and compared to the 2009 panel data survey 2 3 5 initialization the model is run for one of its ten sub regions at a time with one farmer agent created for each farm land use cover patches are created following a landsat 5 based lucc classification map from 1996 roberts et al 2002 toomey et al 2013 deforestation and abandonment suitability maps are loaded into netlogo and their values are assigned to forest and agricultural patches respectively similarly land use cover patches are assigned to their respective farmer agent owners the farmer agents count the patches in each land use cover class within their farms and store those values for use during the model run netlogo then displays the 1996 land use cover map on which it overlays property boundary polygons fig 2 finally initial sizes ha of mature forests secondary forests and agricultural lands at the landscape level are reported and community welfare is calculated as the sum of all households annual farm profit 2 3 6 submodels profit maximization submodel the household decision making process is assumed to follow a dynamic maximization framework such a framework is required because any additional deforestation expands the household agricultural area households maximize a profit function under perfect labor market conditions angelsen 1999 takasaki 2013 1 π p a f a t l t p e i f a t w l t γ d t c a a t c d d t where p a is the price of the agricultural production output f a t l t is the production function that predicts output produced as a function of agricultural area a t and agricultural labor l t at time t w is the wage rate associated with agricultural labor and the deforestation and site preparation labor γ per unit of deforested area d t c a and c d are non labor costs associated with maintaining agricultural areas a t and deforestation and site preparation costs required per hectare of deforested area d t respectively the profit function also incorporates the financial benefit from an environmental service payment represented by p e i f a t where p e i is the redd payment unit 1 for payment type i discussed further below based on the total farm area f not converted for agricultural land use a t under the assumption that all farms were once forested to allow an analytical solution a cobb douglas production function was chosen for its closed form solution property e g angelsen 1999 bronfenbrenner and douglas 1939 2 f a t l t α a t β l t φ where β and φ are the land area and labor output elasticities respectively and α is the total factor of productivity these three parameters were estimated from the socio economic panel data based on the agricultural area and family size of the household number of family members with off farm jobs and number of people hired to work on the farm throughout a year given that production functions are likely to vary with property size production function parameters were estimated based on data from small farms with 28 large lots excluded from the simulations among the excluded farms are shared legal reserves that belong to the settlements areas that must remain forested to comply with brazilian environmental regulations the unit of p e i varies with the redd payments scheme i chosen with the redd policy option payments p e ρ are based on the remaining mature forest area and the area allocated for secondary forest regrowth in the farm ha 1 with the carbon market option payments p e μ are based on the net carbon emissions from avoided conversion of forest to agricultural land and net carbon sequestration from regeneration of abandoned agricultural patches mg co2 1 captured by an additional term δ rifai et al 2015 because payments for a given mg co2 can only occur once over a carbon project lifetime the average annual redd payment under the carbon market scheme becomes 3 p e μ p e ρ δ t where t represents a hypothetical redd project lifetime implemented in accordance with voluntary carbon market standards verified carbon standard 2017b redd payments are annualized by t which is the time assumed for agricultural patches set aside for forest regrowth to achieve the carbon stock status of mature forest t years is also the ex ante time assumed for all forest patches within a given farm to be cleared in the absence of redd payments see figs s8 and s9 for details given that a t a t 1 d t the deforestation process is captured as 4 a t t a t d t additionally the amount of deforestation cannot exceed the amount of remaining forest yielding an additional constraint 5 f a t d t 0 the optimal solution can be solved with the construction of the following hamiltonian equation michel 1982 6 h p a α a t β l φ a φ p e i f t a t w l t γ d t c a a t c d d t λ t d t ψ f a t d t where λ t is the co state variable associated with the expansion of agricultural land and ψ is the shadow value for the maximum deforestation constraint i e households cannot clear more land than they possess the first order conditions of the hamiltonian equation then become 7 h l t p a φ α a t β l t φ 1 w 0 8 h d t w γ c d λ t ψ 0 9 h a t p a β α a t β 1 l t φ p e δ c a ψ λ t r λ t 10 h λ t d t a t 11 h ψ f a t d t 0 ψ 0 f a t d t 0 ψ 0 the optimal agricultural land at equilibrium a is given recursively by solving the optimal control problem appendix a 12 a t w β 1 1 φ α 1 φ p a 1 φ r c d p e δ c a r ψ γ r ψ ψ 1 φ 1 φ φ β φ 1 in all of the above equations ψ 0 when there is forest left i e the land restriction is not binding finally the deforestation path d t is defined by 13 d t 0 w γ c d ψ λ t d w γ c d ψ λ t d w γ c d ψ λ t if a t a the household deforests at the maximum possible rate d if a t a the household abandons agricultural land leading to natural regeneration of forest until the optimal amount of agricultural land is reached once the optimal area in agricultural use is reached deforestation d t equals zero and the amount of agricultural land remains at a while this model assumes parameters to remain constant over time the model can easily calculate a new equilibrium land area and the associated implied deforestation or forest regeneration if conditions were to change after initial equilibrium is reached optimal agricultural labor at equilibrium l is then calculated recursively from the profit function equations 1 and 2 given the optimal agricultural area a as 14 π p a α a β l t φ p e i f a w l t γ d t c a a c d d t 15 π l t p a α a β l t φ l t w p a φ α a β l t φ 1 w 0 16 l w p a φ α a β 1 φ 1 finally the optimization submodel calculates community welfare based on the sum of all n individual annual household i profits for each simulation scenario j at the lucc equilibrium state π e 17 π e i j i 1 n p a α a t i j β l t i j φ p e f a t i j w l t i j c a a t i j total annual redd expenditures on pes are also calculated for a given household enrollment rate in the program because welfare is calculated at the household level equation 17 can easily be used to compare the impacts of redd payments in terms of equity across household socioeconomic groups e g richest versus poorest lucc allocation submodel once the optimal agricultural land area is defined farmer agents decide what lucc needs to take place to adjust land use on their farms to that value when deforestation is required farmer agents first clear secondary forest patches with preference to patches with higher deforestation suitability closer to previously established agricultural areas if all secondary forest patches are cleared and the optimal amount of land in agricultural use is still not yet reached farmer agents convert mature forest patches also giving preference to patches with higher deforestation suitability until the optimal land use level is reached or until the entire farm is under agricultural use when the optimal amount of agriculture land is less than the amount previously used for agriculture in the farm farmer agents abandon agricultural patches affording priority to patches with higher abandonment suitability closer to forested areas until the optimal level is reached finally the size of each land use cover class and the net carbon balance from the lucc processes co2 emissions minus sequestration are reported at the equilibrium state of the model 2 4 model verification and sensitivity analysis model verification was conducted for debugging purposes wilensky and rand 2015 local sensitivity analysis was conducted to examine which parameters most affect simulation results railsback and grimm 2012 sensitivity analysis of this model was particularly important because parameter values were either empirically estimated with the 2009 panel data and assumed to be constant through the simulations or obtained from studies based on different regions of the amazon basin tables 1 and 2 sensitivity was analyzed based on the relative change of the following model parameters and their effects on the optimal level of agricultural land at the equilibria cobb douglas production function parameters price of the agricultural output agricultural costs net carbon emission factor redd payments discount rate and hypothetical carbon project lifetime see equations 1 3 for each step of the sensitivity analyses all independent variables were held constant except the variable of interest which assumed original values decreased and increased by 100 from base value by 10 intervals the analysis was conducted under both the carbon market based and the redd policy payment based option schemes additionally a sensitivity analysis of the artificial neural network based model used to create the deforestation suitability map for the mature forest was conducted with terrset v 18 11 software the latter analysis was based on three methods 1 forcing a single independent variable to be constant at each step of the analysis 2 forcing all independent variables except one to be constant at each step of the analysis and 3 backwards stepwise constant forcing the latter procedure initiates the analysis by holding constant every variable to determine which has the least effect on model accuracy it then tests every possible pair of variables that include the one with the least effect to identify which pair of variables has the least effect on accuracy when held constant the procedure is repeated holding an extra variable constant at each step until only one variable remains eastman 2015 2 5 model validation face and empirical validations were conducted at micro and macroscales in accordance with wilensky and rand 2015 face validation involves demonstration that the model s mechanics and properties correspond to the mechanics and properties observed in the studied system as assessed through visual interpretation while the empirical validation is based on simulated and real numerical data comparisons similarly microvalidation evaluates whether the behaviors and mechanisms encoded into the farmer agents match real farmer analogs while macrovalidation checks whether the emergent simulated behaviors from the model correspond to aggregated behaviors observed at the landscape level at the microscale face validation focused on the magnitude of the optimal area of deforestation observed during simulations while the empirical validation compared the simulated annual household revenues costs and profits with their analogs obtained from the 2009 panel data caviglia harris et al 2014 only a proportion of the products harvested were sold and that proportion is not captured in equation 1 therefore the proportion of products sold θ was empirically estimated and discounted from the outcome of the profit function to render model estimates comparable to the panel data table 2 at the macroscale the model was face validated through pattern comparison of lucc simulation maps at baseline equilibrium without redd payments and the most recent lucc classification map 2010 available from time series prepared by toomey et al 2013 empirical validation was conducted with the figure of merit method pontius et al 2008 the latter is a straightforward way to assess a lucc model s prediction accuracy its calculation is based on the proportion of the area of agreements between satellite based and simulation maps over the sum of agreements and the area of disagreement between the same two maps lucc models assessed with the figure of merit method generally exhibit prediction accuracy 50 e g fuller et al 2011 kim 2010 li et al 2012 müller and mburu 2009 vieilledent et al 2013 but the proposed model faced an additional limitation to validation insofar as households had not necessarily reached their optimal lucc configurations by 2010 furthermore recent changes in the brazilian forest code soares filho et al 2014 increasing efforts to control illegal deforestation brasil 2013 2008 and the creation of conservation incentives soares filho et al 2016 might reduce the once widespread failures of compliance with forest conservation regulations in amazonian brazil fearnside 2005 these changes could imply that the total farm area available for conversion to agriculture f should be altered to capture the minimum forested area that households must preserve e g 0 8 f however these factors have arguably not yet affected the historical patterns of poor conformance with environmental regulations by small farm households nunes et al 2015 in any case the fact that the proposed model does not account for ongoing lucc in brazil is another source of uncertainty in validation 3 results simulated maps successfully mimicked lucc patterns observed on deforestation frontiers at the baseline equilibrium state the entire deforestation frontier region was cleared as redd payments become a factor at the household level lucc decision making patches of forest appear at the equilibrium state when increases in costs of agricultural and deforestation activities result in financial losses deforestation does not occur and agricultural areas are abandoned figs 2 and 6 the lucc patterns observed in the simulated landscape are similar to the fishbone patterns observed in the real study area and other deforestation frontier regions roberts et al 2002 3 1 empirical parameter estimation the parameters of the production function estimated with an ordinary least squares regression resulted in the following equation table 1 18 f a t l t 2 63 a t 0 45 l t 0 15 in which all parameters were significantly and positively correlated with the production outcome other empirically estimated parameters were based on the average or weighted average values from the panel data parameters that could not be estimated were based on values reported in the literature tables 1 and 2 figs s5 and s7 3 2 sensitivity analysis results distinct but expected simulation behaviors resulted from changes in parameter settings during the sensitivity analysis figs 3 and 4 at equilibrium parameters α β and φ from the cobb douglas production function and the sale price of agricultural outputs p a were all positively correlated with the optimal agricultural area ha this result is expected because increasing any of these variables increases the potential revenue obtained from agricultural land for a given amount of area and labor among these parameters β which determines how the agricultural area affects rice equivalent production was the most sensitive to the point that its effect on the optimal agricultural area was the only one that required expression on a logarithmic scale as expected increases in production and crop prices led to higher profits which sustains agricultural land use covers deforestation expenses and reduces the efficacy of redd payments in contrast financial and labor costs associated with maintenance of agricultural patches w and c a deforestation w c d and γ and the discount rate r were all negatively correlated with the optimal area in agriculture intuitively when labor non labor costs increase maintaining agricultural land becomes prohibitively expensive and deforestation is no longer a profitable lucc decision similarly the amount of redd payments p e i and for the carbon market scheme payment option the net carbon emission factor associated with the conversion from forest to agricultural land use or the net carbon sequestration factor from the conversion from agriculture to forest δ table 2 were also negatively correlated with the optimal agricultural area as redd payments or carbon stocks increase compared to the stocks on agricultural lands baseline the forest conservation option becomes more attractive to households lastly the redd project lifetime t under the carbon market scheme option was positively correlated with the optimal agricultural area this effect reflects the assumption that a given ton of co2 for which redd payments have been made will only become eligible for additional payments under a new carbon project lifetime based on the relationships established by the artificial neutral networks model in all three sensitivity analyses of the model used to create the deforestation suitability map distance from previous deforested areas was the most influential biophysical variable to inform the location of future lucc while the administrative areas at the municipality level was the least influential the order of importance of the other biophysical variables varied by the sensitivity analysis method employed tables s7 s10 3 3 model validation results the optimal area of deforestation simulated by the model conformed to expectations at the baseline equilibrium entire farms are converted to agricultural land most important for the model s empirical microvalidation the estimated agricultural household annual revenues average 4927 costs average 2155 and profits average 2772 from the profit function differed little from the revenues average 5390 costs average 2429 and profits average 2961 obtained from the panel data p 0 44 0 25 and 0 73 respectively fig 5 the fishbone patterns of deforestation that emerged during the simulations provide good evidence to support the model s face macrovalidation furthermore despite the limitations of adopting the figure of meri for the spatially explicit and empirical macrovalidation of the model at equilibrium the prediction accuracy score was reasonably high 59 fig 6 4 discussion the most critical component of a lucc abm is arguably the decision making processes of its agents many decision making processes in such models are based on decision trees acosta et al 2014 deadman et al 2004 salvini et al 2016 in contrast the model presented here adopted a microeconomic framework based on the assumption that perfectly market integrated households behave as profit maximizers consequently the decision making process was framed as a dynamic optimization problem like those often adopted in the environmental and natural resource economics literature e g amit 1986 barbier 1999 brown 2008 a potential criticism of the profit maximization assumption is that many theories of farm households posit them as utility maximizers rather than profit maximizers barnum and squire 1979 chayanov 1926 taylor and adelman 2003 using these theories a household s utility is often captured as a function of trade offs between consumption and leisure constrained by limited access to labor markets these characteristics are not necessarily accurate representations of farm households on the old deforestation frontier of the ouro preto do oeste region where labor markets function well as demonstrated in the microeconomic household models of deforestation developed by angelsen 1999 the utility maximization behavior can be reduced to profit maximization under perfect labor market conditions profit or utility maximization dichotomy aside we acknowledge that the optimizing behavior assumption itself does not fully explain human decision making jager et al 2000 le et al 2008 alternatives to maximization behaviors discussed in the literature include i achieving minimum levels of satisfaction or ii decision trees where agents repeat the same action until they run out of resources such as labor or capital deadman et al 2004 salvini et al 2016 furthermore an additional stochastic factor could be added to the final optimal lucc calculation as an attempt to mimic imperfect household decisions or a stochastic optimization approach based on the generation and use of random variables could be adopted e g ermolieva et al 2015 the key component of the profit function incorporated into the model lucc decision making process is the agricultural production function while the functional form of the production function can affect the behavior of the farmer agents angelsen 1999 a cobb douglas form was chosen due to its closed solution properties and widespread use in the farm household and lucc literature angelsen 1999 darwin et al 1996 despite the numerous data transformations required for the conversion of heterogeneous household production outputs to the rice equivalent the production function generated parameter values similar to those in the literature for example in the function estimated by barnum and squire 1979 for paddy rice farmers in malaysia the respective β and φ associated with the agricultural land size and labor were estimated as 0 62 and 0 29 although the overall fit of our function was somewhat low r2 0 12 compared to others in the literature based on single crop systems e g r2 0 67 reported by the previous authors the simulated annual agricultural revenue did not differ substantially from the values in the panel data reported by the households in the region caviglia harris et al 2014 additionally the global concavity of the profit function ensured that the simulation had achieved a maximum finally the sensitivity analysis of the model parameters did not reveal unexpected or contradictory outcomes the fact that our production function does not discriminate between family and hired labor combined with the assumption of perfect labor markets implies that households are treated as homogenous agents such characteristics are arguably in contrast to general lucc abms that are often based on heterogeneity and stochasticity an 2012 le et al 2008 nevertheless heterogeneity is incorporated into the model through the lucc allocation submodel as expansion of agricultural land is constrained by farm size f which varies across settlements for instance the first orthogonal i e fishbone arrangement settlements from the 1970s were 100 ha lots while the newer ones follow a radial pattern with each land holding approximately 12 ha caviglia harris and harris 2011 the lucc allocation submodel generated the fishbone deforestation patterns in the simulations since redd payments caused shifts in land use cover equilibria at the landscape level based on sensitivity analyses of the artificial neural networks model used to create the deforestation suitability map distance from previously deforested areas was the most influential biophysical variable explaining the location of observed deforestation between 1986 and 1996 followed by distance from protected areas and elevation these results are in agreement with a recent meta analysis about the drivers of deforestation conducted by ferretti gallon and busch 2014 and other similar studies müller et al 2012 pfaff 1999 soares filho et al 2010 the figure of merit of this model was high compared to others in the literature and in comparison to those for decentralized private redd projects for instance pontius et al 2008 found that close to one third of the lucc models they tested produced figures of merit 10 with only one 50 similarly an assessment of deforestation models conducted by kim 2010 found no figure of merit 8 for a highly deforested area in santa cruz bolivia the median figure of merit from 41 model validation runs conducted by fuller et al 2011 for the peat swamp forests of central kalimantan indonesia was 17 while li et al 2012 reported a 27 figure of merit for simulated scenarios of biofuel crops expansion in the great plains states of the united states müller and mburu 2009 found figures of merit ranging 37 41 after substantially large training cycles to forecast lucc in the kakamega forest of western kenya while the deforestation simulations for madagascar carried out by vieilledent et al 2013 achieved values of 10 23 however the spatially explicit validation of simredd benefited from the regional context of the study in one of the most degraded old deforestation frontiers in the brazilian amazon toomey et al 2013 so much forest clearing has occurred in the study region that the 2010 lucc classification map could be empirically validated against the lucc model output at the baseline equilibrium state the implications of starting the model with the 1996 lucc configurations are somewhat important there were three main reasons for the selection of 1996 as the first year of the model first it precedes the year when the kyoto protocol the first international climate mitigation agreement was signed at the 7th conference of parties to the united nations framework convention on climate change unfccc 1998 and discussions of whether or not to include mitigation activities based on avoided deforestation were still underway tolba and rummel bulska 1998 second it matches the first year of the panel data collection in the settlements of the ouro preto do oeste region caviglia harris et al 2014 third it allows the model to begin with a large area of mature forest in the virtual landscape which gives the model flexibility to predict deforestation and or reforestation fig s1 abandonment of agricultural patches occurs in the equilibrium state if a household has previously established more agricultural land than the optimal level dictated in the scenario settings the abandonment decision can result from two distinct implications of the model settings households abandon farmed land when 1 revenues are lower than costs and agricultural production is not lucrative or 2 when redd payments exceed agricultural profits both settings imply that farmer agents set aside part of their agricultural land so that their new mixture of agricultural land and forest area maximizes profit simulations from the model present a redd payment framework similar to the current 2017 situation the redd policy payment scheme is based on annual payments for forest area maintained or recovered which is similar to pes program in countries like costa rica ecuador and mexico arriagada et al 2012 de koning et al 2011 honey rosés et al 2009 torres et al 2013 in contrast the carbon market option assumes payments based on net carbon emissions avoided mg co2 this option in accordance with carbon accounting methodologies proposed by the intergovernmental panel on climate change ipcc 2006 2003 the united nations framework convention on climate change unfccc 2013a 2013b 2007 and the standards and methodologies for decentralized private redd initiatives developed for the voluntary carbon market e g avoided deforestation partners 2012 pedroni 2012 verified carbon standard 2017b numerous ongoing examples of redd projects that follow the latter payment option are also described in the literature e g west 2016 settings for three key model parameters determine whether households are better off with the former or the latter redd payment option 1 the net carbon emissions factor δ 2 the length of the hypothetical redd project t and 3 the size of redd payments p e i for instance the monarch butterfly conservation fund in mexico offered annual conservation payments of 12 ha 1 of forest honey rosés et al 2009 while the average carbon offset price in the voluntary carbon market was 3 30 mg co2 1 in recent reports with a range between minimum and maximum prices of 21 60 co2 1 hamrick and goldstein 2016 the length of the decentralized private redd projects is often in agreement with the 20 100 year range set by the leading voluntary carbon standard the verified carbon standard goldstein and neyland 2015 verified carbon standard 2017b lastly aboveground biomass stocks which are used as references for estimating avoided net carbon emission vary substantially across tropical forests with averages from 40 to 310 mg ha 1 baccini et al 2012 ipcc 2006 the distinct values and ranges presented above illustrate the variety of potential scenarios that can lead to substantially different lucc and welfare outcomes given the pes scheme adopted hence the proposed model with its two payment options can provide meaningful insights into the design and potential outcomes of redd initiatives 5 conclusion a hybrid optimization abm model was proposed to investigate the effects of redd payments on lucc and community welfare in settlements on an old deforestation frontier in brazil the model built into an abm platform is based on two distinct submodels one for the dynamic lucc optimization decision and the other for identification of areas suitable for land use cover change and persistence within the households lots in the study region the optimization submodel which assumes households behave as profit maximizers under perfect labor markets was calibrated empirically with unbalanced socioeconomic panel data the key component of the profit function maximized by the farmer agents in the model is the cobb douglas production function based on a standard crop output the rice equivalent and function of agricultural area and labor while the dynamic optimization process returns deterministic and homogenous results for the farmer agents differences in household landholdings and initial land use cover configurations incorporate heterogeneity into the model the outcome of the model is the optimal land use cover configuration for households at the equilibrium state taking into account the revenues from redd payments based on the conservation of forest areas and promotion of natural regeneration this submodel allows for the quantification of changes in forest cover and agricultural production in response to different redd payment options as well as a means to calculate the changes in welfare at the community level which is tightly related to equity issues associated with pes the lucc allocation submodel was constructed with an artificial neural networks based algorithm and spatially explicit biophysical variables correlated with lucc observed between 1986 and 1996 the latter submodel is responsible for the emergence of the classic fishbone patterns of deforestation observed in the output of the simulations when redd payments occur results from this submodel can estimate impacts of redd payments on habitat fragmentation and biological conservation at the landscape level the submodel can also serve for the identification and ranking of forest areas more threatened by future deforestation a key feature of simredd is its ability to simulate two distinct and widely adopted redd payment options one based on forest and forest regrowth areas ha 1 in accordance with national redd like conservation programs and the other based on avoided net carbon emissions and net carbon sequestration mg co2 1 in conformance with voluntary carbon market rules as expected without redd payments and with the assumption of continued widespread non compliance with environmental regulations the land use cover at equilibrium state is complete conversion of forests to agricultural land when redd payments are incorporated into the simulation forest patches appear in the virtual landscape at model equilibria similar results are observed as agricultural and deforestation costs increase or as agricultural revenues decrease these results indicate the mechanics behind the model are in conformance to what we would expect to observe in reality overall the model can serve as a virtual impact evaluation tool to explore the effects of pes interventions intended to impede deforestation impacts can be measured in terms of lucc greenhouse gas emissions program enrollment and costs food production and community welfare hence simredd can shed light on a wide range of topics e g natural resource economics and governance rural development food security landscape ecology habitat fragmentation climate change mitigation and equity studies insights from simulation scenarios can improve the design of more effective efficient and equitable redd programs and initiatives that involve farm household participation acknowledgements this research was funded by the national science foundation grant ses 0752936 additional funds to the first author were provided by the brazilian national counsel of technological and scientific development cnpq grant 201138 2012 3 the william c and bertha m cornett fellowship and the tropical conservation and development graduate assistantship at the university of florida and the world wildlife fund s prince bernhard scholarship for nature conservation appendix a analytical solution for the optimal agricultural land definition the step by step analytical solution involves solving for λ t in equation 8 19 λ t w γ c d ψ λ t 0 since λ t and λ t are known l t is given recursively by equation 9 as function of a t 20 p a β α a t β 1 l t φ p e δ c a ψ r w γ c d ψ 21 l t a t 1 β r c d p e δ c a r ψ r w γ ψ p a β α 1 φ once l t is defined the expression for the optimal agricultural land at equilibrium a is given recursively by equation 7 22 p a θ φ α a t β a t 1 β r c d p e δ c a r ψ r w γ ψ p a β α 1 φ φ 1 w 23 a t w β 1 1 φ α 1 φ p a 1 φ r c d p e δ c a r ψ γ r ψ ψ 1 φ 1 φ φ β φ 1 appendix b supplementary data the following is the supplementary data related to this article online data online data appendix b supplementary data supplementary data related to this article can be found at https doi org 10 1016 j envsoft 2017 11 007 
26456,redd was initially conceived of as a multi level carbon based payment for environmental services pes it is still often assumed to be a cost effective climate change mitigation strategy but this assumption is mostly based on theoretical studies and static opportunity cost calculations we used spatial and socioeconomic datasets from an amazonian deforestation frontier in brazil to construct a simulation model of redd payments to households that can be used to assess redd interventions our simredd model consists of dynamic optimization and land use cover change allocation submodels built into an agent based model platform the model assumes that households maximize profit under perfect market conditions and calculates the optimal household land use cover configuration at equilibrium under a given redd pes scenario these scenarios include pes based on 1 forest area and 2 carbon stocks insights gained from simulations under different conditions can assist in the design of more effective efficient and equitable redd programs keywords redd payments for environmental services land use cover change agent based modeling farm household optimal control software and or data availability the model described simredd was built in netlogo v 5 3 1 software for agent based modeling wilensky 1999 the first and current version of the model was finalized in october 2016 netlogo and therefore simredd should work on any platform on which java 5 or later is installed computers with 8 gb of ram might run out of memory due to the substantial number of virtual agents simulated in the model simredd script is freely available from the corresponding author after signature of a data use agreement between the requester the corresponding author and the principal investigators responsible for acquisition of the sensitive household level socioeconomic data 1 introduction spatially explicit biophysical and anthropogenic factors correlated with tropical deforestation are well discussed in the literature e g geist and lambin 2002 müller et al 2012 pfaff 1999 and are often key components of land use cover change lucc simulation models eastman 2015 soares filho et al 2002 examples of these factors are land slopes and distances from roads markets and protected areas the observation that the relationships among such factors shed little light on the decision making processes that undergird lucc deadman et al 2004 le et al 2008 parker et al 2003 motivates attempts to elucidate the causal effects of policy interventions on deforestation e g blackman 2013 börner et al 2016 one way to remedy this deficiency is through use of agent based models abm which offer a powerful way to explore coupled human natural complex systems wilensky and rand 2015 abms can be used for example to inform the design of policies or programs to curb deforestation while minimizing negative impacts on local stakeholders iwamura et al 2016 purnomo et al 2013 snilstveit et al 2016 in this paper we describe a hybrid optimization abm that simulates the effects of payments for environmental services pes to settlers on an old amazonian deforestation frontier on lucc and community welfare payments were structured in accordance with redd reducing emissions from deforestation and forest degradation with recognition of the roles of conservation sustainable forest management and enhancement of carbon stocks un redd 2008 2011 practices the model presented here was designed to shed light on issues related to the long run effectiveness efficiency and equity of redd interventions an abm is a complexity theory based computerized simulation snooks 2008 of a number of decision makers agents who interact through prescribed rules in a dynamic environment an 2012 liu et al 2007 the independence of agents represented in an abm and their interactions allow researchers to capture characteristics over time of complex systems like heterogeneity nonlinearity feedbacks and emergent properties an 2012 an et al 2005 liu et al 2007 abms have the potential to assess impacts of policy scenarios on coupled human natural systems as they attempt to mimic the behavior of institutions and stakeholders in a virtual representation of reality e g andersen et al 2014 iwamura et al 2014 purnomo et al 2013 thus development and application of abms represent a reasonable early step in policy making farmer and foley 2009 which is substantially less costly than field based randomized control trials and can provide guidance sooner than quasi experimental evaluations of pilot initiatives sills et al 2017 lucc models based on abm frameworks are often coupled with biophysical information about the landscape that can be used for spatially explicit model validation deadman et al 2004 ngo and see 2012 pontius et al 2011 the spatial component of lucc abm allows for better virtual representation of real landscapes and consideration of both natural and socioeconomic contexts an et al 2005 deadman et al 2004 mena et al 2011 arguably the most important component of a lucc abm is the decision making processes of its virtual agents e g farm households in general those processes are based on decision trees e g deadman et al 2004 salvini et al 2016 or the maximization of utility functions e g andersen et al 2014 monticino et al 2007 maximization problems often include labor income and physical constraints e g evans et al 2001 and are grounded in farm household economics and agricultural production theories angelsen 1999 darwin et al 1996 redd was initially conceived as a system of pes for carbon retention or sequestration un redd 2011 it has garnered substantial international attention as a potentially cost effective approach to climate change mitigation kindermann et al 2008 negotiations at the 21st conference of parties of the united nations framework convention on climate change acknowledged redd as a strategy to be incorporated into future international climate regulations unfccc 2015 yet it is widely accepted that redd will only be effective if payments offset stakeholders opportunity costs which can vary substantially abram et al 2016 borrego and skutsch 2014 torres et al 2013 a recent meta analysis by phan et al 2014 reported opportunity costs for redd range from us 0 05 to 92 mg co2 1 the average cost in latin america was approximately 5 4 mg co2 1 within the 0 2 to 13 mg co2 1 range reported by börner and wunder 2008 for the brazilian amazon ickowitz et al 2017 reported substantially higher opportunity costs to smallholders over a 30 year time horizon with a 9 discount rate ranging from 11 to 77 mg co2 1 across five sites in the brazilian amazon in contrast nepstad et al 2007 use a much lower opportunity estimate of 1 5 mg co2 1 across the entire region multiple reasons explain why opportunity costs range so widely across the tropics but the primary one stems from variation in possible land uses not all tropical forests are located in plantations due to logistical e g inaccessible regions or environmental reasons e g soil quality precipitation regimes and presence of phytopathogens in addition the lack of high quality spatial and economic data to estimate opportunity costs often forces researchers to extrapolate them from one region to others with very different conditions land use options and opportunity costs e g börner et al 2014 nepstad et al 2007 which could result in inaccurate estimates furthermore when opportunity cost estimates are static i e they are not allowed to change with changes in economic conditions over time pana and gheyssens 2016 takasaki 2012 interpretation of results must be made with extra caution given that market conditions can change substantially over time more dynamic analyses are warranted abms can shed light on the extent to which different redd payments are likely to change business as usual behavior through time by simulating lucc decision making by farm households which avoids the potential pitfalls of static opportunity cost analyses andersen et al 2014 purnomo et al 2013 salvini et al 2016 here we describe a hybrid optimization abm e g andersen et al 2014 we developed to evaluate the potential impacts of redd payments on settlers on an amazonian deforestation frontier impacts are evaluated in terms of land use cover change co2 emissions total program expenditures on pes given program enrollment rates and community welfare even though our model was constructed on an abm platform we consider it a hybrid model because agents lucc decisions are strictly based on the solution of the dynamic optimization problem in which each household seeks to maximize farm profits 2 methods 2 1 study region the study region covers approximately 6000 km2 in rondônia state brazil including the municipality of ouro preto do oeste and its five contiguous neighbors vale do paraíso nova união teixeirópolis urupá and mirante da serra all of which are near the major federal highway br 364 fig 1 the area lies within the arc of deforestation in the brazilian amazon caviglia harris 2005 a well documented heavily deforested old frontier where settlers began arriving in the 1960s in response mostly to government sponsored programs these include the national integration program the northwest region integrated development program and operation amazonia caviglia harris 2004 sills and caviglia harris 2008 in the study region the federal land reform agency incra allocated nearly 9000 lots laid out in a regular pattern along roads to settlers arriving from other regions deforestation in the study region has therefore followed a road related fishbone pattern and has been the subject of numerous lucc studies since the early 1990s e g dale et al 1994 2 2 datasets 2 2 1 socioeconomic data socioeconomic household survey data from a systematic random sample stratified by municipality collected from 697 households in 2009 were used to parameterize the model caviglia harris et al 2014 the majority of households in the region virtually all of which immigrated from outside the amazon are small scale producers 98 with an average property size of 65 ha range 2 240 ha while the average for medium and large lots top 2 is 745 ha range 240 3000 ha households initially focused on annual and perennial crops but revenues from these crops declined by almost 70 between 1996 and 2009 and dairy and beef cattle production largely replaced that income milk production increased from an average of approximately 17 000 l per year in 1996 to approximately 30 000 l in 2009 overall annual income per household increased from 4000 to more than 9000 inflation adjusted for the same period and the average number of cattle owned per household increased from approximately 70 to 140 head the settlements experienced high rates of deforestation between 1996 and 2009 with mature forest cover decreasing by more than 50 overall only about 10 of forest cover remained in the study region in 2009 caviglia harris and harris 2011 caviglia harris et al 2015 2 2 2 remote sensing and gis data the hybrid model incorporates a spatially explicit lucc submodel based on five spatial layers 1 the first is a map displaying the location of 8900 settlement properties caviglia harris et al 2015 fig 1 2 the second is a landsat 5 satellite based lucc classification map generated in 1996 the year the first survey was completed showing mature forest secondary forest i e forest that regenerated naturally after abandonment of agricultural land and agricultural use classes fig s1 roberts et al 2002 toomey et al 2013 3 the third map created using the artificial neural network based algorithm shows the ranked suitability i e risk for deforestation of mature forest patches the deforestation suitability map was constructed with the multi layer perceptron mlp an artificial neural network based algorithm available in terrset v 18 11 software eastman 2015 it indicates the forest patches within the study region that are more likely to be cleared in the future fig s2 the map was based on the mapped land use cover changes and persistence between 1986 and 1996 and 11 biophysical maps of the study region the maps included in the mlp were administrative areas at the municipality level macro political regions soil quality elevation timber market zones distances from previously deforested areas distance from major roads distance from major rivers distance from protected areas distance from logging areas and distance from major markets table s1 and fig s3 the calibration accuracy of the mlp was 71 table s2 4 the fourth is a map of ranked suitability of secondary forest clearing based on euclidian distances to agricultural land in 1996 5 the final map provides the ranked suitability of agricultural abandonment based on euclidian distances to forest patches in 1996 additionally the tropical carbon density map developed by baccini et al 2012 was used to estimate per hectare averages of net carbon emissions from lucc figs s4 and s5 2 3 model description the lucc model named simredd is described in accordance to the overview design concepts and details odd protocol a standard guideline for the description of abms grimm et al 2010 simredd was built in netlogo v 5 3 1 wilensky 1999 general data analyses were conducted with r v 3 3 0 statistical software r core team 2016 algebraic transformations were performed in mathematica v 9 0 wolfram research inc 2012 spatial analyses were conducted with r terrset v 18 11 eastman 2015 and arcgis v 10 3 1 esri 2015 software 2 3 1 purpose the model aims to simulate the effects of direct redd payments conditional on forest cover including both retention of mature forest and regeneration of secondary forest the objective of the model is to assist in the design of more effective efficient and equitable redd programs and policies in the long run the first step in the model is to create a baseline lucc scenario at equilibrium without redd payments the second step introduces redd payments as an additional source of annual income for households payments are proportional to the sum of the area in mature forest and the area set aside for secondary forest regrowth on each property given a pes scenario households choose a new optimal land use cover configuration based on estimates of carbon stocks associated with each land use cover class the model quantifies net carbon emissions mg co2 from the lucc activities two types of pes for forest conservation are often discussed in the literature 1 payments based on forest carbon stocks and 2 payments based on forest area the first scheme is associated to carbon market based initiatives e g hamrick and goldstein 2016 while the second approach is often taken by government conservation programs e g arriagada et al 2012 de koning et al 2011 simredd can simulate these two pes schemes redd payments under the carbon market option are applied in accordance with decentralized private pes for avoided deforestation and voluntary carbon market rules that are expected to be followed by national redd programs verified carbon standard 2017a redd payments based on area ha 1 are referred to as the policy option in simredd and are assumed to be annual constant and based on the current area of mature forest cover plus agricultural areas set aside for secondary forest regrowth on each property in practical terms the implementation of pes policy option is easier when compared to the carbon market option described above as it does not require estimation of forest carbon stocks under the carbon market option redd payments are based on the differences in carbon emissions under the baseline and project scenarios mg co2 1 this option also assumes that redd payments for a given ton of co2 either removed or not emitted to the atmosphere i e additional co2 are made only once during a hypothetical carbon project lifetime in accordance with carbon market standards verified carbon standard 2017b the project lifetime considered in our simulations is 20 years constant renewal of the hypothetical carbon project is assumed after expiration because the model is projected into infinity as result a given ton of additional co2 is rewarded multiple times but only once every 20 years therefore a direct implication of changing the hypothetical carbon project lifetime is that if length 20 years a given ton of additional co2 is rewarded more frequently in the long term and hence redd revenues for the household increases the opposite effect is observed if the length of the project is 20 years it is our understanding that these two schemes encompass the majority of currently implemented pes interventions focused on forest conservation and restoration in accordance with the framework of the united nations collaborative program on redd un redd 2015 the model considers but makes no distinctions between payments made for avoided deforestation and promotion of natural regeneration this simplification seems justified because secondary forests present at the beginning of model simulations in 1996 and newly formed forest patches created through the abandonment of agricultural land are both assumed to reach the status of mature forests when projected into infinity when no lucc occurs at model equilibrium see section 2 3 6 and the supplementary material for details the model reports the total annual amount of money spent by the simulated redd scheme on pes at equilibrium the model assumes that enrollment is voluntary and unconstrained and that the amount of land households allocate to redd if any depends on the extent to which the offered pes offset the households opportunity costs the model does not incorporate the total cost of the redd interventions as it does not capture transaction or administrative costs luttrell et al 2017 thompson et al 2013 2 3 2 entities state variables and scales there are two types of entities in the model households or farmer agents and land use cover patches 28 5 m resolution the approximately 6000 km2 of settlements were divided into ten sub regions to cope with the computational limitations of netlogo railsback et al 2006 fig s6 table s3 agricultural patches represent different and unspecified mixes of annual and perennial crops and pasture three variables describe each patch grid cell in the model 1 a land use cover category mature forest secondary forest or agriculture 2 a patch owner identification number and 3 a value related to its suitability of change to a different land use cover class either the risk of deforestation for forest patches or of abandonment for agricultural patches aboveground carbon stocks were estimated in mg co2 ha 1 the standard unit of carbon emission offsets rifai et al 2015 unfccc 2013a based on the weight ratio between co2 and c 44 12 a biomass carbon fraction of 0 47 and below to aboveground biomass ratios of 0 24 and 0 20 for mature and secondary forest respectively ipcc 2006 mokany et al 2006 the averages of aboveground carbon stocks associated with the mature and secondary forest land cover classes were estimated at 537 and 418 mg co2 ha 1 respectively baccini et al 2012 fig s4 for agricultural patches a total carbon stock at equilibrium of 104 mg co2 ha 1 was adopted fearnside 1996 farmer agents are assigned a patch owner identification number that indicates which patches they own and the size of their farms farms are not allowed to change sizes over the course of the simulation i e household members do not split or acquire lots 2 3 3 process overview and scheduling the model represents two processes related to lucc in the settlements the first is quantification of the optimal agricultural area for each farm the second is the spatial allocation of deforestation of forest patches and abandonment of agricultural patches shifts in the optimal agricultural area result from changes in the parameter values displayed on the model interface market price of agricultural outputs redd payments for avoided deforestation and promotion of natural regeneration and wage rates simulations end when the lucc equilibrium state is reached as defined by the individual farmer agents optimal agricultural area under each payment scenario 2 3 4 design concepts basic principles the model is based on the theory of farm households operating in complete markets darwin et al 1996 in which households are assumed to behave as if they maximize profits angelsen 1999 takasaki 2013 these farm households are the farmer agents in the model a cobb douglas production function is embedded into the farmer agent s profit function the literature on farm household production is generally focused on one crop to avoid problems often encountered in multicrop production models barnum and squire 1979 in light of this limitation and to make the lucc model as generalizable as possible the rice equivalent andersen et al 2014 was adopted as a single standard agricultural output annual and perennial crops as well as animal production and their respective prices were converted into their rice equivalent based on energy content and annual productivity tables s4 and s5 fig s7 emergence lucc patterns emerge based on the optimal agricultural area calculation and the areas more suitable for land use cover change and persistence due to the fact that transitions of one patch affect the suitability for transitions of neighboring patches adaptation learning agents do not present adaptive traits i e they do not change behavior over time in response to learning objective the farmer agent s objective is to maximize a profit function through a dynamic optimization framework a cobb douglas production function e g barnum and squire 1979 determines the volume of rice equivalents produced as a function of the household s agricultural area and labor predicted production combined with agricultural output prices determine the households revenue the model assumes that a given agricultural patch maintains the same productivity into infinity which is an average of the production potential of the agricultural land obtained from the panel data through an ordinary least squares regression maintenance costs of agricultural patches deforestation costs and labor costs are subtracted from household revenue in cases that incorporate redd payments the latter are added to household profit prediction sensing households are treated as homogenous agents they are aware of the patches within their farms boundaries the current land use cover class of those patches and the suitability of mature and secondary forest patches for deforestation or of agricultural patches for abandonment through the dynamic optimization exercise the farmer agents become aware of the optimal size of agricultural area and change their land use cover patches to reach that level interaction collectives land use cover patches belong to a farm and a farmer agent farmer agents interact exclusively with their own land use cover patches with no interactions among households nonetheless neighboring patches influence the suitability of lucc of areas that belong to different farmer agents stochasticity no stochastic processes are incorporated in the model the initialization conditions are based on a lucc classification map of 1996 while the dynamic optimization process is based on an internal analytical solution both procedures are deterministic observations changes in areas of forest and agricultural land at the sub region level in the simulation are monitored and compared to the areas in lucc classification maps derived from landsat satellite imagery annual household profits at the equilibrium state of the model run are also monitored and compared to the 2009 panel data survey 2 3 5 initialization the model is run for one of its ten sub regions at a time with one farmer agent created for each farm land use cover patches are created following a landsat 5 based lucc classification map from 1996 roberts et al 2002 toomey et al 2013 deforestation and abandonment suitability maps are loaded into netlogo and their values are assigned to forest and agricultural patches respectively similarly land use cover patches are assigned to their respective farmer agent owners the farmer agents count the patches in each land use cover class within their farms and store those values for use during the model run netlogo then displays the 1996 land use cover map on which it overlays property boundary polygons fig 2 finally initial sizes ha of mature forests secondary forests and agricultural lands at the landscape level are reported and community welfare is calculated as the sum of all households annual farm profit 2 3 6 submodels profit maximization submodel the household decision making process is assumed to follow a dynamic maximization framework such a framework is required because any additional deforestation expands the household agricultural area households maximize a profit function under perfect labor market conditions angelsen 1999 takasaki 2013 1 π p a f a t l t p e i f a t w l t γ d t c a a t c d d t where p a is the price of the agricultural production output f a t l t is the production function that predicts output produced as a function of agricultural area a t and agricultural labor l t at time t w is the wage rate associated with agricultural labor and the deforestation and site preparation labor γ per unit of deforested area d t c a and c d are non labor costs associated with maintaining agricultural areas a t and deforestation and site preparation costs required per hectare of deforested area d t respectively the profit function also incorporates the financial benefit from an environmental service payment represented by p e i f a t where p e i is the redd payment unit 1 for payment type i discussed further below based on the total farm area f not converted for agricultural land use a t under the assumption that all farms were once forested to allow an analytical solution a cobb douglas production function was chosen for its closed form solution property e g angelsen 1999 bronfenbrenner and douglas 1939 2 f a t l t α a t β l t φ where β and φ are the land area and labor output elasticities respectively and α is the total factor of productivity these three parameters were estimated from the socio economic panel data based on the agricultural area and family size of the household number of family members with off farm jobs and number of people hired to work on the farm throughout a year given that production functions are likely to vary with property size production function parameters were estimated based on data from small farms with 28 large lots excluded from the simulations among the excluded farms are shared legal reserves that belong to the settlements areas that must remain forested to comply with brazilian environmental regulations the unit of p e i varies with the redd payments scheme i chosen with the redd policy option payments p e ρ are based on the remaining mature forest area and the area allocated for secondary forest regrowth in the farm ha 1 with the carbon market option payments p e μ are based on the net carbon emissions from avoided conversion of forest to agricultural land and net carbon sequestration from regeneration of abandoned agricultural patches mg co2 1 captured by an additional term δ rifai et al 2015 because payments for a given mg co2 can only occur once over a carbon project lifetime the average annual redd payment under the carbon market scheme becomes 3 p e μ p e ρ δ t where t represents a hypothetical redd project lifetime implemented in accordance with voluntary carbon market standards verified carbon standard 2017b redd payments are annualized by t which is the time assumed for agricultural patches set aside for forest regrowth to achieve the carbon stock status of mature forest t years is also the ex ante time assumed for all forest patches within a given farm to be cleared in the absence of redd payments see figs s8 and s9 for details given that a t a t 1 d t the deforestation process is captured as 4 a t t a t d t additionally the amount of deforestation cannot exceed the amount of remaining forest yielding an additional constraint 5 f a t d t 0 the optimal solution can be solved with the construction of the following hamiltonian equation michel 1982 6 h p a α a t β l φ a φ p e i f t a t w l t γ d t c a a t c d d t λ t d t ψ f a t d t where λ t is the co state variable associated with the expansion of agricultural land and ψ is the shadow value for the maximum deforestation constraint i e households cannot clear more land than they possess the first order conditions of the hamiltonian equation then become 7 h l t p a φ α a t β l t φ 1 w 0 8 h d t w γ c d λ t ψ 0 9 h a t p a β α a t β 1 l t φ p e δ c a ψ λ t r λ t 10 h λ t d t a t 11 h ψ f a t d t 0 ψ 0 f a t d t 0 ψ 0 the optimal agricultural land at equilibrium a is given recursively by solving the optimal control problem appendix a 12 a t w β 1 1 φ α 1 φ p a 1 φ r c d p e δ c a r ψ γ r ψ ψ 1 φ 1 φ φ β φ 1 in all of the above equations ψ 0 when there is forest left i e the land restriction is not binding finally the deforestation path d t is defined by 13 d t 0 w γ c d ψ λ t d w γ c d ψ λ t d w γ c d ψ λ t if a t a the household deforests at the maximum possible rate d if a t a the household abandons agricultural land leading to natural regeneration of forest until the optimal amount of agricultural land is reached once the optimal area in agricultural use is reached deforestation d t equals zero and the amount of agricultural land remains at a while this model assumes parameters to remain constant over time the model can easily calculate a new equilibrium land area and the associated implied deforestation or forest regeneration if conditions were to change after initial equilibrium is reached optimal agricultural labor at equilibrium l is then calculated recursively from the profit function equations 1 and 2 given the optimal agricultural area a as 14 π p a α a β l t φ p e i f a w l t γ d t c a a c d d t 15 π l t p a α a β l t φ l t w p a φ α a β l t φ 1 w 0 16 l w p a φ α a β 1 φ 1 finally the optimization submodel calculates community welfare based on the sum of all n individual annual household i profits for each simulation scenario j at the lucc equilibrium state π e 17 π e i j i 1 n p a α a t i j β l t i j φ p e f a t i j w l t i j c a a t i j total annual redd expenditures on pes are also calculated for a given household enrollment rate in the program because welfare is calculated at the household level equation 17 can easily be used to compare the impacts of redd payments in terms of equity across household socioeconomic groups e g richest versus poorest lucc allocation submodel once the optimal agricultural land area is defined farmer agents decide what lucc needs to take place to adjust land use on their farms to that value when deforestation is required farmer agents first clear secondary forest patches with preference to patches with higher deforestation suitability closer to previously established agricultural areas if all secondary forest patches are cleared and the optimal amount of land in agricultural use is still not yet reached farmer agents convert mature forest patches also giving preference to patches with higher deforestation suitability until the optimal land use level is reached or until the entire farm is under agricultural use when the optimal amount of agriculture land is less than the amount previously used for agriculture in the farm farmer agents abandon agricultural patches affording priority to patches with higher abandonment suitability closer to forested areas until the optimal level is reached finally the size of each land use cover class and the net carbon balance from the lucc processes co2 emissions minus sequestration are reported at the equilibrium state of the model 2 4 model verification and sensitivity analysis model verification was conducted for debugging purposes wilensky and rand 2015 local sensitivity analysis was conducted to examine which parameters most affect simulation results railsback and grimm 2012 sensitivity analysis of this model was particularly important because parameter values were either empirically estimated with the 2009 panel data and assumed to be constant through the simulations or obtained from studies based on different regions of the amazon basin tables 1 and 2 sensitivity was analyzed based on the relative change of the following model parameters and their effects on the optimal level of agricultural land at the equilibria cobb douglas production function parameters price of the agricultural output agricultural costs net carbon emission factor redd payments discount rate and hypothetical carbon project lifetime see equations 1 3 for each step of the sensitivity analyses all independent variables were held constant except the variable of interest which assumed original values decreased and increased by 100 from base value by 10 intervals the analysis was conducted under both the carbon market based and the redd policy payment based option schemes additionally a sensitivity analysis of the artificial neural network based model used to create the deforestation suitability map for the mature forest was conducted with terrset v 18 11 software the latter analysis was based on three methods 1 forcing a single independent variable to be constant at each step of the analysis 2 forcing all independent variables except one to be constant at each step of the analysis and 3 backwards stepwise constant forcing the latter procedure initiates the analysis by holding constant every variable to determine which has the least effect on model accuracy it then tests every possible pair of variables that include the one with the least effect to identify which pair of variables has the least effect on accuracy when held constant the procedure is repeated holding an extra variable constant at each step until only one variable remains eastman 2015 2 5 model validation face and empirical validations were conducted at micro and macroscales in accordance with wilensky and rand 2015 face validation involves demonstration that the model s mechanics and properties correspond to the mechanics and properties observed in the studied system as assessed through visual interpretation while the empirical validation is based on simulated and real numerical data comparisons similarly microvalidation evaluates whether the behaviors and mechanisms encoded into the farmer agents match real farmer analogs while macrovalidation checks whether the emergent simulated behaviors from the model correspond to aggregated behaviors observed at the landscape level at the microscale face validation focused on the magnitude of the optimal area of deforestation observed during simulations while the empirical validation compared the simulated annual household revenues costs and profits with their analogs obtained from the 2009 panel data caviglia harris et al 2014 only a proportion of the products harvested were sold and that proportion is not captured in equation 1 therefore the proportion of products sold θ was empirically estimated and discounted from the outcome of the profit function to render model estimates comparable to the panel data table 2 at the macroscale the model was face validated through pattern comparison of lucc simulation maps at baseline equilibrium without redd payments and the most recent lucc classification map 2010 available from time series prepared by toomey et al 2013 empirical validation was conducted with the figure of merit method pontius et al 2008 the latter is a straightforward way to assess a lucc model s prediction accuracy its calculation is based on the proportion of the area of agreements between satellite based and simulation maps over the sum of agreements and the area of disagreement between the same two maps lucc models assessed with the figure of merit method generally exhibit prediction accuracy 50 e g fuller et al 2011 kim 2010 li et al 2012 müller and mburu 2009 vieilledent et al 2013 but the proposed model faced an additional limitation to validation insofar as households had not necessarily reached their optimal lucc configurations by 2010 furthermore recent changes in the brazilian forest code soares filho et al 2014 increasing efforts to control illegal deforestation brasil 2013 2008 and the creation of conservation incentives soares filho et al 2016 might reduce the once widespread failures of compliance with forest conservation regulations in amazonian brazil fearnside 2005 these changes could imply that the total farm area available for conversion to agriculture f should be altered to capture the minimum forested area that households must preserve e g 0 8 f however these factors have arguably not yet affected the historical patterns of poor conformance with environmental regulations by small farm households nunes et al 2015 in any case the fact that the proposed model does not account for ongoing lucc in brazil is another source of uncertainty in validation 3 results simulated maps successfully mimicked lucc patterns observed on deforestation frontiers at the baseline equilibrium state the entire deforestation frontier region was cleared as redd payments become a factor at the household level lucc decision making patches of forest appear at the equilibrium state when increases in costs of agricultural and deforestation activities result in financial losses deforestation does not occur and agricultural areas are abandoned figs 2 and 6 the lucc patterns observed in the simulated landscape are similar to the fishbone patterns observed in the real study area and other deforestation frontier regions roberts et al 2002 3 1 empirical parameter estimation the parameters of the production function estimated with an ordinary least squares regression resulted in the following equation table 1 18 f a t l t 2 63 a t 0 45 l t 0 15 in which all parameters were significantly and positively correlated with the production outcome other empirically estimated parameters were based on the average or weighted average values from the panel data parameters that could not be estimated were based on values reported in the literature tables 1 and 2 figs s5 and s7 3 2 sensitivity analysis results distinct but expected simulation behaviors resulted from changes in parameter settings during the sensitivity analysis figs 3 and 4 at equilibrium parameters α β and φ from the cobb douglas production function and the sale price of agricultural outputs p a were all positively correlated with the optimal agricultural area ha this result is expected because increasing any of these variables increases the potential revenue obtained from agricultural land for a given amount of area and labor among these parameters β which determines how the agricultural area affects rice equivalent production was the most sensitive to the point that its effect on the optimal agricultural area was the only one that required expression on a logarithmic scale as expected increases in production and crop prices led to higher profits which sustains agricultural land use covers deforestation expenses and reduces the efficacy of redd payments in contrast financial and labor costs associated with maintenance of agricultural patches w and c a deforestation w c d and γ and the discount rate r were all negatively correlated with the optimal area in agriculture intuitively when labor non labor costs increase maintaining agricultural land becomes prohibitively expensive and deforestation is no longer a profitable lucc decision similarly the amount of redd payments p e i and for the carbon market scheme payment option the net carbon emission factor associated with the conversion from forest to agricultural land use or the net carbon sequestration factor from the conversion from agriculture to forest δ table 2 were also negatively correlated with the optimal agricultural area as redd payments or carbon stocks increase compared to the stocks on agricultural lands baseline the forest conservation option becomes more attractive to households lastly the redd project lifetime t under the carbon market scheme option was positively correlated with the optimal agricultural area this effect reflects the assumption that a given ton of co2 for which redd payments have been made will only become eligible for additional payments under a new carbon project lifetime based on the relationships established by the artificial neutral networks model in all three sensitivity analyses of the model used to create the deforestation suitability map distance from previous deforested areas was the most influential biophysical variable to inform the location of future lucc while the administrative areas at the municipality level was the least influential the order of importance of the other biophysical variables varied by the sensitivity analysis method employed tables s7 s10 3 3 model validation results the optimal area of deforestation simulated by the model conformed to expectations at the baseline equilibrium entire farms are converted to agricultural land most important for the model s empirical microvalidation the estimated agricultural household annual revenues average 4927 costs average 2155 and profits average 2772 from the profit function differed little from the revenues average 5390 costs average 2429 and profits average 2961 obtained from the panel data p 0 44 0 25 and 0 73 respectively fig 5 the fishbone patterns of deforestation that emerged during the simulations provide good evidence to support the model s face macrovalidation furthermore despite the limitations of adopting the figure of meri for the spatially explicit and empirical macrovalidation of the model at equilibrium the prediction accuracy score was reasonably high 59 fig 6 4 discussion the most critical component of a lucc abm is arguably the decision making processes of its agents many decision making processes in such models are based on decision trees acosta et al 2014 deadman et al 2004 salvini et al 2016 in contrast the model presented here adopted a microeconomic framework based on the assumption that perfectly market integrated households behave as profit maximizers consequently the decision making process was framed as a dynamic optimization problem like those often adopted in the environmental and natural resource economics literature e g amit 1986 barbier 1999 brown 2008 a potential criticism of the profit maximization assumption is that many theories of farm households posit them as utility maximizers rather than profit maximizers barnum and squire 1979 chayanov 1926 taylor and adelman 2003 using these theories a household s utility is often captured as a function of trade offs between consumption and leisure constrained by limited access to labor markets these characteristics are not necessarily accurate representations of farm households on the old deforestation frontier of the ouro preto do oeste region where labor markets function well as demonstrated in the microeconomic household models of deforestation developed by angelsen 1999 the utility maximization behavior can be reduced to profit maximization under perfect labor market conditions profit or utility maximization dichotomy aside we acknowledge that the optimizing behavior assumption itself does not fully explain human decision making jager et al 2000 le et al 2008 alternatives to maximization behaviors discussed in the literature include i achieving minimum levels of satisfaction or ii decision trees where agents repeat the same action until they run out of resources such as labor or capital deadman et al 2004 salvini et al 2016 furthermore an additional stochastic factor could be added to the final optimal lucc calculation as an attempt to mimic imperfect household decisions or a stochastic optimization approach based on the generation and use of random variables could be adopted e g ermolieva et al 2015 the key component of the profit function incorporated into the model lucc decision making process is the agricultural production function while the functional form of the production function can affect the behavior of the farmer agents angelsen 1999 a cobb douglas form was chosen due to its closed solution properties and widespread use in the farm household and lucc literature angelsen 1999 darwin et al 1996 despite the numerous data transformations required for the conversion of heterogeneous household production outputs to the rice equivalent the production function generated parameter values similar to those in the literature for example in the function estimated by barnum and squire 1979 for paddy rice farmers in malaysia the respective β and φ associated with the agricultural land size and labor were estimated as 0 62 and 0 29 although the overall fit of our function was somewhat low r2 0 12 compared to others in the literature based on single crop systems e g r2 0 67 reported by the previous authors the simulated annual agricultural revenue did not differ substantially from the values in the panel data reported by the households in the region caviglia harris et al 2014 additionally the global concavity of the profit function ensured that the simulation had achieved a maximum finally the sensitivity analysis of the model parameters did not reveal unexpected or contradictory outcomes the fact that our production function does not discriminate between family and hired labor combined with the assumption of perfect labor markets implies that households are treated as homogenous agents such characteristics are arguably in contrast to general lucc abms that are often based on heterogeneity and stochasticity an 2012 le et al 2008 nevertheless heterogeneity is incorporated into the model through the lucc allocation submodel as expansion of agricultural land is constrained by farm size f which varies across settlements for instance the first orthogonal i e fishbone arrangement settlements from the 1970s were 100 ha lots while the newer ones follow a radial pattern with each land holding approximately 12 ha caviglia harris and harris 2011 the lucc allocation submodel generated the fishbone deforestation patterns in the simulations since redd payments caused shifts in land use cover equilibria at the landscape level based on sensitivity analyses of the artificial neural networks model used to create the deforestation suitability map distance from previously deforested areas was the most influential biophysical variable explaining the location of observed deforestation between 1986 and 1996 followed by distance from protected areas and elevation these results are in agreement with a recent meta analysis about the drivers of deforestation conducted by ferretti gallon and busch 2014 and other similar studies müller et al 2012 pfaff 1999 soares filho et al 2010 the figure of merit of this model was high compared to others in the literature and in comparison to those for decentralized private redd projects for instance pontius et al 2008 found that close to one third of the lucc models they tested produced figures of merit 10 with only one 50 similarly an assessment of deforestation models conducted by kim 2010 found no figure of merit 8 for a highly deforested area in santa cruz bolivia the median figure of merit from 41 model validation runs conducted by fuller et al 2011 for the peat swamp forests of central kalimantan indonesia was 17 while li et al 2012 reported a 27 figure of merit for simulated scenarios of biofuel crops expansion in the great plains states of the united states müller and mburu 2009 found figures of merit ranging 37 41 after substantially large training cycles to forecast lucc in the kakamega forest of western kenya while the deforestation simulations for madagascar carried out by vieilledent et al 2013 achieved values of 10 23 however the spatially explicit validation of simredd benefited from the regional context of the study in one of the most degraded old deforestation frontiers in the brazilian amazon toomey et al 2013 so much forest clearing has occurred in the study region that the 2010 lucc classification map could be empirically validated against the lucc model output at the baseline equilibrium state the implications of starting the model with the 1996 lucc configurations are somewhat important there were three main reasons for the selection of 1996 as the first year of the model first it precedes the year when the kyoto protocol the first international climate mitigation agreement was signed at the 7th conference of parties to the united nations framework convention on climate change unfccc 1998 and discussions of whether or not to include mitigation activities based on avoided deforestation were still underway tolba and rummel bulska 1998 second it matches the first year of the panel data collection in the settlements of the ouro preto do oeste region caviglia harris et al 2014 third it allows the model to begin with a large area of mature forest in the virtual landscape which gives the model flexibility to predict deforestation and or reforestation fig s1 abandonment of agricultural patches occurs in the equilibrium state if a household has previously established more agricultural land than the optimal level dictated in the scenario settings the abandonment decision can result from two distinct implications of the model settings households abandon farmed land when 1 revenues are lower than costs and agricultural production is not lucrative or 2 when redd payments exceed agricultural profits both settings imply that farmer agents set aside part of their agricultural land so that their new mixture of agricultural land and forest area maximizes profit simulations from the model present a redd payment framework similar to the current 2017 situation the redd policy payment scheme is based on annual payments for forest area maintained or recovered which is similar to pes program in countries like costa rica ecuador and mexico arriagada et al 2012 de koning et al 2011 honey rosés et al 2009 torres et al 2013 in contrast the carbon market option assumes payments based on net carbon emissions avoided mg co2 this option in accordance with carbon accounting methodologies proposed by the intergovernmental panel on climate change ipcc 2006 2003 the united nations framework convention on climate change unfccc 2013a 2013b 2007 and the standards and methodologies for decentralized private redd initiatives developed for the voluntary carbon market e g avoided deforestation partners 2012 pedroni 2012 verified carbon standard 2017b numerous ongoing examples of redd projects that follow the latter payment option are also described in the literature e g west 2016 settings for three key model parameters determine whether households are better off with the former or the latter redd payment option 1 the net carbon emissions factor δ 2 the length of the hypothetical redd project t and 3 the size of redd payments p e i for instance the monarch butterfly conservation fund in mexico offered annual conservation payments of 12 ha 1 of forest honey rosés et al 2009 while the average carbon offset price in the voluntary carbon market was 3 30 mg co2 1 in recent reports with a range between minimum and maximum prices of 21 60 co2 1 hamrick and goldstein 2016 the length of the decentralized private redd projects is often in agreement with the 20 100 year range set by the leading voluntary carbon standard the verified carbon standard goldstein and neyland 2015 verified carbon standard 2017b lastly aboveground biomass stocks which are used as references for estimating avoided net carbon emission vary substantially across tropical forests with averages from 40 to 310 mg ha 1 baccini et al 2012 ipcc 2006 the distinct values and ranges presented above illustrate the variety of potential scenarios that can lead to substantially different lucc and welfare outcomes given the pes scheme adopted hence the proposed model with its two payment options can provide meaningful insights into the design and potential outcomes of redd initiatives 5 conclusion a hybrid optimization abm model was proposed to investigate the effects of redd payments on lucc and community welfare in settlements on an old deforestation frontier in brazil the model built into an abm platform is based on two distinct submodels one for the dynamic lucc optimization decision and the other for identification of areas suitable for land use cover change and persistence within the households lots in the study region the optimization submodel which assumes households behave as profit maximizers under perfect labor markets was calibrated empirically with unbalanced socioeconomic panel data the key component of the profit function maximized by the farmer agents in the model is the cobb douglas production function based on a standard crop output the rice equivalent and function of agricultural area and labor while the dynamic optimization process returns deterministic and homogenous results for the farmer agents differences in household landholdings and initial land use cover configurations incorporate heterogeneity into the model the outcome of the model is the optimal land use cover configuration for households at the equilibrium state taking into account the revenues from redd payments based on the conservation of forest areas and promotion of natural regeneration this submodel allows for the quantification of changes in forest cover and agricultural production in response to different redd payment options as well as a means to calculate the changes in welfare at the community level which is tightly related to equity issues associated with pes the lucc allocation submodel was constructed with an artificial neural networks based algorithm and spatially explicit biophysical variables correlated with lucc observed between 1986 and 1996 the latter submodel is responsible for the emergence of the classic fishbone patterns of deforestation observed in the output of the simulations when redd payments occur results from this submodel can estimate impacts of redd payments on habitat fragmentation and biological conservation at the landscape level the submodel can also serve for the identification and ranking of forest areas more threatened by future deforestation a key feature of simredd is its ability to simulate two distinct and widely adopted redd payment options one based on forest and forest regrowth areas ha 1 in accordance with national redd like conservation programs and the other based on avoided net carbon emissions and net carbon sequestration mg co2 1 in conformance with voluntary carbon market rules as expected without redd payments and with the assumption of continued widespread non compliance with environmental regulations the land use cover at equilibrium state is complete conversion of forests to agricultural land when redd payments are incorporated into the simulation forest patches appear in the virtual landscape at model equilibria similar results are observed as agricultural and deforestation costs increase or as agricultural revenues decrease these results indicate the mechanics behind the model are in conformance to what we would expect to observe in reality overall the model can serve as a virtual impact evaluation tool to explore the effects of pes interventions intended to impede deforestation impacts can be measured in terms of lucc greenhouse gas emissions program enrollment and costs food production and community welfare hence simredd can shed light on a wide range of topics e g natural resource economics and governance rural development food security landscape ecology habitat fragmentation climate change mitigation and equity studies insights from simulation scenarios can improve the design of more effective efficient and equitable redd programs and initiatives that involve farm household participation acknowledgements this research was funded by the national science foundation grant ses 0752936 additional funds to the first author were provided by the brazilian national counsel of technological and scientific development cnpq grant 201138 2012 3 the william c and bertha m cornett fellowship and the tropical conservation and development graduate assistantship at the university of florida and the world wildlife fund s prince bernhard scholarship for nature conservation appendix a analytical solution for the optimal agricultural land definition the step by step analytical solution involves solving for λ t in equation 8 19 λ t w γ c d ψ λ t 0 since λ t and λ t are known l t is given recursively by equation 9 as function of a t 20 p a β α a t β 1 l t φ p e δ c a ψ r w γ c d ψ 21 l t a t 1 β r c d p e δ c a r ψ r w γ ψ p a β α 1 φ once l t is defined the expression for the optimal agricultural land at equilibrium a is given recursively by equation 7 22 p a θ φ α a t β a t 1 β r c d p e δ c a r ψ r w γ ψ p a β α 1 φ φ 1 w 23 a t w β 1 1 φ α 1 φ p a 1 φ r c d p e δ c a r ψ γ r ψ ψ 1 φ 1 φ φ β φ 1 appendix b supplementary data the following is the supplementary data related to this article online data online data appendix b supplementary data supplementary data related to this article can be found at https doi org 10 1016 j envsoft 2017 11 007 
26457,modelling of land use change plays an important role in many areas of environmental planning however land use change models remain challenging to calibrate as they contain many sensitive parameters making the calibration process time consuming we present a multi objective optimisation framework for automatic calibration of cellular automata land use models with multiple dynamic land use classes the framework considers objectives related to locational agreement and landscape pattern structure as well as the inherent stochasticity of land use models the framework was tested on the randstad region in the netherlands identifying 77 model parameter sets that generated a pareto front of optimal trade off solutions between the objectives a selection of these parameter sets was assessed further based on heuristic knowledge evaluating the simulated output maps and parameter values to determine a final calibrated model this research demonstrates that heuristic knowledge complements the evaluation of land use models calibrated using formal optimisation methods keywords cellular automata land use model automatic calibration automatic parameter adjustment multi objective optimisation software availability name of software parallel nsgaii developer jeffrey newman contact address the university of adelaide and bnhcrc north terrace adelaide sa 5005 contact email jeffrey newman au gmail com year first available 2016 hardware software required cross platform compiles under clang visual studio and the gnu compiler chain hardware requirements dependent on land use model used program language c program size 13 mb availability and cost gpl 2 0 open source software downloadable from https github com jeffrey newman parallel nsgaii backend 1 introduction modelling of land use change plays an important role in many areas of environmental planning such as river basin management van delden et al 2007 natural area preservation hewitt et al 2014 the development of sustainable agricultural practises murray rust et al 2014a 2014b and the influence of urban dynamics on surrounding regions haase et al 2012 lauf et al 2012 to better understand the influences of land use changes models are increasingly being used as part of decision support systems to evaluate policy that influences spatial planning van delden et al 2011 to represent land use dynamics realistically such models must incorporate complex socio economic and biophysical drivers with human environment interactions lambin et al 2001 as a result land use cellular automata luca have become a popular modelling framework for evaluating land use changes as they are able to simulate the behaviour of complex systems with a high degree of realism hewitt et al 2014 historically cellular automata methods were proposed for application to geographic systems by tobler 1979 with luca models first used to replicate observed fractal patterns of urban evolution couclelis 1985 1989 batty and longley 1994 followed by their development into dynamic land use models white and engelen 1993c clarke et al 1997 much effort has been invested in developing luca models for different global regions with applications reviewed by santé et al 2010 this includes the advent of generic spatial modelling platforms sleuth clarke et al 1997 and metronamica van delden and hurkens 2011 which provide well tested models for a range of applications to different study regions with such generic platforms simplifying model development requirements significantly research focus on the calibration of luca models has increased in recent years e g blecic et al 2015 cao et al 2014 garcía et al 2013 li et al 2013 van vliet et al 2013b van vliet et al 2016 calibration of a land use change model is the process of determining a model parameter set through the initial setting of model parameters the iterative adjustment of these parameters based on comparison of the model output with observations and the selection of a final parameter set for application to a specific case for long term scenario analysis adapted from van vliet et al 2016 the iterative adjustment stage of calibration of luca models is extremely complex as land use change is a path dependent process that is driven by multiple interdependent processes with uncertain outcomes brown et al 2005 conventionally this stage of calibration of luca models is manual van delden et al 2012 incorporating the modellers process understanding to address this inherent uncertainty however implementation of such methods is time consuming subjective jafarnezhad et al 2016 and lacks transparency and repeatability garcía et al 2013 consequently in order to make parameter adjustment more efficient and repeatable there has been an increasing focus on automating this process van vliet et al 2013a automatic parameter adjustment methods generally make use of formal optimisation methods that maximise model performance metrics blecic et al 2015 cao et al 2014 garcía et al 2013 li et al 2013 consequently the success of these methods relies heavily on the ability to assess performance in a quantitative fashion this assessment has to consider two separate properties of luca model performance i locational agreement alternatively termed cell by cell agreement hagen zanker 2009 which is the match of pixels between simulated outputs and the corresponding observed data van vliet et al 2013a hagen zanker 2009 and ii landscape pattern structure which is the inferred realism of land use change processes captured by the difference between observed and simulated landscape patterns engelen and white 2008 consequently automatic parameter adjustment of luca models can be considered a multi objective optimisation problem hagen zanker 2008 at present multi objective optimisation has only been applied to the parameter adjustment stage of calibration of luca urban growth models that are implemented with two land use classes despite the capacity of these models to consider a broader range of land use classes using the sleuth metrics trunfio 2006 or logit regression model fitness functions cao et al 2014 whilst this work has merit in characterising urban and non urban interactions it represents a less complex calibration problem than luca models that consider multiple dynamic land use classes as these are more complex models that possess a significantly larger number of parameters for calibration in contrast studies that have used optimisation approaches for the calibration of luca models with multiple dynamic land use classes have only considered a single objective for example blecic et al 2015 considered only the locational agreement element of performance for parameter tuning and while garcía et al 2013 used both locational agreement and landscape pattern structure metrics these were combined into a single objective during the optimisation process where as a result important trade offs between locational agreement and landscape pattern structure could not be examined both studies also generated only one possible model parameterisation for future scenario analysis limiting the ability to understand how calibrated parameters are potentially influenced by the metrics used for optimisation to address these shortcomings the objectives of this paper are i to present a multi objective optimisation framework that automates the parameter adjustment stage of calibration of luca models with multiple dynamic land use classes enabling the identification of multiple model parameter sets that could be suitable for long term scenario analysis and ii to demonstrate the application of the framework on the case study comprising the randstad region in the netherlands the remainder of this paper is organised as follows the proposed multi objective optimisation based calibration framework is introduced in section 2 followed by a description of an application to a case study of randstad in section 3 the results for the case study are presented and discussed in sections 4 and 5 the conclusions and recommendations of this work are presented in section 6 2 proposed multi objective optimisation based calibration framework the proposed multi objective optimisation framework for calibration of luca models with multiple dynamic land use classes is presented in fig 1 as shown the framework is comprised of four stages first in the selection stage the components required for optimisation are chosen next in the specification stage to ensure an efficient and robust output certain aspects prevalent to the previously selected components are specified following this the multi objective optimisation parameter adjustment is implemented and run to completion finally the resulting model outputs are assessed quantitatively evaluated using a neutral model followed by heuristic interpretation of the outputs to decide on a final model parameter set 2 1 selection stage in the selection stage the four main components for optimisation are chosen as shown in fig 1 the luca model to be used and the parameters to be adjusted the optimisation algorithm used for the parameter adjustment process and the map comparison metrics used to assess model performance 2 1 1 model and parameters the luca model determines the number and type of parameters that require adjustment the parameters within luca models are used to capture the processes that influence land use changes such as the physical suitability of the landscape and the influence different land use classes exert on each other the consideration of multiple dynamic land use classes as is the case for transition potential models derived from white and engelen 1993c introduces a large number of parameters that must be adjusted to capture the respective influences of each process on different land use classes garcía et al 2013 the most notable example of parameters for transition potential models are neighbourhood rules which characterise the influence different land use classes exert on each other at different distances riks 2015 for example considering a neighbourhood size of eight cells introduces 30 parameters for each neighbourhood rule as the number of neighbourhood rules is the product of the total number of land use classes and the number of actively allocated land use classes the resulting number of parameters that could be adjusted is large consequently it is desirable to be judicious about which parameters to include in the automatic adjustment process there are several approaches for selecting which parameters to include in the automatic adjustment process including empirical understanding of the region of interest in order to select the parameters that correspond to the dominant processes driving land use change hewitt et al 2014 and the use of quantitative analysis methods such as data mining or sensitivity analysis li and yeh 2004 wang et al 2011 gibbs et al 2012 alternatively the computational burden associated with the automatic adjustment process can be reduced by using empirical methods to identify reasonable values of certain parameters such as neighbourhood rules van vliet et al 2013a which can then be used to initialise the optimisation at a good solution which has been shown to reduce the computational demands of optimisation problems in other fields e g bi et al 2016 in addition to the number of parameters that require adjustment the size of the space of potential parameters values that need to be explored during the automatic adjustment process which is commonly referred to as the search space is also affected by the potential ranges different parameters can take maier et al 2014 consequently the upper and lower limits of the parameters that are included in the automatic calibration process need to be selected carefully balancing the needs to ensure the best possible combination of parameter values can be identified with the desire to reduce the size of the search space the size of the search space can also be reduced by restricting the values parameters can take to be in accordance with underlying domain knowledge for example neighbourhood effects can be parameterised to generate shapes consistent with discursive knowledge hagoort et al 2008 reducing the number of parameters from the order of thousands to hundreds blecic et al 2015 garcía et al 2013 2 1 2 multi objective optimisation algorithm as part of the proposed framework it is suggested a population based metaheuristic algorithm is used for the automatic parameter adjustment process such algorithms are advantageous because they are able to find near globally optimal solutions for highly complex e g non linear non convex problems and it is straightforward to link them with existing simulation models such as luca models without the need for problem simplification maier et al 2015 this is because they work in an iterative fashion using a population of separate solutions where the model parameters are adjusted for each member of the population based on the search strategy of the algorithm under consideration e g survival of the fittest in the case of genetic algorithms or the foraging behaviour of ants in their search for food in the case of ant colony optimisation and how well these adjustments perform is evaluated by running the model with the altered parameters information on how well the suggested changes have performed are fed back to the algorithm informing which changes are made to the model parameters in the next iteration or generation and so on in addition metaheuristic algorithms have the capacity to handle multiple objectives maier et al 2014 enabling them to optimise measures of locational agreement and landscape pattern structure simultaneously there are a number of population based metaheuristics kingston et al 2008 which can be used to automate luca model parameter adjustment utilising various heuristic mechanisms with heuristic selection mainly due to preference applied algorithms include ant colony optimisation liu et al 2012 memetic algorithms veerbeek et al 2015 particle swarm algorithms blecic et al 2015 feng et al 2011 and genetic algorithms cao et al 2014 garcía et al 2013 li et al 2013 trunfio 2006 clarke lauer and clarke 2011 the proposed framework caters to different population based metaheuristics but requires an algorithm that can handle multiple objectives to allow for exploration of the trade offs between the objectives 2 1 3 map comparison metrics as mentioned previously optimisation requires quantifiable objectives to assess performance consequently the proposed framework requires the selection of metrics that can quantify the objectives of maximising locational agreement and minimising landscape pattern structure error for locational agreement potential metrics include percentage correct santé et al 2010 the figure of merit pontius jr et al 2008 allocation and quantity agreement pontius jr and petrova 2010 or cohen s kappa or one of its variations van vliet et al 2011 van vliet et al 2013a there are also many landscape pattern structure metrics mcgarigal 2014 that quantify different aspects of the landscape and have been applied previously in land use modelling studies these include amongst others edge density garcía et al 2013 largest patch index li et al 2013 and clumpiness van delden et al 2012 2 2 specification stage as shown in fig 1 this stage follows the selection stage and requires the specification of certain relevant components of the model and optimisation algorithm to allow for effective optimisation this includes how model stochasticity is taken into account and the parameterisation of the selected optimisation algorithm 2 2 1 model stochasticity luca models generally include a stochastic element to capture the variability of human decisions that drive land use changes by including a random perturbation factor in the model this stochasticity must be considered appropriately to gain meaningful optimisation results in order to achieve this guo et al 2017 recommend using a number of model runs with different random seeds but to use the same seeds during each iteration of the optimisation in order to ensure the impact of changing model parameters from one optimisation iteration to the next is not diluted or confused by the stochastic nature of the model used to assess the objective function values therefore as part of the proposed framework n luca model runs with different random number seeds are used to assess model performance for a given set of model parameters during each iteration of the automatic parameter adjustment optimisation process however the same random number seeds are used in every iteration of the optimisation process to ensure that any changes in the objectives from one iteration to the next are due to changes in model parameters rather than a combination of these changes and randomness in luca models 2 2 2 optimisation configuration three aspects of the optimisation process must be specified prior to running the optimisation algorithm first values of the parameters that influence optimisation algorithm searching behaviour such as population size probability of cross over and probability of mutation in the case of genetic algorithms must be defined zheng et al 2016 zecchin et al 2012 it should be noted that these parameters are different from the parameters of the luca models that are to be determined with the proposed automatic parameter adjustment process second termination criteria for the optimisation process such as a pre defined number of iterations or no significant improvement in performance must be specified finally the number of times the entire optimisation process has to be repeated r has to be specified such repetition is needed due to the stochastic nature of population based metaheuristics and increases the chance that the best possible combination of luca model parameters is identified and that the results of the automatic parameter adjustment process are robust it should be noted that the stochastic nature of the optimisation process is distinct from the uncertainty associated with the luca models while the luca model is run n times during each iteration of a single optimisation run in order to account for the stochasticity of luca models the entire optimisation process is repeated r times to account for the stochasticity of the metaheuristic optimisation process 2 3 adjustment stage 2 3 1 optimisation process the purpose of this stage of the proposed framework is to use the selected optimisation algorithm see section 2 1 2 to identify the combinations of luca model parameters see section 2 1 1 that provide the best possible trade offs between the selected performance metrics see section 2 1 3 as mentioned previously this is achieved in an iterative fashion by using a metaheuristic optimisation algorithm to determine changes to the model parameters at each generation of the optimisation process based on resulting changes in the performance metrics the steps in this process are shown schematically in fig 1 and in more detail in fig 2 as can be seen in fig 2 given that population based metaheuristics are used for optimisation a population of p solutions p sets of luca model parameter values is generated for each iteration of the optimisation process next a luca model with each of these p sets of parameter values θ the parameters selected for optimisation input into the model is run from time t 0 until time t n with an actual land use map at time t 0 providing the initial conditions for each of the p model runs fig 1 each simulated output map at time t n is quantitatively compared with a corresponding map of actual land use using the metrics in the comparison set selected previously one to assess locational agreement and another to assess the error of landscape pattern structure as mentioned previously given the stochastic nature of land use change each of the p land use models is run with n different random number seeds generating n simulated land use maps each of which is individually compared with the corresponding map of actual land use the performance of each model with the p sets of selected model parameters is quantified by taking the average of the metric values across the n different outputs figs 1 and 2 hence the final metric value for one member of the population of the metaheuristic is given by the following objective 1 l a p θ 1 n i 1 n l a i θ where la p is the locational agreement objective function value of the member of the population p θ are the input parameters corresponding to that member of the population and n is the number of luca model random number seeds being considered similarly for landscape pattern structure 2 l p s p θ 1 n i 1 n l p s e r r o r i θ where lps p is the landscape pattern structure error objective function value of the member of the population p and n is the number of luca model random number seeds as mentioned previously when metaheuristic optimisation algorithms are used θ values are optimised in an iterative fashion with the aim of improving the performance metrics from one generation to the next consequently the above steps are repeated for a total of g generations fig 2 until the desired stopping criteria have been met see section 2 2 2 finally as also mentioned previously each optimisation run is repeated r times to account for the stochastic nature of the searching behaviour of population based metaheuristics as parameter adjustment is a multi objective optimisation problem the optimisation process does not generate a single set of model parameters rather the optimisation generates a series of model parameter sets that generate optimal non dominated trade offs between the two objectives the output of the optimisation is shown in fig 1 by the curve called the pareto front of objectives the pareto front indicates the optimal trade off between objectives where improved performance in one objective cannot be achieved without inferior performance in the other objective 2 3 2 implementation the proposed optimisation process is very computationally expensive because of the relatively long simulation times associated with running luca models e g run times can vary from 10 s to 10 min depending on the number of classes spatial extent and resolution and the large number of simulations required as illustrated in fig 2 for example for typical values of p 200 n 10 g 500 and r 5 the luca model would have to be run 200 10 500 5 5 000 000 times resulting in a total run time ranging from 50 000 000 to 3 000 000 000 s or 600 to 35 000 days for individual luca model run times of 10 and 600 s respectively which is not feasible from a practical perspective however as has been discussed in other problem domains there are a number of avenues for increasing the computational efficiency of this process including the incorporation of heuristic information to reduce the size of the search space szemis et al 2012 and section 2 1 1 or to improve the efficiency of the optimisation process nguyen et al 2016 by using meta models as surrogates for the computationally expensive luca models broad et al 2015 or the use of parallel computing resources blecic et al 2015 by using these approaches automatic parameter adjustment methods can be made feasible for application to complex luca models 2 4 assessment stage to determine the final calibrated model the resultant model parameter sets corresponding to different pareto front solutions are assessed using a comprehensive set of methods that have been used in similar calibration studies van vliet et al 2013b hewitt et al 2014 including assessment of objective output using a neutral model benchmark test and a final evaluation comprising heuristic interpretation of the solutions and parameter values obtained 2 4 1 neutral model benchmark test an issue with calibrating luca models is the impact of boundary conditions due to land use commonly persisting metrics often indicate good model performance though this can largely be due to limited variation of the final landscape from the initial situation hagen zanker and lajoie 2008 hence the proposed framework uses a neutral model benchmark test to evaluate the output in the objectives space to determine which model parameter sets have captured land use change processes appropriately this is illustrated by the black lines over laid on the pareto front in the assessment stage in fig 1 a model that outperforms the benchmark i e generates superior metric values is considered valid because model performance can be attributed to correct capture of processes rather than inherent land use persistence hagen zanker and lajoie 2008 2 4 2 final evaluation a final evaluation of the optimisation results is performed by assessing the solution and parameter spaces fig 1 this can be performed by a modelling expert or in a participatory manner where stake holders are included in the evaluation to decide on the most appropriate model as with manual calibration hewitt et al 2014 evaluation of the solution space refers to interpreting the land use maps generated by the different model parameter sets corresponding to different solutions along the pareto front evaluation of the parameter space refers to assessing the parameters corresponding to the different solutions to determine if these are consistent with expected land use change processes of the study region by doing so a final model parameter set can be recommended 3 case study application of proposed framework the proposed framework outlined in section 2 is applied to a real world case study of the randstad region in the netherlands from 1989 to 2000 randstad is a conurbation of the four largest cities in the netherlands and the surrounding region the model for the case study region has a 500 metre resolution and covers a spatial extent of 14 175 km2 shown in fig 3 3 1 selection stage 3 1 1 land use model the transition potential luca model metronamica is used van delden and hurkens 2011 metronamica is a generic modelling platform evolved from the pioneer model by white and engelen 1993c that facilitates direct application to the study region of interest capturing the dynamics between urban and regional systems riks 2015 which is able to consider multiple dynamic land use classes it has been used successfully in numerous decision support systems in diverse global regions such as the regional and national socio economic policies in the netherlands engelen et al 2003 long term regional planning in waikato new zealand rutledge et al 2008 river basin management for mediterranean watersheds van delden et al 2007 assessment of the impact of intra urban land prices furtado et al 2012 impact assessment of agricultural policies in europe van delden et al 2010 and modelling shifting cultivation practises in sri lanka wickramasuriya et al 2009 in metronamica a transition potential is used to allocate land use classes calculated to update land use at each time step using various factors to define the potential for each cell to support each land use class considered land use classes are allocated until all demand requirements are met demands for particular land uses are exogenously defined based on the transition potential for each given class or until there are no locations available metronamica considers three land use class categories active which are dynamically modelled and allocated commonly urban classes features which are fixed land uses that influence landscape dynamics but do not have dynamic behaviour such as airports or water bodies and passive which occupy the remaining landscape and change in extent as a result of changes to the other categories these are normally occupied with land use types that have low transition costs for example agricultural land the metronamica model developed for the randstad case study has ten land use classes there are seven active greenhouses residential industrial services socio cultural uses nature and recreation areas one passive agriculture and two features airport and water to determine the transition potential metronamica considers four processes accessibility the provision of infrastructure required for certain land use classes suitability the influence of physical factors neighbourhood effects the spatial interactions between land use classes representing the behaviour of actors and activities taking place on those classes and zoning the influence of spatial planning a stochastic component is also included to incorporate the inherent uncertainty of land use change decisions the transition potential is calculated by 3 t p c k a c k s c k n c k z c k where tp c k is the transition potential for the cell c to support land use class k subscripts for all symbols above have the same interpretation a c k is the accessibility s c k is the suitability n c k is the neighbourhood effect z c k is the zoning influence and γ is the stochastic component the form of which is given in the metronamica documentation riks 2015 this documentation also includes further details of the transition potential and underlying equations for each process the metronamica model developed for randstad includes two accessibility layers motorways and other roads four physical attributes are included for suitability noise elevation natural hydrology and slope zoning for ecological corridors the noise contour for schiphol and urban expansion plans are also included 3 1 2 parameters as the case study is a region of urban growth with a short time span the main sources of land use conversion will relate to the expansion of the existing socio economic land uses hence the major processes driving land use changes will be the self organising behaviour of the system for increased expansion of the urban cores couclelis 1989 batty and longley 1994 white and engelen 1993a 1993b as a result the most important processes to include for tuning are those driving this behaviour the neighbourhood interactions and accessibility verburg et al 2004 consequently parameters for neighbourhood rules and accessibility are included for automatic tuning neighbourhood rules are parameterised to define the influence that different land use classes exert on each other the cumulative transition potential due to neighbourhood influence for the conversion from one land use to another is calculated by 4 n c k c d c w k k c d c c where n c k is the neighbourhood transition potential for cell c transitioning to land use type k d c is the set of all cells in the neighbourhood of the cell of interest c k c is a look up function that returns the land use class for cell c d c c is the distance between cells c and c and w k j x expresses the influence that a cell with land use class j returned by the look up function k c exerts on a cell of potential land use class k at a linear distance of x between the two cells accessibility defines the importance of infrastructure elements for different land use classes parameterised as 5 a c k a s k d s c a s k if a s k 0 0 if a s k 0 1 a s k d s c a s k otherwise where d s c is the cellular distance between cell c and the nearest cell containing the type of infrastructure s and a s k is the accessibility decay parameter that expresses the importance of the type of infrastructure s to land use k values for a s k and a weighting parameter are tuned for each actively allocated land use class k and each infrastructure type s for this case study 28 parameters are tuned for accessibility because there are 7 actively allocated land use classes and 2 types of infrastructure layers motorways and other roads to reduce the size of the search space during the automatic parameter adjustment stage the neighbourhood rules are delineated into two specific parts as shown in fig 4 the locus point dot which defines the inertia self influential rules or conversion interactive rules of the location of interest and the tail line which defines the interaction effects for different distances there are several common shapes that the neighbourhood rule tails can take van vliet et al 2013b hagoort et al 2008 the most common forms are characterised by a high influence at shorter distances a point of inflection and a slow gradually decaying influence over large distances to capture such dynamics this research parameterises neighbourhood effect tails using exponential decay functions thereby reducing the number of parameters that require calibration of the form 6 y x c for x 0 a e b x for 0 x x c 0 for x x c where a and b are the controlling parameters of the neighbourhood rule x is the distance y x is the influence value c is the locus point of persistence and conversion and x c is the critical distance where the influence is set to zero for each neighbourhood rule three parameters are tuned a b and c for this case study there are 10 land use classes and 7 dynamic land use classes hence 70 neighbourhood rules are included for adjustment by parameterising neighbourhood rules using exponential decay functions 210 parameters are tuned for neighbourhood rules hence a total of 238 parameters are tuned as part of the adjustment process for this case study 3 1 3 multi objective optimisation algorithm this research uses the non dominated sorting genetic algorithm ii nsga ii first proposed by deb et al 2002 which is a population based multi objective genetic algorithm moga regarded as an industry standard wang et al 2015 with a demonstrated ability to tune luca model parameters cao et al 2014 trunfio 2006 the moga is used to find multiple alternative luca parameter sets that represent the best possible trade off between the calibration objectives the pareto front of solutions illustrated in fig 1 by the curved blue line mogas use simple heuristics computationally efficient rules across a number of generations to derive information about which decision variable values result in better performing objectives and use this information about the present parent population to generate the next child generation of solutions some of which are likely to have superior objective function values the heuristics mimic those of survival of the fittest and are referred to as selection cross over and mutation selection is used to promote better performing solutions to the subsequent generations by comparing different solutions and selecting by some mechanism those which perform better cross over takes a subset of decision variable values from a pair of parent solutions and randomly recombines them to form new child solutions the purpose is to exploit good solutions as better performing decision variable values from the parents when combined differently may result in children with superior performance this is governed by the probability of cross over which is set prior to optimisation mutation takes a small subset of decision variable values in a child and perturbs them the purpose of which is to diversify the search to explore a wider possible range of solutions which might lead to superior objective function values the specific moga operators employed by the nsga ii as well as the additional features that distinguish it from conventional mogas are illustrated in fig 5 as shown nsga ii begins with an initialisation step where an initial population of solutions with randomly generated decision variable values is created the associated objective function values are then calculated next the standard genetic operators previously discussed selection cross over mutation are used to generate a child population from the existing parent population this implementation of nsga ii uses tournament selection simulated binary crossover and polynomial mutation as implemented by deb et al 2002 because this problem uses real value decision variables the key performance advantage of using nsga ii is achieved via the additional operators implemented in the algorithm which preserve non dominated i e the best performing solutions from generation to generation as shown in fig 5 nsga ii recombines the parent p and child q populations and ranks the combined population in order of non dominated fronts that occur which are a series of pareto fronts from this a new parent population p is formed by iteratively selecting the solutions belonging to the best fronts if only a subset of solutions from a front can be included crowding distance sorting is used which aims to select the most diverse set of solutions from the final front that is included this process is repeated until the stopping criteria are met 3 1 4 map comparison metrics for the case study locational agreement is measured using fuzzy kappa simulation fks developed by van vliet et al 2013a the fks metric is an adaptation of cohen s kappa santé et al 2010 van vliet et al 2016 which measures the fuzzy agreement of location and class between two data sets relative to a baseline of random allocation 7 f k s θ p o θ p e θ 1 p e θ where po θ is the observed agreement the similarity of the simulated land use transitions with the observed land use transitions and pe θ is the expected agreement the agreement obtained from a random allocation of the given class transitions relative to the initial land use map fks is used because it measures the agreement of land use transitions and hence includes an implicit baseline it also uses fuzzy interpretation which attributes partial agreement relative to proximity in this research partial locational agreement of correct transitions is considered for a neighbourhood radius of two cells with strength of agreement using a halving distance of one hence the locational agreement measure for a single member of the population la p θ is equivalent to fks agreement averaged across the total number of random number seeds considered to assess the error of landscape pattern structure the average of the absolute class level clumpiness error between actual and simulated values is used this metric is used based on successful previous applications as a part of the manual parameter tuning of luca models van delden et al 2012 clumpiness is an aggregation metric that measures the proportional deviation of the proportion of like adjacencies involving the corresponding class from what is expected under a spatially random distribution mcgarigal 2014 it is calculated first by determining the proportion of like adjacencies 8 g k g k k l 1 l g k l min e k where g k is the proportion of like adjacencies for land use class k g kl is the count of like adjacencies between patches of class k and l using the double count method mcgarigal 2014 l is the total number of land use classes min e k is minimum perimeter of a maximally clumped patch of class k defined as 9 min e k 4 n 4 n 2 4 n 4 n 2 a k n 2 a k n 1 n a k n 1 n where a k is the area of class k in terms of number of cells and n is the length of a side of the largest integer square possible with a smaller area than a k with the proportion of like adjacencies calculated it follows that clumpiness is calculated by 10 c l u m p y k g k p k p k g k p k 1 p k for g k p k and p k 0 5 otherwise where p k is the proportion of the landscape occupied by patch type class k as clumpiness is calculated at the class level it is aggregated by taking the average across the dynamic land use classes hence for this case study landscape pattern structure error is given by the average error between the observed and simulated clumpiness values δclu for the seven actively allocated land use classes averaged across the total number of random number seeds considered 3 2 specification stage 3 2 1 model stochasticity stochasticity in the metronamica model is accounted for by calculating the map comparison metrics using ten stochastically generated replicates of simulated land use maps for each iteration of nsga ii i e n 10 in accordance with equations 1 and 2 3 2 2 optimisation configuration although it is generally recommended to fine tune the parameters controlling the searching behaviour of the optimisation algorithm such as the population size the probability of mutation and probability of cross over via sensitivity analysis before application for large problems this is extremely computationally expensive and therefore impractical wang et al 2015 as this research falls into this category of problems recommended nsga ii parameter values are adopted zheng et al 2016 wang et al 2015 namely a probability of crossover of 0 9 and a probability of mutation equalling the inverse of the number of decision variables 0 0042 in this case the selection of population size which corresponds to how many chromosomes would be included per generation is also informed by the work of wang et al 2015 into optimisation problems with many decision variables their results show that the population size for large problems must be greater than the number of problem decision variables to find non dominated solutions hence this research uses a population size of 256 i e p 256 the stopping criterion adopted is based on solution convergence and reaching a certain number of generations the required number of generations used is 500 i e g 500 convergence implies that for the given optimisation algorithm configuration negligible improvement is obtained in the objective space with continued optimisation there are numerous metrics for measuring convergence maier et al 2014 this research uses the hyper volume metric zitzler 1999 a commonly employed metric of multi objective performance reed et al 2013 hadka and reed 2012 that captures convergence and diversity of the objective space the solutions are considered to have converged when there is negligible improvement in the hyper volume for 50 generations defined as less than 0 5 improvement in the hyper volume metric the results for all optimisation runs show that the use of 500 generations is sufficient for hyper volume convergence corresponding to a 0 5 change given the computational demands of the framework five different optimisation seeds are used i e r 5 following the convergence of the five optimisation runs the pareto optimal solutions are combined to identify the best overall front as is commonly done in multi optimisation studies e g wu et al 2012 3 3 adjustment stage 3 3 1 implementation for the proposed case study running the optimisation on an intel i5 347os cpu core processor a single evaluation took approximately 6 s i e a single run of the metronamica model consequently extrapolating this out to a complete optimisation run the total computational time would be approximately p n g r 6 256 10 500 5 6 444 4 days hence to make the optimisation process feasible from a practical perspective a parallelised version of nsga ii is implemented to increase computational efficiency this uses the phoenix cluster a high performance computing facility operated by research services at the university of adelaide the separate optimisation runs are parallelised by distributing the 256 evaluations one for each member of the population over 129 cpu processing cores across eight computational nodes for the phoenix cluster each computational node consists of 2 xeon e5 2698v3 chipsets each containing 16 cores to control the parallelisation a master slave model of computation is implemented wherein 128 slave processes running on separate cores are used in parallel to evaluate the objective functions for separate members of the population and the 129th is the master process that coordinates the search and runs the recombination that is the selection crossover mutation non dominated sorting and crowding distance operators parallelisation is achieved using the message passing interface mpi using asynchronous communication to improve algorithm efficiency using mpi the master process passes messages containing a set of decision variable values a luca model parameter set to each of the slaves upon receiving this message the slave runs the luca model evaluates the calibration objectives and passes a message back to the master containing these objective values through this allocation near linear speedup is achieved as communication time is orders of magnitude lower than time taken to evaluate the calibration objectives with nsga ii completing 500 generations within 48 h code in c for this implementation of the nsga ii is fully open sourced and available as per the details in the software availability section of this paper 3 4 assessment stage 3 4 1 benchmark test the benchmark test is conducted by applying two neutral models the growing clusters van vliet et al 2013a and random constraint match riks 2010 neutral models which generate maps of the region according to two different growth strategies as fks includes an inherent baseline by only considering transitions van vliet et al 2013a a benchmark value of 0 is used for fks neutral model benchmark testing is used to evaluate solutions with respect to the landscape pattern structure error for consistency the clumpiness of the active classes the optimisation objective from five different growing cluster neutral model outputs is averaged and the error between this and the observed clumpiness for the year 2000 taken as the benchmark landscape pattern structure metric value outputs of the optimisation routine with less error than this value are assumed to have captured model processes adequately and are considered for the final evaluation 3 4 2 final evaluation a final evaluation of the model parameter sets is conducted by reviewing the solution maps and model parameters obtained after application of the benchmark test in conjunction with an experienced land use modeller through a collaborative evaluation of these solutions a final model parameter set is decided upon based on heuristic evaluation combining understanding of both the model and study region 4 results this section presents and evaluates the results of the multi objective optimisation framework application to the randstad case study this comprises three parts as shown in fig 1 first the resultant pareto front is evaluated to assess the trade off found between the metrics and the number of solutions that pass the benchmark test second the solution space is assessed by heuristic evaluation of the simulated output maps third the parameter space is evaluated through heuristic interpretation of the trends in the parameter values observed across the pareto front as well as the interaction between different parameters the results are not compared with a more common manual calibration method due to the complexity and highly subjective nature of such an approach 4 1 identified pareto optimal solutions the pareto front comprised of non dominated solutions from the five optimisation runs is shown in fig 6 in total 77 pareto optimal solutions were identified each corresponding to a model parameter set that generated non dominated metric values the output metric values are coloured for each parameter set with metric values for 10 independent simulations shown for each illustrating the influence of simulation stochasticity red points correspond to parameter sets that generated a low δclu and low fks agreement blue points correspond to parameter sets that balance the objectives and pink points show parameter sets that resulted in a relatively high δclu and a relatively high fks agreement the black points show solutions that were analysed in further detail to determine a final model parameter set see section 4 2 as shown in fig 6 there was a meaningful trade off between the two objectives used values of the average absolute clumpiness error across the actively allocated land use classes varied from 0 001 to 0 067 and values of fks ranged from approximately 0 169 to 0 222 with a reasonably uniform distribution of points in between the extremes the solutions generated corresponded with the more commonly used kappa values between 0 758 and 0 789 comparable with other calibration studies cao et al 2015 chaudhuri and clarke 2013 garcía et al 2013 jafarnezhad et al 2016 to filter the pareto front solutions based on objective performance following the proposed framework performance was compared to the two baseline models presented in section 3 4 1 the two horizontal dashed black lines show the benchmark metrics values calculated for average absolute clumpiness error where the line of δclu 0 060 corresponding to the random constraint match benchmark metric value and the line of δclu 0 018 corresponding to the growing clusters benchmark metric value of the parameter sets obtained 74 resulted in superior performance to the random constraint match neutral model and 50 resulted in superior performance to the growing clusters neutral model all solutions were considered valid with respect to fks as all had fks agreement values greater than 0 fig 6 also shows the importance of considering model stochasticity as part of the automated adjustment process the independent simulation objective values vary considerably from the average performance value shown by the black line with a noticeable increase in the spread of the independent solutions from the average for pareto front solutions with higher fks values this variation is attributed to the influence of the stochastic factor which characterises the degree of randomness that is built into the land use allocation for each model realisation see transition potential definition in equation 3 this highlights the need to consider model stochasticity appropriately when using automatic parameter tuning methods as not doing so would result in over calibration to a single model realisation 4 2 simulated output maps following the assessment of the obtained pareto front this section presents an evaluation of a subset of the pareto front solutions obtained following benchmark testing the simulated output maps generated by certain parameter sets corresponding to the selected pareto front solutions shown by the black dots in fig 6 were assessed to obtain a better understanding of the nature of the trade off between the two objectives by considering how the trade off is reflected in the simulated output maps five pareto front solutions were evaluated with metric values given in table 1 the selected solutions correspond to parameter sets that resulted in the lowest average values of fks agreement and lowest average values of δclu the highest average value of fks agreement with the highest average values of δclu that passed the benchmark test and three intermediate solutions approximately equally spaced with respect to the map comparison metrics the corresponding simulated output maps are presented in fig 7 for each figure an enlarged version of the area in the black square in the left hand panel is shown in the right hand panel to highlight an area of significant change the variation between the simulated output maps can be largely attributed to the amount of interspersion observed between the different classes the simulated output maps for parameter sets that achieved superior δclu performance shown in fig 7b and c exhibit larger clusters of the socio economic land use classes such as residential red industrial purple and recreation areas dark yellow the simulated output maps for parameter sets that achieved superior fks performance such as fig 7d e and 7f exhibited an increasingly large amount of interspersion of these land use classes the most notable examples can be seen with the interspersion of recreation areas amongst clusters of residential land use throughout the map also shown in fig 7d 2 7e 2 and 7f 2 there is also a growing presence of socio cultural uses pink interspersed amongst greenhouses orange for these simulated output maps and a large cluster comprised of greenhouses and residential land uses that grows with improving fks performance from visual inspection of the simulated output maps and comparison with the data certain solutions could be rejected despite passing the benchmark test due to the patterns generated being too inconsistent with the data for example the solutions corresponding to maps shown in fig 7d e and 7f are considered unsuitable due to the over interspersion of the socio economic land use classes which are not observed in the data it appears that the use of fks placed an over emphasis on generating transitions which resulted in the patterns observed in the simulated output maps being inconsistent with the data to evaluate this the kappa and fuzzy kappa metrics more commonly used locational agreement metrics in calibration studies e g cao et al 2015 chaudhuri and clarke 2013 garcía et al 2013 wickramasuriya et al 2009 hagen zanker 2009 that consider the entire landscape were calculated values between 0 758 and 0 789 for kappa and 0 828 and 0 854 for fuzzy kappa were obtained for the solutions generated for kappa a benchmark value of 0 789 was calculated for the growing clusters neutral model which is outperformed by only one solution and 0 774 for the random constraint match neutral model which is outperformed by 19 solutions which corresponded to solutions that generated lower fks values for fuzzy kappa a benchmark value of 0 869 was calculated for the growing clusters neutral model which is not outperformed by any of the solutions and 0 828 for the random constraint match neutral model which is outperformed by 71 solutions such performance with fewer solutions outperforming baseline metric values is consistent with the over emphasis on the agreement of transition cells with the use of the fks metric to measure locational agreement 4 3 parameter variation along pareto front this section presents an analysis of the parameters corresponding to the different pareto front solutions coloured by solutions as shown in fig 6 to evaluate the interplay between different parameters and determine if this is consistent with process understanding and to provide insight into how the preference for the different objectives was achieved through different parameter value settings given the large number of parameters in the luca model that were tuned 238 key parameters corresponding to the pareto front solutions evaluated in section 4 2 have been selected for detailed consideration the inertia parameters for each land use class the conversion parameters for transitions to the class residential the tails of the interaction rules for transitions to the class residential the interactions conversion parameters and tails between greenhouses and socio cultural uses and the distance decay parameters for the accessibility fig 8 shows the inertia parameter values the c value in equation 6 for the self influence rules ordered and coloured by the pareto front solution as can be seen there are trends in the parameters generated by the optimisation process for the land use classes services ser and socio cultural uses scu the optimisation process resulted in high influence values relative to the values for the other land use classes across all five solutions evaluated hence it can be concluded that both objectives where optimised when these land use classes were relatively inert for the other classes the optimisation process generated a residential res inertia value that was relatively high for solutions that favoured δclu i e solutions 1 and 9 whereas the optimisation process resulted in all other classes having a relatively low inertia value that was relatively constant between classes however for solutions that balance both objectives solution 23 the inertia for residential drops to a relatively low value whilst the classes industry ind nature nat and recreation areas rec jump to relatively higher values a trend that persists for solutions favouring fks solutions 36 and 50 this suggests that to generate better fks values there must be more transition from the class residential this is reflected in data and confusion matrices for each solution appendix a however such a result is in conflict with common understanding of land use dynamics where the large initial investment required for urban classes such as residential van vliet et al 2013a means socio economic land use classes generally have relatively high inertia values given the high associated transition costs additionally the high inertia of the class nature also conflicts with common understanding of land use dynamics as natural areas are more often preserved due to zoning regulations e g van delden et al 2007 as opposed to being inert as shown in fig 8 hence the pareto front solutions analysed exhibit certain inertia values that are consistent with expectation such as residential for solution 1 and industry for solution 50 other inertia values such as the industry for solution 1 as well as residential and nature for solution 50 are not fig 9 shows the values of the conversion parameter the c value in equation 6 for the interaction rules for different land use classes converting to the class residential ordered and coloured by the pareto front solution generally solutions favouring fks exhibited higher conversion values to generate more conversions to the class residential which were exhibited in the data see appendix a the trends shown in fig 9 were also found for other conversion parameters as shown the optimisation process resulted in certain conversion parameters that did not exhibit any particular trend across the solutions for example the conversion from industry to residential which has a relatively constant value for solutions 1 and 9 that favour δclu and solutions 36 and 50 that favour fks but a negative value for solution 23 a trend that was observed was that the optimisation process generated parameters that changed sign from negative to positive as pareto front solutions shifted from favouring δclu to fks to promote certain conversions conversion from greenhouses recreation areas and services to residential are negative for solutions 1 and 9 that favour δclu preserving the landscape pattern by minimising potential conversions moving to solutions 23 36 and 50 the optimisation process has resulted in positive parameter values with increasing magnitude to promote more conversions the tail parameters the calculated y x value in equation 6 corresponding to the neighbourhood rules exhibited similar trends to the conversion parameters examples of which are illustrated for the influence of different land use classes in the neighbourhood of the class residential presented in appendix b there were certain neighbourhood rules that were tuned to relatively constant forms across the solutions such as the weak attractive influence for the presence of industry in residential locations likely due to both being socio economic land use classes other rules exhibited a trend across the solutions of an increasing attractive or repulsive strength an example of which is shown in fig 10 for the attraction of residential location in the neighbourhood of other residential locations as shown the attractive influences increase from relatively low values for solutions favouring δclu to relatively high values for solutions favouring fks there were also rules such as the presence of nature in the neighbourhood of residential land uses which displayed no trend across the solutions solutions 1 and 9 that favoured δclu and solutions 36 and 50 that favoured fks both featured a repulsive influence but solution 23 which balances the two objectives features a weak attractive influence examination of neighbourhood rules also provides insight into the unexpected model behaviour exhibited in the results maps such as the large clusters of socio cultural uses and greenhouses interspersed between each other in solutions 23 36 and 50 highlighted in fig 7d e and 7f such behaviour could either result from higher conversion values the c parameter or a strong attractive influence between the two classes as a result of the a and b parameters equation 6 the conversion values as well as the influence values at a distance of one are presented table 2 to indicate the resultant influence different land use classes have on each other across the different solutions as shown the conversion values for transition from greenhouses to socio cultural uses are all positive facilitating such transitions across all solutions however only solutions 23 36 and 50 had positive conversion values for transition from socio cultural uses to greenhouses a more likely explanation of the resultant behaviour is provided by the neighbourhood rule tails indicated by the influence values at a distance of one across the five solutions solutions that preference δclu have a repulsive influence i e a negative influence value between both land use classes whilst solutions that preference fks have an attractive influence between both land use classes producing a mutual attraction that explains the observed simulated output maps to illustrate this further the a parameters for the conversion from greenhouses to socio cultural uses which control the type attractive or repulsive and partially the strength of the influence that the presence of the class greenhouses has in the neighbourhood of cells transitioning to socio cultural uses is shown in fig 11 for all pareto front solutions ordered by increasing fks value as shown there is a clear point at approximately solution 20 where the a parameter changes from a negative to a positive value changing the relationship from repulsive to attractive given the trend shown in fig 11 this appears to be a driver in generating solutions that preference fks agreement similar behaviour was also found for the a parameter controlling the influence the class socio cultural uses has in the neighbourhood of cells transitioning to greenhouses figs 12 and 13 show the distance decay parameter values for motorways and other roads respectively ordered and coloured by the selected solution as shown the optimisation process resulted in several trends across the pareto front solutions some parameters were tuned to relatively constant values across the solutions such as industry and services for motorways and recreation areas and socio cultural uses for other roads parameters for other classes were tuned to either relatively high or relatively low values depending on the relative trade off between the two objectives for example the distance decay for greenhouses and residential increased from a relatively low value for solutions favouring δclu to a relatively high value for solutions that favour fks for both motorways and other roads which is reflected in the simulation maps with the increasingly growing clusters of inter connected patches of these land uses between the two concentrated urban areas in fig 7b 2 for solutions with a preference for δclu there are two distinct clusters of urban land uses most notably residential that are sparsely connected whereas in fig 7f 2 for solutions with a preference for fks the two patches of residential area are essentially connected by the land use classes residential and greenhouses in an area where both motorways and other roads are present 4 4 final parameter set following the evaluation of the identified pareto optimal solutions considering the simulated output maps parameters and the influence of the objectives the final parameter set recommended corresponds to pareto front solution 1 which generated the solution map shown in fig 7b see appendix c for parameters this pareto front solution outperformed the largest number of benchmark tests and generated plausible solution maps the parameters obtained more often aligned with general understanding of land use change processes although more neighbourhood rules were included than would be for a conventional manual calibration though this was true for all solutions generated solutions that correspond to higher values of fks agreement as discussed were not recommended because based on the output maps and parameters they did not result in realistic performance 5 discussion this section considers the results obtained from the optimisation process with additional interpretation considering how the structure of the framework influences the results obtained 5 1 enhanced understanding of applied metrics as the metrics drive the optimisation process it is important to understand what impact different metrics have on the parameters and land use maps obtained with regard to the clumpiness metric this was aggregated from class level into a single objective for optimisation purposes which was achieved by taking the average consequently it is important to understand the impact this averaging has on the results obtained this was achieved by analysing the error for each class for the five solutions considered as shown in fig 14 with the colours corresponding to the different classes and the average shown by the black bar on the left hand side for each solution as shown in the figure solutions 1 and 9 have relatively equal error across each class which is captured well by the average however moving to solutions 23 36 and 50 it is observed that there is a large variation in the errors across the classes especially in solutions 23 and 36 where one class services and nature respectively has a much higher error relative to the other classes these results suggest potential improvements could be achieved by using a different aggregation strategy for converting the class level clumpiness errors into a single value for example by taking the maximum as this could potentially penalise the outputs more appropriately to generate patterns more consistent with the observed data as the approach uses multi objective optimisation the individual class level errors could also be considered as independent objectives which for this case study would require eight objectives including locational agreement and the seven clumpiness error values generating an 8 dimensional objective space with regard to the fks measure its use as a measure of locational agreement may have resulted in an over emphasis on the agreement of transitions between the simulated output and the data i e the corresponding actual land use map which is potentially not appropriately balanced with the agreement of inertia this was confirmed by assessing the solutions with the kappa and fuzzy kappa statistic see table 3 as shown in this table the values with lower fks correspond to higher kappa and fuzzy kappa values as previously mentioned all solutions outperform the fks benchmark by being greater than 0 but only solution 1 outperforms the benchmark kappa value and no solutions outperform the benchmark fuzzy kappa value generated by a growing clusters neutral model solutions 1 and 9 outperform the benchmark kappa value generated by using a random constraint match neutral model and all solutions outperform the benchmark fuzzy kappa values generated by using a random constraint match neutral model unlike for the landscape pattern structure metric where improvement could be achieved with alteration of the aggregation method the issues presented with the use of the fks statistic are inherent to the metric and cannot be adjusted without modifying the underlying calculation of the metric the fuzzy kappa or kappa statistic could potentially be used as an alternative but this would likely introduce the opposite problem of the optimisation being too heavily weighted towards inertia within the measure of locational agreement one solution may be to use both metrics as part of multi objective optimisation which would give more weight to the inertia a land use transition to the same land use class used in the calculation of fks further investigation of the metrics used is a clear avenue for further research to improve the outputs of automated parameter tuning methods based on multi objective optimisation 5 2 balancing automatic and heuristic analysis in calibration the application of the multi objective optimisation framework highlights the need to balance automatic and heuristic analysis too often metrics quantifying different aspects of the fit between simulated output and data are used exclusively to evaluate the quality of a luca model calibration method as covered in the results sections 4 2 and 4 3 evaluation can be improved with the incorporation of heuristic analysis incorporating domain knowledge to evaluate the output solution maps and parameters the heuristic analysis also provides insights into the potential improvement of the calibration framework with the incorporation of additional process knowledge though many objectively valid solutions are produced certain parameters are not necessarily consistent with those expected from process understanding the incorporation of additional process knowledge regarding certain parameters presents a clear avenue to improving the optimisation procedure by reducing the search space and computational effort required resulting in faster solution convergence and parameters that are more consistent with expectations in turn using an automatic approach also provides insights into the model structure and its behaviour certain model parameter values though inconsistent with heuristic expectation still generated results that were acceptable according to the metrics used this provided insight into what aspects of the parameterisation and model results should be captured by the objectives section 5 1 and also showed the importance of providing plausible boundaries for model parameters further work on the automatic calibration including a thorough analysis of the solutions obtained presents a path for future research to improve the understanding of luca model calibration the model itself and the interplay of factors driving land use change 6 conclusions and recommendations accurate and effective calibration of luca models is essential to allow for application to the study region of interest this paper presents an automatic calibration framework specifically designed for such luca models the framework is generic allowing for substitution of the various components e g the luca model metrics and the optimisation algorithm utilises multi objective optimisation to allow for the exploration of trade offs between the selected objectives appropriately considers the inherent stochasticity included in luca models and facilitates increased computational efficiency through its implementation the capability of the generic framework has been illustrated with an application to the randstad region of the netherlands the case study was implemented with parallel computing to reduce the parameter adjustment computing time from the order of hundreds of days to the order of days the approach generated 77 possible model parameter sets that resulted in pareto optimal solutions based on the selected objective metrics of which 50 were considered plausible following evaluation of both the simulated output and parameters against process knowledge a final parameter set was recommended the framework provided crucial insights into the influence the metrics had on driving the optimisation process and how more realistic performance could potentially be achieved with the alteration to the optimisation objectives the results are also driven by the genetic algorithm parameters used as with other applications of genetic algorithms to the automatic parameter adjustment stage of calibration of luca models e g jafarnezhad et al 2016 it would be meaningful to investigate the influence these parameters had on the output obtained though this would depend on computational resource availability the framework highlights the importance of heuristic interpretation when evaluating the result map and obtained parameters and the potential of such methods to enhance luca model understanding the demonstrated application of the framework and its generic nature show promising potential for future applications of luca models to support long term planning and policy development acknowledgements the authors wish to acknowledge the financial support from the bushfire and natural hazards cooperative research centre made available by the commonwealth of australia through the cooperative research centre program the authors also wish to acknowledge that this work was supported with supercomputing resources provided by the phoenix high performance computing services at the university of adelaide appendices appendix a confusion matrices this appendix contains confusions matrices for the data and the 6 solution maps presented in section 4 2 showing a count of land use class presence per cell between 1989 and 2000 these tables highlight the volume and type of land use transitions across the different pareto front solutions table a1 confusion matrix for data between 1989 and 2000 table a1 map 2 map 1 lu agr gre res ind ser scu nat rec air wat total agr 26128 152 559 453 12 17 641 336 12 188 28498 gre 81 349 25 9 1 2 1 1 0 5 474 res 160 3 3511 35 18 37 34 123 0 22 3943 ind 67 1 45 1205 7 10 43 28 7 65 1478 ser 3 0 32 37 68 12 6 9 0 4 171 scu 13 0 55 17 2 212 39 20 0 3 361 nat 92 1 19 26 0 3 5814 129 0 50 6134 rec 105 6 81 28 2 21 141 1229 0 44 1657 air 34 0 0 0 0 0 3 0 31 0 68 wat 110 2 23 85 0 3 56 37 0 1685 2001 total 26793 514 4350 1895 110 317 6778 1912 50 2066 44785 table a2 confusion matrix for pareto optimal solution with fks 0 173 and δclu 0 002 between 1989 and 2000 table a2 map 2 map 1 lu agr gre res ind ser scu nat rec air wat total agr 26529 41 426 447 0 0 787 268 0 0 28498 gre 0 473 0 0 0 0 0 1 0 0 474 res 0 0 3921 0 0 0 0 22 0 0 3943 ind 30 0 1 1426 0 0 1 20 0 0 1478 ser 11 0 1 1 110 0 2 46 0 0 171 scu 25 0 1 5 0 317 0 13 0 0 361 nat 64 0 0 15 0 0 5962 93 0 0 6134 rec 181 0 0 1 0 0 26 1449 0 0 1657 air 0 0 0 0 0 0 0 0 68 0 68 wat 0 0 0 0 0 0 0 0 0 2001 2001 total 26840 514 4350 1895 110 317 6778 1912 68 2001 44785 table a3 confusion matrix for pareto optimal solution with fks 0 181 and δclu 0 003 between 1989 and 2000 table a3 map 2 map 1 lu agr gre res ind ser scu nat rec air wat total agr 26499 108 475 398 0 0 807 211 0 0 28498 gre 14 406 4 27 0 0 6 17 0 0 474 res 0 0 3870 0 0 0 0 73 0 0 3943 ind 0 0 0 1448 0 0 1 29 0 0 1478 ser 15 0 1 4 110 0 2 39 0 0 171 scu 26 0 0 3 0 317 2 13 0 0 361 nat 117 0 0 15 0 0 5913 89 0 0 6134 rec 169 0 0 0 0 0 47 1441 0 0 1657 air 0 0 0 0 0 0 0 0 68 0 68 wat 0 0 0 0 0 0 0 0 0 2001 2001 total 26840 514 4350 1895 110 317 6778 1912 68 2001 44785 table a4 confusion matrix for pareto optimal solution with fks 0 190 and δclu 0 006 between 1989 and 2000 table a4 map 2 map 1 lu agr gre res ind ser scu nat rec air wat total agr 26182 150 770 381 1 13 666 335 0 0 28498 gre 26 363 17 3 0 50 1 14 0 0 474 res 252 1 3477 6 4 3 5 195 0 0 3943 ind 0 0 1 1466 1 0 0 10 0 0 1478 ser 25 0 21 8 101 0 3 13 0 0 171 scu 56 0 36 5 0 250 1 13 0 0 361 nat 21 0 19 13 1 0 6025 55 0 0 6134 rec 278 0 9 13 2 1 77 1277 0 0 1657 air 0 0 0 0 0 0 0 0 68 0 68 wat 0 0 0 0 0 0 0 0 0 2001 2001 total 26840 514 4350 1895 110 317 6778 1912 68 2001 44785 table a5 confusion matrix for pareto optimal solution with fks 0 201 and δclu 0 009 between 1989 and 2000 table a5 map 2 map 1 lu agr gre res ind ser scu nat rec air wat total agr 26145 183 946 358 0 24 634 208 0 0 28498 gre 40 323 25 6 0 73 0 7 0 0 474 res 275 5 3257 6 0 5 10 385 0 0 3943 ind 0 0 3 1466 0 1 0 8 0 0 1478 ser 9 1 24 10 110 0 2 15 0 0 171 scu 61 0 68 6 0 212 3 11 0 0 361 nat 0 0 4 1 0 0 6077 52 0 0 6134 rec 310 2 23 42 0 2 52 1226 0 0 1657 air 0 0 0 0 0 0 0 0 68 0 68 wat 0 0 0 0 0 0 0 0 0 2001 2001 total 26840 514 4350 1895 110 317 6778 1912 68 2001 44785 table a6 confusion matrix for pareto optimal solution with fks 0 210 and δclu 0 019 between 1989 and 2000 table a6 map 2 map 1 lu agr gre res ind ser scu nat rec air wat total agr 26031 203 938 365 0 22 650 289 0 0 28498 gre 69 289 31 7 0 64 2 12 0 0 474 res 357 13 3250 12 0 4 6 301 0 0 3943 ind 0 0 6 1453 0 1 0 18 0 0 1478 ser 11 4 22 6 110 0 2 16 0 0 171 scu 59 1 46 12 0 224 3 16 0 0 361 nat 0 0 14 0 0 0 6044 76 0 0 6134 rec 313 4 43 40 0 2 71 1184 0 0 1657 air 0 0 0 0 0 0 0 0 68 0 68 wat 0 0 0 0 0 0 0 0 0 2001 2001 total 26840 514 4350 1895 110 317 6778 1912 68 2001 44785 appendix b neighbourhood tail influence figures this appendix contains a set of figures corresponding to the different neighbourhood rules describing the influence different land use classes exert on the class residential for the solutions analysed in detail fig b1 neighbourhood rule tail for conversions from agriculture to residential fig b1 fig b2 neighbourhood rule tail for conversions from greenhouses to residential fig b2 fig b3 neighbourhood rule tail for conversions from industrial to residential fig b3 fig b4 neighbourhood rule tail for conversions from nature to residential fig b4 fig b5 neighbourhood rule tail for conversions from recreation areas to residential fig b5 fig b6 neighbourhood rule tail for conversions from residential to residential fig b6 fig b7 neighbourhood rule tail for conversions from socio cultural uses to residential fig b7 fig b8 neighbourhood rule tail for conversions from services to residential fig b8 appendix c final model parameterisation table c1 inertia conversion parameters table c1 from to gre res ind ser scu nat rec agr 94 62 81 10 35 30 48 96 26 93 20 45 15 88 gre 2608 65 40 30 80 86 35 52 58 04 38 79 81 79 res 58 02 4142 51 29 37 69 19 45 49 79 16 71 61 ind 2 56 45 18 2504 04 37 57 31 13 36 41 46 91 ser 25 18 31 74 43 44 4508 40 71 34 4 68 48 02 scu 28 37 29 59 4 36 7 58 4124 71 33 40 37 45 nat 72 61 81 08 37 25 70 26 38 65 2360 85 100 00 rec 69 41 32 05 49 51 46 85 3 60 89 57 2957 07 air 53 73 88 52 17 15 57 37 18 60 53 27 69 24 wat 49 51 31 98 70 93 44 79 88 21 62 55 31 23 table c2 neighbourhood rule a parameters table c2 from to gre res ind ser scu nat rec agr 179 73 704 44 720 20 374 93 86 68 657 23 680 28 gre 707 69 247 79 466 96 233 29 990 06 237 31 811 14 res 996 46 369 06 105 07 110 30 485 26 815 79 817 88 ind 127 06 184 53 137 92 234 48 768 52 487 25 56 00 ser 678 64 897 13 47 45 845 38 447 53 69 96 117 35 scu 565 31 394 57 985 62 83 81 171 47 336 22 778 66 nat 195 83 875 47 246 59 844 79 683 76 171 47 988 66 rec 283 93 85 62 663 00 302 89 691 96 602 16 613 24 air 12 20 942 51 851 11 461 78 134 85 984 62 50 08 wat 63 18 636 50 433 31 266 81 898 21 776 96 119 15 table c3 neighbourhood rule b parameters table c3 from to gre res ind ser scu nat rec agr 4 32 3 58 1 37 0 66 4 06 4 91 0 96 gre 3 21 4 47 4 62 2 02 3 33 2 32 3 58 res 3 58 1 87 4 81 4 73 4 91 3 23 1 21 ind 3 29 2 53 0 73 3 93 4 13 1 99 4 48 ser 1 54 2 32 4 10 3 30 4 03 2 41 0 19 scu 0 05 3 32 4 73 2 82 3 16 3 31 3 87 nat 2 89 1 96 1 16 3 90 4 70 3 16 2 43 rec 4 45 3 12 1 84 1 40 4 02 1 88 2 12 air 3 74 1 76 2 55 4 49 3 34 2 10 1 91 wat 4 85 3 76 3 74 3 79 1 48 3 04 4 24 table c4 accessibility parameters table c4 land use class motorway distance decay motorway weight other roads distance decay other roads weight gre 14 33 0 48 0 01 0 46 res 3 56 0 93 12 39 0 66 ind 7 00 0 38 13 42 0 59 ser 1 27 0 48 4 34 0 21 scu 7 75 0 26 10 56 0 37 nat 1 73 0 71 12 14 0 99 rec 8 40 0 19 3 20 0 13 
26457,modelling of land use change plays an important role in many areas of environmental planning however land use change models remain challenging to calibrate as they contain many sensitive parameters making the calibration process time consuming we present a multi objective optimisation framework for automatic calibration of cellular automata land use models with multiple dynamic land use classes the framework considers objectives related to locational agreement and landscape pattern structure as well as the inherent stochasticity of land use models the framework was tested on the randstad region in the netherlands identifying 77 model parameter sets that generated a pareto front of optimal trade off solutions between the objectives a selection of these parameter sets was assessed further based on heuristic knowledge evaluating the simulated output maps and parameter values to determine a final calibrated model this research demonstrates that heuristic knowledge complements the evaluation of land use models calibrated using formal optimisation methods keywords cellular automata land use model automatic calibration automatic parameter adjustment multi objective optimisation software availability name of software parallel nsgaii developer jeffrey newman contact address the university of adelaide and bnhcrc north terrace adelaide sa 5005 contact email jeffrey newman au gmail com year first available 2016 hardware software required cross platform compiles under clang visual studio and the gnu compiler chain hardware requirements dependent on land use model used program language c program size 13 mb availability and cost gpl 2 0 open source software downloadable from https github com jeffrey newman parallel nsgaii backend 1 introduction modelling of land use change plays an important role in many areas of environmental planning such as river basin management van delden et al 2007 natural area preservation hewitt et al 2014 the development of sustainable agricultural practises murray rust et al 2014a 2014b and the influence of urban dynamics on surrounding regions haase et al 2012 lauf et al 2012 to better understand the influences of land use changes models are increasingly being used as part of decision support systems to evaluate policy that influences spatial planning van delden et al 2011 to represent land use dynamics realistically such models must incorporate complex socio economic and biophysical drivers with human environment interactions lambin et al 2001 as a result land use cellular automata luca have become a popular modelling framework for evaluating land use changes as they are able to simulate the behaviour of complex systems with a high degree of realism hewitt et al 2014 historically cellular automata methods were proposed for application to geographic systems by tobler 1979 with luca models first used to replicate observed fractal patterns of urban evolution couclelis 1985 1989 batty and longley 1994 followed by their development into dynamic land use models white and engelen 1993c clarke et al 1997 much effort has been invested in developing luca models for different global regions with applications reviewed by santé et al 2010 this includes the advent of generic spatial modelling platforms sleuth clarke et al 1997 and metronamica van delden and hurkens 2011 which provide well tested models for a range of applications to different study regions with such generic platforms simplifying model development requirements significantly research focus on the calibration of luca models has increased in recent years e g blecic et al 2015 cao et al 2014 garcía et al 2013 li et al 2013 van vliet et al 2013b van vliet et al 2016 calibration of a land use change model is the process of determining a model parameter set through the initial setting of model parameters the iterative adjustment of these parameters based on comparison of the model output with observations and the selection of a final parameter set for application to a specific case for long term scenario analysis adapted from van vliet et al 2016 the iterative adjustment stage of calibration of luca models is extremely complex as land use change is a path dependent process that is driven by multiple interdependent processes with uncertain outcomes brown et al 2005 conventionally this stage of calibration of luca models is manual van delden et al 2012 incorporating the modellers process understanding to address this inherent uncertainty however implementation of such methods is time consuming subjective jafarnezhad et al 2016 and lacks transparency and repeatability garcía et al 2013 consequently in order to make parameter adjustment more efficient and repeatable there has been an increasing focus on automating this process van vliet et al 2013a automatic parameter adjustment methods generally make use of formal optimisation methods that maximise model performance metrics blecic et al 2015 cao et al 2014 garcía et al 2013 li et al 2013 consequently the success of these methods relies heavily on the ability to assess performance in a quantitative fashion this assessment has to consider two separate properties of luca model performance i locational agreement alternatively termed cell by cell agreement hagen zanker 2009 which is the match of pixels between simulated outputs and the corresponding observed data van vliet et al 2013a hagen zanker 2009 and ii landscape pattern structure which is the inferred realism of land use change processes captured by the difference between observed and simulated landscape patterns engelen and white 2008 consequently automatic parameter adjustment of luca models can be considered a multi objective optimisation problem hagen zanker 2008 at present multi objective optimisation has only been applied to the parameter adjustment stage of calibration of luca urban growth models that are implemented with two land use classes despite the capacity of these models to consider a broader range of land use classes using the sleuth metrics trunfio 2006 or logit regression model fitness functions cao et al 2014 whilst this work has merit in characterising urban and non urban interactions it represents a less complex calibration problem than luca models that consider multiple dynamic land use classes as these are more complex models that possess a significantly larger number of parameters for calibration in contrast studies that have used optimisation approaches for the calibration of luca models with multiple dynamic land use classes have only considered a single objective for example blecic et al 2015 considered only the locational agreement element of performance for parameter tuning and while garcía et al 2013 used both locational agreement and landscape pattern structure metrics these were combined into a single objective during the optimisation process where as a result important trade offs between locational agreement and landscape pattern structure could not be examined both studies also generated only one possible model parameterisation for future scenario analysis limiting the ability to understand how calibrated parameters are potentially influenced by the metrics used for optimisation to address these shortcomings the objectives of this paper are i to present a multi objective optimisation framework that automates the parameter adjustment stage of calibration of luca models with multiple dynamic land use classes enabling the identification of multiple model parameter sets that could be suitable for long term scenario analysis and ii to demonstrate the application of the framework on the case study comprising the randstad region in the netherlands the remainder of this paper is organised as follows the proposed multi objective optimisation based calibration framework is introduced in section 2 followed by a description of an application to a case study of randstad in section 3 the results for the case study are presented and discussed in sections 4 and 5 the conclusions and recommendations of this work are presented in section 6 2 proposed multi objective optimisation based calibration framework the proposed multi objective optimisation framework for calibration of luca models with multiple dynamic land use classes is presented in fig 1 as shown the framework is comprised of four stages first in the selection stage the components required for optimisation are chosen next in the specification stage to ensure an efficient and robust output certain aspects prevalent to the previously selected components are specified following this the multi objective optimisation parameter adjustment is implemented and run to completion finally the resulting model outputs are assessed quantitatively evaluated using a neutral model followed by heuristic interpretation of the outputs to decide on a final model parameter set 2 1 selection stage in the selection stage the four main components for optimisation are chosen as shown in fig 1 the luca model to be used and the parameters to be adjusted the optimisation algorithm used for the parameter adjustment process and the map comparison metrics used to assess model performance 2 1 1 model and parameters the luca model determines the number and type of parameters that require adjustment the parameters within luca models are used to capture the processes that influence land use changes such as the physical suitability of the landscape and the influence different land use classes exert on each other the consideration of multiple dynamic land use classes as is the case for transition potential models derived from white and engelen 1993c introduces a large number of parameters that must be adjusted to capture the respective influences of each process on different land use classes garcía et al 2013 the most notable example of parameters for transition potential models are neighbourhood rules which characterise the influence different land use classes exert on each other at different distances riks 2015 for example considering a neighbourhood size of eight cells introduces 30 parameters for each neighbourhood rule as the number of neighbourhood rules is the product of the total number of land use classes and the number of actively allocated land use classes the resulting number of parameters that could be adjusted is large consequently it is desirable to be judicious about which parameters to include in the automatic adjustment process there are several approaches for selecting which parameters to include in the automatic adjustment process including empirical understanding of the region of interest in order to select the parameters that correspond to the dominant processes driving land use change hewitt et al 2014 and the use of quantitative analysis methods such as data mining or sensitivity analysis li and yeh 2004 wang et al 2011 gibbs et al 2012 alternatively the computational burden associated with the automatic adjustment process can be reduced by using empirical methods to identify reasonable values of certain parameters such as neighbourhood rules van vliet et al 2013a which can then be used to initialise the optimisation at a good solution which has been shown to reduce the computational demands of optimisation problems in other fields e g bi et al 2016 in addition to the number of parameters that require adjustment the size of the space of potential parameters values that need to be explored during the automatic adjustment process which is commonly referred to as the search space is also affected by the potential ranges different parameters can take maier et al 2014 consequently the upper and lower limits of the parameters that are included in the automatic calibration process need to be selected carefully balancing the needs to ensure the best possible combination of parameter values can be identified with the desire to reduce the size of the search space the size of the search space can also be reduced by restricting the values parameters can take to be in accordance with underlying domain knowledge for example neighbourhood effects can be parameterised to generate shapes consistent with discursive knowledge hagoort et al 2008 reducing the number of parameters from the order of thousands to hundreds blecic et al 2015 garcía et al 2013 2 1 2 multi objective optimisation algorithm as part of the proposed framework it is suggested a population based metaheuristic algorithm is used for the automatic parameter adjustment process such algorithms are advantageous because they are able to find near globally optimal solutions for highly complex e g non linear non convex problems and it is straightforward to link them with existing simulation models such as luca models without the need for problem simplification maier et al 2015 this is because they work in an iterative fashion using a population of separate solutions where the model parameters are adjusted for each member of the population based on the search strategy of the algorithm under consideration e g survival of the fittest in the case of genetic algorithms or the foraging behaviour of ants in their search for food in the case of ant colony optimisation and how well these adjustments perform is evaluated by running the model with the altered parameters information on how well the suggested changes have performed are fed back to the algorithm informing which changes are made to the model parameters in the next iteration or generation and so on in addition metaheuristic algorithms have the capacity to handle multiple objectives maier et al 2014 enabling them to optimise measures of locational agreement and landscape pattern structure simultaneously there are a number of population based metaheuristics kingston et al 2008 which can be used to automate luca model parameter adjustment utilising various heuristic mechanisms with heuristic selection mainly due to preference applied algorithms include ant colony optimisation liu et al 2012 memetic algorithms veerbeek et al 2015 particle swarm algorithms blecic et al 2015 feng et al 2011 and genetic algorithms cao et al 2014 garcía et al 2013 li et al 2013 trunfio 2006 clarke lauer and clarke 2011 the proposed framework caters to different population based metaheuristics but requires an algorithm that can handle multiple objectives to allow for exploration of the trade offs between the objectives 2 1 3 map comparison metrics as mentioned previously optimisation requires quantifiable objectives to assess performance consequently the proposed framework requires the selection of metrics that can quantify the objectives of maximising locational agreement and minimising landscape pattern structure error for locational agreement potential metrics include percentage correct santé et al 2010 the figure of merit pontius jr et al 2008 allocation and quantity agreement pontius jr and petrova 2010 or cohen s kappa or one of its variations van vliet et al 2011 van vliet et al 2013a there are also many landscape pattern structure metrics mcgarigal 2014 that quantify different aspects of the landscape and have been applied previously in land use modelling studies these include amongst others edge density garcía et al 2013 largest patch index li et al 2013 and clumpiness van delden et al 2012 2 2 specification stage as shown in fig 1 this stage follows the selection stage and requires the specification of certain relevant components of the model and optimisation algorithm to allow for effective optimisation this includes how model stochasticity is taken into account and the parameterisation of the selected optimisation algorithm 2 2 1 model stochasticity luca models generally include a stochastic element to capture the variability of human decisions that drive land use changes by including a random perturbation factor in the model this stochasticity must be considered appropriately to gain meaningful optimisation results in order to achieve this guo et al 2017 recommend using a number of model runs with different random seeds but to use the same seeds during each iteration of the optimisation in order to ensure the impact of changing model parameters from one optimisation iteration to the next is not diluted or confused by the stochastic nature of the model used to assess the objective function values therefore as part of the proposed framework n luca model runs with different random number seeds are used to assess model performance for a given set of model parameters during each iteration of the automatic parameter adjustment optimisation process however the same random number seeds are used in every iteration of the optimisation process to ensure that any changes in the objectives from one iteration to the next are due to changes in model parameters rather than a combination of these changes and randomness in luca models 2 2 2 optimisation configuration three aspects of the optimisation process must be specified prior to running the optimisation algorithm first values of the parameters that influence optimisation algorithm searching behaviour such as population size probability of cross over and probability of mutation in the case of genetic algorithms must be defined zheng et al 2016 zecchin et al 2012 it should be noted that these parameters are different from the parameters of the luca models that are to be determined with the proposed automatic parameter adjustment process second termination criteria for the optimisation process such as a pre defined number of iterations or no significant improvement in performance must be specified finally the number of times the entire optimisation process has to be repeated r has to be specified such repetition is needed due to the stochastic nature of population based metaheuristics and increases the chance that the best possible combination of luca model parameters is identified and that the results of the automatic parameter adjustment process are robust it should be noted that the stochastic nature of the optimisation process is distinct from the uncertainty associated with the luca models while the luca model is run n times during each iteration of a single optimisation run in order to account for the stochasticity of luca models the entire optimisation process is repeated r times to account for the stochasticity of the metaheuristic optimisation process 2 3 adjustment stage 2 3 1 optimisation process the purpose of this stage of the proposed framework is to use the selected optimisation algorithm see section 2 1 2 to identify the combinations of luca model parameters see section 2 1 1 that provide the best possible trade offs between the selected performance metrics see section 2 1 3 as mentioned previously this is achieved in an iterative fashion by using a metaheuristic optimisation algorithm to determine changes to the model parameters at each generation of the optimisation process based on resulting changes in the performance metrics the steps in this process are shown schematically in fig 1 and in more detail in fig 2 as can be seen in fig 2 given that population based metaheuristics are used for optimisation a population of p solutions p sets of luca model parameter values is generated for each iteration of the optimisation process next a luca model with each of these p sets of parameter values θ the parameters selected for optimisation input into the model is run from time t 0 until time t n with an actual land use map at time t 0 providing the initial conditions for each of the p model runs fig 1 each simulated output map at time t n is quantitatively compared with a corresponding map of actual land use using the metrics in the comparison set selected previously one to assess locational agreement and another to assess the error of landscape pattern structure as mentioned previously given the stochastic nature of land use change each of the p land use models is run with n different random number seeds generating n simulated land use maps each of which is individually compared with the corresponding map of actual land use the performance of each model with the p sets of selected model parameters is quantified by taking the average of the metric values across the n different outputs figs 1 and 2 hence the final metric value for one member of the population of the metaheuristic is given by the following objective 1 l a p θ 1 n i 1 n l a i θ where la p is the locational agreement objective function value of the member of the population p θ are the input parameters corresponding to that member of the population and n is the number of luca model random number seeds being considered similarly for landscape pattern structure 2 l p s p θ 1 n i 1 n l p s e r r o r i θ where lps p is the landscape pattern structure error objective function value of the member of the population p and n is the number of luca model random number seeds as mentioned previously when metaheuristic optimisation algorithms are used θ values are optimised in an iterative fashion with the aim of improving the performance metrics from one generation to the next consequently the above steps are repeated for a total of g generations fig 2 until the desired stopping criteria have been met see section 2 2 2 finally as also mentioned previously each optimisation run is repeated r times to account for the stochastic nature of the searching behaviour of population based metaheuristics as parameter adjustment is a multi objective optimisation problem the optimisation process does not generate a single set of model parameters rather the optimisation generates a series of model parameter sets that generate optimal non dominated trade offs between the two objectives the output of the optimisation is shown in fig 1 by the curve called the pareto front of objectives the pareto front indicates the optimal trade off between objectives where improved performance in one objective cannot be achieved without inferior performance in the other objective 2 3 2 implementation the proposed optimisation process is very computationally expensive because of the relatively long simulation times associated with running luca models e g run times can vary from 10 s to 10 min depending on the number of classes spatial extent and resolution and the large number of simulations required as illustrated in fig 2 for example for typical values of p 200 n 10 g 500 and r 5 the luca model would have to be run 200 10 500 5 5 000 000 times resulting in a total run time ranging from 50 000 000 to 3 000 000 000 s or 600 to 35 000 days for individual luca model run times of 10 and 600 s respectively which is not feasible from a practical perspective however as has been discussed in other problem domains there are a number of avenues for increasing the computational efficiency of this process including the incorporation of heuristic information to reduce the size of the search space szemis et al 2012 and section 2 1 1 or to improve the efficiency of the optimisation process nguyen et al 2016 by using meta models as surrogates for the computationally expensive luca models broad et al 2015 or the use of parallel computing resources blecic et al 2015 by using these approaches automatic parameter adjustment methods can be made feasible for application to complex luca models 2 4 assessment stage to determine the final calibrated model the resultant model parameter sets corresponding to different pareto front solutions are assessed using a comprehensive set of methods that have been used in similar calibration studies van vliet et al 2013b hewitt et al 2014 including assessment of objective output using a neutral model benchmark test and a final evaluation comprising heuristic interpretation of the solutions and parameter values obtained 2 4 1 neutral model benchmark test an issue with calibrating luca models is the impact of boundary conditions due to land use commonly persisting metrics often indicate good model performance though this can largely be due to limited variation of the final landscape from the initial situation hagen zanker and lajoie 2008 hence the proposed framework uses a neutral model benchmark test to evaluate the output in the objectives space to determine which model parameter sets have captured land use change processes appropriately this is illustrated by the black lines over laid on the pareto front in the assessment stage in fig 1 a model that outperforms the benchmark i e generates superior metric values is considered valid because model performance can be attributed to correct capture of processes rather than inherent land use persistence hagen zanker and lajoie 2008 2 4 2 final evaluation a final evaluation of the optimisation results is performed by assessing the solution and parameter spaces fig 1 this can be performed by a modelling expert or in a participatory manner where stake holders are included in the evaluation to decide on the most appropriate model as with manual calibration hewitt et al 2014 evaluation of the solution space refers to interpreting the land use maps generated by the different model parameter sets corresponding to different solutions along the pareto front evaluation of the parameter space refers to assessing the parameters corresponding to the different solutions to determine if these are consistent with expected land use change processes of the study region by doing so a final model parameter set can be recommended 3 case study application of proposed framework the proposed framework outlined in section 2 is applied to a real world case study of the randstad region in the netherlands from 1989 to 2000 randstad is a conurbation of the four largest cities in the netherlands and the surrounding region the model for the case study region has a 500 metre resolution and covers a spatial extent of 14 175 km2 shown in fig 3 3 1 selection stage 3 1 1 land use model the transition potential luca model metronamica is used van delden and hurkens 2011 metronamica is a generic modelling platform evolved from the pioneer model by white and engelen 1993c that facilitates direct application to the study region of interest capturing the dynamics between urban and regional systems riks 2015 which is able to consider multiple dynamic land use classes it has been used successfully in numerous decision support systems in diverse global regions such as the regional and national socio economic policies in the netherlands engelen et al 2003 long term regional planning in waikato new zealand rutledge et al 2008 river basin management for mediterranean watersheds van delden et al 2007 assessment of the impact of intra urban land prices furtado et al 2012 impact assessment of agricultural policies in europe van delden et al 2010 and modelling shifting cultivation practises in sri lanka wickramasuriya et al 2009 in metronamica a transition potential is used to allocate land use classes calculated to update land use at each time step using various factors to define the potential for each cell to support each land use class considered land use classes are allocated until all demand requirements are met demands for particular land uses are exogenously defined based on the transition potential for each given class or until there are no locations available metronamica considers three land use class categories active which are dynamically modelled and allocated commonly urban classes features which are fixed land uses that influence landscape dynamics but do not have dynamic behaviour such as airports or water bodies and passive which occupy the remaining landscape and change in extent as a result of changes to the other categories these are normally occupied with land use types that have low transition costs for example agricultural land the metronamica model developed for the randstad case study has ten land use classes there are seven active greenhouses residential industrial services socio cultural uses nature and recreation areas one passive agriculture and two features airport and water to determine the transition potential metronamica considers four processes accessibility the provision of infrastructure required for certain land use classes suitability the influence of physical factors neighbourhood effects the spatial interactions between land use classes representing the behaviour of actors and activities taking place on those classes and zoning the influence of spatial planning a stochastic component is also included to incorporate the inherent uncertainty of land use change decisions the transition potential is calculated by 3 t p c k a c k s c k n c k z c k where tp c k is the transition potential for the cell c to support land use class k subscripts for all symbols above have the same interpretation a c k is the accessibility s c k is the suitability n c k is the neighbourhood effect z c k is the zoning influence and γ is the stochastic component the form of which is given in the metronamica documentation riks 2015 this documentation also includes further details of the transition potential and underlying equations for each process the metronamica model developed for randstad includes two accessibility layers motorways and other roads four physical attributes are included for suitability noise elevation natural hydrology and slope zoning for ecological corridors the noise contour for schiphol and urban expansion plans are also included 3 1 2 parameters as the case study is a region of urban growth with a short time span the main sources of land use conversion will relate to the expansion of the existing socio economic land uses hence the major processes driving land use changes will be the self organising behaviour of the system for increased expansion of the urban cores couclelis 1989 batty and longley 1994 white and engelen 1993a 1993b as a result the most important processes to include for tuning are those driving this behaviour the neighbourhood interactions and accessibility verburg et al 2004 consequently parameters for neighbourhood rules and accessibility are included for automatic tuning neighbourhood rules are parameterised to define the influence that different land use classes exert on each other the cumulative transition potential due to neighbourhood influence for the conversion from one land use to another is calculated by 4 n c k c d c w k k c d c c where n c k is the neighbourhood transition potential for cell c transitioning to land use type k d c is the set of all cells in the neighbourhood of the cell of interest c k c is a look up function that returns the land use class for cell c d c c is the distance between cells c and c and w k j x expresses the influence that a cell with land use class j returned by the look up function k c exerts on a cell of potential land use class k at a linear distance of x between the two cells accessibility defines the importance of infrastructure elements for different land use classes parameterised as 5 a c k a s k d s c a s k if a s k 0 0 if a s k 0 1 a s k d s c a s k otherwise where d s c is the cellular distance between cell c and the nearest cell containing the type of infrastructure s and a s k is the accessibility decay parameter that expresses the importance of the type of infrastructure s to land use k values for a s k and a weighting parameter are tuned for each actively allocated land use class k and each infrastructure type s for this case study 28 parameters are tuned for accessibility because there are 7 actively allocated land use classes and 2 types of infrastructure layers motorways and other roads to reduce the size of the search space during the automatic parameter adjustment stage the neighbourhood rules are delineated into two specific parts as shown in fig 4 the locus point dot which defines the inertia self influential rules or conversion interactive rules of the location of interest and the tail line which defines the interaction effects for different distances there are several common shapes that the neighbourhood rule tails can take van vliet et al 2013b hagoort et al 2008 the most common forms are characterised by a high influence at shorter distances a point of inflection and a slow gradually decaying influence over large distances to capture such dynamics this research parameterises neighbourhood effect tails using exponential decay functions thereby reducing the number of parameters that require calibration of the form 6 y x c for x 0 a e b x for 0 x x c 0 for x x c where a and b are the controlling parameters of the neighbourhood rule x is the distance y x is the influence value c is the locus point of persistence and conversion and x c is the critical distance where the influence is set to zero for each neighbourhood rule three parameters are tuned a b and c for this case study there are 10 land use classes and 7 dynamic land use classes hence 70 neighbourhood rules are included for adjustment by parameterising neighbourhood rules using exponential decay functions 210 parameters are tuned for neighbourhood rules hence a total of 238 parameters are tuned as part of the adjustment process for this case study 3 1 3 multi objective optimisation algorithm this research uses the non dominated sorting genetic algorithm ii nsga ii first proposed by deb et al 2002 which is a population based multi objective genetic algorithm moga regarded as an industry standard wang et al 2015 with a demonstrated ability to tune luca model parameters cao et al 2014 trunfio 2006 the moga is used to find multiple alternative luca parameter sets that represent the best possible trade off between the calibration objectives the pareto front of solutions illustrated in fig 1 by the curved blue line mogas use simple heuristics computationally efficient rules across a number of generations to derive information about which decision variable values result in better performing objectives and use this information about the present parent population to generate the next child generation of solutions some of which are likely to have superior objective function values the heuristics mimic those of survival of the fittest and are referred to as selection cross over and mutation selection is used to promote better performing solutions to the subsequent generations by comparing different solutions and selecting by some mechanism those which perform better cross over takes a subset of decision variable values from a pair of parent solutions and randomly recombines them to form new child solutions the purpose is to exploit good solutions as better performing decision variable values from the parents when combined differently may result in children with superior performance this is governed by the probability of cross over which is set prior to optimisation mutation takes a small subset of decision variable values in a child and perturbs them the purpose of which is to diversify the search to explore a wider possible range of solutions which might lead to superior objective function values the specific moga operators employed by the nsga ii as well as the additional features that distinguish it from conventional mogas are illustrated in fig 5 as shown nsga ii begins with an initialisation step where an initial population of solutions with randomly generated decision variable values is created the associated objective function values are then calculated next the standard genetic operators previously discussed selection cross over mutation are used to generate a child population from the existing parent population this implementation of nsga ii uses tournament selection simulated binary crossover and polynomial mutation as implemented by deb et al 2002 because this problem uses real value decision variables the key performance advantage of using nsga ii is achieved via the additional operators implemented in the algorithm which preserve non dominated i e the best performing solutions from generation to generation as shown in fig 5 nsga ii recombines the parent p and child q populations and ranks the combined population in order of non dominated fronts that occur which are a series of pareto fronts from this a new parent population p is formed by iteratively selecting the solutions belonging to the best fronts if only a subset of solutions from a front can be included crowding distance sorting is used which aims to select the most diverse set of solutions from the final front that is included this process is repeated until the stopping criteria are met 3 1 4 map comparison metrics for the case study locational agreement is measured using fuzzy kappa simulation fks developed by van vliet et al 2013a the fks metric is an adaptation of cohen s kappa santé et al 2010 van vliet et al 2016 which measures the fuzzy agreement of location and class between two data sets relative to a baseline of random allocation 7 f k s θ p o θ p e θ 1 p e θ where po θ is the observed agreement the similarity of the simulated land use transitions with the observed land use transitions and pe θ is the expected agreement the agreement obtained from a random allocation of the given class transitions relative to the initial land use map fks is used because it measures the agreement of land use transitions and hence includes an implicit baseline it also uses fuzzy interpretation which attributes partial agreement relative to proximity in this research partial locational agreement of correct transitions is considered for a neighbourhood radius of two cells with strength of agreement using a halving distance of one hence the locational agreement measure for a single member of the population la p θ is equivalent to fks agreement averaged across the total number of random number seeds considered to assess the error of landscape pattern structure the average of the absolute class level clumpiness error between actual and simulated values is used this metric is used based on successful previous applications as a part of the manual parameter tuning of luca models van delden et al 2012 clumpiness is an aggregation metric that measures the proportional deviation of the proportion of like adjacencies involving the corresponding class from what is expected under a spatially random distribution mcgarigal 2014 it is calculated first by determining the proportion of like adjacencies 8 g k g k k l 1 l g k l min e k where g k is the proportion of like adjacencies for land use class k g kl is the count of like adjacencies between patches of class k and l using the double count method mcgarigal 2014 l is the total number of land use classes min e k is minimum perimeter of a maximally clumped patch of class k defined as 9 min e k 4 n 4 n 2 4 n 4 n 2 a k n 2 a k n 1 n a k n 1 n where a k is the area of class k in terms of number of cells and n is the length of a side of the largest integer square possible with a smaller area than a k with the proportion of like adjacencies calculated it follows that clumpiness is calculated by 10 c l u m p y k g k p k p k g k p k 1 p k for g k p k and p k 0 5 otherwise where p k is the proportion of the landscape occupied by patch type class k as clumpiness is calculated at the class level it is aggregated by taking the average across the dynamic land use classes hence for this case study landscape pattern structure error is given by the average error between the observed and simulated clumpiness values δclu for the seven actively allocated land use classes averaged across the total number of random number seeds considered 3 2 specification stage 3 2 1 model stochasticity stochasticity in the metronamica model is accounted for by calculating the map comparison metrics using ten stochastically generated replicates of simulated land use maps for each iteration of nsga ii i e n 10 in accordance with equations 1 and 2 3 2 2 optimisation configuration although it is generally recommended to fine tune the parameters controlling the searching behaviour of the optimisation algorithm such as the population size the probability of mutation and probability of cross over via sensitivity analysis before application for large problems this is extremely computationally expensive and therefore impractical wang et al 2015 as this research falls into this category of problems recommended nsga ii parameter values are adopted zheng et al 2016 wang et al 2015 namely a probability of crossover of 0 9 and a probability of mutation equalling the inverse of the number of decision variables 0 0042 in this case the selection of population size which corresponds to how many chromosomes would be included per generation is also informed by the work of wang et al 2015 into optimisation problems with many decision variables their results show that the population size for large problems must be greater than the number of problem decision variables to find non dominated solutions hence this research uses a population size of 256 i e p 256 the stopping criterion adopted is based on solution convergence and reaching a certain number of generations the required number of generations used is 500 i e g 500 convergence implies that for the given optimisation algorithm configuration negligible improvement is obtained in the objective space with continued optimisation there are numerous metrics for measuring convergence maier et al 2014 this research uses the hyper volume metric zitzler 1999 a commonly employed metric of multi objective performance reed et al 2013 hadka and reed 2012 that captures convergence and diversity of the objective space the solutions are considered to have converged when there is negligible improvement in the hyper volume for 50 generations defined as less than 0 5 improvement in the hyper volume metric the results for all optimisation runs show that the use of 500 generations is sufficient for hyper volume convergence corresponding to a 0 5 change given the computational demands of the framework five different optimisation seeds are used i e r 5 following the convergence of the five optimisation runs the pareto optimal solutions are combined to identify the best overall front as is commonly done in multi optimisation studies e g wu et al 2012 3 3 adjustment stage 3 3 1 implementation for the proposed case study running the optimisation on an intel i5 347os cpu core processor a single evaluation took approximately 6 s i e a single run of the metronamica model consequently extrapolating this out to a complete optimisation run the total computational time would be approximately p n g r 6 256 10 500 5 6 444 4 days hence to make the optimisation process feasible from a practical perspective a parallelised version of nsga ii is implemented to increase computational efficiency this uses the phoenix cluster a high performance computing facility operated by research services at the university of adelaide the separate optimisation runs are parallelised by distributing the 256 evaluations one for each member of the population over 129 cpu processing cores across eight computational nodes for the phoenix cluster each computational node consists of 2 xeon e5 2698v3 chipsets each containing 16 cores to control the parallelisation a master slave model of computation is implemented wherein 128 slave processes running on separate cores are used in parallel to evaluate the objective functions for separate members of the population and the 129th is the master process that coordinates the search and runs the recombination that is the selection crossover mutation non dominated sorting and crowding distance operators parallelisation is achieved using the message passing interface mpi using asynchronous communication to improve algorithm efficiency using mpi the master process passes messages containing a set of decision variable values a luca model parameter set to each of the slaves upon receiving this message the slave runs the luca model evaluates the calibration objectives and passes a message back to the master containing these objective values through this allocation near linear speedup is achieved as communication time is orders of magnitude lower than time taken to evaluate the calibration objectives with nsga ii completing 500 generations within 48 h code in c for this implementation of the nsga ii is fully open sourced and available as per the details in the software availability section of this paper 3 4 assessment stage 3 4 1 benchmark test the benchmark test is conducted by applying two neutral models the growing clusters van vliet et al 2013a and random constraint match riks 2010 neutral models which generate maps of the region according to two different growth strategies as fks includes an inherent baseline by only considering transitions van vliet et al 2013a a benchmark value of 0 is used for fks neutral model benchmark testing is used to evaluate solutions with respect to the landscape pattern structure error for consistency the clumpiness of the active classes the optimisation objective from five different growing cluster neutral model outputs is averaged and the error between this and the observed clumpiness for the year 2000 taken as the benchmark landscape pattern structure metric value outputs of the optimisation routine with less error than this value are assumed to have captured model processes adequately and are considered for the final evaluation 3 4 2 final evaluation a final evaluation of the model parameter sets is conducted by reviewing the solution maps and model parameters obtained after application of the benchmark test in conjunction with an experienced land use modeller through a collaborative evaluation of these solutions a final model parameter set is decided upon based on heuristic evaluation combining understanding of both the model and study region 4 results this section presents and evaluates the results of the multi objective optimisation framework application to the randstad case study this comprises three parts as shown in fig 1 first the resultant pareto front is evaluated to assess the trade off found between the metrics and the number of solutions that pass the benchmark test second the solution space is assessed by heuristic evaluation of the simulated output maps third the parameter space is evaluated through heuristic interpretation of the trends in the parameter values observed across the pareto front as well as the interaction between different parameters the results are not compared with a more common manual calibration method due to the complexity and highly subjective nature of such an approach 4 1 identified pareto optimal solutions the pareto front comprised of non dominated solutions from the five optimisation runs is shown in fig 6 in total 77 pareto optimal solutions were identified each corresponding to a model parameter set that generated non dominated metric values the output metric values are coloured for each parameter set with metric values for 10 independent simulations shown for each illustrating the influence of simulation stochasticity red points correspond to parameter sets that generated a low δclu and low fks agreement blue points correspond to parameter sets that balance the objectives and pink points show parameter sets that resulted in a relatively high δclu and a relatively high fks agreement the black points show solutions that were analysed in further detail to determine a final model parameter set see section 4 2 as shown in fig 6 there was a meaningful trade off between the two objectives used values of the average absolute clumpiness error across the actively allocated land use classes varied from 0 001 to 0 067 and values of fks ranged from approximately 0 169 to 0 222 with a reasonably uniform distribution of points in between the extremes the solutions generated corresponded with the more commonly used kappa values between 0 758 and 0 789 comparable with other calibration studies cao et al 2015 chaudhuri and clarke 2013 garcía et al 2013 jafarnezhad et al 2016 to filter the pareto front solutions based on objective performance following the proposed framework performance was compared to the two baseline models presented in section 3 4 1 the two horizontal dashed black lines show the benchmark metrics values calculated for average absolute clumpiness error where the line of δclu 0 060 corresponding to the random constraint match benchmark metric value and the line of δclu 0 018 corresponding to the growing clusters benchmark metric value of the parameter sets obtained 74 resulted in superior performance to the random constraint match neutral model and 50 resulted in superior performance to the growing clusters neutral model all solutions were considered valid with respect to fks as all had fks agreement values greater than 0 fig 6 also shows the importance of considering model stochasticity as part of the automated adjustment process the independent simulation objective values vary considerably from the average performance value shown by the black line with a noticeable increase in the spread of the independent solutions from the average for pareto front solutions with higher fks values this variation is attributed to the influence of the stochastic factor which characterises the degree of randomness that is built into the land use allocation for each model realisation see transition potential definition in equation 3 this highlights the need to consider model stochasticity appropriately when using automatic parameter tuning methods as not doing so would result in over calibration to a single model realisation 4 2 simulated output maps following the assessment of the obtained pareto front this section presents an evaluation of a subset of the pareto front solutions obtained following benchmark testing the simulated output maps generated by certain parameter sets corresponding to the selected pareto front solutions shown by the black dots in fig 6 were assessed to obtain a better understanding of the nature of the trade off between the two objectives by considering how the trade off is reflected in the simulated output maps five pareto front solutions were evaluated with metric values given in table 1 the selected solutions correspond to parameter sets that resulted in the lowest average values of fks agreement and lowest average values of δclu the highest average value of fks agreement with the highest average values of δclu that passed the benchmark test and three intermediate solutions approximately equally spaced with respect to the map comparison metrics the corresponding simulated output maps are presented in fig 7 for each figure an enlarged version of the area in the black square in the left hand panel is shown in the right hand panel to highlight an area of significant change the variation between the simulated output maps can be largely attributed to the amount of interspersion observed between the different classes the simulated output maps for parameter sets that achieved superior δclu performance shown in fig 7b and c exhibit larger clusters of the socio economic land use classes such as residential red industrial purple and recreation areas dark yellow the simulated output maps for parameter sets that achieved superior fks performance such as fig 7d e and 7f exhibited an increasingly large amount of interspersion of these land use classes the most notable examples can be seen with the interspersion of recreation areas amongst clusters of residential land use throughout the map also shown in fig 7d 2 7e 2 and 7f 2 there is also a growing presence of socio cultural uses pink interspersed amongst greenhouses orange for these simulated output maps and a large cluster comprised of greenhouses and residential land uses that grows with improving fks performance from visual inspection of the simulated output maps and comparison with the data certain solutions could be rejected despite passing the benchmark test due to the patterns generated being too inconsistent with the data for example the solutions corresponding to maps shown in fig 7d e and 7f are considered unsuitable due to the over interspersion of the socio economic land use classes which are not observed in the data it appears that the use of fks placed an over emphasis on generating transitions which resulted in the patterns observed in the simulated output maps being inconsistent with the data to evaluate this the kappa and fuzzy kappa metrics more commonly used locational agreement metrics in calibration studies e g cao et al 2015 chaudhuri and clarke 2013 garcía et al 2013 wickramasuriya et al 2009 hagen zanker 2009 that consider the entire landscape were calculated values between 0 758 and 0 789 for kappa and 0 828 and 0 854 for fuzzy kappa were obtained for the solutions generated for kappa a benchmark value of 0 789 was calculated for the growing clusters neutral model which is outperformed by only one solution and 0 774 for the random constraint match neutral model which is outperformed by 19 solutions which corresponded to solutions that generated lower fks values for fuzzy kappa a benchmark value of 0 869 was calculated for the growing clusters neutral model which is not outperformed by any of the solutions and 0 828 for the random constraint match neutral model which is outperformed by 71 solutions such performance with fewer solutions outperforming baseline metric values is consistent with the over emphasis on the agreement of transition cells with the use of the fks metric to measure locational agreement 4 3 parameter variation along pareto front this section presents an analysis of the parameters corresponding to the different pareto front solutions coloured by solutions as shown in fig 6 to evaluate the interplay between different parameters and determine if this is consistent with process understanding and to provide insight into how the preference for the different objectives was achieved through different parameter value settings given the large number of parameters in the luca model that were tuned 238 key parameters corresponding to the pareto front solutions evaluated in section 4 2 have been selected for detailed consideration the inertia parameters for each land use class the conversion parameters for transitions to the class residential the tails of the interaction rules for transitions to the class residential the interactions conversion parameters and tails between greenhouses and socio cultural uses and the distance decay parameters for the accessibility fig 8 shows the inertia parameter values the c value in equation 6 for the self influence rules ordered and coloured by the pareto front solution as can be seen there are trends in the parameters generated by the optimisation process for the land use classes services ser and socio cultural uses scu the optimisation process resulted in high influence values relative to the values for the other land use classes across all five solutions evaluated hence it can be concluded that both objectives where optimised when these land use classes were relatively inert for the other classes the optimisation process generated a residential res inertia value that was relatively high for solutions that favoured δclu i e solutions 1 and 9 whereas the optimisation process resulted in all other classes having a relatively low inertia value that was relatively constant between classes however for solutions that balance both objectives solution 23 the inertia for residential drops to a relatively low value whilst the classes industry ind nature nat and recreation areas rec jump to relatively higher values a trend that persists for solutions favouring fks solutions 36 and 50 this suggests that to generate better fks values there must be more transition from the class residential this is reflected in data and confusion matrices for each solution appendix a however such a result is in conflict with common understanding of land use dynamics where the large initial investment required for urban classes such as residential van vliet et al 2013a means socio economic land use classes generally have relatively high inertia values given the high associated transition costs additionally the high inertia of the class nature also conflicts with common understanding of land use dynamics as natural areas are more often preserved due to zoning regulations e g van delden et al 2007 as opposed to being inert as shown in fig 8 hence the pareto front solutions analysed exhibit certain inertia values that are consistent with expectation such as residential for solution 1 and industry for solution 50 other inertia values such as the industry for solution 1 as well as residential and nature for solution 50 are not fig 9 shows the values of the conversion parameter the c value in equation 6 for the interaction rules for different land use classes converting to the class residential ordered and coloured by the pareto front solution generally solutions favouring fks exhibited higher conversion values to generate more conversions to the class residential which were exhibited in the data see appendix a the trends shown in fig 9 were also found for other conversion parameters as shown the optimisation process resulted in certain conversion parameters that did not exhibit any particular trend across the solutions for example the conversion from industry to residential which has a relatively constant value for solutions 1 and 9 that favour δclu and solutions 36 and 50 that favour fks but a negative value for solution 23 a trend that was observed was that the optimisation process generated parameters that changed sign from negative to positive as pareto front solutions shifted from favouring δclu to fks to promote certain conversions conversion from greenhouses recreation areas and services to residential are negative for solutions 1 and 9 that favour δclu preserving the landscape pattern by minimising potential conversions moving to solutions 23 36 and 50 the optimisation process has resulted in positive parameter values with increasing magnitude to promote more conversions the tail parameters the calculated y x value in equation 6 corresponding to the neighbourhood rules exhibited similar trends to the conversion parameters examples of which are illustrated for the influence of different land use classes in the neighbourhood of the class residential presented in appendix b there were certain neighbourhood rules that were tuned to relatively constant forms across the solutions such as the weak attractive influence for the presence of industry in residential locations likely due to both being socio economic land use classes other rules exhibited a trend across the solutions of an increasing attractive or repulsive strength an example of which is shown in fig 10 for the attraction of residential location in the neighbourhood of other residential locations as shown the attractive influences increase from relatively low values for solutions favouring δclu to relatively high values for solutions favouring fks there were also rules such as the presence of nature in the neighbourhood of residential land uses which displayed no trend across the solutions solutions 1 and 9 that favoured δclu and solutions 36 and 50 that favoured fks both featured a repulsive influence but solution 23 which balances the two objectives features a weak attractive influence examination of neighbourhood rules also provides insight into the unexpected model behaviour exhibited in the results maps such as the large clusters of socio cultural uses and greenhouses interspersed between each other in solutions 23 36 and 50 highlighted in fig 7d e and 7f such behaviour could either result from higher conversion values the c parameter or a strong attractive influence between the two classes as a result of the a and b parameters equation 6 the conversion values as well as the influence values at a distance of one are presented table 2 to indicate the resultant influence different land use classes have on each other across the different solutions as shown the conversion values for transition from greenhouses to socio cultural uses are all positive facilitating such transitions across all solutions however only solutions 23 36 and 50 had positive conversion values for transition from socio cultural uses to greenhouses a more likely explanation of the resultant behaviour is provided by the neighbourhood rule tails indicated by the influence values at a distance of one across the five solutions solutions that preference δclu have a repulsive influence i e a negative influence value between both land use classes whilst solutions that preference fks have an attractive influence between both land use classes producing a mutual attraction that explains the observed simulated output maps to illustrate this further the a parameters for the conversion from greenhouses to socio cultural uses which control the type attractive or repulsive and partially the strength of the influence that the presence of the class greenhouses has in the neighbourhood of cells transitioning to socio cultural uses is shown in fig 11 for all pareto front solutions ordered by increasing fks value as shown there is a clear point at approximately solution 20 where the a parameter changes from a negative to a positive value changing the relationship from repulsive to attractive given the trend shown in fig 11 this appears to be a driver in generating solutions that preference fks agreement similar behaviour was also found for the a parameter controlling the influence the class socio cultural uses has in the neighbourhood of cells transitioning to greenhouses figs 12 and 13 show the distance decay parameter values for motorways and other roads respectively ordered and coloured by the selected solution as shown the optimisation process resulted in several trends across the pareto front solutions some parameters were tuned to relatively constant values across the solutions such as industry and services for motorways and recreation areas and socio cultural uses for other roads parameters for other classes were tuned to either relatively high or relatively low values depending on the relative trade off between the two objectives for example the distance decay for greenhouses and residential increased from a relatively low value for solutions favouring δclu to a relatively high value for solutions that favour fks for both motorways and other roads which is reflected in the simulation maps with the increasingly growing clusters of inter connected patches of these land uses between the two concentrated urban areas in fig 7b 2 for solutions with a preference for δclu there are two distinct clusters of urban land uses most notably residential that are sparsely connected whereas in fig 7f 2 for solutions with a preference for fks the two patches of residential area are essentially connected by the land use classes residential and greenhouses in an area where both motorways and other roads are present 4 4 final parameter set following the evaluation of the identified pareto optimal solutions considering the simulated output maps parameters and the influence of the objectives the final parameter set recommended corresponds to pareto front solution 1 which generated the solution map shown in fig 7b see appendix c for parameters this pareto front solution outperformed the largest number of benchmark tests and generated plausible solution maps the parameters obtained more often aligned with general understanding of land use change processes although more neighbourhood rules were included than would be for a conventional manual calibration though this was true for all solutions generated solutions that correspond to higher values of fks agreement as discussed were not recommended because based on the output maps and parameters they did not result in realistic performance 5 discussion this section considers the results obtained from the optimisation process with additional interpretation considering how the structure of the framework influences the results obtained 5 1 enhanced understanding of applied metrics as the metrics drive the optimisation process it is important to understand what impact different metrics have on the parameters and land use maps obtained with regard to the clumpiness metric this was aggregated from class level into a single objective for optimisation purposes which was achieved by taking the average consequently it is important to understand the impact this averaging has on the results obtained this was achieved by analysing the error for each class for the five solutions considered as shown in fig 14 with the colours corresponding to the different classes and the average shown by the black bar on the left hand side for each solution as shown in the figure solutions 1 and 9 have relatively equal error across each class which is captured well by the average however moving to solutions 23 36 and 50 it is observed that there is a large variation in the errors across the classes especially in solutions 23 and 36 where one class services and nature respectively has a much higher error relative to the other classes these results suggest potential improvements could be achieved by using a different aggregation strategy for converting the class level clumpiness errors into a single value for example by taking the maximum as this could potentially penalise the outputs more appropriately to generate patterns more consistent with the observed data as the approach uses multi objective optimisation the individual class level errors could also be considered as independent objectives which for this case study would require eight objectives including locational agreement and the seven clumpiness error values generating an 8 dimensional objective space with regard to the fks measure its use as a measure of locational agreement may have resulted in an over emphasis on the agreement of transitions between the simulated output and the data i e the corresponding actual land use map which is potentially not appropriately balanced with the agreement of inertia this was confirmed by assessing the solutions with the kappa and fuzzy kappa statistic see table 3 as shown in this table the values with lower fks correspond to higher kappa and fuzzy kappa values as previously mentioned all solutions outperform the fks benchmark by being greater than 0 but only solution 1 outperforms the benchmark kappa value and no solutions outperform the benchmark fuzzy kappa value generated by a growing clusters neutral model solutions 1 and 9 outperform the benchmark kappa value generated by using a random constraint match neutral model and all solutions outperform the benchmark fuzzy kappa values generated by using a random constraint match neutral model unlike for the landscape pattern structure metric where improvement could be achieved with alteration of the aggregation method the issues presented with the use of the fks statistic are inherent to the metric and cannot be adjusted without modifying the underlying calculation of the metric the fuzzy kappa or kappa statistic could potentially be used as an alternative but this would likely introduce the opposite problem of the optimisation being too heavily weighted towards inertia within the measure of locational agreement one solution may be to use both metrics as part of multi objective optimisation which would give more weight to the inertia a land use transition to the same land use class used in the calculation of fks further investigation of the metrics used is a clear avenue for further research to improve the outputs of automated parameter tuning methods based on multi objective optimisation 5 2 balancing automatic and heuristic analysis in calibration the application of the multi objective optimisation framework highlights the need to balance automatic and heuristic analysis too often metrics quantifying different aspects of the fit between simulated output and data are used exclusively to evaluate the quality of a luca model calibration method as covered in the results sections 4 2 and 4 3 evaluation can be improved with the incorporation of heuristic analysis incorporating domain knowledge to evaluate the output solution maps and parameters the heuristic analysis also provides insights into the potential improvement of the calibration framework with the incorporation of additional process knowledge though many objectively valid solutions are produced certain parameters are not necessarily consistent with those expected from process understanding the incorporation of additional process knowledge regarding certain parameters presents a clear avenue to improving the optimisation procedure by reducing the search space and computational effort required resulting in faster solution convergence and parameters that are more consistent with expectations in turn using an automatic approach also provides insights into the model structure and its behaviour certain model parameter values though inconsistent with heuristic expectation still generated results that were acceptable according to the metrics used this provided insight into what aspects of the parameterisation and model results should be captured by the objectives section 5 1 and also showed the importance of providing plausible boundaries for model parameters further work on the automatic calibration including a thorough analysis of the solutions obtained presents a path for future research to improve the understanding of luca model calibration the model itself and the interplay of factors driving land use change 6 conclusions and recommendations accurate and effective calibration of luca models is essential to allow for application to the study region of interest this paper presents an automatic calibration framework specifically designed for such luca models the framework is generic allowing for substitution of the various components e g the luca model metrics and the optimisation algorithm utilises multi objective optimisation to allow for the exploration of trade offs between the selected objectives appropriately considers the inherent stochasticity included in luca models and facilitates increased computational efficiency through its implementation the capability of the generic framework has been illustrated with an application to the randstad region of the netherlands the case study was implemented with parallel computing to reduce the parameter adjustment computing time from the order of hundreds of days to the order of days the approach generated 77 possible model parameter sets that resulted in pareto optimal solutions based on the selected objective metrics of which 50 were considered plausible following evaluation of both the simulated output and parameters against process knowledge a final parameter set was recommended the framework provided crucial insights into the influence the metrics had on driving the optimisation process and how more realistic performance could potentially be achieved with the alteration to the optimisation objectives the results are also driven by the genetic algorithm parameters used as with other applications of genetic algorithms to the automatic parameter adjustment stage of calibration of luca models e g jafarnezhad et al 2016 it would be meaningful to investigate the influence these parameters had on the output obtained though this would depend on computational resource availability the framework highlights the importance of heuristic interpretation when evaluating the result map and obtained parameters and the potential of such methods to enhance luca model understanding the demonstrated application of the framework and its generic nature show promising potential for future applications of luca models to support long term planning and policy development acknowledgements the authors wish to acknowledge the financial support from the bushfire and natural hazards cooperative research centre made available by the commonwealth of australia through the cooperative research centre program the authors also wish to acknowledge that this work was supported with supercomputing resources provided by the phoenix high performance computing services at the university of adelaide appendices appendix a confusion matrices this appendix contains confusions matrices for the data and the 6 solution maps presented in section 4 2 showing a count of land use class presence per cell between 1989 and 2000 these tables highlight the volume and type of land use transitions across the different pareto front solutions table a1 confusion matrix for data between 1989 and 2000 table a1 map 2 map 1 lu agr gre res ind ser scu nat rec air wat total agr 26128 152 559 453 12 17 641 336 12 188 28498 gre 81 349 25 9 1 2 1 1 0 5 474 res 160 3 3511 35 18 37 34 123 0 22 3943 ind 67 1 45 1205 7 10 43 28 7 65 1478 ser 3 0 32 37 68 12 6 9 0 4 171 scu 13 0 55 17 2 212 39 20 0 3 361 nat 92 1 19 26 0 3 5814 129 0 50 6134 rec 105 6 81 28 2 21 141 1229 0 44 1657 air 34 0 0 0 0 0 3 0 31 0 68 wat 110 2 23 85 0 3 56 37 0 1685 2001 total 26793 514 4350 1895 110 317 6778 1912 50 2066 44785 table a2 confusion matrix for pareto optimal solution with fks 0 173 and δclu 0 002 between 1989 and 2000 table a2 map 2 map 1 lu agr gre res ind ser scu nat rec air wat total agr 26529 41 426 447 0 0 787 268 0 0 28498 gre 0 473 0 0 0 0 0 1 0 0 474 res 0 0 3921 0 0 0 0 22 0 0 3943 ind 30 0 1 1426 0 0 1 20 0 0 1478 ser 11 0 1 1 110 0 2 46 0 0 171 scu 25 0 1 5 0 317 0 13 0 0 361 nat 64 0 0 15 0 0 5962 93 0 0 6134 rec 181 0 0 1 0 0 26 1449 0 0 1657 air 0 0 0 0 0 0 0 0 68 0 68 wat 0 0 0 0 0 0 0 0 0 2001 2001 total 26840 514 4350 1895 110 317 6778 1912 68 2001 44785 table a3 confusion matrix for pareto optimal solution with fks 0 181 and δclu 0 003 between 1989 and 2000 table a3 map 2 map 1 lu agr gre res ind ser scu nat rec air wat total agr 26499 108 475 398 0 0 807 211 0 0 28498 gre 14 406 4 27 0 0 6 17 0 0 474 res 0 0 3870 0 0 0 0 73 0 0 3943 ind 0 0 0 1448 0 0 1 29 0 0 1478 ser 15 0 1 4 110 0 2 39 0 0 171 scu 26 0 0 3 0 317 2 13 0 0 361 nat 117 0 0 15 0 0 5913 89 0 0 6134 rec 169 0 0 0 0 0 47 1441 0 0 1657 air 0 0 0 0 0 0 0 0 68 0 68 wat 0 0 0 0 0 0 0 0 0 2001 2001 total 26840 514 4350 1895 110 317 6778 1912 68 2001 44785 table a4 confusion matrix for pareto optimal solution with fks 0 190 and δclu 0 006 between 1989 and 2000 table a4 map 2 map 1 lu agr gre res ind ser scu nat rec air wat total agr 26182 150 770 381 1 13 666 335 0 0 28498 gre 26 363 17 3 0 50 1 14 0 0 474 res 252 1 3477 6 4 3 5 195 0 0 3943 ind 0 0 1 1466 1 0 0 10 0 0 1478 ser 25 0 21 8 101 0 3 13 0 0 171 scu 56 0 36 5 0 250 1 13 0 0 361 nat 21 0 19 13 1 0 6025 55 0 0 6134 rec 278 0 9 13 2 1 77 1277 0 0 1657 air 0 0 0 0 0 0 0 0 68 0 68 wat 0 0 0 0 0 0 0 0 0 2001 2001 total 26840 514 4350 1895 110 317 6778 1912 68 2001 44785 table a5 confusion matrix for pareto optimal solution with fks 0 201 and δclu 0 009 between 1989 and 2000 table a5 map 2 map 1 lu agr gre res ind ser scu nat rec air wat total agr 26145 183 946 358 0 24 634 208 0 0 28498 gre 40 323 25 6 0 73 0 7 0 0 474 res 275 5 3257 6 0 5 10 385 0 0 3943 ind 0 0 3 1466 0 1 0 8 0 0 1478 ser 9 1 24 10 110 0 2 15 0 0 171 scu 61 0 68 6 0 212 3 11 0 0 361 nat 0 0 4 1 0 0 6077 52 0 0 6134 rec 310 2 23 42 0 2 52 1226 0 0 1657 air 0 0 0 0 0 0 0 0 68 0 68 wat 0 0 0 0 0 0 0 0 0 2001 2001 total 26840 514 4350 1895 110 317 6778 1912 68 2001 44785 table a6 confusion matrix for pareto optimal solution with fks 0 210 and δclu 0 019 between 1989 and 2000 table a6 map 2 map 1 lu agr gre res ind ser scu nat rec air wat total agr 26031 203 938 365 0 22 650 289 0 0 28498 gre 69 289 31 7 0 64 2 12 0 0 474 res 357 13 3250 12 0 4 6 301 0 0 3943 ind 0 0 6 1453 0 1 0 18 0 0 1478 ser 11 4 22 6 110 0 2 16 0 0 171 scu 59 1 46 12 0 224 3 16 0 0 361 nat 0 0 14 0 0 0 6044 76 0 0 6134 rec 313 4 43 40 0 2 71 1184 0 0 1657 air 0 0 0 0 0 0 0 0 68 0 68 wat 0 0 0 0 0 0 0 0 0 2001 2001 total 26840 514 4350 1895 110 317 6778 1912 68 2001 44785 appendix b neighbourhood tail influence figures this appendix contains a set of figures corresponding to the different neighbourhood rules describing the influence different land use classes exert on the class residential for the solutions analysed in detail fig b1 neighbourhood rule tail for conversions from agriculture to residential fig b1 fig b2 neighbourhood rule tail for conversions from greenhouses to residential fig b2 fig b3 neighbourhood rule tail for conversions from industrial to residential fig b3 fig b4 neighbourhood rule tail for conversions from nature to residential fig b4 fig b5 neighbourhood rule tail for conversions from recreation areas to residential fig b5 fig b6 neighbourhood rule tail for conversions from residential to residential fig b6 fig b7 neighbourhood rule tail for conversions from socio cultural uses to residential fig b7 fig b8 neighbourhood rule tail for conversions from services to residential fig b8 appendix c final model parameterisation table c1 inertia conversion parameters table c1 from to gre res ind ser scu nat rec agr 94 62 81 10 35 30 48 96 26 93 20 45 15 88 gre 2608 65 40 30 80 86 35 52 58 04 38 79 81 79 res 58 02 4142 51 29 37 69 19 45 49 79 16 71 61 ind 2 56 45 18 2504 04 37 57 31 13 36 41 46 91 ser 25 18 31 74 43 44 4508 40 71 34 4 68 48 02 scu 28 37 29 59 4 36 7 58 4124 71 33 40 37 45 nat 72 61 81 08 37 25 70 26 38 65 2360 85 100 00 rec 69 41 32 05 49 51 46 85 3 60 89 57 2957 07 air 53 73 88 52 17 15 57 37 18 60 53 27 69 24 wat 49 51 31 98 70 93 44 79 88 21 62 55 31 23 table c2 neighbourhood rule a parameters table c2 from to gre res ind ser scu nat rec agr 179 73 704 44 720 20 374 93 86 68 657 23 680 28 gre 707 69 247 79 466 96 233 29 990 06 237 31 811 14 res 996 46 369 06 105 07 110 30 485 26 815 79 817 88 ind 127 06 184 53 137 92 234 48 768 52 487 25 56 00 ser 678 64 897 13 47 45 845 38 447 53 69 96 117 35 scu 565 31 394 57 985 62 83 81 171 47 336 22 778 66 nat 195 83 875 47 246 59 844 79 683 76 171 47 988 66 rec 283 93 85 62 663 00 302 89 691 96 602 16 613 24 air 12 20 942 51 851 11 461 78 134 85 984 62 50 08 wat 63 18 636 50 433 31 266 81 898 21 776 96 119 15 table c3 neighbourhood rule b parameters table c3 from to gre res ind ser scu nat rec agr 4 32 3 58 1 37 0 66 4 06 4 91 0 96 gre 3 21 4 47 4 62 2 02 3 33 2 32 3 58 res 3 58 1 87 4 81 4 73 4 91 3 23 1 21 ind 3 29 2 53 0 73 3 93 4 13 1 99 4 48 ser 1 54 2 32 4 10 3 30 4 03 2 41 0 19 scu 0 05 3 32 4 73 2 82 3 16 3 31 3 87 nat 2 89 1 96 1 16 3 90 4 70 3 16 2 43 rec 4 45 3 12 1 84 1 40 4 02 1 88 2 12 air 3 74 1 76 2 55 4 49 3 34 2 10 1 91 wat 4 85 3 76 3 74 3 79 1 48 3 04 4 24 table c4 accessibility parameters table c4 land use class motorway distance decay motorway weight other roads distance decay other roads weight gre 14 33 0 48 0 01 0 46 res 3 56 0 93 12 39 0 66 ind 7 00 0 38 13 42 0 59 ser 1 27 0 48 4 34 0 21 scu 7 75 0 26 10 56 0 37 nat 1 73 0 71 12 14 0 99 rec 8 40 0 19 3 20 0 13 
26458,process based models can predict stream response to streambank stabilization however a framework does not exist on how to explicitly utilize these models to evaluate stabilization measures prior to implementation this research developed a framework to evaluate stabilization practices using hydraulic and sediment transport models landowner preferences construction costs and effectiveness this framework produces sediment reduction graphs to determine the stabilization length as well as cost graphs the methodology was applied to fivemile creek in western oklahoma a concepts simulation was developed for a 10 25 km reach and several stabilization techniques grade control riprap toe and vegetation were simulated incorporating multiple stabilization practices simultaneously resulted in higher sediment loads but also higher costs which were quantifiable using the framework vegetation with 2 1 bank slopes was the most cost effective stabilization technique with that said the framework provided a process based understanding of the system that also highlighted the need for grade control for long term effectiveness keywords framework process based models sediment streambank erosion streambank stabilization conservation effects assessment project ceap cost effectiveness software and or data availability the framework is not specific to any one model but can be applied to any reach scale bank erosion stability model the primary software used in this manuscript to form the basis of the modeling framework is the conservational channel evolution and pollutant transport system concepts developed by dr eddy langendoen at the usda ars national sedimentation laboratory in oxford ms address 598 mcelroy drive oxford ms 38655 telephone 662 232 2924 email eddy langendoen ars usda gov the model can be downloaded free of charge at the following website https www ars usda gov southeast area oxford ms national sedimentation laboratory watershed physical processes research research concepts concepts overview the downloadable executable file is 660 kb the program was first developed in 1999 described and evaluated in the research report in langendoen 2000 and evaluated in a number of stream systems since that time 1 introduction excess sediment from upland sources channel and gully erosion and the resuspension of bed material is a major polluter of surface waters across the united states with streambank erosion from unstable channels contributing as much as 50 90 wilson et al 2008 fox et al 2016 stream restoration or stabilization can reduce sediment contributions from the streambanks and these practices have become more common in recent years with the goal of correcting anthropogenic disruptions to streams beechie et al 2010 however an increase in stream restoration has not reduced the number of degraded miles of streams since the early 1990s langendoen 2011 restoration typically involves extensive channel modification and integrates channel stabilization to lock the channel in place florsheim et al 2008 highlighted several shortcomings of current streambank erosion management strategies including failure to understand erosion processes failure to consider bank erosion on the appropriate scale and failure to understand secondary effects of bank infrastructure current channel modification strategies place an emphasis on channel form rather than channel erosion processes kondolf 1996 and often fail to address the cause of degradation beechie et al 2010 typically a cookbook approach that relies on channel classification rather than erosion process is applied to stream restoration and stabilization projects kondolf 1996 lave 2009 this method often relies on creating a certain channel form that is considered good but this channel form may not be suitable for the amount of sediment or the valley slope and will eventually fail beechie et al 2010 understanding erosion processes such as fluvial erosion of the bed and bank and mass wasting are vital to a successful restoration or stabilization project shields et al 2003 for example river stabilization often only addresses fluvial erosion and will fail where mass wasting is a dominant process florsheim et al 2008 streams adjust to changes within the watershed by the processes of erosion until a dynamic equilibrium is reached channel modification projects that do not allow for a balance of sediment supply and transport capacity often fail shields et al 2008 and lead to either aggradation or degradation of the channel stabilization practices often address erosion at the site scale focusing on local scour and deposition not considering sediment transport outside of the project site and system wide instability kondolf 1996 shields et al 2008 a basin wide analysis or the potential for geomorphic processes to impact the project site rarely occurs miller and kochel 2010 the limited focus of stabilization on the site and ignoring the location within the watershed is a common reason for project failure palmer and allan 2006 langendoen 2011 the consideration of upstream condition is vital as sediment and water discharge are influenced by land use and affect channel response up and downstream morris 1995 palmer and allan 2006 while the effect of stabilization on sediment transport and downstream bank erosion is apparent literature discussing actual sediment reduction to be expected from streambank stabilization is limited many bank stabilization projects do not consider the downstream impacts or include a long term monitoring plan therefore the amount of sediment reduction on the reach or watershed scale is not known stabilization or restoration projects often utilize an empirical cookbook approach rather than utilizing process based models that are available to determine the effect of restoration on sediment reduction prior to implementation in addition the lack of guidelines for the evaluation of stabilization or restoration practices through the use of process based models limit the applicability of these tools research is needed to quantify the amount of sediment reduction from bank stabilization on the reach scale and prioritize stabilization practices prior to implementation furthermore the cost of streambank erosion practices is often quite high and a major factor for stakeholders when determining which practices to adopt several conservation programs funded by federal and state governments are available to assist with the cost of erosion control including the conservation reserve program crp conservation stewardship program csp and environmental quality incentives program eqip tong et al 2017 with a finite amount of resources for these programs it becomes vital to understand which practices are the most cost effective for a particular stream system to achieve optimal sediment reduction furthermore the costs of streambank stabilization projects are highly variable depending on the type of stabilization materials used amount of earthwork needed channel dimensions and other factors nchrp 2005 for example bair 2000 reported costs of typical stream restoration projects ranging between 40 to 220 per linear meter of stream therefore the objectives of this research were to develop a framework for prioritizing streambank stabilization practices for sediment reduction to evaluate the potential sediment load reduction from those practices and to determine the cost associated with a desired amount of sediment reduction this framework was applied using a tributary to the fort cobb reservoir fivemile creek as a case study 2 methods and materials 2 1 process based framework a graphical representation of the proposed methodology for evaluating streambank stabilization is shown in fig 1 several factors contribute to a successful stabilization project and are integrated into this process including public and landowner perception costs and most importantly effectiveness this methodology results in the development of a set of sediment reduction graphs one for each stabilization practice to determine the length of stream that needs to be stabilized to achieve a desired sediment reduction and a second set of graphs to determine the cost of stabilization based upon length of stream stabilized this framework was designed to be in line with the shields et al 2003 stream restoration design approach with stabilization as the key design objective shields et al 2003 specifically noted that stability checks are required within their design approach including the use of either simple qualitative indicators or more in depth bank stability and sediment transport calculations this research specifically calls for the move towards more in depth approaches that make use of the most recent scientific and engineering research 2 1 1 determine study reach the study reach should include an entire stream system if possible or at least highly unstable sites within the stream system and areas immediately up and downstream of the unstable areas to evaluate potential negative geomorphic effects reid and church 2015 study reach lengths will vary depending on scale of erosion problems and size of the channel a rapid geomorphic assessment rga simon and klimetz 2008b or historic aerial photos can be used to aid in the selection of the study reach 2 1 2 set stabilization objectives once the study reach is determined specific and measurable project parameters should be set e g a desired sediment reduction or cost constraint ultimately both cost and sediment reduction will be considered but one or the other may be a driving factor for the project for example if a certain amount of money is available for stabilization the objective could be to determine the most effective stabilization practice for that investment alternatively a certain amount of sediment reduction may be required to be in compliance with water quality standards thus the objective may be to find the least expensive solution to achieve the sediment reduction goal 2 1 3 select stream channel model an appropriate stream channel model should incorporate sediment transport and bed adjustment fluvial erosion and mass wasting processes of the streambank and should be able to simulate these processes on a reach scale incorporation of a reach scale model allows for the consideration of any potential negative effects of stream stabilization upstream and downstream of the site of interest a number of one two or three dimensional numerical models for hydraulics and sediment transport are available while one dimensional models cannot simulate complex flows around in stream structures or localized changes in morphology as accurately as a two or three dimensional models they are more computationally efficient and can accurately evaluate long term channel stability following stabilization langendoen 2011 two examples of one dimensional reach scale bank erosion and stability models are the conservational channel evolution and pollutant transport system concepts and hec ras with the bank stability and toe erosion model bstem version 5 0 or higher once a model has been selected the input for the specific model can be determined in addition the user should determine the cross sectional spacing needed to give the desired resolution of the model streambank erosion models typically incorporate the linear excess shear stress equation for fluvial detachment partheniades 1965 hanson 1990a 1990b 1 ε r k d τ τ c a where ε r is the erosion rate cm s 1 k d is the erodibility coefficient cm3 n 1 s 1 τ is the average hydraulic boundary shear stress pa τ c is the critical shear stress pa and a is an empirical exponent that is assumed to be one calculations of bank stability take the form of a factor of safety fos approach that balances the resisting f r to driving f d forces for bank collapse the fos is determined through both horizontal layers and vertical slices for horizontal layers bank stability models typically use a limit equilibrium analysis in which the mohr coulomb failure criterion is used for the saturated bank layers and the fredlund criterion is used for the unsaturated bank layers 2 f o s f r f d i 1 i c i l i ψ i l i tan φ i b w i cos β u i p i cos α β tan φ i i 1 i w i sin β p i sin α β where c i is effective cohesion of ith vertical layer kpa l i is the surface length along the failure plane incorporated within the ith vertical layer m w i is the weight of the vertical layer kn p i is the hydrostatic confining force due to external water level kn m 1 ψ i is the matric suction or the difference between the air pressure and pore water pressure kpa u i is a hydrostatic uplift force due to positive pore water pressures on the saturated part of the failure plane β is failure plane angle degrees from horizontal α is local bank angle degrees from horizontal ϕ i is the soil internal angle of friction degrees from horizontal and ϕ b i is an angle that describes the relationship between shear strength and matric suction degrees a more detailed description of stability calculations are reported by simon et al 2009 midgley et al 2012 daly et al 2015 and klavon et al 2016 2 1 4 collect critical data for model setup critical input data may vary slightly depending on the numerical model selected but should include cross sectional geometries and soil properties for each cross section channel cross sections can be determined using field survey data however it may not be time efficient to survey enough cross sections to give the desired resolution of the model in this case critical areas can be surveyed and additional cross sections can be determined from digital elevation data e g dems or lidar lidar data has been used in many studies to provide morphological data of stream corridors for hydraulic modelling and streambank erosion estimates where intensive ground surveying was not possible bowen and waltermire 2002 thoma et al 2005 cavalli et al 2008 gichamo et al 2012 however the accuracy of this data will depend on the channel size and may not be suitable for lower order headwater streams under dense canopy james et al 2007 critical soil properties will include soil resistance to geotechnical failure c and ϕ and resistance to fluvial erosion typically in the form of τ c and k d several methods are available to determine the soil resistance to geotechnical failure in the form of effective shear strength parameters c and ϕ including direct shear tests triaxial tests and borehole shear tests erodibility parameters τ c and k d can also be determined in a number of ways including laboratory tests flumes and hole erosion tests and in situ tests such as jet erosion tests jets hanson 1990a 1990b site specific soil properties should be determined when possible typically multiple tests are conducted at each location and average values are used for that particular site a more detailed description and comparison of the methods available to estimate these soil properties can be found in klavon et al 2016 other soil properties that may be required include soil particle size for bed and bank layers bulk density and particle density in addition detailed information on soil layering and the associated soil properties for each layer should be determined to account for bank heterogeneity suarto et al 2014 discharge and sediment inflow data for a desired time period are also needed for the model setup long term flow data from a gauging station is ideal for the inflow hydrograph the usgs has gauging stations at 8000 locations across the united states usgs 2016 most usgs gauging stations also monitor sediment concentrations which can be utilized for sediment inflow data however a gauging station may not be available for all project locations or the gauging station may be located downstream of the study reach in this case flow and sediment discharge estimates can be obtained from a watershed model such as swat moradkhani et al 2010 jähnig et al 2012 giacomoni et al 2014 or annagnps simon et al 2002 schwartz and drumm 2010 finally information on historical bank retreat for the same time period of the flow data is needed previously collected survey data or erosion pin measurements at the study sites can be used however this information is not typically available therefore most streambank erosion studies calculate bank retreat from historical aerial imagery klavon et al 2016 daly et al 2015 pointed out that short monitoring periods were likely to provide bank retreat estimates that were not representative of average retreat rates they suggested longer than a three year monitoring period in order to capture the occurrence and variance of major retreat events 2 1 5 model calibration upon incorporation of the critical data for each cross section model calibration is conducted applied τ which is used for fluvial detachment predictions is a function of water depth therefore calibration of the open channel hydraulics is important for accurate model estimation this can be done by adjusting channel roughness typically manning s n initial estimates of manning s n can be obtained using the median particle size d 50 or from chow 1959 several metrics are available for evaluation of the hydraulic calculations moriasi et al 2007 data from stream gauges can be used for calibration of manning s n or a water level logger can be installed in the stream and used for roughness adjustments after calibration of the hydraulics erodibility parameters can be adjusted for accurate prediction of historical retreat at each cross section several studies suggest calibration based on k d alone and others suggest adjusting both erodibility parameters vegetation and channel meandering can impact applied τ and can be accounted for in one dimensional models using a lumped α factor as discussed in langendoen and simon 2008 and daly et al 2015 in addition c can be adjusted to account for the presence of roots in the streambank the calibrated model is then used to determine locations which are contributing the highest sediment yield and to help understand the dominant erosion processes occurring within the study reach a baseline sediment yield is also determined 2 1 6 select potential stabilization practices a successful stream stabilization practice will either reduce the forces acting on a bank or increase the forces resisting erosion simon et al 2011 several common stabilization practices include toe protection with riprap or large woody debris grade control of the bed vegetative planting and bank grading insights into the dominant erosion processes obtained during the model calibration can be used to determine appropriate stabilization practices for the stream system for example if the stream is incising practices involving grade control may be necessary if the dominant erosion process is fluvial erosion of the bank toe practices involving toe protection should be considered while steep unstable banks may require bank grading to reduce geotechnical failures in addition to considering the dominant erosion processes the willingness of landowners to adopt specific practices and the public s perception of various stabilization practices need to be considered a survey of landowners should occur to determine the most appealing practices and what factors will influence their willingness to adopt the practices for example aesthetics costs available cost sharing programs maintenance requirements and understanding the benefits all may influence the likelihood of a landowner to adopt a practice in addition the amount of land they are willing to dedicate to erosion control may be an important factor establishment of riparian zones in agricultural areas may take land out of production and sloping banks may result in a significant loss of land depending on the height of the banks taking into account the dominant erosion processes and the willingness of landowners to adopt certain practices a list of potential stabilization practices can be determined once the practices are selected methods for simulating the practices in the numerical model must be determined extensive literature exists on how to incorporate stabilization practices in process based models or how various stabilization materials modify soil properties simon et al 2009 langendoen 2011 klavon et al 2016 2 1 7 simulate stabilization via the calibrated model for each stabilization practice a set of scenarios in which stabilization is applied to varying stream lengths and locations should be generated to limit the number of model simulations the stream can be divided into segments based on landownership changes in land use roads or other metrics because location of stabilization will also impact sediment yield reduction various combinations of stream segments including upstream and downstream locations should be stabilized lengths of stream stabilized should range from zero kilometers to the entire reach the sediment yield from each scenario can then be compared to the baseline prediction from the calibrated model to calculate a percent reduction percent reduction can then be plotted against the length of stream stabilized for each stabilization practice the cost for each scenario can be determined and plotted against the stabilization length this process will result in a set of sediment reduction graphs and a corresponding set of cost graphs while daly et al 2015 suggested longer than a three year monitoring period for assessing bank erosion we recommend that the numerical modeling be conducted over a greater time scale a minimum of 5 10 years to assess long term impacts of streambank stabilization 2 1 8 decision making the length of stabilization associated sediment reduction and cost to meet the project objectives can be determined using the two sets of graphs there will likely be a set of potential solutions that will meet the goals the willingness of landowners to adopt specific practices can be taken into account during this step to determine the best practice to implement in the area conversely if the project goals cannot be met the objectives can be re evaluated once the length and stabilization practices have been determined locations of the stabilization can also be determined from the set of scenarios for that stabilization practice any potential negative effects can then be evaluated i e increase in erosion downstream as well as the bank retreat at specific locations 2 2 case study fivemile creek the framework developed for prioritizing streambank stabilization for sediment reduction was applied to fivemile creek a tributary to the fort cobb reservoir in western oklahoma fort cobb reservoir provides public water supply recreation and wildlife habitat and is listed on the oklahoma 303 d list for impairment by nutrients siltation and sediment storm et al 2003 the watershed is located in the central great plains ecoregion and is predominately agricultural with only 5 of the watershed classified as urban land use and less than 2 water starks et al 2014 as part of the usda s conservation effects assessment program ceap numerous upland and riparian conservation and structural and water management practices were implemented to reduce sediment loading to the reservoir steiner et al 2008 the reservoir still fails to meet water quality standards based on sediment and it was estimated that 50 of the suspended sediment in the reservoir originated from streambanks wilson et al 2008 streambanks in the watershed consist of either a single sand or sandy loam layer while others exhibit a layering effect of sand or sandy loam layers above and below a more cohesive layer with higher clay content 2 2 1 set stabilization objectives although determining a cost or sediment reduction constraint is likely a necessary step in a real world application the objective for the case study was to determine the most cost effective practice for fivemile creek a series of stabilization scenarios described below are tacitly assumed to be the stabilization practices needed to achieve our unstated sediment reduction goals 2 2 2 determine study reach an rga of the fort cobb reservoir watershed was completed in 2006 and identified the entire length of fivemile creek to be in stages iv and v of the channel evolution model simon and klimetz 2008a moriasi et al 2014 stage iv degradation and widening and stage v aggradation and widening are the two most unstable stages of channel evolution a 10 25 km reach was selected for this study 2 2 3 select stream channel model concepts langendoen 2000 was selected as the stream channel model for simulations on fivemile creek concepts simulates one dimensional hydraulics fluvial erosion and mass wasting processes and graded sediment transport cross sectional geometry fluvial erodibility parameters soil geotechnical parameters bank soil and bed sediment particle size distributions soil and sediment layer and sediment and flow data are required to set up a concepts simulation 2 2 4 collect critical data for model setup five sites along fivemile creek fm1 fm5 were selected for field data collection fig 2 at least one cross sectional survey was conducted at each site using an automatic level one site fm2 was severely impacted by a series of headcuts seven cross sectional surveys were conducted at this site during surveying detailed information on layering and vegetation were recorded soil samples were collected from the thalweg of the channel at each cross section and were analyzed for soil particle size distribution using hydrometer and sieves astm standards d421 astm 2002a and d422 astm 2002b a hobo water level logger onset bourne ma was installed at each site to measure absolute pressure pressures were converted to water depth using hoboware software and atmospheric pressures determined from the nearby oklahoma mesonet site http www mesonet org at each site erodibility parameters were determined using mini jets khanal et al 2016 at least two mini jets were conducted at each site within each visible layer table 1 streambank soil samples were also collected to analyze for bulk density and particle size as described above borehole shear tests bsts handy and fox 1967 were conducted to determine geotechnical parameters φ and c for each soil layer however the texture of the soils and the location of the water table within this watershed resulted in unreliable bst results therefore values based on soil texture were selected from a list of defaults incorporated into bstem a commonly used site scale streambank stability model simon et al 2011 2 2 5 determination of long term erosion rates aerial imagery was used to estimate long term streambank retreat at each site national agricultural imagery program naip images 1 m resolution were obtained for caddo county for 2008 and 2013 using arcmap v10 0 each image was georeferenced and used to determine bank retreat streambanks were digitized at each site for 2008 and 2013 and the average distance between polylines were used as lateral retreat for that site purvis and fox 2016 for site fm5 dense vegetation on both images did not allow for analysis therefore the retreat was estimated from the nearest visible streambank 2 2 6 model set up and calibration because of the large distance between the selected research sites additional cross sections ac were interpolated from lidar data at approximately every 500 m along the channel for a total of 29 acs on fivemile creek fig 2 the acs were more closely spaced around fm2 where several surveyed cross sections were closely spaced due to the presence of headcuts to increase the stability of the model average soil particle size distributions bulk densities and default geotechnical parameters were used for the soil and sediment data of each ac the τ c values from the mini jet data were grouped by soil layer to identify the probability distribution using the individual distribution identification function in minitab v16 minitab inc state college pa the anderson darling ad statistic was used to evaluate the distributions the τ c values followed log normal ad 0 196 p 0 848 and weibull distributions ad 0 276 p 0 25 for the sand and clay layers respectively using these distributions random numbers were generated for τ c for each ac a regression equation was used to determine the relationship between τ c and k d this regression was used to determine k d for each ac for field data collection sites site specific soil data was used average τ c and k d from the jets conducted at each site were used as input a usgs stream gauge was not available for fivemile creek therefore a daily average stream flow hydrograph for 2008 2013 generated from a calibrated swat model for the fort cobb watershed moriasi and starks 2010 guzman et al 2015 was used as flow input fluvial erosion is a function of applied τ and is sensitive to peak flows q therefore the daily average streamflow hydrograph was converted to an hourly triangular hydrograph using the scs triangular hydrograph method scs 1972 the converted hydrograph started and ended at base flow that was determined from the daily averaged flow for flows less than or equal to baseflow hourly discharge was set to the daily average flow one tributary was incorporated into the model concepts requires sediment inflow data q s for tributaries for each sediment class size in the form of power equations 3 q s a q b where a and b are regression parameters table 2 in order to determine the regression parameters of the power equations data from three usgs gauges cobb creek near eakly lake creek and willow creek located in the fort cobb reservoir watershed were combined to develop sediment rating curves for each concepts particle size the usgs collected grab samples during various storm events which were analyzed for particle size using these data and the streamflow at each gauge rating curves between sediment discharge q s kg s 1 and streamflow q cms were created by fitting a regression equation to the data using sigmaplot v12 5 systat software san jose ca water levels from the hobo loggers and a swat generated flow file were used to calibrate the roughness of the bed and banks using the hydraulic submodel in concepts fivemile creek was divided into five sections based on proximity to surveyed cross sections and roughness for all cross sections within each section was assumed to be the same water depth output from concepts was compared to the water depth measured by the water level loggers at each site the roughness of the bed was calibrated during periods of base flow bank roughness was calibrated based on peak flows for storm events table 3 concepts predicted retreat was compared to measured aerial retreat and τ c and k d for each cross section were adjusted as needed fig 3 a brier skill score bss was used to evaluate the calibration based on lateral bank retreat abderrezzak et al 2016 a bss equal to 0 60 was determined which suggested a good model fit streambanks at some sites were heavily vegetated because vegetation can significantly alter τ thompson et al 2004 the effect of vegetation was taken into account during the calibration period the model did not allow for the direct adjustment of τ therefore an α factor was used to modify τ by adjusting τ c and k d in the same manner that daly et al 2015 used to account for the increase in τ around a meander bend 4 ε r k d α τ τ c α k d τ τ c α 2 2 7 select potential stabilization practices an informal survey of landowners along fivemile creek was performed to identify the most appealing practices and what factors might influence their willingness to adopt the practices aesthetics costs available cost sharing programs maintenance requirements absentee landowners and understanding the benefits of each practice were all identified as factors influencing the likelihood of a landowner to adopt a practice tong et al 2017 a primary concern in the watershed was the amount of land necessary for a specific erosion control practice establishment of riparian zones in agricultural areas may take land out of production and sloping banks may result in a significant loss of land depending on the height of the bank therefore several factors were considered when selecting stabilization practices to be simulated including stream incision the amount of land required for stabilization practice and cost three stabilization practices were selected for analysis based on input from landowners and also typical practices grade control gc riprap toe rrt and vegetation and bank grading with both 2 1 veg21 and 3 1 bank slopes veg31 four scenarios of single practices gc rrt veg21 and veg31 and seven combinations of the practices rrt gc veg21 gc veg21 rrt veg21 rrt gc veg31 gc veg31 rrt veg31 rrt gc were simulated 2 2 8 apply stabilization practices to calibrated model a riprap toe was simulated by modifying the erodibility parameters of the bank toe riprap size was determined using the d 50 factor of safety procedure described by stevens et al 1976 the shield s diagram shields 1936 was used to determine τ c and the relationship between τ c and k d developed by simon et al 2011 was used to determine k d for the riprap bank grading was simulated by changing the channel geometry of the cross sections vegetation was simulated by using an α factor of 0 15 thompson et al 2004 root cohesion c r was determined using the riproot model that is incorporated into bstem pollen and simon 2005 a bank coverage of 50 grass and 50 trees was assumed to calculate the additional cohesion for the soils grade control was simulated by setting the bedrock elevation equal to the thalweg elevation langendoen 2011 fivemile creek was divided into five sections based upon land ownership to limit the number of combinations of sites for each of the 11 stabilization scenarios the stabilization practice was first applied to a single cross section eight simulations of single cross sections were simulated next stabilization was applied to a single land owner and then combinations of two three and four landowners finally a scenario with the entire stream stabilized was considered a total of 38 model simulations were performed for each of the stabilization scenarios the sediment yield from each scenario was compared to the baseline sediment yield from the calibrated model to calculate sediment reduction a regression between sediment reduction and fraction of the stream stabilized λ was calculated using sigmaplot v12 5 systat software san jose ca for each of the stabilization scenarios 2 2 9 cost calculations costs for each practice were calculated using rsmeans 2016 facilities and construction cost a stoecker personal communication and site specific channel dimensions excavation costs were based on a 0 38 m3 excavator riprap costs are for random broken stone 45 kg average in place vegetation costs include 900 n tensile strength geotextile fabric bare root willow seedlings with a density of 10 per m2 and fescue seeding using a tractor spreader grade control cost were estimated based on cross vanes spaced approximately five to seven channel widths finally a 5 engineering and surveying expense was included in each cost estimate 3 results and discussion 3 1 sediment reduction and stabilization costs the sediment reduction and cost for each model simulation were determined to develop a pair of graphs for each of the 11 stabilization practices examples are provided in fig 4 a values of λ less than 0 05 corresponds with a single cross section stabilized in the model and represents less than 500 m of streambank stabilized for all practices evaluated this resulted in a small amount of sediment reduction when considering the entire reach a range of λ from 0 1 to 0 3 represents the entire length of stream for a single landowner while λ between 0 3 and 0 5 represents two landowners 0 5 to 0 7 represents three landowners and 0 7 to 0 9 represents four landowners the λ 1 is the scenario where stabilization is applied to the entire length of stream higher r2 values 0 91 0 93 were observed for scenarios involving vegetation veg21 veg21 rrt etc than the scenarios incorporating rrt and gc alone r2 0 49 0 68 3 2 evaluation of stabilization practices after regression equations were developed for the stabilization practices the effectiveness of each practice was compared fig 5 the gc alone scenario resulted in an increase in sediment as it was applied to increasing lengths of the stream if bank protection was not incorporated with grade control the stream began to adjust laterally and an increase in bank erosion was predicted practices that incorporated vegetation as a means of bank stabilization resulted in a higher amount of sediment reduction when compared to rrt only rrt gc and gc only scenarios indicating vegetation needed to be incorporated the veg31 rrt gc scenario resulted in the highest amount of sediment reduction for all lengths of stream stabilized in addition bank protection alone does not prevent channel incision and resulted in lower sediment reduction when compared to the same practice with the addition of grade control i e a higher sediment reduction was observed with the veg21 gc scenario when compared to the veg21 or rrt gc scenarios when compared to the rrt scenario as previous research has shown both bed and bank protection should be incorporated for optimal sediment reduction shields et al 1995 2003 while incorporating multiple practices in stabilization resulted in higher sediment reduction the cost was also much higher cost effectiveness was determined for each scenario and an average cost effectiveness ton and per percent reduction was calculated for each practice table 4 the gc scenario resulted in an increase in sediment yield and therefore the cost effectiveness was not calculated although the veg31 rrt gc scenario was the most effective in terms of sediment reduction it ranked seventh in terms of cost effectiveness the veg21 scenario was the most cost effective stabilization practice with the veg21 gc scenario a close second if a longer time period was considered a higher degree of incision may be observed which could have ultimately caused an increase occurrence of bank failure and higher sediment loads if grade control was not included although slightly less cost effective the veg21 gc scenario would be recommended for fivemile creek for long term stability due to the highly incised nature of this stream system ideally these sediment reduction and cost effectiveness graphs would now be shared with landowners prior to the installation of any stabilization techniques this research did not provide funds for the installation of any techniques but the critical role and input of the stakeholders in the final decision making process cannot be forgotten the sets of graphs can also be used to determine the amount of sediment reduction to be expected based upon an amount invested between different stabilization practices for example if 800 000 was available to invest in a streambank stabilization project along fivemile creek a λ of 0 48 is expected for the rrt scenario which would result in approximately 20 reduction in sediment load fig 4 conversely if the stabilization practice veg21 gc was selected 75 of the stream could be stabilized for a predicted sediment load reduction of 70 fig 4 the veg31 rrt gc scenario resulted in a λ equal to 0 25 and a predicted sediment load reduction of 28 fig 4 for the veg21 only scenario a sediment reduction of 75 was expected therefore for an investment of 800 000 the veg21 scenario would result in the highest sediment reduction for fivemile creek however as previously stated gc may be needed for long term stability the advantage of the proposed framework is the focus on the reach scale impact of site specific stabilization through the modeling of sediment transport stabilization using bank infrastructure has morphological impacts throughout a reach channel bank infrastructure alters the geomorphic process and can lead to more erosion locally and at great distances up and downstream of the stabilized site florsheim et al 2008 reid and church 2015 engineered structures may be ineffective over the long term florsheim et al 2008 the use of riprap or other hard structures increases flood velocities disrupts lateral sediment exchanges florsheim et al 2008 alters flow conditions kondolf 1996 and influences local and downstream sediment transport reid and church 2015 bank stabilization reduces sediment supply downstream but the transport capacity remains the same leading to scour of the bed and banks downstream thereby displacing the original problem kondolf 1996 watson et al 2002 piégay et al 2005 reid and church 2015 if a stream can no longer adjust laterally due to stabilization such as riprap reid and church 2015 the channel will begin to adjust downward leading to incision the increase in erosion downstream may require additional bank stabilization to compensate and could ultimately harden an entire reach florsheim et al 2008 miller and kochel 2010 reid and church 2015 additionally bank protection or stabilization can lead to knickpoint migration upstream causing further instability gregory 2006 a major component of the framework is being able to analyze the construction costs associated with the various stabilization techniques for the case study costs were estimated based on relevant assumptions in oklahoma users would benefit from the development of a spreadsheet tool for estimating project costs relative to different stabilization techniques location and project size for the pre design consideration of potential alternatives the authors are unaware of any such spreadsheet tool other than general guidance provided in technical reports such as nchrp 2005 finally additional research efforts are still needed to provide further guidance on data collection parametrization calibration and use of the hydraulic sediment transport and bank stability models used in such frameworks for example one dimensional hydraulic and sediment transport models require adjustments to appropriate simulate applied shear stress distributions on meandering streambanks with vegetation higher dimensional models may alleviate this issue but also require more intense data collection to characterize streams especially considering the need to simulate reach scale processes advancements are also needed in understanding how to incorporate or scale variability in erodibility properties measured at select cross sections to the reach scale 4 conclusions streambank stabilization practices can be very costly therefore it is important to understand the possible benefits prior to implementation and to evaluate potential alternatives process based models are useful tools to evaluate potential streambank stabilization practices and a number of validated reach scale sediment transport and bank erosion stability models are available the framework presented in this paper provides an example of how to use these tools to determine the most effective practice for a particular stream system based on both sediment reduction and cost analyses choosing practices without regard to cost would result in greater costs per ton at the expense of other projects or more miles of river stabilized this methodology was applied to fivemile creek to evaluate various stabilization scenarios to determine the most cost effective stabilization practice it was determined that vegetation with 2 1 bank slopes and grade control would be the best choice for this stream system acknowledgments this project was supported by national integrated water quality program grant no 2013 51130 2184 from the usda national institute of food and agriculture this research was partially supported by the usda nrcs cropland ceap effort and is also a contribution of the usda ars southern plains long term agroecosystem research site the authors would like to acknowledge periann russell of the north carolina department of environmental quality and amanda fox for reviewing earlier versions of this manuscript 
26458,process based models can predict stream response to streambank stabilization however a framework does not exist on how to explicitly utilize these models to evaluate stabilization measures prior to implementation this research developed a framework to evaluate stabilization practices using hydraulic and sediment transport models landowner preferences construction costs and effectiveness this framework produces sediment reduction graphs to determine the stabilization length as well as cost graphs the methodology was applied to fivemile creek in western oklahoma a concepts simulation was developed for a 10 25 km reach and several stabilization techniques grade control riprap toe and vegetation were simulated incorporating multiple stabilization practices simultaneously resulted in higher sediment loads but also higher costs which were quantifiable using the framework vegetation with 2 1 bank slopes was the most cost effective stabilization technique with that said the framework provided a process based understanding of the system that also highlighted the need for grade control for long term effectiveness keywords framework process based models sediment streambank erosion streambank stabilization conservation effects assessment project ceap cost effectiveness software and or data availability the framework is not specific to any one model but can be applied to any reach scale bank erosion stability model the primary software used in this manuscript to form the basis of the modeling framework is the conservational channel evolution and pollutant transport system concepts developed by dr eddy langendoen at the usda ars national sedimentation laboratory in oxford ms address 598 mcelroy drive oxford ms 38655 telephone 662 232 2924 email eddy langendoen ars usda gov the model can be downloaded free of charge at the following website https www ars usda gov southeast area oxford ms national sedimentation laboratory watershed physical processes research research concepts concepts overview the downloadable executable file is 660 kb the program was first developed in 1999 described and evaluated in the research report in langendoen 2000 and evaluated in a number of stream systems since that time 1 introduction excess sediment from upland sources channel and gully erosion and the resuspension of bed material is a major polluter of surface waters across the united states with streambank erosion from unstable channels contributing as much as 50 90 wilson et al 2008 fox et al 2016 stream restoration or stabilization can reduce sediment contributions from the streambanks and these practices have become more common in recent years with the goal of correcting anthropogenic disruptions to streams beechie et al 2010 however an increase in stream restoration has not reduced the number of degraded miles of streams since the early 1990s langendoen 2011 restoration typically involves extensive channel modification and integrates channel stabilization to lock the channel in place florsheim et al 2008 highlighted several shortcomings of current streambank erosion management strategies including failure to understand erosion processes failure to consider bank erosion on the appropriate scale and failure to understand secondary effects of bank infrastructure current channel modification strategies place an emphasis on channel form rather than channel erosion processes kondolf 1996 and often fail to address the cause of degradation beechie et al 2010 typically a cookbook approach that relies on channel classification rather than erosion process is applied to stream restoration and stabilization projects kondolf 1996 lave 2009 this method often relies on creating a certain channel form that is considered good but this channel form may not be suitable for the amount of sediment or the valley slope and will eventually fail beechie et al 2010 understanding erosion processes such as fluvial erosion of the bed and bank and mass wasting are vital to a successful restoration or stabilization project shields et al 2003 for example river stabilization often only addresses fluvial erosion and will fail where mass wasting is a dominant process florsheim et al 2008 streams adjust to changes within the watershed by the processes of erosion until a dynamic equilibrium is reached channel modification projects that do not allow for a balance of sediment supply and transport capacity often fail shields et al 2008 and lead to either aggradation or degradation of the channel stabilization practices often address erosion at the site scale focusing on local scour and deposition not considering sediment transport outside of the project site and system wide instability kondolf 1996 shields et al 2008 a basin wide analysis or the potential for geomorphic processes to impact the project site rarely occurs miller and kochel 2010 the limited focus of stabilization on the site and ignoring the location within the watershed is a common reason for project failure palmer and allan 2006 langendoen 2011 the consideration of upstream condition is vital as sediment and water discharge are influenced by land use and affect channel response up and downstream morris 1995 palmer and allan 2006 while the effect of stabilization on sediment transport and downstream bank erosion is apparent literature discussing actual sediment reduction to be expected from streambank stabilization is limited many bank stabilization projects do not consider the downstream impacts or include a long term monitoring plan therefore the amount of sediment reduction on the reach or watershed scale is not known stabilization or restoration projects often utilize an empirical cookbook approach rather than utilizing process based models that are available to determine the effect of restoration on sediment reduction prior to implementation in addition the lack of guidelines for the evaluation of stabilization or restoration practices through the use of process based models limit the applicability of these tools research is needed to quantify the amount of sediment reduction from bank stabilization on the reach scale and prioritize stabilization practices prior to implementation furthermore the cost of streambank erosion practices is often quite high and a major factor for stakeholders when determining which practices to adopt several conservation programs funded by federal and state governments are available to assist with the cost of erosion control including the conservation reserve program crp conservation stewardship program csp and environmental quality incentives program eqip tong et al 2017 with a finite amount of resources for these programs it becomes vital to understand which practices are the most cost effective for a particular stream system to achieve optimal sediment reduction furthermore the costs of streambank stabilization projects are highly variable depending on the type of stabilization materials used amount of earthwork needed channel dimensions and other factors nchrp 2005 for example bair 2000 reported costs of typical stream restoration projects ranging between 40 to 220 per linear meter of stream therefore the objectives of this research were to develop a framework for prioritizing streambank stabilization practices for sediment reduction to evaluate the potential sediment load reduction from those practices and to determine the cost associated with a desired amount of sediment reduction this framework was applied using a tributary to the fort cobb reservoir fivemile creek as a case study 2 methods and materials 2 1 process based framework a graphical representation of the proposed methodology for evaluating streambank stabilization is shown in fig 1 several factors contribute to a successful stabilization project and are integrated into this process including public and landowner perception costs and most importantly effectiveness this methodology results in the development of a set of sediment reduction graphs one for each stabilization practice to determine the length of stream that needs to be stabilized to achieve a desired sediment reduction and a second set of graphs to determine the cost of stabilization based upon length of stream stabilized this framework was designed to be in line with the shields et al 2003 stream restoration design approach with stabilization as the key design objective shields et al 2003 specifically noted that stability checks are required within their design approach including the use of either simple qualitative indicators or more in depth bank stability and sediment transport calculations this research specifically calls for the move towards more in depth approaches that make use of the most recent scientific and engineering research 2 1 1 determine study reach the study reach should include an entire stream system if possible or at least highly unstable sites within the stream system and areas immediately up and downstream of the unstable areas to evaluate potential negative geomorphic effects reid and church 2015 study reach lengths will vary depending on scale of erosion problems and size of the channel a rapid geomorphic assessment rga simon and klimetz 2008b or historic aerial photos can be used to aid in the selection of the study reach 2 1 2 set stabilization objectives once the study reach is determined specific and measurable project parameters should be set e g a desired sediment reduction or cost constraint ultimately both cost and sediment reduction will be considered but one or the other may be a driving factor for the project for example if a certain amount of money is available for stabilization the objective could be to determine the most effective stabilization practice for that investment alternatively a certain amount of sediment reduction may be required to be in compliance with water quality standards thus the objective may be to find the least expensive solution to achieve the sediment reduction goal 2 1 3 select stream channel model an appropriate stream channel model should incorporate sediment transport and bed adjustment fluvial erosion and mass wasting processes of the streambank and should be able to simulate these processes on a reach scale incorporation of a reach scale model allows for the consideration of any potential negative effects of stream stabilization upstream and downstream of the site of interest a number of one two or three dimensional numerical models for hydraulics and sediment transport are available while one dimensional models cannot simulate complex flows around in stream structures or localized changes in morphology as accurately as a two or three dimensional models they are more computationally efficient and can accurately evaluate long term channel stability following stabilization langendoen 2011 two examples of one dimensional reach scale bank erosion and stability models are the conservational channel evolution and pollutant transport system concepts and hec ras with the bank stability and toe erosion model bstem version 5 0 or higher once a model has been selected the input for the specific model can be determined in addition the user should determine the cross sectional spacing needed to give the desired resolution of the model streambank erosion models typically incorporate the linear excess shear stress equation for fluvial detachment partheniades 1965 hanson 1990a 1990b 1 ε r k d τ τ c a where ε r is the erosion rate cm s 1 k d is the erodibility coefficient cm3 n 1 s 1 τ is the average hydraulic boundary shear stress pa τ c is the critical shear stress pa and a is an empirical exponent that is assumed to be one calculations of bank stability take the form of a factor of safety fos approach that balances the resisting f r to driving f d forces for bank collapse the fos is determined through both horizontal layers and vertical slices for horizontal layers bank stability models typically use a limit equilibrium analysis in which the mohr coulomb failure criterion is used for the saturated bank layers and the fredlund criterion is used for the unsaturated bank layers 2 f o s f r f d i 1 i c i l i ψ i l i tan φ i b w i cos β u i p i cos α β tan φ i i 1 i w i sin β p i sin α β where c i is effective cohesion of ith vertical layer kpa l i is the surface length along the failure plane incorporated within the ith vertical layer m w i is the weight of the vertical layer kn p i is the hydrostatic confining force due to external water level kn m 1 ψ i is the matric suction or the difference between the air pressure and pore water pressure kpa u i is a hydrostatic uplift force due to positive pore water pressures on the saturated part of the failure plane β is failure plane angle degrees from horizontal α is local bank angle degrees from horizontal ϕ i is the soil internal angle of friction degrees from horizontal and ϕ b i is an angle that describes the relationship between shear strength and matric suction degrees a more detailed description of stability calculations are reported by simon et al 2009 midgley et al 2012 daly et al 2015 and klavon et al 2016 2 1 4 collect critical data for model setup critical input data may vary slightly depending on the numerical model selected but should include cross sectional geometries and soil properties for each cross section channel cross sections can be determined using field survey data however it may not be time efficient to survey enough cross sections to give the desired resolution of the model in this case critical areas can be surveyed and additional cross sections can be determined from digital elevation data e g dems or lidar lidar data has been used in many studies to provide morphological data of stream corridors for hydraulic modelling and streambank erosion estimates where intensive ground surveying was not possible bowen and waltermire 2002 thoma et al 2005 cavalli et al 2008 gichamo et al 2012 however the accuracy of this data will depend on the channel size and may not be suitable for lower order headwater streams under dense canopy james et al 2007 critical soil properties will include soil resistance to geotechnical failure c and ϕ and resistance to fluvial erosion typically in the form of τ c and k d several methods are available to determine the soil resistance to geotechnical failure in the form of effective shear strength parameters c and ϕ including direct shear tests triaxial tests and borehole shear tests erodibility parameters τ c and k d can also be determined in a number of ways including laboratory tests flumes and hole erosion tests and in situ tests such as jet erosion tests jets hanson 1990a 1990b site specific soil properties should be determined when possible typically multiple tests are conducted at each location and average values are used for that particular site a more detailed description and comparison of the methods available to estimate these soil properties can be found in klavon et al 2016 other soil properties that may be required include soil particle size for bed and bank layers bulk density and particle density in addition detailed information on soil layering and the associated soil properties for each layer should be determined to account for bank heterogeneity suarto et al 2014 discharge and sediment inflow data for a desired time period are also needed for the model setup long term flow data from a gauging station is ideal for the inflow hydrograph the usgs has gauging stations at 8000 locations across the united states usgs 2016 most usgs gauging stations also monitor sediment concentrations which can be utilized for sediment inflow data however a gauging station may not be available for all project locations or the gauging station may be located downstream of the study reach in this case flow and sediment discharge estimates can be obtained from a watershed model such as swat moradkhani et al 2010 jähnig et al 2012 giacomoni et al 2014 or annagnps simon et al 2002 schwartz and drumm 2010 finally information on historical bank retreat for the same time period of the flow data is needed previously collected survey data or erosion pin measurements at the study sites can be used however this information is not typically available therefore most streambank erosion studies calculate bank retreat from historical aerial imagery klavon et al 2016 daly et al 2015 pointed out that short monitoring periods were likely to provide bank retreat estimates that were not representative of average retreat rates they suggested longer than a three year monitoring period in order to capture the occurrence and variance of major retreat events 2 1 5 model calibration upon incorporation of the critical data for each cross section model calibration is conducted applied τ which is used for fluvial detachment predictions is a function of water depth therefore calibration of the open channel hydraulics is important for accurate model estimation this can be done by adjusting channel roughness typically manning s n initial estimates of manning s n can be obtained using the median particle size d 50 or from chow 1959 several metrics are available for evaluation of the hydraulic calculations moriasi et al 2007 data from stream gauges can be used for calibration of manning s n or a water level logger can be installed in the stream and used for roughness adjustments after calibration of the hydraulics erodibility parameters can be adjusted for accurate prediction of historical retreat at each cross section several studies suggest calibration based on k d alone and others suggest adjusting both erodibility parameters vegetation and channel meandering can impact applied τ and can be accounted for in one dimensional models using a lumped α factor as discussed in langendoen and simon 2008 and daly et al 2015 in addition c can be adjusted to account for the presence of roots in the streambank the calibrated model is then used to determine locations which are contributing the highest sediment yield and to help understand the dominant erosion processes occurring within the study reach a baseline sediment yield is also determined 2 1 6 select potential stabilization practices a successful stream stabilization practice will either reduce the forces acting on a bank or increase the forces resisting erosion simon et al 2011 several common stabilization practices include toe protection with riprap or large woody debris grade control of the bed vegetative planting and bank grading insights into the dominant erosion processes obtained during the model calibration can be used to determine appropriate stabilization practices for the stream system for example if the stream is incising practices involving grade control may be necessary if the dominant erosion process is fluvial erosion of the bank toe practices involving toe protection should be considered while steep unstable banks may require bank grading to reduce geotechnical failures in addition to considering the dominant erosion processes the willingness of landowners to adopt specific practices and the public s perception of various stabilization practices need to be considered a survey of landowners should occur to determine the most appealing practices and what factors will influence their willingness to adopt the practices for example aesthetics costs available cost sharing programs maintenance requirements and understanding the benefits all may influence the likelihood of a landowner to adopt a practice in addition the amount of land they are willing to dedicate to erosion control may be an important factor establishment of riparian zones in agricultural areas may take land out of production and sloping banks may result in a significant loss of land depending on the height of the banks taking into account the dominant erosion processes and the willingness of landowners to adopt certain practices a list of potential stabilization practices can be determined once the practices are selected methods for simulating the practices in the numerical model must be determined extensive literature exists on how to incorporate stabilization practices in process based models or how various stabilization materials modify soil properties simon et al 2009 langendoen 2011 klavon et al 2016 2 1 7 simulate stabilization via the calibrated model for each stabilization practice a set of scenarios in which stabilization is applied to varying stream lengths and locations should be generated to limit the number of model simulations the stream can be divided into segments based on landownership changes in land use roads or other metrics because location of stabilization will also impact sediment yield reduction various combinations of stream segments including upstream and downstream locations should be stabilized lengths of stream stabilized should range from zero kilometers to the entire reach the sediment yield from each scenario can then be compared to the baseline prediction from the calibrated model to calculate a percent reduction percent reduction can then be plotted against the length of stream stabilized for each stabilization practice the cost for each scenario can be determined and plotted against the stabilization length this process will result in a set of sediment reduction graphs and a corresponding set of cost graphs while daly et al 2015 suggested longer than a three year monitoring period for assessing bank erosion we recommend that the numerical modeling be conducted over a greater time scale a minimum of 5 10 years to assess long term impacts of streambank stabilization 2 1 8 decision making the length of stabilization associated sediment reduction and cost to meet the project objectives can be determined using the two sets of graphs there will likely be a set of potential solutions that will meet the goals the willingness of landowners to adopt specific practices can be taken into account during this step to determine the best practice to implement in the area conversely if the project goals cannot be met the objectives can be re evaluated once the length and stabilization practices have been determined locations of the stabilization can also be determined from the set of scenarios for that stabilization practice any potential negative effects can then be evaluated i e increase in erosion downstream as well as the bank retreat at specific locations 2 2 case study fivemile creek the framework developed for prioritizing streambank stabilization for sediment reduction was applied to fivemile creek a tributary to the fort cobb reservoir in western oklahoma fort cobb reservoir provides public water supply recreation and wildlife habitat and is listed on the oklahoma 303 d list for impairment by nutrients siltation and sediment storm et al 2003 the watershed is located in the central great plains ecoregion and is predominately agricultural with only 5 of the watershed classified as urban land use and less than 2 water starks et al 2014 as part of the usda s conservation effects assessment program ceap numerous upland and riparian conservation and structural and water management practices were implemented to reduce sediment loading to the reservoir steiner et al 2008 the reservoir still fails to meet water quality standards based on sediment and it was estimated that 50 of the suspended sediment in the reservoir originated from streambanks wilson et al 2008 streambanks in the watershed consist of either a single sand or sandy loam layer while others exhibit a layering effect of sand or sandy loam layers above and below a more cohesive layer with higher clay content 2 2 1 set stabilization objectives although determining a cost or sediment reduction constraint is likely a necessary step in a real world application the objective for the case study was to determine the most cost effective practice for fivemile creek a series of stabilization scenarios described below are tacitly assumed to be the stabilization practices needed to achieve our unstated sediment reduction goals 2 2 2 determine study reach an rga of the fort cobb reservoir watershed was completed in 2006 and identified the entire length of fivemile creek to be in stages iv and v of the channel evolution model simon and klimetz 2008a moriasi et al 2014 stage iv degradation and widening and stage v aggradation and widening are the two most unstable stages of channel evolution a 10 25 km reach was selected for this study 2 2 3 select stream channel model concepts langendoen 2000 was selected as the stream channel model for simulations on fivemile creek concepts simulates one dimensional hydraulics fluvial erosion and mass wasting processes and graded sediment transport cross sectional geometry fluvial erodibility parameters soil geotechnical parameters bank soil and bed sediment particle size distributions soil and sediment layer and sediment and flow data are required to set up a concepts simulation 2 2 4 collect critical data for model setup five sites along fivemile creek fm1 fm5 were selected for field data collection fig 2 at least one cross sectional survey was conducted at each site using an automatic level one site fm2 was severely impacted by a series of headcuts seven cross sectional surveys were conducted at this site during surveying detailed information on layering and vegetation were recorded soil samples were collected from the thalweg of the channel at each cross section and were analyzed for soil particle size distribution using hydrometer and sieves astm standards d421 astm 2002a and d422 astm 2002b a hobo water level logger onset bourne ma was installed at each site to measure absolute pressure pressures were converted to water depth using hoboware software and atmospheric pressures determined from the nearby oklahoma mesonet site http www mesonet org at each site erodibility parameters were determined using mini jets khanal et al 2016 at least two mini jets were conducted at each site within each visible layer table 1 streambank soil samples were also collected to analyze for bulk density and particle size as described above borehole shear tests bsts handy and fox 1967 were conducted to determine geotechnical parameters φ and c for each soil layer however the texture of the soils and the location of the water table within this watershed resulted in unreliable bst results therefore values based on soil texture were selected from a list of defaults incorporated into bstem a commonly used site scale streambank stability model simon et al 2011 2 2 5 determination of long term erosion rates aerial imagery was used to estimate long term streambank retreat at each site national agricultural imagery program naip images 1 m resolution were obtained for caddo county for 2008 and 2013 using arcmap v10 0 each image was georeferenced and used to determine bank retreat streambanks were digitized at each site for 2008 and 2013 and the average distance between polylines were used as lateral retreat for that site purvis and fox 2016 for site fm5 dense vegetation on both images did not allow for analysis therefore the retreat was estimated from the nearest visible streambank 2 2 6 model set up and calibration because of the large distance between the selected research sites additional cross sections ac were interpolated from lidar data at approximately every 500 m along the channel for a total of 29 acs on fivemile creek fig 2 the acs were more closely spaced around fm2 where several surveyed cross sections were closely spaced due to the presence of headcuts to increase the stability of the model average soil particle size distributions bulk densities and default geotechnical parameters were used for the soil and sediment data of each ac the τ c values from the mini jet data were grouped by soil layer to identify the probability distribution using the individual distribution identification function in minitab v16 minitab inc state college pa the anderson darling ad statistic was used to evaluate the distributions the τ c values followed log normal ad 0 196 p 0 848 and weibull distributions ad 0 276 p 0 25 for the sand and clay layers respectively using these distributions random numbers were generated for τ c for each ac a regression equation was used to determine the relationship between τ c and k d this regression was used to determine k d for each ac for field data collection sites site specific soil data was used average τ c and k d from the jets conducted at each site were used as input a usgs stream gauge was not available for fivemile creek therefore a daily average stream flow hydrograph for 2008 2013 generated from a calibrated swat model for the fort cobb watershed moriasi and starks 2010 guzman et al 2015 was used as flow input fluvial erosion is a function of applied τ and is sensitive to peak flows q therefore the daily average streamflow hydrograph was converted to an hourly triangular hydrograph using the scs triangular hydrograph method scs 1972 the converted hydrograph started and ended at base flow that was determined from the daily averaged flow for flows less than or equal to baseflow hourly discharge was set to the daily average flow one tributary was incorporated into the model concepts requires sediment inflow data q s for tributaries for each sediment class size in the form of power equations 3 q s a q b where a and b are regression parameters table 2 in order to determine the regression parameters of the power equations data from three usgs gauges cobb creek near eakly lake creek and willow creek located in the fort cobb reservoir watershed were combined to develop sediment rating curves for each concepts particle size the usgs collected grab samples during various storm events which were analyzed for particle size using these data and the streamflow at each gauge rating curves between sediment discharge q s kg s 1 and streamflow q cms were created by fitting a regression equation to the data using sigmaplot v12 5 systat software san jose ca water levels from the hobo loggers and a swat generated flow file were used to calibrate the roughness of the bed and banks using the hydraulic submodel in concepts fivemile creek was divided into five sections based on proximity to surveyed cross sections and roughness for all cross sections within each section was assumed to be the same water depth output from concepts was compared to the water depth measured by the water level loggers at each site the roughness of the bed was calibrated during periods of base flow bank roughness was calibrated based on peak flows for storm events table 3 concepts predicted retreat was compared to measured aerial retreat and τ c and k d for each cross section were adjusted as needed fig 3 a brier skill score bss was used to evaluate the calibration based on lateral bank retreat abderrezzak et al 2016 a bss equal to 0 60 was determined which suggested a good model fit streambanks at some sites were heavily vegetated because vegetation can significantly alter τ thompson et al 2004 the effect of vegetation was taken into account during the calibration period the model did not allow for the direct adjustment of τ therefore an α factor was used to modify τ by adjusting τ c and k d in the same manner that daly et al 2015 used to account for the increase in τ around a meander bend 4 ε r k d α τ τ c α k d τ τ c α 2 2 7 select potential stabilization practices an informal survey of landowners along fivemile creek was performed to identify the most appealing practices and what factors might influence their willingness to adopt the practices aesthetics costs available cost sharing programs maintenance requirements absentee landowners and understanding the benefits of each practice were all identified as factors influencing the likelihood of a landowner to adopt a practice tong et al 2017 a primary concern in the watershed was the amount of land necessary for a specific erosion control practice establishment of riparian zones in agricultural areas may take land out of production and sloping banks may result in a significant loss of land depending on the height of the bank therefore several factors were considered when selecting stabilization practices to be simulated including stream incision the amount of land required for stabilization practice and cost three stabilization practices were selected for analysis based on input from landowners and also typical practices grade control gc riprap toe rrt and vegetation and bank grading with both 2 1 veg21 and 3 1 bank slopes veg31 four scenarios of single practices gc rrt veg21 and veg31 and seven combinations of the practices rrt gc veg21 gc veg21 rrt veg21 rrt gc veg31 gc veg31 rrt veg31 rrt gc were simulated 2 2 8 apply stabilization practices to calibrated model a riprap toe was simulated by modifying the erodibility parameters of the bank toe riprap size was determined using the d 50 factor of safety procedure described by stevens et al 1976 the shield s diagram shields 1936 was used to determine τ c and the relationship between τ c and k d developed by simon et al 2011 was used to determine k d for the riprap bank grading was simulated by changing the channel geometry of the cross sections vegetation was simulated by using an α factor of 0 15 thompson et al 2004 root cohesion c r was determined using the riproot model that is incorporated into bstem pollen and simon 2005 a bank coverage of 50 grass and 50 trees was assumed to calculate the additional cohesion for the soils grade control was simulated by setting the bedrock elevation equal to the thalweg elevation langendoen 2011 fivemile creek was divided into five sections based upon land ownership to limit the number of combinations of sites for each of the 11 stabilization scenarios the stabilization practice was first applied to a single cross section eight simulations of single cross sections were simulated next stabilization was applied to a single land owner and then combinations of two three and four landowners finally a scenario with the entire stream stabilized was considered a total of 38 model simulations were performed for each of the stabilization scenarios the sediment yield from each scenario was compared to the baseline sediment yield from the calibrated model to calculate sediment reduction a regression between sediment reduction and fraction of the stream stabilized λ was calculated using sigmaplot v12 5 systat software san jose ca for each of the stabilization scenarios 2 2 9 cost calculations costs for each practice were calculated using rsmeans 2016 facilities and construction cost a stoecker personal communication and site specific channel dimensions excavation costs were based on a 0 38 m3 excavator riprap costs are for random broken stone 45 kg average in place vegetation costs include 900 n tensile strength geotextile fabric bare root willow seedlings with a density of 10 per m2 and fescue seeding using a tractor spreader grade control cost were estimated based on cross vanes spaced approximately five to seven channel widths finally a 5 engineering and surveying expense was included in each cost estimate 3 results and discussion 3 1 sediment reduction and stabilization costs the sediment reduction and cost for each model simulation were determined to develop a pair of graphs for each of the 11 stabilization practices examples are provided in fig 4 a values of λ less than 0 05 corresponds with a single cross section stabilized in the model and represents less than 500 m of streambank stabilized for all practices evaluated this resulted in a small amount of sediment reduction when considering the entire reach a range of λ from 0 1 to 0 3 represents the entire length of stream for a single landowner while λ between 0 3 and 0 5 represents two landowners 0 5 to 0 7 represents three landowners and 0 7 to 0 9 represents four landowners the λ 1 is the scenario where stabilization is applied to the entire length of stream higher r2 values 0 91 0 93 were observed for scenarios involving vegetation veg21 veg21 rrt etc than the scenarios incorporating rrt and gc alone r2 0 49 0 68 3 2 evaluation of stabilization practices after regression equations were developed for the stabilization practices the effectiveness of each practice was compared fig 5 the gc alone scenario resulted in an increase in sediment as it was applied to increasing lengths of the stream if bank protection was not incorporated with grade control the stream began to adjust laterally and an increase in bank erosion was predicted practices that incorporated vegetation as a means of bank stabilization resulted in a higher amount of sediment reduction when compared to rrt only rrt gc and gc only scenarios indicating vegetation needed to be incorporated the veg31 rrt gc scenario resulted in the highest amount of sediment reduction for all lengths of stream stabilized in addition bank protection alone does not prevent channel incision and resulted in lower sediment reduction when compared to the same practice with the addition of grade control i e a higher sediment reduction was observed with the veg21 gc scenario when compared to the veg21 or rrt gc scenarios when compared to the rrt scenario as previous research has shown both bed and bank protection should be incorporated for optimal sediment reduction shields et al 1995 2003 while incorporating multiple practices in stabilization resulted in higher sediment reduction the cost was also much higher cost effectiveness was determined for each scenario and an average cost effectiveness ton and per percent reduction was calculated for each practice table 4 the gc scenario resulted in an increase in sediment yield and therefore the cost effectiveness was not calculated although the veg31 rrt gc scenario was the most effective in terms of sediment reduction it ranked seventh in terms of cost effectiveness the veg21 scenario was the most cost effective stabilization practice with the veg21 gc scenario a close second if a longer time period was considered a higher degree of incision may be observed which could have ultimately caused an increase occurrence of bank failure and higher sediment loads if grade control was not included although slightly less cost effective the veg21 gc scenario would be recommended for fivemile creek for long term stability due to the highly incised nature of this stream system ideally these sediment reduction and cost effectiveness graphs would now be shared with landowners prior to the installation of any stabilization techniques this research did not provide funds for the installation of any techniques but the critical role and input of the stakeholders in the final decision making process cannot be forgotten the sets of graphs can also be used to determine the amount of sediment reduction to be expected based upon an amount invested between different stabilization practices for example if 800 000 was available to invest in a streambank stabilization project along fivemile creek a λ of 0 48 is expected for the rrt scenario which would result in approximately 20 reduction in sediment load fig 4 conversely if the stabilization practice veg21 gc was selected 75 of the stream could be stabilized for a predicted sediment load reduction of 70 fig 4 the veg31 rrt gc scenario resulted in a λ equal to 0 25 and a predicted sediment load reduction of 28 fig 4 for the veg21 only scenario a sediment reduction of 75 was expected therefore for an investment of 800 000 the veg21 scenario would result in the highest sediment reduction for fivemile creek however as previously stated gc may be needed for long term stability the advantage of the proposed framework is the focus on the reach scale impact of site specific stabilization through the modeling of sediment transport stabilization using bank infrastructure has morphological impacts throughout a reach channel bank infrastructure alters the geomorphic process and can lead to more erosion locally and at great distances up and downstream of the stabilized site florsheim et al 2008 reid and church 2015 engineered structures may be ineffective over the long term florsheim et al 2008 the use of riprap or other hard structures increases flood velocities disrupts lateral sediment exchanges florsheim et al 2008 alters flow conditions kondolf 1996 and influences local and downstream sediment transport reid and church 2015 bank stabilization reduces sediment supply downstream but the transport capacity remains the same leading to scour of the bed and banks downstream thereby displacing the original problem kondolf 1996 watson et al 2002 piégay et al 2005 reid and church 2015 if a stream can no longer adjust laterally due to stabilization such as riprap reid and church 2015 the channel will begin to adjust downward leading to incision the increase in erosion downstream may require additional bank stabilization to compensate and could ultimately harden an entire reach florsheim et al 2008 miller and kochel 2010 reid and church 2015 additionally bank protection or stabilization can lead to knickpoint migration upstream causing further instability gregory 2006 a major component of the framework is being able to analyze the construction costs associated with the various stabilization techniques for the case study costs were estimated based on relevant assumptions in oklahoma users would benefit from the development of a spreadsheet tool for estimating project costs relative to different stabilization techniques location and project size for the pre design consideration of potential alternatives the authors are unaware of any such spreadsheet tool other than general guidance provided in technical reports such as nchrp 2005 finally additional research efforts are still needed to provide further guidance on data collection parametrization calibration and use of the hydraulic sediment transport and bank stability models used in such frameworks for example one dimensional hydraulic and sediment transport models require adjustments to appropriate simulate applied shear stress distributions on meandering streambanks with vegetation higher dimensional models may alleviate this issue but also require more intense data collection to characterize streams especially considering the need to simulate reach scale processes advancements are also needed in understanding how to incorporate or scale variability in erodibility properties measured at select cross sections to the reach scale 4 conclusions streambank stabilization practices can be very costly therefore it is important to understand the possible benefits prior to implementation and to evaluate potential alternatives process based models are useful tools to evaluate potential streambank stabilization practices and a number of validated reach scale sediment transport and bank erosion stability models are available the framework presented in this paper provides an example of how to use these tools to determine the most effective practice for a particular stream system based on both sediment reduction and cost analyses choosing practices without regard to cost would result in greater costs per ton at the expense of other projects or more miles of river stabilized this methodology was applied to fivemile creek to evaluate various stabilization scenarios to determine the most cost effective stabilization practice it was determined that vegetation with 2 1 bank slopes and grade control would be the best choice for this stream system acknowledgments this project was supported by national integrated water quality program grant no 2013 51130 2184 from the usda national institute of food and agriculture this research was partially supported by the usda nrcs cropland ceap effort and is also a contribution of the usda ars southern plains long term agroecosystem research site the authors would like to acknowledge periann russell of the north carolina department of environmental quality and amanda fox for reviewing earlier versions of this manuscript 
26459,to fill a need for risk based environmental management optimization we have developed pestpp opt a model independent tool for resource management optimization under uncertainty pestpp opt solves a sequential linear programming slp problem and also implements optional efficient on the fly without user intervention first order second moment fosm uncertainty techniques to estimate model derived constraint uncertainty combined with a user specified risk value the constraint uncertainty estimates are used to form chance constraints for the slp solution process so that any optimal solution includes contributions from model input and observation uncertainty in this way a single answer that includes uncertainty is yielded from the modeling analysis pestpp opt uses the familiar pest pest model interface protocols which makes it widely applicable to many modeling analyses the use of pestpp opt is demonstrated with a synthetic integrated surface water groundwater model the function and implications of chance constraints for this synthetic model are discussed keywords uncertainty quantification model independent parameter estimation optimization under uncertainty software availability the source code for pestpp opt is available as part of the pest software suite welter et al 2015 https github com dwelter pestpp in addition to the source code the git repository includes statically linked osx and pc executables as well as three example problems adapted from gwm ahlfeld et al 2005 e g the dewater problem the seawater problem and the supply2 problem including the example problem presented herein 1 introduction environmental modeling analyses are frequently undertaken with the focus of providing a decision support tool for resource managers a critical role for modeling gorelick and zheng 2015 horne et al 2016 rigorous resource management optimization is a widely recognized approach for providing optimal and unbiased answers to resource management questions readers are referred to singh 2012 yeh 2015 gorelick and zheng 2015 horne et al 2016 among others for recent reviews of the importance and application of environmental resource management optimization however for environmental models to be used appropriately in a risk based decision making context these models should include estimates of uncertainty in important model outcomes this uncertainty conveys a clear understanding of the reliability of the simulation results and the management solutions that depend on these simulation results providing a margin of safety to account for imperfections in the model and data supplying it anderson et al 2015 unfortunately in practice providing resource managers with a range of possible model outcomes can raise more questions than the modeling analysis answers phrases such as how can we manage to a range of outcomes or we need a single number are common responses to modeling analyses presented in the context of uncertainty one possible solution to this conundrum is the use of optimization under uncertainty techniques sahinidis 2004 this type of management optimization problem seeks an optimal solution to a resource management problem but includes recognized sources of uncertainty in this way the optimal solution provides a single answer but that answer includes uncertainty uncertainty arising from uncertain model inputs and optionally observation noise is propagated to constraints which in turn affects the optimal solution unfortunately while many approaches and techniques to optimization under uncertainty have been proposed in the literature joodavi et al 2015 zekri et al 2015 nouiri et al 2015 tsoukalas and makropoulos 2015 sreekanth et al 2016 beh et al 2017 few if any generalized model independent non intrusive tools exist for practitioners to apply these techniques to environmental models gorelick and zheng 2015 horne et al 2016 compounding this lack of tools is the large computational burden and high dimensionality associated with many types of environmental models especially groundwater or integrated surface water groundwater models that can preclude the application of many approaches to optimization and optimization under uncertainty herein we present pestpp opt an efficient model independent non intrusive tool for optimization under uncertainty pestpp opt implements the simplex algorithm dantzig et al 1955 to solve the linear programming problem and also implements sequential linear programming problem slp ahlfeld and mulligan 2000 to resolve mild nonlinearities in the relation between decision variables and constraints arising from the numerical model we extend the work of wagner and gorelick 1987 to use a bayesian formulation of first order second moment fosm uncertainty techniques to efficiently and seamlessly without user intervention estimate prior or posterior uncertainty in the constraints derived from model output thereby propagating model parameter and observation uncertainty to the constraints used in the optimization process this fosm based constraint uncertainty estimation happens on the fly programmatically and without user intervention and depending on the selected settings requires no additional model runs because pestpp opt uses the familiar pest doherty 2015 pest welter et al 2015 model independent non intrusive framework the existing pest and pest user base will now be able to easily apply these sophisticated management optimization techniques with little additional user effort beyond typical parameter estimation the following sections briefly describe the theory of slp and fosm and present an example application of pestpp opt to solve a chance constrained integrated groundwater surface water management problem 2 theory 2 1 terminology parameter estimation and management optimization share many common elements but in some cases employ different terminology or worse have similar terminology with differing definitions therefore we now explicitly define how we use these terms in this paper in parameter estimation pe and uncertainty quantification uq parlance parameters are uncertain model inputs any numeric quantity used both the simulator that are nominated for adjustment during history matching and or uncertainty analysis observations in pe and uq analyses are measured data typically collected from the environmental system being modeled in management optimization parlance following ahlfeld et al 2009 decision variables are model inputs whose values are to be determined by the optimization process that is decision variables are some model inputs that can be controlled constraints are conditions that must be satisfied by any optimal solution including maximum and minimum allowable values for decision variables or alternatively constraints may be derived from model output or both constraints based on model output may include simulated groundwater levels stream flows and or streamflow depletions barlow and leake 2012 fienen et al 2017 constraints may also be derived from model output for example simulated differences in model simulated states in space or time e g drawdowns or water level difference across a confining unit most simulators only output simulated states but users can post process these states to calculate differences in pe and uq parlance forecasts are unobserved quantities of interest derived at least in part from model outputs because model inputs e g parameters are uncertain so too are forecasts in as much as a given forecast depend on the parameters similarly constraints that are derived from model output are unobserved quantities that because of parameter uncertainty are also uncertain note that observations of system states may be available at constraint locations e g water levels from an existing well however the simulated response to a given management scenario remains uncertain and is therefore similar to a forecast in a pe and uq analysis in pe and uq parlance the objective function is the functional typically composed of differences between observations and model simulated equivalents typically this functional is based on the ℒ 2 norm sum of squares of the differences the focus of pe is to minimize this functional in management optimization the objective function is composed of combinations of decision variables and may either be minimized or maximized depending on the specific application subject to constraints moreover herein we use linear programming so that the objective function is a weighted linear combination of decision variables 2 2 linear programming linear programming lp is a solution to the optimization problem that relies on an assumption of a linear relationship between decision variables and constraints as well as an objective function that is a linear combination of decision variables nocedal and wright 2006 using vector notation lp can be summarized as 1 minimize c t x subject to a x b x 0 where x is a vector of m decision variables c is vector of m objective function coefficients a is an n m matrix of constraint coefficients and b is a vector of n specified constraint values following ahlfeld and mulligan 2000 herein the matrix a is called a response matrix and is calculated in part by evaluating the model with the perturbed decision variables and recording the change in constraints specifically 2 a i x j a i x j δ x j a i x j δ x j where a i is a vector of simulated constraint values row of a x j is the j t h decision variable and δ x j is a small perturbation of the j t h decision variable this formulation is similar to the finite difference approximation used to fill the jacobian matrix j of sensitivities in many pe algorithms e g doherty 2015 welter et al 2015 constraints in the lp process can be formed directly from information about decision variables such as maximum and minimum acceptable values and or weighted linear combinations of decision variables constraints may also be derived from numerical model output such as simulated water level or drawdown simulated stream flow or stream flow depletion e g barlow and leake 2012 fienen et al 2017 herein we refer to constraints derived wholly or partially from numerical model output as model derived constraints note that only model derived constraints require application of equation 2 for calculation of response coefficients constraints not dependent on model outputs can be formulated without the need to evaluate the model to account for non linearity in the relation between decision variables and constraints the lp solution process can be combined with an iterative process where new estimates of the decision variable values x vector in equation 1 are calculated using the current a matrix then new a matrix is calculated using the updated decision variable values this process which is the slp method ahlfeld and mulligan 2000 is continued until the decision variables and objective function converge to stable values an slp approach often is necessary for groundwater and integrated surface water groundwater models that include mild nonlinearities caused by a moving water table and head dependent boundary conditions such as streams ahlfeld et al 2005 2 3 chance constraints using fosm model derived constraints must be evaluated by running the model using specified values of decision variables the simulated values associated with model derived constraints are uncertain because of uncertainty in parameters e g inputs to the model that are not known uncertainties or imperfections in the model structure and uncertainty in the observations used to condition the parameters in the pe process incomplete knowledge of parameters is recognized as a major source of uncertainty in subsurface models moore and doherty 2006 to account for the parameter and observation uncertainty in the simulated values associated with model derived constraints we use first order second moment fosm doherty 2015 white et al 2016 welter et al 2015 uncertainty estimation to extend the chance constraint approach of wagner and gorelick 1987 fosm techniques are efficient scale well to large numbers of parameters and observations and have been applied in many environmental modeling studies fienen et al 2010 dausman et al 2010 leaf et al 2015 brakefield et al 2015 briefly the posterior parameter covariance matrix can be calculated as 3 σ θ σ θ σ θ j t j σ θ j t σ ε 1 j σ θ where σ θ is the prior parameter covariance matrix σ ε is the epistemic observation noise covariance matrix and j is the jacobian matrix of partial first derivatives sensitivities of observations with respect to parameters in a linear pe framework the simulated value for a given model derived constraint s can be calculated as 4 s y t θ where y is the vector of constraint sensitivity to each parameter and θ is the vector of parameter values it follows then that the prior and posterior uncertainty estimate e g variance for the simulated value associated with a model derived constraint s σ s 2 and σ s 2 can be calculated by projecting the requisite parameter covariance matrix to the constraint output space using a constraint sensitivity vector 5 σ s 2 y t σ θ y and 6 σ s 2 y t σ θ y where σ θ and σ θ are the prior and posterior parameter covariance matrices respectively σ s 2 and σ s 2 are the prior and posterior variances of constraint s readers are referred to menke 1989 golub and van loan 1996 tarantola 2005 fienen et al 2010 doherty 2015 welter et al 2015 and white et al 2016 for a full treatment of the concepts surrounding fosm based uncertainty estimation in environmental modeling note if no observations are available to condition parameters then σ s 2 of equation 5 is equal to σ s 2 of equation 6 in summary the amount to which any given model derived constraint is uncertain depends on several factors including the model used for evaluating simulated values associated with model derived constraints number and types of parameter used in the pe and uq processes number and types of observations used in the pe and uq process the prior parameter covariance matrix σ θ in equation 3 the observation noise covariance matrix σ ε in equation 3 the relation between parameters and observations encapsulated in the jacobian matrix j in equation 3 the relation between parameters and simulated values associated with model derived constraints the vector y in equations 5 and 6 when model derived constraint uncertainty is estimated using equations 3 5 and 6 a user specified risk value and the probit function bliss 1934 are used to shift the simulated value towards either a risk tolerant or risk averse value using the gaussian distribution implied by equations 5 and 6 the shifted values are then used in the slp solution process the technique of shifting simulated values associated with model derived constraints with respect to uncertainty and risk is referred to as chance constraint programming charnes and cooper 1959 miller and wagner 1965 tung 1986 wagner and gorelick 1987 hantush and marino 1989 chan 1994 calculating the statistical distribution associated with the simulated value for model derived constraints is accomplished by using fosm techniques the value of risk which ranges from 0 0 to 1 0 represents a tolerance in a probabilistic sense in concluding that an optimal solution is truly optimal with respect to uncertain model derived constraints because the true value of the model derived constraint is not known e g is uncertain the value of risk represents a probability that the simulated values yielded by the model that are used to evaluate model derived constraints in the optimization solution process are truly satisfied a risk value of 0 5 corresponds to a risk neutral optimal solution the probability density of the statistical distribution associated with a given model derived constraint is equally distributed around the simulated value there is a 50 probability that model derived constraints are truly satisfied the risk neutral solution is the same as a deterministic slp solution where the simulated value associated with the model derived constraint yielded by the model is used a risk value greater than 0 5 is considered risk averse the simulated values associated with model derived constraints are shifted towards the violation region in constraint space conversely risk values less than 0 5 are considered risk tolerant the simulated values associated with model derived constraints are shifted towards the satisfactory region of constraint space consider a hypothetical groundwater management problem with a single pumping well and single less than type water level constraint a model derived constraint the objective of the management optimization analysis is to minimize the amount of pumping e g minimize costs while meeting the required constraint of the water level being less than 19 0 meters that is 19 0 meters is the highest acceptable water level values less than 19 0 meters are acceptable but not optimal because less pumping could be used the water level constraint is evaluated using a numerical groundwater flow model so the simulated value associated with the model derived model output water level is uncertain this can be attributed in part to imperfect knowledge of subsurface properties model parameters fig 1 displays several hypothetical cases using a statistical distribution implied by fosm techniques around the simulated model output value the width of the gaussian distribution around the simulated mean value on fig 1 is based on σ of equation 6 in this illustration we discuss the interpretation of optimality for representative risk tolerant 0 05 risk neutral 0 5 and risk averse 0 95 levels of risk note other values of risk in the range 0 0 1 0 are also acceptable we select 0 05 and 0 95 for demonstration purposes in the context of environmental resource management one would pursue a risk averse solution when there are legal financial penalties associated with constraint violation or when the results of constraint violations are irreversible e g subsidence stream depletion alternatively a risk tolerant solution can be pursued when any undesirable results of constraint violations are easily reversed e g seasonal drawdowns fig 1a shows the hypothetical case of risk equals 0 95 risk averse the algorithm concludes with an optimal solution when the 95 probability value of the fosm implied posterior distribution of the simulated value is equal to the specified constraint value of 19 meters in this case additional pumping is needed to produce a model output water level of 13 meters so that the upper 95 probability value is less than or equal to 19 meters using a risk value of 0 95 indicates that there is 95 probability or chance that the true value of the constraint is satisfied in the optimal solution in this way a risk value of 0 95 leads to a conservative optimal solution to the management problem where it is more important to meet the constraint than to save money this idea is similar to a margin of safety or design factor in engineering applications e g shigley and mischke 1996 where the design criterion incorporates a certain probability that the design will be inadequate either due to uncertainty in the design process or uncertainty in the stress applied to the constructed object fig 1b shows the hypothetical case of risk equals 0 5 risk neutral in this case the simulated value model output water level is used directly in the optimization solution process a risk value of 0 5 yields a 50 probability or chance that the constraint is truly satisfied this is the same solution that would be obtained if chance constraints were not used at all fianlly fig 1 c shows the hypothetical case of risk equals 0 05 risk tolerant in this case much less pumping is required to satisfy the constraint the model output water level is 25 meters which requires substantially less pumping compared to the risk averse case however there is only a 5 probability or chance that the constraint is truly satisfied this case represents a situation where one would err on the side of permissibility such that the importance of keeping pumping costs low is greater than actually maintaining the water level below the specified constraint value of 19 meters 3 limitations as with any modeling analysis tool pestpp opt is limited by the validity of the implicit and explicit assumptions used in its application these include but are not limited to an approximately linear relation between decision variables and model derived constraints is a valid representation of the relation an approximately linear relation between parameters and model derived constraints is valid is a valid representation of the relation the second moment of the posterior parameter distribution is appropriately described by a covariance matrix σ θ of equation 3 the second moment of the prior and posterior constraint distributions is appropriately described by a variance σ s 2 and σ s 2 of equations 5 and 6 the environmental model used to simulate the relation between parameter observations decision variables and constraints is an appropriate simulator practitioners should carefully evaluate the validity and implications of these assumptions for a given application of pestpp opt 4 implementation pestpp opt implements management optimization with fosm based chance constraints in a model independent non intrusive framework based on the model interface protocols from pest doherty 2010 within the pestpp opt tool the open source optimization library clp forrest et al 2016 part of the computational infrastructure for operations research coin or lougee heimer 2003 project is used to solve linear programs using the simplex algorithm dantzig et al 1955 the clp solution process is combined with an iterative framework to repeatedly call the simplex algorithm while updating the values of decision variables the iterative process referred to as sequential linear programming ahlfeld and mulligan 2000 or successive linear programming baker and lasdon 1985 slp allows pestpp opt to handle nonlinearities in the relation between decision variables and model based constraints the process is as follows 1 calculate response coefficients of model derived constraints with respect to decision variables to fill the response matrix a matrix of equation 1 2 optional calculate sensitivities of observations and model derived constraints with respect to parameters if chance constraints are to be used 3 optional using fosm and user specified risk shift model derived constraints 4 formulate and solve the lp problem implied by equation 1 using clp 5 update the decision variables to the optimal solution found in step 2 and evaluate model at new decision variables 6 check convergence if not converged go to step 1 convergence is determined using a single tolerance opt iter tol in appendix a this tolerance is used to monitor changes in both the optimal objective function and decision variables if the objective function and each decision variable changes less than this tolerance the slp process is considered converged however users are encouraged to monitor the results of each slp iteration recall that the model must be run once for each decision variable for each iteration to fill the rows of a that correspond to model derived constraints however users can nominate decision variables that do not have any effect on model derived constraints as external decision variables external decision variables do not require a model run for response coefficients if fosm based chance constraints are being used the model must also be run once for each adjustable parameter to fill the j matrix for calculation of σ θ and y vectors in equation 6 however users can control how often the fosm based chance constraints are updated or supply an existing jacobian matrix from a previous parameter estimation analysis to reduce or eliminate the additional computation burden needed to implement chance constraints in many cases the changes to fosm constraints due to candidate solutions of the decision variables are likely to be minor the j of equation 3 and y of equations 5 and 6 are not affected by changing decision variables therefore these quantities can be reused for each slp iteration reusing the j can result in great computational savings but implies that the decision variables have no effect on the relation between parameters constraints and observations if used for conditioning the validity of this assumption is specific to the optimization problem that is being solved to that end the capability to update j and y during each slp iteration is included in case changes in decision variables affect these quantities users should construct a pest model independent dataset that includes decision variables such as rates for nominated pumping wells decision variables are input as parameters in the pest datasets upper and lower bounds for decision variables are used as constraints on decision variable values decision variables are differentiated from model parameters using parameter group designations parameters such as hydraulic conductivity storage and boundary condition elements the prior uncertainty for these parameters should be described using the parameter bounds or with a user specified covariance matrix parameters are differentiated from decision variables by using parameter group designations model derived constraints such as water levels or drawdowns stream flows or streamflow depletion model derived constraints are defined as observations in the pest datasets with zero weight and in a separate group from pe observations used for history matching conditioning in the pest datasets observations used for history matching conditioning with weights commensurate with how well the model can match these observations these also must be in groups designated separately from the optimization constraints if no observations are included in the analysis constraint uncertainty is estimated only on the basis of prior parameter uncertainty equation 5 linear summation constraints such as a minimum and or maximum total pumping rate created by summing in an algebraic sense decision variables linear summation constraints are input as prior information equations in the pest datasets linear objective function as a prior information equation or as a space delimited ascii file with decision variable name and objective function coefficient pestpp opt supports a wide range of input options to provide flexibility to users but all options have default values appendix a summarizes the optional pestpp opt arguments in the pest control file several components of the existing pest code base were leveraged to enhance the capabilities of the pestpp opt tool including support for a wide range of options related to calculating response coefficients including capabilities to calculate central 3 point coefficients as well as control of the size of the decision variable perturbation distributed high throughput computing resources via the tcp ip yamr run manager welter et al 2015 to parallelize the calculation of response coefficients a in equation 1 and sensitivities for fosm based constraint uncertainty calculation j of equation 3 and y of equations 5 and 6 5 an example of linear programming with chance constraints here we present an example application of pestpp opt based on the supply2 problem distributed with the gwm 2005 groundwater management software source code ahlfeld et al 2005 the numerical model is a groundwater flow model implemented in modflow the original supply groundwater management problem is described by ahlfeld et al 2005 p 84 85 this sample problem represents a transient water supply problem in which total ground water withdrawals over a 3 year period are limited by the amount of streamflow depletion allowed in two streams that are in hydraulic connection with the aquifer the 3 year period is divided into 12 seasons winter spring summer and fall of each year each of which is represented by a single stress period the aquifer is confined and the area of interest is 6000 ft long by 5000 ft wide the model consists of a single layer with 25 rows and 30 columns each model cell is 200 ft by 200 ft the aquifer is homogeneous and isotropic with a transmissivity of 5000 ft2 d and a storage coefficient of 0 05 the modeled area is bounded on the east and west by no flow conditions and on the north and south by constant heads that decrease in elevation from west to east the aquifer is recharged at a rate of 0 0005 ft d in the winter 0 002 ft d in the spring 0 ft d in the summer and 0 001 ft d in the fall both streams are 20 ft wide and have a streambed conductance of 20 000 ft2 d the mainstem has a slope of 0 0025 whereas the tributary stream has a slope of 0 0010 the management objective is to maximize the value of ground water withdrawn from four wells over the 3 year period while simultaneously limiting streamflow depletions at four streamflow constraint locations along the two streams fig 2 presents the relevant features of the supply2 optimization problem note we are using the supply2 version of this synthetic groundwater management problem the supply2 problem only considers three pumping wells q1 q2 and q4 fig 2 ahlfeld et al 2007 2009 the formulation of the objective function for this version of the supply2 problem is presented below the optimization problem for supply2 has the following decision variables q1 is the pumping rate at well q1 during the entire simulation q2a is the pumping rate at well q2 during winter stress periods 1 5 9 q2b is the pumping rate at well q2 during spring stress periods 2 6 10 q2c is the pumping rate at well q2 during summer stress periods 3 7 11 q2d is the pumping rate at well q2 during fall stress periods 4 8 12 q4a is the pumping rate at well q4 during spring stress periods 2 6 10 q4b is the pumping rate at well q4 during fall stress 4 8 12 im9 is an external decision variable representing an imported source of water during stress period 9 im10 is an external decision variable representing an imported source of water during stress period 10 im11 is an external decision variable representing an imported source of water during stress period 11 im12 is an external decision variable representing an imported source of water during stress period 12 subject to the following constraints pumping rates for each well must be greater than or equal to 0 0 f t 3 d a y pumping rates for each well must be less than or equal to 50 000 f t 3 d a y total water supply rate for each season in the first year stress periods 1 4 must be greater than or equal to 30 000 f t 3 d a y total water supply rate for each season in the second year stress periods 5 8 must be greater than or equal to 25 000 f t 3 d a y total water supply rate for each season in the third year stress periods 9 12 must be greater than or equal to 45 000 f t 3 d a y total water supply rate for any stress period must be less than or equal to 80 000 f t 3 d a y streamflow depletion at c1 must be less than or equal to 15 000 f t 3 d a y for any stress period streamflow depletion at c3 must be less than or equal to 15 000 f t 3 d a y for any stress period streamflow depletion at c2 must be less than or equal to 20 000 f t 3 d a y for summer and fall seasons stress periods 3 4 7 8 11 12 streamflow depletion at c4 must be less than or equal to 30 000 f t 3 d a y for summer and fall seasons stress periods 3 4 7 8 11 12 where streamflow depletion is defined as the decrease in simulated streamflow resulting from simulated pumping streamflow depletion constraints are model derived constraints and are therefore treated as uncertain the supply2 example problem specifies the value of produced water versus imported water there is a net benefit from pumping at each well of 1 per 1000 f t 3 of water withdrawn that is 0 001 f t 3 ahlfeld et al 2005 the 0 0012 f t 3 rate from the external water source variables indicates a cost for importing the water that outweighs the benefit from pumping groundwater by summing the total number of simulation days that each decision variable is active and multiplying by the specified rates the following objective function can be formulated 7 1 095 q 1 0 276 q 2 a 0 273 q 2 b 0 273 q 2 c 0 273 q 2 d 0 273 q 4 a 0 273 q 4 b 0 1104 i m 9 0 1092 i m 10 0 1092 i m 11 0 1092 i m 12 the optimal risk neutral objective function value for the supply2 optimization problem found by pestpp opt is 53 561 which is comparable to the optimal value found by gwm 2005 53 028 the difference in the optimal solution from gwm and pestpp opt is attributable to nonlinearity in the optimization problem furthermore optimal solution to this example problem is sensitive to the choice of decision variable perturbation further evidence of nonlinearity by using different decision variable perturbation values the presence and relative importance of nonlinearity can be detected by changes in the resulting optimal objective function value and simulated constraint values to implement fosm based chance constraints the transmissivity in each model cell and the conductance of each sfr reach were assumed to be uncertain and were therefore treated as parameters a total of 753 transmissivity and conductance parameters were added to the pest control file the uncertainty in each transmissivity and conductance parameter is specified using the parameter upper and lower bounds transmissivity and conductance upper and lower bounds were specified to be 4000 f t 2 d a y and 6000 f t 2 d a y and 4 5 f t 2 d a y and 5 5 f t 2 d a y respectively to apply chance constaints to this example problem we assumed that no observations were available to condition the uncertain parameters therefore the prior parameter uncertainty described by σ θ along with equation 5 is used to estimate streamflow depletion constraint uncertainty in an effort to elucidate how the pestpp opt chance constraints affect the optimal solution we performed several pestpp opt scenarios for a range of risk values hantush and marino 1989 completed a similar analysis this analysis was completed in less than 5 min on a laptop computer fig 3 shows how the choice of risk affects the optimal objective function value as risk increases the benefit of pumping groundwater decreases until the problem becomes infeasible no solution that simultaneously satisfies all constraints can be found for risk values greater than 0 65 for example a risk value of 0 01 risk tolerant stance increases the optimal objective function value by nearly 10 000 compared to the risk neutral stance that is by permitting a 1 chance that the streamflow depletion constraints are truly satisfied one can increase the amount of money made in the water supply problem however a risk value of 0 65 risk averse stance decreases optimal objective function by approximately 10 000 which is the cost of going from a 50 chance of constraint satisfaction to 65 chance of constraint satisfaction in total model derived constraint uncertainty arising from transmissivity and sfr conductance uncertainty has a value of nearly 20 000 the optimal decision variable values for the most risk tolerant risk neutral and most risk averse stances are shown in fig 4 6 conclusion we have developed pestpp opt a model independent tool for efficient optimization under uncertainty with minimal user intervention pestpp opt implements the proven simplex algorithm in an iterative fashion to solve the sequential linear programming problem using fosm based uncertainty estimation and a user specified risk tolerance pestpp opt implements on the fly constraint uncertainty estimation facilitating optimal solutions to resource management questions that include uncertainty given the encapsulated nature of the pestpp opt source code future work will focus on adding support for mixed integer programming non linear optimization solvers as well as capabilities for stack based optimization under uncertainty e g bayer et al 2008 for non linear optimization under uncertainty problems acknowledgments the authors thank brian wagner and two anonymous reviewers for the helpful suggestions any use of trade firm or product names is for descriptive purposes only and does not imply endorsement by the u s government appendix a supplementary data the following is the supplementary data related to this article data profile data profile appendix a supplementary data supplementary data related to this article can be found at https doi org 10 1016 j envsoft 2017 11 019 appendix a pestpp opt optional arguments argument name argument type description default behavior opt dec var groups group names comma separated string identifies which parameter groups to treat as decision variables all parameters are treated as decision variables opt constraint groups group names comma separated string identifies which observation groups to treat as model based constraints observation groups listed must start with the le or less for less than constraints and ge or greater for greater than constraints all observation groups meeting the naming rules will be treated as constraints opt obj func obj func name string prior information equation name or two column ascii file that contains the objective function coefficients use objective function coefficient of 1 0 for every decision variable opt risk risk float in range 0 0 1 0 risk value for use in chance constrained optimization disable chance constraints opt recalc fosm every reuse iter integer number of iterations of slp to reuse the fosm based chance constraint calculations recalculate fosm chance constraints every iteration of slp opt iter tol tolerance float used to monitor changes in objective function and decision variable values during slp iterations assigned a value of 0 001 
26459,to fill a need for risk based environmental management optimization we have developed pestpp opt a model independent tool for resource management optimization under uncertainty pestpp opt solves a sequential linear programming slp problem and also implements optional efficient on the fly without user intervention first order second moment fosm uncertainty techniques to estimate model derived constraint uncertainty combined with a user specified risk value the constraint uncertainty estimates are used to form chance constraints for the slp solution process so that any optimal solution includes contributions from model input and observation uncertainty in this way a single answer that includes uncertainty is yielded from the modeling analysis pestpp opt uses the familiar pest pest model interface protocols which makes it widely applicable to many modeling analyses the use of pestpp opt is demonstrated with a synthetic integrated surface water groundwater model the function and implications of chance constraints for this synthetic model are discussed keywords uncertainty quantification model independent parameter estimation optimization under uncertainty software availability the source code for pestpp opt is available as part of the pest software suite welter et al 2015 https github com dwelter pestpp in addition to the source code the git repository includes statically linked osx and pc executables as well as three example problems adapted from gwm ahlfeld et al 2005 e g the dewater problem the seawater problem and the supply2 problem including the example problem presented herein 1 introduction environmental modeling analyses are frequently undertaken with the focus of providing a decision support tool for resource managers a critical role for modeling gorelick and zheng 2015 horne et al 2016 rigorous resource management optimization is a widely recognized approach for providing optimal and unbiased answers to resource management questions readers are referred to singh 2012 yeh 2015 gorelick and zheng 2015 horne et al 2016 among others for recent reviews of the importance and application of environmental resource management optimization however for environmental models to be used appropriately in a risk based decision making context these models should include estimates of uncertainty in important model outcomes this uncertainty conveys a clear understanding of the reliability of the simulation results and the management solutions that depend on these simulation results providing a margin of safety to account for imperfections in the model and data supplying it anderson et al 2015 unfortunately in practice providing resource managers with a range of possible model outcomes can raise more questions than the modeling analysis answers phrases such as how can we manage to a range of outcomes or we need a single number are common responses to modeling analyses presented in the context of uncertainty one possible solution to this conundrum is the use of optimization under uncertainty techniques sahinidis 2004 this type of management optimization problem seeks an optimal solution to a resource management problem but includes recognized sources of uncertainty in this way the optimal solution provides a single answer but that answer includes uncertainty uncertainty arising from uncertain model inputs and optionally observation noise is propagated to constraints which in turn affects the optimal solution unfortunately while many approaches and techniques to optimization under uncertainty have been proposed in the literature joodavi et al 2015 zekri et al 2015 nouiri et al 2015 tsoukalas and makropoulos 2015 sreekanth et al 2016 beh et al 2017 few if any generalized model independent non intrusive tools exist for practitioners to apply these techniques to environmental models gorelick and zheng 2015 horne et al 2016 compounding this lack of tools is the large computational burden and high dimensionality associated with many types of environmental models especially groundwater or integrated surface water groundwater models that can preclude the application of many approaches to optimization and optimization under uncertainty herein we present pestpp opt an efficient model independent non intrusive tool for optimization under uncertainty pestpp opt implements the simplex algorithm dantzig et al 1955 to solve the linear programming problem and also implements sequential linear programming problem slp ahlfeld and mulligan 2000 to resolve mild nonlinearities in the relation between decision variables and constraints arising from the numerical model we extend the work of wagner and gorelick 1987 to use a bayesian formulation of first order second moment fosm uncertainty techniques to efficiently and seamlessly without user intervention estimate prior or posterior uncertainty in the constraints derived from model output thereby propagating model parameter and observation uncertainty to the constraints used in the optimization process this fosm based constraint uncertainty estimation happens on the fly programmatically and without user intervention and depending on the selected settings requires no additional model runs because pestpp opt uses the familiar pest doherty 2015 pest welter et al 2015 model independent non intrusive framework the existing pest and pest user base will now be able to easily apply these sophisticated management optimization techniques with little additional user effort beyond typical parameter estimation the following sections briefly describe the theory of slp and fosm and present an example application of pestpp opt to solve a chance constrained integrated groundwater surface water management problem 2 theory 2 1 terminology parameter estimation and management optimization share many common elements but in some cases employ different terminology or worse have similar terminology with differing definitions therefore we now explicitly define how we use these terms in this paper in parameter estimation pe and uncertainty quantification uq parlance parameters are uncertain model inputs any numeric quantity used both the simulator that are nominated for adjustment during history matching and or uncertainty analysis observations in pe and uq analyses are measured data typically collected from the environmental system being modeled in management optimization parlance following ahlfeld et al 2009 decision variables are model inputs whose values are to be determined by the optimization process that is decision variables are some model inputs that can be controlled constraints are conditions that must be satisfied by any optimal solution including maximum and minimum allowable values for decision variables or alternatively constraints may be derived from model output or both constraints based on model output may include simulated groundwater levels stream flows and or streamflow depletions barlow and leake 2012 fienen et al 2017 constraints may also be derived from model output for example simulated differences in model simulated states in space or time e g drawdowns or water level difference across a confining unit most simulators only output simulated states but users can post process these states to calculate differences in pe and uq parlance forecasts are unobserved quantities of interest derived at least in part from model outputs because model inputs e g parameters are uncertain so too are forecasts in as much as a given forecast depend on the parameters similarly constraints that are derived from model output are unobserved quantities that because of parameter uncertainty are also uncertain note that observations of system states may be available at constraint locations e g water levels from an existing well however the simulated response to a given management scenario remains uncertain and is therefore similar to a forecast in a pe and uq analysis in pe and uq parlance the objective function is the functional typically composed of differences between observations and model simulated equivalents typically this functional is based on the ℒ 2 norm sum of squares of the differences the focus of pe is to minimize this functional in management optimization the objective function is composed of combinations of decision variables and may either be minimized or maximized depending on the specific application subject to constraints moreover herein we use linear programming so that the objective function is a weighted linear combination of decision variables 2 2 linear programming linear programming lp is a solution to the optimization problem that relies on an assumption of a linear relationship between decision variables and constraints as well as an objective function that is a linear combination of decision variables nocedal and wright 2006 using vector notation lp can be summarized as 1 minimize c t x subject to a x b x 0 where x is a vector of m decision variables c is vector of m objective function coefficients a is an n m matrix of constraint coefficients and b is a vector of n specified constraint values following ahlfeld and mulligan 2000 herein the matrix a is called a response matrix and is calculated in part by evaluating the model with the perturbed decision variables and recording the change in constraints specifically 2 a i x j a i x j δ x j a i x j δ x j where a i is a vector of simulated constraint values row of a x j is the j t h decision variable and δ x j is a small perturbation of the j t h decision variable this formulation is similar to the finite difference approximation used to fill the jacobian matrix j of sensitivities in many pe algorithms e g doherty 2015 welter et al 2015 constraints in the lp process can be formed directly from information about decision variables such as maximum and minimum acceptable values and or weighted linear combinations of decision variables constraints may also be derived from numerical model output such as simulated water level or drawdown simulated stream flow or stream flow depletion e g barlow and leake 2012 fienen et al 2017 herein we refer to constraints derived wholly or partially from numerical model output as model derived constraints note that only model derived constraints require application of equation 2 for calculation of response coefficients constraints not dependent on model outputs can be formulated without the need to evaluate the model to account for non linearity in the relation between decision variables and constraints the lp solution process can be combined with an iterative process where new estimates of the decision variable values x vector in equation 1 are calculated using the current a matrix then new a matrix is calculated using the updated decision variable values this process which is the slp method ahlfeld and mulligan 2000 is continued until the decision variables and objective function converge to stable values an slp approach often is necessary for groundwater and integrated surface water groundwater models that include mild nonlinearities caused by a moving water table and head dependent boundary conditions such as streams ahlfeld et al 2005 2 3 chance constraints using fosm model derived constraints must be evaluated by running the model using specified values of decision variables the simulated values associated with model derived constraints are uncertain because of uncertainty in parameters e g inputs to the model that are not known uncertainties or imperfections in the model structure and uncertainty in the observations used to condition the parameters in the pe process incomplete knowledge of parameters is recognized as a major source of uncertainty in subsurface models moore and doherty 2006 to account for the parameter and observation uncertainty in the simulated values associated with model derived constraints we use first order second moment fosm doherty 2015 white et al 2016 welter et al 2015 uncertainty estimation to extend the chance constraint approach of wagner and gorelick 1987 fosm techniques are efficient scale well to large numbers of parameters and observations and have been applied in many environmental modeling studies fienen et al 2010 dausman et al 2010 leaf et al 2015 brakefield et al 2015 briefly the posterior parameter covariance matrix can be calculated as 3 σ θ σ θ σ θ j t j σ θ j t σ ε 1 j σ θ where σ θ is the prior parameter covariance matrix σ ε is the epistemic observation noise covariance matrix and j is the jacobian matrix of partial first derivatives sensitivities of observations with respect to parameters in a linear pe framework the simulated value for a given model derived constraint s can be calculated as 4 s y t θ where y is the vector of constraint sensitivity to each parameter and θ is the vector of parameter values it follows then that the prior and posterior uncertainty estimate e g variance for the simulated value associated with a model derived constraint s σ s 2 and σ s 2 can be calculated by projecting the requisite parameter covariance matrix to the constraint output space using a constraint sensitivity vector 5 σ s 2 y t σ θ y and 6 σ s 2 y t σ θ y where σ θ and σ θ are the prior and posterior parameter covariance matrices respectively σ s 2 and σ s 2 are the prior and posterior variances of constraint s readers are referred to menke 1989 golub and van loan 1996 tarantola 2005 fienen et al 2010 doherty 2015 welter et al 2015 and white et al 2016 for a full treatment of the concepts surrounding fosm based uncertainty estimation in environmental modeling note if no observations are available to condition parameters then σ s 2 of equation 5 is equal to σ s 2 of equation 6 in summary the amount to which any given model derived constraint is uncertain depends on several factors including the model used for evaluating simulated values associated with model derived constraints number and types of parameter used in the pe and uq processes number and types of observations used in the pe and uq process the prior parameter covariance matrix σ θ in equation 3 the observation noise covariance matrix σ ε in equation 3 the relation between parameters and observations encapsulated in the jacobian matrix j in equation 3 the relation between parameters and simulated values associated with model derived constraints the vector y in equations 5 and 6 when model derived constraint uncertainty is estimated using equations 3 5 and 6 a user specified risk value and the probit function bliss 1934 are used to shift the simulated value towards either a risk tolerant or risk averse value using the gaussian distribution implied by equations 5 and 6 the shifted values are then used in the slp solution process the technique of shifting simulated values associated with model derived constraints with respect to uncertainty and risk is referred to as chance constraint programming charnes and cooper 1959 miller and wagner 1965 tung 1986 wagner and gorelick 1987 hantush and marino 1989 chan 1994 calculating the statistical distribution associated with the simulated value for model derived constraints is accomplished by using fosm techniques the value of risk which ranges from 0 0 to 1 0 represents a tolerance in a probabilistic sense in concluding that an optimal solution is truly optimal with respect to uncertain model derived constraints because the true value of the model derived constraint is not known e g is uncertain the value of risk represents a probability that the simulated values yielded by the model that are used to evaluate model derived constraints in the optimization solution process are truly satisfied a risk value of 0 5 corresponds to a risk neutral optimal solution the probability density of the statistical distribution associated with a given model derived constraint is equally distributed around the simulated value there is a 50 probability that model derived constraints are truly satisfied the risk neutral solution is the same as a deterministic slp solution where the simulated value associated with the model derived constraint yielded by the model is used a risk value greater than 0 5 is considered risk averse the simulated values associated with model derived constraints are shifted towards the violation region in constraint space conversely risk values less than 0 5 are considered risk tolerant the simulated values associated with model derived constraints are shifted towards the satisfactory region of constraint space consider a hypothetical groundwater management problem with a single pumping well and single less than type water level constraint a model derived constraint the objective of the management optimization analysis is to minimize the amount of pumping e g minimize costs while meeting the required constraint of the water level being less than 19 0 meters that is 19 0 meters is the highest acceptable water level values less than 19 0 meters are acceptable but not optimal because less pumping could be used the water level constraint is evaluated using a numerical groundwater flow model so the simulated value associated with the model derived model output water level is uncertain this can be attributed in part to imperfect knowledge of subsurface properties model parameters fig 1 displays several hypothetical cases using a statistical distribution implied by fosm techniques around the simulated model output value the width of the gaussian distribution around the simulated mean value on fig 1 is based on σ of equation 6 in this illustration we discuss the interpretation of optimality for representative risk tolerant 0 05 risk neutral 0 5 and risk averse 0 95 levels of risk note other values of risk in the range 0 0 1 0 are also acceptable we select 0 05 and 0 95 for demonstration purposes in the context of environmental resource management one would pursue a risk averse solution when there are legal financial penalties associated with constraint violation or when the results of constraint violations are irreversible e g subsidence stream depletion alternatively a risk tolerant solution can be pursued when any undesirable results of constraint violations are easily reversed e g seasonal drawdowns fig 1a shows the hypothetical case of risk equals 0 95 risk averse the algorithm concludes with an optimal solution when the 95 probability value of the fosm implied posterior distribution of the simulated value is equal to the specified constraint value of 19 meters in this case additional pumping is needed to produce a model output water level of 13 meters so that the upper 95 probability value is less than or equal to 19 meters using a risk value of 0 95 indicates that there is 95 probability or chance that the true value of the constraint is satisfied in the optimal solution in this way a risk value of 0 95 leads to a conservative optimal solution to the management problem where it is more important to meet the constraint than to save money this idea is similar to a margin of safety or design factor in engineering applications e g shigley and mischke 1996 where the design criterion incorporates a certain probability that the design will be inadequate either due to uncertainty in the design process or uncertainty in the stress applied to the constructed object fig 1b shows the hypothetical case of risk equals 0 5 risk neutral in this case the simulated value model output water level is used directly in the optimization solution process a risk value of 0 5 yields a 50 probability or chance that the constraint is truly satisfied this is the same solution that would be obtained if chance constraints were not used at all fianlly fig 1 c shows the hypothetical case of risk equals 0 05 risk tolerant in this case much less pumping is required to satisfy the constraint the model output water level is 25 meters which requires substantially less pumping compared to the risk averse case however there is only a 5 probability or chance that the constraint is truly satisfied this case represents a situation where one would err on the side of permissibility such that the importance of keeping pumping costs low is greater than actually maintaining the water level below the specified constraint value of 19 meters 3 limitations as with any modeling analysis tool pestpp opt is limited by the validity of the implicit and explicit assumptions used in its application these include but are not limited to an approximately linear relation between decision variables and model derived constraints is a valid representation of the relation an approximately linear relation between parameters and model derived constraints is valid is a valid representation of the relation the second moment of the posterior parameter distribution is appropriately described by a covariance matrix σ θ of equation 3 the second moment of the prior and posterior constraint distributions is appropriately described by a variance σ s 2 and σ s 2 of equations 5 and 6 the environmental model used to simulate the relation between parameter observations decision variables and constraints is an appropriate simulator practitioners should carefully evaluate the validity and implications of these assumptions for a given application of pestpp opt 4 implementation pestpp opt implements management optimization with fosm based chance constraints in a model independent non intrusive framework based on the model interface protocols from pest doherty 2010 within the pestpp opt tool the open source optimization library clp forrest et al 2016 part of the computational infrastructure for operations research coin or lougee heimer 2003 project is used to solve linear programs using the simplex algorithm dantzig et al 1955 the clp solution process is combined with an iterative framework to repeatedly call the simplex algorithm while updating the values of decision variables the iterative process referred to as sequential linear programming ahlfeld and mulligan 2000 or successive linear programming baker and lasdon 1985 slp allows pestpp opt to handle nonlinearities in the relation between decision variables and model based constraints the process is as follows 1 calculate response coefficients of model derived constraints with respect to decision variables to fill the response matrix a matrix of equation 1 2 optional calculate sensitivities of observations and model derived constraints with respect to parameters if chance constraints are to be used 3 optional using fosm and user specified risk shift model derived constraints 4 formulate and solve the lp problem implied by equation 1 using clp 5 update the decision variables to the optimal solution found in step 2 and evaluate model at new decision variables 6 check convergence if not converged go to step 1 convergence is determined using a single tolerance opt iter tol in appendix a this tolerance is used to monitor changes in both the optimal objective function and decision variables if the objective function and each decision variable changes less than this tolerance the slp process is considered converged however users are encouraged to monitor the results of each slp iteration recall that the model must be run once for each decision variable for each iteration to fill the rows of a that correspond to model derived constraints however users can nominate decision variables that do not have any effect on model derived constraints as external decision variables external decision variables do not require a model run for response coefficients if fosm based chance constraints are being used the model must also be run once for each adjustable parameter to fill the j matrix for calculation of σ θ and y vectors in equation 6 however users can control how often the fosm based chance constraints are updated or supply an existing jacobian matrix from a previous parameter estimation analysis to reduce or eliminate the additional computation burden needed to implement chance constraints in many cases the changes to fosm constraints due to candidate solutions of the decision variables are likely to be minor the j of equation 3 and y of equations 5 and 6 are not affected by changing decision variables therefore these quantities can be reused for each slp iteration reusing the j can result in great computational savings but implies that the decision variables have no effect on the relation between parameters constraints and observations if used for conditioning the validity of this assumption is specific to the optimization problem that is being solved to that end the capability to update j and y during each slp iteration is included in case changes in decision variables affect these quantities users should construct a pest model independent dataset that includes decision variables such as rates for nominated pumping wells decision variables are input as parameters in the pest datasets upper and lower bounds for decision variables are used as constraints on decision variable values decision variables are differentiated from model parameters using parameter group designations parameters such as hydraulic conductivity storage and boundary condition elements the prior uncertainty for these parameters should be described using the parameter bounds or with a user specified covariance matrix parameters are differentiated from decision variables by using parameter group designations model derived constraints such as water levels or drawdowns stream flows or streamflow depletion model derived constraints are defined as observations in the pest datasets with zero weight and in a separate group from pe observations used for history matching conditioning in the pest datasets observations used for history matching conditioning with weights commensurate with how well the model can match these observations these also must be in groups designated separately from the optimization constraints if no observations are included in the analysis constraint uncertainty is estimated only on the basis of prior parameter uncertainty equation 5 linear summation constraints such as a minimum and or maximum total pumping rate created by summing in an algebraic sense decision variables linear summation constraints are input as prior information equations in the pest datasets linear objective function as a prior information equation or as a space delimited ascii file with decision variable name and objective function coefficient pestpp opt supports a wide range of input options to provide flexibility to users but all options have default values appendix a summarizes the optional pestpp opt arguments in the pest control file several components of the existing pest code base were leveraged to enhance the capabilities of the pestpp opt tool including support for a wide range of options related to calculating response coefficients including capabilities to calculate central 3 point coefficients as well as control of the size of the decision variable perturbation distributed high throughput computing resources via the tcp ip yamr run manager welter et al 2015 to parallelize the calculation of response coefficients a in equation 1 and sensitivities for fosm based constraint uncertainty calculation j of equation 3 and y of equations 5 and 6 5 an example of linear programming with chance constraints here we present an example application of pestpp opt based on the supply2 problem distributed with the gwm 2005 groundwater management software source code ahlfeld et al 2005 the numerical model is a groundwater flow model implemented in modflow the original supply groundwater management problem is described by ahlfeld et al 2005 p 84 85 this sample problem represents a transient water supply problem in which total ground water withdrawals over a 3 year period are limited by the amount of streamflow depletion allowed in two streams that are in hydraulic connection with the aquifer the 3 year period is divided into 12 seasons winter spring summer and fall of each year each of which is represented by a single stress period the aquifer is confined and the area of interest is 6000 ft long by 5000 ft wide the model consists of a single layer with 25 rows and 30 columns each model cell is 200 ft by 200 ft the aquifer is homogeneous and isotropic with a transmissivity of 5000 ft2 d and a storage coefficient of 0 05 the modeled area is bounded on the east and west by no flow conditions and on the north and south by constant heads that decrease in elevation from west to east the aquifer is recharged at a rate of 0 0005 ft d in the winter 0 002 ft d in the spring 0 ft d in the summer and 0 001 ft d in the fall both streams are 20 ft wide and have a streambed conductance of 20 000 ft2 d the mainstem has a slope of 0 0025 whereas the tributary stream has a slope of 0 0010 the management objective is to maximize the value of ground water withdrawn from four wells over the 3 year period while simultaneously limiting streamflow depletions at four streamflow constraint locations along the two streams fig 2 presents the relevant features of the supply2 optimization problem note we are using the supply2 version of this synthetic groundwater management problem the supply2 problem only considers three pumping wells q1 q2 and q4 fig 2 ahlfeld et al 2007 2009 the formulation of the objective function for this version of the supply2 problem is presented below the optimization problem for supply2 has the following decision variables q1 is the pumping rate at well q1 during the entire simulation q2a is the pumping rate at well q2 during winter stress periods 1 5 9 q2b is the pumping rate at well q2 during spring stress periods 2 6 10 q2c is the pumping rate at well q2 during summer stress periods 3 7 11 q2d is the pumping rate at well q2 during fall stress periods 4 8 12 q4a is the pumping rate at well q4 during spring stress periods 2 6 10 q4b is the pumping rate at well q4 during fall stress 4 8 12 im9 is an external decision variable representing an imported source of water during stress period 9 im10 is an external decision variable representing an imported source of water during stress period 10 im11 is an external decision variable representing an imported source of water during stress period 11 im12 is an external decision variable representing an imported source of water during stress period 12 subject to the following constraints pumping rates for each well must be greater than or equal to 0 0 f t 3 d a y pumping rates for each well must be less than or equal to 50 000 f t 3 d a y total water supply rate for each season in the first year stress periods 1 4 must be greater than or equal to 30 000 f t 3 d a y total water supply rate for each season in the second year stress periods 5 8 must be greater than or equal to 25 000 f t 3 d a y total water supply rate for each season in the third year stress periods 9 12 must be greater than or equal to 45 000 f t 3 d a y total water supply rate for any stress period must be less than or equal to 80 000 f t 3 d a y streamflow depletion at c1 must be less than or equal to 15 000 f t 3 d a y for any stress period streamflow depletion at c3 must be less than or equal to 15 000 f t 3 d a y for any stress period streamflow depletion at c2 must be less than or equal to 20 000 f t 3 d a y for summer and fall seasons stress periods 3 4 7 8 11 12 streamflow depletion at c4 must be less than or equal to 30 000 f t 3 d a y for summer and fall seasons stress periods 3 4 7 8 11 12 where streamflow depletion is defined as the decrease in simulated streamflow resulting from simulated pumping streamflow depletion constraints are model derived constraints and are therefore treated as uncertain the supply2 example problem specifies the value of produced water versus imported water there is a net benefit from pumping at each well of 1 per 1000 f t 3 of water withdrawn that is 0 001 f t 3 ahlfeld et al 2005 the 0 0012 f t 3 rate from the external water source variables indicates a cost for importing the water that outweighs the benefit from pumping groundwater by summing the total number of simulation days that each decision variable is active and multiplying by the specified rates the following objective function can be formulated 7 1 095 q 1 0 276 q 2 a 0 273 q 2 b 0 273 q 2 c 0 273 q 2 d 0 273 q 4 a 0 273 q 4 b 0 1104 i m 9 0 1092 i m 10 0 1092 i m 11 0 1092 i m 12 the optimal risk neutral objective function value for the supply2 optimization problem found by pestpp opt is 53 561 which is comparable to the optimal value found by gwm 2005 53 028 the difference in the optimal solution from gwm and pestpp opt is attributable to nonlinearity in the optimization problem furthermore optimal solution to this example problem is sensitive to the choice of decision variable perturbation further evidence of nonlinearity by using different decision variable perturbation values the presence and relative importance of nonlinearity can be detected by changes in the resulting optimal objective function value and simulated constraint values to implement fosm based chance constraints the transmissivity in each model cell and the conductance of each sfr reach were assumed to be uncertain and were therefore treated as parameters a total of 753 transmissivity and conductance parameters were added to the pest control file the uncertainty in each transmissivity and conductance parameter is specified using the parameter upper and lower bounds transmissivity and conductance upper and lower bounds were specified to be 4000 f t 2 d a y and 6000 f t 2 d a y and 4 5 f t 2 d a y and 5 5 f t 2 d a y respectively to apply chance constaints to this example problem we assumed that no observations were available to condition the uncertain parameters therefore the prior parameter uncertainty described by σ θ along with equation 5 is used to estimate streamflow depletion constraint uncertainty in an effort to elucidate how the pestpp opt chance constraints affect the optimal solution we performed several pestpp opt scenarios for a range of risk values hantush and marino 1989 completed a similar analysis this analysis was completed in less than 5 min on a laptop computer fig 3 shows how the choice of risk affects the optimal objective function value as risk increases the benefit of pumping groundwater decreases until the problem becomes infeasible no solution that simultaneously satisfies all constraints can be found for risk values greater than 0 65 for example a risk value of 0 01 risk tolerant stance increases the optimal objective function value by nearly 10 000 compared to the risk neutral stance that is by permitting a 1 chance that the streamflow depletion constraints are truly satisfied one can increase the amount of money made in the water supply problem however a risk value of 0 65 risk averse stance decreases optimal objective function by approximately 10 000 which is the cost of going from a 50 chance of constraint satisfaction to 65 chance of constraint satisfaction in total model derived constraint uncertainty arising from transmissivity and sfr conductance uncertainty has a value of nearly 20 000 the optimal decision variable values for the most risk tolerant risk neutral and most risk averse stances are shown in fig 4 6 conclusion we have developed pestpp opt a model independent tool for efficient optimization under uncertainty with minimal user intervention pestpp opt implements the proven simplex algorithm in an iterative fashion to solve the sequential linear programming problem using fosm based uncertainty estimation and a user specified risk tolerance pestpp opt implements on the fly constraint uncertainty estimation facilitating optimal solutions to resource management questions that include uncertainty given the encapsulated nature of the pestpp opt source code future work will focus on adding support for mixed integer programming non linear optimization solvers as well as capabilities for stack based optimization under uncertainty e g bayer et al 2008 for non linear optimization under uncertainty problems acknowledgments the authors thank brian wagner and two anonymous reviewers for the helpful suggestions any use of trade firm or product names is for descriptive purposes only and does not imply endorsement by the u s government appendix a supplementary data the following is the supplementary data related to this article data profile data profile appendix a supplementary data supplementary data related to this article can be found at https doi org 10 1016 j envsoft 2017 11 019 appendix a pestpp opt optional arguments argument name argument type description default behavior opt dec var groups group names comma separated string identifies which parameter groups to treat as decision variables all parameters are treated as decision variables opt constraint groups group names comma separated string identifies which observation groups to treat as model based constraints observation groups listed must start with the le or less for less than constraints and ge or greater for greater than constraints all observation groups meeting the naming rules will be treated as constraints opt obj func obj func name string prior information equation name or two column ascii file that contains the objective function coefficients use objective function coefficient of 1 0 for every decision variable opt risk risk float in range 0 0 1 0 risk value for use in chance constrained optimization disable chance constraints opt recalc fosm every reuse iter integer number of iterations of slp to reuse the fosm based chance constraint calculations recalculate fosm chance constraints every iteration of slp opt iter tol tolerance float used to monitor changes in objective function and decision variable values during slp iterations assigned a value of 0 001 
