index,text
26375,the development of rainfall runoff models involves extensive computation and the availability of different coexisting platforms including numerical flow models and gis for their physiographical characterization in this paper we present a data driven approach which avoids the use of gis but is based on a combination of principal component analysis pca and an adaptive neuro fuzzy inference system anfis to produce a simple and effective output flow prediction based on previous rainfall runoff data in the catchment the emphasis of the paper is on how to set up an efficient data structure that produces a good output flow estimation the pca approach is compared to the thiessen polygons method requiring gis and we demonstrate that the former can produce a better anfis model with less algorithmic complexity and improved accuracy the combined pca anfis procedure is applied to two minor river basins in tuscany italy to demonstrate its effectiveness graphical abstract image 1 keywords rainfall runoff modelling principal component analysis neuro fuzzy networks data driven modelling anfis software availability name of the software pca anfis rainfall runoff developer stefano marsili libelli programming language matlab 2016 needs the matlab fuzzy toolbox availability bundle of matlab functions provided free of charge as is by the corresponding author stefano marsililibelli unifi it upon qualified email request 1 introduction rainfall runoff models have reached an advanced stage of maturity and the diversity of their mathematical background is staggering as shown by chow et al 1988 bobba et al 2000 voinov et al 2004 mcintyre and wheater 2004 bowden et al 2005 brunner 2010 di baldassarre and claps 2010 wu et al 2014 li et al 2015 bennett et al 2016 douinot et al 2017 to name but a few all these studies concentrate predominantly on the flow routing relationship than on the quality and possible redundancy of the available data on the other hand it should be considered that the deployment of rain gauges in small river catchments is not always optimal with the result that the recorded data are often redundant and or little informative further often it is difficult to relate the output flow to any specific rain gauge in the catchment the method described in this paper combines two popular data driven algorithms principal component analysis pca to eliminate the redundancy of the rainfall data and a neuro fuzzy network anfis to model the relation between the input rainfall and the output flow in a compact and flexible way 1 1 a data driven approach to flow routing the rainfall data come from a variety of rain gauges deployed over the catchment so the need arises to combine all this wealth of information into a meaningful yet concise form the thiessen polygons tp are normally used to obtain a synthetic equivalent precipitation by aggregating the data coming from various sources this technique to be explained in details later involves a considerable computational burden and the availability of a geographical information system gis an alternate method is proposed here based on principal component analysis pca to produce a significant reduction in data complexity while controlling the associated loss of information before applying these data to the neuro fuzzy model and estimate the output discharge we propose to model the relationship between rainfall and output flow by soft computing tools to trade off the complexity of the system with a parsimonious mathematical representation the adaptive neuro fuzzy inference system anfis jang 1993 jang et al 1996 1998 was considered here because it represents an effective way of combining the fuzzy modelling flexibility with the learning capability of neural networks further we describe the combination between a synthetic representation of the rainfall data and the anfis model the comparison between the two algorithms is shown in fig 1 where the original precipitation data r t are first processed either by tp or pca then the equivalent set of rainfall data r ˆ t is used by anfis to estimate output discharge q ˆ t in the pca case the anfis output returns the transformed estimated discharge in the pca reference space from which the actual discharge is obtained through the inverse pca transform to demonstrate the method the algorithm is applied to two minor catchments in tuscany central italy for which abundant rainfall data are available from a large number of rain gauges it is shown that the combined pca anfis method produces an efficient flow routing model with a simple and flexible computational structure which compares favourably to the thiessen polygons approach 2 materials and methods 2 1 the adaptive neuro fuzzy inference system anfis the core of the rainfall runoff algorithm is represented by the adaptive neuro fuzzy inference system anfis jang 1993 jang et al 1996 1998 which in the vast paradigm of artificial neural networks wu et al 2014 has the merit of combining the learning features of a neural network with the approximating capability of a fuzzy inference system kosko 1992 for this reason it has been successfully applied in the environmental engineering area aronica et al 1998 genovesi et al 2000 tay and zhang 2000 steyer et al 2001 marcé et al 2004 azamathulla et al 2009 kumar et al 2009 tekta 2010 alves et al 2011 ge et al 2013 wu et al 2014 yeganeh et al 2017 and particularly in flood forecasting and water management chang and chang 2006 chen et al 2006 in wastewater treatment processes tay and zhang 1999 2000 yoo et al 2008 drinking water processes wu et al 2014 and in the context of soft sensors development dürrenmatt and gujer 2011 haimi et al 2013 the anfis version used throughout this paper is the one available in the matlab platform the mathworks natick usa fuzzy toolbox it is a composite neuro fuzzy structure that implements a sugeno type takagi and sugeno 1985 inferential logic system based upon fuzzy rules in the form 1 r i i f r 1 i s a 1 i a n d r n i s a n i t h e n f i a i t r ˆ b i where r ˆ r 1 r n is the vector of inputs to be specified later and a i b i are the coefficients of the linear sugeno consequents the added value of anfis with respect to the classical sugeno models is that this algorithm can adapt both the antecedent membership functions mfs and the linear consequents using a hybrid learning algorithm the anfis structure used for the development of the rainfall runoff model is shown in fig 2 it consists of a bank of antecedent membership functions which imply in the fuzzy sense an equivalent number of linear sugeno consequents according to a set of generic rules in the form of eq 1 two feedback paths provide the adjusting mechanism to adapt the neuro fuzzy network to the data in order to minimize the model error defined as the sum of squares differences between the observed flow q t and its anfis estimation q ˆ t namely 2 δ q t q ˆ t q t 2 min the inputs r ˆ are fuzzified using the membership functions in the first layer leftmost blocks in fig 2 and the combined degree of truth w i is evaluated as the product of the individual degrees of truth of the antecedents μ i r ˆ 3 w i i μ i r ˆ then each overall degree of truth w i is normalized with respect to the sum of degrees of truth 4 ξ i w i i w i and each consequent is then evaluated as 5 f i a i t r ˆ b i so that the global model response q ˆ t is given by the sum of each individual rule output f i weighted by its normalized degree of truth ξ i 6 q ˆ t i ξ i f i the optimization algorithm adjusting the parameters of each rule in order to minimize the model error of eq 2 can be based on either a back propagation method a least squares search or a combination of both if the network has p inputs and q is the number of membership functions for each of them anfis generates a complete set of 7 n r u l e s q p rules hence the need to keep the number of inputs as low as possible to avoid an explosive increase in network complexity also known as the curse of dimensionality 2 2 data structuring for anfis processing the anfis structure available in the matlab fuzzy toolbox requires that the data are organized as a matrix in which each row contains the input samples at any given time t while the system output is located in the last place to be used for comparison either in training or validation since anfis was not conceived to model dynamical systems it does not possess any internal feedback capability the time dependence must therefore be introduced by a proper structuring of the input data matrix which must include delayed copies of the input variables as additional columns let us suppose that there are n rain gauges in the catchment r 1 t r n t plus the output flow q t so that the data are naturally arranged as follows 8 r 1 t r n t r 1 t δ r n t δ r 1 t 2 δ r n t 2 δ r 1 t k δ r n t k δ rainfall data from n rain gauges and q t q t δ q t 2 δ q t m δ o u t p u t f l o w where δ is the sampling interval while the number of lags for the rainfall k and for the output flow m depends on the specific features of the catchment in the corresponding anfis input matrix the data of expression 8 should be reshaped as follows 9 r ˆ r 1 1 r 1 2 r 1 k f i r s t r a i n g a u g e r n 1 r n 2 r n k n t h r a i n g a u g e q 2 q m p a s t output f l o w q 1 c u r r e n t f l o w r 1 2 r 1 3 r 1 k 1 r n 2 r n 3 r n k 1 q 3 q m 1 q 2 where the data from each rain gauge generate an indexed array of k columns of shifted samples followed by m columns of shifted output flow data for a total length of 10 s n k m the last column contains the output flow samples to be used in the model error eq 2 for calibration or validation in comverting the time indexed measurements of expressions 8 into the matrix form of eq 9 the indexes 1 m k replace the physically δ timed samples t t δ t k δ for the rainfall and t t δ t m δ for the output flow usually with k m further the anfis data structuring of eq 9 required by the lack of inherent network memory clearly suffers from the already mentioned curse of dimensionality in the sense that as the number of model lags either k or m increases the dimension of the model in terms of the number of fuzzy rules increases exponentially according to eq 7 with detrimental consequences on its numerical performance hence the need to use a parsimonious representation by keeping the model complexity as low as possible the r ˆ matrix is supplied to anfis as a single block though it is used one row at a time by the algorithm the anfis input structuring required to model dynamical systems is fully described in marsili libelli 2016 at any one time t the i th rule of eq 1 then takes the form 11 r i t i f r 1 1 i s f 1 i a n d a n d r n k i s f n i past rainfall data a n d q 1 i s h 1 i a n d a n d q m i s h m i past output flow data t h e n q ˆ 1 i a 1 i q 2 a m i q m b 1 i r 1 1 b n i r n k c i sugeno linear consequent where f 1 f n and h 1 h m are the membership functions for the rainfall and the discharge respectively the antecedent implications use the first s 1 elements of the r ˆ matrix row corresponding to the current time t with s computed by eq 10 the specific composition of the data matrix r ˆ defined by eq 9 depends on the method used to represent the equivalent rainfall data either using the thiessen polygons or the pca 2 2 1 equivalent rainfall computed via the thiessen polygons given a catchment where n rain gauges are deployed together with a hydrometer placed at the output the classical approach to obtain an equivalent precipitation data set is based on the thiessen polygons which are voronoi cells de berg et al 2008 aurenhammer et al 2013 used to define an equivalent precipitation polygon of surface area a i to weigh the rainfall of each rain gauge r i t then the equivalent area weighted precipitation is obtained as 12 r t h t i 1 n r i t a i i 1 n a i the computation of eq 12 is normally carried out by the gis software used to define the spatial characteristics of the catchment the generic anfis input matrix of eq 9 is then composed of past rainfall samples obtained from eq 12 plus the last m 1 output flow samples with the current flow q t in last column for training and validation 13 r ˆ t h r t h 1 r t h 2 r t h k t h i e s s e n e q u i v a l e n t p r e c i p i t a t i o n q 2 q m p a s t f l o w s a m p l e s q 1 c u r r e n t f l o w r t h 2 r t h 3 r t h k 1 q 3 q m 1 q 2 when the equivalent rainfall is computed by thiessen polygons the anfis model takes the form of fig 3 2 2 2 equivalent rainfall computed by principal component analysis an alternate method for constructing the equivalent precipitation is to aggregate the rainfall data by principal component analysis pca dunteman 1989 li et al 2000 jolliffe 2002 ringnér 2008 abdi and williams 2010 qin 2012 this transform eliminates the information redundancy originating from the cross correlation among the differing rain gauges further through pca the data dimension can be reduced an important factor in view of anfis tendency to an explosive dimensional increase in performing the pca transform the user can decide the amount of information to be preserved in terms of explained percentage of the total data variability by retaining only a fraction of the most significant principal components dunteman 1989 jolliffe 2002 given a set r ℝ w s of w time indexed measurements with s defined in eq 10 its principal components transformation is given by 14 z r p where p ℝ s s is the matrix of the eigenvectors of the correlation matrix of r whose columns are sorted according to the descending magnitude of the corresponding eigenvalues λ 15 p p 11 p 12 p 1 s p 21 p 22 p 2 s p s 1 p s 2 p s s ℝ s s w i t h s k n m λ 1 λ 2 λ s thus the anfis input matrix of eq 9 is transformed according to 16 z r ˆ p a general guideline to reduce the data dimension is to retain as many principal components as required to explain more than a prescribed fraction of the total variability dunteman 1989 thus if only the first a s components are retained under the assumption that they explain more than a prescribed percentage of the total variability then from eq 16 the reduced representation is obtained by partitioning the p matrix and considering only the first a columns 17 r ˆ p c a r ˆ p a w h e r e p p a p s a so that r ˆ p c a represents the reduced order anfis input the combined pca anfis model requires a further computational step though since the pca transformed data processed by the network bear no resemblance to the original ones so in order to retrieve the estimated output discharge the anfis output must undergo the inverse pca transformation whereby the reconstructed output discharge q ˆ t is recovered as the a th last column of the inverse pca transform i e 18 q ˆ t r ˆ p c a p a t a f o r t 1 w the generic input data matrix of eq 9 is now redefined for the pca transformed data assuming that only the first a pcs are retained the anfis input matrix r ˆ p c a is obtained from eq 9 according to eq 17 19 r ˆ p c a z 1 1 z 1 2 z 1 k f i r s t p c z a 1 z a 2 z a k a t h r e t a i n e d p c z 1 2 z 1 3 z 1 k 1 z a 2 z a 3 z a k 1 notice that in this arrangement the output flow does not appear explicitly in any column nonetheless the last column can still be used in the model error eq 2 and for the retrieval of the estimated output flow as in eq 18 further unlike in the thiessen polygons case the number of retained pcs is independent of the number of physical rain gauges and can be varied in order to obtain a prescribed accuracy of the reduced order representation while controlling the netwok complexity the flow diagram of the pca anfis combination is shown in fig 4 2 3 anfis initialization there are three possible methods to provide an initial anfis data structure either by grid partitioning genfis1 subtractive clustering genfis2 or fuzzy c means fcm clustering genfis3 this last method generates a fis by extracting a set of rules that models the data behaviour using the fcm clustering method bezdek 1981 after defining the number and shape of the membership functions for the antecedents and the kind of consequents either constant or linear of the three initialization methods genfis3 produced the most robust fis in terms of stability and generalization in rainfall runoff modelling and was adopted throughout whereas the other two methods produced non converging models and were therefore not used 2 4 data pre filtering the input data should be regularized by pre filtering in order to remove noise and unwanted artefacts prior to the construction of the anfis input matrix r ˆ as described in the previous sect 2 2 the same procedure may also be used to replace missing data with suitably reconstructed approximations both these results can be achieved by the use of approximating cubic splines which smooth the original data by minimizing the following criterion s p which seeks to reconcile data fidelity with regularity reinsch 1967 goodall 1990 de boor 2001 mathworks 2017 20 s p p k 1 n y k y s k a p p r o x i m a t i o n 2 1 p λ t d 2 y s k d t 2 2 d t s m o o t h i n g where y s k is the smoothed approximation of the input variable y k in eq 20 the smoothing parameter p determines the relative importance of the approximation first term with the roughness reduction second term thus reconciling the conflicting objectives of data fidelity with the smoothness of the approximation the apparent conflict between the discrete nature of the first term and the continuous version of the second is resolved by a suitable discrete approximation of the second derivative the matlab function csaps available in the curve fitting toolbox mathworks 2017 implements eq 20 the smoothing parameter p 0 1 shifts the emphasis on either term so that a small value p 0 produces a large amount of smoothing with loose approximation while a large value p 1 yields a close approximation with very little smoothing several factors should be considered in selecting the appropriate value for p as described in marsili libelli 2016 insufficient smoothing may leave too much noise in the data while excessive smoothing may reduce their information content frequency analysis may assist in selecting the appropriate value for p by checking whether the high frequency components mainly attributed to noise have been sufficiently attenuated with respect to the low frequency components which are supposed to represent the relevant information 3 results to demonstrate the proposed method two small catchments of about the same extension in the tuscany region of italy were selected whose details are shown in fig 5 in both cases there are many rain gauges deployed over each catchment and one single hydrometer at the basin outlet public domain certified data were used extracted from the regional environmental data repository www sir toscana it as explained in sect 2 the rainfall and flow data originally in the form of the matrix r ˆ of eq 9 were re organized according either to the thiessen equivalent precipitation r ˆ t h of eq 13 or to the pca reduction r ˆ p c a of eq 19 then an anfis network was trained to estimate of the output flow which was then compared to the observed flow the two case studies are now examined 3 1 application to the ombrone pistoiese catchment the ombrone pistoiese river is a right bank tributary to the arno river and flows through a steep hilly terrain it collects water from a large number of minor streams covering a drainage basin of nearly 490 km2 surface area with a perimeter of about 140 km the hydrographic mesh is shown in fig 5a where the location of sixteen rain gauges in and around the catchment boundaries is indicated together with the output flow hydrometer two major flood events occurring between 5th nov and 25th dec 2012 were considered lasting 480 and 720 h respectively during which the data were collected with a sampling period of 15 min 3 1 1 data smoothing and denoising since the data were originally sampled every 15 min t s 0 25 h the nyquist limit f n for the frequency analysis is see e g oppenheim and schafer 2010 marsili libelli 2016 21 f n f s 2 1 2 t s 1 2 1 0 25 2 h 1 however the frequency plots of fig 6 are limited to the most interesting low frequency part selective conditioning was applied in data smoothing as described in sect 2 3 the rainfall data required considerable smoothing p 0 9 given their inherent roughness fig 6 shows that this value produced a cut off frequency around f c 0 25 h 1 corresponding to a period of about 4 h while for the output flow a lower smoothing parameter was required p 0 4 which produced a cut off frequency f c 0 1 h 1 corresponding to a period of about 10 h 3 1 2 anfis input data structuring the two possible approaches to the input data structuring described in sect 2 2 are now considered for organising the anfis input data 3 1 2 1 equivalent rainfall by thiessen polygons the corresponding thiessen polygons were computed by quantum gis www qgis org shekar and xiong 2008 petrasova et al 2015 as shown in fig 7 and the equivalent rainfall was computed according to eq 12 the smoothed data of fig 6 were resampled at 1 h intervals and after testing several differing parametrizations the best anfis structure proved to be the one including the output flow one hour earlier and the rainfall sampled at 5 and 10 h earlier as antecedents thus the generic k th row of the anfis input data matrix of eq 13 takes the form 22 r ˆ t h k r t h k 5 r t h k 10 q k 1 q k where k denotes the current sample and the last element of each column q k is used for training and validation 3 1 2 2 equivalent rainfall by pca transform in the proposed method illustrated in sect 2 2 2 the pca transform is applied to the rainfall and flow smoothed data and only the first a s significant components are retained the starting matrix is composed of the current samples from each rain gauge and their previous time shifted replicas arranged as in eq 9 in this case there are sixteen rain gauges so the complete anfis input matrix r ˆ consists of 2 16 2 34 columns and the generic k th row has the following structure 23 r ˆ k r 1 k 1 r 1 k r 2 k 1 r 2 k r 16 k 1 r 16 k q k 1 q k however the scree plot shows that the first three principal components explain more than 85 of the total data variability therefore by selecting a 3 the generic row of anfis input data matrix of eq 23 after pca transform reduces to 24 r ˆ p c a k z a 1 k z a 2 k z a 3 k this example shows that if many parallel data streams are recorded this approach can achieve a considerable data reduction from 34 to 3 in this case while controlling the amount of information loss and unlike the thiessen polygons preserves the identification of each physical rain gauge after inverse pca transform 3 1 3 anfis rainfall runoff modelling for the ombrone pistoiese basin the anfis network fed with the data structure of eq 22 or of eq 24 was used to estimate the output flow the first flood event was used to check the performance of the anfis fed with the thiessen polygons rain data the original 480 h long data set was split in two parts the model was trained with the first 310 h while the remaining 170 h were used for validation the second flood event lasting 720 h was instead processed by pca with the first 400 h used for calibration and the remaining 320 h for validation regardless of the type of input the anfis input layer consisted of three double gaussian shaped membership functions gauss2mf for each input the initial anfis structure was generated by fuzzy c means fcm clustering bezdek 1981 using the genfis3 method as explained in sect 2 3 this method proved to be the best by far producing a model with fast convergence only 30 epochs were required during training and a good generalization capability good validation performance fig 8 shows the reconstruction of the output flow with the thiessen polygons input data some minor discrepancies between the observed and reconstructed flow are apparent in the low flow condition at times around 320 350 h and 400 450 h on the other hand the model fed with the pca transformed data shown in fig 9 performs better in terms of output approximation especially in low flow conditions and has the additional merit of a better generalization because it can accurately predict the peak at t 500 h in the validation data which is higher than any peak appearing in the training phase 3 2 application to the cecina catchment the portion of the cecina drainage basin considered in this study is shown in fig 5 b located in southern tuscany it has a surface area of about 633 km2 and a perimeter of over 150 km there are eight rain gauges in the basin and their data show a considerable correlation given the relatively short distance among some of them therefore a pca data reduction technique was directly considered to lower their dimensionality and eliminate the inherent correlation the rainfall and flow data used for this catchment are the daily averages recorded during the years 2013 and 2014 given the comparatively flatter morphology of this catchment with respect to the previous example resulting in a smaller river bed slope its response is considerably slower further since daily averages data were available it was decided to maintain the daily sampling interval for anfis the original data set comprising 720 days was split into a training subset of 420 days and a validation subset of 300 days 3 2 1 equivalent rainfall by pca transform if the anfis data matrix were to include the rainfall from the eight rain gauges at time t and at the previous sampling time t δ with δ 1 d plus the current output flow q t the k th row of the input data matrix would include 2 8 2 18 elements 25 r ˆ k r 1 k 1 r 1 k 2 r 8 k 1 r 8 k 2 q k 1 q k of which the first seventeen would be used as anfis inputs in this case the network complexity would be overwhelming because assuming that three membership functions were used for each input then according to eq 7 the complete network would generate the staggering number of 3 17 129 140 163 rules instead the scree plot of fig 10 shows that the first five components suffice in recovering almost the entire data variability thus the k th row of the reduced anfis input data matrix takes the following form 26 r ˆ p c a k z a 1 k z a 2 k z a 3 k z a 4 k z a 5 k which corresponds to a network with five inputs generating 3 5 243 rules if 3 mfs are defined for each input the response of the combined pca anfis algorithm is shown in fig 11 with excellent data agreement both in training and validation showing that the model has a good generalization capability 4 discussion the data driven approach to rainfall runoff modelling just described consists of two parts the data structuring and their use to train an adaptive neuro fuzzy network now let us assess each step a widely used technique to obtain an equivalent precipitation namely the thiessen polygons tp approach is compared to a principal components analysis pca transformation of the rainfall data in the former method which requires a gis platform the spatial characterization of the rainfall is entirely lost with the result that possible time delays among differing rain gauges are not taken into account in other words it is assumed that the same precipitation occurs over the entire basin this may be a crude approximation when a spotty rain event starts and ends at different times in different areas of the catchment further there is no way of modulating the approximation because the weighted sum yields one single result without any flexibility on the contrary the pca approach provides some degree of freedom by choosing the number of components to be retained according to the scree plot or f statistics as explained later still preserving the separate space and time characterization of each rain gauge in this way the degree of approximation can be controlled allowing a trade off between accuracy and network complexity as exemplified by the cecina basin example where fig 12 shows that varying the number of retained principal components has a dramatic effect on the model accuracy the only minor price to pay is the inverse pca transform required to retrieve the output discharge from the transformed anfis output data further in the pca case the network complexity is a function of the required accuracy and not of the number of rain gauges in fact in the pca approach the network dimension depends on the data reduction irrespective of the number of rain gauges while the prediction accuracy is computed via eq 2 its dependence on the number of retained pca components is shown in fig 12 which confirms that there is no optimal data reduction as the accuracy improves with the number of retained components therefore a trade off can be determined by the user by weighing prediction accuracy vs network complexity another aspect of preliminary data organization is their conditioning in terms of noise removal and regularization smoothing with cubic approximating splines was used to reduce noise and artefacts with a comparative frequency analysis to assist in determining the appropriate extent of denoising marsili libelli 2016 while smoothing the rainfall data care should be placed in avoiding negative values during the dry periods corresponding to zero values and in general these data require more smoothing than the flow data which are by nature more regular an f statistics test haefner 2005 marsili libelli 2016 can assist in selecting the appropriate number of retained principal components pcs better than the scree plot of fig 13 with reference to the cecina basin of sect 3 2 this test was carried out by varying the number of retained components fig 13 shows how the model accuracy improves as more pcs are retained until a threshold is reached beyond which no further improvement is achieved with only three components a 3 the test fails in both the training and validation cases while with a 4 the trained model passes the test but fails in the validation phase finally with a 5 the model passes the test in both instances in this example the limit values of the f statistics were 3 0166 for the training data and 3 0267 for the validation data this criterion together with the cumulative error distribution of fig 12 can be used to select the number of retained pcs as the smallest number five in this example which enables the model to pass the f test both in training and validation this result is confirmed by fig 12 showing that the distribution of error drastically shrinks when the number of retained components a is increased from four to five while the further improvement for a 5 is marginal coming to anfis the core of the algorithm its use for dynamical systems modelling requires some preliminary data structuring as discussed in marsili libelli 2016 given the lack of anfis internal feedback introducing the time factor requires the set up of a data input matrix in the generic form of eq 9 which turns into eq 13 in the thiessen polygons approach or into eq 19 in the pca case in any case the input matrix consists of as many delayed replicas of the original rainfall data as required to get a good model agreement however while in the tp case the input structuring is rigidly determined by the polygons with pca the trade off between accuracy and complexity can be controlled by the number of retained components whose selection can be assisted by the scree plot or by the f test as shown in figs 12 and 13 a crucial anfis feature is its initialization as discussed in sect 2 3 of the three available methods only the one based on fcm clustering genfis3 provided good results in this application especially in the network generalization capability while the initial gridding method genfis1 resulted in an under performing model as to the number and shape of the input membership functions double gaussian functions gauss2mf provided the best results in the cecina example the minimum number of two membership functions for each input was enough to produce a valid in the f test sense model while increasing their number did not result in any appreciable improvement once trained and validated the resulting pca anfis combination can be used as a stand alone fuzzy model incorporating the rules just obtained the complete flowchart of this software is shown in fig 14 5 conclusion in data driven rainfall runoff models the emphasis is mainly focussed on the flow routing aspect rather than on the handling of the input data in this paper both aspects have been put into context by challenging the use of gis in sorting the often over abundant wealth of rainfall data the approach pursued here is based on a two step procedure to select a significant input data set via pca reduction and to use those data to train an adaptive neuro fuzzy inference system anfis the examples discussed in this paper show that this approach compares favourably to the conventional equivalent rainfall method based on the thiessen polygons and that the equivalent precipitation obtained by pca reduction is more effective in training the anfis network and in controlling its accuracy vs complexity tradeoff in fact its complexity is considerably lower than in the thiessen case for which an ad hoc structure must be defined on a case by case basis conversely the pca transform not only makes the use of gis unnecessary but also drastically reduces the data dimension eliminates their cross correlation and results in a much simpler network with a controllable trade off between accuracy and complexity further the resulting network has better generalization capabilities and conserves the time stamp of each rain gauge thus the pca anfis combination represents an efficient data driven approach to flow routing modelling of course the minor price to pay is the inverse pca transform to reconstruct the estimated output flow from the anfis output appendix a supplementary data the following is the supplementary data related to this article data profile data profile appendix a supplementary data supplementary data related to this article can be found at https doi org 10 1016 j envsoft 2017 11 026 
26375,the development of rainfall runoff models involves extensive computation and the availability of different coexisting platforms including numerical flow models and gis for their physiographical characterization in this paper we present a data driven approach which avoids the use of gis but is based on a combination of principal component analysis pca and an adaptive neuro fuzzy inference system anfis to produce a simple and effective output flow prediction based on previous rainfall runoff data in the catchment the emphasis of the paper is on how to set up an efficient data structure that produces a good output flow estimation the pca approach is compared to the thiessen polygons method requiring gis and we demonstrate that the former can produce a better anfis model with less algorithmic complexity and improved accuracy the combined pca anfis procedure is applied to two minor river basins in tuscany italy to demonstrate its effectiveness graphical abstract image 1 keywords rainfall runoff modelling principal component analysis neuro fuzzy networks data driven modelling anfis software availability name of the software pca anfis rainfall runoff developer stefano marsili libelli programming language matlab 2016 needs the matlab fuzzy toolbox availability bundle of matlab functions provided free of charge as is by the corresponding author stefano marsililibelli unifi it upon qualified email request 1 introduction rainfall runoff models have reached an advanced stage of maturity and the diversity of their mathematical background is staggering as shown by chow et al 1988 bobba et al 2000 voinov et al 2004 mcintyre and wheater 2004 bowden et al 2005 brunner 2010 di baldassarre and claps 2010 wu et al 2014 li et al 2015 bennett et al 2016 douinot et al 2017 to name but a few all these studies concentrate predominantly on the flow routing relationship than on the quality and possible redundancy of the available data on the other hand it should be considered that the deployment of rain gauges in small river catchments is not always optimal with the result that the recorded data are often redundant and or little informative further often it is difficult to relate the output flow to any specific rain gauge in the catchment the method described in this paper combines two popular data driven algorithms principal component analysis pca to eliminate the redundancy of the rainfall data and a neuro fuzzy network anfis to model the relation between the input rainfall and the output flow in a compact and flexible way 1 1 a data driven approach to flow routing the rainfall data come from a variety of rain gauges deployed over the catchment so the need arises to combine all this wealth of information into a meaningful yet concise form the thiessen polygons tp are normally used to obtain a synthetic equivalent precipitation by aggregating the data coming from various sources this technique to be explained in details later involves a considerable computational burden and the availability of a geographical information system gis an alternate method is proposed here based on principal component analysis pca to produce a significant reduction in data complexity while controlling the associated loss of information before applying these data to the neuro fuzzy model and estimate the output discharge we propose to model the relationship between rainfall and output flow by soft computing tools to trade off the complexity of the system with a parsimonious mathematical representation the adaptive neuro fuzzy inference system anfis jang 1993 jang et al 1996 1998 was considered here because it represents an effective way of combining the fuzzy modelling flexibility with the learning capability of neural networks further we describe the combination between a synthetic representation of the rainfall data and the anfis model the comparison between the two algorithms is shown in fig 1 where the original precipitation data r t are first processed either by tp or pca then the equivalent set of rainfall data r ˆ t is used by anfis to estimate output discharge q ˆ t in the pca case the anfis output returns the transformed estimated discharge in the pca reference space from which the actual discharge is obtained through the inverse pca transform to demonstrate the method the algorithm is applied to two minor catchments in tuscany central italy for which abundant rainfall data are available from a large number of rain gauges it is shown that the combined pca anfis method produces an efficient flow routing model with a simple and flexible computational structure which compares favourably to the thiessen polygons approach 2 materials and methods 2 1 the adaptive neuro fuzzy inference system anfis the core of the rainfall runoff algorithm is represented by the adaptive neuro fuzzy inference system anfis jang 1993 jang et al 1996 1998 which in the vast paradigm of artificial neural networks wu et al 2014 has the merit of combining the learning features of a neural network with the approximating capability of a fuzzy inference system kosko 1992 for this reason it has been successfully applied in the environmental engineering area aronica et al 1998 genovesi et al 2000 tay and zhang 2000 steyer et al 2001 marcé et al 2004 azamathulla et al 2009 kumar et al 2009 tekta 2010 alves et al 2011 ge et al 2013 wu et al 2014 yeganeh et al 2017 and particularly in flood forecasting and water management chang and chang 2006 chen et al 2006 in wastewater treatment processes tay and zhang 1999 2000 yoo et al 2008 drinking water processes wu et al 2014 and in the context of soft sensors development dürrenmatt and gujer 2011 haimi et al 2013 the anfis version used throughout this paper is the one available in the matlab platform the mathworks natick usa fuzzy toolbox it is a composite neuro fuzzy structure that implements a sugeno type takagi and sugeno 1985 inferential logic system based upon fuzzy rules in the form 1 r i i f r 1 i s a 1 i a n d r n i s a n i t h e n f i a i t r ˆ b i where r ˆ r 1 r n is the vector of inputs to be specified later and a i b i are the coefficients of the linear sugeno consequents the added value of anfis with respect to the classical sugeno models is that this algorithm can adapt both the antecedent membership functions mfs and the linear consequents using a hybrid learning algorithm the anfis structure used for the development of the rainfall runoff model is shown in fig 2 it consists of a bank of antecedent membership functions which imply in the fuzzy sense an equivalent number of linear sugeno consequents according to a set of generic rules in the form of eq 1 two feedback paths provide the adjusting mechanism to adapt the neuro fuzzy network to the data in order to minimize the model error defined as the sum of squares differences between the observed flow q t and its anfis estimation q ˆ t namely 2 δ q t q ˆ t q t 2 min the inputs r ˆ are fuzzified using the membership functions in the first layer leftmost blocks in fig 2 and the combined degree of truth w i is evaluated as the product of the individual degrees of truth of the antecedents μ i r ˆ 3 w i i μ i r ˆ then each overall degree of truth w i is normalized with respect to the sum of degrees of truth 4 ξ i w i i w i and each consequent is then evaluated as 5 f i a i t r ˆ b i so that the global model response q ˆ t is given by the sum of each individual rule output f i weighted by its normalized degree of truth ξ i 6 q ˆ t i ξ i f i the optimization algorithm adjusting the parameters of each rule in order to minimize the model error of eq 2 can be based on either a back propagation method a least squares search or a combination of both if the network has p inputs and q is the number of membership functions for each of them anfis generates a complete set of 7 n r u l e s q p rules hence the need to keep the number of inputs as low as possible to avoid an explosive increase in network complexity also known as the curse of dimensionality 2 2 data structuring for anfis processing the anfis structure available in the matlab fuzzy toolbox requires that the data are organized as a matrix in which each row contains the input samples at any given time t while the system output is located in the last place to be used for comparison either in training or validation since anfis was not conceived to model dynamical systems it does not possess any internal feedback capability the time dependence must therefore be introduced by a proper structuring of the input data matrix which must include delayed copies of the input variables as additional columns let us suppose that there are n rain gauges in the catchment r 1 t r n t plus the output flow q t so that the data are naturally arranged as follows 8 r 1 t r n t r 1 t δ r n t δ r 1 t 2 δ r n t 2 δ r 1 t k δ r n t k δ rainfall data from n rain gauges and q t q t δ q t 2 δ q t m δ o u t p u t f l o w where δ is the sampling interval while the number of lags for the rainfall k and for the output flow m depends on the specific features of the catchment in the corresponding anfis input matrix the data of expression 8 should be reshaped as follows 9 r ˆ r 1 1 r 1 2 r 1 k f i r s t r a i n g a u g e r n 1 r n 2 r n k n t h r a i n g a u g e q 2 q m p a s t output f l o w q 1 c u r r e n t f l o w r 1 2 r 1 3 r 1 k 1 r n 2 r n 3 r n k 1 q 3 q m 1 q 2 where the data from each rain gauge generate an indexed array of k columns of shifted samples followed by m columns of shifted output flow data for a total length of 10 s n k m the last column contains the output flow samples to be used in the model error eq 2 for calibration or validation in comverting the time indexed measurements of expressions 8 into the matrix form of eq 9 the indexes 1 m k replace the physically δ timed samples t t δ t k δ for the rainfall and t t δ t m δ for the output flow usually with k m further the anfis data structuring of eq 9 required by the lack of inherent network memory clearly suffers from the already mentioned curse of dimensionality in the sense that as the number of model lags either k or m increases the dimension of the model in terms of the number of fuzzy rules increases exponentially according to eq 7 with detrimental consequences on its numerical performance hence the need to use a parsimonious representation by keeping the model complexity as low as possible the r ˆ matrix is supplied to anfis as a single block though it is used one row at a time by the algorithm the anfis input structuring required to model dynamical systems is fully described in marsili libelli 2016 at any one time t the i th rule of eq 1 then takes the form 11 r i t i f r 1 1 i s f 1 i a n d a n d r n k i s f n i past rainfall data a n d q 1 i s h 1 i a n d a n d q m i s h m i past output flow data t h e n q ˆ 1 i a 1 i q 2 a m i q m b 1 i r 1 1 b n i r n k c i sugeno linear consequent where f 1 f n and h 1 h m are the membership functions for the rainfall and the discharge respectively the antecedent implications use the first s 1 elements of the r ˆ matrix row corresponding to the current time t with s computed by eq 10 the specific composition of the data matrix r ˆ defined by eq 9 depends on the method used to represent the equivalent rainfall data either using the thiessen polygons or the pca 2 2 1 equivalent rainfall computed via the thiessen polygons given a catchment where n rain gauges are deployed together with a hydrometer placed at the output the classical approach to obtain an equivalent precipitation data set is based on the thiessen polygons which are voronoi cells de berg et al 2008 aurenhammer et al 2013 used to define an equivalent precipitation polygon of surface area a i to weigh the rainfall of each rain gauge r i t then the equivalent area weighted precipitation is obtained as 12 r t h t i 1 n r i t a i i 1 n a i the computation of eq 12 is normally carried out by the gis software used to define the spatial characteristics of the catchment the generic anfis input matrix of eq 9 is then composed of past rainfall samples obtained from eq 12 plus the last m 1 output flow samples with the current flow q t in last column for training and validation 13 r ˆ t h r t h 1 r t h 2 r t h k t h i e s s e n e q u i v a l e n t p r e c i p i t a t i o n q 2 q m p a s t f l o w s a m p l e s q 1 c u r r e n t f l o w r t h 2 r t h 3 r t h k 1 q 3 q m 1 q 2 when the equivalent rainfall is computed by thiessen polygons the anfis model takes the form of fig 3 2 2 2 equivalent rainfall computed by principal component analysis an alternate method for constructing the equivalent precipitation is to aggregate the rainfall data by principal component analysis pca dunteman 1989 li et al 2000 jolliffe 2002 ringnér 2008 abdi and williams 2010 qin 2012 this transform eliminates the information redundancy originating from the cross correlation among the differing rain gauges further through pca the data dimension can be reduced an important factor in view of anfis tendency to an explosive dimensional increase in performing the pca transform the user can decide the amount of information to be preserved in terms of explained percentage of the total data variability by retaining only a fraction of the most significant principal components dunteman 1989 jolliffe 2002 given a set r ℝ w s of w time indexed measurements with s defined in eq 10 its principal components transformation is given by 14 z r p where p ℝ s s is the matrix of the eigenvectors of the correlation matrix of r whose columns are sorted according to the descending magnitude of the corresponding eigenvalues λ 15 p p 11 p 12 p 1 s p 21 p 22 p 2 s p s 1 p s 2 p s s ℝ s s w i t h s k n m λ 1 λ 2 λ s thus the anfis input matrix of eq 9 is transformed according to 16 z r ˆ p a general guideline to reduce the data dimension is to retain as many principal components as required to explain more than a prescribed fraction of the total variability dunteman 1989 thus if only the first a s components are retained under the assumption that they explain more than a prescribed percentage of the total variability then from eq 16 the reduced representation is obtained by partitioning the p matrix and considering only the first a columns 17 r ˆ p c a r ˆ p a w h e r e p p a p s a so that r ˆ p c a represents the reduced order anfis input the combined pca anfis model requires a further computational step though since the pca transformed data processed by the network bear no resemblance to the original ones so in order to retrieve the estimated output discharge the anfis output must undergo the inverse pca transformation whereby the reconstructed output discharge q ˆ t is recovered as the a th last column of the inverse pca transform i e 18 q ˆ t r ˆ p c a p a t a f o r t 1 w the generic input data matrix of eq 9 is now redefined for the pca transformed data assuming that only the first a pcs are retained the anfis input matrix r ˆ p c a is obtained from eq 9 according to eq 17 19 r ˆ p c a z 1 1 z 1 2 z 1 k f i r s t p c z a 1 z a 2 z a k a t h r e t a i n e d p c z 1 2 z 1 3 z 1 k 1 z a 2 z a 3 z a k 1 notice that in this arrangement the output flow does not appear explicitly in any column nonetheless the last column can still be used in the model error eq 2 and for the retrieval of the estimated output flow as in eq 18 further unlike in the thiessen polygons case the number of retained pcs is independent of the number of physical rain gauges and can be varied in order to obtain a prescribed accuracy of the reduced order representation while controlling the netwok complexity the flow diagram of the pca anfis combination is shown in fig 4 2 3 anfis initialization there are three possible methods to provide an initial anfis data structure either by grid partitioning genfis1 subtractive clustering genfis2 or fuzzy c means fcm clustering genfis3 this last method generates a fis by extracting a set of rules that models the data behaviour using the fcm clustering method bezdek 1981 after defining the number and shape of the membership functions for the antecedents and the kind of consequents either constant or linear of the three initialization methods genfis3 produced the most robust fis in terms of stability and generalization in rainfall runoff modelling and was adopted throughout whereas the other two methods produced non converging models and were therefore not used 2 4 data pre filtering the input data should be regularized by pre filtering in order to remove noise and unwanted artefacts prior to the construction of the anfis input matrix r ˆ as described in the previous sect 2 2 the same procedure may also be used to replace missing data with suitably reconstructed approximations both these results can be achieved by the use of approximating cubic splines which smooth the original data by minimizing the following criterion s p which seeks to reconcile data fidelity with regularity reinsch 1967 goodall 1990 de boor 2001 mathworks 2017 20 s p p k 1 n y k y s k a p p r o x i m a t i o n 2 1 p λ t d 2 y s k d t 2 2 d t s m o o t h i n g where y s k is the smoothed approximation of the input variable y k in eq 20 the smoothing parameter p determines the relative importance of the approximation first term with the roughness reduction second term thus reconciling the conflicting objectives of data fidelity with the smoothness of the approximation the apparent conflict between the discrete nature of the first term and the continuous version of the second is resolved by a suitable discrete approximation of the second derivative the matlab function csaps available in the curve fitting toolbox mathworks 2017 implements eq 20 the smoothing parameter p 0 1 shifts the emphasis on either term so that a small value p 0 produces a large amount of smoothing with loose approximation while a large value p 1 yields a close approximation with very little smoothing several factors should be considered in selecting the appropriate value for p as described in marsili libelli 2016 insufficient smoothing may leave too much noise in the data while excessive smoothing may reduce their information content frequency analysis may assist in selecting the appropriate value for p by checking whether the high frequency components mainly attributed to noise have been sufficiently attenuated with respect to the low frequency components which are supposed to represent the relevant information 3 results to demonstrate the proposed method two small catchments of about the same extension in the tuscany region of italy were selected whose details are shown in fig 5 in both cases there are many rain gauges deployed over each catchment and one single hydrometer at the basin outlet public domain certified data were used extracted from the regional environmental data repository www sir toscana it as explained in sect 2 the rainfall and flow data originally in the form of the matrix r ˆ of eq 9 were re organized according either to the thiessen equivalent precipitation r ˆ t h of eq 13 or to the pca reduction r ˆ p c a of eq 19 then an anfis network was trained to estimate of the output flow which was then compared to the observed flow the two case studies are now examined 3 1 application to the ombrone pistoiese catchment the ombrone pistoiese river is a right bank tributary to the arno river and flows through a steep hilly terrain it collects water from a large number of minor streams covering a drainage basin of nearly 490 km2 surface area with a perimeter of about 140 km the hydrographic mesh is shown in fig 5a where the location of sixteen rain gauges in and around the catchment boundaries is indicated together with the output flow hydrometer two major flood events occurring between 5th nov and 25th dec 2012 were considered lasting 480 and 720 h respectively during which the data were collected with a sampling period of 15 min 3 1 1 data smoothing and denoising since the data were originally sampled every 15 min t s 0 25 h the nyquist limit f n for the frequency analysis is see e g oppenheim and schafer 2010 marsili libelli 2016 21 f n f s 2 1 2 t s 1 2 1 0 25 2 h 1 however the frequency plots of fig 6 are limited to the most interesting low frequency part selective conditioning was applied in data smoothing as described in sect 2 3 the rainfall data required considerable smoothing p 0 9 given their inherent roughness fig 6 shows that this value produced a cut off frequency around f c 0 25 h 1 corresponding to a period of about 4 h while for the output flow a lower smoothing parameter was required p 0 4 which produced a cut off frequency f c 0 1 h 1 corresponding to a period of about 10 h 3 1 2 anfis input data structuring the two possible approaches to the input data structuring described in sect 2 2 are now considered for organising the anfis input data 3 1 2 1 equivalent rainfall by thiessen polygons the corresponding thiessen polygons were computed by quantum gis www qgis org shekar and xiong 2008 petrasova et al 2015 as shown in fig 7 and the equivalent rainfall was computed according to eq 12 the smoothed data of fig 6 were resampled at 1 h intervals and after testing several differing parametrizations the best anfis structure proved to be the one including the output flow one hour earlier and the rainfall sampled at 5 and 10 h earlier as antecedents thus the generic k th row of the anfis input data matrix of eq 13 takes the form 22 r ˆ t h k r t h k 5 r t h k 10 q k 1 q k where k denotes the current sample and the last element of each column q k is used for training and validation 3 1 2 2 equivalent rainfall by pca transform in the proposed method illustrated in sect 2 2 2 the pca transform is applied to the rainfall and flow smoothed data and only the first a s significant components are retained the starting matrix is composed of the current samples from each rain gauge and their previous time shifted replicas arranged as in eq 9 in this case there are sixteen rain gauges so the complete anfis input matrix r ˆ consists of 2 16 2 34 columns and the generic k th row has the following structure 23 r ˆ k r 1 k 1 r 1 k r 2 k 1 r 2 k r 16 k 1 r 16 k q k 1 q k however the scree plot shows that the first three principal components explain more than 85 of the total data variability therefore by selecting a 3 the generic row of anfis input data matrix of eq 23 after pca transform reduces to 24 r ˆ p c a k z a 1 k z a 2 k z a 3 k this example shows that if many parallel data streams are recorded this approach can achieve a considerable data reduction from 34 to 3 in this case while controlling the amount of information loss and unlike the thiessen polygons preserves the identification of each physical rain gauge after inverse pca transform 3 1 3 anfis rainfall runoff modelling for the ombrone pistoiese basin the anfis network fed with the data structure of eq 22 or of eq 24 was used to estimate the output flow the first flood event was used to check the performance of the anfis fed with the thiessen polygons rain data the original 480 h long data set was split in two parts the model was trained with the first 310 h while the remaining 170 h were used for validation the second flood event lasting 720 h was instead processed by pca with the first 400 h used for calibration and the remaining 320 h for validation regardless of the type of input the anfis input layer consisted of three double gaussian shaped membership functions gauss2mf for each input the initial anfis structure was generated by fuzzy c means fcm clustering bezdek 1981 using the genfis3 method as explained in sect 2 3 this method proved to be the best by far producing a model with fast convergence only 30 epochs were required during training and a good generalization capability good validation performance fig 8 shows the reconstruction of the output flow with the thiessen polygons input data some minor discrepancies between the observed and reconstructed flow are apparent in the low flow condition at times around 320 350 h and 400 450 h on the other hand the model fed with the pca transformed data shown in fig 9 performs better in terms of output approximation especially in low flow conditions and has the additional merit of a better generalization because it can accurately predict the peak at t 500 h in the validation data which is higher than any peak appearing in the training phase 3 2 application to the cecina catchment the portion of the cecina drainage basin considered in this study is shown in fig 5 b located in southern tuscany it has a surface area of about 633 km2 and a perimeter of over 150 km there are eight rain gauges in the basin and their data show a considerable correlation given the relatively short distance among some of them therefore a pca data reduction technique was directly considered to lower their dimensionality and eliminate the inherent correlation the rainfall and flow data used for this catchment are the daily averages recorded during the years 2013 and 2014 given the comparatively flatter morphology of this catchment with respect to the previous example resulting in a smaller river bed slope its response is considerably slower further since daily averages data were available it was decided to maintain the daily sampling interval for anfis the original data set comprising 720 days was split into a training subset of 420 days and a validation subset of 300 days 3 2 1 equivalent rainfall by pca transform if the anfis data matrix were to include the rainfall from the eight rain gauges at time t and at the previous sampling time t δ with δ 1 d plus the current output flow q t the k th row of the input data matrix would include 2 8 2 18 elements 25 r ˆ k r 1 k 1 r 1 k 2 r 8 k 1 r 8 k 2 q k 1 q k of which the first seventeen would be used as anfis inputs in this case the network complexity would be overwhelming because assuming that three membership functions were used for each input then according to eq 7 the complete network would generate the staggering number of 3 17 129 140 163 rules instead the scree plot of fig 10 shows that the first five components suffice in recovering almost the entire data variability thus the k th row of the reduced anfis input data matrix takes the following form 26 r ˆ p c a k z a 1 k z a 2 k z a 3 k z a 4 k z a 5 k which corresponds to a network with five inputs generating 3 5 243 rules if 3 mfs are defined for each input the response of the combined pca anfis algorithm is shown in fig 11 with excellent data agreement both in training and validation showing that the model has a good generalization capability 4 discussion the data driven approach to rainfall runoff modelling just described consists of two parts the data structuring and their use to train an adaptive neuro fuzzy network now let us assess each step a widely used technique to obtain an equivalent precipitation namely the thiessen polygons tp approach is compared to a principal components analysis pca transformation of the rainfall data in the former method which requires a gis platform the spatial characterization of the rainfall is entirely lost with the result that possible time delays among differing rain gauges are not taken into account in other words it is assumed that the same precipitation occurs over the entire basin this may be a crude approximation when a spotty rain event starts and ends at different times in different areas of the catchment further there is no way of modulating the approximation because the weighted sum yields one single result without any flexibility on the contrary the pca approach provides some degree of freedom by choosing the number of components to be retained according to the scree plot or f statistics as explained later still preserving the separate space and time characterization of each rain gauge in this way the degree of approximation can be controlled allowing a trade off between accuracy and network complexity as exemplified by the cecina basin example where fig 12 shows that varying the number of retained principal components has a dramatic effect on the model accuracy the only minor price to pay is the inverse pca transform required to retrieve the output discharge from the transformed anfis output data further in the pca case the network complexity is a function of the required accuracy and not of the number of rain gauges in fact in the pca approach the network dimension depends on the data reduction irrespective of the number of rain gauges while the prediction accuracy is computed via eq 2 its dependence on the number of retained pca components is shown in fig 12 which confirms that there is no optimal data reduction as the accuracy improves with the number of retained components therefore a trade off can be determined by the user by weighing prediction accuracy vs network complexity another aspect of preliminary data organization is their conditioning in terms of noise removal and regularization smoothing with cubic approximating splines was used to reduce noise and artefacts with a comparative frequency analysis to assist in determining the appropriate extent of denoising marsili libelli 2016 while smoothing the rainfall data care should be placed in avoiding negative values during the dry periods corresponding to zero values and in general these data require more smoothing than the flow data which are by nature more regular an f statistics test haefner 2005 marsili libelli 2016 can assist in selecting the appropriate number of retained principal components pcs better than the scree plot of fig 13 with reference to the cecina basin of sect 3 2 this test was carried out by varying the number of retained components fig 13 shows how the model accuracy improves as more pcs are retained until a threshold is reached beyond which no further improvement is achieved with only three components a 3 the test fails in both the training and validation cases while with a 4 the trained model passes the test but fails in the validation phase finally with a 5 the model passes the test in both instances in this example the limit values of the f statistics were 3 0166 for the training data and 3 0267 for the validation data this criterion together with the cumulative error distribution of fig 12 can be used to select the number of retained pcs as the smallest number five in this example which enables the model to pass the f test both in training and validation this result is confirmed by fig 12 showing that the distribution of error drastically shrinks when the number of retained components a is increased from four to five while the further improvement for a 5 is marginal coming to anfis the core of the algorithm its use for dynamical systems modelling requires some preliminary data structuring as discussed in marsili libelli 2016 given the lack of anfis internal feedback introducing the time factor requires the set up of a data input matrix in the generic form of eq 9 which turns into eq 13 in the thiessen polygons approach or into eq 19 in the pca case in any case the input matrix consists of as many delayed replicas of the original rainfall data as required to get a good model agreement however while in the tp case the input structuring is rigidly determined by the polygons with pca the trade off between accuracy and complexity can be controlled by the number of retained components whose selection can be assisted by the scree plot or by the f test as shown in figs 12 and 13 a crucial anfis feature is its initialization as discussed in sect 2 3 of the three available methods only the one based on fcm clustering genfis3 provided good results in this application especially in the network generalization capability while the initial gridding method genfis1 resulted in an under performing model as to the number and shape of the input membership functions double gaussian functions gauss2mf provided the best results in the cecina example the minimum number of two membership functions for each input was enough to produce a valid in the f test sense model while increasing their number did not result in any appreciable improvement once trained and validated the resulting pca anfis combination can be used as a stand alone fuzzy model incorporating the rules just obtained the complete flowchart of this software is shown in fig 14 5 conclusion in data driven rainfall runoff models the emphasis is mainly focussed on the flow routing aspect rather than on the handling of the input data in this paper both aspects have been put into context by challenging the use of gis in sorting the often over abundant wealth of rainfall data the approach pursued here is based on a two step procedure to select a significant input data set via pca reduction and to use those data to train an adaptive neuro fuzzy inference system anfis the examples discussed in this paper show that this approach compares favourably to the conventional equivalent rainfall method based on the thiessen polygons and that the equivalent precipitation obtained by pca reduction is more effective in training the anfis network and in controlling its accuracy vs complexity tradeoff in fact its complexity is considerably lower than in the thiessen case for which an ad hoc structure must be defined on a case by case basis conversely the pca transform not only makes the use of gis unnecessary but also drastically reduces the data dimension eliminates their cross correlation and results in a much simpler network with a controllable trade off between accuracy and complexity further the resulting network has better generalization capabilities and conserves the time stamp of each rain gauge thus the pca anfis combination represents an efficient data driven approach to flow routing modelling of course the minor price to pay is the inverse pca transform to reconstruct the estimated output flow from the anfis output appendix a supplementary data the following is the supplementary data related to this article data profile data profile appendix a supplementary data supplementary data related to this article can be found at https doi org 10 1016 j envsoft 2017 11 026 
26376,monsoons have been widely studied in the literature due to their climatic impact related to precipitation and temperature over different regions around the world in this work data mining techniques namely imbalanced classification techniques are proposed in order to check the capability of climate indices to capture and forecast the evolution of the western north pacific summer monsoon thus the main goal is to predict if the monsoon will be an extreme monsoon for a temporal horizon of a month firstly a new monthly index of the monsoon related to its intensity has been generated later the problem of forecasting has been transformed into a binary imbalanced classification problem and a set of representative techniques such as models based on trees models based on rules black box models and ensemble techniques are applied to obtain the forecasts from the results obtained it can be concluded that the methodology proposed here reports promising results according to the quality measures evaluated and predicts extreme monsoons for a temporal horizon of a month with a high accuracy keywords climatic time series monsoon forecasting imbalanced classification 1 introduction the asian summer monsoon is one of the atmospheric phenomena with a highest socio economic impact in the world the length and intensity of the monsoon season determines total precipitation in a wide and very densely populated area extending through the southern and eastern coasts of asia including part of the continental areas and most of the islands in that zone wang et al 2001 weng et al 2011 but even when it is usual to read about the asian summer monsoon there is not one but three summer monsoons in asia probably the best known and the one with most publications about its characteristics predictability and variability is the indian summer monsoon ism which controls the precipitation amount and the duration of the rainy season over southern asia in a broad area centered over the indian subcontinent ordóñez et al 2016 then the east asian summer monsoon easm is a monsoon system affecting the continental eastern asia including southeastern china korea and japan finally the western north pacific summer monsoon wnpsm is often considered as an oceanic monsoon and its area of influence includes part of the south china sea the philippine sea their continental coasts including the indochina peninsula and the islands in the area mainly the philippines murakami and matsumoto 1994 this third monsoon the wnpsm was shown to have a great impact both over the global climate and over the total precipitation of very densely populated areas wang et al 2001 lee et al 2014 hsu et al 2014 precisely due to the oceanic character of this monsoon and the lack of continuous instrumental observations over the oceans it has been difficult to obtain long lasting series representing its evolution and characteristics publications about this monsoon analyze a period of approximately 50 years starting in the mid 20th century those analyses are mostly based on reanalyzed data i e relying on climate assimilation models and thus are sensible to the possible lack of observed data over some of the areas analyzed recently taking advantage of the meteorological data available through the icoads project international comprehensive ocean atmosphere data set http icoads noaa gov and using a method similar to the one proposed in gallego et al 2015 a new completely instrumental long lasting series representing the wnpsm evolution and extending its length from about 50 years to more than 100 years has been developed in particular this new index consists on a monthly time series quantifying the persistence of a particular wind direction in two sectors of the western north pacific wnp monsoon area and the value of the index determines the intensity of the monsoon the new index is perfectly comparable with the previous ones during the second half of the 20th century and was named western north pacific directional index wnpdi vega et al 2017 in recent years different authors have proposed that the variability of the wnpsm can be associated to many different well known and usually better predictable climatic indices such as el niño el niño modoki or the indian ocean dipole weng et al 2011 feng and chen 2014 lu and lu 2015 it has been widely accepted the idea proposed in wang et al 2001 where it was shown that the phase and intensity of el niño southern oscillation enso events represented by the niño3 4 index affected the intensity of the wnpsm in this way a weak wnpsm would be expected to occur the summer after an intense el niño event and a strong wnpsm would develop the year after an intense la niña more recently feng and chen 2014 proposed that el niño modoki a climatic pattern somehow related to el niño but with some important differences was a better predictor for the intensity of the wnpsm than other enso indices in their paper it is suggested that wnpsm tends to be stronger during el niño modoki intense positive events nevertheless feng and chen 2014 suggested that another climatic oscillation the indian ocean dipole iod modulated the impact of el niño modoki over the wnpsm intensity they proposed that the relationship between wnpsm and el niño modoki becomes weaker when iod is in its positive phase additionally zhang et al in zhang et al 2015 pointed out that zonal position of el niño modoki rather than its intensity is related to the iod intensity on the other hand the reverse linear relationship between indian ocean sea surface temperature and precipitation over wnp area is modulated by the enso lu and lu 2015 finally the pacific decadal oscillation pdo is a slow varying climatic oscillation which affects to the characteristics of most of the previous climatic patterns and has been proposed to affect to the characteristics of the precipitation of the wnp area chan and zhou 2005 yoon and yeh 2016 data analytics has turned into an emerging research field due to the increasing amount of data being created and stored in particular data mining techniques try to infer knowledge from data with the purpose of automatically predicting trends and behaviors or describing models that simulate a system in the last few years data mining techniques have been successfully applied to forecast time series in different areas such as energy seismology or environment martínez álvarez et al 2015 in this work we have applied data mining techniques namely imbalanced classification techniques in order to check the capability of traditional climate indices such as el niño el niño modoki the iod or the pdo to capture and forecast the evolution of a monsoonal system as the one represented by the wnpsm the main goal is to predict the occurrence of extreme monsoons for a temporal horizon of a month given that the number of extreme monsoons is much lower than the number of non extreme monsoons the resultant classification problem is highly imbalanced where the class representing the extreme monsoons is a minority class but the class of interest therefore the main contributions of this work can be summarized as the novel formulation of the forecasting problem based on imbalanced classification techniques which has not been to the best of authors knowledge exploited to forecast the occurrence of monsoons so far and on the other hand to deal with the wnpsm since most of the published papers deal with the ism forecasting due to the wnpsm is predominantly an oceanic monsoon and historical data about its evolution are much scarcer the rest of the paper is structured as follows next section makes a revision of the main data mining works published in the literature about the monsoon forecasting section 3 describes the methodology and the imbalanced classification techniques evaluated in this paper section 4 presents the experimental part of the paper where we carry out a comparative evaluation of the results when predicting the intensity of the wnpsm section 5 closes the paper giving some final conclusions 2 related work data mining techniques have been used in the literature to deal with the problem of predicting if a monsoon is extreme nevertheless the intensity of a monsoon is related to the amount of rainfalls and therefore the main approaches have been focused on two kind of problems to forecast the monsoon rainfalls and to find the best attributes to be predictors to carry out the prediction the majority of the current papers published deal with the forecasting for the ism due to the wnpsm is predominantly oceanic and fewer historical data are recorded an autoencoder neural network has been proposed to obtain variables to be predictors to forecast the rainfalls for ism saha et al 2016 namely climatic indices such as air temperature sea surface temperature sst and sea level pressure are the inputs of the autoencoder a graphical analysis is carried out to determine predictors for forecasting the rainfalls for the ism in cannon and mckendry 2002 namely the relationships between the predictions obtained by a multiple linear regression and neural networks and their inputs sea level pressure and geopotential height are illustrated graphically recently clustering techniques have been applied as a previous step to improve the forecasting in saha and mitra 2015 the authors show that the same set of climatic predictors do not have to be good for all the years thus a clustering technique is applied in two dimensions to obtain groups of years and predictors simultaneously unsupervised neural networks chattopadhyay and chattopadhyay 2016 have also been used to obtain different clusters composed of years depending on the variability of the annual precipitations later a support vector machine svm is applied for each cluster to provide the forecasting of annual rainfalls for ism in this work climatic predictors were not used but the input for each svm was the monthly rainfalls for march april and may for the years belong to the corresponding cluster the anomalies of the sst are climatic indices which are related to the rainfall variability it is well known that the tropical pacific sst anomalies are related to enso therefore many works have proposed the sst and el niño indices as predictors which have been successfully applied in the last years as inputs of neural networks in shukla et al 2011 the authors analyzed the correlations between the rainfalls for ism and different indices of el niño for lags from 1 to 8 seasons the five indices with the highest correlations along with the principal component were used as inputs for seven linear regression models and for a multi layer feed forward neural network the neural network was trained with the back propagation algorithm and it provided the best results showing that the relationships between the rainfalls and these climatic indices are mainly non linear the most highly correlated average of the sst anomalies over different areas with precipitations in june were chosen as predictors for a neural network in acharya et al 2012 an ensemble of five neural networks with a different number of neurons in the hidden layer was proposed to forecast the annual rainfalls and the rainfalls for june july august and september separately for the ism in singh and borah 2013 in this case just historical data of the precipitations were used as inputs to the neural networks other unsupervised learning techniques as association rules dhanya and kumar 2009 have been also applied to predict the rainfalls associated to the monsoon of india a previous discretization in five intervals was required to use the well known fp growth algorithm to obtain rules the antecedent of the rules was composed of the predictors the enso and equatorial indian ocean oscillation zonal wind eqwin indices and the historical precipitations and the consequent of the rules was the rainfalls to be predicted on the other hand association rules have been proposed as feature selection approach to select the best predictors in vathsala and koolagudi 2015 2017 authors generate association rules from the closed itemsets and select those rules with the highest confidence measure finally attributes that appear in the antecedent are selected to be predictors in the last years evolutionary techniques have been also proposed to forecast the amount of precipitations derived from monsoons kashid and maity 2015 chaudhuri et al 2014 namely genetic programming has been applied to obtain a global function to predict the rainfalls from the enso and tropical indian ocean equinoo indices with lags from 1 to 3 as independent variables kashid and maity 2015 on the other hand since precipitations present a certain grade of correlation with the pheromones of ants an ant colony optimization approach has been applied to obtain the intervals in which precipitations range for the next day in chaudhuri et al 2014 3 methodology the methodology applied in order to forecast the occurrence of an extreme monsoon for the next month is described in this section first a description of the dataset is provided in section 3 1 later how the problem is formulated the kind of methodology followed to solve it and a brief description of the imbalance classification techniques used in this work are detailed in section 3 2 3 1 data the wnpdi is a climatic index constructed from historical observations of wind direction taken aboard ships it is optimized to be sensitive to the spatial pattern of the wind field during the monsoon season when easterly westerly winds dominate within the domain d1 d2 see fig 1 the index is computed by the addition of the rate of days in one month with predominant winds from the east at d1 plus the rate of days characterized by winds from the west at d2 with the available data in the icoads database it has been possible to extend the wnpdi record from 1900 to 2007 at monthly resolution without missing values vega et al 2017 fig 2 uppermost panel shows the resulting series which is characterized by a clear annual cycle peaking during the most active phase of the monsoon between august and september accordingly extreme values of the wnpdi defined as those values over the 95th percentile of the total record occur mainly in september 36 cases followed by august 23 only 12 extremes have been found for other months october july may and june with 7 3 1 and 1 cases respectively fig 1 shows the corresponding precipitation anomalies related to extreme wnpdi it is clear that large values of the wnpdi are concurrent with significant increases of precipitation in large areas of philippines and indochina in accordance with the expected monsoonal signal as stated in the introduction different climatic indices have been proposed as good predictors of the wnpsm intensity fig 2 includes the monthly evolution of those indices including the evolution of the wnpdi the el niño modoki index emi ashok et al 2007 the niño3 4 index n34 rayner et al 2003 and the southern oscillation index soi allan et al 1991 representing enso the indian ocean dipole index dmi saji et al 1999 and the pacific decadal oscillation index pdo mantua et al 1997 the influence that the climatic processes characterized by these indices exert on the wnpsm depends on the previous history of those same indices chan and zhou 2005 feng and chen 2014 wang et al 2001 yoon and yeh 2016 therefore following these references it has been assumed that the value of the wnpdi in a particular month could depend on the value of these six indices in the previous five months in the previous three seasons and or in the four seasons preceding that particular monsoon month a total of 72 possible predictors then in order to select which of those predictors should be included in the imbalanced classification techniques pearson correlation coefficients between the series of wnpdi for each month from june to december months corresponding to an extended monsoon season and all the lagged indices for each climatic index have been computed a total of 504 correlation coefficients finally 20 predictors were selected from those lags and climate indices with a correlation coefficient greater than a certain threshold for the highest number of months the threshold was calculated so that it identified 20 predictors with the condition that each climate index had to include a minimum of two predictors using this feature selection scheme the complete collection of considered monsoon precursors is shown in table 1 in addition it is important to notice that all the potential climate phenomena and possible lags suggested in the previous bibliography is well represented with this selection in the case of the emi pattern we considered lags of 1 3 and 4 months before an extreme wnpdi column months and 1 season before an extreme wnpdi column seasons additionally we considered the emi value for the summer before an extreme wnpdi column particular season seasons are defined as june august column sum march may column spr december february column win and september november column aut it must be stressed that we included lagged values of the wnpdi eii itself in order to explicitly consider climatological persistence in successive the monsoon precursors will be denoted by the name of the index followed by an indication of the lag according to the notation in table 1 for example emi m1 emi s1 and emi sum stands for the emi value 1 month before 1 season before and for the summer before an extreme monsoon respectively 3 2 imbalanced classification the forecasting problem is formulated as a classification problem where the attributes are composed of 20 predictors described in the previous section in addition to the month and year and the class can be 1 or 0 depending on whether an extreme monsoon has occurred or not in the prediction horizon since the number of extreme monsoons is low an imbalanced classification problem is obtained when the prediction of extreme monsoons is formulated as a binary classification problem therefore the class representing the extreme monsoons is a minority class but the class of interest approaches proposed to solve imbalanced classification problems can be split into two differentiated groups algorithm based approaches that design specific algorithms to deal with the minority class and data based approaches which apply a preprocessing step to try to balance the classes before applying a learning algorithm galar et al 2012 the methodology proposed here is focused on the second group thus to deal with the high grade of imbalance between the two classes the synthetic minority over sampling technique smote chawla et al 2002 has been selected as preprocessing method in this work a comparative analysis was performed to compare smote to other similar techniques such as adasyn he et al 2008 adoms tang and chen 2008 ros batista et al 2004 and spider stefanowski and wilk 2008 providing smote the best results the smote technique interpolates instances of the minority class that are close to generate new artificial instances of the minority class later a selection of representative classification methods will be combined with the smote preprocessing method which will be applied a certain number of times in order to obtain different grades of imbalance and to improve the results of the forecasts classification methods have been selected in order to cover different paradigms namely models based on trees models based on rules black box models and finally ensemble techniques the models based on trees or rules have been mainly proposed due to interpretable results are always desirable for the environmental research community on the other hand the black box models are models that obtain very good results for many real applications and the ensemble techniques usually improve the results obtained by a single classifier the following is a description of all the methods used for each paradigm within the models based on trees the c4 5 algorithm salzberg 1994 has been used to forecast the occurrence of extreme monsoons this algorithm builds a tree by splitting the data based on the values of predictive attributes the attributes that form the nodes of the tree are determined by maximizing a measure as the information gain the label in the leaf nodes is computed by a majority vote of the labels from instances of the training set that reach the node the furia approach hühn and hüllermeier 2009 is a rule induction algorithm that obtains a set of fuzzy rules which have a high grade of coverage thanks to a stretching mechanism the main black box models selected for predicting the monsoons have been the svm chang and lin 2011 k nearest neighbors knn troncoso et al 2007 and neural networks shukla et al 2011 singh and borah 2013 the svm uses a hyperplane obtained through a quadratic minimization problem to separate the positive and negative classes the knn algorithm classifies a point according to the majority vote of the labels from the k instances that are the nearest neighbors to the point to be predicted the number of neighbors selected to classify is an input parameter and it has a great influence on the results the multilayer perceptron mlp neural network has been widely used to predict the rainfalls and it is composed of a certain number of perceptrons organized by layers each perceptron has several inputs and a single output whose value is a non linear function of the inputs each perceptron s input is affected by a weighting factor which must be determined during the training phase finally three ensemble techniques have been considered such as random forests breiman 2001 the consolidated tree construction algorithm ibarguren et al 2015 and the probability threshold selector algorithm witten et al 2011 random forests rf is an ensemble of decision trees trained separately the trees generated are different because different training sets from a bootstrap resampling are used the consolidated tree construction ctc algorithm is especially designed to imbalanced problems and reduces to a single tree a set of trees generated by the c4 5 algorithm finally the probability threshold selector pts approach is a meta classifier which can be used with any classifier and obtains as result the probability of belonging to the positive class in addition to the optimal threshold for the probability thus if the probability is greater than the threshold then the algorithm classifies with the label 1 and 0 otherwise the threshold is set by minimizing a particular performance measure 4 results the above described methodology has been applied to the wnpsm this section is structured as follows first a brief description of the usual quality parameters in an imbalanced context is presented in section 4 1 and later the accuracy of the predictions of extreme monsoons is validated in section 4 2 4 1 evaluation measures note that in subsequent equations true positives or t p is the number of extreme monsoons properly predicted true negatives or t n is the number of monsoons that were not extreme properly predicted false positives or f p is the number of monsoons that were not extreme and were predicted as extreme monsoons false negatives or f n is the number of extreme monsoons which were predicted as non extreme monsoons according to these definitions the sensitivity s n is the ratio of extreme monsoons properly predicted by the classification technique its formula is defined as follows 1 s n t p t p f n another parameter is the specificity s p which is the ratio of monsoons that were not extreme properly predicted the mathematical expression is 2 s p t n t n f p the positive predictive value ppv also known as precision is the probability to predict an extreme monsoon correctly its formula is 3 p p v t p t p f p finally the negative predictive value npv is the probability that a monsoon that was not extreme was properly predicted its formula is 4 n p v t n t n f n the performance of most classifiers is evaluated with the accuracy or error measures defined by the proportion of instances correctly or incorrectly classified for both classes however these measures do not distinguish between the number of correct labels for each class which in the context of imbalanced classification problems is important as the class corresponding to extreme monsoons is the class of interest in this kind of problems for that measures intending to achieve good quality results for both classes are preferred in order to assess the performance of the imbalanced classification techniques for that the following measures have been considered f measure or balanced f score f is the harmonic mean of the ppv and s n measures 5 f 2 p p v s n s n p p v the area under roc curve auc the curve roc shows the relation between sensitivity and specificity that is trade offs between benefits true positives and costs false positives 6 a u c 1 s n f p r a t e 2 where f p r a t e is the false positive rate that is the ratio between the number of false positives and the total number of monsoons that are not extreme geometric mean gm of the sensitivity and specificity measures 7 g m s n s p 4 2 analysis of results in this section the results following the proposed methodology are described and discussed four datasets were used in the experimentation smote x1 smote x2 smote x3 and smote x4 these datasets were achieved applying successively the smote algorithm up to 4 times table 2 shows the distribution of examples in the positive class extreme monsoons and in the negative class non extreme monsoons for each dataset of study the first row shows the original distribution of examples original the aim was to check the performance of the studied classifiers for different ratios of imbalance in the data the results of the different measures of evaluation for all the classifiers for smote x1 smote x2 smote x3 and smote x4 datasets are shown in table 3 table 4 table 5 and table 6 respectively the best performance in terms of the best combination between precision and sensitivity was achieved by the classifiers pts and ctc applying smote four times for the training set dataset smote x4 specifically it can be observed from table 6 that pts and ctc achieved the best values for the f measure metric 0 69 and the best precision 0 35 and 0 34 for ppv respectively analyzing the effect of increasing the number of smote applications it can be noticed an increment of the sensitivity of the classifiers but a decrement of the precision ppv only ensemble based classifiers such as rf ctc and pts have demonstrated to be able to keep or improve the precision while improving notably the sensitivity when the number of smote applied increases nearest neighbors classifiers were applied using the values of 1 3 5 7 9 11 and 13 for the number of neighbors the performance follows the same trend when the number of neighbors increases in particular the sensitivity increases when the number of neighbors k increases specially for the greatest number of smote applications from 0 51 of sensitivity for k 1 to 0 80 for k 13 for the smote x4 dataset no significant ppv variations depending on the number of times that smote was applied were found in the nearest neighbors classifiers all values for ppv were around 0 26 for such reason just 1nn and 7nn were shown in this study the svm approach has shown generally the best sensitivity values reaching up to 0 82 for both smote x3 and smote x4 datasets however its ppv is lower than rf ctc and pts ensemble based methods in general terms the mlp and furia classifiers do not present competitive performance values for all analyzed datasets the decision trees based c4 5 algorithm was the classifier that reduced its precision less than that of the remaining when increasing the number of smote applied the sensitivity of the c4 5 is increased up to 0 49 in smote x4 moreover this classifier generates interpretable tree based models for all these reasons the three tree based ensemble methods rf ctc and pts with the c4 5 as classifier were adopted in this study in order to overcome the c4 5 results the rf ensemble technique was trained by using 100 decision trees and although it improves the ppv of c4 5 it gets worse results in terms of sensitivity the ctc algorithm improved notably the sensitivity of the c4 5 classifier unlike other methods the ctc approach improves its ppv when the number of smote increases in addition to it generates the most compact and easily interpretable knowledge models comparing with that of the other trees based methods proposed here finally the pts approach achieved the best trade off between ppv and sensitivity p p v 0 35 and s n 0 59 for the smote x4 dataset however it obtains complex tree based knowledge models which are difficult to interpret fig 3 shows the model obtained by the pts algorithm using the smote x4 dataset as training the classifier pts showed that in august and september the first criterion applied in the tree was based in the value of the emi index in the previous season or months according to the idea proposed in feng and chen 2014 in fact the agreement goes even further in september when the next criterion depends on the value of the dmi index and based on those two indices the model correctly forecasts near 95 of the total extreme events the analysis of decision trees from other models is remarkably similar results for model c4 5 with smote x2 in september and august basically show the same classification as pts not shown in august and september the first division is based on the value of emi index during the months or season previous to an extreme monsoon and in september the next criterion is based on the value of dmi this model shows that the most important conditions for an extreme monsoon event to occur during september are a value of emi during summer ranging from 0 3931 to 0 4720 and a value of dmi in april lower than 0 1700 in concordance with feng and chen 2014 where it was postulated that a high value of emi would lead to an extreme monsoon event as long as the iod remains out of its positive phase these two criteria in the c4 5 decision tree affecting to the conditions in the upper branches for september can lead to more than 80 of extreme monsoon events correctly predicted finally a model like ctc with smote x1 is characterized by a very simplified tree not shown but even in this case the only criterion to predict an extreme monsoon event during september is that the value of emi three months before june has to be higher than 0 358 it is important to note that even when the phases of emi and dmi in the months previous to an extreme event are apparently the main conditions for its occurrence all the other climatic patterns suggested in the bibliography appear in upper branches of those trees and or in the branches corresponding to other months with a lower occurrence of extreme events for example enso represented by soi or n34 appears as a predictor in all the models and is the most important criterion for an extreme monsoon event to occur in july or october on the other hand the pdo has a much more residual impact and it only shows up as a criterion in a secondary branch of the pts model for september and affecting to less than 5 of the correctly predicted occurrence of an extreme monsoon finally it is interesting to notice that most of these predictors are based on short lags a few months or one season in advance but the evolution of the monsoon index eii and the atmospheric soi index include some important predictors which can go as far back as the previous winter or even to the situation during the previous autumn almost one year before the occurrence of an extreme monsoon these long lags are good indicators of the complex processes involved in the physical mechanisms connecting the western north pacific and the climatic conditions over the rest of the pacific and indian oceans and concur with the classical idea of a lagged connection between monsoon conditions in one summer with the evolution of enso many seasons before wang et al 2001 the notable similitudes in the classification criteria identified from these three models and the agreement of these criteria with existing literature lead us to consider as quite robust the relationship between extreme monsoon events and the climatological patterns involved additionally decision trees show some evident advantages over classical analysis for example they provide a range of quantitative values of each index related to the occurrence of an extreme monsoon opposed to a more classical approximation where it is usual to consider only the phase of a particular climate pattern as the main predictor 5 conclusions a new methodology based on the definition of a new index for the wnpsm and on the transformation of the problem into an imbalanced classification problem has been proposed to forecast the intensity of the wnpsm that is if the wnpsm will be an extreme monsoon for the next month a representative number of classification algorithms specifically designed for imbalanced problems has been tested showing that the svm black box models obtain the best sensitivity that is they present a ratio of extreme monsoons correctly forecasted greater than that of the remaining ones equally the random forest ensemble technique shows the best positive predictive value that is the probability to predict an extreme monsoon correctly is the highest however the models based on trees have reached the best balance between sensitivity and positive predictive value in fact results from those models are very much in agreement with previous literature and show that the el niño modoki is the climatic pattern with the highest influence on the occurrence of an extreme wnpsm but that this pattern is modulated by the situation over the indian ocean characterized by the indian ocean dipole future work is directed towards formulating the problem as a regression problem to predict the monthly wnp index to forecast extremely low values of the wnpdi associated to very dry monsoons and to extend the analysis to seasonal monsoon forecasting acknowledgments the authors would like to thank the spanish ministry of economy and competitiveness support under projects tin2014 55894 c2 r and cgl2013 44530 p grant bes 2014 069733 and the junta de andalucía under project p12 tic 1728 appendix a supplementary data the following is the supplementary data related to this article data monsoon envsoft 2017 163 data monsoon envsoft 2017 163 appendix a supplementary data supplementary data related to this article can be found at https doi org 10 1016 j envsoft 2017 11 024 
26376,monsoons have been widely studied in the literature due to their climatic impact related to precipitation and temperature over different regions around the world in this work data mining techniques namely imbalanced classification techniques are proposed in order to check the capability of climate indices to capture and forecast the evolution of the western north pacific summer monsoon thus the main goal is to predict if the monsoon will be an extreme monsoon for a temporal horizon of a month firstly a new monthly index of the monsoon related to its intensity has been generated later the problem of forecasting has been transformed into a binary imbalanced classification problem and a set of representative techniques such as models based on trees models based on rules black box models and ensemble techniques are applied to obtain the forecasts from the results obtained it can be concluded that the methodology proposed here reports promising results according to the quality measures evaluated and predicts extreme monsoons for a temporal horizon of a month with a high accuracy keywords climatic time series monsoon forecasting imbalanced classification 1 introduction the asian summer monsoon is one of the atmospheric phenomena with a highest socio economic impact in the world the length and intensity of the monsoon season determines total precipitation in a wide and very densely populated area extending through the southern and eastern coasts of asia including part of the continental areas and most of the islands in that zone wang et al 2001 weng et al 2011 but even when it is usual to read about the asian summer monsoon there is not one but three summer monsoons in asia probably the best known and the one with most publications about its characteristics predictability and variability is the indian summer monsoon ism which controls the precipitation amount and the duration of the rainy season over southern asia in a broad area centered over the indian subcontinent ordóñez et al 2016 then the east asian summer monsoon easm is a monsoon system affecting the continental eastern asia including southeastern china korea and japan finally the western north pacific summer monsoon wnpsm is often considered as an oceanic monsoon and its area of influence includes part of the south china sea the philippine sea their continental coasts including the indochina peninsula and the islands in the area mainly the philippines murakami and matsumoto 1994 this third monsoon the wnpsm was shown to have a great impact both over the global climate and over the total precipitation of very densely populated areas wang et al 2001 lee et al 2014 hsu et al 2014 precisely due to the oceanic character of this monsoon and the lack of continuous instrumental observations over the oceans it has been difficult to obtain long lasting series representing its evolution and characteristics publications about this monsoon analyze a period of approximately 50 years starting in the mid 20th century those analyses are mostly based on reanalyzed data i e relying on climate assimilation models and thus are sensible to the possible lack of observed data over some of the areas analyzed recently taking advantage of the meteorological data available through the icoads project international comprehensive ocean atmosphere data set http icoads noaa gov and using a method similar to the one proposed in gallego et al 2015 a new completely instrumental long lasting series representing the wnpsm evolution and extending its length from about 50 years to more than 100 years has been developed in particular this new index consists on a monthly time series quantifying the persistence of a particular wind direction in two sectors of the western north pacific wnp monsoon area and the value of the index determines the intensity of the monsoon the new index is perfectly comparable with the previous ones during the second half of the 20th century and was named western north pacific directional index wnpdi vega et al 2017 in recent years different authors have proposed that the variability of the wnpsm can be associated to many different well known and usually better predictable climatic indices such as el niño el niño modoki or the indian ocean dipole weng et al 2011 feng and chen 2014 lu and lu 2015 it has been widely accepted the idea proposed in wang et al 2001 where it was shown that the phase and intensity of el niño southern oscillation enso events represented by the niño3 4 index affected the intensity of the wnpsm in this way a weak wnpsm would be expected to occur the summer after an intense el niño event and a strong wnpsm would develop the year after an intense la niña more recently feng and chen 2014 proposed that el niño modoki a climatic pattern somehow related to el niño but with some important differences was a better predictor for the intensity of the wnpsm than other enso indices in their paper it is suggested that wnpsm tends to be stronger during el niño modoki intense positive events nevertheless feng and chen 2014 suggested that another climatic oscillation the indian ocean dipole iod modulated the impact of el niño modoki over the wnpsm intensity they proposed that the relationship between wnpsm and el niño modoki becomes weaker when iod is in its positive phase additionally zhang et al in zhang et al 2015 pointed out that zonal position of el niño modoki rather than its intensity is related to the iod intensity on the other hand the reverse linear relationship between indian ocean sea surface temperature and precipitation over wnp area is modulated by the enso lu and lu 2015 finally the pacific decadal oscillation pdo is a slow varying climatic oscillation which affects to the characteristics of most of the previous climatic patterns and has been proposed to affect to the characteristics of the precipitation of the wnp area chan and zhou 2005 yoon and yeh 2016 data analytics has turned into an emerging research field due to the increasing amount of data being created and stored in particular data mining techniques try to infer knowledge from data with the purpose of automatically predicting trends and behaviors or describing models that simulate a system in the last few years data mining techniques have been successfully applied to forecast time series in different areas such as energy seismology or environment martínez álvarez et al 2015 in this work we have applied data mining techniques namely imbalanced classification techniques in order to check the capability of traditional climate indices such as el niño el niño modoki the iod or the pdo to capture and forecast the evolution of a monsoonal system as the one represented by the wnpsm the main goal is to predict the occurrence of extreme monsoons for a temporal horizon of a month given that the number of extreme monsoons is much lower than the number of non extreme monsoons the resultant classification problem is highly imbalanced where the class representing the extreme monsoons is a minority class but the class of interest therefore the main contributions of this work can be summarized as the novel formulation of the forecasting problem based on imbalanced classification techniques which has not been to the best of authors knowledge exploited to forecast the occurrence of monsoons so far and on the other hand to deal with the wnpsm since most of the published papers deal with the ism forecasting due to the wnpsm is predominantly an oceanic monsoon and historical data about its evolution are much scarcer the rest of the paper is structured as follows next section makes a revision of the main data mining works published in the literature about the monsoon forecasting section 3 describes the methodology and the imbalanced classification techniques evaluated in this paper section 4 presents the experimental part of the paper where we carry out a comparative evaluation of the results when predicting the intensity of the wnpsm section 5 closes the paper giving some final conclusions 2 related work data mining techniques have been used in the literature to deal with the problem of predicting if a monsoon is extreme nevertheless the intensity of a monsoon is related to the amount of rainfalls and therefore the main approaches have been focused on two kind of problems to forecast the monsoon rainfalls and to find the best attributes to be predictors to carry out the prediction the majority of the current papers published deal with the forecasting for the ism due to the wnpsm is predominantly oceanic and fewer historical data are recorded an autoencoder neural network has been proposed to obtain variables to be predictors to forecast the rainfalls for ism saha et al 2016 namely climatic indices such as air temperature sea surface temperature sst and sea level pressure are the inputs of the autoencoder a graphical analysis is carried out to determine predictors for forecasting the rainfalls for the ism in cannon and mckendry 2002 namely the relationships between the predictions obtained by a multiple linear regression and neural networks and their inputs sea level pressure and geopotential height are illustrated graphically recently clustering techniques have been applied as a previous step to improve the forecasting in saha and mitra 2015 the authors show that the same set of climatic predictors do not have to be good for all the years thus a clustering technique is applied in two dimensions to obtain groups of years and predictors simultaneously unsupervised neural networks chattopadhyay and chattopadhyay 2016 have also been used to obtain different clusters composed of years depending on the variability of the annual precipitations later a support vector machine svm is applied for each cluster to provide the forecasting of annual rainfalls for ism in this work climatic predictors were not used but the input for each svm was the monthly rainfalls for march april and may for the years belong to the corresponding cluster the anomalies of the sst are climatic indices which are related to the rainfall variability it is well known that the tropical pacific sst anomalies are related to enso therefore many works have proposed the sst and el niño indices as predictors which have been successfully applied in the last years as inputs of neural networks in shukla et al 2011 the authors analyzed the correlations between the rainfalls for ism and different indices of el niño for lags from 1 to 8 seasons the five indices with the highest correlations along with the principal component were used as inputs for seven linear regression models and for a multi layer feed forward neural network the neural network was trained with the back propagation algorithm and it provided the best results showing that the relationships between the rainfalls and these climatic indices are mainly non linear the most highly correlated average of the sst anomalies over different areas with precipitations in june were chosen as predictors for a neural network in acharya et al 2012 an ensemble of five neural networks with a different number of neurons in the hidden layer was proposed to forecast the annual rainfalls and the rainfalls for june july august and september separately for the ism in singh and borah 2013 in this case just historical data of the precipitations were used as inputs to the neural networks other unsupervised learning techniques as association rules dhanya and kumar 2009 have been also applied to predict the rainfalls associated to the monsoon of india a previous discretization in five intervals was required to use the well known fp growth algorithm to obtain rules the antecedent of the rules was composed of the predictors the enso and equatorial indian ocean oscillation zonal wind eqwin indices and the historical precipitations and the consequent of the rules was the rainfalls to be predicted on the other hand association rules have been proposed as feature selection approach to select the best predictors in vathsala and koolagudi 2015 2017 authors generate association rules from the closed itemsets and select those rules with the highest confidence measure finally attributes that appear in the antecedent are selected to be predictors in the last years evolutionary techniques have been also proposed to forecast the amount of precipitations derived from monsoons kashid and maity 2015 chaudhuri et al 2014 namely genetic programming has been applied to obtain a global function to predict the rainfalls from the enso and tropical indian ocean equinoo indices with lags from 1 to 3 as independent variables kashid and maity 2015 on the other hand since precipitations present a certain grade of correlation with the pheromones of ants an ant colony optimization approach has been applied to obtain the intervals in which precipitations range for the next day in chaudhuri et al 2014 3 methodology the methodology applied in order to forecast the occurrence of an extreme monsoon for the next month is described in this section first a description of the dataset is provided in section 3 1 later how the problem is formulated the kind of methodology followed to solve it and a brief description of the imbalance classification techniques used in this work are detailed in section 3 2 3 1 data the wnpdi is a climatic index constructed from historical observations of wind direction taken aboard ships it is optimized to be sensitive to the spatial pattern of the wind field during the monsoon season when easterly westerly winds dominate within the domain d1 d2 see fig 1 the index is computed by the addition of the rate of days in one month with predominant winds from the east at d1 plus the rate of days characterized by winds from the west at d2 with the available data in the icoads database it has been possible to extend the wnpdi record from 1900 to 2007 at monthly resolution without missing values vega et al 2017 fig 2 uppermost panel shows the resulting series which is characterized by a clear annual cycle peaking during the most active phase of the monsoon between august and september accordingly extreme values of the wnpdi defined as those values over the 95th percentile of the total record occur mainly in september 36 cases followed by august 23 only 12 extremes have been found for other months october july may and june with 7 3 1 and 1 cases respectively fig 1 shows the corresponding precipitation anomalies related to extreme wnpdi it is clear that large values of the wnpdi are concurrent with significant increases of precipitation in large areas of philippines and indochina in accordance with the expected monsoonal signal as stated in the introduction different climatic indices have been proposed as good predictors of the wnpsm intensity fig 2 includes the monthly evolution of those indices including the evolution of the wnpdi the el niño modoki index emi ashok et al 2007 the niño3 4 index n34 rayner et al 2003 and the southern oscillation index soi allan et al 1991 representing enso the indian ocean dipole index dmi saji et al 1999 and the pacific decadal oscillation index pdo mantua et al 1997 the influence that the climatic processes characterized by these indices exert on the wnpsm depends on the previous history of those same indices chan and zhou 2005 feng and chen 2014 wang et al 2001 yoon and yeh 2016 therefore following these references it has been assumed that the value of the wnpdi in a particular month could depend on the value of these six indices in the previous five months in the previous three seasons and or in the four seasons preceding that particular monsoon month a total of 72 possible predictors then in order to select which of those predictors should be included in the imbalanced classification techniques pearson correlation coefficients between the series of wnpdi for each month from june to december months corresponding to an extended monsoon season and all the lagged indices for each climatic index have been computed a total of 504 correlation coefficients finally 20 predictors were selected from those lags and climate indices with a correlation coefficient greater than a certain threshold for the highest number of months the threshold was calculated so that it identified 20 predictors with the condition that each climate index had to include a minimum of two predictors using this feature selection scheme the complete collection of considered monsoon precursors is shown in table 1 in addition it is important to notice that all the potential climate phenomena and possible lags suggested in the previous bibliography is well represented with this selection in the case of the emi pattern we considered lags of 1 3 and 4 months before an extreme wnpdi column months and 1 season before an extreme wnpdi column seasons additionally we considered the emi value for the summer before an extreme wnpdi column particular season seasons are defined as june august column sum march may column spr december february column win and september november column aut it must be stressed that we included lagged values of the wnpdi eii itself in order to explicitly consider climatological persistence in successive the monsoon precursors will be denoted by the name of the index followed by an indication of the lag according to the notation in table 1 for example emi m1 emi s1 and emi sum stands for the emi value 1 month before 1 season before and for the summer before an extreme monsoon respectively 3 2 imbalanced classification the forecasting problem is formulated as a classification problem where the attributes are composed of 20 predictors described in the previous section in addition to the month and year and the class can be 1 or 0 depending on whether an extreme monsoon has occurred or not in the prediction horizon since the number of extreme monsoons is low an imbalanced classification problem is obtained when the prediction of extreme monsoons is formulated as a binary classification problem therefore the class representing the extreme monsoons is a minority class but the class of interest approaches proposed to solve imbalanced classification problems can be split into two differentiated groups algorithm based approaches that design specific algorithms to deal with the minority class and data based approaches which apply a preprocessing step to try to balance the classes before applying a learning algorithm galar et al 2012 the methodology proposed here is focused on the second group thus to deal with the high grade of imbalance between the two classes the synthetic minority over sampling technique smote chawla et al 2002 has been selected as preprocessing method in this work a comparative analysis was performed to compare smote to other similar techniques such as adasyn he et al 2008 adoms tang and chen 2008 ros batista et al 2004 and spider stefanowski and wilk 2008 providing smote the best results the smote technique interpolates instances of the minority class that are close to generate new artificial instances of the minority class later a selection of representative classification methods will be combined with the smote preprocessing method which will be applied a certain number of times in order to obtain different grades of imbalance and to improve the results of the forecasts classification methods have been selected in order to cover different paradigms namely models based on trees models based on rules black box models and finally ensemble techniques the models based on trees or rules have been mainly proposed due to interpretable results are always desirable for the environmental research community on the other hand the black box models are models that obtain very good results for many real applications and the ensemble techniques usually improve the results obtained by a single classifier the following is a description of all the methods used for each paradigm within the models based on trees the c4 5 algorithm salzberg 1994 has been used to forecast the occurrence of extreme monsoons this algorithm builds a tree by splitting the data based on the values of predictive attributes the attributes that form the nodes of the tree are determined by maximizing a measure as the information gain the label in the leaf nodes is computed by a majority vote of the labels from instances of the training set that reach the node the furia approach hühn and hüllermeier 2009 is a rule induction algorithm that obtains a set of fuzzy rules which have a high grade of coverage thanks to a stretching mechanism the main black box models selected for predicting the monsoons have been the svm chang and lin 2011 k nearest neighbors knn troncoso et al 2007 and neural networks shukla et al 2011 singh and borah 2013 the svm uses a hyperplane obtained through a quadratic minimization problem to separate the positive and negative classes the knn algorithm classifies a point according to the majority vote of the labels from the k instances that are the nearest neighbors to the point to be predicted the number of neighbors selected to classify is an input parameter and it has a great influence on the results the multilayer perceptron mlp neural network has been widely used to predict the rainfalls and it is composed of a certain number of perceptrons organized by layers each perceptron has several inputs and a single output whose value is a non linear function of the inputs each perceptron s input is affected by a weighting factor which must be determined during the training phase finally three ensemble techniques have been considered such as random forests breiman 2001 the consolidated tree construction algorithm ibarguren et al 2015 and the probability threshold selector algorithm witten et al 2011 random forests rf is an ensemble of decision trees trained separately the trees generated are different because different training sets from a bootstrap resampling are used the consolidated tree construction ctc algorithm is especially designed to imbalanced problems and reduces to a single tree a set of trees generated by the c4 5 algorithm finally the probability threshold selector pts approach is a meta classifier which can be used with any classifier and obtains as result the probability of belonging to the positive class in addition to the optimal threshold for the probability thus if the probability is greater than the threshold then the algorithm classifies with the label 1 and 0 otherwise the threshold is set by minimizing a particular performance measure 4 results the above described methodology has been applied to the wnpsm this section is structured as follows first a brief description of the usual quality parameters in an imbalanced context is presented in section 4 1 and later the accuracy of the predictions of extreme monsoons is validated in section 4 2 4 1 evaluation measures note that in subsequent equations true positives or t p is the number of extreme monsoons properly predicted true negatives or t n is the number of monsoons that were not extreme properly predicted false positives or f p is the number of monsoons that were not extreme and were predicted as extreme monsoons false negatives or f n is the number of extreme monsoons which were predicted as non extreme monsoons according to these definitions the sensitivity s n is the ratio of extreme monsoons properly predicted by the classification technique its formula is defined as follows 1 s n t p t p f n another parameter is the specificity s p which is the ratio of monsoons that were not extreme properly predicted the mathematical expression is 2 s p t n t n f p the positive predictive value ppv also known as precision is the probability to predict an extreme monsoon correctly its formula is 3 p p v t p t p f p finally the negative predictive value npv is the probability that a monsoon that was not extreme was properly predicted its formula is 4 n p v t n t n f n the performance of most classifiers is evaluated with the accuracy or error measures defined by the proportion of instances correctly or incorrectly classified for both classes however these measures do not distinguish between the number of correct labels for each class which in the context of imbalanced classification problems is important as the class corresponding to extreme monsoons is the class of interest in this kind of problems for that measures intending to achieve good quality results for both classes are preferred in order to assess the performance of the imbalanced classification techniques for that the following measures have been considered f measure or balanced f score f is the harmonic mean of the ppv and s n measures 5 f 2 p p v s n s n p p v the area under roc curve auc the curve roc shows the relation between sensitivity and specificity that is trade offs between benefits true positives and costs false positives 6 a u c 1 s n f p r a t e 2 where f p r a t e is the false positive rate that is the ratio between the number of false positives and the total number of monsoons that are not extreme geometric mean gm of the sensitivity and specificity measures 7 g m s n s p 4 2 analysis of results in this section the results following the proposed methodology are described and discussed four datasets were used in the experimentation smote x1 smote x2 smote x3 and smote x4 these datasets were achieved applying successively the smote algorithm up to 4 times table 2 shows the distribution of examples in the positive class extreme monsoons and in the negative class non extreme monsoons for each dataset of study the first row shows the original distribution of examples original the aim was to check the performance of the studied classifiers for different ratios of imbalance in the data the results of the different measures of evaluation for all the classifiers for smote x1 smote x2 smote x3 and smote x4 datasets are shown in table 3 table 4 table 5 and table 6 respectively the best performance in terms of the best combination between precision and sensitivity was achieved by the classifiers pts and ctc applying smote four times for the training set dataset smote x4 specifically it can be observed from table 6 that pts and ctc achieved the best values for the f measure metric 0 69 and the best precision 0 35 and 0 34 for ppv respectively analyzing the effect of increasing the number of smote applications it can be noticed an increment of the sensitivity of the classifiers but a decrement of the precision ppv only ensemble based classifiers such as rf ctc and pts have demonstrated to be able to keep or improve the precision while improving notably the sensitivity when the number of smote applied increases nearest neighbors classifiers were applied using the values of 1 3 5 7 9 11 and 13 for the number of neighbors the performance follows the same trend when the number of neighbors increases in particular the sensitivity increases when the number of neighbors k increases specially for the greatest number of smote applications from 0 51 of sensitivity for k 1 to 0 80 for k 13 for the smote x4 dataset no significant ppv variations depending on the number of times that smote was applied were found in the nearest neighbors classifiers all values for ppv were around 0 26 for such reason just 1nn and 7nn were shown in this study the svm approach has shown generally the best sensitivity values reaching up to 0 82 for both smote x3 and smote x4 datasets however its ppv is lower than rf ctc and pts ensemble based methods in general terms the mlp and furia classifiers do not present competitive performance values for all analyzed datasets the decision trees based c4 5 algorithm was the classifier that reduced its precision less than that of the remaining when increasing the number of smote applied the sensitivity of the c4 5 is increased up to 0 49 in smote x4 moreover this classifier generates interpretable tree based models for all these reasons the three tree based ensemble methods rf ctc and pts with the c4 5 as classifier were adopted in this study in order to overcome the c4 5 results the rf ensemble technique was trained by using 100 decision trees and although it improves the ppv of c4 5 it gets worse results in terms of sensitivity the ctc algorithm improved notably the sensitivity of the c4 5 classifier unlike other methods the ctc approach improves its ppv when the number of smote increases in addition to it generates the most compact and easily interpretable knowledge models comparing with that of the other trees based methods proposed here finally the pts approach achieved the best trade off between ppv and sensitivity p p v 0 35 and s n 0 59 for the smote x4 dataset however it obtains complex tree based knowledge models which are difficult to interpret fig 3 shows the model obtained by the pts algorithm using the smote x4 dataset as training the classifier pts showed that in august and september the first criterion applied in the tree was based in the value of the emi index in the previous season or months according to the idea proposed in feng and chen 2014 in fact the agreement goes even further in september when the next criterion depends on the value of the dmi index and based on those two indices the model correctly forecasts near 95 of the total extreme events the analysis of decision trees from other models is remarkably similar results for model c4 5 with smote x2 in september and august basically show the same classification as pts not shown in august and september the first division is based on the value of emi index during the months or season previous to an extreme monsoon and in september the next criterion is based on the value of dmi this model shows that the most important conditions for an extreme monsoon event to occur during september are a value of emi during summer ranging from 0 3931 to 0 4720 and a value of dmi in april lower than 0 1700 in concordance with feng and chen 2014 where it was postulated that a high value of emi would lead to an extreme monsoon event as long as the iod remains out of its positive phase these two criteria in the c4 5 decision tree affecting to the conditions in the upper branches for september can lead to more than 80 of extreme monsoon events correctly predicted finally a model like ctc with smote x1 is characterized by a very simplified tree not shown but even in this case the only criterion to predict an extreme monsoon event during september is that the value of emi three months before june has to be higher than 0 358 it is important to note that even when the phases of emi and dmi in the months previous to an extreme event are apparently the main conditions for its occurrence all the other climatic patterns suggested in the bibliography appear in upper branches of those trees and or in the branches corresponding to other months with a lower occurrence of extreme events for example enso represented by soi or n34 appears as a predictor in all the models and is the most important criterion for an extreme monsoon event to occur in july or october on the other hand the pdo has a much more residual impact and it only shows up as a criterion in a secondary branch of the pts model for september and affecting to less than 5 of the correctly predicted occurrence of an extreme monsoon finally it is interesting to notice that most of these predictors are based on short lags a few months or one season in advance but the evolution of the monsoon index eii and the atmospheric soi index include some important predictors which can go as far back as the previous winter or even to the situation during the previous autumn almost one year before the occurrence of an extreme monsoon these long lags are good indicators of the complex processes involved in the physical mechanisms connecting the western north pacific and the climatic conditions over the rest of the pacific and indian oceans and concur with the classical idea of a lagged connection between monsoon conditions in one summer with the evolution of enso many seasons before wang et al 2001 the notable similitudes in the classification criteria identified from these three models and the agreement of these criteria with existing literature lead us to consider as quite robust the relationship between extreme monsoon events and the climatological patterns involved additionally decision trees show some evident advantages over classical analysis for example they provide a range of quantitative values of each index related to the occurrence of an extreme monsoon opposed to a more classical approximation where it is usual to consider only the phase of a particular climate pattern as the main predictor 5 conclusions a new methodology based on the definition of a new index for the wnpsm and on the transformation of the problem into an imbalanced classification problem has been proposed to forecast the intensity of the wnpsm that is if the wnpsm will be an extreme monsoon for the next month a representative number of classification algorithms specifically designed for imbalanced problems has been tested showing that the svm black box models obtain the best sensitivity that is they present a ratio of extreme monsoons correctly forecasted greater than that of the remaining ones equally the random forest ensemble technique shows the best positive predictive value that is the probability to predict an extreme monsoon correctly is the highest however the models based on trees have reached the best balance between sensitivity and positive predictive value in fact results from those models are very much in agreement with previous literature and show that the el niño modoki is the climatic pattern with the highest influence on the occurrence of an extreme wnpsm but that this pattern is modulated by the situation over the indian ocean characterized by the indian ocean dipole future work is directed towards formulating the problem as a regression problem to predict the monthly wnp index to forecast extremely low values of the wnpdi associated to very dry monsoons and to extend the analysis to seasonal monsoon forecasting acknowledgments the authors would like to thank the spanish ministry of economy and competitiveness support under projects tin2014 55894 c2 r and cgl2013 44530 p grant bes 2014 069733 and the junta de andalucía under project p12 tic 1728 appendix a supplementary data the following is the supplementary data related to this article data monsoon envsoft 2017 163 data monsoon envsoft 2017 163 appendix a supplementary data supplementary data related to this article can be found at https doi org 10 1016 j envsoft 2017 11 024 
26377,aquatic macroinvertebrates 18 physical chemical water characteristics and 30 hydromorphological variables were assessed at 85 locations on leyte island philippines biological traits derived from literature were linked to the biological samples based on four different trait estimation methods these data were used to determine the relation with river characteristics using negative binomial regression at least five feeding habit modalities were associated with conductivity velocity ph temperature ammonium n concentrations and sediment the various methods of estimating trait abundance differ in determined major patterns and ecological implications therefore the estimation method used should be explicitly described in trait related papers to avoid misinterpretation trait abundance environment relationships can be linear or non linear and therefore a careful selection of the functional relationship should be performed the process of extracting knowledge from data is of paramount importance as relevant ecological insights were extracted providing insights on flow wastewater and nutrient management in the rivers keywords negative binomial regression feeding strategies flow velocity tropical island generalized linear models philippines 1 introduction the traits characteristics of an organism i e body size feeding strategy locomotion are connected to ecosystem functions doledec and statzner 2010 and strongly influence ecosystem properties hooper et al 2005 as organisms traits such as feeding strategies and habit requirements are strongly affected by the habitat conditions heino 2005 relating these traits to habitat and environmental characteristics can provide significant insights into the structure and functioning of biological communities usseglio polatera et al 2000b moreover the analysis of biological traits provides a better understanding of the mechanism of the organisms responses bolam et al 2016 over the past two decades the number of studies related to aquatic macroinvertebrate traits has been increasingly growing most studies focus on the relationship between environmental conditions natural and disturbed and the traits of these organisms menezes et al 2010 schmera et al 2017 they have also been successfully applied to assess aquatic ecosystems bonada et al 2007b dolédec et al 2011 however in depth investigations and documentation on data processing and modelling of aquatic macroinvertebrate traits are lacking recently various macroinvertebrate trait databases have been developed statzner et al 2007 schmidt kloiber and hering 2015 usepa 2016 however the use of these databases has limitations and challenges such as they are hampered by missing data of some taxa violle et al 2015 scattered information among databases poschlod et al 2003 varying trait coding and differences in trait modalities classes categories among databases usseglio polatera et al 2000b and few databases available for taxa found in the tropics thus appropriate measures should be taken when using these databases in a number of studies traits have been allocated to each taxon from these databases but the estimation of trait abundance has not been thoroughly investigated yet various modelling techniques have been used for modelling and analysing macroinvertebrate traits the chosen approach is usually driven by the purpose of the study the nature of the data and the acceptability to the end user of the model mouton et al 2009 forio et al 2017b mondy et al 2016 applied calibrated boosted regression tree models to predict community tolerance using multiple biological traits with their individual responses to specific stressors statistical models such as logistic regression richards et al 1997 zero inflated poisson regression boets et al 2013 and various multivariate analysis methods bremner et al 2006 feld and hering 2007 were used to relate environmental conditions and macroinvertebrate traits these relationships can be linear non linear or skewed unimodal however these assumptions are uncritically considered in statistical procedures austin 2002 although analysis and modelling of macroinvertebrate traits have been applied in temperate aquatic systems statzner et al 2005 bonada et al 2007a boets et al 2013 it has been only occasionally studied in tropical and sub tropical regions to date only limited studies applied functional traits to assess water integrity in neotropical streams tomanova et al 2008 dedieu et al 2015 tupinambas et al 2016 cameroonian tchakonte et al 2015 and iranian aazami et al 2015 rivers the trait based analysis is scarcely explored in the tropical regions thus we ascertain answers to the questions on whether the abundances of macroinvertebrate traits are associated linearly or non linearly e g quadratic to a gradient of an environmental variable and what is the effect of the different methods of allocating trait abundances on the response functions this study therefore aims to determine how the abundance of macroinvertebrate traits particularly the feeding strategies evolve across the gradient of an environmental variable on a tropical island furthermore the effect of different methods of allocating trait abundances on the response functions is investigated hence a case study on the leyte island philippines is presented we applied negative binomial regressions to model the abundance of each feeding strategy modalities categories of aquatic macroinvertebrate as a function of each environmental condition physical chemical and hydromorphological variables the discovered ecological insights are then translated into water management 2 material and methods the overall phases implemented in this study is presented in fig 1 each step is described in detail in the proceeding sections 2 1 study area the leyte island is the eighth largest island in the philippines and has a surface area of 7368 km2 fig 2 the island is irregular in shape and has mountains in the centre of which the highest reaches 1349 m a complex system of short streams drains from the mountains to the coasts pletcher 2015 the climate of the island is characterized by a relatively high temperature 24 33 c high humidity and abundant rainfall average annual rainfall of 2100 4500 mm forio et al 2017a human activities within the island include crop cultivation industry quarrying urbanization and aquaculture the crops cultivated are rice coconut corn maize abaca tobacco bananas pineapple and sugarcane manganese deposits sandstone and limestone are quarried in the north west additionally industrial plants such as coconut oil mills a copper smelting plant a phosphate fertilizer and ethyl alcohol production plants are operating on the island a geothermal production field is also located in the west of the island forio et al 2017a 2 2 data collection aquatic macroinvertebrates were sampled on the island at 85 different locations the sampling campaign was conducted during the dry season april may 2015 macroinvertebrates were monitored through kick sampling with a standard handnet conical net with a frame size of 20 30 cm and a mesh size of 500 μm as described by gabriels et al 2010 for each sampling site a 10 20 m stretch was sampled for 5 min sampling effort was proportionally distributed across all aquatic habitats present at the sampling site including bed substrates stones sand or mud macrophytes floating submerged emerging and other floating or submerged natural and artificial substrates all the collected material was transferred to buckets with covers afterwards the samples were sieved and organisms were sorted alive in the laboratory macroinvertebrates were identified to the family level for each sampling site physical chemical water quality characteristics were measured table 1 additionally hydromorphological characteristics table 1 were determined through field inspection for a concise definition of each hydromorphological variable we refer to table s1 2 3 data pre processing and exploration prior to the analysis total n was removed because most of the observations were below the detection limit bdl for all other physical chemical variables bdl measurements were set to the detection limit to focus on freshwater running waters five brackish sites were not included in the analysis resulting in 80 locations left for analysis one of each pair of highly correlated continuous and ordinal categorical variables was removed wherein the variable with missing observations was eliminated the pearson correlation coefficient was used and a correlation coefficient of 0 8 was considered high information on traits of each macroinvertebrate was gathered from various databases marlin 2006 tachet et al 2000 schmidt kloiber and hering 2015 usepa 2016 as each taxon was identified to the family level which is of a higher level than the one most often given in the trait database the traits of the most dominant and or common species found were used as identified by the taxonomy expert for 78 of the taxa the feeding strategy was found in the database of tachet et al 2000 this database used a fuzzy coding procedure for describing the link between a taxon and its traits which provides information on the amplitude of taxon s preference for further information on the database we refer to usseglio polatera et al 2000b as some taxa can have more than one trait modality category class we used four different methodologies table 2 for estimating trait abundance for a specified taxon in even distributed trait abundance estimation tae the abundance of baetidae was equally distributed over the 2 modalities in weighted tae the abundance of baetidae was proportionally distributed over the two modalities with weights based on the fuzzy coding of the modalities in dominant tae all counts of the taxon were allocated to the modality with the highest fuzzy coding and no counts to the other modalities in case there were two modalities with the same numeric coding and was the highest then both were considered and the counts of the taxa were equally divided between the modalities finally in an all tae all counts of the taxon were assigned to all modalities an example is given in table 2 for baetidae with a total count of 3845 and is fuzzy coded 3 for scraper and 1 for deposit feeder for a few taxa with traits not fuzzy coded we assigned equal coding to different trait modalities if the taxon possesses more than one trait modality these varying trait abundance estimation methods represent different ecological assumptions the even distributed tae assumed that the taxa were exchangeably using these trait modalities weighted tae represents a taxon was using the given modalities but with preference for the traits with highest numeric fuzzy coding and less preference for the traits with lower trait modalities dominant assumes that a taxon was mainly using the most preferred trait modality all allocations represent the maximum potential traits of a taxon wherein the taxon was simultaneously using all these trait modalities to avoid information overload we only focus on feeding habit trait table 3 as this trait i has been considered as an essential factor in structuring invertebrate communities pearson and rosenberg 1987 and ii is commonly analysed probably due to the link between feeding strategy and ecosystem functions schmera et al 2017 to determine the frequency distribution of each trait modality histograms of each trait modality were plotted additionally scatterplots and boxplots of the abundances of each trait modality were plotted as a function of each continuous and categorical environmental variable respectively to explore the data 2 4 statistical modelling we related the trait abundance as a function of environmental variables data transformation prior to modelling was commonly performed in most trait based analysis heino et al 2013 monaghan and soares 2014 however we present a methodology that avoids the transformation of both abundance and environmental variables allowing a straightforward interpretation of results furthermore various environmental data types i e numeric ordinal and nominal were taken into account during the modelling procedure lastly the joint effect of two environmental variables was explored typically not considered in other trait based studies 2 5 general modelling approach to relate the abundance of each trait modality in response to each environmental variable negative binomial regression models nbm were fitted in particular the model assumes that for a given environmental condition the abundance can be described by a negative binomial distribution similar to poisson regression the conditional mean of the negative binomial distribution was related to the environmental variables through a log link function the logarithm of the mean abundance can e g be modelled as a linear or quadratic function of one or more environmental variables whereas the poisson distribution implies that the variance of the abundance equals the mean say μ the negative binomial distribution allows for overdispersion i e the conditional variance equals μ μ2 k where k is an overdispersion parameter overdispersion is known to happen often in biological count processes bowlby and gibson 2015 harrison 2015 fernandez and pledger 2016 sim and ong 2016 all parameters in the nbm were estimated by means of maximum likelihood zuur et al 2009 all model fits were performed after centering the continuous environmental variables i e subtracting the mean because this procedure reduces the strength of the collinearity between the terms in the statistical model and hence reducing the variance inflation effect backward elimination was applied as a model selection procedure to determine whether a linear or quadratic relation exists between counts of a trait modality and a numerical environmental variable i e first the model with a linear and quadratic effect was fitted if the quadratic effect was not significant at the 5 level of significance the term was removed from the model and the linear term was tested again at the 5 level of significance the procedure was repeated for each numerical environmental variable ordinal environmental variables entered the model as ordinal factor variables with orthogonal contrasts resulting in linear quadratic cubic effect parameters degree of polynomial is one less than the number of levels of the ordinal factor for nominal environmental variables no model selection was performed model assumptions were assessed by plotting the deviance residuals against fitted values to assess homogeneity and correctness of the mean model all statistical tests were performed at the 5 level of significance all analyses were performed with the r software r core team 2016 and negative binomial regression was performed with the mass r package venables and ripley 2002 2 6 modelling trait abundances estimated by each tae methods for each estimation method table 2 each trait modality of feeding strategy was modelled as a function of velocity temperature and stream width represent physical river conditions conductivity represents general pollution biological oxygen demand bod5 represents organic pollution and chlorophyll nitrate n and total p represent nutrient pollution to determine how the abundance of each trait modality evolved the nbm results for a number of selected trait modalities were visualized as the estimated mean abundance of the trait modality under study as a function of the selected variables for a selected trait modality scraper the abundance ratio for each pair of tae methods was plotted as a function of selected environmental variables for studying differences between tae methods with respect to the relationships between the mean abundances and environmental variables 2 7 modelling trait abundances as a function of two environmental variables to gain insight into the joint effect of two environmental variables on the trait abundance nbm were fitted for each pair of environmental variables again a backward elimination model building strategy was employed starting with testing for the presence of the interaction effect a significant interaction indicates that the effect of one environmental variable depends on the value of the other environmental variables and vice versa to avoid information overload only the trait abundance based on weighted tae method of each trait modality was considered 2 8 model visualization to visualize the model the results of the nbm per trait modality were visualized as the estimated mean abundance of the selected trait modality as a function of selected environmental variables furthermore to visualize the fractions of each trait modality across a gradient of an environmental variable the mean percentages of each trait modality were plotted using a bar graph as a function of the environmental variables that were significantly associated with five or more trait modalities 3 results 3 1 trait abundance estimation tae methods the mean abundances based on each trait allocation evolved either linearly or quadratically with certain environmental variables table s2 in general the mean abundances of most feeding strategies increased with increasing velocity fig s1 specifically the abundance of filter feeders and piercers were linearly related with velocity while the abundance of scraper was quadratically related with velocity wherein the count of scrapers gradually increased from 0 to 0 75 m s velocity then exponentially increased after a velocity of 0 75 m s the abundances of most feeding strategies were significantly related to conductivity except for the abundance of deposit feeders and piercers the abundance of each trait modality was generally increasing until about 200 μs cm and was subsequently decreasing from about 400 μs cm fig s2 furthermore the mean abundance of the piercers predators and scrapers was decreasing with increasing chlorophyll concentration fig 3 the mean abundance of deposit feeders increased with increasing bod5 and average stream width fig 4 the abundance of filter feeders and deposit feeders peaks at 0 6 mg l nitrate n fig s3 3 2 abundance ratio between trait allocations fig 5 presents the evolution of mean abundance ratios between the tae methods along a gradient of an environmental variable a ratio of one indicates that the mean abundance between two tae methods is equal a ratio higher than one suggests that the mean abundance of the numerator tae method is higher than the denominator tae method a ratio lower than one therefore denotes the opposite in general the mean abundance ratio between tae methods evolves linearly or quadratically along chlorophyll concentration conductivity and velocity gradients only the mean abundance ratio of even distributed and all tae methods was flat along the chlorophyll concentration gradient which suggests that this abundance ratio does not change with the chlorophyll concentration 3 3 mean trait abundance in relation to environmental variables the associations of mean trait abundances with the environmental variables are presented in table s3 at least five feeding strategies were associated with conductivity velocity ph water temperature ammonium n mineral substrate bed substrate and sediment matrix filter feeders piercers and scrapers were negatively associated with elevation while filter feeders and predators were positively related with dissolved oxygen turbidity was negatively related to filter feeders piercers predators and scrapers moreover filter feeders piercers and predators decreased with increasing chemical oxygen demand cod deposit feeders and filter feeders linearly increased with increasing stream width and decreased with increasing stream depth the different feeding strategies were associated with the hydromorphological variables for instance deposit feeders were linearly associated with shading variation in width sludge layer and twigs quadratically associated with mineral substrate sediment angularity and riffle class cubically associated with sediment matrix scrapers were related to land use shading presence of macrophytes channel form variation in width sludge layer presence of twigs mineral substrate bank slope bed compaction sediment matrix presence of macroalgae and quarrying fig 6 presents the visualization of models with two environmental variables with increasing velocity and conductivity the deposit feeders increased in abundance fig 6a the mean abundance of filter feeders and scrapers was quadratically related to conductivity and increased with increasing velocity fig 6b table s4 as with the model developed for filter feeders a significant interaction was observed between the quadratic conductivity and velocity table s5 this suggests that the effect of the conductivity varies with velocity specifically at lower conductivities 400 μs cm the trait abundance increased with increasing velocity but at higher conductivities 400 μs cm the trait abundance decreased with increasing velocity the abundance of predators decreased with increasing chlorophyll concentration and increased with increasing velocity fig 6d the abundance of filter feeders decreased with increasing turbidity and was quadratically associated with nitrate n concentration 3 4 percentages of each trait modality the fraction of each feeding strategy generally evolved along the gradient of an environmental variable fig 7 the percentage of scrapers increased as temperature increased while the percentage of predators and shredders decreased fig 7a with increasing conductivity the percentage of deposit feeders increased with increasing conductivity while the highest percentage of predators and piercers was observed at the lowest conductivity both filter feeders and scrapers had the highest percentages at intermediate conductivities fig 7b the percentage of filter feeders increased with increasing ph while predators and shredders decreased and the percentage of deposit feeders and scrapers changed minimally fig 7c the percentage of filter feeders increased with increasing velocity while that of deposit feeders and shredders decreased with increasing velocity and the percentages of predators and piercers hardly changed fig 7d 4 discussion 4 1 data collection given the fact that studies of running waters in the tropics are scarce the number of observations in our study was relatively large however the observations were only recorded during the dry season it was reported that flowing waters in tropical regions have much lower annual variation in temperature than those in temperate regions but the distinct wet and dry seasons can produce dynamic discharge regimes that interchangeably disturb hydrological connectivity and alter flows carrie et al 2015 macroinvertebrate compositional variation can be a result of these seasonal changes in tropical rivers as observed in various studies sanchez arguello et al 2010 carrie et al 2015 garcia et al 2015 for instance sanchez arguello et al 2010 found higher taxon richness and evenness in the dry season and ramirez et al 2006 found the highest insect densities and biomasses during the dry season however a few other studies documented that seasonal variability does not affect macroinvertebrate communities melo and froehlich 2001 jorcin and nogueira 2008 feio et al 2015 aside from seasonality the effect of temporal variations on the trait environment relationship is understudied heino et al 2013 associations may vary on month to month and year to year temporal scales therefore it is recommended to collect data in different seasons and varying temporal scales to determine the differences of trait response between seasons and periods 4 2 data pre processing an in depth investigation on allocating trait abundance is lacking in several studies the estimation of trait abundance is not explicitly described however some papers mentioned to have applied the weighted tae method gayraud et al 2003 heino et al 2013 van der linden et al 2016 to our knowledge no one has explored the influence of using different approaches to allocate trait abundance based on the results of our study the discovered statistical relationships e g linear or quadratic differ between the allocation approaches although the general patterns e g decreasing or increasing are similar along a gradient of an environmental variable with few exceptions as there can be differences in both the determined major patterns and ecological implications among the tae methods this should be explicitly described in the methods due to the fact that trait abundance data are analysed in various statistical methods e g ordination methods and are used in trait based indices e g shannon wiener diversity index heino et al 2013 our results offer straightforward interpretation wherein the abundance of a trait modality is related to a certain environmental variable this is the strength of the methodology implemented in this paper to our knowledge our study is one of the few trait based analyses that applied untransformed data one of the limitations in most abundance trait based analysis is the practice of logarithmically transformed abundance with trait frequency log x 1 trait frequency monaghan and soares 2014 statistical decisions can modify the ecological model as a result of data transformation such as log x 1 to stabilise the variance this will alter the functional relationship from an additive to a multiplicative form austin 2002 furthermore o hara and kotze 2010 strongly suggested not to transform log square root the count data because statistical analysis of transformed count data may give wrong results only when the dispersion is small and the mean counts are large transformations followed by normal linear regression can give trustworthy results remarkably our data was not transformed prior to analysis as it was not needed when modelling count data with an appropriate statistical model nbm 4 3 ecological modelling the shape of the response relationship curves is an important aspect to consider in statistical methods linear relationships between organisms traits and environmental variables are often used uncritically austin 2002 thus a hypothesis testing might be needed for assessing whether a species is related to environmental variables with a unimodal symmetric or linear function austin 2002 the use of straight lines linear for species environment relationships have to be tested in our study these relationships were tested and results revealed that relationships can be linear quadratic or even cubic the assumptions of function curves should not be generalized with a single shape as environmental variables are not always linearly or quadratically associated with a taxon or its traits as illustrated in our study thus care is needed when applying and interpreting statistical techniques such as the redundancy analysis and canonical correspondence analysis which assumes linear and unimodal relations respectively between traits and environmental variables 4 4 ecological insights the results of this analysis provide insight in what might possibly occur with environmental alterations for instance with an increase in temperature the number of most feeding strategies most likely will increase and the ratio of scrapers will most likely also increase it was reported that the number of families increased linearly with temperature jacobsen et al 1997 the increased ratio of scrapers could be due to more growth of their food source algae as a result of more sun coming into the system thereby increasing temperature most of the feeding strategies are quadratically associated with conductivity wherein they peak at an intermediate conductivity the abundance of deposit feeders starts to decline at a higher conductivity in comparison with the rest of the feeding strategies in addition the percentage of deposit feeders increased with increasing conductivity an ecological insight can be drawn on what might be expected upon for example a dam installation which would result in a low velocity increase in depth and accumulation of fine sediment based on the developed models the number of shredders most likely will decrease fig 6d and scrapers may increase fig s4 and the number of organisms will decline fig 6a e fig s1 the results of this study indicate that feeding strategies are strongly associated with different environmental conditions and provide an understanding of the observed patterns and shifts of invertebrates along a gradient of an environmental variable 4 5 implications for assessment and management usseglio polatera et al 2000a stated that effective water management can be achieved if monitoring and assessment are based on a profound understanding of the mechanisms that lead to the occurrence of organisms in the environment as such the development of trait based indicators to define disturbed and semi natural river conditions are relevant in environmental assessment furthermore the modelling approach presented in this paper can be applied to other traits and can serve as reference information for the development of stress specific indicators for tropical running waters or the functioning of the ecosystem and its relation to the services such as carbon sequestration nutrient cycling and water purification based on the observed patterns and associations insights on river management can be deduced as velocity is clearly linked with the abundance of most trait modalities alteration of velocity may have a serious ecological implication thus proper management is needed to reduce this risk similarly most feeding types are related to conductivity it is reported that conductivity is related to water pollution and land use wang and yin 1997 ometo et al 2000 shrestha and kazama 2007 therefore appropriate regulation related to conductivity is important to minimize the decline of the ecological status of rivers by determining the causes of its unnatural readings it has been reported that the domination of deposit feeders in a system is an indication of organic pollution guilpart et al 2012 coelho et al 2015 khedhri et al 2016 likewise our results show an increase in deposit feeder abundance as the bod5 increases thus the dominance of deposit feeders may reveal an evidence of organic pollution and therefore an appropriate management strategy must be applied such as the installation of wastewater treatment plants waste stabilization ponds or constructed wetlands gibert et al 2010 2012 ecological thresholds are defined as the point at which there is an abrupt change in an ecosystem quality property or phenomenon or where small changes in an environmental driver produce large responses in the ecosystem groffman et al 2006 thus they are necessary as a basis for monitoring assessment and management in developing regions these thresholds are generally adopted from developed and temperate regions however they may not be relevant in the tropics as conditions might be different the outcome of this modelling exercise can be useful to assess the applicability of these thresholds for instance behar 1997 reported that water conductivity in the range of 150 500 μs cm supported diverse aquatic life however based on our results the abundance of most of the feeding strategies already dropped in the range of 100 450 μs cm fig s2 this suggests that the natural water conductivity in the region may be lower than the threshold set for other regions 4 6 knowledge extracted from the data the process of extracting knowledge from the data is of paramount importance as this leads to the discovery of relevant ecological insights subsequently the discovered relations provide insights into river management and improvements in environmental monitoring and assessment however improvement in data collection can be implemented by in situ measurement of traits or through automated trait identification data collection at different seasons and varying temporal scales is recommended data pre processing is crucial gibert et al 2016 as the ecological insights and interpretation largely depend on it methods of estimating trait abundance should be explicitly described as each method differ in both the general patterns and ecological implications furthermore data transformation prior to analysis is sometimes necessary for a particular modelling technique but untransformed data ease interpretation of results due to its straightforwardness mostly only linear relationships between a trait modality and environmental conditions are considered but trait environment relationships might be quadratic or cubic in nature and thus a generalization of curve functions is not recommended and hypothesis testing might be necessary to determine the curve shapes the model visualization as presented in the graphs greatly aids the interpretation of model outcome and aid in extracting ecological insights though this paper only focuses on feeding habit this methodology can be extended to other traits such as respiration locomotion life cycle etc the discovered patterns can also be used to validate observations in other systems with similar environmental conditions acknowledgement marie anne eurie forio is funded by the special research fund of ghent university to support the vlir ecuador biodiversity network the authors would like to thank da bfar department of agriculture bureau of fisheries and aquatic resources for facilitating and providing the sampling permit hannah rissah abad and jik abad for assisting with the logistics and the preparation of the sampling campaign the department of soil science visayas state university for accommodating the laboratory activities eve radam cesar yap john paul poliquit jose talavera anthony sinahon and all the people involved during the sampling campaign and jana van butsel for translating french literature on invertebrates traits into english the authors would like to thank the anonymous reviewers for the valuable suggestions which improved the manuscript appendix a supplementary data the following is the supplementary data related to this article online data online data appendix a supplementary data supplementary data related to this article can be found at https doi org 10 1016 j envsoft 2017 11 025 
26377,aquatic macroinvertebrates 18 physical chemical water characteristics and 30 hydromorphological variables were assessed at 85 locations on leyte island philippines biological traits derived from literature were linked to the biological samples based on four different trait estimation methods these data were used to determine the relation with river characteristics using negative binomial regression at least five feeding habit modalities were associated with conductivity velocity ph temperature ammonium n concentrations and sediment the various methods of estimating trait abundance differ in determined major patterns and ecological implications therefore the estimation method used should be explicitly described in trait related papers to avoid misinterpretation trait abundance environment relationships can be linear or non linear and therefore a careful selection of the functional relationship should be performed the process of extracting knowledge from data is of paramount importance as relevant ecological insights were extracted providing insights on flow wastewater and nutrient management in the rivers keywords negative binomial regression feeding strategies flow velocity tropical island generalized linear models philippines 1 introduction the traits characteristics of an organism i e body size feeding strategy locomotion are connected to ecosystem functions doledec and statzner 2010 and strongly influence ecosystem properties hooper et al 2005 as organisms traits such as feeding strategies and habit requirements are strongly affected by the habitat conditions heino 2005 relating these traits to habitat and environmental characteristics can provide significant insights into the structure and functioning of biological communities usseglio polatera et al 2000b moreover the analysis of biological traits provides a better understanding of the mechanism of the organisms responses bolam et al 2016 over the past two decades the number of studies related to aquatic macroinvertebrate traits has been increasingly growing most studies focus on the relationship between environmental conditions natural and disturbed and the traits of these organisms menezes et al 2010 schmera et al 2017 they have also been successfully applied to assess aquatic ecosystems bonada et al 2007b dolédec et al 2011 however in depth investigations and documentation on data processing and modelling of aquatic macroinvertebrate traits are lacking recently various macroinvertebrate trait databases have been developed statzner et al 2007 schmidt kloiber and hering 2015 usepa 2016 however the use of these databases has limitations and challenges such as they are hampered by missing data of some taxa violle et al 2015 scattered information among databases poschlod et al 2003 varying trait coding and differences in trait modalities classes categories among databases usseglio polatera et al 2000b and few databases available for taxa found in the tropics thus appropriate measures should be taken when using these databases in a number of studies traits have been allocated to each taxon from these databases but the estimation of trait abundance has not been thoroughly investigated yet various modelling techniques have been used for modelling and analysing macroinvertebrate traits the chosen approach is usually driven by the purpose of the study the nature of the data and the acceptability to the end user of the model mouton et al 2009 forio et al 2017b mondy et al 2016 applied calibrated boosted regression tree models to predict community tolerance using multiple biological traits with their individual responses to specific stressors statistical models such as logistic regression richards et al 1997 zero inflated poisson regression boets et al 2013 and various multivariate analysis methods bremner et al 2006 feld and hering 2007 were used to relate environmental conditions and macroinvertebrate traits these relationships can be linear non linear or skewed unimodal however these assumptions are uncritically considered in statistical procedures austin 2002 although analysis and modelling of macroinvertebrate traits have been applied in temperate aquatic systems statzner et al 2005 bonada et al 2007a boets et al 2013 it has been only occasionally studied in tropical and sub tropical regions to date only limited studies applied functional traits to assess water integrity in neotropical streams tomanova et al 2008 dedieu et al 2015 tupinambas et al 2016 cameroonian tchakonte et al 2015 and iranian aazami et al 2015 rivers the trait based analysis is scarcely explored in the tropical regions thus we ascertain answers to the questions on whether the abundances of macroinvertebrate traits are associated linearly or non linearly e g quadratic to a gradient of an environmental variable and what is the effect of the different methods of allocating trait abundances on the response functions this study therefore aims to determine how the abundance of macroinvertebrate traits particularly the feeding strategies evolve across the gradient of an environmental variable on a tropical island furthermore the effect of different methods of allocating trait abundances on the response functions is investigated hence a case study on the leyte island philippines is presented we applied negative binomial regressions to model the abundance of each feeding strategy modalities categories of aquatic macroinvertebrate as a function of each environmental condition physical chemical and hydromorphological variables the discovered ecological insights are then translated into water management 2 material and methods the overall phases implemented in this study is presented in fig 1 each step is described in detail in the proceeding sections 2 1 study area the leyte island is the eighth largest island in the philippines and has a surface area of 7368 km2 fig 2 the island is irregular in shape and has mountains in the centre of which the highest reaches 1349 m a complex system of short streams drains from the mountains to the coasts pletcher 2015 the climate of the island is characterized by a relatively high temperature 24 33 c high humidity and abundant rainfall average annual rainfall of 2100 4500 mm forio et al 2017a human activities within the island include crop cultivation industry quarrying urbanization and aquaculture the crops cultivated are rice coconut corn maize abaca tobacco bananas pineapple and sugarcane manganese deposits sandstone and limestone are quarried in the north west additionally industrial plants such as coconut oil mills a copper smelting plant a phosphate fertilizer and ethyl alcohol production plants are operating on the island a geothermal production field is also located in the west of the island forio et al 2017a 2 2 data collection aquatic macroinvertebrates were sampled on the island at 85 different locations the sampling campaign was conducted during the dry season april may 2015 macroinvertebrates were monitored through kick sampling with a standard handnet conical net with a frame size of 20 30 cm and a mesh size of 500 μm as described by gabriels et al 2010 for each sampling site a 10 20 m stretch was sampled for 5 min sampling effort was proportionally distributed across all aquatic habitats present at the sampling site including bed substrates stones sand or mud macrophytes floating submerged emerging and other floating or submerged natural and artificial substrates all the collected material was transferred to buckets with covers afterwards the samples were sieved and organisms were sorted alive in the laboratory macroinvertebrates were identified to the family level for each sampling site physical chemical water quality characteristics were measured table 1 additionally hydromorphological characteristics table 1 were determined through field inspection for a concise definition of each hydromorphological variable we refer to table s1 2 3 data pre processing and exploration prior to the analysis total n was removed because most of the observations were below the detection limit bdl for all other physical chemical variables bdl measurements were set to the detection limit to focus on freshwater running waters five brackish sites were not included in the analysis resulting in 80 locations left for analysis one of each pair of highly correlated continuous and ordinal categorical variables was removed wherein the variable with missing observations was eliminated the pearson correlation coefficient was used and a correlation coefficient of 0 8 was considered high information on traits of each macroinvertebrate was gathered from various databases marlin 2006 tachet et al 2000 schmidt kloiber and hering 2015 usepa 2016 as each taxon was identified to the family level which is of a higher level than the one most often given in the trait database the traits of the most dominant and or common species found were used as identified by the taxonomy expert for 78 of the taxa the feeding strategy was found in the database of tachet et al 2000 this database used a fuzzy coding procedure for describing the link between a taxon and its traits which provides information on the amplitude of taxon s preference for further information on the database we refer to usseglio polatera et al 2000b as some taxa can have more than one trait modality category class we used four different methodologies table 2 for estimating trait abundance for a specified taxon in even distributed trait abundance estimation tae the abundance of baetidae was equally distributed over the 2 modalities in weighted tae the abundance of baetidae was proportionally distributed over the two modalities with weights based on the fuzzy coding of the modalities in dominant tae all counts of the taxon were allocated to the modality with the highest fuzzy coding and no counts to the other modalities in case there were two modalities with the same numeric coding and was the highest then both were considered and the counts of the taxa were equally divided between the modalities finally in an all tae all counts of the taxon were assigned to all modalities an example is given in table 2 for baetidae with a total count of 3845 and is fuzzy coded 3 for scraper and 1 for deposit feeder for a few taxa with traits not fuzzy coded we assigned equal coding to different trait modalities if the taxon possesses more than one trait modality these varying trait abundance estimation methods represent different ecological assumptions the even distributed tae assumed that the taxa were exchangeably using these trait modalities weighted tae represents a taxon was using the given modalities but with preference for the traits with highest numeric fuzzy coding and less preference for the traits with lower trait modalities dominant assumes that a taxon was mainly using the most preferred trait modality all allocations represent the maximum potential traits of a taxon wherein the taxon was simultaneously using all these trait modalities to avoid information overload we only focus on feeding habit trait table 3 as this trait i has been considered as an essential factor in structuring invertebrate communities pearson and rosenberg 1987 and ii is commonly analysed probably due to the link between feeding strategy and ecosystem functions schmera et al 2017 to determine the frequency distribution of each trait modality histograms of each trait modality were plotted additionally scatterplots and boxplots of the abundances of each trait modality were plotted as a function of each continuous and categorical environmental variable respectively to explore the data 2 4 statistical modelling we related the trait abundance as a function of environmental variables data transformation prior to modelling was commonly performed in most trait based analysis heino et al 2013 monaghan and soares 2014 however we present a methodology that avoids the transformation of both abundance and environmental variables allowing a straightforward interpretation of results furthermore various environmental data types i e numeric ordinal and nominal were taken into account during the modelling procedure lastly the joint effect of two environmental variables was explored typically not considered in other trait based studies 2 5 general modelling approach to relate the abundance of each trait modality in response to each environmental variable negative binomial regression models nbm were fitted in particular the model assumes that for a given environmental condition the abundance can be described by a negative binomial distribution similar to poisson regression the conditional mean of the negative binomial distribution was related to the environmental variables through a log link function the logarithm of the mean abundance can e g be modelled as a linear or quadratic function of one or more environmental variables whereas the poisson distribution implies that the variance of the abundance equals the mean say μ the negative binomial distribution allows for overdispersion i e the conditional variance equals μ μ2 k where k is an overdispersion parameter overdispersion is known to happen often in biological count processes bowlby and gibson 2015 harrison 2015 fernandez and pledger 2016 sim and ong 2016 all parameters in the nbm were estimated by means of maximum likelihood zuur et al 2009 all model fits were performed after centering the continuous environmental variables i e subtracting the mean because this procedure reduces the strength of the collinearity between the terms in the statistical model and hence reducing the variance inflation effect backward elimination was applied as a model selection procedure to determine whether a linear or quadratic relation exists between counts of a trait modality and a numerical environmental variable i e first the model with a linear and quadratic effect was fitted if the quadratic effect was not significant at the 5 level of significance the term was removed from the model and the linear term was tested again at the 5 level of significance the procedure was repeated for each numerical environmental variable ordinal environmental variables entered the model as ordinal factor variables with orthogonal contrasts resulting in linear quadratic cubic effect parameters degree of polynomial is one less than the number of levels of the ordinal factor for nominal environmental variables no model selection was performed model assumptions were assessed by plotting the deviance residuals against fitted values to assess homogeneity and correctness of the mean model all statistical tests were performed at the 5 level of significance all analyses were performed with the r software r core team 2016 and negative binomial regression was performed with the mass r package venables and ripley 2002 2 6 modelling trait abundances estimated by each tae methods for each estimation method table 2 each trait modality of feeding strategy was modelled as a function of velocity temperature and stream width represent physical river conditions conductivity represents general pollution biological oxygen demand bod5 represents organic pollution and chlorophyll nitrate n and total p represent nutrient pollution to determine how the abundance of each trait modality evolved the nbm results for a number of selected trait modalities were visualized as the estimated mean abundance of the trait modality under study as a function of the selected variables for a selected trait modality scraper the abundance ratio for each pair of tae methods was plotted as a function of selected environmental variables for studying differences between tae methods with respect to the relationships between the mean abundances and environmental variables 2 7 modelling trait abundances as a function of two environmental variables to gain insight into the joint effect of two environmental variables on the trait abundance nbm were fitted for each pair of environmental variables again a backward elimination model building strategy was employed starting with testing for the presence of the interaction effect a significant interaction indicates that the effect of one environmental variable depends on the value of the other environmental variables and vice versa to avoid information overload only the trait abundance based on weighted tae method of each trait modality was considered 2 8 model visualization to visualize the model the results of the nbm per trait modality were visualized as the estimated mean abundance of the selected trait modality as a function of selected environmental variables furthermore to visualize the fractions of each trait modality across a gradient of an environmental variable the mean percentages of each trait modality were plotted using a bar graph as a function of the environmental variables that were significantly associated with five or more trait modalities 3 results 3 1 trait abundance estimation tae methods the mean abundances based on each trait allocation evolved either linearly or quadratically with certain environmental variables table s2 in general the mean abundances of most feeding strategies increased with increasing velocity fig s1 specifically the abundance of filter feeders and piercers were linearly related with velocity while the abundance of scraper was quadratically related with velocity wherein the count of scrapers gradually increased from 0 to 0 75 m s velocity then exponentially increased after a velocity of 0 75 m s the abundances of most feeding strategies were significantly related to conductivity except for the abundance of deposit feeders and piercers the abundance of each trait modality was generally increasing until about 200 μs cm and was subsequently decreasing from about 400 μs cm fig s2 furthermore the mean abundance of the piercers predators and scrapers was decreasing with increasing chlorophyll concentration fig 3 the mean abundance of deposit feeders increased with increasing bod5 and average stream width fig 4 the abundance of filter feeders and deposit feeders peaks at 0 6 mg l nitrate n fig s3 3 2 abundance ratio between trait allocations fig 5 presents the evolution of mean abundance ratios between the tae methods along a gradient of an environmental variable a ratio of one indicates that the mean abundance between two tae methods is equal a ratio higher than one suggests that the mean abundance of the numerator tae method is higher than the denominator tae method a ratio lower than one therefore denotes the opposite in general the mean abundance ratio between tae methods evolves linearly or quadratically along chlorophyll concentration conductivity and velocity gradients only the mean abundance ratio of even distributed and all tae methods was flat along the chlorophyll concentration gradient which suggests that this abundance ratio does not change with the chlorophyll concentration 3 3 mean trait abundance in relation to environmental variables the associations of mean trait abundances with the environmental variables are presented in table s3 at least five feeding strategies were associated with conductivity velocity ph water temperature ammonium n mineral substrate bed substrate and sediment matrix filter feeders piercers and scrapers were negatively associated with elevation while filter feeders and predators were positively related with dissolved oxygen turbidity was negatively related to filter feeders piercers predators and scrapers moreover filter feeders piercers and predators decreased with increasing chemical oxygen demand cod deposit feeders and filter feeders linearly increased with increasing stream width and decreased with increasing stream depth the different feeding strategies were associated with the hydromorphological variables for instance deposit feeders were linearly associated with shading variation in width sludge layer and twigs quadratically associated with mineral substrate sediment angularity and riffle class cubically associated with sediment matrix scrapers were related to land use shading presence of macrophytes channel form variation in width sludge layer presence of twigs mineral substrate bank slope bed compaction sediment matrix presence of macroalgae and quarrying fig 6 presents the visualization of models with two environmental variables with increasing velocity and conductivity the deposit feeders increased in abundance fig 6a the mean abundance of filter feeders and scrapers was quadratically related to conductivity and increased with increasing velocity fig 6b table s4 as with the model developed for filter feeders a significant interaction was observed between the quadratic conductivity and velocity table s5 this suggests that the effect of the conductivity varies with velocity specifically at lower conductivities 400 μs cm the trait abundance increased with increasing velocity but at higher conductivities 400 μs cm the trait abundance decreased with increasing velocity the abundance of predators decreased with increasing chlorophyll concentration and increased with increasing velocity fig 6d the abundance of filter feeders decreased with increasing turbidity and was quadratically associated with nitrate n concentration 3 4 percentages of each trait modality the fraction of each feeding strategy generally evolved along the gradient of an environmental variable fig 7 the percentage of scrapers increased as temperature increased while the percentage of predators and shredders decreased fig 7a with increasing conductivity the percentage of deposit feeders increased with increasing conductivity while the highest percentage of predators and piercers was observed at the lowest conductivity both filter feeders and scrapers had the highest percentages at intermediate conductivities fig 7b the percentage of filter feeders increased with increasing ph while predators and shredders decreased and the percentage of deposit feeders and scrapers changed minimally fig 7c the percentage of filter feeders increased with increasing velocity while that of deposit feeders and shredders decreased with increasing velocity and the percentages of predators and piercers hardly changed fig 7d 4 discussion 4 1 data collection given the fact that studies of running waters in the tropics are scarce the number of observations in our study was relatively large however the observations were only recorded during the dry season it was reported that flowing waters in tropical regions have much lower annual variation in temperature than those in temperate regions but the distinct wet and dry seasons can produce dynamic discharge regimes that interchangeably disturb hydrological connectivity and alter flows carrie et al 2015 macroinvertebrate compositional variation can be a result of these seasonal changes in tropical rivers as observed in various studies sanchez arguello et al 2010 carrie et al 2015 garcia et al 2015 for instance sanchez arguello et al 2010 found higher taxon richness and evenness in the dry season and ramirez et al 2006 found the highest insect densities and biomasses during the dry season however a few other studies documented that seasonal variability does not affect macroinvertebrate communities melo and froehlich 2001 jorcin and nogueira 2008 feio et al 2015 aside from seasonality the effect of temporal variations on the trait environment relationship is understudied heino et al 2013 associations may vary on month to month and year to year temporal scales therefore it is recommended to collect data in different seasons and varying temporal scales to determine the differences of trait response between seasons and periods 4 2 data pre processing an in depth investigation on allocating trait abundance is lacking in several studies the estimation of trait abundance is not explicitly described however some papers mentioned to have applied the weighted tae method gayraud et al 2003 heino et al 2013 van der linden et al 2016 to our knowledge no one has explored the influence of using different approaches to allocate trait abundance based on the results of our study the discovered statistical relationships e g linear or quadratic differ between the allocation approaches although the general patterns e g decreasing or increasing are similar along a gradient of an environmental variable with few exceptions as there can be differences in both the determined major patterns and ecological implications among the tae methods this should be explicitly described in the methods due to the fact that trait abundance data are analysed in various statistical methods e g ordination methods and are used in trait based indices e g shannon wiener diversity index heino et al 2013 our results offer straightforward interpretation wherein the abundance of a trait modality is related to a certain environmental variable this is the strength of the methodology implemented in this paper to our knowledge our study is one of the few trait based analyses that applied untransformed data one of the limitations in most abundance trait based analysis is the practice of logarithmically transformed abundance with trait frequency log x 1 trait frequency monaghan and soares 2014 statistical decisions can modify the ecological model as a result of data transformation such as log x 1 to stabilise the variance this will alter the functional relationship from an additive to a multiplicative form austin 2002 furthermore o hara and kotze 2010 strongly suggested not to transform log square root the count data because statistical analysis of transformed count data may give wrong results only when the dispersion is small and the mean counts are large transformations followed by normal linear regression can give trustworthy results remarkably our data was not transformed prior to analysis as it was not needed when modelling count data with an appropriate statistical model nbm 4 3 ecological modelling the shape of the response relationship curves is an important aspect to consider in statistical methods linear relationships between organisms traits and environmental variables are often used uncritically austin 2002 thus a hypothesis testing might be needed for assessing whether a species is related to environmental variables with a unimodal symmetric or linear function austin 2002 the use of straight lines linear for species environment relationships have to be tested in our study these relationships were tested and results revealed that relationships can be linear quadratic or even cubic the assumptions of function curves should not be generalized with a single shape as environmental variables are not always linearly or quadratically associated with a taxon or its traits as illustrated in our study thus care is needed when applying and interpreting statistical techniques such as the redundancy analysis and canonical correspondence analysis which assumes linear and unimodal relations respectively between traits and environmental variables 4 4 ecological insights the results of this analysis provide insight in what might possibly occur with environmental alterations for instance with an increase in temperature the number of most feeding strategies most likely will increase and the ratio of scrapers will most likely also increase it was reported that the number of families increased linearly with temperature jacobsen et al 1997 the increased ratio of scrapers could be due to more growth of their food source algae as a result of more sun coming into the system thereby increasing temperature most of the feeding strategies are quadratically associated with conductivity wherein they peak at an intermediate conductivity the abundance of deposit feeders starts to decline at a higher conductivity in comparison with the rest of the feeding strategies in addition the percentage of deposit feeders increased with increasing conductivity an ecological insight can be drawn on what might be expected upon for example a dam installation which would result in a low velocity increase in depth and accumulation of fine sediment based on the developed models the number of shredders most likely will decrease fig 6d and scrapers may increase fig s4 and the number of organisms will decline fig 6a e fig s1 the results of this study indicate that feeding strategies are strongly associated with different environmental conditions and provide an understanding of the observed patterns and shifts of invertebrates along a gradient of an environmental variable 4 5 implications for assessment and management usseglio polatera et al 2000a stated that effective water management can be achieved if monitoring and assessment are based on a profound understanding of the mechanisms that lead to the occurrence of organisms in the environment as such the development of trait based indicators to define disturbed and semi natural river conditions are relevant in environmental assessment furthermore the modelling approach presented in this paper can be applied to other traits and can serve as reference information for the development of stress specific indicators for tropical running waters or the functioning of the ecosystem and its relation to the services such as carbon sequestration nutrient cycling and water purification based on the observed patterns and associations insights on river management can be deduced as velocity is clearly linked with the abundance of most trait modalities alteration of velocity may have a serious ecological implication thus proper management is needed to reduce this risk similarly most feeding types are related to conductivity it is reported that conductivity is related to water pollution and land use wang and yin 1997 ometo et al 2000 shrestha and kazama 2007 therefore appropriate regulation related to conductivity is important to minimize the decline of the ecological status of rivers by determining the causes of its unnatural readings it has been reported that the domination of deposit feeders in a system is an indication of organic pollution guilpart et al 2012 coelho et al 2015 khedhri et al 2016 likewise our results show an increase in deposit feeder abundance as the bod5 increases thus the dominance of deposit feeders may reveal an evidence of organic pollution and therefore an appropriate management strategy must be applied such as the installation of wastewater treatment plants waste stabilization ponds or constructed wetlands gibert et al 2010 2012 ecological thresholds are defined as the point at which there is an abrupt change in an ecosystem quality property or phenomenon or where small changes in an environmental driver produce large responses in the ecosystem groffman et al 2006 thus they are necessary as a basis for monitoring assessment and management in developing regions these thresholds are generally adopted from developed and temperate regions however they may not be relevant in the tropics as conditions might be different the outcome of this modelling exercise can be useful to assess the applicability of these thresholds for instance behar 1997 reported that water conductivity in the range of 150 500 μs cm supported diverse aquatic life however based on our results the abundance of most of the feeding strategies already dropped in the range of 100 450 μs cm fig s2 this suggests that the natural water conductivity in the region may be lower than the threshold set for other regions 4 6 knowledge extracted from the data the process of extracting knowledge from the data is of paramount importance as this leads to the discovery of relevant ecological insights subsequently the discovered relations provide insights into river management and improvements in environmental monitoring and assessment however improvement in data collection can be implemented by in situ measurement of traits or through automated trait identification data collection at different seasons and varying temporal scales is recommended data pre processing is crucial gibert et al 2016 as the ecological insights and interpretation largely depend on it methods of estimating trait abundance should be explicitly described as each method differ in both the general patterns and ecological implications furthermore data transformation prior to analysis is sometimes necessary for a particular modelling technique but untransformed data ease interpretation of results due to its straightforwardness mostly only linear relationships between a trait modality and environmental conditions are considered but trait environment relationships might be quadratic or cubic in nature and thus a generalization of curve functions is not recommended and hypothesis testing might be necessary to determine the curve shapes the model visualization as presented in the graphs greatly aids the interpretation of model outcome and aid in extracting ecological insights though this paper only focuses on feeding habit this methodology can be extended to other traits such as respiration locomotion life cycle etc the discovered patterns can also be used to validate observations in other systems with similar environmental conditions acknowledgement marie anne eurie forio is funded by the special research fund of ghent university to support the vlir ecuador biodiversity network the authors would like to thank da bfar department of agriculture bureau of fisheries and aquatic resources for facilitating and providing the sampling permit hannah rissah abad and jik abad for assisting with the logistics and the preparation of the sampling campaign the department of soil science visayas state university for accommodating the laboratory activities eve radam cesar yap john paul poliquit jose talavera anthony sinahon and all the people involved during the sampling campaign and jana van butsel for translating french literature on invertebrates traits into english the authors would like to thank the anonymous reviewers for the valuable suggestions which improved the manuscript appendix a supplementary data the following is the supplementary data related to this article online data online data appendix a supplementary data supplementary data related to this article can be found at https doi org 10 1016 j envsoft 2017 11 025 
26378,scoring animal behavior is increasingly needed for better understanding ecological processes for example behavior shapes harvesting likelihood thus management of harvested resources should improve after accounting for behavior driven processes automatic video recording at controlled arenas is the most widespread method for scoring behavior however long term tracking animals while keeping identity is still an opened challenge here we develop an ad hoc algorithm for multi tracking objects during days or even weeks to fulfill the particular needs for a behavioral assay concerning a fish species targeted by recreational fishing specifically we overcome the challenge of keeping fish identity in a context where they often disappeared from the camera when entering a shelter the pixel size was low compared to the size of the arena and the lighting was constrained by the wellbeing of the fish this work may contribute to better assess the behavioral features of fish in long lasting lab conditions keywords multi object tracking tracking by identification long lasting tracking behavioral assays 1 introduction animals in wild populations are continuously at risk of being predated or harvested but some individuals have larger survival likelihood than others for example it is well known that hunting and fishing implies size related selection matsumura et al 2011 uusi heikkilä et al 2015 behavior related selection has been reported as well biro and sampson 2015 ciuti et al 2012 diaz pauli et al 2015 härkönen et al 2016 jørgensen and holt 2013 madden and whiteside 2014 so certain behavioral phenotypes are selected against alós et al 2012 2015 diaz pauli et al 2015 klefoth et al 2013 vainikka et al 2016 when the traits under selection are genetically heritable the population may be driven to a harvest induced evolution de roos et al 2006 law 2007 philipp et al 2015 uusi heikkilä et al 2015 which implies a number of unintended and undesired outcomes that ultimately reduce yield matsumura et al 2011 mollet et al 2016 however such negative outcomes on population dynamics are expected only when selection gradients are consistent in time that is when a specific animal exhibits a given personality i e it tends to behave in a similar way the concept of animal personality dingemanse and wolf 2010 2013 mittelbach et al 2014 is based in two patterns first the existence of between individual differences in behavior that are consistently repeated at the long term i e behavioral types is often recognizable over the within individual variability second several behavioral traits are often correlated shaping what it has been named as behavioral syndromes sih et al 2004 thus animal personality can be scored after knowing how the animal uses space in a number of contexts technological improvements have led to the widespread use of underwater devices for data collection continuous monitoring and estimates of population parameters in the field williams et al 2010 aguzzi et al 2013 indeed cabled video observatories for the remote long term and high frequency monitoring of fish and their environment have been used in coastal temperate areas aguzzi et al 2013 2015 matabos et al 2014 to study behavioural response to environmental changes or human perturbations mecho et al 2017 some studies in the field by using camera devices e g alós et al 2015 mecho et al 2017 have already focused on the role of the behaviour in processes of artificial selection nevertheless the study of behavioural types or personality traits by video recording in the field is still an open challenge since recognition at the individual level is required therefore the actual limitation for a more widespread implementation of behavior focused management is seemingly not conceptual but technical fish personality can be scored from their patterns of space use but only after a long lasting observation time which includes individual fish recognition in that context video recording fish in experimental conditions i e tanks is becoming a widespread method laskowski et al 2016 ozbilgin and glass 2004 papadakis et al 2012 in spite that some potential drawbacks e g extrapolating the results to field conditions niemelä and dingemanse 2014 závorka et al 2015 alongside unsupervised tracking on video recorded is also increasingly required to better quantify long term behavioral patterns which in turn would allow scoring individuals in terms of animal personality hence real time tracking of video images should be preferred because rigorous statistical analysis of the behavioral patterns associated with movement involve not only long lasting experiments but also unsupervised extraction of a number of space use metrics papadakis et al 2012 unsupervised tracking of moving targets over a video sequence has been addressed from different fields burghardt et al 2004 dankert et al 2009 de chaumont et al 2012 kabra et al 2012 kühl and burghardt 2013 and trucco and plakas 2006 even concerning the specific case of fish tracking several techniques have been proposed delcourt et al 2013 dell et al 2014 for a review enabling advances in behavioral studies however some challenges remain concerning long lasting precise and accurate tracking of animals especially when the experimental settings do not fit high standards on one hand how to preserve individuals identities throughout long time periods remains an opened question delcourt et al 2013 but see noldus et al 2001 and straw and dickinson 2009 on the other specific experimental settings usually impose additional difficulties e g arenas with uniform white and artificial light conditions are generally used delcourt et al 2006 most tracking algorithms uses kalman filters or particle filters for predicting positions of moving targets kalman et al 1960 nummiaro et al 2003 pinkiewicz et al 2008 target detection is mainly based on detecting shape color or finding blobs which may be achieved via histograms analysis or attempting to maximize correlations between statistical models sattar and dudek 2006 further detection and tracking stages can be combined together in order to remove faulty detections fontaine et al 2008 spampinato et al 2008 xia et al 2016 pérez escudero et al 2014 chuang and hwang 2016 recently methods based on deep learning have been popularized for example the features of the fish head have been used to identify targets along frames xu et al 2017 similarly qian et al 2014 combined kalman filtering with the determinant of hesssian as deep learning methods have reveled as efficient alternatives for recognizing visual patterns convolutional neural networks cnn has been implemented also xu et al 2017 for instance wang et al 2016 combined cnn with head fish identification being able to track several individuals at the same time however actual applications of these methods require highly standardized settings in terms of for example a well lightened and small scenario this is not the case of the experiments that are currently conducted for assessing the links between behavior and vulnerability to fishing of a small bodied marine fish alós et al 2013 this species serranus scriba is one of the main targets of the recreational angler from mallorca western mediterranean alós et al 2013 in this case the experimental settings were particularly challenging individuals can appear and disappear from the scene because they were allowed to spend long periods of time in a shelter however fish identity must be preserved along days which have been achieved by tagging the fish with colored marks provided that fish were submitted to several behavioral experiments the tank must be continuously monitored during all the day light period moreover the experimental arena was very large in comparison to the sizes of colored tags which imposed to deal with high resolution images in order to represent tags with enough number of pixels finally the lightening conditions of the scenario must emulate those experienced by the fish at wild conditions thus light had a strong blue component that difficult to discriminate fish from background and distorts colors in addition the lightening setting produces reflections and it was not uniform even under those unfavorable experimental settings we developed and described an ad hoc algorithm for remote long term unsupervised tracking of a number of fish moving in a tank moreover the tracking algorithm has been coupled with proper statistical tools for extracting a number of metrics related with space use from fish trajectories which will be used for analyzing of fish behavior and scoring fish personality 2 materials and methods 2 1 model species serranus scriba is a simultaneous hermaphrodite serranid widely distributed in the mediterranean sea it is heavily exploited by the local recreational angling fishery alós et al 2013 which preferentially captures individuals with low reproductive investment and high adult body size alós et al 2014 further evidences of behavioural related selection on exploration rates has been reported alós et al 2012 2 2 capture and tagging procedure fish were obtained from the wild by angling in coastal waters from 5 up to 10 m depth near the research experimental station limia in andratx bay on the sw coast of mallorca where experiments were conducted fish were double tagged with t tags painted in different fluorescent colours white 0 red r and green g tags were combined in pairs tagging was done by means of a hollow needle tagging gun according to dell 1968 length and weight measures were taken at the same time manipulation time did not exceed 1 min overall transport tagging and release into tank procedure took about 30 min no mortality was observed neither during this procedure nor during the experiment the experiments were conducted with adult fish total body length 15 7 1 9 cm mean mass 53 21 5 g mean standard deviation fish were distributed in groups of 5 individuals of approximately the same size 0 6 individuals m 2 temperature water was the same as that of the sea 26 2 0 6c mean 1 standard deviation over all days of trials 2 3 scenario and experiment description 2 3 1 scenario the experimental tank with dimensions 4 m 2 m length x with and 30 cm water depth was filled with sea water and fitted with an open circuit filtering system due to the evidences that the behavioral performance of fish may be affected by environmental light conditions e g marchesan et al 2005 berdahl et al 2013 the experimental tank was illuminated emulating the natural light conditions of the habitat of the species and at the same time covering the requirement of an homogeneous illumination in all the extension of the tank the experimental light atmosphere was provided by a light system led strip lights with three modes on off switching that automatically let the transition from 6500 k white daylight to 460 nm blue sunset light to darkness a diurnal nocturnal sequence of illumination was projected onto the water i e a day night light cycle specifically diurnal conditions ranged from 7am to 7pm white light was switched on an hour after the blue one and was switched off an hour before to ensure a progressive illumination of the tank with conditions similar to the sunrise and sunset ones and the darkness conditions ranged from 7pm to 7am the experimental tank was divided in two areas the shelter 1 m long and the arena 3 m long shelter was done with stone blocks and artificial plastic plants to provide a safe area for the fish fig 1 to ensure fish were not disturbed during the experiment and to avoid changes in the illumination conditions that could affect the quality or recordings the tank was isolated from the rest of the laboratory with black curtains avoiding in that way changes in illumination or reflexes in the surface of the water from abroad and minimizing visual stimulus or other unintended stimulus cameras and lights were controlled remotely out of sight of the experimental arena 2 3 2 camera and electronic equipment an axis p1428 e ip camera captures the arena images from a zenithal position nearly 2 m above the water surface and send them to a computer in order to be processed the axis p1428 e camera is compact and outdoor ready providing up to 8 3 mp 4k hd of resolution at 30 frames per second to achieve real time requirements the camera works at 10 frames per second the computer has a intel i7 cpu with 6 cores and 12 threads 32 gb of ram and 2 tb of hard disk the software developed in python 3 5 using opencv 3 0 runs on ubuntu 16 04 os that machine has also a matlab 2016a license to perform post processing tasks the time required to process an image is always less than 90ms the system is configured to process ten images per second 2 3 3 experiment description the general objective was to describe the trajectories and the space use of every fish within groups of a maximum of 5 individuals fish were identified with a combination of one or two red r or green g tags according the combinations r rr rg g gg fig 2 a given fish group remained in the tank for one or two weeks during such a time fish were submitted to several behavioral test every day behavioral tests are fully detailed elsewhere campos candela et al in prep however provided that the objective of this paper is to demonstrate the usefulness of the general strategy here we focused in a single behavioral test exploration test are aimed to describe the space use when a new item is located at he center of the arena walsh and cummins 1976 adriaenssens and johnsson 2013 one exploration test was completed daily between 8 30am to 10 30am here we focused in the first four exploration tests experienced by a given fish group 2 4 multi tracking algorithm let us consider an incoming color image i n x y x and y indexes pixel positions and n frames which is pre processed time to time in order to detect and identify fishes by means of tag colors i n x y is composed by their red green and blue components r n x y g n x y and b n x y as i n x y r n x y g n x y b n x y although to simplify notation x and y are omitted to get i n r n g n b n 2 4 1 detection in the rgb color space red and green colors are directly identified from r n and g n components the first step consists on the subtraction of the remaining background red and green components which in small proportions are present in the water from the original image the result is multiplied by a factor 1 8 as it is shown 1 r n 1 8 r n r n o 2 g n 1 8 g n g n o o stands for the all ones matrix and r n and g n are measures of the red and green background levels values r n and g n are computed every frame in order to follow possible variations of lighting such values are obtained by computing the mean of all points inside a sub image which is large enough to average the effect of a tag inside it in the moment of the update the factor 1 8 have been obtained empirically and it depends on the lighting proceeding in this way the pixels from red and green tags will reach levels near saturation that operation will simplify color detection further down then r n g n b n components are used to compute two of the three hsv color space components h n and v n while s n is not required because the modification performed in r n and g n by 1 and 2 makes detection independent of saturation and very few dependent of v n h n and v n are used to construct b r n and b g n binary images from which the red and green objects will be detected respectively the hsv color space is closed to the way in which humans perceive color in hsv hue h represents color which are clearly defined according human perception value v is the brightness of the color and saturation s indicates the range of grey in the color space considering that h n and v n take values in the range 0 1 b r n and b g n matrices are obtained by applying the next conditions to their pixels according to 3 b r n h n 0 0894 o r h n 0 7821 a n d v n 0 38 and 4 b g n h n 0 1117 a n d h n 0 5028 a n d v n 0 38 then b r n and b g n present groups of connected pixels of value 1 concentrated in the small regions where red and green tags if any appear respectively b r n and b g n provide the area of interest roi in the original image the pixel positions of those small connected regions identify red and green tags from the background 0s by identifying those regions and computing their center of masses and areas in number of pixels the specimens present in each image are precisely located however previously to characterize tags two major issues must be attended the first due to structural constrains in the emplacement of lights some reflections appear inevitably in the scenario and there is not a perfect uniform lighting in all the area the strategy of dynamically adapt the red and green components presented in 1 2 fights against non uniform lighting but light reflections contain some red and green components that causes false object detection the simplest way to combat it is by eliminating those regions with a mask m that mask is obtained by detecting the areas where reflections are produced and applying a morphological dilation to be sure of capturing reflections when they move for the presence of small waves in the water surface that causes the lose of tags in the moment that fishes crosses the zone of reflections in the similar way that tags are lost when they suffer an occlusion the second the operations described in 3 and 4 occasionally split a single tag in more than one region so previously to count regions and computing sizes and centers a morphological closing must be done in order that disconnected closed regions will be reconnected that operation is performed by a square of 5 5 pixels as structuring element se the closing is obtained by applying the dilation of b r n and b g n by se followed by the erosion of the outcome by the same se the size of se will depend on the imatge resolution and the size of tags then for each frame n centroids of red and green detected objects are stored in c r n and c g n lists ordered by their areas in descending order areas are also stored in a r n and a g n lists c r n and c g n are lists of points of different lengths containing their x y coordinates and their frame number if no detection of red color is produced in frame n then corresponding c r n and a r n remain empty in order to introduce the notation used lets suppose that in frame number 256 there was observed three red regions then c r 256 could take the form shown in 5 and the information stored inside can be addressed as it is shown in 6 5 c r 256 x 1 y 1 256 x 2 y 2 256 x 3 y 3 256 6 c r 256 2 x 2 y 2 256 c r 256 2 1 2 x 2 y 2 note that we only need c r n c g n a r n and a g n to register the whole experiment this data are obtained and stored in real time from these data the tracks can be obtained by applying different strategies that can be executed not necessarily in real time in this way any algorithmic improvement can always be tested again 2 4 2 early identification and post processing stage theoretically according section 2 3 only four tags of the same color are significant to identify the five possible specimens in the scene however there are considered a maximum of 5 objects of the same color in order to deal with possible classification errors that step is done once per frame and do not require analyzing past results for every frame n the early classification takes information from c r n c g n a r n and a g n as inputs and computes the lists of positions p r r n p r n p r g n p g g n p g n by taking information of colors and analyzing distances of centroids as multiple assignment errors can appear caused by detection mistakes in that first classification we admit the possibility of obtaining more than one individual identified as having same tag that occurs for instance if in one frame there are detected more than one r because individual rr or rg present in the scene are bad detected a post processing step will correct those isolated cases however if in frame n there is no presence of a particular specimen its corresponding list remains empty early identification pseudo code is provided in algorithm 3 in which constants a b 250 and a s 5 are empirically assigned as they are dependent on the scene the camera resolution and the size of tags a b is used to control cases where double tags of the same color are merged to form a bigger one and a s controls some noise introduced occasionally in by light reflections basic red and green tag identification and early fish identification strongly compacts information required for tracking specimens as we replace a high resolution image by the few values stored in the those lists those list of few elements per frame avoid to transfer and store several terabytes of 4k high resolution images videos as they contain the required information to track specimens in the post processing stage as pointed before although almost tags are properly detected natural movements of fishes light reflections etc cause that composed tags rr rg and gg are occasionally bad identified for instance in the set of rr tags drawing a sequential continuous path one miss classified r appears in that particular case in frame n p r n will has more than one value if the real r tag is also present that is the reason because lists can contain more than one element that supposes a big inconsistency that can be corrected we can consider re labeling or simply removing the point as in the post processing stage there is no important time restrictions we have considered the reallocation by applying logical criteria and computing space time distances after applying those algorithms p r r n p r n p r g n p g g n and p g n have a maximum of one element in position n otherwise they are empty fig 3 shows the simplified sequence of operations of the early identification stage in a block diagram the following sequence of operations which we call the post processing stage can be performed indistinctly in real time because they are computationally very efficient or not the post processing stage will allow future algorithm improvements and it is mainly based on kalman filtering which is explained in next section the results of the early identification stage are shown in fig 4 a for a time sequence of 500 frames then in the post processing stage a linear kalman filter is used to obtain positions and instant velocities as well as to estimate state variables when measures are not available due to occlusions or failures in tag segmentation fig 4 b shows for the same sequence of 500 frames the track coordinates obtained after applying the kalman filter those tracks are represented in figure image 1 2 4 3 the kalman filter according to the state space linear model 7 x n a x n 1 w n where vector x n x v x y v y t is the state variable at instant n and x y and v x v y define position and velocity components of a given specimen respectively a is the state transition matrix 8 a 1 δ t 0 0 0 1 0 0 0 0 1 δ t 0 0 0 1 and w n the process noise vector which is assumed to follow a zero mean normal multivariate distribution with covariance q n w n n 0 q n the measurement equation takes the form 9 z n h x n v n being z n the vector of measures i e when dealing with rr specimen z n p r r n t v n the vector of measurement noise for which it is also assumed a zero mean normal distribution with covariance r n v n n 0 r n h the state to measurement matrix takes the form 10 h 1 0 0 0 0 0 1 0 under normal operating conditions when the pre processing step provides z n for frame n the recursive kalman filter is defined by the well known four steps considering p n the error covariance the first step is devoted to predict state x ˆ n and error covariance as follows 11 x ˆ n a x ˆ n 1 12 p n a p n 1 a t q note that symbols and indicate estimation and prediction respectively then p n is used to compute the kalman gain k n as 13 k n p n h t h p n h t r 1 the state is estimated by weighting through k n the prediction and the measure according to 14 x ˆ n x ˆ n k n z n h x ˆ n finally last step computes the error covariance 15 p n p n k n h p n and the algorithm is prepared to next iteration in the case where no measures are available the estimation of the estate is performed directly by the prediction part according the model described in 7 the same is performed in the case in which the distance between the predicted value and the measure is greater than a value c p given in pixels impossible to be covered by a fish in 0 1 s the velocity v x and v y components are obtained directly by the kalman observations by returning the complete state variable every time the fish enters in the shelter the filter status is reset and each new track fragment starts with the parameters by default from v x and v y we obtain the modulus of the instant velocity 2 5 scoring behavior from trajectories crude fish trajectories were not readily usable for comparing fish behavior instead to describe general patterns of activity and exploration six metrics were derived first phi was the proportion of time spent active i e outside the shelter second preliminary histograms of the fish position along the longitudinal axis of the tank fig 1 strongly suggest that fish tend to spend more time either near the shore or near the wall opposite to the shore while spending short time at the middle of the arena according to such a bimodal use of space the data was fitted to an ad hoc distribution consisting in the mixture of two negative exponential distributions with respectively the origins at the shelter and at the opposite wall this distribution is described by three parameters p proportion of time spend near the shelter being 1 p the proportion of the time spend near the opposite wall λ s h e l t e r the rate of the exponential distribution when the fish is near the shelter and λ o p p the rate of the exponential distribution when the fish is near the opposite wall third d t o t a l was the total distance traveled during 2 h measured as the sum of all between frame displacements finally preliminary histograms of the fish velocity displacements between consecutive frames strongly suggest that empirical speed distribution may be adjusted to a negative exponential distribution which was described by λ s p e e d the rate of the exponential distribution the three exponential rates were inversed 1 rate in order to facilitate interpretation of the parameter in terms of average empirical data was fitted to the distributions either the ad hoc distribution for positions along the longitudinal axis of the tank or the exponential distribution for speed using a bayesian fitting procedure raw data was reduced in order to alleviate temporal autocorrelation 1000 evenly distributed points in the temporal series were selected model data and initial values were prepared using the r package https www r project org from where a mcmc iterator was call jags http mcmc jags sourceforge net in all the cases uninformative flat priors were used and convergence was checked using conventional criteria after appropriate burning and thinning posterior distributions were estimated from three independent mcmcs 1000 valid iterations each the remaining parameters d t o t a l and ϕ were computed directly from the trajectories using an r ad hoc script 3 results the experimental setting has demonstrated to be able to successfully video recording the tank during all the experiment duration the multi tracking algorithm has been tested in four behavioral test of 2 h each in order to validate the strategy three fishes tagged as rr rg and r were continuously tracked during each of this four tests the estimated tracks for each specimen were verified by checking the coherence of transitions between the arena and the shelter and by visual inspection kalman filters in the post processing stage help to fill missing points and contributes to clean tracks furthermore the state model used also provides estimations of x y instantaneous velocity components fig 5 a shows the time sequences of x and y coordinates and the modulus of the instant velocity for one specimen in an entire experiment while 5 b shows the representation of those tracks in the arena the trajectories of the three fish considered in one of the 2 h long test is shown in fig 6 the multi tracking algorithm has successfully used to reconstruct the trajectory of the fish even when moving in a group as suggested by the examples displayed at fig 6 3 1 an overview of the activity patterns detected concerning the behavioral pattern depicted by the trajectories the general patterns and the between and within fish variability of the six parameters distilled from the trajectories are shown in fig 7 in spite that the results refers only to three fish and four 2 h trajectories per day some general patterns can be suggested for instance the individual 1 7 was the individual that spent less time out the shelter but when it is outside the shelter it spent more time near the shelter and with an average position closer to it it also showed the highest average speed overall this metrics seems to suggest a conservative low risk behavior opposite the individual 3 in blue at fig 7 spent more time out the shelter and father away despite showing less average speed than red and similar to green it was the one that travelled more distance so it explored a larger area during the four days 4 conclusions the experimental setting has demonstrated to be able to successfully video record the tank during all the experiment duration furthermore in order to deal with the experimental constrains and avoid storing huge amount of video sequences we present an ad hoc approach performed in two steps the first step is done in real time and detects and transfers frame to frame object positions and other characteristics as their areas towards a computer the information extracted in this step is required to identify the tags and hugely compacts high resolution video images 4k processed at a rate of 10 frames per second into a smallest set of values in long lasting experiments that will be the information preserved and stored the second step consists on processing that information to obtain the final tracks it must not necessarily be done in real time moreover we propose up to six metric for scoring fishes from trajectories based in the use of shelter the distribution fish position in relation to the shelter the total distance traveled and the average speed the preliminary results presented here are a successful proof of concept that the combination of video recording and long lasting trajectory reconstruction can be used for scoring fish behavior and contribute to better understand the behavioral features of fish from long lasting experiments a better understanding of individual behavioural patterns in experimental conditions may help to understand better the observed behavioural patterns in the field e g in comparative studies between areas with different conditions and to figure out what could be the role of behaviour in artificial selection contexts ethics animal care and all experimental procedures were authorized by those responsible for the ethics committee for animal experimentation of the university of the balearic islands ceea uib through a permit ref ceea 60 0916 to the fenofish project ref ctm2015 69126 c2 1 r funded by the spanish ministry of science and competitiveness and were carried out in strict accordance with the recommendations from directive 2010 175 63 ue adhering to 176 spanish law rd53 2013 boe n 34 february 8th 2013 our study did not involve endangered or protected species all efforts were made to minimize fish handling and harm acknowledgments this work has been partially supported by the spanish government projects phenofish with references ctm2015 69126 c2 1 r and ctm2015 69126 2 r and is a contribution of the joint research unit imedea limia a c c was supported by a fpu predoctoral fellowship ref fpu13 01440 from the spanish ministry of education culture and sports mecd the authors thank josé antonio antonio garcía del arco from icm csic for his help in the selection of the ip camera 
26378,scoring animal behavior is increasingly needed for better understanding ecological processes for example behavior shapes harvesting likelihood thus management of harvested resources should improve after accounting for behavior driven processes automatic video recording at controlled arenas is the most widespread method for scoring behavior however long term tracking animals while keeping identity is still an opened challenge here we develop an ad hoc algorithm for multi tracking objects during days or even weeks to fulfill the particular needs for a behavioral assay concerning a fish species targeted by recreational fishing specifically we overcome the challenge of keeping fish identity in a context where they often disappeared from the camera when entering a shelter the pixel size was low compared to the size of the arena and the lighting was constrained by the wellbeing of the fish this work may contribute to better assess the behavioral features of fish in long lasting lab conditions keywords multi object tracking tracking by identification long lasting tracking behavioral assays 1 introduction animals in wild populations are continuously at risk of being predated or harvested but some individuals have larger survival likelihood than others for example it is well known that hunting and fishing implies size related selection matsumura et al 2011 uusi heikkilä et al 2015 behavior related selection has been reported as well biro and sampson 2015 ciuti et al 2012 diaz pauli et al 2015 härkönen et al 2016 jørgensen and holt 2013 madden and whiteside 2014 so certain behavioral phenotypes are selected against alós et al 2012 2015 diaz pauli et al 2015 klefoth et al 2013 vainikka et al 2016 when the traits under selection are genetically heritable the population may be driven to a harvest induced evolution de roos et al 2006 law 2007 philipp et al 2015 uusi heikkilä et al 2015 which implies a number of unintended and undesired outcomes that ultimately reduce yield matsumura et al 2011 mollet et al 2016 however such negative outcomes on population dynamics are expected only when selection gradients are consistent in time that is when a specific animal exhibits a given personality i e it tends to behave in a similar way the concept of animal personality dingemanse and wolf 2010 2013 mittelbach et al 2014 is based in two patterns first the existence of between individual differences in behavior that are consistently repeated at the long term i e behavioral types is often recognizable over the within individual variability second several behavioral traits are often correlated shaping what it has been named as behavioral syndromes sih et al 2004 thus animal personality can be scored after knowing how the animal uses space in a number of contexts technological improvements have led to the widespread use of underwater devices for data collection continuous monitoring and estimates of population parameters in the field williams et al 2010 aguzzi et al 2013 indeed cabled video observatories for the remote long term and high frequency monitoring of fish and their environment have been used in coastal temperate areas aguzzi et al 2013 2015 matabos et al 2014 to study behavioural response to environmental changes or human perturbations mecho et al 2017 some studies in the field by using camera devices e g alós et al 2015 mecho et al 2017 have already focused on the role of the behaviour in processes of artificial selection nevertheless the study of behavioural types or personality traits by video recording in the field is still an open challenge since recognition at the individual level is required therefore the actual limitation for a more widespread implementation of behavior focused management is seemingly not conceptual but technical fish personality can be scored from their patterns of space use but only after a long lasting observation time which includes individual fish recognition in that context video recording fish in experimental conditions i e tanks is becoming a widespread method laskowski et al 2016 ozbilgin and glass 2004 papadakis et al 2012 in spite that some potential drawbacks e g extrapolating the results to field conditions niemelä and dingemanse 2014 závorka et al 2015 alongside unsupervised tracking on video recorded is also increasingly required to better quantify long term behavioral patterns which in turn would allow scoring individuals in terms of animal personality hence real time tracking of video images should be preferred because rigorous statistical analysis of the behavioral patterns associated with movement involve not only long lasting experiments but also unsupervised extraction of a number of space use metrics papadakis et al 2012 unsupervised tracking of moving targets over a video sequence has been addressed from different fields burghardt et al 2004 dankert et al 2009 de chaumont et al 2012 kabra et al 2012 kühl and burghardt 2013 and trucco and plakas 2006 even concerning the specific case of fish tracking several techniques have been proposed delcourt et al 2013 dell et al 2014 for a review enabling advances in behavioral studies however some challenges remain concerning long lasting precise and accurate tracking of animals especially when the experimental settings do not fit high standards on one hand how to preserve individuals identities throughout long time periods remains an opened question delcourt et al 2013 but see noldus et al 2001 and straw and dickinson 2009 on the other specific experimental settings usually impose additional difficulties e g arenas with uniform white and artificial light conditions are generally used delcourt et al 2006 most tracking algorithms uses kalman filters or particle filters for predicting positions of moving targets kalman et al 1960 nummiaro et al 2003 pinkiewicz et al 2008 target detection is mainly based on detecting shape color or finding blobs which may be achieved via histograms analysis or attempting to maximize correlations between statistical models sattar and dudek 2006 further detection and tracking stages can be combined together in order to remove faulty detections fontaine et al 2008 spampinato et al 2008 xia et al 2016 pérez escudero et al 2014 chuang and hwang 2016 recently methods based on deep learning have been popularized for example the features of the fish head have been used to identify targets along frames xu et al 2017 similarly qian et al 2014 combined kalman filtering with the determinant of hesssian as deep learning methods have reveled as efficient alternatives for recognizing visual patterns convolutional neural networks cnn has been implemented also xu et al 2017 for instance wang et al 2016 combined cnn with head fish identification being able to track several individuals at the same time however actual applications of these methods require highly standardized settings in terms of for example a well lightened and small scenario this is not the case of the experiments that are currently conducted for assessing the links between behavior and vulnerability to fishing of a small bodied marine fish alós et al 2013 this species serranus scriba is one of the main targets of the recreational angler from mallorca western mediterranean alós et al 2013 in this case the experimental settings were particularly challenging individuals can appear and disappear from the scene because they were allowed to spend long periods of time in a shelter however fish identity must be preserved along days which have been achieved by tagging the fish with colored marks provided that fish were submitted to several behavioral experiments the tank must be continuously monitored during all the day light period moreover the experimental arena was very large in comparison to the sizes of colored tags which imposed to deal with high resolution images in order to represent tags with enough number of pixels finally the lightening conditions of the scenario must emulate those experienced by the fish at wild conditions thus light had a strong blue component that difficult to discriminate fish from background and distorts colors in addition the lightening setting produces reflections and it was not uniform even under those unfavorable experimental settings we developed and described an ad hoc algorithm for remote long term unsupervised tracking of a number of fish moving in a tank moreover the tracking algorithm has been coupled with proper statistical tools for extracting a number of metrics related with space use from fish trajectories which will be used for analyzing of fish behavior and scoring fish personality 2 materials and methods 2 1 model species serranus scriba is a simultaneous hermaphrodite serranid widely distributed in the mediterranean sea it is heavily exploited by the local recreational angling fishery alós et al 2013 which preferentially captures individuals with low reproductive investment and high adult body size alós et al 2014 further evidences of behavioural related selection on exploration rates has been reported alós et al 2012 2 2 capture and tagging procedure fish were obtained from the wild by angling in coastal waters from 5 up to 10 m depth near the research experimental station limia in andratx bay on the sw coast of mallorca where experiments were conducted fish were double tagged with t tags painted in different fluorescent colours white 0 red r and green g tags were combined in pairs tagging was done by means of a hollow needle tagging gun according to dell 1968 length and weight measures were taken at the same time manipulation time did not exceed 1 min overall transport tagging and release into tank procedure took about 30 min no mortality was observed neither during this procedure nor during the experiment the experiments were conducted with adult fish total body length 15 7 1 9 cm mean mass 53 21 5 g mean standard deviation fish were distributed in groups of 5 individuals of approximately the same size 0 6 individuals m 2 temperature water was the same as that of the sea 26 2 0 6c mean 1 standard deviation over all days of trials 2 3 scenario and experiment description 2 3 1 scenario the experimental tank with dimensions 4 m 2 m length x with and 30 cm water depth was filled with sea water and fitted with an open circuit filtering system due to the evidences that the behavioral performance of fish may be affected by environmental light conditions e g marchesan et al 2005 berdahl et al 2013 the experimental tank was illuminated emulating the natural light conditions of the habitat of the species and at the same time covering the requirement of an homogeneous illumination in all the extension of the tank the experimental light atmosphere was provided by a light system led strip lights with three modes on off switching that automatically let the transition from 6500 k white daylight to 460 nm blue sunset light to darkness a diurnal nocturnal sequence of illumination was projected onto the water i e a day night light cycle specifically diurnal conditions ranged from 7am to 7pm white light was switched on an hour after the blue one and was switched off an hour before to ensure a progressive illumination of the tank with conditions similar to the sunrise and sunset ones and the darkness conditions ranged from 7pm to 7am the experimental tank was divided in two areas the shelter 1 m long and the arena 3 m long shelter was done with stone blocks and artificial plastic plants to provide a safe area for the fish fig 1 to ensure fish were not disturbed during the experiment and to avoid changes in the illumination conditions that could affect the quality or recordings the tank was isolated from the rest of the laboratory with black curtains avoiding in that way changes in illumination or reflexes in the surface of the water from abroad and minimizing visual stimulus or other unintended stimulus cameras and lights were controlled remotely out of sight of the experimental arena 2 3 2 camera and electronic equipment an axis p1428 e ip camera captures the arena images from a zenithal position nearly 2 m above the water surface and send them to a computer in order to be processed the axis p1428 e camera is compact and outdoor ready providing up to 8 3 mp 4k hd of resolution at 30 frames per second to achieve real time requirements the camera works at 10 frames per second the computer has a intel i7 cpu with 6 cores and 12 threads 32 gb of ram and 2 tb of hard disk the software developed in python 3 5 using opencv 3 0 runs on ubuntu 16 04 os that machine has also a matlab 2016a license to perform post processing tasks the time required to process an image is always less than 90ms the system is configured to process ten images per second 2 3 3 experiment description the general objective was to describe the trajectories and the space use of every fish within groups of a maximum of 5 individuals fish were identified with a combination of one or two red r or green g tags according the combinations r rr rg g gg fig 2 a given fish group remained in the tank for one or two weeks during such a time fish were submitted to several behavioral test every day behavioral tests are fully detailed elsewhere campos candela et al in prep however provided that the objective of this paper is to demonstrate the usefulness of the general strategy here we focused in a single behavioral test exploration test are aimed to describe the space use when a new item is located at he center of the arena walsh and cummins 1976 adriaenssens and johnsson 2013 one exploration test was completed daily between 8 30am to 10 30am here we focused in the first four exploration tests experienced by a given fish group 2 4 multi tracking algorithm let us consider an incoming color image i n x y x and y indexes pixel positions and n frames which is pre processed time to time in order to detect and identify fishes by means of tag colors i n x y is composed by their red green and blue components r n x y g n x y and b n x y as i n x y r n x y g n x y b n x y although to simplify notation x and y are omitted to get i n r n g n b n 2 4 1 detection in the rgb color space red and green colors are directly identified from r n and g n components the first step consists on the subtraction of the remaining background red and green components which in small proportions are present in the water from the original image the result is multiplied by a factor 1 8 as it is shown 1 r n 1 8 r n r n o 2 g n 1 8 g n g n o o stands for the all ones matrix and r n and g n are measures of the red and green background levels values r n and g n are computed every frame in order to follow possible variations of lighting such values are obtained by computing the mean of all points inside a sub image which is large enough to average the effect of a tag inside it in the moment of the update the factor 1 8 have been obtained empirically and it depends on the lighting proceeding in this way the pixels from red and green tags will reach levels near saturation that operation will simplify color detection further down then r n g n b n components are used to compute two of the three hsv color space components h n and v n while s n is not required because the modification performed in r n and g n by 1 and 2 makes detection independent of saturation and very few dependent of v n h n and v n are used to construct b r n and b g n binary images from which the red and green objects will be detected respectively the hsv color space is closed to the way in which humans perceive color in hsv hue h represents color which are clearly defined according human perception value v is the brightness of the color and saturation s indicates the range of grey in the color space considering that h n and v n take values in the range 0 1 b r n and b g n matrices are obtained by applying the next conditions to their pixels according to 3 b r n h n 0 0894 o r h n 0 7821 a n d v n 0 38 and 4 b g n h n 0 1117 a n d h n 0 5028 a n d v n 0 38 then b r n and b g n present groups of connected pixels of value 1 concentrated in the small regions where red and green tags if any appear respectively b r n and b g n provide the area of interest roi in the original image the pixel positions of those small connected regions identify red and green tags from the background 0s by identifying those regions and computing their center of masses and areas in number of pixels the specimens present in each image are precisely located however previously to characterize tags two major issues must be attended the first due to structural constrains in the emplacement of lights some reflections appear inevitably in the scenario and there is not a perfect uniform lighting in all the area the strategy of dynamically adapt the red and green components presented in 1 2 fights against non uniform lighting but light reflections contain some red and green components that causes false object detection the simplest way to combat it is by eliminating those regions with a mask m that mask is obtained by detecting the areas where reflections are produced and applying a morphological dilation to be sure of capturing reflections when they move for the presence of small waves in the water surface that causes the lose of tags in the moment that fishes crosses the zone of reflections in the similar way that tags are lost when they suffer an occlusion the second the operations described in 3 and 4 occasionally split a single tag in more than one region so previously to count regions and computing sizes and centers a morphological closing must be done in order that disconnected closed regions will be reconnected that operation is performed by a square of 5 5 pixels as structuring element se the closing is obtained by applying the dilation of b r n and b g n by se followed by the erosion of the outcome by the same se the size of se will depend on the imatge resolution and the size of tags then for each frame n centroids of red and green detected objects are stored in c r n and c g n lists ordered by their areas in descending order areas are also stored in a r n and a g n lists c r n and c g n are lists of points of different lengths containing their x y coordinates and their frame number if no detection of red color is produced in frame n then corresponding c r n and a r n remain empty in order to introduce the notation used lets suppose that in frame number 256 there was observed three red regions then c r 256 could take the form shown in 5 and the information stored inside can be addressed as it is shown in 6 5 c r 256 x 1 y 1 256 x 2 y 2 256 x 3 y 3 256 6 c r 256 2 x 2 y 2 256 c r 256 2 1 2 x 2 y 2 note that we only need c r n c g n a r n and a g n to register the whole experiment this data are obtained and stored in real time from these data the tracks can be obtained by applying different strategies that can be executed not necessarily in real time in this way any algorithmic improvement can always be tested again 2 4 2 early identification and post processing stage theoretically according section 2 3 only four tags of the same color are significant to identify the five possible specimens in the scene however there are considered a maximum of 5 objects of the same color in order to deal with possible classification errors that step is done once per frame and do not require analyzing past results for every frame n the early classification takes information from c r n c g n a r n and a g n as inputs and computes the lists of positions p r r n p r n p r g n p g g n p g n by taking information of colors and analyzing distances of centroids as multiple assignment errors can appear caused by detection mistakes in that first classification we admit the possibility of obtaining more than one individual identified as having same tag that occurs for instance if in one frame there are detected more than one r because individual rr or rg present in the scene are bad detected a post processing step will correct those isolated cases however if in frame n there is no presence of a particular specimen its corresponding list remains empty early identification pseudo code is provided in algorithm 3 in which constants a b 250 and a s 5 are empirically assigned as they are dependent on the scene the camera resolution and the size of tags a b is used to control cases where double tags of the same color are merged to form a bigger one and a s controls some noise introduced occasionally in by light reflections basic red and green tag identification and early fish identification strongly compacts information required for tracking specimens as we replace a high resolution image by the few values stored in the those lists those list of few elements per frame avoid to transfer and store several terabytes of 4k high resolution images videos as they contain the required information to track specimens in the post processing stage as pointed before although almost tags are properly detected natural movements of fishes light reflections etc cause that composed tags rr rg and gg are occasionally bad identified for instance in the set of rr tags drawing a sequential continuous path one miss classified r appears in that particular case in frame n p r n will has more than one value if the real r tag is also present that is the reason because lists can contain more than one element that supposes a big inconsistency that can be corrected we can consider re labeling or simply removing the point as in the post processing stage there is no important time restrictions we have considered the reallocation by applying logical criteria and computing space time distances after applying those algorithms p r r n p r n p r g n p g g n and p g n have a maximum of one element in position n otherwise they are empty fig 3 shows the simplified sequence of operations of the early identification stage in a block diagram the following sequence of operations which we call the post processing stage can be performed indistinctly in real time because they are computationally very efficient or not the post processing stage will allow future algorithm improvements and it is mainly based on kalman filtering which is explained in next section the results of the early identification stage are shown in fig 4 a for a time sequence of 500 frames then in the post processing stage a linear kalman filter is used to obtain positions and instant velocities as well as to estimate state variables when measures are not available due to occlusions or failures in tag segmentation fig 4 b shows for the same sequence of 500 frames the track coordinates obtained after applying the kalman filter those tracks are represented in figure image 1 2 4 3 the kalman filter according to the state space linear model 7 x n a x n 1 w n where vector x n x v x y v y t is the state variable at instant n and x y and v x v y define position and velocity components of a given specimen respectively a is the state transition matrix 8 a 1 δ t 0 0 0 1 0 0 0 0 1 δ t 0 0 0 1 and w n the process noise vector which is assumed to follow a zero mean normal multivariate distribution with covariance q n w n n 0 q n the measurement equation takes the form 9 z n h x n v n being z n the vector of measures i e when dealing with rr specimen z n p r r n t v n the vector of measurement noise for which it is also assumed a zero mean normal distribution with covariance r n v n n 0 r n h the state to measurement matrix takes the form 10 h 1 0 0 0 0 0 1 0 under normal operating conditions when the pre processing step provides z n for frame n the recursive kalman filter is defined by the well known four steps considering p n the error covariance the first step is devoted to predict state x ˆ n and error covariance as follows 11 x ˆ n a x ˆ n 1 12 p n a p n 1 a t q note that symbols and indicate estimation and prediction respectively then p n is used to compute the kalman gain k n as 13 k n p n h t h p n h t r 1 the state is estimated by weighting through k n the prediction and the measure according to 14 x ˆ n x ˆ n k n z n h x ˆ n finally last step computes the error covariance 15 p n p n k n h p n and the algorithm is prepared to next iteration in the case where no measures are available the estimation of the estate is performed directly by the prediction part according the model described in 7 the same is performed in the case in which the distance between the predicted value and the measure is greater than a value c p given in pixels impossible to be covered by a fish in 0 1 s the velocity v x and v y components are obtained directly by the kalman observations by returning the complete state variable every time the fish enters in the shelter the filter status is reset and each new track fragment starts with the parameters by default from v x and v y we obtain the modulus of the instant velocity 2 5 scoring behavior from trajectories crude fish trajectories were not readily usable for comparing fish behavior instead to describe general patterns of activity and exploration six metrics were derived first phi was the proportion of time spent active i e outside the shelter second preliminary histograms of the fish position along the longitudinal axis of the tank fig 1 strongly suggest that fish tend to spend more time either near the shore or near the wall opposite to the shore while spending short time at the middle of the arena according to such a bimodal use of space the data was fitted to an ad hoc distribution consisting in the mixture of two negative exponential distributions with respectively the origins at the shelter and at the opposite wall this distribution is described by three parameters p proportion of time spend near the shelter being 1 p the proportion of the time spend near the opposite wall λ s h e l t e r the rate of the exponential distribution when the fish is near the shelter and λ o p p the rate of the exponential distribution when the fish is near the opposite wall third d t o t a l was the total distance traveled during 2 h measured as the sum of all between frame displacements finally preliminary histograms of the fish velocity displacements between consecutive frames strongly suggest that empirical speed distribution may be adjusted to a negative exponential distribution which was described by λ s p e e d the rate of the exponential distribution the three exponential rates were inversed 1 rate in order to facilitate interpretation of the parameter in terms of average empirical data was fitted to the distributions either the ad hoc distribution for positions along the longitudinal axis of the tank or the exponential distribution for speed using a bayesian fitting procedure raw data was reduced in order to alleviate temporal autocorrelation 1000 evenly distributed points in the temporal series were selected model data and initial values were prepared using the r package https www r project org from where a mcmc iterator was call jags http mcmc jags sourceforge net in all the cases uninformative flat priors were used and convergence was checked using conventional criteria after appropriate burning and thinning posterior distributions were estimated from three independent mcmcs 1000 valid iterations each the remaining parameters d t o t a l and ϕ were computed directly from the trajectories using an r ad hoc script 3 results the experimental setting has demonstrated to be able to successfully video recording the tank during all the experiment duration the multi tracking algorithm has been tested in four behavioral test of 2 h each in order to validate the strategy three fishes tagged as rr rg and r were continuously tracked during each of this four tests the estimated tracks for each specimen were verified by checking the coherence of transitions between the arena and the shelter and by visual inspection kalman filters in the post processing stage help to fill missing points and contributes to clean tracks furthermore the state model used also provides estimations of x y instantaneous velocity components fig 5 a shows the time sequences of x and y coordinates and the modulus of the instant velocity for one specimen in an entire experiment while 5 b shows the representation of those tracks in the arena the trajectories of the three fish considered in one of the 2 h long test is shown in fig 6 the multi tracking algorithm has successfully used to reconstruct the trajectory of the fish even when moving in a group as suggested by the examples displayed at fig 6 3 1 an overview of the activity patterns detected concerning the behavioral pattern depicted by the trajectories the general patterns and the between and within fish variability of the six parameters distilled from the trajectories are shown in fig 7 in spite that the results refers only to three fish and four 2 h trajectories per day some general patterns can be suggested for instance the individual 1 7 was the individual that spent less time out the shelter but when it is outside the shelter it spent more time near the shelter and with an average position closer to it it also showed the highest average speed overall this metrics seems to suggest a conservative low risk behavior opposite the individual 3 in blue at fig 7 spent more time out the shelter and father away despite showing less average speed than red and similar to green it was the one that travelled more distance so it explored a larger area during the four days 4 conclusions the experimental setting has demonstrated to be able to successfully video record the tank during all the experiment duration furthermore in order to deal with the experimental constrains and avoid storing huge amount of video sequences we present an ad hoc approach performed in two steps the first step is done in real time and detects and transfers frame to frame object positions and other characteristics as their areas towards a computer the information extracted in this step is required to identify the tags and hugely compacts high resolution video images 4k processed at a rate of 10 frames per second into a smallest set of values in long lasting experiments that will be the information preserved and stored the second step consists on processing that information to obtain the final tracks it must not necessarily be done in real time moreover we propose up to six metric for scoring fishes from trajectories based in the use of shelter the distribution fish position in relation to the shelter the total distance traveled and the average speed the preliminary results presented here are a successful proof of concept that the combination of video recording and long lasting trajectory reconstruction can be used for scoring fish behavior and contribute to better understand the behavioral features of fish from long lasting experiments a better understanding of individual behavioural patterns in experimental conditions may help to understand better the observed behavioural patterns in the field e g in comparative studies between areas with different conditions and to figure out what could be the role of behaviour in artificial selection contexts ethics animal care and all experimental procedures were authorized by those responsible for the ethics committee for animal experimentation of the university of the balearic islands ceea uib through a permit ref ceea 60 0916 to the fenofish project ref ctm2015 69126 c2 1 r funded by the spanish ministry of science and competitiveness and were carried out in strict accordance with the recommendations from directive 2010 175 63 ue adhering to 176 spanish law rd53 2013 boe n 34 february 8th 2013 our study did not involve endangered or protected species all efforts were made to minimize fish handling and harm acknowledgments this work has been partially supported by the spanish government projects phenofish with references ctm2015 69126 c2 1 r and ctm2015 69126 2 r and is a contribution of the joint research unit imedea limia a c c was supported by a fpu predoctoral fellowship ref fpu13 01440 from the spanish ministry of education culture and sports mecd the authors thank josé antonio antonio garcía del arco from icm csic for his help in the selection of the ip camera 
26379,the aim of this paper is to describe the state of the art computer based techniques for data analysis to improve operation of wastewater treatment plants a comprehensive review of peer reviewed papers shows that european researchers have led academic computer based method development during the last two decades the most cited techniques are artificial neural networks principal component analysis fuzzy logic clustering independent component analysis and partial least squares regression even though there has been progress on techniques related to the development of environmental decision support systems knowledge discovery and management the research sector is still far from delivering systems that smoothly integrate several types of knowledge and different methods of reasoning several limitations that currently prevent the application of computer based techniques in practice are highlighted keywords data mining data processing data quality wwtp knowledge abbreviations ann artificial neural networks cbr case based reasoning edss environmental decision support systems ffnn feed forward neural net fcm fuzzy c means gk fcm gustafson kessel fuzzy c means ica independent component analysis iwa international water association mlp multi layer perceptron anfis neural fuzzy inferences systems orp oxidation reduction potential pls partial least squares pca principal component analysis pcr principal component regression qta qualitative trend analysis som self organizing maps sbr sequencing batch reactor svm support vector machines svr support vector regression upgma unweighted pair group method with arithmetic mean wwtps wastewater treatment plants wnn wavelet neural nets 1 introduction how do we turn passive data into actionable knowledge or something compelling that improves wastewater treatment operation or supports decision making the aim of this paper is to describe the state of the art computer based techniques for data analysis as applied in the context of wastewater treatment operation this critical review targets method developers mostly within the research community by discussing the evolution of a selection of methods and identifying limitations of method development and selection as well as plant managers and software developers by identifying barriers that limit bringing methods into practice this paper is structured as follows first we briefly define the driving forces within the wastewater treatment field that pushed for the development of computer based techniques for data analysis second we describe the variety of available techniques that enable the transformation of data into information and beyond that into knowledge by means of a review of the techniques applied thus far in wastewater treatment plants wwtps in parallel a critical analysis of the maturity and temporal evolution of each technique is given finally a discussion is provided on the limitations in this field 1 1 driving forces wwtps treat wastewater collected from households and industries before being discharged to a receiving water body wwtps are complex systems which have to maintain high performance at all times despite suffering from hourly daily and seasonal dynamics wwtp operations have the particular feature that any raw material i e wastewater must be accepted while the product i e treated effluent must adhere to its standards at all times furthermore wwtps have to adapt to new challenges posed by the society such as the removal of emerging pollutants the minimization of greenhouse gases emissions etc hadjimichael et al 2016 overall large amounts of data from wwtps are being generated which need to be properly transformed into knowledge for enhancing their operation such knowledge can then be encapsulated into controllers or environmental decision support systems edss that allow maintaining high performance and low emissions at all times during the last two decades several driving forces that have intensified the development of computer based techniques to transform data into knowledge in the wastewater treatment field the first driving force was control implementation to increase the stability of the process ensuring good performance at all times and to optimize the usage of resources e g energy and chemicals control stimulated developments since the early 1970s olsson 2012 on data cleaning selection and transformation which renders the data interpretable and useful for human inspection and automatic feedback control today many sensors such as those used to monitor dissolved oxygen several nutrients suspended solids and organic matter have undergone important transformations rendering them reliable and affordable see vanrolleghem and lee 2003 see manufacturers hach endress hauser s can etc the development of such sensors itself required the usage of data treatment methods e g regression applied to information gathered from uv vis sensors however we realized that the installation of sensors and their maintenance efforts is insufficient to guarantee data quality and hence methods were incorporated to allow for fast detection and diagnosis of faults also we incorporated methods to verify process normalcy and to create useful knowledge concerning plant malfunctioning and how to either improve plant performance or return it to normal operation hence this stimulated the development of methods dealing with mass balances and data reconciliation for basic information extraction control development evolved from unit process control to sophisticated optimization and automation software packages including rule based systems and expert systems åmand et al 2013 ingildsen 2002 the second driving force was the transformation of data graveyards into data mines it is evident that the incorporation of new challenges from aeration control to system wide control and the increased levels of monitoring control and supervision have led to the need for the handling of a large number of signals our current experience suggests that small wwtps 20 000 population equivalents pe can generate up to 500 signals including online and offline signals whereas larger ones 0 8 3 million pe register analogical and digital signals exceeding 30 000 in number olsson et al 2014 freixó 2016 as has been recognized however data rich is all too often equivalent to information poor nopens et al 2007 poynter 2013 indeed vast amounts of data are languishing in databases which are at best described as data graveyards and can certainly not be considered data mines indeed current practice is arranged such that plant operators have an overwhelming stream of data at their hands which is very difficult to process and analyse in a timely enough fashion to allow for better understanding or proper decision making as the effort to analyse data is costly because of a lack of trusted analytic data tools potentially valuable information remains unavailable and unexploited yoo et al 2008 hence methods appeared for advanced information extraction to facilitate the interpretation of large datasets with multiple variables i e multivariate methods such as principal component analysis pca independent component analysis ica and clustering in addition the large amounts of data stimulated the development of black box models such as artificial neural networks ann or support vector machines svm which could be used for process optimization finally other methods appeared for human interpretable information extraction within the field of knowledge discovery rule induction decision trees etc and management ontologies taking advantage of increasing computing capacity innovative knowledge based systems have evolved to make use of both numerical models and heuristic knowledge in tandem with classical and innovative knowledge acquisition techniques in edss the current data rich information poor condition is a general problem that is not unique to the wastewater treatment industry indeed many tools have been developed already and are popular within the chemical processing and paper and pulp sectors wastewater treatment operations are unique however for the following reasons first material inputs i e wastewater 1 cannot be stored in large quantities if the supply exceeds the process capacity e g storm water 2 cannot be discarded and ignored if they are of low quality i e all discharged waters are accounted for in performance evaluations and 3 are characterized by high temporal variability in both volume and quality therefore borrowing methods from other engineering fields is not sufficient to guarantee the successful transformation of data into knowledge the field of wastewater treatment requires specific adaptation of the methods to account for the uniqueness of the wastewater treatment process the iwa international water association instrumentation control and automation conferences in particular have provided an excellent platform for such adaptations a summary is found in olsson 2012 2 literature review 2 1 methodological approach this section presents a review of peer reviewed international journal papers that developed or evaluated techniques applied to wwtps to maximize the potential of generated data and turn it into useful information and knowledge to improve wastewater treatment operation or support decision making the techniques that have been reviewed are organized according to the three following levels table 1 basic information extraction advanced information extraction and human interpretable information extraction note that the levels of complexity time required to compute a solution increase gradually ranging from the first analysing single datasets or variables to the third level considering multiple variables and many types of knowledge within the advanced information extraction level we have grouped the techniques as follows the first group includes techniques based on dimension reduction the second group includes techniques to infer linear functions from labeled training data the third group describes techniques to infer non linear functions the fourth group consists of clustering techniques and the last group includes qualitative feature detection techniques within the human interpretable information extraction level we included rules cbr fuzzy logic edss and ontologies hence we mostly include data mining techniques which come from an interdisciplinary field to discover patterns in large data sets and subsumes both machine learning statistical modelling or visualization disciplines a specific review of machine learning methods applied to the field of water and wastewater can be found in hadjimichael et al 2016 mechanistic methods models were excluded from this review the searches were executed in scopus by including the names of the techniques and the relevant variations plus the term wastewater treatment and limiting the search to papers published until 2015 the search was also limited to papers dealing with conventional activated sludge systems or anaerobic digestion and dealing with conventional compounds nutrients organic matter etc hence excluding papers dealing with emerging contaminants papers published before 2010 and with less than 5 citations were excluded in the supporting material we provide information from all reviewed papers on software platforms used to evaluate the methods technology challenges addressed by the techniques number of variables involved types of data used pilot plant full scale synthetic 2 2 generalities of the review the scopus search after selection for relevance resulted in 340 papers a majority of these papers discuss ann 21 pca 13 and fuzzy logic 12 fig 1 ann has been used in literature for prediction of process performance soft sensing or control table 2 pca has mainly been used for fault detection and process understanding and fuzzy logic has been applied for control and prediction purposes partial least squares pls and multiple linear regression 7 and 4 of the papers respectively have mostly been applied to data obtained from uv vis measurements to estimate water quality parameters e g chemical oxygen demand total suspended solids nitrate etc clustering has been used in 8 of the papers to increase process understanding qualitative features have been applied in 6 of the papers to detect bending points from low cost sensors e g ph orp installed in sequencing batch reactors sbrs or alternating systems to control the length of the aerobic and anoxic phases 9 of the papers have proposed edsss that provide a supervisory level to the controllers or edsss used for wastewater treatment technology selection these edss have mainly been developed using rules or cbr that encapsulate the knowledge gathered from the process rule induction and decision trees less than 1 each have been used for knowledge discovery and ontologies 2 for knowledge management a minority of the studies have applied control chart and mass balances for fault detection or data reconciliation purposes som has mainly been applied to increase process understanding unsupervised dimension reduction and svm for soft sensing and chemometrics eu is the leader region in this field with the largest number of contributions with presence in 61 of the papers followed by asia oceania which contributed to 34 of the papers and north america with a presence in 12 of the studies fig 1 a minority of studies less than 4 have been conducted by south american or african research groups for each of the techniques eu has the largest number of contributions fig 2 still asia oceania region has largely contributed to ann 38 studies fuzzy logic 17 and pca 20 there are also 37 papers 12 which are result of cooperation between research groups from different regions with regards to the academic background of the authors involved in the studies the largest contribution comes from engineers meaning environmental civil water engineers with a participation in 60 of all reviewed papers followed by electrical engineers 10 and computer engineers 3 there exists cooperation between groups of different backgrounds it is worth mentioning the cooperation between engineers environmental civil water and computer engineers 7 of the papers or between engineers and electrical engineers 3 to develop edss including rules and decision trees which are directly related to edss the publication of work related to wastewater and data information and knowledge is spread over 93 journals the journal that has published the largest number of papers in this topic is water sci technol with 21 of the papers published followed by water res and environ model softw with 7 and 5 of the papers respectively other journals with smaller contributions are computers and chemical engineering engineering applications of artificial intelligence chemical engineering journal chemometrics and intelligent laboratory systems journal of hazardous materials journal of environmental management journal of process control bioprocess and biosystems engineering etc 3 details for each of the techniques 3 1 basic information extraction this group contains methods that provide the capacity to extract basic information single variables or gross error detection we include analyses of single variables using univariate control charts and gross error detection through mass balances control charts have been used for monitoring purposes to generate warnings and alarms when drift shift outliers and unsatisfactory calibration curves are detected berthouex et al 1989 thomann et al 2002 rieger et al 2004 schraa et al 2006 thomann 2008 corominas et al 2011 mass balance calculations provide useful information about the validity of operational measurements from wwtps control charts on mass balance errors can be used to analyse time series and to identify systematic errors in full scale wwtp data sets lumley 2002 thomann 2008 spindler and vanrolleghem 2012 the redundancy obtained after conducting mass balances over all possible system boundaries can be used to evaluate systematic errors and to conduct proper data reconciliation meijer et al 2002 puig et al 2008 spindler 2014 a comprehensive approach for data quality evaluation has been provided by rieger et al 2010 simple descriptive statistics also belong to this level as this is how data is managed today in wwtps however we did not find in literature scientific papers dealing exclusively with simple descriptive statistics 3 2 advanced information extraction most of the techniques described in this section were conceived in the area of data science with roots in statistics artificial intelligence or machine learning within this group there are methods focused on the determination of discrete variables e g clustering and on the determination of continuous variables e g pca one can also distinguish between i supervised modelling methods i e methods that aim to predict a variable of interest output on the basis of available measurements inputs e g classification and regression methods and ii unsupervised methods i e methods which are not aimed at characterizing an input output relationship but rather at the analysis of relationships between multiple measurements e g clustering and pca applications of classification methods supervised learning with discrete model outcomes are rather rare a special category of methods focuses on the recognition of qualitative features in a data series such as minima and inflection points these are discussed separately at the end of this section 3 2 1 dimensionality reduction techniques significant efforts have been made in the context of multivariate data analysis in particular to address how the information in multivariate data sets can be condensed into a small yet informative set of variables by and large all approaches do this consist of unsupervised learning methods with continuous data representation usually referred to as scores or latent variables pca is applied most frequently in the context of bacterial community analysis pca is usually deployed in its original exploratory form meaning that its primary use is to visualize the main sources of variations in collected datasets e g esteban et al 1991 victorio et al 1996 zhang et al 2010 pca is used in a similar fashion to analyse the physical chemical composition of wastewaters e g singh et al 2005 santos et al 2009 alternatively pca is frequently used as a way to describe the distribution of data corresponding to normal operational conditions in this case an obtained distribution is used to construct statistics for which theoretical or empirical confidence limits are available and a pca model is used as a normative or confirmatory model for future data samples as such pca can be used for the online detection of anomalous data samples and process behaviour in addition to data visualization e g rosen and olsson 1998 yoo 2003 villez and habermacher 2016 it can also be used as a dimension reduction strategy in subsequent modelling steps for predictive e g choi and park 2001 or fault diagnostic purposes e g villez et al 2008 several strategies have been proposed to address dynamic multi model non normal and nonlinear process characteristics for which pca is known not to be optimal these strategies include alternative latent variable models such as ica e g yoo et al 2004 lee et al 2006a yu 2012 and som e g garcía and gonzález 2004 dürrenmatt and gujer 2011 liukkonen et al 2013 note that som is included here on the basis its use for dimension reduction even though soms show many similarities with the neural networks used for predictive purposes i e supervised learning described below the available basic data models are often improved by means of suitable extensions based on adaptive updating e g lennox and rosén 2002 lee and vanrolleghem 2003 rosén et al 2003 lee et al 2005a data restructuring e g lee and vanrolleghem 2003 lee et al 2004 unfolding e g lee et al 2005a missing data imputation e g rosén et al 2003 mixture modelling e g yoo et al 2007 nonlinear transformation e g lee et al 2004 or wavelet decomposition e g rosen and lennox 2001 lennox and rosén 2002 rosén et al 2003 lee et al 2005a the pca and ica techniques are fairly simple to use as long as the data are close to stationary complications however arise when selecting among the extensions enumerated above there are no guidelines to systematically select among the many possible extensions in addition there is a serious lack of benchmarking for the reported methods in most cases claims of improvements are based on case specific datasets that have not been studied in great detail or that have only been subjected to a few of the available pre processing options the review of soft sensors provided by haimi et al 2013 led to similar conclusions 3 2 2 linear supervised learning techniques principal component regression pcr and pls are methods for regression that are based on a dimensionality reduction of the input space similarly to methods in 3 1 1 pcr and pls are both developed for regression with highly correlated input variables the main difference between pcr and pls is that pcr is based on a two step procedure pca and linear regression whereas pls computes all model elements simultaneously pls was first proposed in the context of chemometrics ricker 1988 its application to environmental processes is well known e g rosen and olsson 1998 choi and lee 2005 lee et al 2006b 2007 ferrer et al 2008 singh et al 2010 the alternative and simpler pcr model has been used frequently e g sundberg 1999 jansson et al 2002 aguado et al 2006 şahin et al 2007 du et al 2012 mašić et al 2015 3 2 3 nonlinear supervised learning techniques for nonlinear classification and regression problems nonlinear supervised learning both ann and svm are commonly used in spite of criticisms in the statistical literature e g hastie et al 2009 anns are still commonly used for nonlinear regression the ann model structure is usually of the multi layer perceptron mlp type i e the feed forward neural net ffnn and includes one hidden layer the use of other structures such as recurrent artificial neural nets has also been reported e g çoruh et al 2014 no recent advances in the identification of neural nets with multiple hidden layers aka deep learning have been reported for wastewater treatment applications applied activation functions are typically of the sigmoid type e g hyperbolic tangent or log sigmoid or bell type e g radial basis functions a few exceptions exist such as the use of the morlet wavelet function leading to so called wavelet neural nets wnn bello et al 2013 and the use of fuzzy membership functions leading to so called adaptive neural fuzzy inferences systems anfis yetilmezsoy et al 2011 the latter type of model is claimed to be more straightforwardly interpretable and useful for optimization as it allows for efficient model inversion there are the studies reporting on the use of such models for wastewater treatment processes e g tay and zhang 1999 huang and wang 1999 perendeci et al 2007 pai et al 2009a b erdirencelebi and yalpir 2011 ay and kisi 2014 support vector regression svr is also used for nonlinear regression purposes although it is usually referred to as svm except for a few select cases e g xie et al 2011 fang et al 2011 in a smaller number of cases it is also used for nonlinear classification e g zhang 2008 for both classification and regression conventional methods of svm and svr optimization and the use of gaussian or polynomial kernel functions are studied in most studies the intended use of the resulting nonlinear models is to enable the estimation or prediction of key performance indicators which are difficult or impossible to measure online which exact indicators are being predicted depends on the exact application often this provides assistance to process monitoring process optimization or process control e g hochedlinger et al 2006 lee et al 2006b pendashteh et al 2011 xie et al 2011 yetilmezsoy et al 2011 lin et al 2012 yu et al 2013 zhang et al 2012 soleimani et al 2013 in a few cases however the exact purpose of the resulting model based predictions is not clear e g wei et al 2013 3 2 4 clustering techniques by definition clustering methods are unsupervised learning methods that group data samples into a discrete number of subsets that are similar to each other in the context of microbial community analysis clustering methods are popular for relating the bacterial community structures to sampling locations or macroscopic process variables this is almost exclusively based on agglomerative aka bottom up hierarchical clustering methods such as ward s method or the unweighted pair group method with arithmetic mean upgma e g wittebolle et al 2005 he et al 2006 tan et al 2008 akarsubasi et al 2009 zhang et al 2010 clustering of process data is used primarily to understand wwtp operations and to construct diagnostic tools in contrast to the clustering applications in microbial community analysis process data are almost never analysed by means of hierarchical clustering methods instead methods based on centroids are typically used popularly used algorithms include k means clustering e g hong et al 2003 garcía and gonzález 2004 grieu et al 2005 aguado and rosen 2008 and fuzzy clustering methods e g fuzzy c means fcm or gustafson kessel fuzyy c means gk fcm clustering müller et al 1997 teppola et al 1998 1999 yoo 2003 rosén and yuan 2001 aguado et al 2008 dovžan and škrjanc 2011a b specialized algorithms to address discrete and continuous data simultaneously villez et al 2008 and to facilitate interpretability gibert et al 2010 have been proposed as well but have gained little traction thus far a comparison of several clustering methods has been conducted in gibert et al 2005 3 2 5 qualitative feature detection techniques a special data interpretation problem often arises when plant operators can visually assess process states and performance on the basis of qualitative features such as maxima minima or inflection points in time series data however the automated identification of such qualitative features can be a challenging problem in the presence of time varying process properties spatial variability e g plant to plant and lane to lane variations and measurement errors e g noise in the existing literature the sought after qualitative features are described by a wide range of terms including terms that describe patterns i by means of purely visual cues e g minimum inflection point bending point bend point ii by reference to chemical equilibrium reactions e g breakpoint in the case of oxidation reduction potential orp measurements iii by process state descriptions e g depletion iv by conversion rate or reaction rate descriptions e g end of reaction phase endogenous exogenous respiration reaction endpoint starting point treatment stage v by means of process specific terminology ammonia valley nitrate apex nitrate knee or vi by more general terms change point characteristic point control point signal level event given this diversity in terminology it is difficult to guarantee that the following overview of available methods is complete most of the currently available studies focus on detecting qualitative features in time series of cyclic processes such as in sbrs and alternating intermittent aerated system aias for wastewater treatment the appearance of qualitative features usually corresponds to the end of a desired reaction or conversion e g ammonia valley online detection of such features can then be used for the online adjustment of a wastewater treatment process in many cases an ad hoc solution can be provided on the basis of fairly simple schemes based on filtering differentiation descriptive statistics and or thresholds for univariate signals e g vanrolleghem and coen 1995 puig et al 2005 casellas et al 2006 corominas et al 2006 guisasola et al 2006 irizar et al 2008 vargas and buitrón 2008 ga and ra 2009 won and ra 2011 antileo et al 2013 unfortunately the fine tuning of these methods is seldom described in detail and may lead to case specific solutions in addition these schemes cannot take the multivariate and nonlinear natures of measurement signals into account explicitly to address this several methods have been proposed on the basis of anns e g lee et al 2005b bisschops et al 2006 luccarini et al 2010 fuzzy logic marsili libelli et al 2008 claros et al 2012 multivariate statistics e g villez et al 2010 and unsupervised learning methods e g garcía and gonzález 2004 the main tenet of these methods is that targeted qualitative features are described by means of quantitative properties while relatively popular these methods are not inherently robust in terms of the spatial and temporal variability of biological processes an interesting alternative is based on methods of qualitative trend analysis qta e g puig et al 2006 villez et al 2008 this group of methods converts time series directly into qualitative representations and thereby promises better generalization properties such as the capacity for extrapolation similar methods have also been more recently proposed for sensor diagnosis e g villez et al 2012 fault detection e g villez and habermacher 2015 sludge settleability characterization derlon et al 2017 and continuous flow system automation thürlimann et al 2017 3 3 human interpretable information extraction many tools for reconciliation detection diagnosis are based on statistical theory and lead to straightforward statistical limits and decision boundaries however such methods can often be outperformed by human experts therefore expert knowledge is often valuable to obtain finer tuning of decision boundaries human experts show the ability to reason through process operations without assuming accurate models human interaction and process knowledge is thus important to refine the definitions of normal data behaviour throughout the information extraction process all computed variables including statistics and performance indices as well as the associated decisions require some degree of expert knowledge rule and case based decision trees and cbr techniques are commonly used for collecting and representing expert knowledge which can be framed in more complex and functional structures such as in edss decision support systems may include specific reasoning methods that are focused on providing knowledge regarding organizational domain specific structures and on integrating different techniques and are globally intended to mimic domain expert reasoning 3 3 1 rule induction and decision trees rule induction and decision trees are important areas of machine learning in which formal rules are extracted from a set of observations in the context of wastewater treatment the main objectives of induction methods are to clarify data mining algorithms for all treatment processes and to create an understanding of all rules that are obtained from the collected data certain classifier models are particularly interesting as they generate models in the form of if then rules which are more expressive and easier for humans to comprehend and validate the subsequent use of the resulting decision trees is often seen as a good strategy for further improving the predictive capabilities of unseen data in treatment plants e g kusiak et al 2013 verma et al 2013 decision trees can also be used for problem solving situations e g ma et al 2009 modelling e g dürrenmatt and gujer 2011 flores alsina et al 2009 or even as a complementary tool for decision making processes e g comas et al 2001 comas et al 2003 evenson and baetz 1994 freitas et al 2000 ma et al 2009 r roda et al 2001 3 3 2 case based reasoning in regard to capturing and especially to drawing conclusions from past experiences cbr appears to be a powerful method cbr is a flexible paradigm that supports the implementation of a dynamic learning environment information on how problems were solved in the past and identification of the reasons that caused such problems has been demonstrated to be useful for supervisory systems with operation purposes e g rodriguez roda et al 2002 sànchez marrè et al 1997 as well as for prediction purposes e g wiese et al 2005 comas et al 2006 martínez et al 2006 and automatic learning e g krovvidy et al 1991 krovvidy and wee 1993 wiese et al 2005 cbr performance can be improved continuously by adding results from new experiences however a weak point of cbr is its incapacity to cope with future situations thus a cbr system is a not suitable tool for predictions and it may require reliance on mathematical models or neural networks for this purpose 3 3 3 fuzzy logic fuzzy logic is not a substitute for statistics but it can be used when statistical reasoning is inappropriate allowing the meshing of precise approaches with inexact approximate representations within the wastewater treatment context fuzzy logic has traditionally been applied to improve process control e g tsai et al 1993 marsili libelli and müller 1996 müller et al 1997 manesis et al 1998 meyer and pöpel 2003 puñal et al 2003 carrasco et al 2004 marsili libelli 2006 puig et al 2006 waewsak et al 2010 belchior et al 2012 examples of the predictive capabilities of fuzzy logic are also abundant in the literature e g tsai et al 1993 marsili libelli and giunti 2002 carlos hernandez et al 2009 civelekoglu et al 2009 turkdogan aydinol and yetilmezsoy 2010 angulo et al 2012 surprisingly fuzzy logic is frequently used to address the uncertain economics that surround wwtp costs e g fiter et al 2005 baroni et al 2006 other potential uses although less popular and more diverse include process optimization e g du et al 1999 bongards 2001 and automated decision making e g ling and hang 1998 ebner et al 2010 moon et al 2011 given the number of citations and related papers fuzzy logic is clearly one of the most popular techniques used today fig 2 and one of the few techniques that have been validated in full scale by many users e g manesis et al 1998 bongards 2001 3 3 4 ontologies ontologies try to play the role of common language among agents 1 1 an agent is anything that is capable of acting upon information it perceives from its environment an intelligent agent is an agent capable of making decisions about how it acts based on experience an autonomous intelligent agent is an intelligent agent that is free to choose between different actions gómez sebastià et al 2017 trying to solve problems but they are still far from delivering systems that smoothly integrate several types of knowledge and different methods of reasoning for wwtp problem solving after several seminal ideas were presented regarding the application of ontologies to wastewater treatment e g boo and aguilar 1998 cabezut boo et al 1999 only a few applied attempts have been put into practice mainly for instrumentation control and automation e g chau 2007 scholten 2008 the use of ontologies in agent baset simulations is a new research topic that opens the field to integrate the use of intelligent agents rational making decisions that will affect the status of the wwtp and the river along the simulation oliva felipe et al 2017 3 3 5 environmental decision support systems edss focus on integrating different techniques and methods into unified structures to both use and reuse knowledge e g via coupled models databases and assessment tools or to generate or expand knowledge rule induction data driven techniques self learning etc to support decision making reducing the time in which decisions are made and improving the consistency of decisions edss have been used widely for wastewater treatment having a high level of popularity as academic exercises although little validation e g fedra and winkelbauer 2002 maturana et al 2004 mcintosh et al 2011 and few full scale implementations have been reported in practice poch et al 2004 2017 garrido baserba et al 2014 how a particular edss is built depends on the type of problem available data and types of information and knowledge that can be acquired depending on the objective which can range from monitoring and data storage to prediction decision analyses as well as communication with society these unifying structures may result in or be the product of a combination of advanced control methods e g rodriguez roda et al 2002 comas et al 2004 2006 martínez et al 2006 or knowledge related techniques e g krovvidy et al 1994 comas et al 2003 poch et al 2004 zhu and mcbean 2007 garrido baserba et al 2012 despite the capabilities of each edss which depends on the implemented techniques or a combination of them the main attraction of edss is their user friendly interfaces which are intended to present results in a readily understandable form that should facilitate users to confront complex wwtp related decision making problems therefore success depends in part on how easily the information is presented to end users shim et al 2002 lardon et al 2005 matthies et al 2007 mcintosh et al 2011 4 method trends and popularity of the techniques 4 1 trends figure 3 shows the sum of the citations per year for the reviewed papers separately for each technique for some methods we observe a steadily increase of citations along the years ann and pca are the methods that generate more citations per year more than 200 after 2010 followed by fuzzy logic clustering ica pls with around 100 citations after 2010 wastewater treatment process improvements due to the application of this plethora of techniques may have been a driving force behind the steadily increasing interest in the field between 50 and 100 citations per year receive the papers published on som regression svm and qualitative features the methods that receive less than 50 citations per year are control charts and mass balance due to the fact that a limited number of papers on these topics are published per year one paper per year in the best case in addition the papers on knowledge generation and management rules ontologies which receive less than 20 citations per year have retained the same level of interest through the years either because they are highly specific to solving only select problems or lack novelty or because their potential cannot be efficiently exploited e g ontologies cbr there is a slight increase in the application of ontologies but far less popular than in other sectors 4 2 popularity of papers among scientists we calculated the average citation rate for each year i e the ratio between number of citations and number of papers eq 1 which gives an indication on how influential a paper has been within academia 1 c i t a t i o n r a t e y e a r 1983 y e a r c i t a t i o n s 1983 y e a r p a p e r s the most influential method has been ica with a citation rate of 63 in 2015 fig 4 meaning that each new paper published receives on average 63 citations followed by svm with a rate of 51 pca and cbr obtain a citation rate of 38 most of the other techniques lie on the range between 20 and 40 except for control chart and mass balance that result in a citation rate smaller than 20 we hypothesize that newer methods are cited more because they are new in particular control chart and mass balance may be considered general knowledge and therefore not cited as much 4 3 academic expectations based on the information extracted from the trends and the popularity we subjectively represented the academic expectations within the research field of wastewater treatment of all reviewed techniques in the form of a gartner hype cycle graphic fig 5 the analysis comes from purely academic sources and might not correspond to maturity levels of these techniques as seen in the industrial sector still many industrial partners contributed to peer reviewed journal papers and their vision is partially captured in the academic literature we established some rules to assign the techniques to each of the levels of the gartner hype cycle if the technique has a large number of citations and the citations trend reaches a plateau we consider the method is at the plateau of productivity if there is clear evidence that the method is useful works well widely adopted by research community we consider it to the at plateau of productivity e g mass balancing and control charts and simple statistics if the number of citations is high but does not reach a plateau still showing a positive trend we believe the technique still has potential and hence we consider the technique at the enlightenment e g clustering techniques showing smaller number of citations but there is evidence that they evolved are also considered in the enlightenment if the number of citations decreases over time we consider the technique to be at the trough of disillusionment if the number of citations does not reach a plateau but the citation rate is high we consider that the technique is at the peak of inflated expectation if the number of papers is rather small e g 12 papers using som and 8 papers using ontologies but the citation rate is relatively high e g close to 20 we consider the technique is at the trigger at the plateau of productivity we find ann including anfis and fuzzy logic are the well established methods in the research field with not only many research applications but also with several examples of real world implementation and transformations into products ready to be exported to the industrial sector e g ferrer et al 1998 manesis et al 1998 wen and vassiliadis 1998 bongards 2001 fiter et al 2005 lee et al 2008 mingzhi et al 2009 pca pls and regression also belong to the plateau of productivity even if they received less citations and got a lower citation rate compared to ann and fuzzy logic pls has been widely used in academia with successful results in chemometrics being essential for the development of commercial products used to monitor water quality variables e g spectrophotometric sensor from s can pca coupled to clustering has also been brought to practice e g lee et al 2008 martín de la vega et al 2012 finally mass balancing and control charts also belong the plateau of productivity as they are widely adopted by research community e g mass balancing for the design of wwtps as in flores alsina et al 2012 at the enlightenment we find qualitative features detection which has shown large benefits on improving process performance with low investment costs and many real world applications are found in literature e g yu et al 1997 yu 2004 gonzález and garcía 2006 zeng et al 2008 won and ra 2011 with regard to edss there have been many successful academic experiences e g baroni et al 2006 garrido baserba et al 2012 monclús et al 2012 and the current limitation relates to crossing the death valley towards a general use implementation of edss by society the market poch et al 2017 research around rules e g decision trees are also in the enlightenment phase as they are evolving together with edss even though cbr reaches a plateau in the number of citations the absolute number of citations is rather low although the concept of learning and improving from past experiences proved to work and should be of the upmost importance in any complex and multi objective project the integration of cbr with other tools or self organizing capabilities may still be required to overcome the current situation and move towards the plateau of productivity we did not found any technique belonging to the trough of disillusionment we can observe in figs 3 and 4 that fuzzy logic showed a decrease in the number of citations per year in the 90s and a decrease in the citation rate in 1995 we could state that fuzzy logic was in the trough of disillusionment in the late 90s but not nowadays at the peak of inflated expectation we find ica that was incorporated into the field after 2005 and experienced a rapid increase of interest and large number of citations similarly due to the increase in computational capabilities svm and deep learning will likely come to fruition in the near future ontologies and som are methods which are not mature yet in the trigger phase without proven practical experience in the real world but with large potential as demonstrated in other fields as is the case in natural language translation recommending systems normative systems etc 5 discussion 5 1 bringing the techniques into practice about 16 of the publications included in the review provided evidences that a commercial product resulted out of the research efforts still only 9 of the total publications clearly stated that these products were validated at full scale e g controller running at real time the remaining 7 represent products which have not been tested under full scale real conditions and hence are less mature the other publications 84 of the papers remain as an academic exercise even if full scale data were used which is the case for 46 of the papers about 25 of these academic exercises compared several methods to serve the same purpose other academic exercises discussed improvements or adaptations in classical methods to accommodate to particularities of wwtps we believe that those methods that rise in popularity in academia are often gaining interest for commercial implementation software development companies play a crucial role in here to enhance usability of the techniques e g through dedicated workflows and user friendly interfaces even though many companies offer wastewater data management software only a limited subset claim their focus is on data quality e g monitoring control platform from inctrl solutions canada or ritune from rittmeyer ag switzerland still it is difficult to keep track on which methods made it into practice as this is outside the scope of scientific literature and would require a targeted search through commercial products in our experience it is essential to guarantee a high level of end user participation e g wwtp operators to ensure the transfer of a method into practice examples of successful techniques in practice are decision trees and fuzzy logic such end user participation results in a much higher level of confidence and guarantees the continued use and tuning of the techniques furthermore most statistical techniques have been adopted by academics who have limited practical experience mostly using toy or benchmark examples today introducing such advanced techniques has proven difficult because practitioners do not participate in learning during their development this has major consequences for the future and we must learn from such past errors hence practitioners should either be educated on current existing methods or wwtps should contract services from well trained consultants similarly the research community should explore alternative data analysis methods that are more intuitive to operators to ensure their transfer into practice for example qualitative reasoning and analysis is promising but has been applied in only a limited context within the wastewater treatment community another problem in academia is that technology transfer from research results to implementation is seldom rewarded adequately 5 2 what are the current limitations of method development method validation is an extremely important step but it is especially challenging because of the difficulties to obtain high quality data this is in part because the installation of sensors and their maintenance efforts is insufficient to guarantee data quality in addition wastewater processes are characterized by changes at many different time scales spanning from seconds ph dynamics to years plant lay out changes and construction periods as such obtaining datasets that include all of the relevant dynamics over meaningful periods of time and with sufficiently high quality is not easy hence it would be of great value to create a repository of data sets that can be used for validation and comparison purposes in fact only 7 of the reviewed papers used data from repositories another limitation that we identified is the lack of an association between the fields of engineering and statistics the major cooperation between fields has been between environmental chemical civil engineering and computer engineering including the ai experts mainly for the development of edss 7 of the papers and environmental chemical civil engineering and electrical engineering 3 there is a need for more in depth interactions between engineers and other disciplines when developing methods to guarantee their successful application the optimal case would include researchers practitioners who are trained not only in engineering but also in statistics computer science etc through the creation of multidisciplinary training programmes we also feel that there is limited guidance in method development there are many different implementations for each of the methods but defining what constitutes a good implementation for a solution remains unclear good practices for data mining and information extraction methods should be defined at least for those methods that have been established for decades the handbook on good modelling practice for identification of mechanistic wwtp models can be used as an example rieger et al 2012 the knowledge management discipline is the one we identified has large potential but has been understudied in the field of wastewater treatment ontologies are the natural way to integrate knowledge from different sources and to share it among different actors ontologies serve to deliver systems that smoothly integrate several types of knowledge and different methods of reasoning however faster computing facilities cheaper memory storage and more reliable sensors augment the pace of development ontologies are already a consumable in the internet of things environment were wwtps make up just one kind of elements next generation edss must be able to capture and integrate lessons learned to organize memory and to share high value pieces of knowledge in such systems ontologies should be able to provide the appropriate means and terminology for tagging content and mapping to databases repositories and knowledge bases they should provide a framework for cataloguing identifying human expertise where it exists as a mechanism for capturing tacit and expert knowledge 5 3 what are the current limitations of method selection finally there is no sufficient guidance for method selection which makes it difficult for practitioners to select the best method to address a particular issue several methods can serve a similar purpose or a single method can show variations in its implementation this makes an effective comparison hard additionally different techniques highlight different data features or patterns such that comparison may be meaningless in such cases in fact 21 of the reviewed papers compared the proposed methods against other existing ones and highlighted the advantages as referred to these still the comparison is partial generally with a small subset of techniques on average 2 techniques are compared which means that a transparent comparison of a large number of methods providing the same function is needed to provide guidance on method selection overall there is a lack of standardized protocols for the choosing and implementation of data analytic techniques even for academics the effort to characterize and map the field has proven to be challenging for this reason we are of the opinion that there is more of a need than ever to develop guidelines and manuals rather than new techniques of special relevance is the existence of the bsm1 lt platform gernaey et al 2014 this is a plant wide simulation model of a standard wwtp which was constructed to enable the effective comparison of process monitoring diagnosis and control techniques this platform also allows facilitates cost benefit analyses of specialized process monitoring diagnosis and control algorithms in fact about 9 of the studies already used that platform for method development and comparison furthermore researchers should take profit of the fact that at least 25 of the papers published used matlab for method implementation and that percentage could be even higher as 38 of the papers did not specify the software platform used establishing a community that shares method implementation at least in matlab or through github would effectively contribute to effective comparison of methods and come up with proper guidance for method selection 6 conclusions the historical evolution of signal processing from data to information and knowledge management from information to knowledge shows that different techniques have been developed implemented at different levels of management from simple control loops to environmental decision support systems the increased number of potentially conflicting objectives that have been established for wastewater treatment systems from simple dissolved oxygen control at the wwtp to system wide control demands the adoption of more advanced data management techniques in turn requiring a higher level of specialist education basic and advanced statistics artificial intelligence machine learning the review of techniques shows that even though a plethora of methods have been developed tested there are still challenges to be addressed related to the objective comparison of the techniques the lack of guidelines the limited validation at full scale the lack of collaborative methods and the limited options for active optimization of data information content and quality in addition more efforts are needed to develop implement knowledge generation management and application techniques acknowledgements the authors acknowledge funding for the reach project ctm2015 66892 r mineco feder ue and from the spanish ministry for the ryc 2013 14595 also we acknowledge the economy and knowledge department of the catalan government consolidated research group 2014 sgr 291 icra and the 2014 sgr 1168 lequia prof cortés is a member of the sistema nacional de investigadores sni conacyt 
26379,the aim of this paper is to describe the state of the art computer based techniques for data analysis to improve operation of wastewater treatment plants a comprehensive review of peer reviewed papers shows that european researchers have led academic computer based method development during the last two decades the most cited techniques are artificial neural networks principal component analysis fuzzy logic clustering independent component analysis and partial least squares regression even though there has been progress on techniques related to the development of environmental decision support systems knowledge discovery and management the research sector is still far from delivering systems that smoothly integrate several types of knowledge and different methods of reasoning several limitations that currently prevent the application of computer based techniques in practice are highlighted keywords data mining data processing data quality wwtp knowledge abbreviations ann artificial neural networks cbr case based reasoning edss environmental decision support systems ffnn feed forward neural net fcm fuzzy c means gk fcm gustafson kessel fuzzy c means ica independent component analysis iwa international water association mlp multi layer perceptron anfis neural fuzzy inferences systems orp oxidation reduction potential pls partial least squares pca principal component analysis pcr principal component regression qta qualitative trend analysis som self organizing maps sbr sequencing batch reactor svm support vector machines svr support vector regression upgma unweighted pair group method with arithmetic mean wwtps wastewater treatment plants wnn wavelet neural nets 1 introduction how do we turn passive data into actionable knowledge or something compelling that improves wastewater treatment operation or supports decision making the aim of this paper is to describe the state of the art computer based techniques for data analysis as applied in the context of wastewater treatment operation this critical review targets method developers mostly within the research community by discussing the evolution of a selection of methods and identifying limitations of method development and selection as well as plant managers and software developers by identifying barriers that limit bringing methods into practice this paper is structured as follows first we briefly define the driving forces within the wastewater treatment field that pushed for the development of computer based techniques for data analysis second we describe the variety of available techniques that enable the transformation of data into information and beyond that into knowledge by means of a review of the techniques applied thus far in wastewater treatment plants wwtps in parallel a critical analysis of the maturity and temporal evolution of each technique is given finally a discussion is provided on the limitations in this field 1 1 driving forces wwtps treat wastewater collected from households and industries before being discharged to a receiving water body wwtps are complex systems which have to maintain high performance at all times despite suffering from hourly daily and seasonal dynamics wwtp operations have the particular feature that any raw material i e wastewater must be accepted while the product i e treated effluent must adhere to its standards at all times furthermore wwtps have to adapt to new challenges posed by the society such as the removal of emerging pollutants the minimization of greenhouse gases emissions etc hadjimichael et al 2016 overall large amounts of data from wwtps are being generated which need to be properly transformed into knowledge for enhancing their operation such knowledge can then be encapsulated into controllers or environmental decision support systems edss that allow maintaining high performance and low emissions at all times during the last two decades several driving forces that have intensified the development of computer based techniques to transform data into knowledge in the wastewater treatment field the first driving force was control implementation to increase the stability of the process ensuring good performance at all times and to optimize the usage of resources e g energy and chemicals control stimulated developments since the early 1970s olsson 2012 on data cleaning selection and transformation which renders the data interpretable and useful for human inspection and automatic feedback control today many sensors such as those used to monitor dissolved oxygen several nutrients suspended solids and organic matter have undergone important transformations rendering them reliable and affordable see vanrolleghem and lee 2003 see manufacturers hach endress hauser s can etc the development of such sensors itself required the usage of data treatment methods e g regression applied to information gathered from uv vis sensors however we realized that the installation of sensors and their maintenance efforts is insufficient to guarantee data quality and hence methods were incorporated to allow for fast detection and diagnosis of faults also we incorporated methods to verify process normalcy and to create useful knowledge concerning plant malfunctioning and how to either improve plant performance or return it to normal operation hence this stimulated the development of methods dealing with mass balances and data reconciliation for basic information extraction control development evolved from unit process control to sophisticated optimization and automation software packages including rule based systems and expert systems åmand et al 2013 ingildsen 2002 the second driving force was the transformation of data graveyards into data mines it is evident that the incorporation of new challenges from aeration control to system wide control and the increased levels of monitoring control and supervision have led to the need for the handling of a large number of signals our current experience suggests that small wwtps 20 000 population equivalents pe can generate up to 500 signals including online and offline signals whereas larger ones 0 8 3 million pe register analogical and digital signals exceeding 30 000 in number olsson et al 2014 freixó 2016 as has been recognized however data rich is all too often equivalent to information poor nopens et al 2007 poynter 2013 indeed vast amounts of data are languishing in databases which are at best described as data graveyards and can certainly not be considered data mines indeed current practice is arranged such that plant operators have an overwhelming stream of data at their hands which is very difficult to process and analyse in a timely enough fashion to allow for better understanding or proper decision making as the effort to analyse data is costly because of a lack of trusted analytic data tools potentially valuable information remains unavailable and unexploited yoo et al 2008 hence methods appeared for advanced information extraction to facilitate the interpretation of large datasets with multiple variables i e multivariate methods such as principal component analysis pca independent component analysis ica and clustering in addition the large amounts of data stimulated the development of black box models such as artificial neural networks ann or support vector machines svm which could be used for process optimization finally other methods appeared for human interpretable information extraction within the field of knowledge discovery rule induction decision trees etc and management ontologies taking advantage of increasing computing capacity innovative knowledge based systems have evolved to make use of both numerical models and heuristic knowledge in tandem with classical and innovative knowledge acquisition techniques in edss the current data rich information poor condition is a general problem that is not unique to the wastewater treatment industry indeed many tools have been developed already and are popular within the chemical processing and paper and pulp sectors wastewater treatment operations are unique however for the following reasons first material inputs i e wastewater 1 cannot be stored in large quantities if the supply exceeds the process capacity e g storm water 2 cannot be discarded and ignored if they are of low quality i e all discharged waters are accounted for in performance evaluations and 3 are characterized by high temporal variability in both volume and quality therefore borrowing methods from other engineering fields is not sufficient to guarantee the successful transformation of data into knowledge the field of wastewater treatment requires specific adaptation of the methods to account for the uniqueness of the wastewater treatment process the iwa international water association instrumentation control and automation conferences in particular have provided an excellent platform for such adaptations a summary is found in olsson 2012 2 literature review 2 1 methodological approach this section presents a review of peer reviewed international journal papers that developed or evaluated techniques applied to wwtps to maximize the potential of generated data and turn it into useful information and knowledge to improve wastewater treatment operation or support decision making the techniques that have been reviewed are organized according to the three following levels table 1 basic information extraction advanced information extraction and human interpretable information extraction note that the levels of complexity time required to compute a solution increase gradually ranging from the first analysing single datasets or variables to the third level considering multiple variables and many types of knowledge within the advanced information extraction level we have grouped the techniques as follows the first group includes techniques based on dimension reduction the second group includes techniques to infer linear functions from labeled training data the third group describes techniques to infer non linear functions the fourth group consists of clustering techniques and the last group includes qualitative feature detection techniques within the human interpretable information extraction level we included rules cbr fuzzy logic edss and ontologies hence we mostly include data mining techniques which come from an interdisciplinary field to discover patterns in large data sets and subsumes both machine learning statistical modelling or visualization disciplines a specific review of machine learning methods applied to the field of water and wastewater can be found in hadjimichael et al 2016 mechanistic methods models were excluded from this review the searches were executed in scopus by including the names of the techniques and the relevant variations plus the term wastewater treatment and limiting the search to papers published until 2015 the search was also limited to papers dealing with conventional activated sludge systems or anaerobic digestion and dealing with conventional compounds nutrients organic matter etc hence excluding papers dealing with emerging contaminants papers published before 2010 and with less than 5 citations were excluded in the supporting material we provide information from all reviewed papers on software platforms used to evaluate the methods technology challenges addressed by the techniques number of variables involved types of data used pilot plant full scale synthetic 2 2 generalities of the review the scopus search after selection for relevance resulted in 340 papers a majority of these papers discuss ann 21 pca 13 and fuzzy logic 12 fig 1 ann has been used in literature for prediction of process performance soft sensing or control table 2 pca has mainly been used for fault detection and process understanding and fuzzy logic has been applied for control and prediction purposes partial least squares pls and multiple linear regression 7 and 4 of the papers respectively have mostly been applied to data obtained from uv vis measurements to estimate water quality parameters e g chemical oxygen demand total suspended solids nitrate etc clustering has been used in 8 of the papers to increase process understanding qualitative features have been applied in 6 of the papers to detect bending points from low cost sensors e g ph orp installed in sequencing batch reactors sbrs or alternating systems to control the length of the aerobic and anoxic phases 9 of the papers have proposed edsss that provide a supervisory level to the controllers or edsss used for wastewater treatment technology selection these edss have mainly been developed using rules or cbr that encapsulate the knowledge gathered from the process rule induction and decision trees less than 1 each have been used for knowledge discovery and ontologies 2 for knowledge management a minority of the studies have applied control chart and mass balances for fault detection or data reconciliation purposes som has mainly been applied to increase process understanding unsupervised dimension reduction and svm for soft sensing and chemometrics eu is the leader region in this field with the largest number of contributions with presence in 61 of the papers followed by asia oceania which contributed to 34 of the papers and north america with a presence in 12 of the studies fig 1 a minority of studies less than 4 have been conducted by south american or african research groups for each of the techniques eu has the largest number of contributions fig 2 still asia oceania region has largely contributed to ann 38 studies fuzzy logic 17 and pca 20 there are also 37 papers 12 which are result of cooperation between research groups from different regions with regards to the academic background of the authors involved in the studies the largest contribution comes from engineers meaning environmental civil water engineers with a participation in 60 of all reviewed papers followed by electrical engineers 10 and computer engineers 3 there exists cooperation between groups of different backgrounds it is worth mentioning the cooperation between engineers environmental civil water and computer engineers 7 of the papers or between engineers and electrical engineers 3 to develop edss including rules and decision trees which are directly related to edss the publication of work related to wastewater and data information and knowledge is spread over 93 journals the journal that has published the largest number of papers in this topic is water sci technol with 21 of the papers published followed by water res and environ model softw with 7 and 5 of the papers respectively other journals with smaller contributions are computers and chemical engineering engineering applications of artificial intelligence chemical engineering journal chemometrics and intelligent laboratory systems journal of hazardous materials journal of environmental management journal of process control bioprocess and biosystems engineering etc 3 details for each of the techniques 3 1 basic information extraction this group contains methods that provide the capacity to extract basic information single variables or gross error detection we include analyses of single variables using univariate control charts and gross error detection through mass balances control charts have been used for monitoring purposes to generate warnings and alarms when drift shift outliers and unsatisfactory calibration curves are detected berthouex et al 1989 thomann et al 2002 rieger et al 2004 schraa et al 2006 thomann 2008 corominas et al 2011 mass balance calculations provide useful information about the validity of operational measurements from wwtps control charts on mass balance errors can be used to analyse time series and to identify systematic errors in full scale wwtp data sets lumley 2002 thomann 2008 spindler and vanrolleghem 2012 the redundancy obtained after conducting mass balances over all possible system boundaries can be used to evaluate systematic errors and to conduct proper data reconciliation meijer et al 2002 puig et al 2008 spindler 2014 a comprehensive approach for data quality evaluation has been provided by rieger et al 2010 simple descriptive statistics also belong to this level as this is how data is managed today in wwtps however we did not find in literature scientific papers dealing exclusively with simple descriptive statistics 3 2 advanced information extraction most of the techniques described in this section were conceived in the area of data science with roots in statistics artificial intelligence or machine learning within this group there are methods focused on the determination of discrete variables e g clustering and on the determination of continuous variables e g pca one can also distinguish between i supervised modelling methods i e methods that aim to predict a variable of interest output on the basis of available measurements inputs e g classification and regression methods and ii unsupervised methods i e methods which are not aimed at characterizing an input output relationship but rather at the analysis of relationships between multiple measurements e g clustering and pca applications of classification methods supervised learning with discrete model outcomes are rather rare a special category of methods focuses on the recognition of qualitative features in a data series such as minima and inflection points these are discussed separately at the end of this section 3 2 1 dimensionality reduction techniques significant efforts have been made in the context of multivariate data analysis in particular to address how the information in multivariate data sets can be condensed into a small yet informative set of variables by and large all approaches do this consist of unsupervised learning methods with continuous data representation usually referred to as scores or latent variables pca is applied most frequently in the context of bacterial community analysis pca is usually deployed in its original exploratory form meaning that its primary use is to visualize the main sources of variations in collected datasets e g esteban et al 1991 victorio et al 1996 zhang et al 2010 pca is used in a similar fashion to analyse the physical chemical composition of wastewaters e g singh et al 2005 santos et al 2009 alternatively pca is frequently used as a way to describe the distribution of data corresponding to normal operational conditions in this case an obtained distribution is used to construct statistics for which theoretical or empirical confidence limits are available and a pca model is used as a normative or confirmatory model for future data samples as such pca can be used for the online detection of anomalous data samples and process behaviour in addition to data visualization e g rosen and olsson 1998 yoo 2003 villez and habermacher 2016 it can also be used as a dimension reduction strategy in subsequent modelling steps for predictive e g choi and park 2001 or fault diagnostic purposes e g villez et al 2008 several strategies have been proposed to address dynamic multi model non normal and nonlinear process characteristics for which pca is known not to be optimal these strategies include alternative latent variable models such as ica e g yoo et al 2004 lee et al 2006a yu 2012 and som e g garcía and gonzález 2004 dürrenmatt and gujer 2011 liukkonen et al 2013 note that som is included here on the basis its use for dimension reduction even though soms show many similarities with the neural networks used for predictive purposes i e supervised learning described below the available basic data models are often improved by means of suitable extensions based on adaptive updating e g lennox and rosén 2002 lee and vanrolleghem 2003 rosén et al 2003 lee et al 2005a data restructuring e g lee and vanrolleghem 2003 lee et al 2004 unfolding e g lee et al 2005a missing data imputation e g rosén et al 2003 mixture modelling e g yoo et al 2007 nonlinear transformation e g lee et al 2004 or wavelet decomposition e g rosen and lennox 2001 lennox and rosén 2002 rosén et al 2003 lee et al 2005a the pca and ica techniques are fairly simple to use as long as the data are close to stationary complications however arise when selecting among the extensions enumerated above there are no guidelines to systematically select among the many possible extensions in addition there is a serious lack of benchmarking for the reported methods in most cases claims of improvements are based on case specific datasets that have not been studied in great detail or that have only been subjected to a few of the available pre processing options the review of soft sensors provided by haimi et al 2013 led to similar conclusions 3 2 2 linear supervised learning techniques principal component regression pcr and pls are methods for regression that are based on a dimensionality reduction of the input space similarly to methods in 3 1 1 pcr and pls are both developed for regression with highly correlated input variables the main difference between pcr and pls is that pcr is based on a two step procedure pca and linear regression whereas pls computes all model elements simultaneously pls was first proposed in the context of chemometrics ricker 1988 its application to environmental processes is well known e g rosen and olsson 1998 choi and lee 2005 lee et al 2006b 2007 ferrer et al 2008 singh et al 2010 the alternative and simpler pcr model has been used frequently e g sundberg 1999 jansson et al 2002 aguado et al 2006 şahin et al 2007 du et al 2012 mašić et al 2015 3 2 3 nonlinear supervised learning techniques for nonlinear classification and regression problems nonlinear supervised learning both ann and svm are commonly used in spite of criticisms in the statistical literature e g hastie et al 2009 anns are still commonly used for nonlinear regression the ann model structure is usually of the multi layer perceptron mlp type i e the feed forward neural net ffnn and includes one hidden layer the use of other structures such as recurrent artificial neural nets has also been reported e g çoruh et al 2014 no recent advances in the identification of neural nets with multiple hidden layers aka deep learning have been reported for wastewater treatment applications applied activation functions are typically of the sigmoid type e g hyperbolic tangent or log sigmoid or bell type e g radial basis functions a few exceptions exist such as the use of the morlet wavelet function leading to so called wavelet neural nets wnn bello et al 2013 and the use of fuzzy membership functions leading to so called adaptive neural fuzzy inferences systems anfis yetilmezsoy et al 2011 the latter type of model is claimed to be more straightforwardly interpretable and useful for optimization as it allows for efficient model inversion there are the studies reporting on the use of such models for wastewater treatment processes e g tay and zhang 1999 huang and wang 1999 perendeci et al 2007 pai et al 2009a b erdirencelebi and yalpir 2011 ay and kisi 2014 support vector regression svr is also used for nonlinear regression purposes although it is usually referred to as svm except for a few select cases e g xie et al 2011 fang et al 2011 in a smaller number of cases it is also used for nonlinear classification e g zhang 2008 for both classification and regression conventional methods of svm and svr optimization and the use of gaussian or polynomial kernel functions are studied in most studies the intended use of the resulting nonlinear models is to enable the estimation or prediction of key performance indicators which are difficult or impossible to measure online which exact indicators are being predicted depends on the exact application often this provides assistance to process monitoring process optimization or process control e g hochedlinger et al 2006 lee et al 2006b pendashteh et al 2011 xie et al 2011 yetilmezsoy et al 2011 lin et al 2012 yu et al 2013 zhang et al 2012 soleimani et al 2013 in a few cases however the exact purpose of the resulting model based predictions is not clear e g wei et al 2013 3 2 4 clustering techniques by definition clustering methods are unsupervised learning methods that group data samples into a discrete number of subsets that are similar to each other in the context of microbial community analysis clustering methods are popular for relating the bacterial community structures to sampling locations or macroscopic process variables this is almost exclusively based on agglomerative aka bottom up hierarchical clustering methods such as ward s method or the unweighted pair group method with arithmetic mean upgma e g wittebolle et al 2005 he et al 2006 tan et al 2008 akarsubasi et al 2009 zhang et al 2010 clustering of process data is used primarily to understand wwtp operations and to construct diagnostic tools in contrast to the clustering applications in microbial community analysis process data are almost never analysed by means of hierarchical clustering methods instead methods based on centroids are typically used popularly used algorithms include k means clustering e g hong et al 2003 garcía and gonzález 2004 grieu et al 2005 aguado and rosen 2008 and fuzzy clustering methods e g fuzzy c means fcm or gustafson kessel fuzyy c means gk fcm clustering müller et al 1997 teppola et al 1998 1999 yoo 2003 rosén and yuan 2001 aguado et al 2008 dovžan and škrjanc 2011a b specialized algorithms to address discrete and continuous data simultaneously villez et al 2008 and to facilitate interpretability gibert et al 2010 have been proposed as well but have gained little traction thus far a comparison of several clustering methods has been conducted in gibert et al 2005 3 2 5 qualitative feature detection techniques a special data interpretation problem often arises when plant operators can visually assess process states and performance on the basis of qualitative features such as maxima minima or inflection points in time series data however the automated identification of such qualitative features can be a challenging problem in the presence of time varying process properties spatial variability e g plant to plant and lane to lane variations and measurement errors e g noise in the existing literature the sought after qualitative features are described by a wide range of terms including terms that describe patterns i by means of purely visual cues e g minimum inflection point bending point bend point ii by reference to chemical equilibrium reactions e g breakpoint in the case of oxidation reduction potential orp measurements iii by process state descriptions e g depletion iv by conversion rate or reaction rate descriptions e g end of reaction phase endogenous exogenous respiration reaction endpoint starting point treatment stage v by means of process specific terminology ammonia valley nitrate apex nitrate knee or vi by more general terms change point characteristic point control point signal level event given this diversity in terminology it is difficult to guarantee that the following overview of available methods is complete most of the currently available studies focus on detecting qualitative features in time series of cyclic processes such as in sbrs and alternating intermittent aerated system aias for wastewater treatment the appearance of qualitative features usually corresponds to the end of a desired reaction or conversion e g ammonia valley online detection of such features can then be used for the online adjustment of a wastewater treatment process in many cases an ad hoc solution can be provided on the basis of fairly simple schemes based on filtering differentiation descriptive statistics and or thresholds for univariate signals e g vanrolleghem and coen 1995 puig et al 2005 casellas et al 2006 corominas et al 2006 guisasola et al 2006 irizar et al 2008 vargas and buitrón 2008 ga and ra 2009 won and ra 2011 antileo et al 2013 unfortunately the fine tuning of these methods is seldom described in detail and may lead to case specific solutions in addition these schemes cannot take the multivariate and nonlinear natures of measurement signals into account explicitly to address this several methods have been proposed on the basis of anns e g lee et al 2005b bisschops et al 2006 luccarini et al 2010 fuzzy logic marsili libelli et al 2008 claros et al 2012 multivariate statistics e g villez et al 2010 and unsupervised learning methods e g garcía and gonzález 2004 the main tenet of these methods is that targeted qualitative features are described by means of quantitative properties while relatively popular these methods are not inherently robust in terms of the spatial and temporal variability of biological processes an interesting alternative is based on methods of qualitative trend analysis qta e g puig et al 2006 villez et al 2008 this group of methods converts time series directly into qualitative representations and thereby promises better generalization properties such as the capacity for extrapolation similar methods have also been more recently proposed for sensor diagnosis e g villez et al 2012 fault detection e g villez and habermacher 2015 sludge settleability characterization derlon et al 2017 and continuous flow system automation thürlimann et al 2017 3 3 human interpretable information extraction many tools for reconciliation detection diagnosis are based on statistical theory and lead to straightforward statistical limits and decision boundaries however such methods can often be outperformed by human experts therefore expert knowledge is often valuable to obtain finer tuning of decision boundaries human experts show the ability to reason through process operations without assuming accurate models human interaction and process knowledge is thus important to refine the definitions of normal data behaviour throughout the information extraction process all computed variables including statistics and performance indices as well as the associated decisions require some degree of expert knowledge rule and case based decision trees and cbr techniques are commonly used for collecting and representing expert knowledge which can be framed in more complex and functional structures such as in edss decision support systems may include specific reasoning methods that are focused on providing knowledge regarding organizational domain specific structures and on integrating different techniques and are globally intended to mimic domain expert reasoning 3 3 1 rule induction and decision trees rule induction and decision trees are important areas of machine learning in which formal rules are extracted from a set of observations in the context of wastewater treatment the main objectives of induction methods are to clarify data mining algorithms for all treatment processes and to create an understanding of all rules that are obtained from the collected data certain classifier models are particularly interesting as they generate models in the form of if then rules which are more expressive and easier for humans to comprehend and validate the subsequent use of the resulting decision trees is often seen as a good strategy for further improving the predictive capabilities of unseen data in treatment plants e g kusiak et al 2013 verma et al 2013 decision trees can also be used for problem solving situations e g ma et al 2009 modelling e g dürrenmatt and gujer 2011 flores alsina et al 2009 or even as a complementary tool for decision making processes e g comas et al 2001 comas et al 2003 evenson and baetz 1994 freitas et al 2000 ma et al 2009 r roda et al 2001 3 3 2 case based reasoning in regard to capturing and especially to drawing conclusions from past experiences cbr appears to be a powerful method cbr is a flexible paradigm that supports the implementation of a dynamic learning environment information on how problems were solved in the past and identification of the reasons that caused such problems has been demonstrated to be useful for supervisory systems with operation purposes e g rodriguez roda et al 2002 sànchez marrè et al 1997 as well as for prediction purposes e g wiese et al 2005 comas et al 2006 martínez et al 2006 and automatic learning e g krovvidy et al 1991 krovvidy and wee 1993 wiese et al 2005 cbr performance can be improved continuously by adding results from new experiences however a weak point of cbr is its incapacity to cope with future situations thus a cbr system is a not suitable tool for predictions and it may require reliance on mathematical models or neural networks for this purpose 3 3 3 fuzzy logic fuzzy logic is not a substitute for statistics but it can be used when statistical reasoning is inappropriate allowing the meshing of precise approaches with inexact approximate representations within the wastewater treatment context fuzzy logic has traditionally been applied to improve process control e g tsai et al 1993 marsili libelli and müller 1996 müller et al 1997 manesis et al 1998 meyer and pöpel 2003 puñal et al 2003 carrasco et al 2004 marsili libelli 2006 puig et al 2006 waewsak et al 2010 belchior et al 2012 examples of the predictive capabilities of fuzzy logic are also abundant in the literature e g tsai et al 1993 marsili libelli and giunti 2002 carlos hernandez et al 2009 civelekoglu et al 2009 turkdogan aydinol and yetilmezsoy 2010 angulo et al 2012 surprisingly fuzzy logic is frequently used to address the uncertain economics that surround wwtp costs e g fiter et al 2005 baroni et al 2006 other potential uses although less popular and more diverse include process optimization e g du et al 1999 bongards 2001 and automated decision making e g ling and hang 1998 ebner et al 2010 moon et al 2011 given the number of citations and related papers fuzzy logic is clearly one of the most popular techniques used today fig 2 and one of the few techniques that have been validated in full scale by many users e g manesis et al 1998 bongards 2001 3 3 4 ontologies ontologies try to play the role of common language among agents 1 1 an agent is anything that is capable of acting upon information it perceives from its environment an intelligent agent is an agent capable of making decisions about how it acts based on experience an autonomous intelligent agent is an intelligent agent that is free to choose between different actions gómez sebastià et al 2017 trying to solve problems but they are still far from delivering systems that smoothly integrate several types of knowledge and different methods of reasoning for wwtp problem solving after several seminal ideas were presented regarding the application of ontologies to wastewater treatment e g boo and aguilar 1998 cabezut boo et al 1999 only a few applied attempts have been put into practice mainly for instrumentation control and automation e g chau 2007 scholten 2008 the use of ontologies in agent baset simulations is a new research topic that opens the field to integrate the use of intelligent agents rational making decisions that will affect the status of the wwtp and the river along the simulation oliva felipe et al 2017 3 3 5 environmental decision support systems edss focus on integrating different techniques and methods into unified structures to both use and reuse knowledge e g via coupled models databases and assessment tools or to generate or expand knowledge rule induction data driven techniques self learning etc to support decision making reducing the time in which decisions are made and improving the consistency of decisions edss have been used widely for wastewater treatment having a high level of popularity as academic exercises although little validation e g fedra and winkelbauer 2002 maturana et al 2004 mcintosh et al 2011 and few full scale implementations have been reported in practice poch et al 2004 2017 garrido baserba et al 2014 how a particular edss is built depends on the type of problem available data and types of information and knowledge that can be acquired depending on the objective which can range from monitoring and data storage to prediction decision analyses as well as communication with society these unifying structures may result in or be the product of a combination of advanced control methods e g rodriguez roda et al 2002 comas et al 2004 2006 martínez et al 2006 or knowledge related techniques e g krovvidy et al 1994 comas et al 2003 poch et al 2004 zhu and mcbean 2007 garrido baserba et al 2012 despite the capabilities of each edss which depends on the implemented techniques or a combination of them the main attraction of edss is their user friendly interfaces which are intended to present results in a readily understandable form that should facilitate users to confront complex wwtp related decision making problems therefore success depends in part on how easily the information is presented to end users shim et al 2002 lardon et al 2005 matthies et al 2007 mcintosh et al 2011 4 method trends and popularity of the techniques 4 1 trends figure 3 shows the sum of the citations per year for the reviewed papers separately for each technique for some methods we observe a steadily increase of citations along the years ann and pca are the methods that generate more citations per year more than 200 after 2010 followed by fuzzy logic clustering ica pls with around 100 citations after 2010 wastewater treatment process improvements due to the application of this plethora of techniques may have been a driving force behind the steadily increasing interest in the field between 50 and 100 citations per year receive the papers published on som regression svm and qualitative features the methods that receive less than 50 citations per year are control charts and mass balance due to the fact that a limited number of papers on these topics are published per year one paper per year in the best case in addition the papers on knowledge generation and management rules ontologies which receive less than 20 citations per year have retained the same level of interest through the years either because they are highly specific to solving only select problems or lack novelty or because their potential cannot be efficiently exploited e g ontologies cbr there is a slight increase in the application of ontologies but far less popular than in other sectors 4 2 popularity of papers among scientists we calculated the average citation rate for each year i e the ratio between number of citations and number of papers eq 1 which gives an indication on how influential a paper has been within academia 1 c i t a t i o n r a t e y e a r 1983 y e a r c i t a t i o n s 1983 y e a r p a p e r s the most influential method has been ica with a citation rate of 63 in 2015 fig 4 meaning that each new paper published receives on average 63 citations followed by svm with a rate of 51 pca and cbr obtain a citation rate of 38 most of the other techniques lie on the range between 20 and 40 except for control chart and mass balance that result in a citation rate smaller than 20 we hypothesize that newer methods are cited more because they are new in particular control chart and mass balance may be considered general knowledge and therefore not cited as much 4 3 academic expectations based on the information extracted from the trends and the popularity we subjectively represented the academic expectations within the research field of wastewater treatment of all reviewed techniques in the form of a gartner hype cycle graphic fig 5 the analysis comes from purely academic sources and might not correspond to maturity levels of these techniques as seen in the industrial sector still many industrial partners contributed to peer reviewed journal papers and their vision is partially captured in the academic literature we established some rules to assign the techniques to each of the levels of the gartner hype cycle if the technique has a large number of citations and the citations trend reaches a plateau we consider the method is at the plateau of productivity if there is clear evidence that the method is useful works well widely adopted by research community we consider it to the at plateau of productivity e g mass balancing and control charts and simple statistics if the number of citations is high but does not reach a plateau still showing a positive trend we believe the technique still has potential and hence we consider the technique at the enlightenment e g clustering techniques showing smaller number of citations but there is evidence that they evolved are also considered in the enlightenment if the number of citations decreases over time we consider the technique to be at the trough of disillusionment if the number of citations does not reach a plateau but the citation rate is high we consider that the technique is at the peak of inflated expectation if the number of papers is rather small e g 12 papers using som and 8 papers using ontologies but the citation rate is relatively high e g close to 20 we consider the technique is at the trigger at the plateau of productivity we find ann including anfis and fuzzy logic are the well established methods in the research field with not only many research applications but also with several examples of real world implementation and transformations into products ready to be exported to the industrial sector e g ferrer et al 1998 manesis et al 1998 wen and vassiliadis 1998 bongards 2001 fiter et al 2005 lee et al 2008 mingzhi et al 2009 pca pls and regression also belong to the plateau of productivity even if they received less citations and got a lower citation rate compared to ann and fuzzy logic pls has been widely used in academia with successful results in chemometrics being essential for the development of commercial products used to monitor water quality variables e g spectrophotometric sensor from s can pca coupled to clustering has also been brought to practice e g lee et al 2008 martín de la vega et al 2012 finally mass balancing and control charts also belong the plateau of productivity as they are widely adopted by research community e g mass balancing for the design of wwtps as in flores alsina et al 2012 at the enlightenment we find qualitative features detection which has shown large benefits on improving process performance with low investment costs and many real world applications are found in literature e g yu et al 1997 yu 2004 gonzález and garcía 2006 zeng et al 2008 won and ra 2011 with regard to edss there have been many successful academic experiences e g baroni et al 2006 garrido baserba et al 2012 monclús et al 2012 and the current limitation relates to crossing the death valley towards a general use implementation of edss by society the market poch et al 2017 research around rules e g decision trees are also in the enlightenment phase as they are evolving together with edss even though cbr reaches a plateau in the number of citations the absolute number of citations is rather low although the concept of learning and improving from past experiences proved to work and should be of the upmost importance in any complex and multi objective project the integration of cbr with other tools or self organizing capabilities may still be required to overcome the current situation and move towards the plateau of productivity we did not found any technique belonging to the trough of disillusionment we can observe in figs 3 and 4 that fuzzy logic showed a decrease in the number of citations per year in the 90s and a decrease in the citation rate in 1995 we could state that fuzzy logic was in the trough of disillusionment in the late 90s but not nowadays at the peak of inflated expectation we find ica that was incorporated into the field after 2005 and experienced a rapid increase of interest and large number of citations similarly due to the increase in computational capabilities svm and deep learning will likely come to fruition in the near future ontologies and som are methods which are not mature yet in the trigger phase without proven practical experience in the real world but with large potential as demonstrated in other fields as is the case in natural language translation recommending systems normative systems etc 5 discussion 5 1 bringing the techniques into practice about 16 of the publications included in the review provided evidences that a commercial product resulted out of the research efforts still only 9 of the total publications clearly stated that these products were validated at full scale e g controller running at real time the remaining 7 represent products which have not been tested under full scale real conditions and hence are less mature the other publications 84 of the papers remain as an academic exercise even if full scale data were used which is the case for 46 of the papers about 25 of these academic exercises compared several methods to serve the same purpose other academic exercises discussed improvements or adaptations in classical methods to accommodate to particularities of wwtps we believe that those methods that rise in popularity in academia are often gaining interest for commercial implementation software development companies play a crucial role in here to enhance usability of the techniques e g through dedicated workflows and user friendly interfaces even though many companies offer wastewater data management software only a limited subset claim their focus is on data quality e g monitoring control platform from inctrl solutions canada or ritune from rittmeyer ag switzerland still it is difficult to keep track on which methods made it into practice as this is outside the scope of scientific literature and would require a targeted search through commercial products in our experience it is essential to guarantee a high level of end user participation e g wwtp operators to ensure the transfer of a method into practice examples of successful techniques in practice are decision trees and fuzzy logic such end user participation results in a much higher level of confidence and guarantees the continued use and tuning of the techniques furthermore most statistical techniques have been adopted by academics who have limited practical experience mostly using toy or benchmark examples today introducing such advanced techniques has proven difficult because practitioners do not participate in learning during their development this has major consequences for the future and we must learn from such past errors hence practitioners should either be educated on current existing methods or wwtps should contract services from well trained consultants similarly the research community should explore alternative data analysis methods that are more intuitive to operators to ensure their transfer into practice for example qualitative reasoning and analysis is promising but has been applied in only a limited context within the wastewater treatment community another problem in academia is that technology transfer from research results to implementation is seldom rewarded adequately 5 2 what are the current limitations of method development method validation is an extremely important step but it is especially challenging because of the difficulties to obtain high quality data this is in part because the installation of sensors and their maintenance efforts is insufficient to guarantee data quality in addition wastewater processes are characterized by changes at many different time scales spanning from seconds ph dynamics to years plant lay out changes and construction periods as such obtaining datasets that include all of the relevant dynamics over meaningful periods of time and with sufficiently high quality is not easy hence it would be of great value to create a repository of data sets that can be used for validation and comparison purposes in fact only 7 of the reviewed papers used data from repositories another limitation that we identified is the lack of an association between the fields of engineering and statistics the major cooperation between fields has been between environmental chemical civil engineering and computer engineering including the ai experts mainly for the development of edss 7 of the papers and environmental chemical civil engineering and electrical engineering 3 there is a need for more in depth interactions between engineers and other disciplines when developing methods to guarantee their successful application the optimal case would include researchers practitioners who are trained not only in engineering but also in statistics computer science etc through the creation of multidisciplinary training programmes we also feel that there is limited guidance in method development there are many different implementations for each of the methods but defining what constitutes a good implementation for a solution remains unclear good practices for data mining and information extraction methods should be defined at least for those methods that have been established for decades the handbook on good modelling practice for identification of mechanistic wwtp models can be used as an example rieger et al 2012 the knowledge management discipline is the one we identified has large potential but has been understudied in the field of wastewater treatment ontologies are the natural way to integrate knowledge from different sources and to share it among different actors ontologies serve to deliver systems that smoothly integrate several types of knowledge and different methods of reasoning however faster computing facilities cheaper memory storage and more reliable sensors augment the pace of development ontologies are already a consumable in the internet of things environment were wwtps make up just one kind of elements next generation edss must be able to capture and integrate lessons learned to organize memory and to share high value pieces of knowledge in such systems ontologies should be able to provide the appropriate means and terminology for tagging content and mapping to databases repositories and knowledge bases they should provide a framework for cataloguing identifying human expertise where it exists as a mechanism for capturing tacit and expert knowledge 5 3 what are the current limitations of method selection finally there is no sufficient guidance for method selection which makes it difficult for practitioners to select the best method to address a particular issue several methods can serve a similar purpose or a single method can show variations in its implementation this makes an effective comparison hard additionally different techniques highlight different data features or patterns such that comparison may be meaningless in such cases in fact 21 of the reviewed papers compared the proposed methods against other existing ones and highlighted the advantages as referred to these still the comparison is partial generally with a small subset of techniques on average 2 techniques are compared which means that a transparent comparison of a large number of methods providing the same function is needed to provide guidance on method selection overall there is a lack of standardized protocols for the choosing and implementation of data analytic techniques even for academics the effort to characterize and map the field has proven to be challenging for this reason we are of the opinion that there is more of a need than ever to develop guidelines and manuals rather than new techniques of special relevance is the existence of the bsm1 lt platform gernaey et al 2014 this is a plant wide simulation model of a standard wwtp which was constructed to enable the effective comparison of process monitoring diagnosis and control techniques this platform also allows facilitates cost benefit analyses of specialized process monitoring diagnosis and control algorithms in fact about 9 of the studies already used that platform for method development and comparison furthermore researchers should take profit of the fact that at least 25 of the papers published used matlab for method implementation and that percentage could be even higher as 38 of the papers did not specify the software platform used establishing a community that shares method implementation at least in matlab or through github would effectively contribute to effective comparison of methods and come up with proper guidance for method selection 6 conclusions the historical evolution of signal processing from data to information and knowledge management from information to knowledge shows that different techniques have been developed implemented at different levels of management from simple control loops to environmental decision support systems the increased number of potentially conflicting objectives that have been established for wastewater treatment systems from simple dissolved oxygen control at the wwtp to system wide control demands the adoption of more advanced data management techniques in turn requiring a higher level of specialist education basic and advanced statistics artificial intelligence machine learning the review of techniques shows that even though a plethora of methods have been developed tested there are still challenges to be addressed related to the objective comparison of the techniques the lack of guidelines the limited validation at full scale the lack of collaborative methods and the limited options for active optimization of data information content and quality in addition more efforts are needed to develop implement knowledge generation management and application techniques acknowledgements the authors acknowledge funding for the reach project ctm2015 66892 r mineco feder ue and from the spanish ministry for the ryc 2013 14595 also we acknowledge the economy and knowledge department of the catalan government consolidated research group 2014 sgr 291 icra and the 2014 sgr 1168 lequia prof cortés is a member of the sistema nacional de investigadores sni conacyt 
