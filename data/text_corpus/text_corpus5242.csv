index,text
26210,forest landscape models flm are widely used for simulating forest ecosystems as flms have become more mechanistic more input parameters are required which increases model parameter uncertainty to better understand the increased mechanistic detail provided by landis ii pnet succession we studied the effects of parameter uncertainty on model outputs based on three different approaches global sensitivity analyses summarized the influence of each parameter a local sensitivity analysis determined the magnitude of and degree of nonlinearity of variation in model outputs alongside variation in individual parameters and a regression tree analysis identified hierarchical relationships among and interaction effects between parameters foliar nitrogen maintenance respiration and atmospheric carbon dioxide concentration were the most influential parameters in the global analysis knowing where parameter influence is concentrated will help model users interpret results from landis ii pnet succession to address ecological questions and should guide priorities for data acquisition keywords uncertainty landis ii pnet forest landscape model fourier amplitude sensitivity test regression tree 1 introduction forest landscape models flms are a class of spatially interactive stochastic simulation models that are widely used by researchers and natural resource managers to project long term and broad scale changes to forested landscapes shifley et al 2017 flms are increasingly used to simulate the effects of global change drivers such as climate and land use change on future forest composition and function duveneck and thompson 2017 liang et al 2017 because global change scenarios are complex and often without observed precedent flms have become more mechanistic regarding the major drivers of forest change with steps toward inclusion of the physiological responses to changing temperature moisture atmospheric conditions fire and defoliation gustafson 2013 while mechanistic approaches for simulating global change offer a strong foundation for simulating emergent and novel conditions they require many more input parameters than did their statistically based predecessors the presumed increase in conceptual robustness gained by using a mechanistic model could potentially be undermined by increased parameter uncertainty associated with a lack of empirical data and or mechanistic understanding of each of the model coefficients cuddington et al 2013 dietze 2017 a sensitivity analysis sa can be used to quantify the impact of model input parameters on model outputs saltelli et al 2000 by systematically varying input parameters and quantifying the relationship of this variation to the resulting variation in model outputs a sa identifies the parameters that are most influential over model outcomes as well as the critical regions in which parameter values have a disproportionate effect on the model output saltelli et al 2006 thus while guiding model use a sensitivity analysis can also prioritize future research by suggesting which ecological parameters require more accurate estimation from an ecological perspective sensitivity analyses may also be useful for identifying dominant mechanisms in the behavior of the model and thus can improve our understanding of how the modeled system functions reusser et al 2011 additionally sensitivity analyses can reveal parameter interactions and correlations and they can simplify models by suggesting which input parameters are less relevant saltelli et al 2006 there are two broad approaches for conducting sensitivity analyses local sensitivity analysis lsa and global sensitivity analysis gsa each approach has strengths and weaknesses lsa also called one at a time sensitivity analysis is relatively simple to implement and has been widely used to evaluate flms sturtevant et al 2009 thompson et al 2011 xu et al 2009 in a lsa all parameters are kept constant at their mean or another predefined reference value while a single parameter is varied within a specified range this process is repeated for all of the parameters of interest and the absolute and or relative influences on a given response variable are compared the strength of lsa is that it is easy to perform and straightforward to interpret because just one input parameter is varied for each test however interpretations from lsa are limited when several parameters might interact dynamically through the range of individual values campolongo et al 2007 saltelli and annoni 2010 to capture model sensitivity throughout all of input parameter space gsas are required in gsa all parameters are varied simultaneously gsa is more robust than lsa for summarizing parameter effects in complex models because it determines parameter sensitivities across large regions of parameter space reusser et al 2011 there are many types of gsa including variance based tests like the fourier amplitude sensitivity test fast sobol s method the derivatives based morris method and various regression based approaches sobol s method is an extensive global sensitivity analysis it calculates total sensitivity indices for each parameter including influence due to interaction effects sobol 1993 however sobol s method relies on a search function that requires many model runs and is therefore not computationally feasible for highly complex models fast like sobol s method is a variance based global sensitivity analysis summarizing the effect of each parameter on model outputs cukier et al 1975 1973 mcrae et al 1982 fast varies all tested input parameters simultaneously through the full input parameter space and then uses fourier transforms to identify the degree to which each parameter is responsible for variation in model outputs fast relies on a characteristic frequency of variation assigned to each input parameter to estimate sensitivity indices this allows fast to be much more efficient for calculating main effects than comparable methods such as sobol s method saltelli and bolado 1998 a variation of fast known as extended fast efast also exists for calculating total effect i e including interactions indices for each parameter saltelli et al 1999 however the total effect index for each parameter comes at the cost of many more simulations another type of sensitivity analysis similar to fast is the method of morris mom which randomizes one at a time testing in a structured framework for global parameter screening morris 1991 like fast mom assigns indices to parameters indicating their global influence additionally mom indicates the directionality of each parameter s influence and it returns an index for each parameter that corresponds to the combination of its non linear effects on model outputs and its interactions with all other parameters wainwright et al 2014 however the numeric values of mom indices are specific to the units of the model outputs this is a contrast with fast whose indices represent the proportion of variance explained additionally while mom returns an index corresponding to the combined interactions and nonlinear effects of each parameter it cannot distinguish between interactions and nonlinearities or determine where they occur in parameter space brevault et al 2013 beyond fast and mom classification and regression trees have been used in predictive modeling and more recently as methods of global sensitivity analysis almeida et al 2017 breiman et al 1984 cutler et al 2007 iverson and prasad 1998 pappenberger et al 2006 because classification and regression tree models are defined by a hierarchy of split input parameters fitted regression trees can identify regions in the ranges of individual input parameters where they interact with other input parameters in this way classification and regression trees can supplement global sensitivity analyses that either exclude interaction effects e g fast or return a single value per parameter that corresponds to the summed interactive and nonlinear effects of that parameter e g mom sobol s method and efast regression tree models are relatively robust to various sampling schemes especially when compared with fast which requires very specific sampling this allows classification or regression tree analysis to be performed directly on the same data used for a separate global sensitivity analysis until recently the use of tree based approaches has been uncommon in sensitivity analyses and what little work has been done was mostly limited to slope stability and hydrological models almeida et al 2017 singh et al 2014 however a regression tree was used in a study of the landis ii biomass succession extension flm simons legaard et al 2015 landis ii scheller et al 2007 is an flm modeling platform that has been used extensively to simulate landscape scale temperate and boreal forest dynamics duveneck and scheller 2015 duveneck and thompson 2017 kretchun et al 2014 loudermilk et al 2013 landis ii simulates the establishment growth competition and mortality of tree species by age cohorts as they are affected by climate and disturbance seed dispersal and various disturbances are simulated across a landscape as spatially interactive processes scheller et al 2007 landis ii requires users to select a succession extension based on their specific research question previous sensitivity analyses have been completed for the biomass succession extension scheller and mladenoff 2004 thompson et al 2011 scheller and mladenoff 2004 and thompson et al 2011 assessed the sensitivity of six and nine key parameters respectively with lsas varying each parameter by 10 they found that biomass succession was not overly sensitive to any one of the parameters tested but that the model outputs were most sensitive to the parameters specifying the maximum allowable biomass and maximum annual net primary productivity npp xu et al 2009 completed a fast global sensitivity analysis with biomass succession using different climate variables including temperature temp precipitation precip atmospheric co2 co2 and photosynthetically active radiation par of the climate variables they found forest composition to be most sensitive to temp followed by par co2 and precip simons legaard et al 2015 conducted a global and temporal sensitivity analysis of nine key parameters of biomass succession and also found that maximum allowable biomass and maximum annual npp were most influential on predicted biomass in addition they found that temporal variation and interactions between parameters influenced the biomass output partly in response to the influence of the maximum allowable biomass and maximum annual npp parameters of biomass succession both of which are phenomenological and without mechanistic basis de bruijn et al 2014 developed the pnet succession extension replacing these parameters with a more mechanistic approach to simulating biomass accumulation gustafson et al 2014 pnet succession de bruijn et al 2014 incorporates algorithms of the pnet ii ecophysiological model aber et al 1995 which uses first principles of photosynthesis to simulate competition for resources within vertical canopy layers because landis ii tracks biomass rather than canopy height variation in the biomass of cohorts within a cell is used to define distinct canopy layers incoming radiation is allocated to the separate layers to simulate extinction and competition for light photosynthetic production is allocated to foliage wood roots and reserves non structural carbon pnet succession has been evaluated at several sites de bruijn et al 2014 duveneck et al 2017 gustafson and sturtevant 2013 and has been used to answer a variety of research questions duveneck and thompson 2019 2017 a local sensitivity analysis by gustafson et al 2017a explored the sensitivity of model outputs to six climate related input parameters to demonstrate the advantage of employing first principles to predict forest responses to climate change the analysis quantified the relative effects of climate parameters with directionality and extended the lsa by varying multiple parameters simultaneously highlighting the interactions between parameters however the analysis did not directly compare the absolute effects of parameters to one another and it excluded several pnet succession parameters not related to climate that which affect forest growth to build upon the recent work by gustafson et al 2017a we describe the results of a three tiered sa that attempts to harness the strength of the sa approaches discussed above as applied to the pnet ii succession extension used within the landis ii v 6 1 flm framework for our initial gsa we selected 20 parameters that had previously demonstrated influential behavior table 1 the objective of this study was to identify the variables most influential in determining the outputs of landis ii pnet succession across time and to identify both interactive effects among parameters and nonlinear responses of model outputs to variation in individual parameters identifying highly influential parameters will help model users understand the drivers of model outputs and will help prioritize areas for model refinement our hybrid approach employs three separate analyses a global sensitivity analysis a one at a time local sensitivity analysis and regression tree analysis and leverages the strengths of each to quantify the sensitivity of two types of model response variables to variation in input parameters offering users a more complete understanding of model responses to variation in model inputs this approach also provides a replicable framework for assessing parameter uncertainty and for prioritizing data collection to improve the accuracy of model predictions 2 methods 2 1 approach using landis ii v 6 1 and pnet succession v 1 0 0 we simulated an artificial 25 by 25 pixel grid landscape totaling 625 cells each representing 6 ha of forested land for 200 years with five year time steps we populated the landscape with 50 unique initial communities i e species age cohort mixes sampled from the fia database bechtold and patterson 2005 in new england usa these initial communities were made up of 26 tree species each of which is common among temperate forests of the northeastern united states table 2 our general approach to conduct sensitivity analyses was as follows fig 1 to summarize the influence of each parameter table 1 on selected response variables through the full parameter space of the model we began the sensitivity analysis using the mom and a fast gsa see global sensitivity analysis details below the mom and fast analyses each ranked parameters and mom indicated directionality and the combined effects of nonlinearity and interactions for each parameter from the fast analysis we evaluated the total proportion of variance explained by each parameter at each five year time step of the model we also conducted hierarchical partitioning of biomass outputs at years 0 and 200 chosen to capture temporal differences in parameter influence in order to determine the break points in the influence of each interacting parameter see regression tree analysis details below finally we identified the most influential parameters from the results of the global analyses and conducted a lsa of each of those parameters on biomass and area occupied by specific tree species to detect nonlinear relationships between input parameters and their effects on model outputs see lsa details below our sensitivity analyses examined variation in two types of outputs total landscape aboveground biomass and the number of landscape cells occupied by each of three dominant new england species red maple acer rubrum red oak quercus rubra and white pine pinus strobus aboveground biomass is the forest growth currency in landis ii and cell occupancy of individual species defines forest composition spatially taken together changes in aboveground biomass and forest composition are useful metrics to capture forest growth and successional responses to disturbance and are therefore appropriate response variables for our sensitivity analysis duveneck et al 2017 we limited our parameter selection to twenty based on what was computationally feasible we selected parameters that were newly introduced with the pnet succession extension or were linked to the photosynthetic mechanisms targeted by the extension parameter selection was motivated by either a parameter s known importance to the model or by large uncertainty about the input value for the parameter the selected parameters were chosen to lead to a better understanding of both the model and the specific contribution of each parameter to output variance some parameters tested foliar nitrogen foln maximum specific leaf weight slwmax optimal temperature for photosynthesis psntopt and the fraction of the amount of active woody biomass that is allocated to foliage per year fracfol were species specific and we jointly varied these linearly across species the values sampled by fast for these parameters ranged from 0 to 1 where 0 meant that each species sampled parameter value was 90 of its default value and 1 meant that it was 110 of its default value for each species the foln range for each species was set to include the middle 90 of values from the nerc foliar chemistry database table 2 northeastern ecosystem research cooperative 2016 and was also varied linearly within those bounds climate parameters average monthly maximum and minimum temperature values temp monthly carbon dioxide concentration co2 monthly precipitation precip and monthly photosynthetically active radiation par were varied across the range predicted from 2000 to 2100 provided by the regional concentration pathway 8 5 emission scenario ipcc 2013 as simulated by the hadley global environment model v 2 earth system global circulation model downscaled for new england obtained from the usgs geo data portal stoner et al 2013 for these parameters we sampled from within the range of average annual climate values a single value per parameter after sampling this average annual parameter value e g the average temperature for the year we then parameterized the model using the monthly values corresponding to the year from the data with the closest average annual value e g the year with the closest average temperature to the sampled value was chosen and then the monthly values were extracted from that year s data table 3 for temp an overall average for each year was used for sampling then the monthly maximum and minimum temperature values tmax and tmin required by pnet landis ii were selected together from the data for the year whose average temperature was closest to the sampled value the sampled climate parameters were held constant across years in each simulation other parameters were not species nor temporally specific and were composed of a single value for a given scenario e g maintenance respiration maintresp and the proportion of non structural carbon to be maintained when net photosynthesis exceeds maintresp dnsc these values were varied within their limits as defined by the pnet succession user manual or expert knowledge of the model table 3 gustafson et al 2017b 2 2 global sensitivity analyses we performed the fast analysis using the r package fast r development core team 2006 reusser et al 2011 fast is a variance based sensitivity analysis that calculates the main effect i e excluding interactions with other parameters of each parameter on variation in model outputs fast generates parameter samples to be used as simulation inputs within the bounds of a user defined minimum and maximum value for each parameter by assigning each parameter a characteristic frequency each parameter is systematically varied at this frequency within the bounds defined by the maximum and minimum values across all of the parameter samples and the model is then run on each set of parameter samples afterward a fast fourier transform is applied to observed variation in model outputs to produce a power spectrum the values from the first four multiples of the characteristic frequencies for each parameter are summed and divided by the total summed power spectrum across all frequencies to calculate the main effect sensitivity indices cukier et al 1975 we generated 8378 samples four times the minimum number required by the nyquist criterion i e the function frequency limits and the corresponding discrete sampling rate required to describe a continuous function see jerri 1977 to ensure convergence mcrae et al 1982 saltelli and bolado 1998 we then used each parameter sample to conduct a landis ii simulation to capture variation in behavior through the successional trajectories experienced over time we calculated fast sensitivity indices at each 5 year time step of the simulations we performed mom using the r package sensitivity to rank parameters in order of importance to determine directionality of parameters and to estimate the extent to which each parameter interacts with other parameters or affects outputs nonlinearly iooss et al 2015 r development core team 2006 in contrast to fast which calculates main effect indices mom estimates total effect indices for the model parameters which include influence over model outputs due to interactions with other parameters we designed our mom analysis to sample 40 one at a time paths r for the 20 parameters k for a total of r k 1 40 20 1 840 model runs in addition to those by fast the minimum and maximum values constraining parameter sampling were the same as those used in fast described above and listed in tables 2 and 3 mom returns a single elementary effect for each parameter for each path which summarizes the influence of that parameter through the individual path the mean of the elementary effects μ is used to calculate the total effect index for each parameter the sign of the mean of the elementary effects then corresponds to directionality of the parameter s influence with the exception of when the parameter has both positive and negative effects on model outputs in which case the sign of the individual elementary effects would have to be analyzed campolongo et al 2007 king and perera 2013 the mean of the absolute values of the elementary effects for each parameter μ has also been used to summarize the parameter s global influence over a particular output while incorporating non monotonic behavior campolongo et al 2007 finally the standard deviation of the elementary effects σ around the mean μ corresponds to the combined nonlinearity and interactions associated with each parameter 2 3 regression tree analysis we performed a regression tree analysis rta using the party package in r hothorn et al 2006 rta is a non parametric technique for recursively partitioning a continuous variable into increasingly homogeneous subsets where the partitions are identified by testing all potential partitions across all values of all the predictor variables then selecting the partition that maximizes the difference between groups de ath and fabricius 2000 rta results in a dendrogram that shows the hierarchical relationships among predictors and between predictors and the response the same samples developed for the fast analysis were reused with biomass outputs from years 0 and 200 as the response variables the specific rta implementation within the party package is called conditional inference trees which requires a significant difference p value 0 05 as determined from a monte carlo randomization test in order to create a partition in the regression tree this technique minimizes bias and prevents over fitting and the need for regression tree pruning de ath and fabricius 2000 hothorn et al 2006 hothorn and zeileis 2015 2 4 local sensitivity analysis finally we conducted a lsa to detect the magnitude of each parameter s influence at a point in parameter space in model output units rather than relative to variance in outputs from other parameters and to determine whether landis ii outputs respond nonlinearly to linear variation across each parameter s range like the mom the lsa results also indicate directionality and magnitude of parameter influence we performed an lsa on the top four influential parameters identified in the fast analysis for each type of output biomass and landscape composition we performed 24 total simulations for the local sensitivity testing on each output type simulations were performed at six evenly spaced values across the parameter s possible range while all other input parameters were held at their calibrated values for each of the four parameters tested in order to assess variation over time output values were assessed at each five year time step for each simulation 3 results 3 1 global sensitivity analyses based on the fast results maintresp was the most influential parameter for determining total landscape biomass output by the model across a 200 year simulation fig 2 a although foln was more influential for determining total biomass than maintresp during model initialization year 0 the influence of foln for determining total landscape biomass declined quickly after year 0 giving foln a slightly lower average index value than that of maintresp other influential parameters included the climate parameters co2 temp and precip and precipitation loss fraction preclossfrac which determines the proportion of water that does not enter the soil the fast analysis also indicated that the same parameters to which biomass outputs are sensitive tend to be important in determining species composition fig 2 b c d the three species differed somewhat regarding the relative influence of the most important parameters for determining species cell occupancy with foln explaining more variance in cell occupancy of quercus rubra than in the other two species however the same parameters were influential for determining cell occupancy for each of the three species foln maintresp dnsc the amount of woody biomass capable of supporting foliage fractwd and sensitivity of establishment to light estrad climate parameters were not influential in determining species composition contrary to our results for outputs which are directly related to growth for both total landscape biomass and species cell occupancy tests par establishment parameters rooting depth rootingdepth and disturbance frequency wrp were not identified as being very influential temporal variation in the fast sensitivity index for each parameter indicated how the relative influences of particular variables on model outputs changed through time as noted above total landscape biomass was especially sensitive to foln early in each simulation where the parameter s variation accounted for one third of the variation in output landscape biomass after 200 simulation years variation explained by foln only accounted for ten percent of output variance in contrast maintresp explained ten percent more variation in total landscape biomass by the end of simulations than at the beginning the change in relative influence of the two variables is a result of increasing biomass in the simulations which is associated with increases in the contribution of maintresp temporal variation was observed in the fast results for cell occupancy as well where the influence of foln increased with time from year 0 to year 200 the influence of foln on quercus rubra occupancy increased from approximately thirty percent to explaining over half the variation in cell occupancy by year 200 building on the results from fast the mom analysis estimated total effect sensitivity values for each parameter when ordered by absolute valued indices μ the parameter rankings of mom closely mirror the rankings of the fast analysis table 4 additionally the sign of mean elementary effects for each parameter indicated directionality for the influence of the parameter figure a1 the large standard deviations σ of the sets of elementary effects for the input parameters also suggested a substantial amount of combined nonlinearity and or interactions between parameters see appendix i for a more detailed explanation of the mom results 3 2 regression tree analysis while we performed the rta across time slices of the model we found that most of the relevant information offered by the rta was contained within the analysis at year 0 and at year 200 which are presented here figs 3 and 4 the rta of total biomass accumulated during model initialization year 0 partitioned the simulations into 12 terminal nodes which are shown as boxplots in fig 3 each boxplot shows the distribution of year 0 biomass within the subset of simulations in that branch of the regression tree consistent with the findings of fast the rta shows that foln co2 and maintresp explain much of the variation in the year 0 biomass indeed of the 11 partitions that the rta identified 10 came from those three variables temp entered as a significant variable only in the relatively small subset of simulations n 1664 where foln and co2 were high and maintresp was low the first partition at the top of the rta was based on whether foln was greater or less than 44 of its potential range indicating foln as the most significant predictor variable overall not surprisingly higher values of foln were associated with greater biomass working down both sides of the tree for high and low values of foln the next splits were based on whether co2 concentration was greater or less than 620 ppm higher values of co2 were associated with higher levels of biomass at this level in the regression tree the analysis has partitioned four groups of simulations that represent high low foln and high low co2 the corresponding values of biomass in these simulations range from 14 449 g m2 with low foln and low co2 to 29 661 g m2 with high foln and high co2 below this level on the regression tree the predictor variables diverge within the branch with high foln the right side of the tree maintresp was identified as the next most predictive for both the high and low co2 groups within the branch with low foln the left side of the tree foln was again identified as the most predictive for both high and low co2 groups overall by the bottom of tree and thus at the point where rta cannot identify any additional significant partitions the simulations with low maintresp 0 003 and the highest levels of foln 0 7 had the highest average year 0 biomass the lowest levels of year 0 biomass occurred when foln was below 0 2 and co2 was below 620 ppm by year 200 of the simulations foln co2 and maintresp were still identified as the most influential variables and thus they represent the most common partitions in the tree fig 4 however their relative importance had shifted at year 200 the first partition is on maintresp followed by co2 then foln fig 4 like time 0 the rta partitioned co2 in the mid 600s working down the side of the tree with higher values of maintresp the climatic variable temp entered as a significant predictor overall after 200 years simulations where maintresp was 0 003 co2 concentrations were 606 ppm foln was 0 43 and towood was 0 01 had the highest average biomass 3 3 local sensitivity analysis results from the lsa showed that simulations at calibrated values responded to variation in selected most influential input parameters as expected relative to each other given the results of the gsa and rta analyses the local analysis supported the direction and magnitude of the influence of each parameter on total biomass fig 5 and species occupancy fig 6 both of which had been indicated by gsa while increasing maintresp and temp inputs decreased landscape biomass increasing foln and co2 inputs increased landscape biomass additionally the lsa revealed particular nonlinear effects in the range of variation for each parameter these effects had been captured in the standard deviation of elementary effects from the mom gsa but mom does not discriminate between interactions and non linearities in contrast the lsa can identify regions of nonlinearity across points in parameter space but cannot identify interactions for example while the change in total biomass corresponded nearly linearly with change in maintresp at year 200 the model showed much higher sensitivity to foln in the middle of the parameter s range than near the lower and upper limits of its range 4 discussion our primary objective in this study was to identify the most influential parameters within a mechanistically based flm i e the coupled landis ii pnet succession framework de brujin et al 2014 developed this succession module with the goal of moving away from earlier succession modules within landis ii which were largely phenomenological and whose most influential parameters were not explicitly linked to processes that govern tree growth and forest development and are essentially unknowable outside of some limited range of observations given that the goal of pnet succession was to develop a module driven by eco physiological first principles it is not surprising that the most influential parameters we identified were foln maintresp and co2 all three parameters are closely linked to the mechanisms of tree growth and survival this contrasts with sensitivity analyses of the biomass succession extension to landis ii scheller and mladenoff 2004 which found that maximum allowable biomass and maximum annual npp were most sensitive simons legaard et al 2015 thompson et al 2011 in this context the pnet model represents significant progress toward an flm that can be reasonably expected to simulate forest processes under future conditions of climate and atmospheric chemistry that have not been observed in the past nonetheless each of these parameters has distinct limitations for use in an flm and present challenges for model users the importance of foln as identified by our global sa reflects the original design of the original pnet family of models developed in the 1990s aber et al 1997 1995 ollinger et al 1998 all of which use foln to represent n availability and to estimate gross photosynthesis in the context of landis ii pnet succession this is consistent with the developers intentions to have broad scale mechanistic underpinnings within the model and reflects the fact that foliar nitrogen concentrations are strongly linked to rates of photosynthesis and ecosystem scale carbon assimilation across forests worldwide reich et al 1997 from the perspective of the user of landis ii pnet succession there are many published studies and public databases documenting foliar n that can aid parameterization such as the foliar chemistry database northeastern ecosystem research cooperative 2016 which we used here within landis ii pnet succession foln is a species specific parameter which results in differential growth rates among species and allows users to simulate within stand competition which is an advancement over the earlier lumped parameter versions of pnet ii while the model allows foln to vary by species foln cannot vary over space nor through vertical layers of a canopy in the current implementation neglecting spatial variability in foliar n and thus n availability is a limitation of the model correlational studies using broad scale remote sensing data like that from the airborne visible infrared imaging spectrometer aviris and the moderate resolution imaging spectroradiometer modis have described the potential use of these datasets for mapping the spatial variation of foln in temperate forests lepine et al 2016 ollinger et al 2008b and modifications to pnet succession are also being tested to allow foln to vary vertically in response to light availability gustafson et al 2018 our results suggest that developing the model s capability to make foln more dynamic rather than as a species constant is a high priority for management and could greatly reduce uncertainty in model results maintresp was also among the most influential parameters in our sensitivity analyses maintresp is the amount of non structural carbon lost to maintenance respiration each month and is used in the calculation of cohort npp and mortality maintresp is affected by temperature de bruijn et al 2014 and is thus consistent with the model s more mechanistic underpinnings and utility for modeling forest change in future climates however the formulation of maintresp is greatly simplified as compared to the respiration calculations in the original version of pnet ii aber et al 1995 and it is applied universally across all species this formulation was required to achieve computational tractability gustafson et al 2017b the consequence of these simplifications is that maintresp is not an empirically measurable value and has a definition that does not exist outside the model from the user s perspective maintresp is effectively a tuning parameter while we see no obvious alternative having a tuning parameter as among the most sensitive in the model is challenging from the perspective of developing an flm based on mechanisms that are expected to perform predictably including under conditions that have not been observed and thus not tuned to future improvements to the model should incorporate maintenance respiration requirements to growth that can be measured and tested preferably at a landscape scale co2 was the other parameter that our sa analyses identified as highly influential greater co2 concentrations result in increased water use efficiency and increased rates of photosynthesis and atmospheric co2 concentrations are expected to continue to rise even further to unprecedented levels keenan et al 2013 our findings support other studies using the pnet family of models including ollinger et al 2008a which found co2 to be more influential than temperature in predicting changes to npp over multiple climate change scenarios throughout the 21st century the long term effects of increasing co2 on forest growth and demographics remain highly uncertain and comprise an area of rapidly developing research norby et al 2010 schimel et al 2015 given this the model s high sensitivity to co2 should be viewed with a level of caution commensurate with the uncertainty in the broader scientific understanding like maintresp and foln co2 s effect on the model should continuously be evaluated as improvements are made to the understanding of the physiological effects of co2 on forest growth using a hybrid methodology of combining gsa rta and lsa to assess output sensitivities provided unique insights about landis ii pnet succession pianosi et al 2016 the main effect indices from the fast analysis allowed us to understand the output variation driven by each parameter tested within the gsa additionally the mom analysis returned sensitivities corresponding to the total effects of each parameter the rankings of parameters under the mom results were similar to those of fast attesting to the robustness of our results however despite their similarity in this study fast and mom results are not directly comparable as they are estimating different values main effect indices for fast and total effect indices for mom depending on the scale of interactions though they might often yield similar rankings as we found in this study mom and lsa indicated the directionality of each parameter s influence on outputs and mom indicated the extent to which each parameter interacted with other parameters and or exhibited nonlinearity for example mom and lsa showed that increasing foln resulted in greater biomass which is consistent with the known role of foliar nitrogen in determining the maximum rate of carboxylation and the maximum rate of electron transport during photosynthesis walker et al 2014 likewise increasing maintresp reduced total landscape biomass because high ratios of respiration rate to photosynthetic rate are known to deplete carbon stores van oijen et al 2010 to then examine in detail how the influence identified by gsa methods is partitioned rta allowed us to identify parameter break points and specific parameter interactions in part the partitioning by rta reflected the parameter rankings from the other analyses additionally we can visually inspect the output variation in the box plots in figs 3 and 4 alongside the parameter variation that produced it to identify interactions for example the rta output at year 200 shows co2 contributing to greater variation at low values of maintresp than at high values of maintresp and towood appears to contribute substantially to variation in outputs when foln and co2 values are high and when maintresp is low finally the lsa allowed us to examine the nonlinearity of the effects of individual parameters on model outputs at a particular part of parameter space 5 conclusions while each sa method offered additional information about the influence of input parameters on model outputs they also overlapped significantly in their conclusions fundamentally all methods supported foln maintresp and co2 as being highly influential this combination approach provided a more robust analysis and associated conclusions than if we had used just one of these methods alone by using all four methods fast mom lsa and rta in consort we were able to assemble a more complete picture about the influence of each parameter and ultimately a greater understanding of landis ii pnet succession pianosi et al 2016 we have presented a thorough analysis of the sensitivity of landis ii pnet succession outputs to variation in twenty input parameters our work here builds upon that of gustafson et al 2017a and where our analysis overlapped our results were consistent specifically the directionalities of the effects of temp and precip parameters on biomass in this study were the same as shown in their sensitivity analysis while the magnitude of the response of biomass to variation in temp versus precip in gustafson et al 2017a suggested that precip was a stronger driver of biomass than temp in their simulations our analysis instead suggests that temp has slightly more influence than precip over biomass outputs this discrepancy is likely due to differences in the sampling of parameter space the local analysis performed by gustafson et al 2017a was not designed to compare the relative influences of parameters to each other and instead aimed to capture individual parameter directionality and parameter interactions as such they only varied temp and precip values over limited ranges on the other hand our analysis was specifically designed to sample each parameter throughout its possible input range allowing us to compare the overall importance of parameters for determining model output values we selected proportional ranges of variation for climate variables and tested the influence of all input parameters through the space of variation of the other parameters which should make comparing the relative influence of parameters more reliable we evaluated the area within a landscape occupied by three species identified the parameters that contribute most to variation in model outputs and described their effects our sensitivity analysis showed little similarity between the effects of input parameters on different response variables with total landscape biomass being driven primarily by parameters directly influencing carbon assimilation such as foln and maintresp and with cell occupancy being driven more by parameters associated with carbon allocation strikingly the sensitivity of cell occupancy to each of various input parameters was highly variable between species this highlights the known danger of extrapolating the results of a sensitivity analysis on one model variable to different response variables as demonstrated in previous studies to understand the sensitivity of a specific model output to various input parameters that output must be individually tested rosolem et al 2012 van werkhoven et al 2008 another important feature of our analysis was implementing the sa across time steps as in reusser et al 2011 our analysis showed that some parameters like foln in the landscape biomass analysis exert much more influence early in the simulations than in later time steps others like maintresp may become more influential through time across temperate forests the influential parameters foln co2 temp and precip are relatively well supported by data for estimating their values however data supporting other important parameters maintresp fractwd dnsc and estrad are less easily estimated despite the ecological realism of these variables given that our analysis has shown a tendency of these parameters to strongly influence model outputs this uncertainty contributes to greater uncertainty in model outputs in the absence of additional data or model modification the process of determining values for these parameters should then be guided by calibration and the analysis of outputs must consider uncertainty in the high influence high uncertainty parameters higgins et al 2003 acknowledgements this research was supported in part by the national science foundation harvard forest long term ecological research program grant no nsf deb 12 37491 and the national science foundation reu site a forest full of big data the harvard forest summer research program in ecology 2015 2019 grant no nsf dbi 14 59519 we thank meghan maclean and eric gustafson for preliminary feedback on this manuscript appendix i method of morris results we performed mom alongside fast in order to 1 corroborate conclusions about relative parameter influences over the total biomass output with estimates of total effects 2 determine the direction of influence of each parameter and 3 investigate the summed effects of parameter interactions and nonlinearities mom is different from fast in that it estimates a total effects index for each model parameter total effects indices differ from the main effects of fast in that they incorporate interactions with other parameters our mom analysis sampled 40 one at a time paths r for the 20 parameters k for a total of r k 1 40 20 1 840 model runs the upper and lower bounds for the mom sampling of each parameter were the same as those used in fast tables 2 and 3 all analysis was done in r using the mom functions included in the package sensitivity the mom analysis like the fast analysis was performed at different time points through the simulations the raw output at each time point consisted of 40 elementary effect indices for each of the 20 parameters a 40 20 matrix the mean of the elementary effects for each parameter gives the estimated total effects μ the sign of the mean of the elementary effects the average directional effect of each parameter and the standard deviation of the elementary effects σ indicates the combined interactions and nonlinearity for the parameter additionally the mean of the absolute values of the elementary effects gives an absolute total effects index μ that better accounts for the overall influence of parameters that can have positive and negative influence on the output campolongo et al 2007 figure a1 shows each of these values from our mom at 5 time points for each parameter figure a1 sensitivity results from the mom analysis the parameters are ranked down the y axis decreasing by average μ the leftmost panel shows μ the absolute total effects index for each parameter the middle panel shows μ the estimated total effects index for each parameter values below zero indicate that the parameter has a negative relationship with total biomass outputs while values above zero indicate that the parameter value has a positive influence on total biomass outputs the rightmost panel shows σ the estimated value of combined interactions and nonlinearities associated with each parameter figure a1 appendix a supplementary data the following is the supplementary data to this article multimedia component 1 multimedia component 1 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2019 03 002 declaration of interest the authors have no competing interests to declare 
26210,forest landscape models flm are widely used for simulating forest ecosystems as flms have become more mechanistic more input parameters are required which increases model parameter uncertainty to better understand the increased mechanistic detail provided by landis ii pnet succession we studied the effects of parameter uncertainty on model outputs based on three different approaches global sensitivity analyses summarized the influence of each parameter a local sensitivity analysis determined the magnitude of and degree of nonlinearity of variation in model outputs alongside variation in individual parameters and a regression tree analysis identified hierarchical relationships among and interaction effects between parameters foliar nitrogen maintenance respiration and atmospheric carbon dioxide concentration were the most influential parameters in the global analysis knowing where parameter influence is concentrated will help model users interpret results from landis ii pnet succession to address ecological questions and should guide priorities for data acquisition keywords uncertainty landis ii pnet forest landscape model fourier amplitude sensitivity test regression tree 1 introduction forest landscape models flms are a class of spatially interactive stochastic simulation models that are widely used by researchers and natural resource managers to project long term and broad scale changes to forested landscapes shifley et al 2017 flms are increasingly used to simulate the effects of global change drivers such as climate and land use change on future forest composition and function duveneck and thompson 2017 liang et al 2017 because global change scenarios are complex and often without observed precedent flms have become more mechanistic regarding the major drivers of forest change with steps toward inclusion of the physiological responses to changing temperature moisture atmospheric conditions fire and defoliation gustafson 2013 while mechanistic approaches for simulating global change offer a strong foundation for simulating emergent and novel conditions they require many more input parameters than did their statistically based predecessors the presumed increase in conceptual robustness gained by using a mechanistic model could potentially be undermined by increased parameter uncertainty associated with a lack of empirical data and or mechanistic understanding of each of the model coefficients cuddington et al 2013 dietze 2017 a sensitivity analysis sa can be used to quantify the impact of model input parameters on model outputs saltelli et al 2000 by systematically varying input parameters and quantifying the relationship of this variation to the resulting variation in model outputs a sa identifies the parameters that are most influential over model outcomes as well as the critical regions in which parameter values have a disproportionate effect on the model output saltelli et al 2006 thus while guiding model use a sensitivity analysis can also prioritize future research by suggesting which ecological parameters require more accurate estimation from an ecological perspective sensitivity analyses may also be useful for identifying dominant mechanisms in the behavior of the model and thus can improve our understanding of how the modeled system functions reusser et al 2011 additionally sensitivity analyses can reveal parameter interactions and correlations and they can simplify models by suggesting which input parameters are less relevant saltelli et al 2006 there are two broad approaches for conducting sensitivity analyses local sensitivity analysis lsa and global sensitivity analysis gsa each approach has strengths and weaknesses lsa also called one at a time sensitivity analysis is relatively simple to implement and has been widely used to evaluate flms sturtevant et al 2009 thompson et al 2011 xu et al 2009 in a lsa all parameters are kept constant at their mean or another predefined reference value while a single parameter is varied within a specified range this process is repeated for all of the parameters of interest and the absolute and or relative influences on a given response variable are compared the strength of lsa is that it is easy to perform and straightforward to interpret because just one input parameter is varied for each test however interpretations from lsa are limited when several parameters might interact dynamically through the range of individual values campolongo et al 2007 saltelli and annoni 2010 to capture model sensitivity throughout all of input parameter space gsas are required in gsa all parameters are varied simultaneously gsa is more robust than lsa for summarizing parameter effects in complex models because it determines parameter sensitivities across large regions of parameter space reusser et al 2011 there are many types of gsa including variance based tests like the fourier amplitude sensitivity test fast sobol s method the derivatives based morris method and various regression based approaches sobol s method is an extensive global sensitivity analysis it calculates total sensitivity indices for each parameter including influence due to interaction effects sobol 1993 however sobol s method relies on a search function that requires many model runs and is therefore not computationally feasible for highly complex models fast like sobol s method is a variance based global sensitivity analysis summarizing the effect of each parameter on model outputs cukier et al 1975 1973 mcrae et al 1982 fast varies all tested input parameters simultaneously through the full input parameter space and then uses fourier transforms to identify the degree to which each parameter is responsible for variation in model outputs fast relies on a characteristic frequency of variation assigned to each input parameter to estimate sensitivity indices this allows fast to be much more efficient for calculating main effects than comparable methods such as sobol s method saltelli and bolado 1998 a variation of fast known as extended fast efast also exists for calculating total effect i e including interactions indices for each parameter saltelli et al 1999 however the total effect index for each parameter comes at the cost of many more simulations another type of sensitivity analysis similar to fast is the method of morris mom which randomizes one at a time testing in a structured framework for global parameter screening morris 1991 like fast mom assigns indices to parameters indicating their global influence additionally mom indicates the directionality of each parameter s influence and it returns an index for each parameter that corresponds to the combination of its non linear effects on model outputs and its interactions with all other parameters wainwright et al 2014 however the numeric values of mom indices are specific to the units of the model outputs this is a contrast with fast whose indices represent the proportion of variance explained additionally while mom returns an index corresponding to the combined interactions and nonlinear effects of each parameter it cannot distinguish between interactions and nonlinearities or determine where they occur in parameter space brevault et al 2013 beyond fast and mom classification and regression trees have been used in predictive modeling and more recently as methods of global sensitivity analysis almeida et al 2017 breiman et al 1984 cutler et al 2007 iverson and prasad 1998 pappenberger et al 2006 because classification and regression tree models are defined by a hierarchy of split input parameters fitted regression trees can identify regions in the ranges of individual input parameters where they interact with other input parameters in this way classification and regression trees can supplement global sensitivity analyses that either exclude interaction effects e g fast or return a single value per parameter that corresponds to the summed interactive and nonlinear effects of that parameter e g mom sobol s method and efast regression tree models are relatively robust to various sampling schemes especially when compared with fast which requires very specific sampling this allows classification or regression tree analysis to be performed directly on the same data used for a separate global sensitivity analysis until recently the use of tree based approaches has been uncommon in sensitivity analyses and what little work has been done was mostly limited to slope stability and hydrological models almeida et al 2017 singh et al 2014 however a regression tree was used in a study of the landis ii biomass succession extension flm simons legaard et al 2015 landis ii scheller et al 2007 is an flm modeling platform that has been used extensively to simulate landscape scale temperate and boreal forest dynamics duveneck and scheller 2015 duveneck and thompson 2017 kretchun et al 2014 loudermilk et al 2013 landis ii simulates the establishment growth competition and mortality of tree species by age cohorts as they are affected by climate and disturbance seed dispersal and various disturbances are simulated across a landscape as spatially interactive processes scheller et al 2007 landis ii requires users to select a succession extension based on their specific research question previous sensitivity analyses have been completed for the biomass succession extension scheller and mladenoff 2004 thompson et al 2011 scheller and mladenoff 2004 and thompson et al 2011 assessed the sensitivity of six and nine key parameters respectively with lsas varying each parameter by 10 they found that biomass succession was not overly sensitive to any one of the parameters tested but that the model outputs were most sensitive to the parameters specifying the maximum allowable biomass and maximum annual net primary productivity npp xu et al 2009 completed a fast global sensitivity analysis with biomass succession using different climate variables including temperature temp precipitation precip atmospheric co2 co2 and photosynthetically active radiation par of the climate variables they found forest composition to be most sensitive to temp followed by par co2 and precip simons legaard et al 2015 conducted a global and temporal sensitivity analysis of nine key parameters of biomass succession and also found that maximum allowable biomass and maximum annual npp were most influential on predicted biomass in addition they found that temporal variation and interactions between parameters influenced the biomass output partly in response to the influence of the maximum allowable biomass and maximum annual npp parameters of biomass succession both of which are phenomenological and without mechanistic basis de bruijn et al 2014 developed the pnet succession extension replacing these parameters with a more mechanistic approach to simulating biomass accumulation gustafson et al 2014 pnet succession de bruijn et al 2014 incorporates algorithms of the pnet ii ecophysiological model aber et al 1995 which uses first principles of photosynthesis to simulate competition for resources within vertical canopy layers because landis ii tracks biomass rather than canopy height variation in the biomass of cohorts within a cell is used to define distinct canopy layers incoming radiation is allocated to the separate layers to simulate extinction and competition for light photosynthetic production is allocated to foliage wood roots and reserves non structural carbon pnet succession has been evaluated at several sites de bruijn et al 2014 duveneck et al 2017 gustafson and sturtevant 2013 and has been used to answer a variety of research questions duveneck and thompson 2019 2017 a local sensitivity analysis by gustafson et al 2017a explored the sensitivity of model outputs to six climate related input parameters to demonstrate the advantage of employing first principles to predict forest responses to climate change the analysis quantified the relative effects of climate parameters with directionality and extended the lsa by varying multiple parameters simultaneously highlighting the interactions between parameters however the analysis did not directly compare the absolute effects of parameters to one another and it excluded several pnet succession parameters not related to climate that which affect forest growth to build upon the recent work by gustafson et al 2017a we describe the results of a three tiered sa that attempts to harness the strength of the sa approaches discussed above as applied to the pnet ii succession extension used within the landis ii v 6 1 flm framework for our initial gsa we selected 20 parameters that had previously demonstrated influential behavior table 1 the objective of this study was to identify the variables most influential in determining the outputs of landis ii pnet succession across time and to identify both interactive effects among parameters and nonlinear responses of model outputs to variation in individual parameters identifying highly influential parameters will help model users understand the drivers of model outputs and will help prioritize areas for model refinement our hybrid approach employs three separate analyses a global sensitivity analysis a one at a time local sensitivity analysis and regression tree analysis and leverages the strengths of each to quantify the sensitivity of two types of model response variables to variation in input parameters offering users a more complete understanding of model responses to variation in model inputs this approach also provides a replicable framework for assessing parameter uncertainty and for prioritizing data collection to improve the accuracy of model predictions 2 methods 2 1 approach using landis ii v 6 1 and pnet succession v 1 0 0 we simulated an artificial 25 by 25 pixel grid landscape totaling 625 cells each representing 6 ha of forested land for 200 years with five year time steps we populated the landscape with 50 unique initial communities i e species age cohort mixes sampled from the fia database bechtold and patterson 2005 in new england usa these initial communities were made up of 26 tree species each of which is common among temperate forests of the northeastern united states table 2 our general approach to conduct sensitivity analyses was as follows fig 1 to summarize the influence of each parameter table 1 on selected response variables through the full parameter space of the model we began the sensitivity analysis using the mom and a fast gsa see global sensitivity analysis details below the mom and fast analyses each ranked parameters and mom indicated directionality and the combined effects of nonlinearity and interactions for each parameter from the fast analysis we evaluated the total proportion of variance explained by each parameter at each five year time step of the model we also conducted hierarchical partitioning of biomass outputs at years 0 and 200 chosen to capture temporal differences in parameter influence in order to determine the break points in the influence of each interacting parameter see regression tree analysis details below finally we identified the most influential parameters from the results of the global analyses and conducted a lsa of each of those parameters on biomass and area occupied by specific tree species to detect nonlinear relationships between input parameters and their effects on model outputs see lsa details below our sensitivity analyses examined variation in two types of outputs total landscape aboveground biomass and the number of landscape cells occupied by each of three dominant new england species red maple acer rubrum red oak quercus rubra and white pine pinus strobus aboveground biomass is the forest growth currency in landis ii and cell occupancy of individual species defines forest composition spatially taken together changes in aboveground biomass and forest composition are useful metrics to capture forest growth and successional responses to disturbance and are therefore appropriate response variables for our sensitivity analysis duveneck et al 2017 we limited our parameter selection to twenty based on what was computationally feasible we selected parameters that were newly introduced with the pnet succession extension or were linked to the photosynthetic mechanisms targeted by the extension parameter selection was motivated by either a parameter s known importance to the model or by large uncertainty about the input value for the parameter the selected parameters were chosen to lead to a better understanding of both the model and the specific contribution of each parameter to output variance some parameters tested foliar nitrogen foln maximum specific leaf weight slwmax optimal temperature for photosynthesis psntopt and the fraction of the amount of active woody biomass that is allocated to foliage per year fracfol were species specific and we jointly varied these linearly across species the values sampled by fast for these parameters ranged from 0 to 1 where 0 meant that each species sampled parameter value was 90 of its default value and 1 meant that it was 110 of its default value for each species the foln range for each species was set to include the middle 90 of values from the nerc foliar chemistry database table 2 northeastern ecosystem research cooperative 2016 and was also varied linearly within those bounds climate parameters average monthly maximum and minimum temperature values temp monthly carbon dioxide concentration co2 monthly precipitation precip and monthly photosynthetically active radiation par were varied across the range predicted from 2000 to 2100 provided by the regional concentration pathway 8 5 emission scenario ipcc 2013 as simulated by the hadley global environment model v 2 earth system global circulation model downscaled for new england obtained from the usgs geo data portal stoner et al 2013 for these parameters we sampled from within the range of average annual climate values a single value per parameter after sampling this average annual parameter value e g the average temperature for the year we then parameterized the model using the monthly values corresponding to the year from the data with the closest average annual value e g the year with the closest average temperature to the sampled value was chosen and then the monthly values were extracted from that year s data table 3 for temp an overall average for each year was used for sampling then the monthly maximum and minimum temperature values tmax and tmin required by pnet landis ii were selected together from the data for the year whose average temperature was closest to the sampled value the sampled climate parameters were held constant across years in each simulation other parameters were not species nor temporally specific and were composed of a single value for a given scenario e g maintenance respiration maintresp and the proportion of non structural carbon to be maintained when net photosynthesis exceeds maintresp dnsc these values were varied within their limits as defined by the pnet succession user manual or expert knowledge of the model table 3 gustafson et al 2017b 2 2 global sensitivity analyses we performed the fast analysis using the r package fast r development core team 2006 reusser et al 2011 fast is a variance based sensitivity analysis that calculates the main effect i e excluding interactions with other parameters of each parameter on variation in model outputs fast generates parameter samples to be used as simulation inputs within the bounds of a user defined minimum and maximum value for each parameter by assigning each parameter a characteristic frequency each parameter is systematically varied at this frequency within the bounds defined by the maximum and minimum values across all of the parameter samples and the model is then run on each set of parameter samples afterward a fast fourier transform is applied to observed variation in model outputs to produce a power spectrum the values from the first four multiples of the characteristic frequencies for each parameter are summed and divided by the total summed power spectrum across all frequencies to calculate the main effect sensitivity indices cukier et al 1975 we generated 8378 samples four times the minimum number required by the nyquist criterion i e the function frequency limits and the corresponding discrete sampling rate required to describe a continuous function see jerri 1977 to ensure convergence mcrae et al 1982 saltelli and bolado 1998 we then used each parameter sample to conduct a landis ii simulation to capture variation in behavior through the successional trajectories experienced over time we calculated fast sensitivity indices at each 5 year time step of the simulations we performed mom using the r package sensitivity to rank parameters in order of importance to determine directionality of parameters and to estimate the extent to which each parameter interacts with other parameters or affects outputs nonlinearly iooss et al 2015 r development core team 2006 in contrast to fast which calculates main effect indices mom estimates total effect indices for the model parameters which include influence over model outputs due to interactions with other parameters we designed our mom analysis to sample 40 one at a time paths r for the 20 parameters k for a total of r k 1 40 20 1 840 model runs in addition to those by fast the minimum and maximum values constraining parameter sampling were the same as those used in fast described above and listed in tables 2 and 3 mom returns a single elementary effect for each parameter for each path which summarizes the influence of that parameter through the individual path the mean of the elementary effects μ is used to calculate the total effect index for each parameter the sign of the mean of the elementary effects then corresponds to directionality of the parameter s influence with the exception of when the parameter has both positive and negative effects on model outputs in which case the sign of the individual elementary effects would have to be analyzed campolongo et al 2007 king and perera 2013 the mean of the absolute values of the elementary effects for each parameter μ has also been used to summarize the parameter s global influence over a particular output while incorporating non monotonic behavior campolongo et al 2007 finally the standard deviation of the elementary effects σ around the mean μ corresponds to the combined nonlinearity and interactions associated with each parameter 2 3 regression tree analysis we performed a regression tree analysis rta using the party package in r hothorn et al 2006 rta is a non parametric technique for recursively partitioning a continuous variable into increasingly homogeneous subsets where the partitions are identified by testing all potential partitions across all values of all the predictor variables then selecting the partition that maximizes the difference between groups de ath and fabricius 2000 rta results in a dendrogram that shows the hierarchical relationships among predictors and between predictors and the response the same samples developed for the fast analysis were reused with biomass outputs from years 0 and 200 as the response variables the specific rta implementation within the party package is called conditional inference trees which requires a significant difference p value 0 05 as determined from a monte carlo randomization test in order to create a partition in the regression tree this technique minimizes bias and prevents over fitting and the need for regression tree pruning de ath and fabricius 2000 hothorn et al 2006 hothorn and zeileis 2015 2 4 local sensitivity analysis finally we conducted a lsa to detect the magnitude of each parameter s influence at a point in parameter space in model output units rather than relative to variance in outputs from other parameters and to determine whether landis ii outputs respond nonlinearly to linear variation across each parameter s range like the mom the lsa results also indicate directionality and magnitude of parameter influence we performed an lsa on the top four influential parameters identified in the fast analysis for each type of output biomass and landscape composition we performed 24 total simulations for the local sensitivity testing on each output type simulations were performed at six evenly spaced values across the parameter s possible range while all other input parameters were held at their calibrated values for each of the four parameters tested in order to assess variation over time output values were assessed at each five year time step for each simulation 3 results 3 1 global sensitivity analyses based on the fast results maintresp was the most influential parameter for determining total landscape biomass output by the model across a 200 year simulation fig 2 a although foln was more influential for determining total biomass than maintresp during model initialization year 0 the influence of foln for determining total landscape biomass declined quickly after year 0 giving foln a slightly lower average index value than that of maintresp other influential parameters included the climate parameters co2 temp and precip and precipitation loss fraction preclossfrac which determines the proportion of water that does not enter the soil the fast analysis also indicated that the same parameters to which biomass outputs are sensitive tend to be important in determining species composition fig 2 b c d the three species differed somewhat regarding the relative influence of the most important parameters for determining species cell occupancy with foln explaining more variance in cell occupancy of quercus rubra than in the other two species however the same parameters were influential for determining cell occupancy for each of the three species foln maintresp dnsc the amount of woody biomass capable of supporting foliage fractwd and sensitivity of establishment to light estrad climate parameters were not influential in determining species composition contrary to our results for outputs which are directly related to growth for both total landscape biomass and species cell occupancy tests par establishment parameters rooting depth rootingdepth and disturbance frequency wrp were not identified as being very influential temporal variation in the fast sensitivity index for each parameter indicated how the relative influences of particular variables on model outputs changed through time as noted above total landscape biomass was especially sensitive to foln early in each simulation where the parameter s variation accounted for one third of the variation in output landscape biomass after 200 simulation years variation explained by foln only accounted for ten percent of output variance in contrast maintresp explained ten percent more variation in total landscape biomass by the end of simulations than at the beginning the change in relative influence of the two variables is a result of increasing biomass in the simulations which is associated with increases in the contribution of maintresp temporal variation was observed in the fast results for cell occupancy as well where the influence of foln increased with time from year 0 to year 200 the influence of foln on quercus rubra occupancy increased from approximately thirty percent to explaining over half the variation in cell occupancy by year 200 building on the results from fast the mom analysis estimated total effect sensitivity values for each parameter when ordered by absolute valued indices μ the parameter rankings of mom closely mirror the rankings of the fast analysis table 4 additionally the sign of mean elementary effects for each parameter indicated directionality for the influence of the parameter figure a1 the large standard deviations σ of the sets of elementary effects for the input parameters also suggested a substantial amount of combined nonlinearity and or interactions between parameters see appendix i for a more detailed explanation of the mom results 3 2 regression tree analysis while we performed the rta across time slices of the model we found that most of the relevant information offered by the rta was contained within the analysis at year 0 and at year 200 which are presented here figs 3 and 4 the rta of total biomass accumulated during model initialization year 0 partitioned the simulations into 12 terminal nodes which are shown as boxplots in fig 3 each boxplot shows the distribution of year 0 biomass within the subset of simulations in that branch of the regression tree consistent with the findings of fast the rta shows that foln co2 and maintresp explain much of the variation in the year 0 biomass indeed of the 11 partitions that the rta identified 10 came from those three variables temp entered as a significant variable only in the relatively small subset of simulations n 1664 where foln and co2 were high and maintresp was low the first partition at the top of the rta was based on whether foln was greater or less than 44 of its potential range indicating foln as the most significant predictor variable overall not surprisingly higher values of foln were associated with greater biomass working down both sides of the tree for high and low values of foln the next splits were based on whether co2 concentration was greater or less than 620 ppm higher values of co2 were associated with higher levels of biomass at this level in the regression tree the analysis has partitioned four groups of simulations that represent high low foln and high low co2 the corresponding values of biomass in these simulations range from 14 449 g m2 with low foln and low co2 to 29 661 g m2 with high foln and high co2 below this level on the regression tree the predictor variables diverge within the branch with high foln the right side of the tree maintresp was identified as the next most predictive for both the high and low co2 groups within the branch with low foln the left side of the tree foln was again identified as the most predictive for both high and low co2 groups overall by the bottom of tree and thus at the point where rta cannot identify any additional significant partitions the simulations with low maintresp 0 003 and the highest levels of foln 0 7 had the highest average year 0 biomass the lowest levels of year 0 biomass occurred when foln was below 0 2 and co2 was below 620 ppm by year 200 of the simulations foln co2 and maintresp were still identified as the most influential variables and thus they represent the most common partitions in the tree fig 4 however their relative importance had shifted at year 200 the first partition is on maintresp followed by co2 then foln fig 4 like time 0 the rta partitioned co2 in the mid 600s working down the side of the tree with higher values of maintresp the climatic variable temp entered as a significant predictor overall after 200 years simulations where maintresp was 0 003 co2 concentrations were 606 ppm foln was 0 43 and towood was 0 01 had the highest average biomass 3 3 local sensitivity analysis results from the lsa showed that simulations at calibrated values responded to variation in selected most influential input parameters as expected relative to each other given the results of the gsa and rta analyses the local analysis supported the direction and magnitude of the influence of each parameter on total biomass fig 5 and species occupancy fig 6 both of which had been indicated by gsa while increasing maintresp and temp inputs decreased landscape biomass increasing foln and co2 inputs increased landscape biomass additionally the lsa revealed particular nonlinear effects in the range of variation for each parameter these effects had been captured in the standard deviation of elementary effects from the mom gsa but mom does not discriminate between interactions and non linearities in contrast the lsa can identify regions of nonlinearity across points in parameter space but cannot identify interactions for example while the change in total biomass corresponded nearly linearly with change in maintresp at year 200 the model showed much higher sensitivity to foln in the middle of the parameter s range than near the lower and upper limits of its range 4 discussion our primary objective in this study was to identify the most influential parameters within a mechanistically based flm i e the coupled landis ii pnet succession framework de brujin et al 2014 developed this succession module with the goal of moving away from earlier succession modules within landis ii which were largely phenomenological and whose most influential parameters were not explicitly linked to processes that govern tree growth and forest development and are essentially unknowable outside of some limited range of observations given that the goal of pnet succession was to develop a module driven by eco physiological first principles it is not surprising that the most influential parameters we identified were foln maintresp and co2 all three parameters are closely linked to the mechanisms of tree growth and survival this contrasts with sensitivity analyses of the biomass succession extension to landis ii scheller and mladenoff 2004 which found that maximum allowable biomass and maximum annual npp were most sensitive simons legaard et al 2015 thompson et al 2011 in this context the pnet model represents significant progress toward an flm that can be reasonably expected to simulate forest processes under future conditions of climate and atmospheric chemistry that have not been observed in the past nonetheless each of these parameters has distinct limitations for use in an flm and present challenges for model users the importance of foln as identified by our global sa reflects the original design of the original pnet family of models developed in the 1990s aber et al 1997 1995 ollinger et al 1998 all of which use foln to represent n availability and to estimate gross photosynthesis in the context of landis ii pnet succession this is consistent with the developers intentions to have broad scale mechanistic underpinnings within the model and reflects the fact that foliar nitrogen concentrations are strongly linked to rates of photosynthesis and ecosystem scale carbon assimilation across forests worldwide reich et al 1997 from the perspective of the user of landis ii pnet succession there are many published studies and public databases documenting foliar n that can aid parameterization such as the foliar chemistry database northeastern ecosystem research cooperative 2016 which we used here within landis ii pnet succession foln is a species specific parameter which results in differential growth rates among species and allows users to simulate within stand competition which is an advancement over the earlier lumped parameter versions of pnet ii while the model allows foln to vary by species foln cannot vary over space nor through vertical layers of a canopy in the current implementation neglecting spatial variability in foliar n and thus n availability is a limitation of the model correlational studies using broad scale remote sensing data like that from the airborne visible infrared imaging spectrometer aviris and the moderate resolution imaging spectroradiometer modis have described the potential use of these datasets for mapping the spatial variation of foln in temperate forests lepine et al 2016 ollinger et al 2008b and modifications to pnet succession are also being tested to allow foln to vary vertically in response to light availability gustafson et al 2018 our results suggest that developing the model s capability to make foln more dynamic rather than as a species constant is a high priority for management and could greatly reduce uncertainty in model results maintresp was also among the most influential parameters in our sensitivity analyses maintresp is the amount of non structural carbon lost to maintenance respiration each month and is used in the calculation of cohort npp and mortality maintresp is affected by temperature de bruijn et al 2014 and is thus consistent with the model s more mechanistic underpinnings and utility for modeling forest change in future climates however the formulation of maintresp is greatly simplified as compared to the respiration calculations in the original version of pnet ii aber et al 1995 and it is applied universally across all species this formulation was required to achieve computational tractability gustafson et al 2017b the consequence of these simplifications is that maintresp is not an empirically measurable value and has a definition that does not exist outside the model from the user s perspective maintresp is effectively a tuning parameter while we see no obvious alternative having a tuning parameter as among the most sensitive in the model is challenging from the perspective of developing an flm based on mechanisms that are expected to perform predictably including under conditions that have not been observed and thus not tuned to future improvements to the model should incorporate maintenance respiration requirements to growth that can be measured and tested preferably at a landscape scale co2 was the other parameter that our sa analyses identified as highly influential greater co2 concentrations result in increased water use efficiency and increased rates of photosynthesis and atmospheric co2 concentrations are expected to continue to rise even further to unprecedented levels keenan et al 2013 our findings support other studies using the pnet family of models including ollinger et al 2008a which found co2 to be more influential than temperature in predicting changes to npp over multiple climate change scenarios throughout the 21st century the long term effects of increasing co2 on forest growth and demographics remain highly uncertain and comprise an area of rapidly developing research norby et al 2010 schimel et al 2015 given this the model s high sensitivity to co2 should be viewed with a level of caution commensurate with the uncertainty in the broader scientific understanding like maintresp and foln co2 s effect on the model should continuously be evaluated as improvements are made to the understanding of the physiological effects of co2 on forest growth using a hybrid methodology of combining gsa rta and lsa to assess output sensitivities provided unique insights about landis ii pnet succession pianosi et al 2016 the main effect indices from the fast analysis allowed us to understand the output variation driven by each parameter tested within the gsa additionally the mom analysis returned sensitivities corresponding to the total effects of each parameter the rankings of parameters under the mom results were similar to those of fast attesting to the robustness of our results however despite their similarity in this study fast and mom results are not directly comparable as they are estimating different values main effect indices for fast and total effect indices for mom depending on the scale of interactions though they might often yield similar rankings as we found in this study mom and lsa indicated the directionality of each parameter s influence on outputs and mom indicated the extent to which each parameter interacted with other parameters and or exhibited nonlinearity for example mom and lsa showed that increasing foln resulted in greater biomass which is consistent with the known role of foliar nitrogen in determining the maximum rate of carboxylation and the maximum rate of electron transport during photosynthesis walker et al 2014 likewise increasing maintresp reduced total landscape biomass because high ratios of respiration rate to photosynthetic rate are known to deplete carbon stores van oijen et al 2010 to then examine in detail how the influence identified by gsa methods is partitioned rta allowed us to identify parameter break points and specific parameter interactions in part the partitioning by rta reflected the parameter rankings from the other analyses additionally we can visually inspect the output variation in the box plots in figs 3 and 4 alongside the parameter variation that produced it to identify interactions for example the rta output at year 200 shows co2 contributing to greater variation at low values of maintresp than at high values of maintresp and towood appears to contribute substantially to variation in outputs when foln and co2 values are high and when maintresp is low finally the lsa allowed us to examine the nonlinearity of the effects of individual parameters on model outputs at a particular part of parameter space 5 conclusions while each sa method offered additional information about the influence of input parameters on model outputs they also overlapped significantly in their conclusions fundamentally all methods supported foln maintresp and co2 as being highly influential this combination approach provided a more robust analysis and associated conclusions than if we had used just one of these methods alone by using all four methods fast mom lsa and rta in consort we were able to assemble a more complete picture about the influence of each parameter and ultimately a greater understanding of landis ii pnet succession pianosi et al 2016 we have presented a thorough analysis of the sensitivity of landis ii pnet succession outputs to variation in twenty input parameters our work here builds upon that of gustafson et al 2017a and where our analysis overlapped our results were consistent specifically the directionalities of the effects of temp and precip parameters on biomass in this study were the same as shown in their sensitivity analysis while the magnitude of the response of biomass to variation in temp versus precip in gustafson et al 2017a suggested that precip was a stronger driver of biomass than temp in their simulations our analysis instead suggests that temp has slightly more influence than precip over biomass outputs this discrepancy is likely due to differences in the sampling of parameter space the local analysis performed by gustafson et al 2017a was not designed to compare the relative influences of parameters to each other and instead aimed to capture individual parameter directionality and parameter interactions as such they only varied temp and precip values over limited ranges on the other hand our analysis was specifically designed to sample each parameter throughout its possible input range allowing us to compare the overall importance of parameters for determining model output values we selected proportional ranges of variation for climate variables and tested the influence of all input parameters through the space of variation of the other parameters which should make comparing the relative influence of parameters more reliable we evaluated the area within a landscape occupied by three species identified the parameters that contribute most to variation in model outputs and described their effects our sensitivity analysis showed little similarity between the effects of input parameters on different response variables with total landscape biomass being driven primarily by parameters directly influencing carbon assimilation such as foln and maintresp and with cell occupancy being driven more by parameters associated with carbon allocation strikingly the sensitivity of cell occupancy to each of various input parameters was highly variable between species this highlights the known danger of extrapolating the results of a sensitivity analysis on one model variable to different response variables as demonstrated in previous studies to understand the sensitivity of a specific model output to various input parameters that output must be individually tested rosolem et al 2012 van werkhoven et al 2008 another important feature of our analysis was implementing the sa across time steps as in reusser et al 2011 our analysis showed that some parameters like foln in the landscape biomass analysis exert much more influence early in the simulations than in later time steps others like maintresp may become more influential through time across temperate forests the influential parameters foln co2 temp and precip are relatively well supported by data for estimating their values however data supporting other important parameters maintresp fractwd dnsc and estrad are less easily estimated despite the ecological realism of these variables given that our analysis has shown a tendency of these parameters to strongly influence model outputs this uncertainty contributes to greater uncertainty in model outputs in the absence of additional data or model modification the process of determining values for these parameters should then be guided by calibration and the analysis of outputs must consider uncertainty in the high influence high uncertainty parameters higgins et al 2003 acknowledgements this research was supported in part by the national science foundation harvard forest long term ecological research program grant no nsf deb 12 37491 and the national science foundation reu site a forest full of big data the harvard forest summer research program in ecology 2015 2019 grant no nsf dbi 14 59519 we thank meghan maclean and eric gustafson for preliminary feedback on this manuscript appendix i method of morris results we performed mom alongside fast in order to 1 corroborate conclusions about relative parameter influences over the total biomass output with estimates of total effects 2 determine the direction of influence of each parameter and 3 investigate the summed effects of parameter interactions and nonlinearities mom is different from fast in that it estimates a total effects index for each model parameter total effects indices differ from the main effects of fast in that they incorporate interactions with other parameters our mom analysis sampled 40 one at a time paths r for the 20 parameters k for a total of r k 1 40 20 1 840 model runs the upper and lower bounds for the mom sampling of each parameter were the same as those used in fast tables 2 and 3 all analysis was done in r using the mom functions included in the package sensitivity the mom analysis like the fast analysis was performed at different time points through the simulations the raw output at each time point consisted of 40 elementary effect indices for each of the 20 parameters a 40 20 matrix the mean of the elementary effects for each parameter gives the estimated total effects μ the sign of the mean of the elementary effects the average directional effect of each parameter and the standard deviation of the elementary effects σ indicates the combined interactions and nonlinearity for the parameter additionally the mean of the absolute values of the elementary effects gives an absolute total effects index μ that better accounts for the overall influence of parameters that can have positive and negative influence on the output campolongo et al 2007 figure a1 shows each of these values from our mom at 5 time points for each parameter figure a1 sensitivity results from the mom analysis the parameters are ranked down the y axis decreasing by average μ the leftmost panel shows μ the absolute total effects index for each parameter the middle panel shows μ the estimated total effects index for each parameter values below zero indicate that the parameter has a negative relationship with total biomass outputs while values above zero indicate that the parameter value has a positive influence on total biomass outputs the rightmost panel shows σ the estimated value of combined interactions and nonlinearities associated with each parameter figure a1 appendix a supplementary data the following is the supplementary data to this article multimedia component 1 multimedia component 1 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2019 03 002 declaration of interest the authors have no competing interests to declare 
26211,as wireless sensor networks become more prevalent tools for analyzing watershed dynamics the integration and management of the myriad of available data streams presents a unique challenge web based platforms provide tools to retrieve and visualize hydrologic and meteorologic data from various sources while addressing syntactic and semantic differences in data formats however current platforms are limited by a lack of data analysis tools the stream hydrology and rainfall knowledge system sharks app was developed to expand upon existing platforms by providing a suite of exploratory data analysis features including the ability to calculate the total precipitation depth recorded for any period interpolate the average annual recurrence interval for precipitation events perform hydrograph separations and calculate the volume of runoff for any period a case study provides a conceptual description of the types of rapid analysis that can be performed using the sharks app keywords hydrologic monitoring smart watershed environmental observations data visualization data analysis abbreviations ari annual recurrence interval cmb conductivity mass balance 1 introduction as a result of the integration of low cost sensors and wireless communications with web services large sensor networks are increasingly being used in environmental monitoring bartos et al 2018 sensor networks have been implemented in projects including habitat monitoring biagioni and bridges 2002 mainwaring et al 2002 environmental observation and forecasting systems keefer et al 1987 steere et al 2000 forest fire detection hefeeda and bagheri 2009 soliman et al 2010 agriculture kim et al 2008 and glacier research padhy et al 2005 in urban areas sensor networks in smart watersheds can also provide information on watershed dynamics and provide insights to issues such as flooding runoff pollution and degradation of aquatic ecosystems bartos et al 2018 furthermore a new generation of intelligent stormwater networks will allow cities to expand real time monitoring and control of stormwater systems to entire watersheds through the implementation of sensors and dynamic controls bartos et al 2018 kerkez et al 2016 lefkowitz et al 2016 mullapudi et al 2017 muschalla et al 2014 as hydrologic sensor networks become more prevalent the effective integration and management of the multitude of data streams including those from government commercial and local sources to produce coherent results presents a unique challenge synthesizing data from different sources can be difficult because each source can have a different way of navigating through pages menus and files to access the data furthermore syntactic and semantic differences in data formats can make it difficult to find organize and interpret data horsburgh et al 2009 finally data retrieval and processing can be hindered by the sheer quantity of available data vitolo et al 2015 several platforms have been created to gather and visualize hydrologic data from various sources notably the consortium of universities for the advancement of hydrologic science inc cuahsi hydroclient http data cuahsi org accesses hydrologic data from over 95 sources stored in the cuahsi hydrologic information system his this system was developed to address syntactic and semantic heterogeneity in environmental data by the use of a standard observations data model odm database format for data storage and the wateroneflow web services for data communication horsburgh et al 2009 the cuahsi hydroclient also provides a basic data series viewer to plot datasets a related platform the time series analyst tsa http data iutahepscor org tsa is able to retrieve any dataset published to the cuahsi his and provides more advanced tools for data visualization including a map based interface faceted filters and the ability to display data using time series box and whisker or histogram plots furthermore the tsa includes the calculation of data summary statistics horsburgh et al 2016 the great lakes dashboard gld platform https www glerl noaa gov data dashboard gld html5 html was specifically designed as a tool to visualize and download datasets from the north american laurentian great lakes and displays plots for a multitude of different time series datasets smith et al 2016 finally the virtual observatory and ecological informatics system voeis data hub https voeis msu montana edu provides a suite of tools for users to compile manage visualize and redistribute datasets the voeis data hub allows users to upload data or retrieve data from networked sensors and includes qa qc capabilities furthermore the voeis data hub includes data access restriction features and allows users to publish data to a cuahsi his hydroserver mason et al 2014 despite the advances made by these platforms they are limited by the shortage of data analysis tools a web application called the stream hydrology and rainfall knowledge system sharks app https bigbadcrad shinyapps io sharks was developed to expand upon existing platforms by integrating data analysis tools with data retrieval and visualization capabilities the sharks app is tailored to stormwater managers emergency managers hydrologists and or meteorologists that need to understand in real time and for retrospective analysis aguilar et al 2019 what is the location intensity and average annual recurrence interval of rainfall bartos et al 2018 what is the corresponding depth discharge in streams and large storm drains bhaskar and welty 2015 what is the weather forecast biagioni and bridges 2002 what is the proportion of precipitation leaving a watershed as runoff bonta et al 2003 how does water quality change during events the app ties together data from multiple syntactically different sources and provides immediately useful information for decision making and retrospective study e g water balance h h modeling the web based interface of the sharks app was designed with the goal of providing an accessible platform for users possessing an understanding of basic hydrology although the sharks app was conceptualized and implemented to perform data analysis for the city of roanoke virginia the app was designed to also retrieve and analyze data for other locations this paper describes the sharks app workflow and presents a case study demonstrating how the sharks app can be used to provide valuable insights to urban hydrologic processes analysis features were chosen for inclusion in sharks based on needs identified by city of roanoke staff and sharks developers the information needed from the app fit into two broad categories 1 rapidly available rainfall and streamflow depth information during intense precipitation events and 2 hydrologic summary information organized systematically for retrospective analysis as a result the design of the app was borne out of the basic need to know report and respond to rainfall e g depth intensity average annual recurrence interval and stream e g depth discharge conditions across the city s service area as well as to use more complex data e g separated hydrographs direct runoff volumes runoff volume coefficients to better understand long term hydrological processes 2 software implementation 2 1 architecture sharks is implemented as a web based application so that users can access the app from a web browser without installing any additional software in addition deploying the app online ensures that all users are using the most recent version of sharks sharks was programmed using the open source r language with the shiny web application framework https shiny rstudio com r and shiny were chosen for development over alternatives such as python and dash because r is generally considered to have superior data visualization features and more sophisticated statistical libraries in contrast python is a general purpose language which facilitates integration among different software components because sharks is focused on data analysis and visualization and was designed to be a stand alone platform the advantages of r s data visualization features were decided to outweigh the advantages of python s ability to integrate with other web components in addition the shiny framework allows developers to create web apps using entirely r code and uses a reactive programming model in which outputs update instantly as inputs are modified thus eliminating the need for messy event handling code chang et al 2018 furthermore shiny includes a library of pre built input and output widgets for controlling apps and displaying outputs r also includes an extensive set of packages and available code that facilitate the retrieval of hydrologic and meteorological data from the united states geological survey usgs and the national oceanic and atmospheric administration noaa finally r and shiny are free and open source so others can modify and adapt the publicly available sharks source code brendel et al 2018 2 1 1 mechanics the sharks shiny framework consists of two components a user interface object and a reactive server function the app s user interface is built primarily of r code but was customized using html css and javascript code directly sharks is controlled using pre built shiny widgets e g date range inputs selection boxes and switches placed in the user interface selections made using the control widgets are passed to the shiny server function which performs the computations and creates output objects e g graphs maps and tables that are passed back to the shiny user interface for display the shiny server is based on the concept of reactivity in which output objects are updated automatically as inputs are changed therefore when users make selections using the control widgets in the sharks user interface the server performs the computations and updates the app outputs immediately unless told otherwise the mechanics of the sharks app are described as follows data is retrieved from sources based on the stations and time series datasets specified using control widgets and joined by date time to create one master data frame the master data frame is structured with separate columns for each time series dataset and rows for each unique date time the various sharks analysis features use and reshape the data from the master data frame as necessary some sharks analysis features are performed automatically and do not require any user input these automated analysis features include performing hydrograph separations section 2 4 1 calculating direct runoff volume depth and runoff volume coefficients section 2 4 2 calculating total precipitation depth and maximum precipitation intensity and plotting hyetographs and hydrographs due to the reactive shiny framework if users make changes using the user interface control widgets then these analyses are re executed and their user interface outputs are updated automatically other sharks analysis features calculating the storm average annual recurrence interval calculation section 2 4 4 the interactive maps section 2 4 5 and the interactive plots section 2 4 6 require user input when users click on these interactive analysis features the user inputs e g date time range selected on the interactive plots are passed to the server to query data from the master data frame and perform any necessary computations before updating the user interface outputs 2 1 2 implementation the sharks app has been implemented for the city of roanoke a medium sized urbanized area in southwest virginia in order to more effectively manage the quality and quantity of stormwater runoff from city watersheds and to improve compliance with the city s municipal separate storm sewer system ms4 permit and total maximum daily load tmdl requirements the city has installed a number of hydrologic and water quality instruments throughout the service area these instruments provide the empirical basis for data driven capital improvement spending and will also allow for the evaluation and iterative adjustment of watershed improvements over long periods of time i e adaptive management the instruments installed throughout the city s service area summarized in table 1 provide observations of hydrology and water quality at sub hourly time steps which creates a wealth of high quality environmental data for management and decision making however this also presents a data management problem because of the large and always growing amount of raw data in addition semantic and syntactic differences between the data sources make it challenging to join and organize datasets because they may use different codes for the same parameter e g p01i vs 00045 for precipitation or use the same name for multiple parameters e g using temperature to describe both air temperature and water temperature encode data using different file types and have different ways of structuring data the sharks app provides the necessary integration of data streams into a dynamic user interface so that city stormwater managers emergency managers hydrologists and or meteorologists can easily gather information and requisite analysis and communicate it to the necessary stakeholders the sharks app was also implemented as a pre processing tool to collate rainfall and runoff time series data for calibration and validation of a forthcoming hydrologic hydraulic modeling study sharks is currently deployed for the city online at https bigbadcrad shinyapps io sharks using the shinyapps io platform http www shinyapps io hosting sharks online not only ensures that all users are utilizing the most recent version of the app but also allows for remote updates to the app the shinyapps io platform was chosen as a low cost alternative to configuring and maintaining an expensive server for sharks in addition shinyapps io offers scalable hosting plans ranging from a free option to a professional option with increasing performance with higher tier plans the shinyapps io platform hosts sharks on a virtualized server termed an instance which is served by worker processes which service requests to the app rstudio 2015 with the current hosting plan each sharks instance is limited to 1024 mb of memory for computations because r is a single threaded application sharks cannot serve two users at exactly the same time typically this is not an issue because computations take place in the order of tens or hundreds of milliseconds enabling a single r process to serve 5 30 requests per second rstudio 2015 however as more users interact with a shiny app simultaneously the demand on the app s resources is increased to maximize the performance of sharks as additional users access the app the shinyapps io server has been configured to trigger the addition of new worker processes and application instances more rapidly to spread the computational load in addition sharks utilizes the shiny reactive framework to compartmentalize computations to ensure that they are not re performed unnecessarily as usage of sharks increases the hosting plan can be upgraded to increase the available memory per app instance as well as the limit of available app instances and worker processes in order to keep the app responsive for large scale implementation alternatively the city could choose to deploy sharks on their own server the sharks source code is publicly available online and is released under the mit license brendel et al 2018 sharks can be launched locally in any r environment console r rgui rstudio etc or deployed to a local or remote server so that users may access the app from a website without needing to install r 2 2 graphical user interface the sharks app user interface consists of a sidebar menu containing the app s inputs and options and a main panel containing the visualization capabilities fig 1 the sidebar menu and main panel are dynamically linked and selections on the sidebar menu determine the meteorological and hydrological stations parameters and date range for which data is retrieved a set of eight navigational tabs the summary map forecast interactive plots tables storm sewer real time flood stages and help tabs fig 1 are also included at the top of the sidebar menu and control which outputs and features are displayed in the main panel the help tab provides user with the developer contact information and a link to download the sharks user manual but is not discussed further in this paper help buttons have been placed throughout the user interface and when clicked display messages providing information regarding the various app inputs outputs and options the aforementioned components are described in more detail in sections 2 2 1 through 2 2 8 2 2 1 sidebar menu the sharks app sidebar menu fig 1 contains inputs and options that control the content displayed in the main panel the main panel is reactive to the sidebar menu and content in the main panel is updated immediately as changes are made to the inputs and options inputs located in the sidebar menu allow users to specify the noaa automated surface observing system asos meteorological station usgs meteorological stations and stream stations and the date range from which data is retrieved although roanoke s noaa asos and usgs stations fig 2 are preloaded into the sharks app the app can retrieve data for any noaa asos or usgs station noaa asos stations are specified by the station s federal aviation administration faa id and usgs stations are specified by the station s usgs site id a drop down list on the sidebar allows users to select which parameters are retrieved for the usgs stream stations available parameters are discharge cfs gauge height ft water temperature c specific conductance μs cm 25 c dissolved oxygen mg l ph and turbidity fnu finally an input is provided to allow users to specify the upstream watershed area for each stream station so that the app can calculate the direct runoff depth plotting options included in the sidebar menu control which meteorological station is used to create the app s hyetographs and which parameter if any is displayed on a secondary y axis in the app s hydrographs additional options control which time zone is used to retrieve and display data how datasets are symbolized in the sharks app hydrographs and if the app should calculate the average annual recurrence interval ari for precipitation events occurring during the specified date range 2 2 2 summary tab the summary tab was designed to provide a suite of tools for data comparison including tables of summary data runoff volume coefficients and calculated aris as well as a stacked hyetograph hydrograph and bar graphs of maximum flood stages the data summary table supplemental figure s1 provides an overview of the precipitation and discharge data for the meteorological and stream stations and facilitates quick comparisons between sites the location and station id of each station are displayed in the table and the table provides a summary of the total precipitation depth and maximum precipitation intensity for each meteorological station and the maximum discharge total direct runoff volume and total direct runoff depth for each stream station data in the table can be sorted by clicking the column headers runoff volume coefficients which represent the proportion of precipitation leaving a watershed as runoff are calculated for each stream station and meteorological station combination section 2 4 2 and displayed in the runoff volume coefficient summary table supplemental figure s2 values in the table indicate the runoff volume coefficients for the stream station and meteorological station in the respective rows and columns the ari summary table supplemental figure s3 displays the interpolated ari section 2 4 4 for precipitation events occurring during the specified date range the table includes the station location station id and calculated ari for each event in addition the table includes the start time and the duration of the window for which the ari was calculated as well as the total precipitation depth and average precipitation intensity occurring during the window although the table is sorted by ari in descending order by default the table can be sorted by any other parameter by clicking on the column headers a download button is also included to download the table as a comma separated values csv file the stacked hyetograph hydrograph supplemental figure s4 provides a visual display of the selected time series datasets allowing users to identify temporal trends in data and the occurrence of precipitation events although only one meteorological station may be displayed in the hyetograph there is no limit on the number of stream stations that are displayed in the hydrograph parameters which can be plotted in the hydrograph are discharge with baseflow gauge height water temperature specific conductance dissolved oxygen ph and turbidity discharge and baseflow are plotted on the primary y axis of the hydrograph by default however the sidebar menu can be used to plot any other parameter on a secondary y axis the color palette used for the hydrograph can be adjusted in the sidebar menu to either use the same color to symbolize all datasets from a stream station or to use a different color to symbolize each dataset from every stream station if users select to retrieve gauge height data from a roanoke usgs stream station then a bar graph of the maximum gauge height measured during the specified date range compared to the united states national weather service nws flood thresholds is displayed fig 3 colors of the bars change from green no flooding to yellow minor flooding red moderate flooding and black major flooding depending on the flood stage exceeded data labels identify the maximum gauge height as well as the date time in which it occurred 2 2 3 map tab the map tab was included to allow users to view summary meteorological and hydrologic data from each specified noaa and usgs station on an interactive map fig 4 section 2 4 5 clicking on the map markers displays a label with the total precipitation depth and maximum precipitation intensity during the specified date range for meteorological stations and the maximum stream discharge during the specified date range for stream stations for stations measuring both precipitation and stream flow the labels display the total precipitation depth maximum precipitation intensity and maximum stream discharge measured during the specified date range for usgs stations clicking the station name in the label will open a new browser tab to the usgs national water information system nwis website for the respective station 2 2 4 forecast tab the forecast tab was designed to allow users to view the current weather conditions and 10 day weather forecast from weather underground https www wunderground com for any user specified location supplemental figure s5 section 2 4 3 current weather conditions for the specified location are displayed using a weather underground weather sticker a widget that displays a visual representation of the current weather conditions and clicking on the sticker will open a new browser window to the weather underground website for the location the 10 day forecast for the specified location is displayed in a tabular format and includes the forecasted high and low temperatures weather condition precipitation probability and precipitation depth both the weather sticker and the forecast table respond dynamically to location inputs made in an input box located on the forecast tab 2 2 5 interactive plots tab the interactive plots tab was designed to allow users to investigate and download data from specific time periods within the specified date range section 2 4 6 a switch on the tab toggles the display between an interactive hyetograph and an interactive hydrograph created using the ggplot2 r package wickham 2016 users may study specific time periods within the hyetograph fig 5 by clicking and dragging a box around the desired data the duration of this brushed time period as well as the total precipitation depth and average precipitation intensity total precipitation depth duration for the period is summarized below the hyetograph below this the maximum precipitation intensity observed during the period and the date time s when it was recorded are also displayed furthermore a brushed data table is included of all incremental precipitation depth and intensity data for the displayed meteorological station during the brushed period data in the table can be sorted by clicking on the column titles and a button is included to download the table as a csv file like the hyetograph users may interact with the hydrograph supplemental figure s6 by brushing data for a specific time period a volume summary table displays the total discharge volume baseflow volume and stormflow runoff volume calculated via trapezoidal integration for each stream station for the brushed time period in addition all discharge and water quality data from the stream stations for the brushed period is displayed in brushed data table data in the table can be sorted by clicking on the column headers and a button is included to download the table as a csv file users may also interact with the hydrograph by clicking any spot on the hydrograph to display data from the nearest point including the location and date time from which the data was recorded as well as the baseflow discharge and any downloaded water quality parameters corresponding to that station and time 2 2 6 tables tab while the interactive plots tab is useful for downloading precipitation or discharge water quality data for specific time periods the tables tab was included in the sharks app to allow users to download all time series data from the app as one file for further offline use to allow users to sort the tables to identify the maximum and minimum recorded measurements of all stations for discharge and water quality data in the interactive hydrograph brushed data table was not joined by date time resulting in duplicate date times if measurements were taken at the same time at different stream stations in contrast the tables tab contains a combined data table in which the time series data for each of the meteorological stations and stream stations are joined by date time supplemental figure s7 thus there are separate columns for each time series dataset and every data point recorded for a specific time is shown on the same row within the table an additional benefit of joining all of the time series data streams into one table is that all of the data are correctly aligned regardless of any missing data points for meteorological stations the presented datasets include incremental precipitation and precipitation intensity and for stream stations the presented datasets include baseflow discharge total discharge direct runoff depth direct runoff volume and the selected water quality parameters data in the table can be sorted by clicking on the column headers and a button is included to download the table as a csv file the noaa precipitation frequency data server pfds tables for all specified noaa and usgs meteorological stations are also displayed on the tables tab supplemental figure s8 and can be downloaded as csv files currently the noaa pfds server https hdsc nws noaa gov hdsc pfds only allows users to retrieve the pfds tables by latitude longitude noaa station address or geocoded address however the sharks app automatically retrieves the latitude and longitude for any specified meteorological station and fetches the noaa pfds table for that location section 2 3 1 this automated process not only saves users time but also eliminates the possibility of transcription errors associated with manually entering station latitude longitudes into the noaa server 2 2 7 storm sewer tab the storm sewer tab fig 6 displays data from nine roanoke storm sewer flow depth sensors section 2 3 3 to access the tab s content users must log in with an authorized google account upon logging in an interactive map of the location of each sensor a stacked hyetograph hydrograph and a data table will appear on the tab the interactive map fig 6 section 2 4 5 displays the spatial location of each selected roanoke storm sewer sensor and clicking on the map markers displays a label with the sensor name and the maximum relative depth section 2 3 3 measured at that location during the specified date range the stacked hyetograph hydrograph fig 7 plots precipitation and relative depth section 2 3 3 time series for the specified date range drop down menus above the stacked plot allow users to select the storm sewer sensor and meteorological station data sources displayed in the plot the storm sewer sensor data table fig 7 displays the sensor stage and relative depth section 2 3 3 measurements from the specified date range as well as the date time and sensor at which they were recorded data in the table can be sorted by clicking on the column headers and a button is included to download the table as a csv file 2 2 8 real time flood stages tab the real time flood stages tab consists of a set of bar graphs display the most recent gauge height measurements for five roanoke stream stations compared to their respective nws flood thresholds fig 3 section 2 4 3 colors of the bars change from green no flooding to yellow minor flooding red moderate flooding and black major flooding depending on the flood stage exceeded data labels identify the most recent gauge height measurement as well as the date time in which it occurred 2 3 data retrieval pre processing 2 3 1 meteorological data the sharks app was designed to retrieve meteorological data from any noaa asos station the noaa asos network is the flagship automated observing network and provides observations for the nws faa and department of defense meteorological data for the user specified asos station is obtained via the iowa environmental mesonet maintained by iowa state university https mesonet agron iastate edu request download phtml data is retrieved from the mesonet via a programmatically created url based on user input state time zone asos station airport faa id and date range the asos network reports precipitation data as a cumulative precipitation depth that generally resets on the hour the time step for the asos data is typically 5 min but varies depending on when the precipitation depth resets the sharks app was also designed to retrieve precipitation data from any usgs meteorological station precipitation data for the user specified usgs meteorological stations is obtained from usgs nwis via the readnwisuv function in the dataretrieval r package hirsch and de cicco 2015 based on the user specified date range and time zone usgs precipitation data is reported as an incremental precipitation depth so no pre processing of the data was required timesteps for the usgs precipitation data vary by station but are typically either 5 or 15 min to calculate the ari of storm events the sharks app retrieves the noaa atlas 14 point precipitation frequency estimates table https hdsc nws noaa gov hdsc pfds via a programmatically created url for each user specified noaa asos and usgs meteorological station based on latitude and longitude of the station although the station latitude and longitude are included with the meteorological data for noaa asos stations this information is not included with the precipitation data for usgs stations thus the readnwissite function in the dataretrieval r package hirsch and de cicco 2015 was used to retrieve the latitude and longitude for each usgs meteorological station 2 3 2 stream discharge water quality data the sharks app was designed to retrieve gauge height stream discharge and water quality data from any usgs stream station this data is obtained from the usgs nwis via the readnwisuv function in the dataretrieval r package hirsch and de cicco 2015 based on the user specified date range and time zone water quality parameters available for download within the app are water temperature c specific conductance μs cm 25 c dissolved oxygen mg l ph and turbidity fnu time steps for the usgs gauge height stream discharge and water quality data vary by station but are typically either 5 or 15 min 2 3 3 storm sewer flow depth data in august 2017 the city of roanoke installed hobo u20 water level loggers built by onset corp http www onsetcomp com to monitor storm sewer flow depth in nine critical downtown pipe network locations fig 8 with the goal of identifying choke points within the network aguilar et al under review the sensors record flow depth data at 5 min increments and data is manually retrieved from each sensor by a city of roanoke employee because the level loggers are installed above the baseflow flow depth negative flow depths are recorded when the water level is below the sensor and the pressure transducers measure the atmospheric pressure instead of the water pressure thus negative flow depth datapoints are removed from the datasets because the measurements are inaccurate during these conditions to prevent floating data when graphing flow depth values immediately before after removed data points are set to 0 001 feet below the sensor position to indicate that the water level starts ends below the sensor position since the storm sewer slope and roughness at each of the nine locations are subject to considerable uncertainty the flow depth data is divided by the full flow depth at the respective location to calculate a relative depth metric to identify choke points aguilar et al under review then the relative depth time series for each of the nine locations are uploaded to google sheets storing the data in a google sheet has two primary benefits first it enables the sharks app to retrieve the data without relying on any local files second it allows for control of user access to the data so access can be limited to authorized accounts to access the storm sewer data the sharks app requires users to login with a google account that has access to the google sheets which store the data the googleauthr r package edmondson 2018 handles google authentication and retrieves the google access token then the sharks app retrieves the relative depth data from the google sheets using the google sheets application program interface api https developers google com sheets api 2 3 4 current weather conditions 10 day weather forecast the google geocoding api https developers google com maps documentation geocoding start is used to retrieve the latitude and longitude of the user specified location for which they wish to retrieve forecast data then the current weather conditions for the location are retrieved from weather underground as a weather sticker https www wunderground com stickers and the 10 day weather forecast for the location is obtained from weather underground via the forecast10day function in the rwunderground r package shum 2018 2 4 analysis features the sharks app includes both automated analysis features which do not require any user inputs and interactive analysis features which require user input from the user interface automated analysis features include performing hydrograph separations calculating direct runoff volume depth and runoff volume coefficients retrieving the current weather conditions and 10 day forecast and retrieving the current flood stages due to the reactive shiny framework the automated analyses are re executed and their outputs are immediately updated upon changes to the selections made with the sharks control widgets interactive analysis features include calculating storm ari interactive maps and interactive plots the interactive analyses require inputs from users such as clicking on a station in the interactive maps or selecting a date time range on the interactive plots this information is then passed to the server to query data from the master data frame and perform any necessary computations before updating the user interface outputs all of the sharks analysis features with the exception of the forecast and real time flood stages features pull data from the master data frame thus the sharks implementation is generic such that any additional exploratory data analysis features can easily be incorporated by adding code to query data from the master data frame perform any necessary computations and display outputs in the user interface 2 4 1 hydrograph baseflow separation the sharks app performs hydrograph separations automatically for each user specified usgs stream flow station partitioning a hydrograph into the stormflow and baseflow components can provide valuable insight into a watershed s response to precipitation or snowmelt events particularly the proportion of added water that becomes runoff runoff volume coefficient and the timing of the hydrologic response results of the hydrograph separations are used in the calculations for the direct runoff volume depth and runoff volume coefficients analysis features detailed in section 2 4 2 hydrograph separations are performed using the baseflowseparation function in the ecohydrology r package fuka et al 2014 this function uses the recursive digital filter from nathan and mcmahon 1990 to partition total streamflow into baseflow and stormflow components based on inputs of the flow time series number of passes and filter parameter alpha the number of passes controls the degree of smoothing applied to the hydrograph separation and a value of three was used similar to nathan and mcmahon 1990 who concluded that an alpha value in the range of 0 90 0 95 produced the most acceptable hydrograph separations to determine the optimal alpha value for hydrograph separations for roanoke s lick run and roanoke river usgs stream stations hydrograph separations performed using the conductivity mass balance cmb method were compared to hydrograph separations performed using the digital filter method with alpha values ranging from 0 900 to 0 995 the cmb hydrograph separation method has been used to calibrate other hydrograph separation methods and is detailed in nathan and mcmahon 1990 pilgrim et al 1979 stewart et al 2007 and bhaskar and welty 2015 during precipitation events stream flow conductance decreases as flow is diluted the cmb method uses this effect to partition total stream flow into baseflow and stormflow components based on measured stream discharge measured stream specific conductance estimated stormflow specific conductance and estimated baseflow specific conductance to compare the cmb and digital filter hydrograph separation methods stream discharge and specific conductance data was downloaded from the lick run and roanoke river stations for a library of precipitation events for the lick run station 11 events from 2017 to 2018 were chosen for analysis specific conductivity data was unavailable from 2012 to 2017 for the roanoke river station so 10 events from 2011 to 2018 were chosen for analysis for the cmb method stormflow specific conductance was estimated as the minimum measured stream specific conductance observed during the event and baseflow specific conductance was estimated as the average stream specific conductance at the stations during extreme low flow periods when it can be assumed that stream flow is entirely baseflow stewart et al 2007 hydrograph separations were performed for each of the 11 lick run and 10 roanoke river events using the cmb method and the digital filter method with alpha values ranging from 0 900 to 0 995 then the spearman s rho percent bias pbias and nash sutcliffe efficiency nse model fit statistics were calculated with the cmb method baseflow estimates as the target values spearman s rho correlations were considered significant for p values 0 05 pbias was considered acceptable for values between 25 and 25 and nse values 0 0 were considered acceptable overall the optimal digital filter alpha values for the lick run and roanoke river stations were selected as the alpha values which resulted in the greatest number of events with significant spearman s rho p values and acceptable pbias and nse values for the lick run station the optimal alpha value was 0 9875 with nine events with p values 0 05 ten events with acceptable pbias and three events with acceptable nse likewise for the roanoke river station the optimal value was 0 9250 with ten events with p values 0 05 three events with acceptable pbias and three events with acceptable nse for any other user specified usgs stream station the nathan and mcmahon 1990 recommended alpha value of 0 925 will be used for hydrograph separations 2 4 2 calculate direct runoff volume direct runoff depth and runoff volume coefficients the sharks app automatically calculates the direct runoff volume direct runoff depth and the proportion of precipitation leaving a watershed runoff volume coefficient for each user specified usgs stream flow station and the results from these analyses are displayed in data tables on the sharks summary tab section 2 2 2 increases in impervious area correspond to increases in runoff volume peak flow rates flooding frequency and runoff volume coefficients bonta et al 2003 goldshleger et al 2009 furthermore as percent impervious area increases from 20 to 40 the range of percent impervious area corresponding to the transformation from pre urban to urban land use there is a sharp increase in runoff volume coefficient goldshleger et al 2009 therefore since this land use transformation is prevalent in areas of high urbanization goldshleger et al 2009 analysis of runoff volume coefficients provides a useful metric for evaluating the threat of storm runoff flooding the direct runoff volume depth and runoff volume coefficient analysis features were included in sharks to aid users in understanding watershed responses to precipitation and to evaluate the threat of flooding from storm runoff the runoff volume coefficient is a particularly important hydrologic parameter as the city will be using it for long term evaluation of trends in hydrologic function as investment in stormwater control measures to restore pre development hydrology continues to increase over time this parameter also allows for comparison across watersheds as a method of comparing one of the city s more highly urbanized watersheds to a less developed watershed in the same region the volume of direct runoff is calculated for each specified stream station using trapezoidal integration of stormflow with respect to time the direct runoff volume is then normalized by the user specified watershed area to calculate a direct runoff depth for each stream station runoff volume coefficients represent the proportion of precipitation leaving a watershed and are calculated for each stream station and meteorological station combination by dividing direct runoff depth by total precipitation depth 2 4 3 forecast real time flood stages the sharks app automatically retrieves the current weather conditions and 10 day weather forecast for a user specified location section 2 3 4 and displays this information on the forecast tab section 2 2 4 in addition sharks automatically retrieves the most recent gauge height measurements from usgs nwis for five roanoke stream stations and displays this information relative to the nws flood thresholds on the real time flood stages tab section 2 2 8 currently there is no way to programmatically retrieve flood thresholds from the nws so only the five roanoke stream stations have been included in sharks combined the sharks forecast and flood stage analysis features provide information to enable users to make informed decisions regarding how an area may respond to future precipitation and weather conditions leading up to and during flood events city of roanoke stormwater and emergency management staff rely on hydrologic measurements and forecasts to make decisions about road closures deployment of flood proofing measures and evacuations in particular staff monitor how close rivers are to nws flood stages current and forecasted weather conditions and field observations as the basic information used to make rapid and sometimes life saving decisions the real time flood stages tab condenses river information from the salient usgs stations with nws flood thresholds so that local officials have the information needed in a single dashboard type interface in addition the sharks forecast analysis features informs local officials if the area is likely to receive more precipitation which could exacerbate flooding 2 4 4 calculate average annual recurrence interval ari users can calculate the ari for storm events at each specified meteorological station by clicking a switch on the sharks sidebar menu section 2 2 1 as an interactive analysis feature the ari computations will only be performed if users set the switch to calculate the aris a data table of all aris calculated for the specified date range supplemental figure s3 is displayed on the sharks summary tab this analysis feature was included in sharks to provide a useful metric for users to study the spatial distribution of event precipitation among separate stations as well as to compare separate storm events in addition expressing the rarity of precipitation in terms of an ari provides an objective criterion that can be easily conveyed to decision makers and the public city of roanoke staff have used the sharks ari analysis feature to study the variability of local precipitation and distribute the results to news stations emergency managers and the public in response to flooding that occurred in the southwest part of the city section 3 2 furthermore quantifying aris allows users to compare actual events to design storms to determine if infrastructure is functioning according to current design standards or if it needs to be upsized to calculate storm aris a sliding window analysis is performed to first calculate the measured precipitation depth at the station for every possible time interval during the specified date range the sliding window analysis divides the date range into a set of intervals or windows with lengths t 2t 3t nt where t is the time increment between precipitation data points and n is the number of precipitation data points in the specified date range fig 9 for each window length a window is created starting at every precipitation data point i i 1 i 2 to obtain every possible window of every possible length within the specified date range fig 9 then the total precipitation depth for each window is calculated as the sum of the incremental precipitation depths within the window next the sliding window dataset is subset to only include windows with lengths equal to the event durations in noaa pfds tables although sliding windows could have been created only for lengths equal to the event durations in the pfds tables creating windows of every possible length does not drastically increase program run time and it facilitates the ability to implement interpolation between event durations in addition to between aris finally the ari for each sliding window is calculated by using the window s precipitation depth to interpolate the ari between the precipitation depths from the station s noaa pfds table for the event duration corresponding to the window length if the precipitation depth for a window is less than the depth of the 1 year storm then the ari for the window is set to not available na 2 4 5 interactive maps interactive maps are included on the sharks map fig 4 and storm sewer fig 6 tabs these maps are automatically populated with markers indicating the spatial location of each selected station sensor users can zoom and pan the map or click on the markers to view summary data e g precipitation depth storm ari maximum stream discharge storm sewer flow depth etc for that location interactive mapping in sharks is handled by the leaflet r package cheng et al 2018 each interactive map in the sharks user interface is supported by a reactive data frame that contains information about the station sensor s name latitude longitude and map marker attributes the data frame also includes the programmatically built label for each station sensor containing the summary data queried from the master data frame interactive mapping is a critical component of any spatial web application and provides spatial context to the data for users who may not be familiar with the locations of the stations sensors based on their names alone the interactive mapping analysis features were included in sharks to allow users to identify spatial patterns across a network of stations in addition the interactive maps were designed to provide a brief summary of the data for users who may not have backgrounds in hydrology and could be intimidated by the other sharks analysis features in roanoke the sharks interactive maps have been used by city staff to study the variability of local rainfall events section 3 2 2 4 6 interactive plots the sharks interactive plots tab contains two interactive analysis features an interactive hyetograph fig 5 and an interactive hydrograph supplemental figure s6 these plots are created using the ggplot2 r package wickham 2016 and allow users to view individual data points by clicking on the data point on the plot or summarize the data from a specific time period by clicking and dragging a selection box around data on the plot termed brushing the shiny framework provides built in support for the click and brush interactions when users click or brush data in the interactive plots this data is subset from the master data frame and stored into a separate data frame for analysis during testing of sharks it was decided that clicking on individual incremental precipitation data points on the hyetograph was difficult and therefore viewing individual incremental precipitation data points was not a useful feature thus the sharks interactive hyetograph only supports brushing whereas the interactive hydrograph supports both brushing and clicking using the interactive hyetograph users can view the duration of a brushed time period as well as the total precipitation depth average precipitation intensity total precipitation depth duration and maximum precipitation intensity observed during the period in addition sharks displays all the incremental precipitation depth and intensity data from the brushed period in a sortable and downloadable data table these analysis features were included in sharks so users could identify how much precipitation was received at a meteorological station for a precise time period unlike other platforms which may only report precipitation in certain increments e g 1 6 12 or 24 h the interactive hydrograph can be used to determine the total discharge volume baseflow volume and stormflow runoff volume calculated via trapezoidal integration at a stream station for a brushed time period users may also click on data points in the interactive hydrograph to view the baseflow discharge and water quality parameters corresponding to that station and time these analysis features were included in sharks to allow users to investigate the temporal variation in the hydrologic datasets 3 case study city of roanoke the purpose of this section is to demonstrate the immediate added value of the sharks application to the city of roanoke and to describe some initial lessons learned that were made possible by the availability of this app although the initial use of the sharks app has been site specific to the city of roanoke the authors reiterate that any usgs or noaa asos site can be used in the application and the underlying r code is publicly available to be tailored to a different site as needed brendel et al 2018 the following sub sections therefore provide a conceptual description of the types of rapid analysis that this app provide that can be applied to other jurisdictions or watersheds 3 1 variability of local rainfall events an ancillary observation that was made during the analysis of storm events was that the intensity and depth of rainfall varies dramatically across the city s service area this had been observed anecdotally for many years in this region though the installation of the rain gage network and viewing capabilities of the sharks app allowed for empirical evidence of this phenomenon for example during a storm event that occurred on 5 27 18 the total observed rainfall depth at two stations varied by an order of magnitude across a distance of approximately two miles this variability was further demonstrated by estimating the ari at each station using the sharks app for another storm event that occurred on 5 17 18 and plotting this variability across the city s service area to do this a grid of aris was created by performing inverse distance weighting idw interpolation between the maximum ari during the storm at each location as calculated from sharks and the results are shown in fig 10 the maximum ari recorded at each station during this storm ranged from 1 7 years in the northeast to 35 5 years in the southwest part of the city and it is clear that this event was centered in a highly localized area the short term benefit of the app for this event was that city staff were able to characterize this event within hours of the end of the rainfall when the memory of the event was still fresh tying anecdotal observations to gage measurements these data were then distributed to news stations emergency managers and the public in response to flooding that occurred in the southwest part of the city the long term benefit of these data is that the high noted variability in rainfall across the city may lead to a more localized rainfall design paradigm for stormwater structures than the regional approach that has historically been taken based on noaa pfds methodology continued monitoring of the spatial distribution of rainfall using the sharks app will allow the city to determine if the high variability observed on 5 17 18 was an outlier or if order of magnitude variability across the city s service area is a typical pattern for rainfall in this area 3 2 storm sewer response to precipitation one issue of considerable interest in the city was the recurring flooding that occurs in the city s central business district cbd during brief intense rainfall see e g chittum 2017 the cause of this flooding was unclear as the stream draining the watershed was buried in large tunnels around the turn of the 20th century and it was therefore not possible to visually observe the stream during storm events to address this issue the nine storm sewer flow depth sensors previously described were installed at critical locations throughout the tunnel system fig 8 and the data from these sensors was integrated with precipitation data in the sharks app the primary interest was to evaluate the hydraulic grade line at these nine locations with respect to the tunnel soffit and manhole rims during a range of different storm event depths durations intensities and corresponding annual recurrence intervals aris pipes that surcharge under ari events that are smaller than a typical 10 year ari design storm could be targeted for hydraulic improvements such as replacement or sediment clearing the preliminary results of this work show that several of the tunnels are surcharging under 5 10 yr ari rainfall events the cause of this surcharging as revealed by the sharks app appears to be undersized local arterial drainage conveyances and sediment build up but not downstream capacity however it has been observed that during larger storm events runoff from an adjacent watershed creates a backwater effect that could prevent the tunnel system from efficient draining furthermore one of the tunnels appears to be flowing in two directions depending on the location of rainfall with respect to the contributing drainage areas the recommended engineering improvement is highly contingent on understanding flow direction through this tunnel and a new velocity sensor will be added to the sharks app to help better understand the complex hydraulics in this reach a second reason for the integration of tunnel hydraulics data into the sharks app was to develop a relationship between rainfall characteristics and the relative fullness of the tunnels under the cbd to serve as an early flood warning system for owners and tenants this would allow for city officials to alert stakeholders of a potential flash flood situation based on noaa s quantitative precipitation estimate so that temporary flood proofing measures could be put in place the sharks app has provided the necessary data analysis tools to develop these simple rainfall runoff relationships though the preliminary results have only shown a tentative correlation and more storm event data is needed 3 3 stream specific conductance during snow events during the march 12th 2018 snow event two large spikes in specific conductance were observed at the lick run stream station fig 11 the initial peak in specific conductance observed around 5 30am corresponded to only a minor increase in stream discharge upon analysis of the sharks app output the city of roanoke concluded that the initial spike in specific conductance was likely the result of application of de icer on an adjacent highway prior to the peak commute time and that the second specific conductance peak was due to the flushing of salts from the watershed during snow melt measured precipitation at the lick run station was only 0 06 inches during the initial specific conductance peak and 0 25 inches during the second specific conductance peak in the long term the ability of snow removal operators to track in stream specific conductance at near real time while viewing accumulated and predicted snowfall amounts through the sharks app will provide valuable information for decisions about additional application of de icing agents 4 conclusions overall the sharks app expands the capabilities of existing web based visualization platforms beyond exclusively data retrieval and visualization through the inclusion of a suite of data analysis tools the combination of data retrieval visualization and analysis tools allows users to quickly compare and export data from any usgs stream station and any noaa asos or usgs meteorological station in addition the app expedites the data analysis process with dynamically linked data visualization and analysis tools which allow users to refine the datasets and time periods analyzed based on the analysis results before exporting the data for further use furthermore the app introduces a framework for integrating time series data from local sources with data from government and commercial sources while providing options for restricting user access to the data through the use of google sheets no specialized coding software or expertise are needed to utilize the sharks app and the web interface ensures that the app can be accessed from a web browser without installing any additional software as implemented for the city of roanoke the sharks app has proven its usefulness as a tool for investigating hydrologic processes in urban watersheds however the sharks app is easily adapted to include preloaded monitoring stations for implementation in other locations and will be useful for other cities and organizations that need to retrieve and analyze hydrologic data from a variety of sources a streamlined version of the sharks app tailored towards use by the general public is currently under development and will provide a subset of the app s analysis features in a simplified user interface software availability software name stream hydrology and rainfall knowledge system sharks developers sharks team contact information cbrendel vt edu hardware required any web enabled device with a modern web browser software required internet browser program languages r html and css availability the sharks app is available at https bigbadcrad shinyapps io sharks and the underlying r code may be obtained on github at https github com bigbadcrad sharks or on zenodo brendel et al 2018 code is released under the mit license dependencies r package version dashboardthemes 1 0 2 data table 1 11 4 dataretrieval 2 7 3 dplyr 0 7 5 dt 0 4 ecohydrology 0 4 12 ggplot2 2 2 1 googleauthr 0 6 2 googleid 0 0 9001 googlesheets 0 2 2 gridextra 2 3 leaflet 2 0 2 lubridate 1 7 4 pracma 2 1 4 rcurl 1 95 4 10 rgdal 1 3 4 rwunderground 0 1 8 shadowtext 0 0 3 shiny 1 1 0 shinyalert 1 0 shinydashboard 0 7 0 shinyjs 1 0 shinywidgets 0 4 3 stringr 1 3 1 zoo 1 8 2 acknowledgements this work was supported by the city of roanoke virginia fund number 466172 and the virginia tech via department of civil environmental engineering special thanks to david woodson and the city of roanoke stormwater division for their valuable insights and helpful feedback during the development and testing of this app conflicts of interest declarations of interest none appendix a supplementary data the following is are the supplementary data to this article supplemental figure s1 summary tab data summary table providing an overview of precipitation and discharge data for selected meteorological and stream stations supplemental figure s1 supplemental figure s2 summary tab runoff volume coefficient summary table presenting runoff volume coefficients for each stream station and meteorological station combination supplemental figure s2 supplemental figure s3 summary tab average annual recurrence interval ari summary table displaying the interpolated ari for meteorological stations supplemental figure s3 supplemental figure s4 summary tab stacked hyetograph hydrograph displaying meteorological and hydrologic data for specified stations supplemental figure s4 supplemental figure s5 forecast tab weather underground weather sticker widget displaying current weather conditions and data table displaying the 10 day weather forecast supplemental figure s6 interactive plots tab interactive hydrograph with brushed data and volume summary supplemental figure s6 supplemental figure s7 tables tab combined data table presents time series data for all meteorological and stream stations joined by date time supplemental figure s7 supplemental figure s8 tables tab pfds tables can be displayed for all meteorological stations supplemental figure s8 multimedia component 9 multimedia component 9 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2019 03 003 
26211,as wireless sensor networks become more prevalent tools for analyzing watershed dynamics the integration and management of the myriad of available data streams presents a unique challenge web based platforms provide tools to retrieve and visualize hydrologic and meteorologic data from various sources while addressing syntactic and semantic differences in data formats however current platforms are limited by a lack of data analysis tools the stream hydrology and rainfall knowledge system sharks app was developed to expand upon existing platforms by providing a suite of exploratory data analysis features including the ability to calculate the total precipitation depth recorded for any period interpolate the average annual recurrence interval for precipitation events perform hydrograph separations and calculate the volume of runoff for any period a case study provides a conceptual description of the types of rapid analysis that can be performed using the sharks app keywords hydrologic monitoring smart watershed environmental observations data visualization data analysis abbreviations ari annual recurrence interval cmb conductivity mass balance 1 introduction as a result of the integration of low cost sensors and wireless communications with web services large sensor networks are increasingly being used in environmental monitoring bartos et al 2018 sensor networks have been implemented in projects including habitat monitoring biagioni and bridges 2002 mainwaring et al 2002 environmental observation and forecasting systems keefer et al 1987 steere et al 2000 forest fire detection hefeeda and bagheri 2009 soliman et al 2010 agriculture kim et al 2008 and glacier research padhy et al 2005 in urban areas sensor networks in smart watersheds can also provide information on watershed dynamics and provide insights to issues such as flooding runoff pollution and degradation of aquatic ecosystems bartos et al 2018 furthermore a new generation of intelligent stormwater networks will allow cities to expand real time monitoring and control of stormwater systems to entire watersheds through the implementation of sensors and dynamic controls bartos et al 2018 kerkez et al 2016 lefkowitz et al 2016 mullapudi et al 2017 muschalla et al 2014 as hydrologic sensor networks become more prevalent the effective integration and management of the multitude of data streams including those from government commercial and local sources to produce coherent results presents a unique challenge synthesizing data from different sources can be difficult because each source can have a different way of navigating through pages menus and files to access the data furthermore syntactic and semantic differences in data formats can make it difficult to find organize and interpret data horsburgh et al 2009 finally data retrieval and processing can be hindered by the sheer quantity of available data vitolo et al 2015 several platforms have been created to gather and visualize hydrologic data from various sources notably the consortium of universities for the advancement of hydrologic science inc cuahsi hydroclient http data cuahsi org accesses hydrologic data from over 95 sources stored in the cuahsi hydrologic information system his this system was developed to address syntactic and semantic heterogeneity in environmental data by the use of a standard observations data model odm database format for data storage and the wateroneflow web services for data communication horsburgh et al 2009 the cuahsi hydroclient also provides a basic data series viewer to plot datasets a related platform the time series analyst tsa http data iutahepscor org tsa is able to retrieve any dataset published to the cuahsi his and provides more advanced tools for data visualization including a map based interface faceted filters and the ability to display data using time series box and whisker or histogram plots furthermore the tsa includes the calculation of data summary statistics horsburgh et al 2016 the great lakes dashboard gld platform https www glerl noaa gov data dashboard gld html5 html was specifically designed as a tool to visualize and download datasets from the north american laurentian great lakes and displays plots for a multitude of different time series datasets smith et al 2016 finally the virtual observatory and ecological informatics system voeis data hub https voeis msu montana edu provides a suite of tools for users to compile manage visualize and redistribute datasets the voeis data hub allows users to upload data or retrieve data from networked sensors and includes qa qc capabilities furthermore the voeis data hub includes data access restriction features and allows users to publish data to a cuahsi his hydroserver mason et al 2014 despite the advances made by these platforms they are limited by the shortage of data analysis tools a web application called the stream hydrology and rainfall knowledge system sharks app https bigbadcrad shinyapps io sharks was developed to expand upon existing platforms by integrating data analysis tools with data retrieval and visualization capabilities the sharks app is tailored to stormwater managers emergency managers hydrologists and or meteorologists that need to understand in real time and for retrospective analysis aguilar et al 2019 what is the location intensity and average annual recurrence interval of rainfall bartos et al 2018 what is the corresponding depth discharge in streams and large storm drains bhaskar and welty 2015 what is the weather forecast biagioni and bridges 2002 what is the proportion of precipitation leaving a watershed as runoff bonta et al 2003 how does water quality change during events the app ties together data from multiple syntactically different sources and provides immediately useful information for decision making and retrospective study e g water balance h h modeling the web based interface of the sharks app was designed with the goal of providing an accessible platform for users possessing an understanding of basic hydrology although the sharks app was conceptualized and implemented to perform data analysis for the city of roanoke virginia the app was designed to also retrieve and analyze data for other locations this paper describes the sharks app workflow and presents a case study demonstrating how the sharks app can be used to provide valuable insights to urban hydrologic processes analysis features were chosen for inclusion in sharks based on needs identified by city of roanoke staff and sharks developers the information needed from the app fit into two broad categories 1 rapidly available rainfall and streamflow depth information during intense precipitation events and 2 hydrologic summary information organized systematically for retrospective analysis as a result the design of the app was borne out of the basic need to know report and respond to rainfall e g depth intensity average annual recurrence interval and stream e g depth discharge conditions across the city s service area as well as to use more complex data e g separated hydrographs direct runoff volumes runoff volume coefficients to better understand long term hydrological processes 2 software implementation 2 1 architecture sharks is implemented as a web based application so that users can access the app from a web browser without installing any additional software in addition deploying the app online ensures that all users are using the most recent version of sharks sharks was programmed using the open source r language with the shiny web application framework https shiny rstudio com r and shiny were chosen for development over alternatives such as python and dash because r is generally considered to have superior data visualization features and more sophisticated statistical libraries in contrast python is a general purpose language which facilitates integration among different software components because sharks is focused on data analysis and visualization and was designed to be a stand alone platform the advantages of r s data visualization features were decided to outweigh the advantages of python s ability to integrate with other web components in addition the shiny framework allows developers to create web apps using entirely r code and uses a reactive programming model in which outputs update instantly as inputs are modified thus eliminating the need for messy event handling code chang et al 2018 furthermore shiny includes a library of pre built input and output widgets for controlling apps and displaying outputs r also includes an extensive set of packages and available code that facilitate the retrieval of hydrologic and meteorological data from the united states geological survey usgs and the national oceanic and atmospheric administration noaa finally r and shiny are free and open source so others can modify and adapt the publicly available sharks source code brendel et al 2018 2 1 1 mechanics the sharks shiny framework consists of two components a user interface object and a reactive server function the app s user interface is built primarily of r code but was customized using html css and javascript code directly sharks is controlled using pre built shiny widgets e g date range inputs selection boxes and switches placed in the user interface selections made using the control widgets are passed to the shiny server function which performs the computations and creates output objects e g graphs maps and tables that are passed back to the shiny user interface for display the shiny server is based on the concept of reactivity in which output objects are updated automatically as inputs are changed therefore when users make selections using the control widgets in the sharks user interface the server performs the computations and updates the app outputs immediately unless told otherwise the mechanics of the sharks app are described as follows data is retrieved from sources based on the stations and time series datasets specified using control widgets and joined by date time to create one master data frame the master data frame is structured with separate columns for each time series dataset and rows for each unique date time the various sharks analysis features use and reshape the data from the master data frame as necessary some sharks analysis features are performed automatically and do not require any user input these automated analysis features include performing hydrograph separations section 2 4 1 calculating direct runoff volume depth and runoff volume coefficients section 2 4 2 calculating total precipitation depth and maximum precipitation intensity and plotting hyetographs and hydrographs due to the reactive shiny framework if users make changes using the user interface control widgets then these analyses are re executed and their user interface outputs are updated automatically other sharks analysis features calculating the storm average annual recurrence interval calculation section 2 4 4 the interactive maps section 2 4 5 and the interactive plots section 2 4 6 require user input when users click on these interactive analysis features the user inputs e g date time range selected on the interactive plots are passed to the server to query data from the master data frame and perform any necessary computations before updating the user interface outputs 2 1 2 implementation the sharks app has been implemented for the city of roanoke a medium sized urbanized area in southwest virginia in order to more effectively manage the quality and quantity of stormwater runoff from city watersheds and to improve compliance with the city s municipal separate storm sewer system ms4 permit and total maximum daily load tmdl requirements the city has installed a number of hydrologic and water quality instruments throughout the service area these instruments provide the empirical basis for data driven capital improvement spending and will also allow for the evaluation and iterative adjustment of watershed improvements over long periods of time i e adaptive management the instruments installed throughout the city s service area summarized in table 1 provide observations of hydrology and water quality at sub hourly time steps which creates a wealth of high quality environmental data for management and decision making however this also presents a data management problem because of the large and always growing amount of raw data in addition semantic and syntactic differences between the data sources make it challenging to join and organize datasets because they may use different codes for the same parameter e g p01i vs 00045 for precipitation or use the same name for multiple parameters e g using temperature to describe both air temperature and water temperature encode data using different file types and have different ways of structuring data the sharks app provides the necessary integration of data streams into a dynamic user interface so that city stormwater managers emergency managers hydrologists and or meteorologists can easily gather information and requisite analysis and communicate it to the necessary stakeholders the sharks app was also implemented as a pre processing tool to collate rainfall and runoff time series data for calibration and validation of a forthcoming hydrologic hydraulic modeling study sharks is currently deployed for the city online at https bigbadcrad shinyapps io sharks using the shinyapps io platform http www shinyapps io hosting sharks online not only ensures that all users are utilizing the most recent version of the app but also allows for remote updates to the app the shinyapps io platform was chosen as a low cost alternative to configuring and maintaining an expensive server for sharks in addition shinyapps io offers scalable hosting plans ranging from a free option to a professional option with increasing performance with higher tier plans the shinyapps io platform hosts sharks on a virtualized server termed an instance which is served by worker processes which service requests to the app rstudio 2015 with the current hosting plan each sharks instance is limited to 1024 mb of memory for computations because r is a single threaded application sharks cannot serve two users at exactly the same time typically this is not an issue because computations take place in the order of tens or hundreds of milliseconds enabling a single r process to serve 5 30 requests per second rstudio 2015 however as more users interact with a shiny app simultaneously the demand on the app s resources is increased to maximize the performance of sharks as additional users access the app the shinyapps io server has been configured to trigger the addition of new worker processes and application instances more rapidly to spread the computational load in addition sharks utilizes the shiny reactive framework to compartmentalize computations to ensure that they are not re performed unnecessarily as usage of sharks increases the hosting plan can be upgraded to increase the available memory per app instance as well as the limit of available app instances and worker processes in order to keep the app responsive for large scale implementation alternatively the city could choose to deploy sharks on their own server the sharks source code is publicly available online and is released under the mit license brendel et al 2018 sharks can be launched locally in any r environment console r rgui rstudio etc or deployed to a local or remote server so that users may access the app from a website without needing to install r 2 2 graphical user interface the sharks app user interface consists of a sidebar menu containing the app s inputs and options and a main panel containing the visualization capabilities fig 1 the sidebar menu and main panel are dynamically linked and selections on the sidebar menu determine the meteorological and hydrological stations parameters and date range for which data is retrieved a set of eight navigational tabs the summary map forecast interactive plots tables storm sewer real time flood stages and help tabs fig 1 are also included at the top of the sidebar menu and control which outputs and features are displayed in the main panel the help tab provides user with the developer contact information and a link to download the sharks user manual but is not discussed further in this paper help buttons have been placed throughout the user interface and when clicked display messages providing information regarding the various app inputs outputs and options the aforementioned components are described in more detail in sections 2 2 1 through 2 2 8 2 2 1 sidebar menu the sharks app sidebar menu fig 1 contains inputs and options that control the content displayed in the main panel the main panel is reactive to the sidebar menu and content in the main panel is updated immediately as changes are made to the inputs and options inputs located in the sidebar menu allow users to specify the noaa automated surface observing system asos meteorological station usgs meteorological stations and stream stations and the date range from which data is retrieved although roanoke s noaa asos and usgs stations fig 2 are preloaded into the sharks app the app can retrieve data for any noaa asos or usgs station noaa asos stations are specified by the station s federal aviation administration faa id and usgs stations are specified by the station s usgs site id a drop down list on the sidebar allows users to select which parameters are retrieved for the usgs stream stations available parameters are discharge cfs gauge height ft water temperature c specific conductance μs cm 25 c dissolved oxygen mg l ph and turbidity fnu finally an input is provided to allow users to specify the upstream watershed area for each stream station so that the app can calculate the direct runoff depth plotting options included in the sidebar menu control which meteorological station is used to create the app s hyetographs and which parameter if any is displayed on a secondary y axis in the app s hydrographs additional options control which time zone is used to retrieve and display data how datasets are symbolized in the sharks app hydrographs and if the app should calculate the average annual recurrence interval ari for precipitation events occurring during the specified date range 2 2 2 summary tab the summary tab was designed to provide a suite of tools for data comparison including tables of summary data runoff volume coefficients and calculated aris as well as a stacked hyetograph hydrograph and bar graphs of maximum flood stages the data summary table supplemental figure s1 provides an overview of the precipitation and discharge data for the meteorological and stream stations and facilitates quick comparisons between sites the location and station id of each station are displayed in the table and the table provides a summary of the total precipitation depth and maximum precipitation intensity for each meteorological station and the maximum discharge total direct runoff volume and total direct runoff depth for each stream station data in the table can be sorted by clicking the column headers runoff volume coefficients which represent the proportion of precipitation leaving a watershed as runoff are calculated for each stream station and meteorological station combination section 2 4 2 and displayed in the runoff volume coefficient summary table supplemental figure s2 values in the table indicate the runoff volume coefficients for the stream station and meteorological station in the respective rows and columns the ari summary table supplemental figure s3 displays the interpolated ari section 2 4 4 for precipitation events occurring during the specified date range the table includes the station location station id and calculated ari for each event in addition the table includes the start time and the duration of the window for which the ari was calculated as well as the total precipitation depth and average precipitation intensity occurring during the window although the table is sorted by ari in descending order by default the table can be sorted by any other parameter by clicking on the column headers a download button is also included to download the table as a comma separated values csv file the stacked hyetograph hydrograph supplemental figure s4 provides a visual display of the selected time series datasets allowing users to identify temporal trends in data and the occurrence of precipitation events although only one meteorological station may be displayed in the hyetograph there is no limit on the number of stream stations that are displayed in the hydrograph parameters which can be plotted in the hydrograph are discharge with baseflow gauge height water temperature specific conductance dissolved oxygen ph and turbidity discharge and baseflow are plotted on the primary y axis of the hydrograph by default however the sidebar menu can be used to plot any other parameter on a secondary y axis the color palette used for the hydrograph can be adjusted in the sidebar menu to either use the same color to symbolize all datasets from a stream station or to use a different color to symbolize each dataset from every stream station if users select to retrieve gauge height data from a roanoke usgs stream station then a bar graph of the maximum gauge height measured during the specified date range compared to the united states national weather service nws flood thresholds is displayed fig 3 colors of the bars change from green no flooding to yellow minor flooding red moderate flooding and black major flooding depending on the flood stage exceeded data labels identify the maximum gauge height as well as the date time in which it occurred 2 2 3 map tab the map tab was included to allow users to view summary meteorological and hydrologic data from each specified noaa and usgs station on an interactive map fig 4 section 2 4 5 clicking on the map markers displays a label with the total precipitation depth and maximum precipitation intensity during the specified date range for meteorological stations and the maximum stream discharge during the specified date range for stream stations for stations measuring both precipitation and stream flow the labels display the total precipitation depth maximum precipitation intensity and maximum stream discharge measured during the specified date range for usgs stations clicking the station name in the label will open a new browser tab to the usgs national water information system nwis website for the respective station 2 2 4 forecast tab the forecast tab was designed to allow users to view the current weather conditions and 10 day weather forecast from weather underground https www wunderground com for any user specified location supplemental figure s5 section 2 4 3 current weather conditions for the specified location are displayed using a weather underground weather sticker a widget that displays a visual representation of the current weather conditions and clicking on the sticker will open a new browser window to the weather underground website for the location the 10 day forecast for the specified location is displayed in a tabular format and includes the forecasted high and low temperatures weather condition precipitation probability and precipitation depth both the weather sticker and the forecast table respond dynamically to location inputs made in an input box located on the forecast tab 2 2 5 interactive plots tab the interactive plots tab was designed to allow users to investigate and download data from specific time periods within the specified date range section 2 4 6 a switch on the tab toggles the display between an interactive hyetograph and an interactive hydrograph created using the ggplot2 r package wickham 2016 users may study specific time periods within the hyetograph fig 5 by clicking and dragging a box around the desired data the duration of this brushed time period as well as the total precipitation depth and average precipitation intensity total precipitation depth duration for the period is summarized below the hyetograph below this the maximum precipitation intensity observed during the period and the date time s when it was recorded are also displayed furthermore a brushed data table is included of all incremental precipitation depth and intensity data for the displayed meteorological station during the brushed period data in the table can be sorted by clicking on the column titles and a button is included to download the table as a csv file like the hyetograph users may interact with the hydrograph supplemental figure s6 by brushing data for a specific time period a volume summary table displays the total discharge volume baseflow volume and stormflow runoff volume calculated via trapezoidal integration for each stream station for the brushed time period in addition all discharge and water quality data from the stream stations for the brushed period is displayed in brushed data table data in the table can be sorted by clicking on the column headers and a button is included to download the table as a csv file users may also interact with the hydrograph by clicking any spot on the hydrograph to display data from the nearest point including the location and date time from which the data was recorded as well as the baseflow discharge and any downloaded water quality parameters corresponding to that station and time 2 2 6 tables tab while the interactive plots tab is useful for downloading precipitation or discharge water quality data for specific time periods the tables tab was included in the sharks app to allow users to download all time series data from the app as one file for further offline use to allow users to sort the tables to identify the maximum and minimum recorded measurements of all stations for discharge and water quality data in the interactive hydrograph brushed data table was not joined by date time resulting in duplicate date times if measurements were taken at the same time at different stream stations in contrast the tables tab contains a combined data table in which the time series data for each of the meteorological stations and stream stations are joined by date time supplemental figure s7 thus there are separate columns for each time series dataset and every data point recorded for a specific time is shown on the same row within the table an additional benefit of joining all of the time series data streams into one table is that all of the data are correctly aligned regardless of any missing data points for meteorological stations the presented datasets include incremental precipitation and precipitation intensity and for stream stations the presented datasets include baseflow discharge total discharge direct runoff depth direct runoff volume and the selected water quality parameters data in the table can be sorted by clicking on the column headers and a button is included to download the table as a csv file the noaa precipitation frequency data server pfds tables for all specified noaa and usgs meteorological stations are also displayed on the tables tab supplemental figure s8 and can be downloaded as csv files currently the noaa pfds server https hdsc nws noaa gov hdsc pfds only allows users to retrieve the pfds tables by latitude longitude noaa station address or geocoded address however the sharks app automatically retrieves the latitude and longitude for any specified meteorological station and fetches the noaa pfds table for that location section 2 3 1 this automated process not only saves users time but also eliminates the possibility of transcription errors associated with manually entering station latitude longitudes into the noaa server 2 2 7 storm sewer tab the storm sewer tab fig 6 displays data from nine roanoke storm sewer flow depth sensors section 2 3 3 to access the tab s content users must log in with an authorized google account upon logging in an interactive map of the location of each sensor a stacked hyetograph hydrograph and a data table will appear on the tab the interactive map fig 6 section 2 4 5 displays the spatial location of each selected roanoke storm sewer sensor and clicking on the map markers displays a label with the sensor name and the maximum relative depth section 2 3 3 measured at that location during the specified date range the stacked hyetograph hydrograph fig 7 plots precipitation and relative depth section 2 3 3 time series for the specified date range drop down menus above the stacked plot allow users to select the storm sewer sensor and meteorological station data sources displayed in the plot the storm sewer sensor data table fig 7 displays the sensor stage and relative depth section 2 3 3 measurements from the specified date range as well as the date time and sensor at which they were recorded data in the table can be sorted by clicking on the column headers and a button is included to download the table as a csv file 2 2 8 real time flood stages tab the real time flood stages tab consists of a set of bar graphs display the most recent gauge height measurements for five roanoke stream stations compared to their respective nws flood thresholds fig 3 section 2 4 3 colors of the bars change from green no flooding to yellow minor flooding red moderate flooding and black major flooding depending on the flood stage exceeded data labels identify the most recent gauge height measurement as well as the date time in which it occurred 2 3 data retrieval pre processing 2 3 1 meteorological data the sharks app was designed to retrieve meteorological data from any noaa asos station the noaa asos network is the flagship automated observing network and provides observations for the nws faa and department of defense meteorological data for the user specified asos station is obtained via the iowa environmental mesonet maintained by iowa state university https mesonet agron iastate edu request download phtml data is retrieved from the mesonet via a programmatically created url based on user input state time zone asos station airport faa id and date range the asos network reports precipitation data as a cumulative precipitation depth that generally resets on the hour the time step for the asos data is typically 5 min but varies depending on when the precipitation depth resets the sharks app was also designed to retrieve precipitation data from any usgs meteorological station precipitation data for the user specified usgs meteorological stations is obtained from usgs nwis via the readnwisuv function in the dataretrieval r package hirsch and de cicco 2015 based on the user specified date range and time zone usgs precipitation data is reported as an incremental precipitation depth so no pre processing of the data was required timesteps for the usgs precipitation data vary by station but are typically either 5 or 15 min to calculate the ari of storm events the sharks app retrieves the noaa atlas 14 point precipitation frequency estimates table https hdsc nws noaa gov hdsc pfds via a programmatically created url for each user specified noaa asos and usgs meteorological station based on latitude and longitude of the station although the station latitude and longitude are included with the meteorological data for noaa asos stations this information is not included with the precipitation data for usgs stations thus the readnwissite function in the dataretrieval r package hirsch and de cicco 2015 was used to retrieve the latitude and longitude for each usgs meteorological station 2 3 2 stream discharge water quality data the sharks app was designed to retrieve gauge height stream discharge and water quality data from any usgs stream station this data is obtained from the usgs nwis via the readnwisuv function in the dataretrieval r package hirsch and de cicco 2015 based on the user specified date range and time zone water quality parameters available for download within the app are water temperature c specific conductance μs cm 25 c dissolved oxygen mg l ph and turbidity fnu time steps for the usgs gauge height stream discharge and water quality data vary by station but are typically either 5 or 15 min 2 3 3 storm sewer flow depth data in august 2017 the city of roanoke installed hobo u20 water level loggers built by onset corp http www onsetcomp com to monitor storm sewer flow depth in nine critical downtown pipe network locations fig 8 with the goal of identifying choke points within the network aguilar et al under review the sensors record flow depth data at 5 min increments and data is manually retrieved from each sensor by a city of roanoke employee because the level loggers are installed above the baseflow flow depth negative flow depths are recorded when the water level is below the sensor and the pressure transducers measure the atmospheric pressure instead of the water pressure thus negative flow depth datapoints are removed from the datasets because the measurements are inaccurate during these conditions to prevent floating data when graphing flow depth values immediately before after removed data points are set to 0 001 feet below the sensor position to indicate that the water level starts ends below the sensor position since the storm sewer slope and roughness at each of the nine locations are subject to considerable uncertainty the flow depth data is divided by the full flow depth at the respective location to calculate a relative depth metric to identify choke points aguilar et al under review then the relative depth time series for each of the nine locations are uploaded to google sheets storing the data in a google sheet has two primary benefits first it enables the sharks app to retrieve the data without relying on any local files second it allows for control of user access to the data so access can be limited to authorized accounts to access the storm sewer data the sharks app requires users to login with a google account that has access to the google sheets which store the data the googleauthr r package edmondson 2018 handles google authentication and retrieves the google access token then the sharks app retrieves the relative depth data from the google sheets using the google sheets application program interface api https developers google com sheets api 2 3 4 current weather conditions 10 day weather forecast the google geocoding api https developers google com maps documentation geocoding start is used to retrieve the latitude and longitude of the user specified location for which they wish to retrieve forecast data then the current weather conditions for the location are retrieved from weather underground as a weather sticker https www wunderground com stickers and the 10 day weather forecast for the location is obtained from weather underground via the forecast10day function in the rwunderground r package shum 2018 2 4 analysis features the sharks app includes both automated analysis features which do not require any user inputs and interactive analysis features which require user input from the user interface automated analysis features include performing hydrograph separations calculating direct runoff volume depth and runoff volume coefficients retrieving the current weather conditions and 10 day forecast and retrieving the current flood stages due to the reactive shiny framework the automated analyses are re executed and their outputs are immediately updated upon changes to the selections made with the sharks control widgets interactive analysis features include calculating storm ari interactive maps and interactive plots the interactive analyses require inputs from users such as clicking on a station in the interactive maps or selecting a date time range on the interactive plots this information is then passed to the server to query data from the master data frame and perform any necessary computations before updating the user interface outputs all of the sharks analysis features with the exception of the forecast and real time flood stages features pull data from the master data frame thus the sharks implementation is generic such that any additional exploratory data analysis features can easily be incorporated by adding code to query data from the master data frame perform any necessary computations and display outputs in the user interface 2 4 1 hydrograph baseflow separation the sharks app performs hydrograph separations automatically for each user specified usgs stream flow station partitioning a hydrograph into the stormflow and baseflow components can provide valuable insight into a watershed s response to precipitation or snowmelt events particularly the proportion of added water that becomes runoff runoff volume coefficient and the timing of the hydrologic response results of the hydrograph separations are used in the calculations for the direct runoff volume depth and runoff volume coefficients analysis features detailed in section 2 4 2 hydrograph separations are performed using the baseflowseparation function in the ecohydrology r package fuka et al 2014 this function uses the recursive digital filter from nathan and mcmahon 1990 to partition total streamflow into baseflow and stormflow components based on inputs of the flow time series number of passes and filter parameter alpha the number of passes controls the degree of smoothing applied to the hydrograph separation and a value of three was used similar to nathan and mcmahon 1990 who concluded that an alpha value in the range of 0 90 0 95 produced the most acceptable hydrograph separations to determine the optimal alpha value for hydrograph separations for roanoke s lick run and roanoke river usgs stream stations hydrograph separations performed using the conductivity mass balance cmb method were compared to hydrograph separations performed using the digital filter method with alpha values ranging from 0 900 to 0 995 the cmb hydrograph separation method has been used to calibrate other hydrograph separation methods and is detailed in nathan and mcmahon 1990 pilgrim et al 1979 stewart et al 2007 and bhaskar and welty 2015 during precipitation events stream flow conductance decreases as flow is diluted the cmb method uses this effect to partition total stream flow into baseflow and stormflow components based on measured stream discharge measured stream specific conductance estimated stormflow specific conductance and estimated baseflow specific conductance to compare the cmb and digital filter hydrograph separation methods stream discharge and specific conductance data was downloaded from the lick run and roanoke river stations for a library of precipitation events for the lick run station 11 events from 2017 to 2018 were chosen for analysis specific conductivity data was unavailable from 2012 to 2017 for the roanoke river station so 10 events from 2011 to 2018 were chosen for analysis for the cmb method stormflow specific conductance was estimated as the minimum measured stream specific conductance observed during the event and baseflow specific conductance was estimated as the average stream specific conductance at the stations during extreme low flow periods when it can be assumed that stream flow is entirely baseflow stewart et al 2007 hydrograph separations were performed for each of the 11 lick run and 10 roanoke river events using the cmb method and the digital filter method with alpha values ranging from 0 900 to 0 995 then the spearman s rho percent bias pbias and nash sutcliffe efficiency nse model fit statistics were calculated with the cmb method baseflow estimates as the target values spearman s rho correlations were considered significant for p values 0 05 pbias was considered acceptable for values between 25 and 25 and nse values 0 0 were considered acceptable overall the optimal digital filter alpha values for the lick run and roanoke river stations were selected as the alpha values which resulted in the greatest number of events with significant spearman s rho p values and acceptable pbias and nse values for the lick run station the optimal alpha value was 0 9875 with nine events with p values 0 05 ten events with acceptable pbias and three events with acceptable nse likewise for the roanoke river station the optimal value was 0 9250 with ten events with p values 0 05 three events with acceptable pbias and three events with acceptable nse for any other user specified usgs stream station the nathan and mcmahon 1990 recommended alpha value of 0 925 will be used for hydrograph separations 2 4 2 calculate direct runoff volume direct runoff depth and runoff volume coefficients the sharks app automatically calculates the direct runoff volume direct runoff depth and the proportion of precipitation leaving a watershed runoff volume coefficient for each user specified usgs stream flow station and the results from these analyses are displayed in data tables on the sharks summary tab section 2 2 2 increases in impervious area correspond to increases in runoff volume peak flow rates flooding frequency and runoff volume coefficients bonta et al 2003 goldshleger et al 2009 furthermore as percent impervious area increases from 20 to 40 the range of percent impervious area corresponding to the transformation from pre urban to urban land use there is a sharp increase in runoff volume coefficient goldshleger et al 2009 therefore since this land use transformation is prevalent in areas of high urbanization goldshleger et al 2009 analysis of runoff volume coefficients provides a useful metric for evaluating the threat of storm runoff flooding the direct runoff volume depth and runoff volume coefficient analysis features were included in sharks to aid users in understanding watershed responses to precipitation and to evaluate the threat of flooding from storm runoff the runoff volume coefficient is a particularly important hydrologic parameter as the city will be using it for long term evaluation of trends in hydrologic function as investment in stormwater control measures to restore pre development hydrology continues to increase over time this parameter also allows for comparison across watersheds as a method of comparing one of the city s more highly urbanized watersheds to a less developed watershed in the same region the volume of direct runoff is calculated for each specified stream station using trapezoidal integration of stormflow with respect to time the direct runoff volume is then normalized by the user specified watershed area to calculate a direct runoff depth for each stream station runoff volume coefficients represent the proportion of precipitation leaving a watershed and are calculated for each stream station and meteorological station combination by dividing direct runoff depth by total precipitation depth 2 4 3 forecast real time flood stages the sharks app automatically retrieves the current weather conditions and 10 day weather forecast for a user specified location section 2 3 4 and displays this information on the forecast tab section 2 2 4 in addition sharks automatically retrieves the most recent gauge height measurements from usgs nwis for five roanoke stream stations and displays this information relative to the nws flood thresholds on the real time flood stages tab section 2 2 8 currently there is no way to programmatically retrieve flood thresholds from the nws so only the five roanoke stream stations have been included in sharks combined the sharks forecast and flood stage analysis features provide information to enable users to make informed decisions regarding how an area may respond to future precipitation and weather conditions leading up to and during flood events city of roanoke stormwater and emergency management staff rely on hydrologic measurements and forecasts to make decisions about road closures deployment of flood proofing measures and evacuations in particular staff monitor how close rivers are to nws flood stages current and forecasted weather conditions and field observations as the basic information used to make rapid and sometimes life saving decisions the real time flood stages tab condenses river information from the salient usgs stations with nws flood thresholds so that local officials have the information needed in a single dashboard type interface in addition the sharks forecast analysis features informs local officials if the area is likely to receive more precipitation which could exacerbate flooding 2 4 4 calculate average annual recurrence interval ari users can calculate the ari for storm events at each specified meteorological station by clicking a switch on the sharks sidebar menu section 2 2 1 as an interactive analysis feature the ari computations will only be performed if users set the switch to calculate the aris a data table of all aris calculated for the specified date range supplemental figure s3 is displayed on the sharks summary tab this analysis feature was included in sharks to provide a useful metric for users to study the spatial distribution of event precipitation among separate stations as well as to compare separate storm events in addition expressing the rarity of precipitation in terms of an ari provides an objective criterion that can be easily conveyed to decision makers and the public city of roanoke staff have used the sharks ari analysis feature to study the variability of local precipitation and distribute the results to news stations emergency managers and the public in response to flooding that occurred in the southwest part of the city section 3 2 furthermore quantifying aris allows users to compare actual events to design storms to determine if infrastructure is functioning according to current design standards or if it needs to be upsized to calculate storm aris a sliding window analysis is performed to first calculate the measured precipitation depth at the station for every possible time interval during the specified date range the sliding window analysis divides the date range into a set of intervals or windows with lengths t 2t 3t nt where t is the time increment between precipitation data points and n is the number of precipitation data points in the specified date range fig 9 for each window length a window is created starting at every precipitation data point i i 1 i 2 to obtain every possible window of every possible length within the specified date range fig 9 then the total precipitation depth for each window is calculated as the sum of the incremental precipitation depths within the window next the sliding window dataset is subset to only include windows with lengths equal to the event durations in noaa pfds tables although sliding windows could have been created only for lengths equal to the event durations in the pfds tables creating windows of every possible length does not drastically increase program run time and it facilitates the ability to implement interpolation between event durations in addition to between aris finally the ari for each sliding window is calculated by using the window s precipitation depth to interpolate the ari between the precipitation depths from the station s noaa pfds table for the event duration corresponding to the window length if the precipitation depth for a window is less than the depth of the 1 year storm then the ari for the window is set to not available na 2 4 5 interactive maps interactive maps are included on the sharks map fig 4 and storm sewer fig 6 tabs these maps are automatically populated with markers indicating the spatial location of each selected station sensor users can zoom and pan the map or click on the markers to view summary data e g precipitation depth storm ari maximum stream discharge storm sewer flow depth etc for that location interactive mapping in sharks is handled by the leaflet r package cheng et al 2018 each interactive map in the sharks user interface is supported by a reactive data frame that contains information about the station sensor s name latitude longitude and map marker attributes the data frame also includes the programmatically built label for each station sensor containing the summary data queried from the master data frame interactive mapping is a critical component of any spatial web application and provides spatial context to the data for users who may not be familiar with the locations of the stations sensors based on their names alone the interactive mapping analysis features were included in sharks to allow users to identify spatial patterns across a network of stations in addition the interactive maps were designed to provide a brief summary of the data for users who may not have backgrounds in hydrology and could be intimidated by the other sharks analysis features in roanoke the sharks interactive maps have been used by city staff to study the variability of local rainfall events section 3 2 2 4 6 interactive plots the sharks interactive plots tab contains two interactive analysis features an interactive hyetograph fig 5 and an interactive hydrograph supplemental figure s6 these plots are created using the ggplot2 r package wickham 2016 and allow users to view individual data points by clicking on the data point on the plot or summarize the data from a specific time period by clicking and dragging a selection box around data on the plot termed brushing the shiny framework provides built in support for the click and brush interactions when users click or brush data in the interactive plots this data is subset from the master data frame and stored into a separate data frame for analysis during testing of sharks it was decided that clicking on individual incremental precipitation data points on the hyetograph was difficult and therefore viewing individual incremental precipitation data points was not a useful feature thus the sharks interactive hyetograph only supports brushing whereas the interactive hydrograph supports both brushing and clicking using the interactive hyetograph users can view the duration of a brushed time period as well as the total precipitation depth average precipitation intensity total precipitation depth duration and maximum precipitation intensity observed during the period in addition sharks displays all the incremental precipitation depth and intensity data from the brushed period in a sortable and downloadable data table these analysis features were included in sharks so users could identify how much precipitation was received at a meteorological station for a precise time period unlike other platforms which may only report precipitation in certain increments e g 1 6 12 or 24 h the interactive hydrograph can be used to determine the total discharge volume baseflow volume and stormflow runoff volume calculated via trapezoidal integration at a stream station for a brushed time period users may also click on data points in the interactive hydrograph to view the baseflow discharge and water quality parameters corresponding to that station and time these analysis features were included in sharks to allow users to investigate the temporal variation in the hydrologic datasets 3 case study city of roanoke the purpose of this section is to demonstrate the immediate added value of the sharks application to the city of roanoke and to describe some initial lessons learned that were made possible by the availability of this app although the initial use of the sharks app has been site specific to the city of roanoke the authors reiterate that any usgs or noaa asos site can be used in the application and the underlying r code is publicly available to be tailored to a different site as needed brendel et al 2018 the following sub sections therefore provide a conceptual description of the types of rapid analysis that this app provide that can be applied to other jurisdictions or watersheds 3 1 variability of local rainfall events an ancillary observation that was made during the analysis of storm events was that the intensity and depth of rainfall varies dramatically across the city s service area this had been observed anecdotally for many years in this region though the installation of the rain gage network and viewing capabilities of the sharks app allowed for empirical evidence of this phenomenon for example during a storm event that occurred on 5 27 18 the total observed rainfall depth at two stations varied by an order of magnitude across a distance of approximately two miles this variability was further demonstrated by estimating the ari at each station using the sharks app for another storm event that occurred on 5 17 18 and plotting this variability across the city s service area to do this a grid of aris was created by performing inverse distance weighting idw interpolation between the maximum ari during the storm at each location as calculated from sharks and the results are shown in fig 10 the maximum ari recorded at each station during this storm ranged from 1 7 years in the northeast to 35 5 years in the southwest part of the city and it is clear that this event was centered in a highly localized area the short term benefit of the app for this event was that city staff were able to characterize this event within hours of the end of the rainfall when the memory of the event was still fresh tying anecdotal observations to gage measurements these data were then distributed to news stations emergency managers and the public in response to flooding that occurred in the southwest part of the city the long term benefit of these data is that the high noted variability in rainfall across the city may lead to a more localized rainfall design paradigm for stormwater structures than the regional approach that has historically been taken based on noaa pfds methodology continued monitoring of the spatial distribution of rainfall using the sharks app will allow the city to determine if the high variability observed on 5 17 18 was an outlier or if order of magnitude variability across the city s service area is a typical pattern for rainfall in this area 3 2 storm sewer response to precipitation one issue of considerable interest in the city was the recurring flooding that occurs in the city s central business district cbd during brief intense rainfall see e g chittum 2017 the cause of this flooding was unclear as the stream draining the watershed was buried in large tunnels around the turn of the 20th century and it was therefore not possible to visually observe the stream during storm events to address this issue the nine storm sewer flow depth sensors previously described were installed at critical locations throughout the tunnel system fig 8 and the data from these sensors was integrated with precipitation data in the sharks app the primary interest was to evaluate the hydraulic grade line at these nine locations with respect to the tunnel soffit and manhole rims during a range of different storm event depths durations intensities and corresponding annual recurrence intervals aris pipes that surcharge under ari events that are smaller than a typical 10 year ari design storm could be targeted for hydraulic improvements such as replacement or sediment clearing the preliminary results of this work show that several of the tunnels are surcharging under 5 10 yr ari rainfall events the cause of this surcharging as revealed by the sharks app appears to be undersized local arterial drainage conveyances and sediment build up but not downstream capacity however it has been observed that during larger storm events runoff from an adjacent watershed creates a backwater effect that could prevent the tunnel system from efficient draining furthermore one of the tunnels appears to be flowing in two directions depending on the location of rainfall with respect to the contributing drainage areas the recommended engineering improvement is highly contingent on understanding flow direction through this tunnel and a new velocity sensor will be added to the sharks app to help better understand the complex hydraulics in this reach a second reason for the integration of tunnel hydraulics data into the sharks app was to develop a relationship between rainfall characteristics and the relative fullness of the tunnels under the cbd to serve as an early flood warning system for owners and tenants this would allow for city officials to alert stakeholders of a potential flash flood situation based on noaa s quantitative precipitation estimate so that temporary flood proofing measures could be put in place the sharks app has provided the necessary data analysis tools to develop these simple rainfall runoff relationships though the preliminary results have only shown a tentative correlation and more storm event data is needed 3 3 stream specific conductance during snow events during the march 12th 2018 snow event two large spikes in specific conductance were observed at the lick run stream station fig 11 the initial peak in specific conductance observed around 5 30am corresponded to only a minor increase in stream discharge upon analysis of the sharks app output the city of roanoke concluded that the initial spike in specific conductance was likely the result of application of de icer on an adjacent highway prior to the peak commute time and that the second specific conductance peak was due to the flushing of salts from the watershed during snow melt measured precipitation at the lick run station was only 0 06 inches during the initial specific conductance peak and 0 25 inches during the second specific conductance peak in the long term the ability of snow removal operators to track in stream specific conductance at near real time while viewing accumulated and predicted snowfall amounts through the sharks app will provide valuable information for decisions about additional application of de icing agents 4 conclusions overall the sharks app expands the capabilities of existing web based visualization platforms beyond exclusively data retrieval and visualization through the inclusion of a suite of data analysis tools the combination of data retrieval visualization and analysis tools allows users to quickly compare and export data from any usgs stream station and any noaa asos or usgs meteorological station in addition the app expedites the data analysis process with dynamically linked data visualization and analysis tools which allow users to refine the datasets and time periods analyzed based on the analysis results before exporting the data for further use furthermore the app introduces a framework for integrating time series data from local sources with data from government and commercial sources while providing options for restricting user access to the data through the use of google sheets no specialized coding software or expertise are needed to utilize the sharks app and the web interface ensures that the app can be accessed from a web browser without installing any additional software as implemented for the city of roanoke the sharks app has proven its usefulness as a tool for investigating hydrologic processes in urban watersheds however the sharks app is easily adapted to include preloaded monitoring stations for implementation in other locations and will be useful for other cities and organizations that need to retrieve and analyze hydrologic data from a variety of sources a streamlined version of the sharks app tailored towards use by the general public is currently under development and will provide a subset of the app s analysis features in a simplified user interface software availability software name stream hydrology and rainfall knowledge system sharks developers sharks team contact information cbrendel vt edu hardware required any web enabled device with a modern web browser software required internet browser program languages r html and css availability the sharks app is available at https bigbadcrad shinyapps io sharks and the underlying r code may be obtained on github at https github com bigbadcrad sharks or on zenodo brendel et al 2018 code is released under the mit license dependencies r package version dashboardthemes 1 0 2 data table 1 11 4 dataretrieval 2 7 3 dplyr 0 7 5 dt 0 4 ecohydrology 0 4 12 ggplot2 2 2 1 googleauthr 0 6 2 googleid 0 0 9001 googlesheets 0 2 2 gridextra 2 3 leaflet 2 0 2 lubridate 1 7 4 pracma 2 1 4 rcurl 1 95 4 10 rgdal 1 3 4 rwunderground 0 1 8 shadowtext 0 0 3 shiny 1 1 0 shinyalert 1 0 shinydashboard 0 7 0 shinyjs 1 0 shinywidgets 0 4 3 stringr 1 3 1 zoo 1 8 2 acknowledgements this work was supported by the city of roanoke virginia fund number 466172 and the virginia tech via department of civil environmental engineering special thanks to david woodson and the city of roanoke stormwater division for their valuable insights and helpful feedback during the development and testing of this app conflicts of interest declarations of interest none appendix a supplementary data the following is are the supplementary data to this article supplemental figure s1 summary tab data summary table providing an overview of precipitation and discharge data for selected meteorological and stream stations supplemental figure s1 supplemental figure s2 summary tab runoff volume coefficient summary table presenting runoff volume coefficients for each stream station and meteorological station combination supplemental figure s2 supplemental figure s3 summary tab average annual recurrence interval ari summary table displaying the interpolated ari for meteorological stations supplemental figure s3 supplemental figure s4 summary tab stacked hyetograph hydrograph displaying meteorological and hydrologic data for specified stations supplemental figure s4 supplemental figure s5 forecast tab weather underground weather sticker widget displaying current weather conditions and data table displaying the 10 day weather forecast supplemental figure s6 interactive plots tab interactive hydrograph with brushed data and volume summary supplemental figure s6 supplemental figure s7 tables tab combined data table presents time series data for all meteorological and stream stations joined by date time supplemental figure s7 supplemental figure s8 tables tab pfds tables can be displayed for all meteorological stations supplemental figure s8 multimedia component 9 multimedia component 9 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2019 03 003 
26212,flood detection and service fd s is of great significance to flood management and decision making but most of the state of art fd s methods are weak in timeliness and extensibility and could not reflect flood processes to resolve the abovementioned problems this paper proposed the process based fd s pfd s method based on sensor web pfd s is a four layer architecture the core components of which are the access adapter and process based detection rules and it is capable of heterogeneous sensor access flood detection and flood phase adaptive services a prototype was developed based on the pfd s method two floods occurring in the huanghan basin hubei china with the area of 5 04 104 km2 during july 2016 was selected as the case studies to validate the pfd s method and prototype the results demonstrated that the proposed pfd s method and prototype could achieve the instant flood process detection and services in 2 7 min keywords flood detection and service full life cycle sensor web water level prediction 1 introduction hydrological disasters took the largest share 51 7 in natural disasters occurrence worldwide in 2016 causing 5092 deaths 78 1 million people reported affected and damages totaling almost us 59 billion among the hydrological disasters 92 7 were floods with the total occurrence of 164 times worldwide the total deaths people reported affected and damages of floods accounted for 92 9 99 7 and 98 8 of the total hydrological disasters respectively guha sapir et al 2016 as the natural disaster type with the highest occurrence frequency and resulting in the most serious casualties and economic losses the requirements for decreasing the life loss and property damage caused by floods are imperious all over the world hirsch and archfield 2015 adequate preparations effective warnings and timely public responses are all of great significance to reduce the losses from flooding demir and krajewski 2013 the flood detection and service fd s is exactly to detect floods in advance and provide the corresponding services i e alert therefore it is of urgent need to conduct researches on fd ss there are many literature relevant to fd ss according to the variable types fd s methods can be divided into inundated area based iab dai et al 2015 sghaier et al 2018 submerged depth based sdb gao et al 2018 and direct observation variables based dovb detections i e water level acosta coll et al 2018 and precipitation darand and sohrabi 2018 etc iab flood detections can be mainly divided into two types including the study of flooded area extraction algorithms cohen et al 2016 du et al 2017a 2017b liu et al 2018 and the study of flood detection systems and applications auynirundronkool et al 2012 dai et al 2015 martinis and rieke 2015 schlaffer et al 2015 mueller et al 2016 pekel et al 2016 amitrano et al 2018 cian et al 2018 giordan et al 2018 martinis et al 2015 sghaier et al 2018 tong et al 2018 veh et al 2018 the study of flood detection systems and applications includes the global flood detection system gfds revillaromero et al 2014 the european flood alert system efas arnal et al 2018 the global flood awareness system glofas alfieri et al 2013 and the dartmouth flood observatory dfo reager and famiglietti 2009 etc the data sources of iab flood detections are usually satellite images satellite images have a wider spatial coverage which makes them suitable for flooded area determination but the temporal resolution of remotely sensed data can be too low to acquire available data during floods when compared with ground observations crowdsourcing data were also used to determine flood ranges poser and dransch 2010 fohringer et al 2015 witherow et al 2017 feng and sester 2018 but crowdsourcing data is the most dispersed data source and features high redundancy thus it is often utilized as an auxiliary data sources for fd s jongman et al 2015 sdb detection often simulates and analyzes the spatio temporal variation of land surface runoff through hydrological or hydraulic models cane et al 2013 caruso et al 2013 moreno et al 2013 shi et al 2015 to completes the fd s detection accuracy of the sdb methods are usually relatively high via the repeated region oriented process of iterative calculation and model parameter calibration although with high detection accuracy for specific areas lots of manpower and computation resources should be invested before these regional detection models could be extended to other regions meanwhile the model simulation can be quite complicated with complex equations of mathematical physics and dozens of parameters and it simultaneously puts forward high demands for historical observations and records therefore it is quite difficult for sdb detections to be used for the flood detecting in regions lack of long time series and comprehensive hydrological and meteorological data dovb detection is usually implemented by threshold filtering of directly observed variables i e water level acosta coll et al 2018 or rainfall darand and sohrabi 2018 ground observations with high temporal resolution ensure the timeliness of the dovb detections and make it suitable for the rapid detection and responding of floods acosta coll et al 2018 but currently this kind of methods mainly focus on the study of real time sensor access and alert releasing for specific regions and the universality and extensibility of the existing dovb flood detections are often very poor resulting in that they could not be reused in other regions or occasions easily in addition the most importantly all the existing fd s methods have the common defects that is they could only determine whether floods would occur or not at specific time instants being unable to achieve the comprehensive understanding the whole process of flood occurrence and development in summary for the rapid detection of floods occurring in the regions without adequate data support the dovb detection utilizing ground observation is more feasible and effective acosta coll et al 2018 however there are still mainly two problems faced with the existing dovb detections utilizing ground observations 1 the abovementioned common defects the rapid full life cycle fd s is absent making it difficult to get a clear picture of the whole process of floods rapidly 2 the universality and extensibility of fd ss are very low with the same or similar problems repeatedly studied resulting in a large amount of resource wastes to solve the abovementioned two problems the objective of this paper is 1 propose a method to realize the rapid flood phase determination and service for every data record and 2 make sure that the proposed method can be reused and extended to realize the full life cycle fd ss meanwhile ensuring the universality and extensibility the process based fd s pfd s designed based on sensor web broring et al 2011 was proposed in this paper the pfd s method is to provide a way for full life cycle fd s and meanwhile ensures the timeliness universality and extensibility of the method it employs ground based observations as data source being able to precisely determine the flood phase according to data changes and to offer customized services to satisfy the varied flood phase based responding requirements in the forthcoming sections we illustrated the pfd s method and validated its feasibility for full life cycle fd ss the development of the pfd s method was presented in section 2 the overall architecture was described in section 2 1 the core components illustrated in section 2 2 and the internal interactions presented in section 2 3 the design implementation and instance of the pfd s prototype was stated in section 3 with the prototype design and implementation described in section 3 1 and 3 2 separately the experiments and results were provided in section 4 with the experimental area and data described in section 4 1 and the sensor provider and event subscriber perspectives elaborated in section 4 2 and 4 3 respectively the discussion about the pfd s method was provided in section 5 including the accuracy analysis in section 5 1 comparisons with other fd s systems in section 5 2 and limitation statement in section 5 3 finally section 6 summed up this work and described possible future directions for this study 2 pfd s method floods occur and develop in the form of process and determining the flood phases precisely is of great importance to flood monitoring different from traditional fd ss the characteristics of the pfd s method is adopting the idea of process management in the detection and service of floods to be specific the main idea of the pfd s method is to firstly divide the flood process into different phases then determine the flood phase type based on the filtering of the flood observations and finally provide flood services according to the requirements of different flood phases 2 1 four layer architecture in order to achieve the goal of flood phase detection and service based on flood observations the data access the flood phase detection and the phase adaptive flood service are mandatory for the pfd s method therefore as demonstrated in fig 1 the pfd s method consists of the sensor layer the data access layer the flood detection layer and the flood service layer from bottom to top the sensor layer is to provide the data source and it is composed of the heterogeneous physical hydrological and meteorological sensors i e water level gauges etc the data access layer is responsible for the access of heterogeneous sensors and it consists of the access adapter aa section 2 2 1 and sensor observation service sos broring et al 2012 the flood detection layer is used to filter the sensor observations and judge on the type of flood phases and its components include sensor event service ses echterhoff and everding 2008 sos ses feeder middleware of sos and ses the processing unit web notification service wns simonis and echterhoff 2006 and the process based detection rules pdr section 2 2 2 the flood service layer mainly to provide customized services according to the type of flood stages and in this paper the service types include the water level prediction wlp service section 3 2 the flood warning service and the flood statistics service the focus of the pfd s method is on the data access layer the flood detection layer and the flood service layer the overall architecture of the pfd s method is based on the open geospatial consortium ogc information models and service interfaces of sensor web zhang et al 2018 the sos ses sos ses feeder and wns used in the pfd s method are the existing ogc service interfaces but these services are isolated and cannot be used for fd s directly the major contribution of the pfd s method is to connect and employ these ogc information models and service interfaces to make them universally applicable for the detection and service of flood process as marked red in fig 1 the focus of the pfd s method is on the aa section 2 2 1 of the data access layer the pdr section 2 2 2 and the internal interactions and the processing unit section 2 3 of the flood detection layer and the wlp service section 3 2 of the flood service layer 2 2 core components 2 2 1 access adapter sos could achieve the sensor access in a standardized way broring et al 2012 but due to the diversity in the flood sensor types and communication protocols how to transform the diverse flood sensors and observations into the standard forms is still a problem aa could act as a middleware between physical flood sensors and sos and it was proposed for the unified access of heterogeneous flood sensors into sos to be specific aa could be utilized to discover the flood sensors in specific spatiotemporal ranges remove the abnormalities from the sensor observations and access the sensors and observations into sos as shown in fig 2 aa receives sensor metadata from sos and observation metadata from sensors and delivers unified observations into sos aa is composed of the data reception dr unit the observation filtering of unit and the observation encoding oe unit and the parameters of all the three units are defined in the configuration file the dr unit is mainly intended for the acquisition and parse of data streams the of unit allows users to perform the observation selection the oe unit takes the responsibility of unified encoding for observations it takes nine steps to use aa to access heterogeneous flood sensors into sos 1 flood sensor metadata is encoded according to sensor model language sensorml botts and robin 2007 and registered into sos 2 the dr unit acquires the flood sensor metadata information from the sensorml files 3 the flood sensor access model is constructed based on the uniform sensor access model template depicted in fig 3 and the data stream is organized in accordance with the structure of the data array sensorid propertyid dataposition datalength dataratio 4 data stream is acquired and parsed 5 7 the attribute temporal and spatial filtering of the data streams are performed respectively 8 the data is encoded according to observations measurements o m cox 2007a 2007b and 9 observations are inserted into sos steps 3 4 are completed in the dr unit steps 5 7 in the of unit and step 8 in the oe unit the dr unit has to change with flood sensor types and communication protocols and the design of separating it from the of and oe unit could enable aa to work more flexibly and efficiently the parameters of the dr of and oe unit are predefined in the configuration file for flood sensor and data stream access 2 2 2 process based detection rules pdr is to define a series of rules to determine the flood process specifically flood phases it can be combined with ses to implement the process based detection of floods the way of representing flood processes by the four phases of diagnosis preparedness response and recovery is adopted in this paper chen et al 2015 the calculation formula of the pdr is as follows where formulas 1 2 3 and 4 are the conditions needed to be met for each transition between two adjacent flood stages i e diagnosis preparedness preparedness response response recovery and recovery diagnosis respectively floods are strictly developed in the sequence of diagnosis preparedness response and recovery in the pdr and satisfying formula 1 indicates flood in the diagnosis stage meeting both formulas 1 and 2 means flood in the preparedness stage simultaneously satisfying formula 1 2 and 3 represents flood in the response phase meeting formulas 1 2 3 and 4 at the same time stands for flood in the recovery phase 1 fret1 wl w1 f1 2 fret2 w1 wl w2 f2 3 fret3 wl w2 f3 4 fret4 wl w3 f4 where wl refers to water level freti condition i i 1 2 3 4 refers to the occurrence frequency of water level values meeting the condition i in the temporal range of ti i 1 2 3 4 w1 w2 and w3 are the water level thresholds of different flood stages respectively satisfying the conditions of w1 w2 w3 w2 t1 t2 t3 and t4 are different time window thresholds and satisfies t1 t2 t3 t4 f1 f2 f3 and f4 are occurrence frequency thresholds of different flood phases as the safety and warning water level values are determined by analyzing lots of historic flood records and regional environments and they are of great guiding significance for detecting floods w1 and w2 here equal to the safety and warning water levels separately and w3 usually satisfies the condition of w1 w3 w2 in pdr the response phase is essential and only the flood events with the response phase are the true flood events that will be dealt with or recorded to reduce the error detection rate 2 3 internal interactions the mechanism of information delivery and internal interactions among the layers of the pfd s method is elaborated in this section as the sensor layer only acts as the sensor and data provider in the pfd s method and it does not have many internal interactions with other layers therefore it will not be further elaborated here the internal interactions of the data access layer flood detection layer and the flood service layer in the pfd s method are demonstrated in fig 4 the internal interactions of the data access layer flood detection layer and the flood service layer could be interpreted from the two perspectives of flood sensor providers and flood event subscribers the two perspectives correspond to the flood sensor oriented and the flood event subscriber oriented service patterns of the pfd s method as for the flood sensor provider oriented service pattern the major contribution of the pfd s method is to provide the mechanism for sensor data publishing and sharing which can be realized by the following five steps 1 heterogeneous sensors are accessed and sensor observations are inserted into sos by aa via the registersensor and insertobservation operations respectively 2 sos ses feeder actively sends the describesensor request to sos and receives the sensorml files from sos 3 sos ses feeder registers the sensor lists to ses through the regiterpublisher operation and gets the registered publisherid from ses 4 sos ses feeder sends the getobservation request to sos and receives the o m files from sos 5 sos ses feeder notifies the sensor observation lists to ses after flood sensor providers publish and share their flood monitoring sensors and observations flood event subscribers could subscribe the floods of their interest by submitting their flood event subscription requests the flood event subscriptions could be implemented by the following six steps 1 flood event subscriber sets the parameters for the pdr i e flood sensor id and threshold etc and the wlp service i e maximum error and iteration etc section 3 2 2 pdr is encoded according to the event pattern markup language eml everding and echterhoff 2008 and the flood event subscription model is formed 3 the flood event subscription model is submitted to ses 4 ses performs the data filtering completes the current flood stage judgment according to the flood event subscription model and further transmits the current flood event phase information to the processing unit 5 the processing unit performs the flood stage change detection and deliver the true flood phase information to wns and 6 corresponding flood service is activated and the true phase detection result and notifications are returned to the flood event subscriber 3 pfd s prototype design and implementation 3 1 prototype design a prototype was developed based on the pfd s method proposed in this paper the prototype is designed conforming to the ogc sensor web standards and it is browser server based with the client and server separated the pfd s prototype can be divided into four tiers including the database tier the intermediary service tier the business logic tier and the user interaction tier the database tier is composed of the sos database the sos ses feeder database the subscription management database of ses and the flood database the intermediary service tier consists of apache storm sos ses sos ses feeder and wns the business logic tier is the core tier of the pfd s prototype and it is composed of the businesses of the flood event subscription encoding and registration ogc sensor web standards parsing login logic judgement flood phase services and data access etc there are six function modules in the user interaction tier including the user registration login the flood sensor access management the flood sensor map display the flood event subscription the flood event management and the flood phase service modules 3 2 prototype implementation the whole architecture of the pfd s prototype was implemented based on the open source code of 52 north 52n https 52north org because of its supporting more operations and continuously updating the client adopts the react framework and utilizes the components of the ant design 2 13 11 to implement the basic design the react amap 1 1 3 to complete the map function and the react highcharts 15 0 0 to enrich the data display the server employs the spring mvc and hibernate framework supported by apache storm 1 0 0 52n sos 3 5 0 52n ses 1 2 2 52n sos ses feeder 1 0 0 and wns 0 1 0 the database used is postgresql 9 2 which is open source and features powerful functions in supporting spatial operations the programming language employed is java javascript css and html improvement was made on the wlp service of the pfd s prototype so how to implement it is elaborated here the wlp service in this paper adopts the back propagation neural network bpnn li et al 2017 method to predict water levels bpnn has the capability of auto learning without requiring prior knowledge the back propagation mechanism of errors and could realize any nonlinear mapping from input to output with higher accuracy making it suitable for resolving the problems with complicated internal mechanisms i e wlp ghose et al 2010 due to the fact that the accuracy of the bpnn results can be affected by many factors except for basin precipitation this paper makes improvement on the input vectors to improve the prediction accuracy instead of just using basin precipitation to make predictions the wlp service here adopts nine factors including upstream water level air pressure air temperature ground temperature wind speed precipitation evapotranspiration relative humidity and sunlight exposure duration of the watershed as input vectors for downstream water level predicting 4 experiments and results 4 1 experimental area and data with the total length of 1532 km the hanjiang river is the first longest tributary of the yangtze river the longest river in china and asia and the third longest river worldwide and it also has the maximum annual variation of runoff among all the tributaries of the yangtze river huanghan basin is one of the sub basins located in the middle and lower reaches of the hanjiang basin with the riverbed slope becoming smaller and the water flow turning slow floodings frequently occur in the huanghan basin the area of the huanghan basin is 5 04 104 km2 with the terrain of the basin high in the northwest while low in the southeast and the geology is mainly composed of middle or low mountains in the northwest while dominated by plains or hills in the southeast located in the subtropical monsoon region the huanghan basin has the mild and humid climate and an ample annual precipitation of 700 1000 mm chen et al 2007 however the rainfall distribution is spatial temporally uneven with the precipitation gradually increasing from the upper to the lower basin and the runoff from may to october accounting for about 75 of the whole year therefore it is of great significance to perform flood detecting research in the huanghan basin as shown in fig 5 there are eight hydrological stations i e huangjiagang xiangyang huangzhuang shayang yuekou xiantao and diaochalake in the upstream and hanchuan station downstream and five meteorological stations i e fangxian gucheng zhongxiang tianmen and xiaogan evenly distributed in the huanghan basin two flood events occurred in the experimental area in july 2016 were taken as examples to verify the feasibility and validation of the pfd s method the annual changes of precipitations in the xiaogan station the nearest meteorological station to the hanchuan hydrological station and those of water levels in the hanchuan station in 2016 are shown in fig 6 the water levels of hanchuan station were used in the full life cycle flood detecting the seven upstream hydrological observation stations i e huangjiagang xiangyang huangzhuang shayang yuekou xiantao and diaochalake and five evenly distributed meteorological stations i e fangxian gucheng zhongxiang tianmen and xiaogan were employed in predicting the water levels of hanchuan station among them the monitoring variable of the hydrological stations is water level and the monitoring variables of the meteorological stations are eight kinds of meteorological factors including air pressure air temperature land surface temperature wind speed precipitation evapotranspiration relative humidity and sunlight exposure duration the data used in the experiment ranged from january 1 2000 to december 31 2016 and the data sampling frequency was once per day in the experiment the flood simulation began from july 1 2016 and to facilitate the simulation of the historic floods the sampling frequency of the experimental data was all set to once per minute therefore the observation at the nth minute from the beginning of the experiment corresponded to that at the nth day from july 1 2016 4 2 sensor provider perspective sensor providers could utilize the data access function of the pfd s prototype to publish and share their sensor observations the data access interface of the pfd s prototype is shown in fig 7 sensor providers could complete the data access by firstly encoding the sensor information according to sensorml secondly defining the parameters of the data array in the data access layer and finally uploading the configuration file for access in this paper the configuration files of the eight hydrological stations i e hanchuan huangjiagang xiangyang huangzhuang shayang yuekou xiantao and diaochalake and the five meteorological stations i e fangxian gucheng zhongxiang tianmen and xiaogan were all uploaded for use the communication protocols of all these sensors include modbus and xph after sensor providers accessed these sensors into the pfd s prototype event subscribers could select and activate the sensors they needed for further processing 4 3 event subscriber perspective from the perspective of event subscribers after submitting flood event subscription to the pfd s prototype they could acquire the detection results and receive messages from the corresponding flood services 4 3 1 event subscription to complete the flood event subscription there are three operations requiring interactions with flood event subscribers including the sensor selection and the parameter setting of pdr and the wlp service the sensor selection interface of the pfd s prototype is shown in fig 8 with all the sensors accessed listed the sensor s selected here would be utilized in pdr for full life cycle flood detecting hanchuan station was selected in the experiment to provide the data source for detecting the floods occurring in the huanghan basin after the user completed the sensor selection they were required to input the threshold time window repeated times and other parameters required in pdr the parameter setting interface of pdr in the pfd s prototype was shown in fig 9 and the system would automatically generate the subscription model when received the parameters the water level thresholds w1 28 w2 29 and w3 28 5 were used in the mitigation preparedness response and recovery phases of pdr in this experiment w1 28 and w2 29 were the guaranteeing and warning water levels of the hanchuan hydrological observation station issued by the hubei provincial department of water resources respectively and w3 28 5 was determined by trial and error the time windows and occurrence frequency thresholds of all the four phases are one second and once respectively fig 10 shows the parameter configuration interface of the wlp service the parameters required to be set include the learning rate the maximum iterations the maximum error and the hydrological or meteorological stations and their observed properties involved the water level of the seven hydrological stations i e huangjiagang xiangyang huangzhuang shayang yuekou xiantao and diaochalake and the eight types of observed properties i e air pressure air temperature land surface temperature wind speed precipitation evapotranspiration relative humidity and sunlight exposure duration of the five meteorological stations i e fangxian gucheng zhongxiang tianmen and xiaogan were all selected as the input of the wlp service in this experiment this experiment used the hydrological and meteorological data of the huanghan basin from 2000 to 2015 as the training data to construct the bpnn and the data in 2016 as the test data specifically the observations in the continuous 100 time instants were used as input and the water levels on the 101st and 102nd time instants were used as output the network structure of the wlp service was set as 47 4 7 1 through trial and error with 47 and 1 the dimensions of the input and output vector separately and 4 and 7 determined by keeping the other unchanged meanwhile lowering the error into minimum the learning rate was set to 0 01 via the mesh filter method the maximum error set to 1 0 10 8 and the maximum number of iterations set to 5 0 104 times in addition the e mail receiving address of the flood event notification message and the alias of the flood event were also set in this module the parameter summary of pdr and the wlp service was shown in fig 11 4 3 2 detection results once receiving the parameters set for pdr the pfd s prototype automatically transformed them into the flood event subscription model based on the subscription model ses filtered the data streams of the sensor selected and determined the current flood phase then the processing unit compared the current flood phase with those of the previous time instants and made judgment on the true flood phase the pfd s method could realize the flood phase judgement for each data item thus here it could determine the flood phase for each minute the detection result of flood process was demonstrated in fig 12 as the historic flood events were simulated as real time in the experiment there exists a correspondence between the experimental time and the actual time the mapping from the experimental time to the actual time was displayed in table 1 the flood simulation experiment began at 13 28 on april 12 2018 and it corresponded to the actual time of july 1 2016 every minute in the experiment was equivalent to one day of the actual time therefore 13 29 13 33 13 39 13 41 13 45 13 46 13 47 13 53 and 13 54 representing july 2 july 6 july 12 july 14 july 18 july 19 july 20 july 26 and july 27 of 2016 respectively as can be seen from table 1 there were two complete flood events from july 1 to july 30 2016 the first flood event lasted from july 1 to july 14 2016 in which it began on july 1 entered the preparedness phase on july 2 further went into the response phase on july 6 then was back to the recovery phase on july 12 and finally ended on july 14 in 2016 the second flood event lasted from july 18 to july 27 in which it began on july 18 entered the preparedness phase on july 19 further went into the response phase on july 20 then was back to the recovery phase on july 26 and finally ended on july 27 in 2016 4 3 3 flood services there were three types of flood services in the pfd s prototype including the wlp the flood warning and the flood statistics services the wlp service was utilized in all the four flood phases of diagnosis preparedness response and recovery the flood warning service was adopted in both the preparedness and response phases the flood statistics service was only employed in the discovery phase 4 3 3 1 wlp service the wlp service of the pfd s prototype was realized based on bpnn and it could predict the water levels in the future 48 time instants by providing users with future trends of water level changes the wlp service can be combined with pdr to enable users to make timely decisions and responses the wlp service worked through the whole process of floods and fig 13 showed the record interface of the wlp service in the pfd s prototype the prediction result chart on the right side of fig 13 displayed the wlp results in 102 time instants of which 100 time instants were before the prediction time and 2 time instants afterwards according to the mapping rule of time the first record in the middle of fig 13 demonstrated that the huanghan basin flood event was in the preparedness phase on july 3 2016 and the predicted water level on july 4 and 5 2016 were 28 41 and 28 42 m respectively indicating that the water level would continue to rise in the near future 4 3 3 2 flood alert service the purpose of the flood alert service is to clearly inform the user of the current flood phase and the wlp results in the future two time instants of the flood events they subscribed once floods entered the preparedness or response phase the pfd s prototype would send a flood warning notification to the user by e mail fig 14 a shows the management interface of the flood alert messages in the pfd s prototype fig 14 b corresponded to the third flood alert record in fig 14 a and demonstrated the content of the flood alert e mail sent by the pfd s prototype it informed users of that the huanghan basin flood event he she subscribed had entered the response stage on july 6 2016 and the water levels on july 7 and 8 2016 would be 29 17 and 29 03 m respectively the predicted results indicated that the water levels would decrease slowly and the flood would be relieved soon 4 3 3 3 flood statistics service flood statistics service is to complete the information archiving and management of flood events according to the four stages of diagnosis preparedness response and recovery after the flood event was completely over the pfd s prototype would invoke the flood statistics service to make statistics on the information of the whole flood process including the maximum water level values and the beginning and ending time of different stages etc fig 15 showed the two complete flood statistics records of this experiment the right side of fig 15 displayed the details of the second flood event statistics record indicating that the event began on july 18 entered the preparedness phase on july 19 further went into the response phase on july 20 and on the same day the water level reached the maximum of 29 66 m then was back to the recovery phase on july 26 and finally ended on july 27 in 2016 5 discussion 5 1 accuracy analysis 5 1 1 detection accuracy the pfd s detection results were compared with the authoritative flood record to achieve the evaluation of the detection accuracy as displayed in table 2 the comparison results demonstrated that in the experiment the pfd s method could accurately detect the beginning time of the response phase of the flood events without no delay or advance moreover the pfd s method is able to detect the entering time of the preparedness time of the flood events 1 or 4 days ahead proving that the pfd s method proposed in this paper possessed the capability of early detection and high detection accuracy at the same time the detection results could elaborately divide the flood process into diagnosis preparedness response and recovery phases more accurately characterizing the floods and providing solid foundations for the subsequent statistics and analysis 5 1 2 prediction accuracy to evaluate the prediction accuracy of the wlp service the prediction results of the wlp service adopting multiple meteorological data were compared with those derived only from the precipitation data and the ground truth the accuracy analysis of the water level values acquired from the three different methods during the periods of the 1st and 2nd flood was displayed in table 3 for the 1st flood lasting from july 1 2016 to july 14 2016 the mean absolute error mae mean relative error mre and root mean square error rmse of the wlp service and the only precipitation based model are 0 67 m and 1 02 m 2 35 and 3 59 0 9942 and 2 1081 separately for the 2nd flood lasting from july 18 2016 to july 27 2016 the mae mre and rmse of the mwlp model and the only precipitation based model are 0 52 m and 0 56 m 1 81 and 1 95 0 6439 and 0 7345 respectively from the abovementioned statistics analysis it was proved that by taking the meteorological factors into consideration the wlp service results were significantly improved when compared with those only using the precipitation data and had quite high prediction accuracy 5 2 fd s system comparisons the pfd s prototype developed in this paper was compared with the existing fd s systems i e gfds revillaromero et al 2014 efas arnal et al 2018 glofas alfieri et al 2013 and dfo reager and famiglietti 2009 from the perspectives of flood monitoring notification to users statistics forecast universal data encoding and access user subscription support processness service extensibility and service instantaneity the comparison results were shown in table 4 5 2 1 universality the pfd s prototype developed in this paper was implemented in accordance with the information models and service interfaces of ogc sensor web broring et al 2011 which enables the unified sharing processing and serving of sensor resources the overall architecture of the pfd s prototype is universal for both sensor providers and event subscribers of floods sensor providers could realize their sensor and observation resource publishing and sharing by conforming to the unified information encoding and data access operations and event subscribers could subscribe their requests and acquire the flood event information of their interest through supplementing the information of the event subscription template defined inside the prototype compared with other systems communicating with specialized standards the pfd s system designed based on the universal standards has better universality 5 2 2 instantaneity three types of time lags exist in this system i e the time lag when transforming the data streams into message flows named tl 1 the time delay caused by flood phase judgment named tl 2 and the responding time lag of each service unit named tl 3 tl 1 was usually produced during the process of sos ses feeder firstly pulling flood sensor observations from sos and then registering them into ses at a specified time interval tl 2 was often generated in the procedure of processing the messages and detecting the flood phase changes tl 3 was often caused by the calculation and treatment steps defined in the algorithms of the flood alert tl 3 1 prediction tl 3 2 or statistics services tl 3 3 tl 1 tl 2 tl 3 1 tl 3 2 and tl 3 3 averages to respectively 24 6 92 6 0 8 42 2 and 1 2 s after the experiment was repeated 10 times under the same computation environment as the three steps of data stream transformation flood phase judging and wlp are mandatory for flood detection the time delay of acquiring the detection results is the sum of tl 1 tl 2 and tl 3 2 that is the pfd s system could achieve the flood detection in 159 4 s with high instantaneity and demonstrating great superiority over other fd s systems i e gfds efas glofas and dfo updating at a daily basis 5 2 3 extensibility to access more sensors serve more flood detections and satisfy more diverse flood service requests the pfd s prototype was designed to be extensible the extensibility of the pfd s prototype lies in the three following aspects the sensor types and numbers the flood detection models and the flood service types the extensibility of sensor types and numbers is reflected in the flexibility of the data access layer of the pfd s prototype due to aa which separates the data reception unit from the data filtering and encoding unit new types of sensors could be accessed easily by modifying the configuration file if event subscribers want to utilize other flood detection models instead of the pdr proposed in this paper for flood detection what they are only required to do is encapsulating their new flood detection model according to the patterns of eml and applying them in ses as for flood services the iphaseservice interface was predefined in the pfd s prototype and new types of flood phase services could be added by implementing the executeservice method of the interface and modifying the configuration file for the type announcement of flood phase services correspondingly 5 2 4 processness as for processness instead of only detecting whether flood events would occur or not coarsely as the existing fd s methods and systems the pfd s method and prototype proposed in this paper could precisely determine the flood phase according to each item of flood sensor observations this flood process detection is able to help in intuitively embodying the whole occurrence and development process of floods making it more efficiently and conveniently for flood preparedness and responding 5 3 limitations although featuring great universality instantaneity extensibility and processness the pfd s method and prototype also has its own weaknesses firstly the four phases defined in pdr occurred sequentially so flash floods which occur in the time period shorter than the data sampling intervals will not be correctly detected but this problem can be resolved by higher data sampling rate secondly this paper only utilized pdr to provide a useful exploration for flood process detection and more elaborated models should be proposed and applied in flood detecting additionally only three types of flood services were provided in the pfd s prototype not enough for diverse service requirements from water authorities and citizens and more service types i e route planning and supply allocation etc should be further added 6 conclusion and outlook as for the rapid and continuous flood detection for regions without adequate data support there are mainly two problems faced with them including how to rapidly implement the full life cycle flood detecting and how to further ensure the universality and extensibility of the flood detection method to solve the abovementioned two problems this paper proposed the pfd s method by combining pdr with sensor web and designed and implemented the pfd s prototype to validate the pfd s method and prototype the huanghan basin hubei china was selected as the experimental area and the two flood events occurring in july 2016 was selected as flood examples two floods were detected in the experiment and the flood phases of them were accurately divided with the process of the 1st and 2nd floods being july 1 july 2 july 6 july 12 july 14 and july 18 july 19 july 20 july 26 july 27 in 2016 respectively compared with the authoritative flood records the 1st and 2nd flood were detected to enter the preparedness phase four and one days in advance respectively and the detected beginning time of the response phase of the two floods were totally accurate without delay or advance in addition the mae mre and rmse of the wlp service for the 1st and 2nd flood was 0 67 m 2 35 0 9942 and 0 52 m 1 81 and 0 6439 separately and the corresponding jobs of flood alert e mail sending and flood information statistics were also completed in addition the whole flood process detection and service of the pfd s method could be finished instantly in 2 7 min moreover the pfd s prototype features great universality extensibility and processness when it was in comparison with other fd s prototypes i e gfds efas glofas and dfo in summary the pfd s method proposed in this paper was proved to have fulfilled its design objective of rapid full life cycle flood detection while possessing the capability of universality and extensibility but flood service types provided by the pfd s prototype were limited more flood service types need to be expanded in future researches to meet more diversified flood management requirements acknowledgements this work was supported by grants from the china postdoctoral science foundation no 2017m622502 the national key r d program of china no 2017yfb0503800 the national natural science foundation of china no 41601406 41771422 the natural science foundation of hubei province china no zrms2017000698 the crsri open research program china no ckwv2018487 ky and the liesmars special research funding china we thank the national meteorological science data sharing service platform of china for the free download of the meteorological data appendix acronyms used in this paper was listed in table a 1 table a 1 acronyms used in this paper table a 1 acronyms complete expressions fd s flood detection and service pfd s process based flood detection and service iab inundated area based sdb submerged depth based dovb direct observation variables based gfds global flood detection system efas european flood alert system glofas global flood awareness system dfo dartmouth flood observatory ogc open geospatial consortium aa access adapter sos sensor observation service sensorml sensor model language o m observations and measurements ses sensor event service wns web notification service pdr process based detection rules wlp water level prediction dr data reception of observation filtering oe observation encoding bpnn back propagation neural network eml event pattern markup language mae mean absolute error mre mean relative error rmse root mean square error appendix a supplementary data the following is the supplementary data to this article multimedia component 1 multimedia component 1 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2019 03 004 
26212,flood detection and service fd s is of great significance to flood management and decision making but most of the state of art fd s methods are weak in timeliness and extensibility and could not reflect flood processes to resolve the abovementioned problems this paper proposed the process based fd s pfd s method based on sensor web pfd s is a four layer architecture the core components of which are the access adapter and process based detection rules and it is capable of heterogeneous sensor access flood detection and flood phase adaptive services a prototype was developed based on the pfd s method two floods occurring in the huanghan basin hubei china with the area of 5 04 104 km2 during july 2016 was selected as the case studies to validate the pfd s method and prototype the results demonstrated that the proposed pfd s method and prototype could achieve the instant flood process detection and services in 2 7 min keywords flood detection and service full life cycle sensor web water level prediction 1 introduction hydrological disasters took the largest share 51 7 in natural disasters occurrence worldwide in 2016 causing 5092 deaths 78 1 million people reported affected and damages totaling almost us 59 billion among the hydrological disasters 92 7 were floods with the total occurrence of 164 times worldwide the total deaths people reported affected and damages of floods accounted for 92 9 99 7 and 98 8 of the total hydrological disasters respectively guha sapir et al 2016 as the natural disaster type with the highest occurrence frequency and resulting in the most serious casualties and economic losses the requirements for decreasing the life loss and property damage caused by floods are imperious all over the world hirsch and archfield 2015 adequate preparations effective warnings and timely public responses are all of great significance to reduce the losses from flooding demir and krajewski 2013 the flood detection and service fd s is exactly to detect floods in advance and provide the corresponding services i e alert therefore it is of urgent need to conduct researches on fd ss there are many literature relevant to fd ss according to the variable types fd s methods can be divided into inundated area based iab dai et al 2015 sghaier et al 2018 submerged depth based sdb gao et al 2018 and direct observation variables based dovb detections i e water level acosta coll et al 2018 and precipitation darand and sohrabi 2018 etc iab flood detections can be mainly divided into two types including the study of flooded area extraction algorithms cohen et al 2016 du et al 2017a 2017b liu et al 2018 and the study of flood detection systems and applications auynirundronkool et al 2012 dai et al 2015 martinis and rieke 2015 schlaffer et al 2015 mueller et al 2016 pekel et al 2016 amitrano et al 2018 cian et al 2018 giordan et al 2018 martinis et al 2015 sghaier et al 2018 tong et al 2018 veh et al 2018 the study of flood detection systems and applications includes the global flood detection system gfds revillaromero et al 2014 the european flood alert system efas arnal et al 2018 the global flood awareness system glofas alfieri et al 2013 and the dartmouth flood observatory dfo reager and famiglietti 2009 etc the data sources of iab flood detections are usually satellite images satellite images have a wider spatial coverage which makes them suitable for flooded area determination but the temporal resolution of remotely sensed data can be too low to acquire available data during floods when compared with ground observations crowdsourcing data were also used to determine flood ranges poser and dransch 2010 fohringer et al 2015 witherow et al 2017 feng and sester 2018 but crowdsourcing data is the most dispersed data source and features high redundancy thus it is often utilized as an auxiliary data sources for fd s jongman et al 2015 sdb detection often simulates and analyzes the spatio temporal variation of land surface runoff through hydrological or hydraulic models cane et al 2013 caruso et al 2013 moreno et al 2013 shi et al 2015 to completes the fd s detection accuracy of the sdb methods are usually relatively high via the repeated region oriented process of iterative calculation and model parameter calibration although with high detection accuracy for specific areas lots of manpower and computation resources should be invested before these regional detection models could be extended to other regions meanwhile the model simulation can be quite complicated with complex equations of mathematical physics and dozens of parameters and it simultaneously puts forward high demands for historical observations and records therefore it is quite difficult for sdb detections to be used for the flood detecting in regions lack of long time series and comprehensive hydrological and meteorological data dovb detection is usually implemented by threshold filtering of directly observed variables i e water level acosta coll et al 2018 or rainfall darand and sohrabi 2018 ground observations with high temporal resolution ensure the timeliness of the dovb detections and make it suitable for the rapid detection and responding of floods acosta coll et al 2018 but currently this kind of methods mainly focus on the study of real time sensor access and alert releasing for specific regions and the universality and extensibility of the existing dovb flood detections are often very poor resulting in that they could not be reused in other regions or occasions easily in addition the most importantly all the existing fd s methods have the common defects that is they could only determine whether floods would occur or not at specific time instants being unable to achieve the comprehensive understanding the whole process of flood occurrence and development in summary for the rapid detection of floods occurring in the regions without adequate data support the dovb detection utilizing ground observation is more feasible and effective acosta coll et al 2018 however there are still mainly two problems faced with the existing dovb detections utilizing ground observations 1 the abovementioned common defects the rapid full life cycle fd s is absent making it difficult to get a clear picture of the whole process of floods rapidly 2 the universality and extensibility of fd ss are very low with the same or similar problems repeatedly studied resulting in a large amount of resource wastes to solve the abovementioned two problems the objective of this paper is 1 propose a method to realize the rapid flood phase determination and service for every data record and 2 make sure that the proposed method can be reused and extended to realize the full life cycle fd ss meanwhile ensuring the universality and extensibility the process based fd s pfd s designed based on sensor web broring et al 2011 was proposed in this paper the pfd s method is to provide a way for full life cycle fd s and meanwhile ensures the timeliness universality and extensibility of the method it employs ground based observations as data source being able to precisely determine the flood phase according to data changes and to offer customized services to satisfy the varied flood phase based responding requirements in the forthcoming sections we illustrated the pfd s method and validated its feasibility for full life cycle fd ss the development of the pfd s method was presented in section 2 the overall architecture was described in section 2 1 the core components illustrated in section 2 2 and the internal interactions presented in section 2 3 the design implementation and instance of the pfd s prototype was stated in section 3 with the prototype design and implementation described in section 3 1 and 3 2 separately the experiments and results were provided in section 4 with the experimental area and data described in section 4 1 and the sensor provider and event subscriber perspectives elaborated in section 4 2 and 4 3 respectively the discussion about the pfd s method was provided in section 5 including the accuracy analysis in section 5 1 comparisons with other fd s systems in section 5 2 and limitation statement in section 5 3 finally section 6 summed up this work and described possible future directions for this study 2 pfd s method floods occur and develop in the form of process and determining the flood phases precisely is of great importance to flood monitoring different from traditional fd ss the characteristics of the pfd s method is adopting the idea of process management in the detection and service of floods to be specific the main idea of the pfd s method is to firstly divide the flood process into different phases then determine the flood phase type based on the filtering of the flood observations and finally provide flood services according to the requirements of different flood phases 2 1 four layer architecture in order to achieve the goal of flood phase detection and service based on flood observations the data access the flood phase detection and the phase adaptive flood service are mandatory for the pfd s method therefore as demonstrated in fig 1 the pfd s method consists of the sensor layer the data access layer the flood detection layer and the flood service layer from bottom to top the sensor layer is to provide the data source and it is composed of the heterogeneous physical hydrological and meteorological sensors i e water level gauges etc the data access layer is responsible for the access of heterogeneous sensors and it consists of the access adapter aa section 2 2 1 and sensor observation service sos broring et al 2012 the flood detection layer is used to filter the sensor observations and judge on the type of flood phases and its components include sensor event service ses echterhoff and everding 2008 sos ses feeder middleware of sos and ses the processing unit web notification service wns simonis and echterhoff 2006 and the process based detection rules pdr section 2 2 2 the flood service layer mainly to provide customized services according to the type of flood stages and in this paper the service types include the water level prediction wlp service section 3 2 the flood warning service and the flood statistics service the focus of the pfd s method is on the data access layer the flood detection layer and the flood service layer the overall architecture of the pfd s method is based on the open geospatial consortium ogc information models and service interfaces of sensor web zhang et al 2018 the sos ses sos ses feeder and wns used in the pfd s method are the existing ogc service interfaces but these services are isolated and cannot be used for fd s directly the major contribution of the pfd s method is to connect and employ these ogc information models and service interfaces to make them universally applicable for the detection and service of flood process as marked red in fig 1 the focus of the pfd s method is on the aa section 2 2 1 of the data access layer the pdr section 2 2 2 and the internal interactions and the processing unit section 2 3 of the flood detection layer and the wlp service section 3 2 of the flood service layer 2 2 core components 2 2 1 access adapter sos could achieve the sensor access in a standardized way broring et al 2012 but due to the diversity in the flood sensor types and communication protocols how to transform the diverse flood sensors and observations into the standard forms is still a problem aa could act as a middleware between physical flood sensors and sos and it was proposed for the unified access of heterogeneous flood sensors into sos to be specific aa could be utilized to discover the flood sensors in specific spatiotemporal ranges remove the abnormalities from the sensor observations and access the sensors and observations into sos as shown in fig 2 aa receives sensor metadata from sos and observation metadata from sensors and delivers unified observations into sos aa is composed of the data reception dr unit the observation filtering of unit and the observation encoding oe unit and the parameters of all the three units are defined in the configuration file the dr unit is mainly intended for the acquisition and parse of data streams the of unit allows users to perform the observation selection the oe unit takes the responsibility of unified encoding for observations it takes nine steps to use aa to access heterogeneous flood sensors into sos 1 flood sensor metadata is encoded according to sensor model language sensorml botts and robin 2007 and registered into sos 2 the dr unit acquires the flood sensor metadata information from the sensorml files 3 the flood sensor access model is constructed based on the uniform sensor access model template depicted in fig 3 and the data stream is organized in accordance with the structure of the data array sensorid propertyid dataposition datalength dataratio 4 data stream is acquired and parsed 5 7 the attribute temporal and spatial filtering of the data streams are performed respectively 8 the data is encoded according to observations measurements o m cox 2007a 2007b and 9 observations are inserted into sos steps 3 4 are completed in the dr unit steps 5 7 in the of unit and step 8 in the oe unit the dr unit has to change with flood sensor types and communication protocols and the design of separating it from the of and oe unit could enable aa to work more flexibly and efficiently the parameters of the dr of and oe unit are predefined in the configuration file for flood sensor and data stream access 2 2 2 process based detection rules pdr is to define a series of rules to determine the flood process specifically flood phases it can be combined with ses to implement the process based detection of floods the way of representing flood processes by the four phases of diagnosis preparedness response and recovery is adopted in this paper chen et al 2015 the calculation formula of the pdr is as follows where formulas 1 2 3 and 4 are the conditions needed to be met for each transition between two adjacent flood stages i e diagnosis preparedness preparedness response response recovery and recovery diagnosis respectively floods are strictly developed in the sequence of diagnosis preparedness response and recovery in the pdr and satisfying formula 1 indicates flood in the diagnosis stage meeting both formulas 1 and 2 means flood in the preparedness stage simultaneously satisfying formula 1 2 and 3 represents flood in the response phase meeting formulas 1 2 3 and 4 at the same time stands for flood in the recovery phase 1 fret1 wl w1 f1 2 fret2 w1 wl w2 f2 3 fret3 wl w2 f3 4 fret4 wl w3 f4 where wl refers to water level freti condition i i 1 2 3 4 refers to the occurrence frequency of water level values meeting the condition i in the temporal range of ti i 1 2 3 4 w1 w2 and w3 are the water level thresholds of different flood stages respectively satisfying the conditions of w1 w2 w3 w2 t1 t2 t3 and t4 are different time window thresholds and satisfies t1 t2 t3 t4 f1 f2 f3 and f4 are occurrence frequency thresholds of different flood phases as the safety and warning water level values are determined by analyzing lots of historic flood records and regional environments and they are of great guiding significance for detecting floods w1 and w2 here equal to the safety and warning water levels separately and w3 usually satisfies the condition of w1 w3 w2 in pdr the response phase is essential and only the flood events with the response phase are the true flood events that will be dealt with or recorded to reduce the error detection rate 2 3 internal interactions the mechanism of information delivery and internal interactions among the layers of the pfd s method is elaborated in this section as the sensor layer only acts as the sensor and data provider in the pfd s method and it does not have many internal interactions with other layers therefore it will not be further elaborated here the internal interactions of the data access layer flood detection layer and the flood service layer in the pfd s method are demonstrated in fig 4 the internal interactions of the data access layer flood detection layer and the flood service layer could be interpreted from the two perspectives of flood sensor providers and flood event subscribers the two perspectives correspond to the flood sensor oriented and the flood event subscriber oriented service patterns of the pfd s method as for the flood sensor provider oriented service pattern the major contribution of the pfd s method is to provide the mechanism for sensor data publishing and sharing which can be realized by the following five steps 1 heterogeneous sensors are accessed and sensor observations are inserted into sos by aa via the registersensor and insertobservation operations respectively 2 sos ses feeder actively sends the describesensor request to sos and receives the sensorml files from sos 3 sos ses feeder registers the sensor lists to ses through the regiterpublisher operation and gets the registered publisherid from ses 4 sos ses feeder sends the getobservation request to sos and receives the o m files from sos 5 sos ses feeder notifies the sensor observation lists to ses after flood sensor providers publish and share their flood monitoring sensors and observations flood event subscribers could subscribe the floods of their interest by submitting their flood event subscription requests the flood event subscriptions could be implemented by the following six steps 1 flood event subscriber sets the parameters for the pdr i e flood sensor id and threshold etc and the wlp service i e maximum error and iteration etc section 3 2 2 pdr is encoded according to the event pattern markup language eml everding and echterhoff 2008 and the flood event subscription model is formed 3 the flood event subscription model is submitted to ses 4 ses performs the data filtering completes the current flood stage judgment according to the flood event subscription model and further transmits the current flood event phase information to the processing unit 5 the processing unit performs the flood stage change detection and deliver the true flood phase information to wns and 6 corresponding flood service is activated and the true phase detection result and notifications are returned to the flood event subscriber 3 pfd s prototype design and implementation 3 1 prototype design a prototype was developed based on the pfd s method proposed in this paper the prototype is designed conforming to the ogc sensor web standards and it is browser server based with the client and server separated the pfd s prototype can be divided into four tiers including the database tier the intermediary service tier the business logic tier and the user interaction tier the database tier is composed of the sos database the sos ses feeder database the subscription management database of ses and the flood database the intermediary service tier consists of apache storm sos ses sos ses feeder and wns the business logic tier is the core tier of the pfd s prototype and it is composed of the businesses of the flood event subscription encoding and registration ogc sensor web standards parsing login logic judgement flood phase services and data access etc there are six function modules in the user interaction tier including the user registration login the flood sensor access management the flood sensor map display the flood event subscription the flood event management and the flood phase service modules 3 2 prototype implementation the whole architecture of the pfd s prototype was implemented based on the open source code of 52 north 52n https 52north org because of its supporting more operations and continuously updating the client adopts the react framework and utilizes the components of the ant design 2 13 11 to implement the basic design the react amap 1 1 3 to complete the map function and the react highcharts 15 0 0 to enrich the data display the server employs the spring mvc and hibernate framework supported by apache storm 1 0 0 52n sos 3 5 0 52n ses 1 2 2 52n sos ses feeder 1 0 0 and wns 0 1 0 the database used is postgresql 9 2 which is open source and features powerful functions in supporting spatial operations the programming language employed is java javascript css and html improvement was made on the wlp service of the pfd s prototype so how to implement it is elaborated here the wlp service in this paper adopts the back propagation neural network bpnn li et al 2017 method to predict water levels bpnn has the capability of auto learning without requiring prior knowledge the back propagation mechanism of errors and could realize any nonlinear mapping from input to output with higher accuracy making it suitable for resolving the problems with complicated internal mechanisms i e wlp ghose et al 2010 due to the fact that the accuracy of the bpnn results can be affected by many factors except for basin precipitation this paper makes improvement on the input vectors to improve the prediction accuracy instead of just using basin precipitation to make predictions the wlp service here adopts nine factors including upstream water level air pressure air temperature ground temperature wind speed precipitation evapotranspiration relative humidity and sunlight exposure duration of the watershed as input vectors for downstream water level predicting 4 experiments and results 4 1 experimental area and data with the total length of 1532 km the hanjiang river is the first longest tributary of the yangtze river the longest river in china and asia and the third longest river worldwide and it also has the maximum annual variation of runoff among all the tributaries of the yangtze river huanghan basin is one of the sub basins located in the middle and lower reaches of the hanjiang basin with the riverbed slope becoming smaller and the water flow turning slow floodings frequently occur in the huanghan basin the area of the huanghan basin is 5 04 104 km2 with the terrain of the basin high in the northwest while low in the southeast and the geology is mainly composed of middle or low mountains in the northwest while dominated by plains or hills in the southeast located in the subtropical monsoon region the huanghan basin has the mild and humid climate and an ample annual precipitation of 700 1000 mm chen et al 2007 however the rainfall distribution is spatial temporally uneven with the precipitation gradually increasing from the upper to the lower basin and the runoff from may to october accounting for about 75 of the whole year therefore it is of great significance to perform flood detecting research in the huanghan basin as shown in fig 5 there are eight hydrological stations i e huangjiagang xiangyang huangzhuang shayang yuekou xiantao and diaochalake in the upstream and hanchuan station downstream and five meteorological stations i e fangxian gucheng zhongxiang tianmen and xiaogan evenly distributed in the huanghan basin two flood events occurred in the experimental area in july 2016 were taken as examples to verify the feasibility and validation of the pfd s method the annual changes of precipitations in the xiaogan station the nearest meteorological station to the hanchuan hydrological station and those of water levels in the hanchuan station in 2016 are shown in fig 6 the water levels of hanchuan station were used in the full life cycle flood detecting the seven upstream hydrological observation stations i e huangjiagang xiangyang huangzhuang shayang yuekou xiantao and diaochalake and five evenly distributed meteorological stations i e fangxian gucheng zhongxiang tianmen and xiaogan were employed in predicting the water levels of hanchuan station among them the monitoring variable of the hydrological stations is water level and the monitoring variables of the meteorological stations are eight kinds of meteorological factors including air pressure air temperature land surface temperature wind speed precipitation evapotranspiration relative humidity and sunlight exposure duration the data used in the experiment ranged from january 1 2000 to december 31 2016 and the data sampling frequency was once per day in the experiment the flood simulation began from july 1 2016 and to facilitate the simulation of the historic floods the sampling frequency of the experimental data was all set to once per minute therefore the observation at the nth minute from the beginning of the experiment corresponded to that at the nth day from july 1 2016 4 2 sensor provider perspective sensor providers could utilize the data access function of the pfd s prototype to publish and share their sensor observations the data access interface of the pfd s prototype is shown in fig 7 sensor providers could complete the data access by firstly encoding the sensor information according to sensorml secondly defining the parameters of the data array in the data access layer and finally uploading the configuration file for access in this paper the configuration files of the eight hydrological stations i e hanchuan huangjiagang xiangyang huangzhuang shayang yuekou xiantao and diaochalake and the five meteorological stations i e fangxian gucheng zhongxiang tianmen and xiaogan were all uploaded for use the communication protocols of all these sensors include modbus and xph after sensor providers accessed these sensors into the pfd s prototype event subscribers could select and activate the sensors they needed for further processing 4 3 event subscriber perspective from the perspective of event subscribers after submitting flood event subscription to the pfd s prototype they could acquire the detection results and receive messages from the corresponding flood services 4 3 1 event subscription to complete the flood event subscription there are three operations requiring interactions with flood event subscribers including the sensor selection and the parameter setting of pdr and the wlp service the sensor selection interface of the pfd s prototype is shown in fig 8 with all the sensors accessed listed the sensor s selected here would be utilized in pdr for full life cycle flood detecting hanchuan station was selected in the experiment to provide the data source for detecting the floods occurring in the huanghan basin after the user completed the sensor selection they were required to input the threshold time window repeated times and other parameters required in pdr the parameter setting interface of pdr in the pfd s prototype was shown in fig 9 and the system would automatically generate the subscription model when received the parameters the water level thresholds w1 28 w2 29 and w3 28 5 were used in the mitigation preparedness response and recovery phases of pdr in this experiment w1 28 and w2 29 were the guaranteeing and warning water levels of the hanchuan hydrological observation station issued by the hubei provincial department of water resources respectively and w3 28 5 was determined by trial and error the time windows and occurrence frequency thresholds of all the four phases are one second and once respectively fig 10 shows the parameter configuration interface of the wlp service the parameters required to be set include the learning rate the maximum iterations the maximum error and the hydrological or meteorological stations and their observed properties involved the water level of the seven hydrological stations i e huangjiagang xiangyang huangzhuang shayang yuekou xiantao and diaochalake and the eight types of observed properties i e air pressure air temperature land surface temperature wind speed precipitation evapotranspiration relative humidity and sunlight exposure duration of the five meteorological stations i e fangxian gucheng zhongxiang tianmen and xiaogan were all selected as the input of the wlp service in this experiment this experiment used the hydrological and meteorological data of the huanghan basin from 2000 to 2015 as the training data to construct the bpnn and the data in 2016 as the test data specifically the observations in the continuous 100 time instants were used as input and the water levels on the 101st and 102nd time instants were used as output the network structure of the wlp service was set as 47 4 7 1 through trial and error with 47 and 1 the dimensions of the input and output vector separately and 4 and 7 determined by keeping the other unchanged meanwhile lowering the error into minimum the learning rate was set to 0 01 via the mesh filter method the maximum error set to 1 0 10 8 and the maximum number of iterations set to 5 0 104 times in addition the e mail receiving address of the flood event notification message and the alias of the flood event were also set in this module the parameter summary of pdr and the wlp service was shown in fig 11 4 3 2 detection results once receiving the parameters set for pdr the pfd s prototype automatically transformed them into the flood event subscription model based on the subscription model ses filtered the data streams of the sensor selected and determined the current flood phase then the processing unit compared the current flood phase with those of the previous time instants and made judgment on the true flood phase the pfd s method could realize the flood phase judgement for each data item thus here it could determine the flood phase for each minute the detection result of flood process was demonstrated in fig 12 as the historic flood events were simulated as real time in the experiment there exists a correspondence between the experimental time and the actual time the mapping from the experimental time to the actual time was displayed in table 1 the flood simulation experiment began at 13 28 on april 12 2018 and it corresponded to the actual time of july 1 2016 every minute in the experiment was equivalent to one day of the actual time therefore 13 29 13 33 13 39 13 41 13 45 13 46 13 47 13 53 and 13 54 representing july 2 july 6 july 12 july 14 july 18 july 19 july 20 july 26 and july 27 of 2016 respectively as can be seen from table 1 there were two complete flood events from july 1 to july 30 2016 the first flood event lasted from july 1 to july 14 2016 in which it began on july 1 entered the preparedness phase on july 2 further went into the response phase on july 6 then was back to the recovery phase on july 12 and finally ended on july 14 in 2016 the second flood event lasted from july 18 to july 27 in which it began on july 18 entered the preparedness phase on july 19 further went into the response phase on july 20 then was back to the recovery phase on july 26 and finally ended on july 27 in 2016 4 3 3 flood services there were three types of flood services in the pfd s prototype including the wlp the flood warning and the flood statistics services the wlp service was utilized in all the four flood phases of diagnosis preparedness response and recovery the flood warning service was adopted in both the preparedness and response phases the flood statistics service was only employed in the discovery phase 4 3 3 1 wlp service the wlp service of the pfd s prototype was realized based on bpnn and it could predict the water levels in the future 48 time instants by providing users with future trends of water level changes the wlp service can be combined with pdr to enable users to make timely decisions and responses the wlp service worked through the whole process of floods and fig 13 showed the record interface of the wlp service in the pfd s prototype the prediction result chart on the right side of fig 13 displayed the wlp results in 102 time instants of which 100 time instants were before the prediction time and 2 time instants afterwards according to the mapping rule of time the first record in the middle of fig 13 demonstrated that the huanghan basin flood event was in the preparedness phase on july 3 2016 and the predicted water level on july 4 and 5 2016 were 28 41 and 28 42 m respectively indicating that the water level would continue to rise in the near future 4 3 3 2 flood alert service the purpose of the flood alert service is to clearly inform the user of the current flood phase and the wlp results in the future two time instants of the flood events they subscribed once floods entered the preparedness or response phase the pfd s prototype would send a flood warning notification to the user by e mail fig 14 a shows the management interface of the flood alert messages in the pfd s prototype fig 14 b corresponded to the third flood alert record in fig 14 a and demonstrated the content of the flood alert e mail sent by the pfd s prototype it informed users of that the huanghan basin flood event he she subscribed had entered the response stage on july 6 2016 and the water levels on july 7 and 8 2016 would be 29 17 and 29 03 m respectively the predicted results indicated that the water levels would decrease slowly and the flood would be relieved soon 4 3 3 3 flood statistics service flood statistics service is to complete the information archiving and management of flood events according to the four stages of diagnosis preparedness response and recovery after the flood event was completely over the pfd s prototype would invoke the flood statistics service to make statistics on the information of the whole flood process including the maximum water level values and the beginning and ending time of different stages etc fig 15 showed the two complete flood statistics records of this experiment the right side of fig 15 displayed the details of the second flood event statistics record indicating that the event began on july 18 entered the preparedness phase on july 19 further went into the response phase on july 20 and on the same day the water level reached the maximum of 29 66 m then was back to the recovery phase on july 26 and finally ended on july 27 in 2016 5 discussion 5 1 accuracy analysis 5 1 1 detection accuracy the pfd s detection results were compared with the authoritative flood record to achieve the evaluation of the detection accuracy as displayed in table 2 the comparison results demonstrated that in the experiment the pfd s method could accurately detect the beginning time of the response phase of the flood events without no delay or advance moreover the pfd s method is able to detect the entering time of the preparedness time of the flood events 1 or 4 days ahead proving that the pfd s method proposed in this paper possessed the capability of early detection and high detection accuracy at the same time the detection results could elaborately divide the flood process into diagnosis preparedness response and recovery phases more accurately characterizing the floods and providing solid foundations for the subsequent statistics and analysis 5 1 2 prediction accuracy to evaluate the prediction accuracy of the wlp service the prediction results of the wlp service adopting multiple meteorological data were compared with those derived only from the precipitation data and the ground truth the accuracy analysis of the water level values acquired from the three different methods during the periods of the 1st and 2nd flood was displayed in table 3 for the 1st flood lasting from july 1 2016 to july 14 2016 the mean absolute error mae mean relative error mre and root mean square error rmse of the wlp service and the only precipitation based model are 0 67 m and 1 02 m 2 35 and 3 59 0 9942 and 2 1081 separately for the 2nd flood lasting from july 18 2016 to july 27 2016 the mae mre and rmse of the mwlp model and the only precipitation based model are 0 52 m and 0 56 m 1 81 and 1 95 0 6439 and 0 7345 respectively from the abovementioned statistics analysis it was proved that by taking the meteorological factors into consideration the wlp service results were significantly improved when compared with those only using the precipitation data and had quite high prediction accuracy 5 2 fd s system comparisons the pfd s prototype developed in this paper was compared with the existing fd s systems i e gfds revillaromero et al 2014 efas arnal et al 2018 glofas alfieri et al 2013 and dfo reager and famiglietti 2009 from the perspectives of flood monitoring notification to users statistics forecast universal data encoding and access user subscription support processness service extensibility and service instantaneity the comparison results were shown in table 4 5 2 1 universality the pfd s prototype developed in this paper was implemented in accordance with the information models and service interfaces of ogc sensor web broring et al 2011 which enables the unified sharing processing and serving of sensor resources the overall architecture of the pfd s prototype is universal for both sensor providers and event subscribers of floods sensor providers could realize their sensor and observation resource publishing and sharing by conforming to the unified information encoding and data access operations and event subscribers could subscribe their requests and acquire the flood event information of their interest through supplementing the information of the event subscription template defined inside the prototype compared with other systems communicating with specialized standards the pfd s system designed based on the universal standards has better universality 5 2 2 instantaneity three types of time lags exist in this system i e the time lag when transforming the data streams into message flows named tl 1 the time delay caused by flood phase judgment named tl 2 and the responding time lag of each service unit named tl 3 tl 1 was usually produced during the process of sos ses feeder firstly pulling flood sensor observations from sos and then registering them into ses at a specified time interval tl 2 was often generated in the procedure of processing the messages and detecting the flood phase changes tl 3 was often caused by the calculation and treatment steps defined in the algorithms of the flood alert tl 3 1 prediction tl 3 2 or statistics services tl 3 3 tl 1 tl 2 tl 3 1 tl 3 2 and tl 3 3 averages to respectively 24 6 92 6 0 8 42 2 and 1 2 s after the experiment was repeated 10 times under the same computation environment as the three steps of data stream transformation flood phase judging and wlp are mandatory for flood detection the time delay of acquiring the detection results is the sum of tl 1 tl 2 and tl 3 2 that is the pfd s system could achieve the flood detection in 159 4 s with high instantaneity and demonstrating great superiority over other fd s systems i e gfds efas glofas and dfo updating at a daily basis 5 2 3 extensibility to access more sensors serve more flood detections and satisfy more diverse flood service requests the pfd s prototype was designed to be extensible the extensibility of the pfd s prototype lies in the three following aspects the sensor types and numbers the flood detection models and the flood service types the extensibility of sensor types and numbers is reflected in the flexibility of the data access layer of the pfd s prototype due to aa which separates the data reception unit from the data filtering and encoding unit new types of sensors could be accessed easily by modifying the configuration file if event subscribers want to utilize other flood detection models instead of the pdr proposed in this paper for flood detection what they are only required to do is encapsulating their new flood detection model according to the patterns of eml and applying them in ses as for flood services the iphaseservice interface was predefined in the pfd s prototype and new types of flood phase services could be added by implementing the executeservice method of the interface and modifying the configuration file for the type announcement of flood phase services correspondingly 5 2 4 processness as for processness instead of only detecting whether flood events would occur or not coarsely as the existing fd s methods and systems the pfd s method and prototype proposed in this paper could precisely determine the flood phase according to each item of flood sensor observations this flood process detection is able to help in intuitively embodying the whole occurrence and development process of floods making it more efficiently and conveniently for flood preparedness and responding 5 3 limitations although featuring great universality instantaneity extensibility and processness the pfd s method and prototype also has its own weaknesses firstly the four phases defined in pdr occurred sequentially so flash floods which occur in the time period shorter than the data sampling intervals will not be correctly detected but this problem can be resolved by higher data sampling rate secondly this paper only utilized pdr to provide a useful exploration for flood process detection and more elaborated models should be proposed and applied in flood detecting additionally only three types of flood services were provided in the pfd s prototype not enough for diverse service requirements from water authorities and citizens and more service types i e route planning and supply allocation etc should be further added 6 conclusion and outlook as for the rapid and continuous flood detection for regions without adequate data support there are mainly two problems faced with them including how to rapidly implement the full life cycle flood detecting and how to further ensure the universality and extensibility of the flood detection method to solve the abovementioned two problems this paper proposed the pfd s method by combining pdr with sensor web and designed and implemented the pfd s prototype to validate the pfd s method and prototype the huanghan basin hubei china was selected as the experimental area and the two flood events occurring in july 2016 was selected as flood examples two floods were detected in the experiment and the flood phases of them were accurately divided with the process of the 1st and 2nd floods being july 1 july 2 july 6 july 12 july 14 and july 18 july 19 july 20 july 26 july 27 in 2016 respectively compared with the authoritative flood records the 1st and 2nd flood were detected to enter the preparedness phase four and one days in advance respectively and the detected beginning time of the response phase of the two floods were totally accurate without delay or advance in addition the mae mre and rmse of the wlp service for the 1st and 2nd flood was 0 67 m 2 35 0 9942 and 0 52 m 1 81 and 0 6439 separately and the corresponding jobs of flood alert e mail sending and flood information statistics were also completed in addition the whole flood process detection and service of the pfd s method could be finished instantly in 2 7 min moreover the pfd s prototype features great universality extensibility and processness when it was in comparison with other fd s prototypes i e gfds efas glofas and dfo in summary the pfd s method proposed in this paper was proved to have fulfilled its design objective of rapid full life cycle flood detection while possessing the capability of universality and extensibility but flood service types provided by the pfd s prototype were limited more flood service types need to be expanded in future researches to meet more diversified flood management requirements acknowledgements this work was supported by grants from the china postdoctoral science foundation no 2017m622502 the national key r d program of china no 2017yfb0503800 the national natural science foundation of china no 41601406 41771422 the natural science foundation of hubei province china no zrms2017000698 the crsri open research program china no ckwv2018487 ky and the liesmars special research funding china we thank the national meteorological science data sharing service platform of china for the free download of the meteorological data appendix acronyms used in this paper was listed in table a 1 table a 1 acronyms used in this paper table a 1 acronyms complete expressions fd s flood detection and service pfd s process based flood detection and service iab inundated area based sdb submerged depth based dovb direct observation variables based gfds global flood detection system efas european flood alert system glofas global flood awareness system dfo dartmouth flood observatory ogc open geospatial consortium aa access adapter sos sensor observation service sensorml sensor model language o m observations and measurements ses sensor event service wns web notification service pdr process based detection rules wlp water level prediction dr data reception of observation filtering oe observation encoding bpnn back propagation neural network eml event pattern markup language mae mean absolute error mre mean relative error rmse root mean square error appendix a supplementary data the following is the supplementary data to this article multimedia component 1 multimedia component 1 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2019 03 004 
26213,lidar products are provided at fine spatial resolutions and the data volume can be huge even for a small study region therefore we have developed a parallel computing toolset that is built on graphics processing units gpus computing techniques to speed up the computational processes on lidar products the toolset provides a set of fundamental processing functions for lidar point cloud data serving as a basic toolkit to derive terrain data products with this toolset scientists with limited access to high end computing facilities can still perform efficient analysis of lidar products without dealing with the technical complexity of developing and deploying tools for these products we have integrated data decomposition methods to handle files that exceed the memory capacity of gpu devices preliminary results show that gpu based implementation yields high speedup ratios and can handle files with a maximum size of 8 gb keywords lidar parallel processing graphics processing units 1 introduction improvements in data collection techniques have significantly contributed to the explosion and availability of high resolution geospatial dataset one technique that contributed to this increased data availability is the usage of light detection and ranging lidar to produce high resolution point clouds to describe various geographic features lidar datasets have been used in many environmental applications such as hydrological modeling barnes 2017 noh et al 2018 tree leaf density estimation béland et al 2014 landslide susceptibility mapping emergency response planning dennison et al 2014 and wind resource assessment lukač et al 2017 in these applications high resolution point clouds provide detailed 3d spatial depictions of geographic structures through which scientists can extract various attribute metrics of geographic features and construct complex models for environmental processes marselis et al 2016 due to the high resolution lidar datasets can be voluminous even within a small study area yang et al 2014 consequently processing massive lidar data in a serial manner takes an extremely long time which imposes constraints on using lidar for time critical environmental applications wendel et al 2015 such as flood simulation for emergency response noh et al 2018 to resolve the issues of processing massive lidar data some researchers either make their study areas small or lower the resolution which leads to inaccurate analysis yang et al 2014 vaze et al 2010 to speed up the computation process on high resolution massive point cloud significant research is taking place in developing high performance lidar processing methods that leverage the power of parallel computing previously developed parallel computing functions include digital elevation model dem generation guan and wu 2010 han et al 2009 huang et al 2011 sharma et al 2016 triangulated irregular network tin creation guan et al 2012 oryspayev et al 2012 and point classification hu et al 2013 sharma et al 2016 some of these functions are optimized for multi core central processing unit cpu environments guan and wu 2010 sharma et al 2016 han et al 2009 huang et al 2011 while other functions run with a graphics processing unit gpu environment using specialized programming interfaces provided such as nvidia s compute unified device architecture cuda hu et al 2013 by utilizing gpu threads it is possible to achieve improvements from 100 times to 1000 times over a cpu serial version tang and feng 2017 if such computational achievements and more functions were widely available and easily accessible to environmental scientists it would allow environmental science researchers to quickly process lidar data for larger spatial extents previous efforts to accelerate lidar data processing such as zhang and li 2015 have not met many requirements for widespread use in environmental research first many studies implement functions in a case by case manner meaning that fundamental processing functions that researchers need are missing fernandez et al 2007 scientists often require access to many basic functions not just one or two for their research without all the functions they need scientists cannot effectively examine geographic features e g changes in terrain characteristics therefore more functions need to be included in a systematic way second a large number of developments are based on high performance computing clusters which impose hardware constraints on users to fully leverage the power of modern computation techniques li et al 2018 in particular cpu based or gpu based clusters are costly limiting the accessibility to high performance computing power cuomo et al 2015 gonzález domínguez et al 2015 third solutions built based on desktops or laptops are limited by the memory constraints of gpu devices which are smaller than the main memory there is a need to integrate data decomposition techniques to support file processing barnes 2017 côte et al 2011 vaze et al 2010 to enable fine level analysis as few users have access to cluster environments providing a high performance solution using conventional devices is promising to enable a wider range of users to process and analyze lidar data on their laptops or desktops in a cost effective high performance manner these types of developments bring the everyday researcher closer to powerful computation and geoprocessing solutions in response to the needs of developing high performance toolsets for lidar data processing that are accessible to conventional users we have developed lightweight gpu based parallel processing tools that can be utilized by end users who do not have access to cluster environments considering the variety of lidar applications in environmental and geospatial research we only focus on terrain surface analysis as many different research disciplines utilize these functions in their data processing workflows and can allow for advanced analysis such as marselis et al 2016 and noh et al 2018 our tools meet the following critical objectives first the toolkit implements a set of parallel lidar geoprocessing functions using nvidia cuda many core gpu computing techniques to deliver high performance computing capabilities to end users using these techniques our solution delivers efficient processing capabilities with conventional computing devices i e desktops or laptops with gpu devices second the toolkit incorporates a data decomposition module and an additional gpu configuration module to process variably sized lidar datasets with the data decomposition module the tools provide data processing capabilities to handle files with sizes exceeding the memory of gpu devices note while the decomposition file allows the tools to handle files that are larger than the gpu memory the toolset runs in conventional computing environments not clusters the size of a single file cannot exceed the memory of the cpu our tools can provide a high performance and accessible data processing solution for researchers seeking to efficiently process lidar products our research benefits the environmental science community in two ways first the toolset delivers an efficient yet extensive lidar processing toolkit to enable a variety of users from the environmental science or other relevant communities to efficiently perform research using lidar products using these tools scientists can execute various terrain analysis and processing tasks on lidar datasets ranging from basic data preparation to terrain surface modeling and can benefit anyone who uses lidar data thus these tools can increase scientific engagement by providing a solution that can be easily deployed to common computing devices including laptops and desktops without the need for a cluster environment and accelerate fundamental research using the gpu second through our research the environmental and geospatial communities gain examples of parallel processing solutions to guide the development of advanced environmental applications by providing a lightweight high performance geoprocessing pipeline using this pipeline practitioners can rapidly prototype deploy and test new algorithms as the tools include both executables and source codes users of various technical knowledge levels can utilize efficient gpu computing for lidar datasets by building a toolkit that contains the previously identified components we can increase researcher access to powerful computation and geoprocessing solutions these development efforts make the following contributions a propose a set of efficient parallel geoprocessing methods for lidar and its derived products with the gpu parallel processing can be used to speed up common but computationally intensive processing functions these methods are fundamental to many advanced environmental applications researchers who have limited access to high end computing devices can leverage parallel processing power to speed up lidar data processing and can expand the tools to support more complex missions b by utilizing data decomposition strategies the toolset can provide processing capabilities to handle large files in this case larger files mean that the file size exceeds the memory of a single gpu device but not that of main memory further the strategies allow the end user to perform parallel processing on a single machine that is equipped with multiple gpu devices the rest of the manuscript is organized as follows section 2 provides an overview and explains the components of the tools section 3 presents the experiments results section 4 summarizes the test results and proposes future work 2 the design and the implementation 2 1 overall design of the library to build upon previous research into improving lidar processing the gpu based tool should meet two major requirements first it should provide fundamental parallel processing functions for lidar point clouds to efficiently support basic analyses we have examined the typical functions provided by popular geospatial solutions to handle lidar datasets and lidar derived data products e g lastools isenburg 2012 and arcgis among these popular tools some functions are designed to derive products to describe terrain characteristics e g dem whereas others are designed to process point cloud data e g point filtering classification as all featured functions have different levels of computational complexity and different types of spatial dependency we must examine the workflows of every function and the spatial dependencies of the input and the output data to design parallel implementation strategies tailored to specific problems our strategies should exploit the parallel computation power of many core gpus while maintaining the appropriate workload balance among computing cores the parallel implementation strategies of our chosen functions are discussed in detail in section 2 2 secondly the toolset should overcome the limitation of gpu on board memory to handle files with a single file size ranging from several hundred megabytes mb to several gigabytes gb this is the common range of the file size within conventional devices when the file size exceeds the onboard memory of gpu the tools should apply an appropriate data decomposition strategy to decompose the data into small subsets so that the gpu can process the decomposed files these strategies should maintain the spatial dependencies of the original datasets and introduce minimal computational overhead in addition to the memory limitations of gpu devices the computational capabilities of computing devices vary in the case of multiple gpu environments multiple gpu devices can run concurrently to fully use the computational power of all gpus the parallel execution strategy evaluates the configuration of gpu devices and determines the data allocation to computing threads for parallel execution the parallel execution strategy adds scalability in the number or type of gpu device s used in data processing and is necessary in maximizing the computational power of a machine below we explain the major components of the tools which include a progressive data loader an adaptive gpu configurator and a set of cuda kernels fig 1 a progressive data loader determines how a dataset denoted as d in fig 1 should be decomposed into small subsets denoted as d based on data volume spatial extent processing methods and hardware constraints of a gpu device e g gpu onboard memory a gpu configurator retrieves the information of the computing capabilities of gpu s to determine the appropriate parallel configuration of the device i e block size and thread size denoted as ω to better utilize the available computing resources when the machine has multiple gpu devices this module also allows for the utilization of multiple gpus in a parallel manner to speed up computation by loading and processing multiple data subsets denoted as d p the final component of the tools are a set of cuda kernels which includes a set of parallel processing functions executed by gpus examples of processing functions are data conversion data filtering and interpolation see table 1 all components are loosely integrated into a processing pipeline and are necessary to meet the requirements for efficient lidar processing for research purposes the progressive data loader enables the toolset to handle files exceeding the memory capacity of gpu devices using this toolset the researcher can read lidar datasets into main memory and prepare datasets in the form required by gpu processing the configurator ensures that the parallel computing strategy is adjustable to better utilize the computing power of the gpu devices with the kernels scientists can perform basic data manipulations on data in a parallel manner since the list of available kernels represents different parallel implementation strategies gis practitioners can expand the toolset by referring to these strategies further the toolset provides necessary interfaces e g simple executables and modular source codes for different components to interact with the pipeline to ensure that users with different levels of technical proficiency can use the library for their own applications not only will the components of the parallel processing tools contribute to the performance of the geoprocessing functions but also the language and libraries used to build it while some programming languages such as python permits more flexible customization and less steep learning curve to ensure the tools can deliver relatively high performance in terms of pre and post data processing we have developed the functions in the c language admittedly the development choice will introduce difficulties in deploying and expanding the toolsets at the current stage our primary focus is to deliver high performance processing capabilities in the future we will explore the wrapper option with other languages further the tools are built with cuda c 1 1 http docs nvidia com cuda cuda c programming guide index html there are a few libraries such as openmp openacc that support the development with cuda enabled gpus e g xu et al 2013 guo and wu 2016 these libraries improve the programmers productivity and yield better portability they are arguably preferable for the expansion of our toolset and for increasing the user community base studies show that these libraries do not provide low level customizations such as shared memory management e g memeti et al 2017 for this reason we used cuda c and expect to expand the toolset when those libraries provide additional features besides cuda c we utilize thrust to perform simple operations such as sorting minimum maximum and unique value identification these tools provide a high level abstraction of cuda implementations according to the results from benchmark testing thrust delivers similar performance gains when compared to native cuda c implementation we use additional geospatial libraries e g lastools and gdal ogr contributors 2018 to implement the reading and writing functions for geospatial files the lastools library is mainly used by our tools to read and write lidar files in a las file format we have developed all geoprocessing functions but used similar naming conventions to those in lastools the tools run on all windows and linux machines also we have customized a virtual machine image in amazon web services aws users can create a virtual machine instance using the image and use the preconfigured library to perform geoprocessing tasks we will consider expanding these tools to other platforms in the future below we discuss the development considerations for each of the components of the parallel processing tools 2 2 cuda kernels to identify the cuda kernels that the tools should provide we have reviewed the functions of typical lidar programs e g arcgis and lastools and identified a set of fundamental geoprocessing functions for lidar points these functions either process the original point cloud e g creating an elevation surface from point clouds or compute derived products e g creating contour lines from elevation surface raster data a summary of the functions can be found in table 1 the types of parallelism in table 1 are further illustrated in figs 2 4 the set of tools also includes a function to produce a tin which is built based on an implementation of cao et al 2014 since we did not develop the function but adopted an existing one we did not include the function in the table the list is still expanding as we are actively adding more functions while more advanced functions are possible the purpose of these functions is to enable fast handling of commonly used yet computationally intensive when performed on massive data processes in geography and related fields the variety of the functions also illustrates different types of parallel implementation strategies which can help gis practitioners expand the tools in creating the functions we consider the design philosophy of the cuda parallel computing paradigm this paradigm is based on data parallelism which distributes datasets to different computing threads for simultaneous processing using the same instruction stream in developing the cuda kernels one challenge is to maintain spatial dependency when distributing data to threads for parallel implementation we have identified several parallel implementation strategies for different types of processing functions the chosen strategy for a function depends on whether the parallel implementation is imposed on the input or the output space these parallel strategies are commonly classified as input space based fig 2 and output space based fig 3 parallelism strategies can also be classified based on how the data are distributed to threads for parallel processing e g by the spatial extent or by the number of points given that a few geoprocessing functions such as zonal statistics require all input data points to produce the final output we follow the principle of divide and conquer which performs multiple rounds of parallel processing for data subsets fig 4 depending on the allocation of data to threads parallelism strategies can be classified based on input space type 1 or output space type 2 for certain operations the parallel execution runs multiple times to produce the final output termed as parallel reduction type 3 below we explain the three typical parallel strategies in detail type 1 parallelism means that every thread processes a subset of input points the gpu kernel runs only one round of computation to produce output data an example of this strategy is clipping which identifies the point within a bounding box or polygons fig 2 every thread evaluates if a set of points are inside the bounding box and labels the status of those points as in or out once the parallel computation finishes steps in green boxes in fig 2b a cpu serial processing function starts which writes the points that are inside the bounding box to an output file we employ this strategy when there is spatial independency in input points that is there is no data communication or transfer between the points from different computing threads type 2 parallelism means that every thread processes a set of output elements e g grid cells in an independent way as to maintain the spatial dependency in the input elements e g points grid cells when multiple points contribute to the result of a basic processing unit e g a grid cell the parallel processing strategy can be applied to output space for example the inverse distance weighting based interpolation method idw produces a raster surface where multiple points contribute to the value of a grid cell lu and wong 2008 in this case every grid cell performs a neighborhood search to identify points and apply a mathematical formula to calculate an elevation value using all points within the neighborhood steps in green boxes in fig 3 there is spatial independency in the output space the value of a grid cell output is determined by the values of points input in the neighborhood of the grid cell the value of the grid cell however does not depend on the values of grid cells in its neighborhood the input is the point dataset and the output is the raster layer every thread processes a set of grid cells independently in the meantime a point maybe be used by multiple threads concurrently for computations see the figure below the parallelism is implemented on the grid cell output space fig 3 describes the data distribution and the corresponding parallel processing steps using interpolation as an example type 3 parallelism involves parallel reduction through a divide and conquer strategy certain geoprocessing functions derive aggregated results from a set of points for example the statistical computations identify the statistical information e g mean maximum of a set of points spatial dependency exists in both input and output space therefore we introduce a parallel reduction approach which launches multiple rounds of parallel computations such as sorting and statistics fig 4 illustrates this type of parallelism strategy using statistical summarization as an example every round processes a subset of data from the previous rounds of computation in parallel and the operation continues as the volume of data reduces in fig 4a in the first round every thread processes a subset of points in the blue rectangular region and derives the summary results in the second round every thread processes five summary results from all blue regions in the red region the regions are spatial extents of the file or the file s subsets when the data volume reduces to the point where the performance gain from parallel processing is not significantly better when compared to serial processing the kernel stops this is currently implemented in a heuristic manner in this section we discussed the parallelism strategies implemented in the cuda kernels which allow users to efficiently process data using various geoprocessing functions depending on the function s spatial dependency 2 3 progressive data loader to efficiently process lidar data the tools must address the fact that gpu devices have lower memory capacity than main memory for example amazon ec2 g3 instances are equipped with nvidia telsa m60 gpus while this instance has 122 gb of main memory every gpu in the instance only has 8 gb of device memory loading an appropriate amount of data for processing is important to address the memory constraints of gpu devices in the context of multi gpu parallel computing tang and feng 2017 propose a multi gpu decomposition method based on indices of points for parallel map projection for massive lidar datasets as the bounding polygons of lidar datasets can be irregularly shaped many decomposition methods for fundamental geoprocessing must be reexamined because they require regularly shaped bounding polygons in the case of computing density the spatial extents of both input and the output datasets should be rectangular shapes a simple point indexing method often fails to provide subsets with rectangular shapes defined by the spatial extent of output raster layers fig 5 b we focus on two methods to decompose data and generate small subsets of data to meet the requirements of memory constraints these methods are indexing based and region based in both cases a post processing step is necessary to combine the outputs from the sub datasets we explain the two chosen approaches below an indexing based data decomposition method retrieves a slice of points from a list based on a pair of index numbers that defines the start and the end indices of the target points because the point indexing does not conform to the spatial order of points the subset of data may not maintain the spatial adjacency of the points even arrays of the same size may not have the same spatial extent as a result a subset of points may cover multiple isolated regions we may apply this method to geoprocessing functions that do not require the maintenance of spatial adjacency such as lasclip lasstats and lasheight a region based decomposition method defines a set of adjacent bounding boxes covering the study area i e the spatial extent of the input dataset and allocates points to different bounding boxes to allocate points to these bounding boxes we apply the clipping function to derive sub datasets and send the sub datasets to the processing pipeline this method is applicable to lasslope aspect lasduplicate lasgridcontour lasdensity las2grid and lasground in the case that raster data are used as the input the region based decomposition retrieves subsets of data based on the row and column numbers fig 5 shows the spatial extents of the two strategies of spatial data decomposition the spatial extent of the original is shown in fig 5a fig 5b shows the index based decomposition the subsets that are produced from the data decomposition have irregular bounding shapes therefore this method is not suitable for geoprocessing functions that require the maintenance of regular spatial extents for example when performing interpolation using an irregular bounding shape a point and its neighboring points may not come from the same subset however this decomposition is well suited for the summary statistics function the parallel calculation is concurrently performed on different groups of points the spatial locations of points do not change the summary results fig 5c shows the region based data decomposition each subset has a rectangular bounding shape with a small portion overlapping with its neighboring subsets this overlapping region contains points that are required by the specific geoprocessing functions for example to calculate the slope value of a point the function requires the elevation values of points in its neighborhood to calculate the maximum rate of elevation change from the point to its neighborhood this maximum rate is the slope value the region based data decomposition method first calculates the rectangular spatial bounding extents for the subsets and performs data filtering using these bounding extents it then expands the regions and identifies the points within the expanded regions in this way points on the edges of the bounding extent can have neighboring points from the same subset for computing there are two variations when applying the above methods first while a few geoprocessing functions can work well with either decomposition method we frequently use the indexing method to avoid unnecessary clipping operations because indexing is significantly more efficient second when the file size exceeds the onboard memory of gpu and spatial region based decomposition is necessary we use a combined indexing and region based decomposition strategy we first use the indexing decomposition to create sub datasets then perform parallel clipping on the subsets and finally retrieve the points from multiple sub datasets that fall in the designated spatial extent in this section we discussed the different methods used to decompose input data in the toolset with the data decomposition strategies the toolset is capable of handling variably sized datasets with the data decomposition method implemented in our toolset users do not need to perform data decomposition before data processing thus simplifying the workflow for the end user 2 4 gpu configurator while the progressive data loader decomposes the data for parallel processing the gpu configurator module examines the configuration of gpu devices and determines an optimized execution strategy to perform parallel computing there are three important functions of this component which include a basic hardware profiling b gpu controller and c maximum resource occupancy based kernel execution fig 6 through profiling we obtain information such as gpu memory number of gpu cores and maximum number of allowed threads in particular the gpu memory determines the amount of data for processing in a single round when multiple rounds of processing are necessary or multiple gpu devices are present the gpu controller manages the execution of every round of processing in the gpu device s finally we use maximum resource occupancy to determine the number of threads to perform geoprocessing tasks except for the gpu controller the other two functions are very straightforward and based on built in cuda functions this configurator connects with the data decomposition module to ensure that the tool can process variably sized files by fully utilizing the users gpu devices below we describe the controller the controller includes a multithreaded design that allows the kernels to scale to multiple gpus two options are possible for multiple gpu enabled parallel processing the first option occurs when every cpu thread controls one gpu device this option leverages the computing power of multiple cpu cores to achieve this we utilize a multi thread technique that launches multiple cpu threads and every thread controls the execution of a kernel on a designated gpu device this option is suitable for most basic kernels where results are only dependent on the input sub datasets e g clipping with polygons the communication between gpu devices is realized through the communication between cpu threads the second option occurs when a single cpu thread controls all gpu devices this option is applicable to functions that require multiple rounds of data processing and the input of the round depends on the output from another gpu device from the previous round e g lasground in this case the communication between gpu devices is realized through enabling peer data transfer among gpus by integrating with the other components of the toolset the gpu configurator allows the toolset to adapt to varying computation environments to implement the configurator we have developed functions based on a few functions of the cuda library nvidia s cuda library provides a set of functions that can determine the hardware configurations of gpu devices we integrate these functions i e device query device switching peer to peer data transfer among devices maximum occupancy based grid and thread configuration for kernel execution into our toolset and call them before a geoprocessing function starts with the gpu memory information from the device query function this module loads a subset of data and the data structure for this subset from the original file based on the profiling information from the same device query function we determine parallel execution strategies for the kernels in particular when multiple gpu devices are available in the same machine e g amazon p2 instances have multiple gpus the toolset can launch multiple kernels at the same time and each gpu device executes one kernel to fully utilize the computing power of devices single gpu vs multiple gpu devices to implement this feature we introduce a multithreaded gpu controller that can launch multiple cpu threads with each controlling one gpu device to execute kernels finally to determine the parallel configuration of the gpu i e the block size and the grid size instead of specifying the block size and the grid size we utilize a built in function of the cuda library which determines block and grid sizes to achieve maximum occupancy this function evaluates the complexity of the cuda parallel kernel data volume and the configuration of the gpu devices to calculate a combination of block sizes that can achieve the maximum utilization of the gpu cores 3 experiments 3 1 an overview of data and computing environment the two major features of the toolset are efficiency and the capacity of data processing with conventional computing environments to help users access these features of the toolset in conventional computing environments we conduct two main sets of experiments to measure the library s performance the first set of experiments aims to compare the performance of running functions with the cpu with the performance of running the same functions with the gpu to demonstrate the toolset s efficiency the second set of experiments demonstrates the toolset s capabilities of processing larger files with data decomposition strategies and with multiple gpu devices the main datasets used for testing purposes are from the usgs 3d elevation program 3dep 3dep is designed to provide high resolution elevation data products derived from lidar and deliver these products online the lidar files from the program have file sizes ranging from 200 mb to 800 mb to examine the effectiveness of the data decomposition strategy in processing larger files using the toolset we clipped or merged the files to produce datasets that are differently sized the maximum size of file tested is 8 gb since our tools were designed to process las files from data vendors e g the usgs s 3dep project which generally have sizes less than 1 gb for a single file and the toolset runs on conventional computing devices for these reasons we determined that an 8 gb maximum file size was sufficient for conventional computing environments it is also important to note that the maximum file size the library can process is determined by the main memory of the computing device and not by the memory of the gpu 3 2 experiment 1 efficiency tests as experiment 1 aims to highlight the efficiency of the toolset we have chosen the following functions considering different types of parallel implementation strategies and data lasclip lasstats lasground lasdensity lasgridcontour and lasslope aspect we perform the processing functions with gpu and cpu separately to evaluate the performance gains in this experiment we compare gpu parallel processing to serial cpu processing however the implemented cpu processing functions can be further optimized such as through parallel cpu processing we group the functions into three categories based on the test files required for the functions the first group includes two functions lasclip and lasstats all of these functions use four files in table 2 that are associated with different land cover types the second group includes lasground and lasdensity we produce three files from file 4 a 4 in table 2 and test the two functions with those files table 3 the third group includes three functions that require raster data as an input table 4 the raster files are created from file 4 a 4 in table 2 the files we used for all the tests were collected in north carolina we have conducted tests with amazon ec2 gpu instances g3 4xlarge 2 2 http pointclouds org which use the windows server 2012 r2 standard a 64 bit operating system have an intel xenon cpu e5 2686 2 30 ghz processor with 122 gb ram and an nvidia tesla m60 gpu to ensure that files can be processed by our toolset as well as functions from other software packages e g arcgis for comparison we perform data preprocessing to produce files in the formats required by those functions we use four main types of datasets for tests of experiment 1 including las files raster tiffs multipoint shapefiles and polygon shapefiles our toolset takes las files and raster tiffs as input files the raster tiff files were created from the las dataset to raster tool from arcgis at various grid sizes the value field for the grid values was elevation and the cell assignment was average we created multipoint shapefiles from las files to allow for comparison with equivalent functions in arcgis such as summary statistics and clip which only take shapefile inputs to create the multipoint shapefiles first we create a multipoint shapefile in arcgis then we use the las2shp tool from the lastools library to write all the points from the las file to the multipoint shapefile because the elevation values for the points are removed in this process we add the elevation values back using the arcmap add z information 3d analyst tool and the mean value for other arcgis comparisons we use create las dataset tool from arcgis that can load las files directly for use in the arcgis comparisons finally we produce the polygon shapefiles for the clipping function using the arcmap editing toolbar each polygon shapefile contains only one simple polygon corresponding with one las file upon the completion of the creation of the appropriate testing files we ran the tests formula 1 shows the computation of the speedup ratio wu et al 2009 we use speedup ratio rather than actual time cost because the actual time costs vary significantly but the actual time costs can be found in appendix a s p e e d u p s e r i a l r u n t i m e o f t h e c p u p a r a l l e l r u n t i m e o f t h e g p u formula 1 calculation of speedup ratio 3 2 1 group a lasclip and lasstats the functions tested in this round process point data for the lasclip test we clipped a las file or input multipoint shapefile using a polygon shapefile created for las files a 1 a 4 we compared the lasclip function with the arcmap clip analysis tool as the only clipping tool in arcmap that was comparable to the lasclip tool is the clip analysis tool we had to create a multipoint shapefile from the las files for this test the multipoint shapefiles have the same number of points as the las files and the same spatial extent like the lasclip test we need to use the multipoint shapefile created from the las files for these tests as the only comparable tool in arcmap was summary statistics for the summary statistics we found the mean of elevation values minimum elevation maximum elevation the range of elevation values the standard deviation of elevation values and the count of the las data we compare the arcmap time to the lasstats time fig 7 a shows the performance comparison between gpu based clipping and cpu based clipping the gpu version always outperforms the cpu version the speedup ratios range from 110 to 140 for the clipping function the small variations in speedups are partly due to the round up problem of thread specification by contrast the speedup ratios of the lasstats function are significantly lower fig 7b summary statistics identifies the aggregated statistical values such as minimum maximum and standard deviations this function requires multiple rounds of statistical computations and takes a parallel reduction approach type 3 in section 2 with this type of parallelism the number of threads for every round of computations reduces which does not fully exploit the parallel computing capabilities of all gpu cores despite different parallelism strategies we obtain relatively steady performance gains which demonstrates the efficiency of the toolset 3 2 2 group b lasground and lasdensity similar to group a this group of tests also processes point data unlike the previous group of tests both lasground and lasdensity include a search process against a large number of points to determine the results the search process is a common step in many advanced geoprocessing functions e g k nearest neighbor and classification we use three clipped files from the a 4 dataset along with entirety of the a 4 dataset to test lasground and lasdensity table 3 since the four files are from the same land cover category and in the same region the point density should be similar in the case of the lasground comparison we compare lasground to the progressive filter function of point cloud library 2 pcl as both implement the same algorithm from zhang et al 2003 lasground identifies non ground points by comparing the point elevation to the average elevation of points in its neighborhood usually a rectangular region centered at the point location if the evaluation difference is larger than a predefined threshold value the point is labeled as a non ground point in the case of the density computation we compare our function to the arcgis kernel density function to execute the geoprocessing functions we specify the required parameters of the functions the lasground comparison is conducted in multiple rounds and each round is associated with different pairs of neighbor sizes and threshold values we follow the example provided by pcl to configure the window sizes and threshold values a total of five rounds of filtering is necessary the window sizes are 3 m 5 m 9 m 17 m and 33 m the lasdensity function produces a raster output layer the cell value represents the density of points within a kernel s region defined by a search radius we set up the search radius of the kernel and the output density raster as 5 m fig 8 a shows the results for the lasground tests since we used the files from the same land cover type the speedup ratio is similar across four files the speedup ratio is about 11x the speedup ratio is not as high as the clipping function partly due to the fact that the function from pcl has a built in octree indexing structure to speed up the point retrieval and search process fig 8b shows the speedup ratios of performing the kernel density computation in this case we obtain an average speedup ratio of 56x the speedup pattern is consistent across multiple files the steady performance gains show that our toolset not only can improve processing efficiency but also provide examples of implementing searching in a parallel manner 3 2 3 group c slope aspect and contour the functions tested in this group process a digital elevation model dem in the form of raster data lasgridcontour creates contour lines we specify the minimum and the maximum elevation values as the range of the contour lines we configure the contour intervals as 100 m lasslope aspect does not require additional parameters as input the slope and aspect computations derive slope and aspect values of every grid cell based on the cell values in its 3 3 window we again produce raster files from the a 4 dataset but adjust the resolutions to obtain raster files with different files sizes table 4 fig 9 shows the test results in fig 9a we obtain speedup ratios ranging from 50x to 75x for our lasslope aspect function however the gpu parallel implementation does not yield any performance gains in the case of contour creation fig 9b the gpu parallel implementation of contour line creation follows the popular marching square algorithm when analyzing the separate time costs of the major steps of the parallel marching square algorithm the time spent on the data transfer is significantly longer than the kernel execution time considering that we compare the time costs between gpu parallel processing and cpu serial processing in this case it is highly possible that cpu parallel processing is more efficient than gpu parallel processing due to the previously stated inefficiency in gpu execution seeking parallel cpu processing method is more preferable detailed break down time costs can be found in appendix b overall this group of tests shows that the toolset can efficiently process the derived products i e dem yet further optimization is necessary in summary the results confirm that a variety of the toolset s functions deliver performance gains within conventional computing environments in this case aws instances the results also illustrate the effectiveness of parallel implementation strategies in processing lidar data and its derived products as expected type 3 data parallelism used for lasstats is least efficient due to the insufficient usage of the parallel computing power of gpu devices gis developers can refer to the results to assess the applicability and the usability of different parallelism strategies to add functions in the future further the variations in speedup ratios of different functions show the suitability of different functions for parallel implementation this suitability information can help users access the best usage of the toolset for example the average speedup ratio of lasgridcontour is close to 1 in this case we recommend using cpu processing methods for contour line creation 3 3 experiment 2 tests on larger file processing and multi gpu support besides efficient parallel processing with gpu devices another feature of the toolset is its capability to handle larger files and perform parallel processing on multiple gpu devices in the same machine group a tests the processing capabilities of the toolset on larger files using data decomposition strategies while group b tests multithreaded gpu techniques that scale cuda kernels on multiple gpu devices in a single computer one should note that since the toolset is designed for users who do not have access to clusters our toolset currently does not have the capability to run on multiple machines or cluster environments 3 3 1 group a data decomposition for larger file processing in this set of experiments we evaluate the performance of the entire geoprocessing workflow with the embedded spatial data decomposition method using lasdensity and lasduplicate as examples we record the original size of the input las file and the number of data subsets after decomposition in all cases we adjust the partition number which defines the number of subsets that are produced after data decomposition we also record the time costs of creating data subsets as well as the time costs of performing the computation to examine the additional overhead introduced by data decomposition for this test we use the following files with file sizes ranging from 1 gb 2gb 4gb and 8 gb table 5 we believe 8 gb is a reasonable file size of users who run the toolset in conventional environments as data decomposition requires the number of partitions as a parameter we also test how the performance changes when specifying a different number of partitions parts overall with our data decomposition methods the toolset can scale to handle files with file size of 8 gb in fig 10 duplicate point checking consumes more memory than kernel density computation therefore to ensure the successful execution of gpu parallel functions when processing the same file the duplicate point checking function requires more partitions than kernel density for example to process d 1 1 gb file duplicate requires data decomposition to produce four data subsets whereas kernel density computation can process the file directly the processing time cost and the data decomposition cost generally increase with the size of the files even though our toolset is able to handle larger files we recommend that las files should be organized as medium sized files to avoid significant computational overhead 3 3 2 group b geoprocessing on multiple gpu devices to compare the performance of utilizing multiple gpus on a machine versus the single gpu configuration we tested three functions a lasclip by polygon b lasduplicate and c lasground these three functions evaluate different data decomposition methods as well as two options of multi gpu processing strategies lasclip by polygon and lasduplicate use option 1 for multithread processing fig 6 the data decompositions are index based and spatial region based respectively lasground employs option 2 for parallel processing because there is a step to transfer data after every round of filtering we perform las clip by polygon and lasduplicate functions using g3 8xlarge and lasground using p2 8xlarge 3 3 https aws amazon com ec2 instance types p2 machines the g3 8xlarge instance has 2 nvidia tesla m60 gpus 244 gb cpu memory and 32 vcpus intel xeon cpu e5 2686 v4 2 3 ghz as option 2 requires peer access capabilities between gpus we use p2 8xlarge the p2 8xlarge instances have 8 nvidia k80 gpus 488 gb ram and 32 vcpus in this set of tests we use the same four files in table 2 we record the time costs of running the functions from transferring data to gpu devices to final data gathering from gpu devices we also record the time costs that every gpu device takes to complete the gpu function execution average time costs from the multi gpu instance are shown alongside times from the single gpu in fig 11 the results show that the toolset can run multiple gpu devices for both options in particular the peer to peer data transfer feature is extremely efficient for ground point detection which takes less than 0 01 of the total processing time the speedup ratio is slightly lower than 2x due to the additional time costs of launching threads distributing data to the gpu device and gathering data from multiple devices the additional costs may take a larger portion of the overall processing time when running simple functions e g clipping on smaller files further optimizations are necessary to reduce additional time costs as well as minimize the waiting period of synchronization of multiple gpu devices to achieve better performance with low computational overhead in summary both groups of tests illustrate the ability of the toolset to support large file processing and geoprocessing on multiple gpu devices group a shows the toolset s capability of handling large files with data decomposition strategies the toolset can process files with sizes up to 8 gb this feature makes it possible to process lidar products for large spatial regions and support high resolution modeling for such regions group b shows the toolset can scale to work with multiple gpu devices on the same machine with the functions of the configurator the toolset can fully utilize the computational power of both gpu devices of the instance while adding low overhead however this feature is not equivalent to the scalability of gpu clusters which allows the execution of geoprocessing tasks on multiple machines 4 conclusion this paper reports an effort in developing a cuda enabled parallel processing toolset for lidar data points this toolset addresses the needs for providing efficient and accessible geoprocessing capabilities for lidar point clouds and derived produces in environmental science research this toolset reduces computation costs for handling lidar datasets by using accessible gpu computing techniques this enhanced processing efficiency is especially useful when compared to traditional cpu only methods of processing e g in serial as they are not optimized for time efficiency since the toolset imposes minimal requirements on devices and delivers significant speedup ratios it can help environmental sciences researchers accelerate studies involving intensive lidar data processing preliminary results figs 7 9 show that significant speedup ratios from cpu to gpu are achieved for most of the functions we have developed this toolkit includes additional modules to ensure the capabilities of the functions when dealing with varying size files in different computing environments figs 10 and 11 the toolset provides multiple fundamental processing functions for terrain analysis e g basic measures of terrain features which can serve as the basis for other advanced environmental applications the solution also provides a complete processing workflow starting from data preparation to data analysis with the toolset environmental scientists can perform efficient processing on lidar datasets to support advanced modeling and analysis missions scientists with different levels of technical proficiency can deploy use evaluate and customize the library through manipulating source codes and executables despite the progress we have made with the toolset it has the following deficiencies that must be addressed the above experiments demonstrate the toolset can handle files up to 8 gb since lidar datasets can be larger than this it can be argued that the toolset does not have the ability to process large files in conventional computing environments additionally the toolset uses cuda c and many dependent libraries producing a significant barrier to users who wish to contribute to the library finally the toolset currently cannot run in a cluster environment further limiting its utility we recognize the importance of addressing these critiques in future research to improve the performance of the toolset future work will include the generation of additional advanced geoprocessing functions such as hydrological modeling murphy et al 2008 solar panel installation lukač and alik 2013 in cities and feature detection homm et al 2010 secondly there is a need to expand the library to gpu cluster environments the toolset currently works with conventional computing environments for users who do not have access to high end computing facilities e g desktop laptops the expansion will be critical for the toolset so it can allow any input function to process almost any dataset regardless of size exceeding the constraints of the main memory of a single machine thirdly providing a high level programming language wrapper e g python for the toolset will be critical for expanding its customizability and utility for a greater number of researchers additionally we plan on building easy deployment scripts which will allow users to configure the toolsets with multiple configuration steps fifthly we will explore the usage of openacc as a way to enhance the toolset s compatibility and reduce development efforts to further improve its scalability to a cluster environment finally providing a set of features such as a python wrapper and guis would enable user friendly processing and open the tools up to users who do not have experience with c or command line software availability our gpu enabled high performance lidar data processing toolset is available at https lidarlib github io gpulidarproj it was developed by a team at the university of denver led by dr jing li this toolset was built using c and requires a windows or linux machine with an nvidia gpu the toolset was first available in 2018 dr jing li developed all functions except the functions to create triangular irregular networks tin and contour lines laura atkinson integrated the tin function you xu developed the contour function hailey macrander tested functions and prepared the documentation tabris thomas configured the subprojects dr mario a lopez provided guidance and consulted on the project dataset availability all lidar datasets used for testing or to derive other testing datasets were from the united states geological survey usgs 3d elevation program 3dep the datasets were located in north carolina usgs lidar datasets can be found at https viewer nationalmap gov basic funding this material is based upon work supported by the u s geological survey under grant cooperative agreement no g16ac00152 disclaimer the views and conclusions contained in this document are those of the authors and should not be interpreted as representing the opinions or policies of the u s geological survey mention of trade names or commercial products does not constitute their endorsement by the u s geological survey this manuscript is submitted for publication with the understanding that the united states government is authorized to reproduce and distribute reprints for governmental purposes acknowledgements we are grateful to nvidia for providing a hardware donation and to amazon web services for offering cloud credits we also thank the reviewers of the manuscript for providing valuable suggestions in the improvement of the manuscript and how to further increase the toolset s capabilities in the future appendix a supplementary data the following is the supplementary data to this article multimedia component 1 multimedia component 1 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2019 03 014 appendix a time cost comparisons table a1 time cost comparisons between arcgis clip and lasclip table a1 file function time cost seconds function time cost seconds a 1 lasclip 0 006 arcgis clip 0 66 a 2 0 02 2 72 a 3 0 04 5 49 a 4 0 078 11 22 table a2 time cost comparisons between arcgis summary statistics and lasstats table a2 file function time cost seconds function time cost seconds a 1 lasstats 0 036 arcgis summary statistics 0 34 a 2 0 102 1 21 a 3 0 243 2 41 a 4 0 446 5 31 table a3 time cost comparison between point cloud library and lasground table a3 file function time cost seconds function time cost seconds b 1 lasground 32 72 point cloud library 385 974 b 2 51 44 593 59 b 3 87 923 1013 041 b 4 170 79 1956 table a4 time cost comparisons between arcgis kernel density and lasdensity table a4 file function time cost seconds function time cost seconds b 1 lasdensity 0 664 kernel density arcgis 32 38 b 2 1 038 57 8 b 3 2 086 107 b 4 3 623 221 table a5 time cost comparisons between arcgis slope and aspect and lasslope aspect table a5 file function time cost seconds function time cost seconds c 1 lasslope aspect 0 032 arcgis slope and aspect total 1 69 c 2 0 125 6 52 c 3 0 271 13 88 c 4 0 414 29 73 c 5 0 772 58 32 table a6 time cost comparisons between arcgis contour and lasgridcontour table a6 file function time cost seconds function time cost seconds c 1 lasgridcontour marching square 1 044 arcgis contour 1 63 c 2 3 493 3 92 c 3 7 949 10 35 c 4 16 899 18 48 c 5 33 544 29 71 appendix b detailed breakdown time costs for contour function the contour function consists of multiple steps including contour line detection using the marching square function contour line connection and final file creation only the marching square algorithm is implemented in parallel we have tested the breakdown time costs of performing the tests for one round of contour line creation and summarized the test results a the breakdown time of performing marching square function using gpu the breakdown time costs include three parts the time cost of transferring data from cpu to gpu the time cost of the kernel execution and the time cost of transferring data from gpu to cpu according to the test results data transfer takes a significantly longer time than the actual kernel execution table b1 time costs of gpu functions table b1 time cost units ms c 1 c 2 c 3 c 4 c 5 cpu to gpu 20 111 291 481 746 gpu kernel 1 3 5 11 28 gpu to cpu 49 214 484 962 2028 b performance between cpu serial and gpu parallel we further tested running the two versions of the marching square algorithm serial cpu and parallel gpu the data transfer is included as part of the total time cost of the gpu parallel algorithm results show that the speedup ratio is around 1 5x given this result is based on the serial version of cpu versus the parallel version of gpu parallel algorithms on multi core cpu can deliver better performance in this case table b2 time costs of cpu serial and gpu parallel functions table b2 time cost units ms c 1 c 2 c 3 c 4 c 5 cpu serial 103 468 1113 1405 4802 gpu parallel 70 328 780 1454 2802 ratio 1 47 1 43 1 43 0 9663 1 71 
26213,lidar products are provided at fine spatial resolutions and the data volume can be huge even for a small study region therefore we have developed a parallel computing toolset that is built on graphics processing units gpus computing techniques to speed up the computational processes on lidar products the toolset provides a set of fundamental processing functions for lidar point cloud data serving as a basic toolkit to derive terrain data products with this toolset scientists with limited access to high end computing facilities can still perform efficient analysis of lidar products without dealing with the technical complexity of developing and deploying tools for these products we have integrated data decomposition methods to handle files that exceed the memory capacity of gpu devices preliminary results show that gpu based implementation yields high speedup ratios and can handle files with a maximum size of 8 gb keywords lidar parallel processing graphics processing units 1 introduction improvements in data collection techniques have significantly contributed to the explosion and availability of high resolution geospatial dataset one technique that contributed to this increased data availability is the usage of light detection and ranging lidar to produce high resolution point clouds to describe various geographic features lidar datasets have been used in many environmental applications such as hydrological modeling barnes 2017 noh et al 2018 tree leaf density estimation béland et al 2014 landslide susceptibility mapping emergency response planning dennison et al 2014 and wind resource assessment lukač et al 2017 in these applications high resolution point clouds provide detailed 3d spatial depictions of geographic structures through which scientists can extract various attribute metrics of geographic features and construct complex models for environmental processes marselis et al 2016 due to the high resolution lidar datasets can be voluminous even within a small study area yang et al 2014 consequently processing massive lidar data in a serial manner takes an extremely long time which imposes constraints on using lidar for time critical environmental applications wendel et al 2015 such as flood simulation for emergency response noh et al 2018 to resolve the issues of processing massive lidar data some researchers either make their study areas small or lower the resolution which leads to inaccurate analysis yang et al 2014 vaze et al 2010 to speed up the computation process on high resolution massive point cloud significant research is taking place in developing high performance lidar processing methods that leverage the power of parallel computing previously developed parallel computing functions include digital elevation model dem generation guan and wu 2010 han et al 2009 huang et al 2011 sharma et al 2016 triangulated irregular network tin creation guan et al 2012 oryspayev et al 2012 and point classification hu et al 2013 sharma et al 2016 some of these functions are optimized for multi core central processing unit cpu environments guan and wu 2010 sharma et al 2016 han et al 2009 huang et al 2011 while other functions run with a graphics processing unit gpu environment using specialized programming interfaces provided such as nvidia s compute unified device architecture cuda hu et al 2013 by utilizing gpu threads it is possible to achieve improvements from 100 times to 1000 times over a cpu serial version tang and feng 2017 if such computational achievements and more functions were widely available and easily accessible to environmental scientists it would allow environmental science researchers to quickly process lidar data for larger spatial extents previous efforts to accelerate lidar data processing such as zhang and li 2015 have not met many requirements for widespread use in environmental research first many studies implement functions in a case by case manner meaning that fundamental processing functions that researchers need are missing fernandez et al 2007 scientists often require access to many basic functions not just one or two for their research without all the functions they need scientists cannot effectively examine geographic features e g changes in terrain characteristics therefore more functions need to be included in a systematic way second a large number of developments are based on high performance computing clusters which impose hardware constraints on users to fully leverage the power of modern computation techniques li et al 2018 in particular cpu based or gpu based clusters are costly limiting the accessibility to high performance computing power cuomo et al 2015 gonzález domínguez et al 2015 third solutions built based on desktops or laptops are limited by the memory constraints of gpu devices which are smaller than the main memory there is a need to integrate data decomposition techniques to support file processing barnes 2017 côte et al 2011 vaze et al 2010 to enable fine level analysis as few users have access to cluster environments providing a high performance solution using conventional devices is promising to enable a wider range of users to process and analyze lidar data on their laptops or desktops in a cost effective high performance manner these types of developments bring the everyday researcher closer to powerful computation and geoprocessing solutions in response to the needs of developing high performance toolsets for lidar data processing that are accessible to conventional users we have developed lightweight gpu based parallel processing tools that can be utilized by end users who do not have access to cluster environments considering the variety of lidar applications in environmental and geospatial research we only focus on terrain surface analysis as many different research disciplines utilize these functions in their data processing workflows and can allow for advanced analysis such as marselis et al 2016 and noh et al 2018 our tools meet the following critical objectives first the toolkit implements a set of parallel lidar geoprocessing functions using nvidia cuda many core gpu computing techniques to deliver high performance computing capabilities to end users using these techniques our solution delivers efficient processing capabilities with conventional computing devices i e desktops or laptops with gpu devices second the toolkit incorporates a data decomposition module and an additional gpu configuration module to process variably sized lidar datasets with the data decomposition module the tools provide data processing capabilities to handle files with sizes exceeding the memory of gpu devices note while the decomposition file allows the tools to handle files that are larger than the gpu memory the toolset runs in conventional computing environments not clusters the size of a single file cannot exceed the memory of the cpu our tools can provide a high performance and accessible data processing solution for researchers seeking to efficiently process lidar products our research benefits the environmental science community in two ways first the toolset delivers an efficient yet extensive lidar processing toolkit to enable a variety of users from the environmental science or other relevant communities to efficiently perform research using lidar products using these tools scientists can execute various terrain analysis and processing tasks on lidar datasets ranging from basic data preparation to terrain surface modeling and can benefit anyone who uses lidar data thus these tools can increase scientific engagement by providing a solution that can be easily deployed to common computing devices including laptops and desktops without the need for a cluster environment and accelerate fundamental research using the gpu second through our research the environmental and geospatial communities gain examples of parallel processing solutions to guide the development of advanced environmental applications by providing a lightweight high performance geoprocessing pipeline using this pipeline practitioners can rapidly prototype deploy and test new algorithms as the tools include both executables and source codes users of various technical knowledge levels can utilize efficient gpu computing for lidar datasets by building a toolkit that contains the previously identified components we can increase researcher access to powerful computation and geoprocessing solutions these development efforts make the following contributions a propose a set of efficient parallel geoprocessing methods for lidar and its derived products with the gpu parallel processing can be used to speed up common but computationally intensive processing functions these methods are fundamental to many advanced environmental applications researchers who have limited access to high end computing devices can leverage parallel processing power to speed up lidar data processing and can expand the tools to support more complex missions b by utilizing data decomposition strategies the toolset can provide processing capabilities to handle large files in this case larger files mean that the file size exceeds the memory of a single gpu device but not that of main memory further the strategies allow the end user to perform parallel processing on a single machine that is equipped with multiple gpu devices the rest of the manuscript is organized as follows section 2 provides an overview and explains the components of the tools section 3 presents the experiments results section 4 summarizes the test results and proposes future work 2 the design and the implementation 2 1 overall design of the library to build upon previous research into improving lidar processing the gpu based tool should meet two major requirements first it should provide fundamental parallel processing functions for lidar point clouds to efficiently support basic analyses we have examined the typical functions provided by popular geospatial solutions to handle lidar datasets and lidar derived data products e g lastools isenburg 2012 and arcgis among these popular tools some functions are designed to derive products to describe terrain characteristics e g dem whereas others are designed to process point cloud data e g point filtering classification as all featured functions have different levels of computational complexity and different types of spatial dependency we must examine the workflows of every function and the spatial dependencies of the input and the output data to design parallel implementation strategies tailored to specific problems our strategies should exploit the parallel computation power of many core gpus while maintaining the appropriate workload balance among computing cores the parallel implementation strategies of our chosen functions are discussed in detail in section 2 2 secondly the toolset should overcome the limitation of gpu on board memory to handle files with a single file size ranging from several hundred megabytes mb to several gigabytes gb this is the common range of the file size within conventional devices when the file size exceeds the onboard memory of gpu the tools should apply an appropriate data decomposition strategy to decompose the data into small subsets so that the gpu can process the decomposed files these strategies should maintain the spatial dependencies of the original datasets and introduce minimal computational overhead in addition to the memory limitations of gpu devices the computational capabilities of computing devices vary in the case of multiple gpu environments multiple gpu devices can run concurrently to fully use the computational power of all gpus the parallel execution strategy evaluates the configuration of gpu devices and determines the data allocation to computing threads for parallel execution the parallel execution strategy adds scalability in the number or type of gpu device s used in data processing and is necessary in maximizing the computational power of a machine below we explain the major components of the tools which include a progressive data loader an adaptive gpu configurator and a set of cuda kernels fig 1 a progressive data loader determines how a dataset denoted as d in fig 1 should be decomposed into small subsets denoted as d based on data volume spatial extent processing methods and hardware constraints of a gpu device e g gpu onboard memory a gpu configurator retrieves the information of the computing capabilities of gpu s to determine the appropriate parallel configuration of the device i e block size and thread size denoted as ω to better utilize the available computing resources when the machine has multiple gpu devices this module also allows for the utilization of multiple gpus in a parallel manner to speed up computation by loading and processing multiple data subsets denoted as d p the final component of the tools are a set of cuda kernels which includes a set of parallel processing functions executed by gpus examples of processing functions are data conversion data filtering and interpolation see table 1 all components are loosely integrated into a processing pipeline and are necessary to meet the requirements for efficient lidar processing for research purposes the progressive data loader enables the toolset to handle files exceeding the memory capacity of gpu devices using this toolset the researcher can read lidar datasets into main memory and prepare datasets in the form required by gpu processing the configurator ensures that the parallel computing strategy is adjustable to better utilize the computing power of the gpu devices with the kernels scientists can perform basic data manipulations on data in a parallel manner since the list of available kernels represents different parallel implementation strategies gis practitioners can expand the toolset by referring to these strategies further the toolset provides necessary interfaces e g simple executables and modular source codes for different components to interact with the pipeline to ensure that users with different levels of technical proficiency can use the library for their own applications not only will the components of the parallel processing tools contribute to the performance of the geoprocessing functions but also the language and libraries used to build it while some programming languages such as python permits more flexible customization and less steep learning curve to ensure the tools can deliver relatively high performance in terms of pre and post data processing we have developed the functions in the c language admittedly the development choice will introduce difficulties in deploying and expanding the toolsets at the current stage our primary focus is to deliver high performance processing capabilities in the future we will explore the wrapper option with other languages further the tools are built with cuda c 1 1 http docs nvidia com cuda cuda c programming guide index html there are a few libraries such as openmp openacc that support the development with cuda enabled gpus e g xu et al 2013 guo and wu 2016 these libraries improve the programmers productivity and yield better portability they are arguably preferable for the expansion of our toolset and for increasing the user community base studies show that these libraries do not provide low level customizations such as shared memory management e g memeti et al 2017 for this reason we used cuda c and expect to expand the toolset when those libraries provide additional features besides cuda c we utilize thrust to perform simple operations such as sorting minimum maximum and unique value identification these tools provide a high level abstraction of cuda implementations according to the results from benchmark testing thrust delivers similar performance gains when compared to native cuda c implementation we use additional geospatial libraries e g lastools and gdal ogr contributors 2018 to implement the reading and writing functions for geospatial files the lastools library is mainly used by our tools to read and write lidar files in a las file format we have developed all geoprocessing functions but used similar naming conventions to those in lastools the tools run on all windows and linux machines also we have customized a virtual machine image in amazon web services aws users can create a virtual machine instance using the image and use the preconfigured library to perform geoprocessing tasks we will consider expanding these tools to other platforms in the future below we discuss the development considerations for each of the components of the parallel processing tools 2 2 cuda kernels to identify the cuda kernels that the tools should provide we have reviewed the functions of typical lidar programs e g arcgis and lastools and identified a set of fundamental geoprocessing functions for lidar points these functions either process the original point cloud e g creating an elevation surface from point clouds or compute derived products e g creating contour lines from elevation surface raster data a summary of the functions can be found in table 1 the types of parallelism in table 1 are further illustrated in figs 2 4 the set of tools also includes a function to produce a tin which is built based on an implementation of cao et al 2014 since we did not develop the function but adopted an existing one we did not include the function in the table the list is still expanding as we are actively adding more functions while more advanced functions are possible the purpose of these functions is to enable fast handling of commonly used yet computationally intensive when performed on massive data processes in geography and related fields the variety of the functions also illustrates different types of parallel implementation strategies which can help gis practitioners expand the tools in creating the functions we consider the design philosophy of the cuda parallel computing paradigm this paradigm is based on data parallelism which distributes datasets to different computing threads for simultaneous processing using the same instruction stream in developing the cuda kernels one challenge is to maintain spatial dependency when distributing data to threads for parallel implementation we have identified several parallel implementation strategies for different types of processing functions the chosen strategy for a function depends on whether the parallel implementation is imposed on the input or the output space these parallel strategies are commonly classified as input space based fig 2 and output space based fig 3 parallelism strategies can also be classified based on how the data are distributed to threads for parallel processing e g by the spatial extent or by the number of points given that a few geoprocessing functions such as zonal statistics require all input data points to produce the final output we follow the principle of divide and conquer which performs multiple rounds of parallel processing for data subsets fig 4 depending on the allocation of data to threads parallelism strategies can be classified based on input space type 1 or output space type 2 for certain operations the parallel execution runs multiple times to produce the final output termed as parallel reduction type 3 below we explain the three typical parallel strategies in detail type 1 parallelism means that every thread processes a subset of input points the gpu kernel runs only one round of computation to produce output data an example of this strategy is clipping which identifies the point within a bounding box or polygons fig 2 every thread evaluates if a set of points are inside the bounding box and labels the status of those points as in or out once the parallel computation finishes steps in green boxes in fig 2b a cpu serial processing function starts which writes the points that are inside the bounding box to an output file we employ this strategy when there is spatial independency in input points that is there is no data communication or transfer between the points from different computing threads type 2 parallelism means that every thread processes a set of output elements e g grid cells in an independent way as to maintain the spatial dependency in the input elements e g points grid cells when multiple points contribute to the result of a basic processing unit e g a grid cell the parallel processing strategy can be applied to output space for example the inverse distance weighting based interpolation method idw produces a raster surface where multiple points contribute to the value of a grid cell lu and wong 2008 in this case every grid cell performs a neighborhood search to identify points and apply a mathematical formula to calculate an elevation value using all points within the neighborhood steps in green boxes in fig 3 there is spatial independency in the output space the value of a grid cell output is determined by the values of points input in the neighborhood of the grid cell the value of the grid cell however does not depend on the values of grid cells in its neighborhood the input is the point dataset and the output is the raster layer every thread processes a set of grid cells independently in the meantime a point maybe be used by multiple threads concurrently for computations see the figure below the parallelism is implemented on the grid cell output space fig 3 describes the data distribution and the corresponding parallel processing steps using interpolation as an example type 3 parallelism involves parallel reduction through a divide and conquer strategy certain geoprocessing functions derive aggregated results from a set of points for example the statistical computations identify the statistical information e g mean maximum of a set of points spatial dependency exists in both input and output space therefore we introduce a parallel reduction approach which launches multiple rounds of parallel computations such as sorting and statistics fig 4 illustrates this type of parallelism strategy using statistical summarization as an example every round processes a subset of data from the previous rounds of computation in parallel and the operation continues as the volume of data reduces in fig 4a in the first round every thread processes a subset of points in the blue rectangular region and derives the summary results in the second round every thread processes five summary results from all blue regions in the red region the regions are spatial extents of the file or the file s subsets when the data volume reduces to the point where the performance gain from parallel processing is not significantly better when compared to serial processing the kernel stops this is currently implemented in a heuristic manner in this section we discussed the parallelism strategies implemented in the cuda kernels which allow users to efficiently process data using various geoprocessing functions depending on the function s spatial dependency 2 3 progressive data loader to efficiently process lidar data the tools must address the fact that gpu devices have lower memory capacity than main memory for example amazon ec2 g3 instances are equipped with nvidia telsa m60 gpus while this instance has 122 gb of main memory every gpu in the instance only has 8 gb of device memory loading an appropriate amount of data for processing is important to address the memory constraints of gpu devices in the context of multi gpu parallel computing tang and feng 2017 propose a multi gpu decomposition method based on indices of points for parallel map projection for massive lidar datasets as the bounding polygons of lidar datasets can be irregularly shaped many decomposition methods for fundamental geoprocessing must be reexamined because they require regularly shaped bounding polygons in the case of computing density the spatial extents of both input and the output datasets should be rectangular shapes a simple point indexing method often fails to provide subsets with rectangular shapes defined by the spatial extent of output raster layers fig 5 b we focus on two methods to decompose data and generate small subsets of data to meet the requirements of memory constraints these methods are indexing based and region based in both cases a post processing step is necessary to combine the outputs from the sub datasets we explain the two chosen approaches below an indexing based data decomposition method retrieves a slice of points from a list based on a pair of index numbers that defines the start and the end indices of the target points because the point indexing does not conform to the spatial order of points the subset of data may not maintain the spatial adjacency of the points even arrays of the same size may not have the same spatial extent as a result a subset of points may cover multiple isolated regions we may apply this method to geoprocessing functions that do not require the maintenance of spatial adjacency such as lasclip lasstats and lasheight a region based decomposition method defines a set of adjacent bounding boxes covering the study area i e the spatial extent of the input dataset and allocates points to different bounding boxes to allocate points to these bounding boxes we apply the clipping function to derive sub datasets and send the sub datasets to the processing pipeline this method is applicable to lasslope aspect lasduplicate lasgridcontour lasdensity las2grid and lasground in the case that raster data are used as the input the region based decomposition retrieves subsets of data based on the row and column numbers fig 5 shows the spatial extents of the two strategies of spatial data decomposition the spatial extent of the original is shown in fig 5a fig 5b shows the index based decomposition the subsets that are produced from the data decomposition have irregular bounding shapes therefore this method is not suitable for geoprocessing functions that require the maintenance of regular spatial extents for example when performing interpolation using an irregular bounding shape a point and its neighboring points may not come from the same subset however this decomposition is well suited for the summary statistics function the parallel calculation is concurrently performed on different groups of points the spatial locations of points do not change the summary results fig 5c shows the region based data decomposition each subset has a rectangular bounding shape with a small portion overlapping with its neighboring subsets this overlapping region contains points that are required by the specific geoprocessing functions for example to calculate the slope value of a point the function requires the elevation values of points in its neighborhood to calculate the maximum rate of elevation change from the point to its neighborhood this maximum rate is the slope value the region based data decomposition method first calculates the rectangular spatial bounding extents for the subsets and performs data filtering using these bounding extents it then expands the regions and identifies the points within the expanded regions in this way points on the edges of the bounding extent can have neighboring points from the same subset for computing there are two variations when applying the above methods first while a few geoprocessing functions can work well with either decomposition method we frequently use the indexing method to avoid unnecessary clipping operations because indexing is significantly more efficient second when the file size exceeds the onboard memory of gpu and spatial region based decomposition is necessary we use a combined indexing and region based decomposition strategy we first use the indexing decomposition to create sub datasets then perform parallel clipping on the subsets and finally retrieve the points from multiple sub datasets that fall in the designated spatial extent in this section we discussed the different methods used to decompose input data in the toolset with the data decomposition strategies the toolset is capable of handling variably sized datasets with the data decomposition method implemented in our toolset users do not need to perform data decomposition before data processing thus simplifying the workflow for the end user 2 4 gpu configurator while the progressive data loader decomposes the data for parallel processing the gpu configurator module examines the configuration of gpu devices and determines an optimized execution strategy to perform parallel computing there are three important functions of this component which include a basic hardware profiling b gpu controller and c maximum resource occupancy based kernel execution fig 6 through profiling we obtain information such as gpu memory number of gpu cores and maximum number of allowed threads in particular the gpu memory determines the amount of data for processing in a single round when multiple rounds of processing are necessary or multiple gpu devices are present the gpu controller manages the execution of every round of processing in the gpu device s finally we use maximum resource occupancy to determine the number of threads to perform geoprocessing tasks except for the gpu controller the other two functions are very straightforward and based on built in cuda functions this configurator connects with the data decomposition module to ensure that the tool can process variably sized files by fully utilizing the users gpu devices below we describe the controller the controller includes a multithreaded design that allows the kernels to scale to multiple gpus two options are possible for multiple gpu enabled parallel processing the first option occurs when every cpu thread controls one gpu device this option leverages the computing power of multiple cpu cores to achieve this we utilize a multi thread technique that launches multiple cpu threads and every thread controls the execution of a kernel on a designated gpu device this option is suitable for most basic kernels where results are only dependent on the input sub datasets e g clipping with polygons the communication between gpu devices is realized through the communication between cpu threads the second option occurs when a single cpu thread controls all gpu devices this option is applicable to functions that require multiple rounds of data processing and the input of the round depends on the output from another gpu device from the previous round e g lasground in this case the communication between gpu devices is realized through enabling peer data transfer among gpus by integrating with the other components of the toolset the gpu configurator allows the toolset to adapt to varying computation environments to implement the configurator we have developed functions based on a few functions of the cuda library nvidia s cuda library provides a set of functions that can determine the hardware configurations of gpu devices we integrate these functions i e device query device switching peer to peer data transfer among devices maximum occupancy based grid and thread configuration for kernel execution into our toolset and call them before a geoprocessing function starts with the gpu memory information from the device query function this module loads a subset of data and the data structure for this subset from the original file based on the profiling information from the same device query function we determine parallel execution strategies for the kernels in particular when multiple gpu devices are available in the same machine e g amazon p2 instances have multiple gpus the toolset can launch multiple kernels at the same time and each gpu device executes one kernel to fully utilize the computing power of devices single gpu vs multiple gpu devices to implement this feature we introduce a multithreaded gpu controller that can launch multiple cpu threads with each controlling one gpu device to execute kernels finally to determine the parallel configuration of the gpu i e the block size and the grid size instead of specifying the block size and the grid size we utilize a built in function of the cuda library which determines block and grid sizes to achieve maximum occupancy this function evaluates the complexity of the cuda parallel kernel data volume and the configuration of the gpu devices to calculate a combination of block sizes that can achieve the maximum utilization of the gpu cores 3 experiments 3 1 an overview of data and computing environment the two major features of the toolset are efficiency and the capacity of data processing with conventional computing environments to help users access these features of the toolset in conventional computing environments we conduct two main sets of experiments to measure the library s performance the first set of experiments aims to compare the performance of running functions with the cpu with the performance of running the same functions with the gpu to demonstrate the toolset s efficiency the second set of experiments demonstrates the toolset s capabilities of processing larger files with data decomposition strategies and with multiple gpu devices the main datasets used for testing purposes are from the usgs 3d elevation program 3dep 3dep is designed to provide high resolution elevation data products derived from lidar and deliver these products online the lidar files from the program have file sizes ranging from 200 mb to 800 mb to examine the effectiveness of the data decomposition strategy in processing larger files using the toolset we clipped or merged the files to produce datasets that are differently sized the maximum size of file tested is 8 gb since our tools were designed to process las files from data vendors e g the usgs s 3dep project which generally have sizes less than 1 gb for a single file and the toolset runs on conventional computing devices for these reasons we determined that an 8 gb maximum file size was sufficient for conventional computing environments it is also important to note that the maximum file size the library can process is determined by the main memory of the computing device and not by the memory of the gpu 3 2 experiment 1 efficiency tests as experiment 1 aims to highlight the efficiency of the toolset we have chosen the following functions considering different types of parallel implementation strategies and data lasclip lasstats lasground lasdensity lasgridcontour and lasslope aspect we perform the processing functions with gpu and cpu separately to evaluate the performance gains in this experiment we compare gpu parallel processing to serial cpu processing however the implemented cpu processing functions can be further optimized such as through parallel cpu processing we group the functions into three categories based on the test files required for the functions the first group includes two functions lasclip and lasstats all of these functions use four files in table 2 that are associated with different land cover types the second group includes lasground and lasdensity we produce three files from file 4 a 4 in table 2 and test the two functions with those files table 3 the third group includes three functions that require raster data as an input table 4 the raster files are created from file 4 a 4 in table 2 the files we used for all the tests were collected in north carolina we have conducted tests with amazon ec2 gpu instances g3 4xlarge 2 2 http pointclouds org which use the windows server 2012 r2 standard a 64 bit operating system have an intel xenon cpu e5 2686 2 30 ghz processor with 122 gb ram and an nvidia tesla m60 gpu to ensure that files can be processed by our toolset as well as functions from other software packages e g arcgis for comparison we perform data preprocessing to produce files in the formats required by those functions we use four main types of datasets for tests of experiment 1 including las files raster tiffs multipoint shapefiles and polygon shapefiles our toolset takes las files and raster tiffs as input files the raster tiff files were created from the las dataset to raster tool from arcgis at various grid sizes the value field for the grid values was elevation and the cell assignment was average we created multipoint shapefiles from las files to allow for comparison with equivalent functions in arcgis such as summary statistics and clip which only take shapefile inputs to create the multipoint shapefiles first we create a multipoint shapefile in arcgis then we use the las2shp tool from the lastools library to write all the points from the las file to the multipoint shapefile because the elevation values for the points are removed in this process we add the elevation values back using the arcmap add z information 3d analyst tool and the mean value for other arcgis comparisons we use create las dataset tool from arcgis that can load las files directly for use in the arcgis comparisons finally we produce the polygon shapefiles for the clipping function using the arcmap editing toolbar each polygon shapefile contains only one simple polygon corresponding with one las file upon the completion of the creation of the appropriate testing files we ran the tests formula 1 shows the computation of the speedup ratio wu et al 2009 we use speedup ratio rather than actual time cost because the actual time costs vary significantly but the actual time costs can be found in appendix a s p e e d u p s e r i a l r u n t i m e o f t h e c p u p a r a l l e l r u n t i m e o f t h e g p u formula 1 calculation of speedup ratio 3 2 1 group a lasclip and lasstats the functions tested in this round process point data for the lasclip test we clipped a las file or input multipoint shapefile using a polygon shapefile created for las files a 1 a 4 we compared the lasclip function with the arcmap clip analysis tool as the only clipping tool in arcmap that was comparable to the lasclip tool is the clip analysis tool we had to create a multipoint shapefile from the las files for this test the multipoint shapefiles have the same number of points as the las files and the same spatial extent like the lasclip test we need to use the multipoint shapefile created from the las files for these tests as the only comparable tool in arcmap was summary statistics for the summary statistics we found the mean of elevation values minimum elevation maximum elevation the range of elevation values the standard deviation of elevation values and the count of the las data we compare the arcmap time to the lasstats time fig 7 a shows the performance comparison between gpu based clipping and cpu based clipping the gpu version always outperforms the cpu version the speedup ratios range from 110 to 140 for the clipping function the small variations in speedups are partly due to the round up problem of thread specification by contrast the speedup ratios of the lasstats function are significantly lower fig 7b summary statistics identifies the aggregated statistical values such as minimum maximum and standard deviations this function requires multiple rounds of statistical computations and takes a parallel reduction approach type 3 in section 2 with this type of parallelism the number of threads for every round of computations reduces which does not fully exploit the parallel computing capabilities of all gpu cores despite different parallelism strategies we obtain relatively steady performance gains which demonstrates the efficiency of the toolset 3 2 2 group b lasground and lasdensity similar to group a this group of tests also processes point data unlike the previous group of tests both lasground and lasdensity include a search process against a large number of points to determine the results the search process is a common step in many advanced geoprocessing functions e g k nearest neighbor and classification we use three clipped files from the a 4 dataset along with entirety of the a 4 dataset to test lasground and lasdensity table 3 since the four files are from the same land cover category and in the same region the point density should be similar in the case of the lasground comparison we compare lasground to the progressive filter function of point cloud library 2 pcl as both implement the same algorithm from zhang et al 2003 lasground identifies non ground points by comparing the point elevation to the average elevation of points in its neighborhood usually a rectangular region centered at the point location if the evaluation difference is larger than a predefined threshold value the point is labeled as a non ground point in the case of the density computation we compare our function to the arcgis kernel density function to execute the geoprocessing functions we specify the required parameters of the functions the lasground comparison is conducted in multiple rounds and each round is associated with different pairs of neighbor sizes and threshold values we follow the example provided by pcl to configure the window sizes and threshold values a total of five rounds of filtering is necessary the window sizes are 3 m 5 m 9 m 17 m and 33 m the lasdensity function produces a raster output layer the cell value represents the density of points within a kernel s region defined by a search radius we set up the search radius of the kernel and the output density raster as 5 m fig 8 a shows the results for the lasground tests since we used the files from the same land cover type the speedup ratio is similar across four files the speedup ratio is about 11x the speedup ratio is not as high as the clipping function partly due to the fact that the function from pcl has a built in octree indexing structure to speed up the point retrieval and search process fig 8b shows the speedup ratios of performing the kernel density computation in this case we obtain an average speedup ratio of 56x the speedup pattern is consistent across multiple files the steady performance gains show that our toolset not only can improve processing efficiency but also provide examples of implementing searching in a parallel manner 3 2 3 group c slope aspect and contour the functions tested in this group process a digital elevation model dem in the form of raster data lasgridcontour creates contour lines we specify the minimum and the maximum elevation values as the range of the contour lines we configure the contour intervals as 100 m lasslope aspect does not require additional parameters as input the slope and aspect computations derive slope and aspect values of every grid cell based on the cell values in its 3 3 window we again produce raster files from the a 4 dataset but adjust the resolutions to obtain raster files with different files sizes table 4 fig 9 shows the test results in fig 9a we obtain speedup ratios ranging from 50x to 75x for our lasslope aspect function however the gpu parallel implementation does not yield any performance gains in the case of contour creation fig 9b the gpu parallel implementation of contour line creation follows the popular marching square algorithm when analyzing the separate time costs of the major steps of the parallel marching square algorithm the time spent on the data transfer is significantly longer than the kernel execution time considering that we compare the time costs between gpu parallel processing and cpu serial processing in this case it is highly possible that cpu parallel processing is more efficient than gpu parallel processing due to the previously stated inefficiency in gpu execution seeking parallel cpu processing method is more preferable detailed break down time costs can be found in appendix b overall this group of tests shows that the toolset can efficiently process the derived products i e dem yet further optimization is necessary in summary the results confirm that a variety of the toolset s functions deliver performance gains within conventional computing environments in this case aws instances the results also illustrate the effectiveness of parallel implementation strategies in processing lidar data and its derived products as expected type 3 data parallelism used for lasstats is least efficient due to the insufficient usage of the parallel computing power of gpu devices gis developers can refer to the results to assess the applicability and the usability of different parallelism strategies to add functions in the future further the variations in speedup ratios of different functions show the suitability of different functions for parallel implementation this suitability information can help users access the best usage of the toolset for example the average speedup ratio of lasgridcontour is close to 1 in this case we recommend using cpu processing methods for contour line creation 3 3 experiment 2 tests on larger file processing and multi gpu support besides efficient parallel processing with gpu devices another feature of the toolset is its capability to handle larger files and perform parallel processing on multiple gpu devices in the same machine group a tests the processing capabilities of the toolset on larger files using data decomposition strategies while group b tests multithreaded gpu techniques that scale cuda kernels on multiple gpu devices in a single computer one should note that since the toolset is designed for users who do not have access to clusters our toolset currently does not have the capability to run on multiple machines or cluster environments 3 3 1 group a data decomposition for larger file processing in this set of experiments we evaluate the performance of the entire geoprocessing workflow with the embedded spatial data decomposition method using lasdensity and lasduplicate as examples we record the original size of the input las file and the number of data subsets after decomposition in all cases we adjust the partition number which defines the number of subsets that are produced after data decomposition we also record the time costs of creating data subsets as well as the time costs of performing the computation to examine the additional overhead introduced by data decomposition for this test we use the following files with file sizes ranging from 1 gb 2gb 4gb and 8 gb table 5 we believe 8 gb is a reasonable file size of users who run the toolset in conventional environments as data decomposition requires the number of partitions as a parameter we also test how the performance changes when specifying a different number of partitions parts overall with our data decomposition methods the toolset can scale to handle files with file size of 8 gb in fig 10 duplicate point checking consumes more memory than kernel density computation therefore to ensure the successful execution of gpu parallel functions when processing the same file the duplicate point checking function requires more partitions than kernel density for example to process d 1 1 gb file duplicate requires data decomposition to produce four data subsets whereas kernel density computation can process the file directly the processing time cost and the data decomposition cost generally increase with the size of the files even though our toolset is able to handle larger files we recommend that las files should be organized as medium sized files to avoid significant computational overhead 3 3 2 group b geoprocessing on multiple gpu devices to compare the performance of utilizing multiple gpus on a machine versus the single gpu configuration we tested three functions a lasclip by polygon b lasduplicate and c lasground these three functions evaluate different data decomposition methods as well as two options of multi gpu processing strategies lasclip by polygon and lasduplicate use option 1 for multithread processing fig 6 the data decompositions are index based and spatial region based respectively lasground employs option 2 for parallel processing because there is a step to transfer data after every round of filtering we perform las clip by polygon and lasduplicate functions using g3 8xlarge and lasground using p2 8xlarge 3 3 https aws amazon com ec2 instance types p2 machines the g3 8xlarge instance has 2 nvidia tesla m60 gpus 244 gb cpu memory and 32 vcpus intel xeon cpu e5 2686 v4 2 3 ghz as option 2 requires peer access capabilities between gpus we use p2 8xlarge the p2 8xlarge instances have 8 nvidia k80 gpus 488 gb ram and 32 vcpus in this set of tests we use the same four files in table 2 we record the time costs of running the functions from transferring data to gpu devices to final data gathering from gpu devices we also record the time costs that every gpu device takes to complete the gpu function execution average time costs from the multi gpu instance are shown alongside times from the single gpu in fig 11 the results show that the toolset can run multiple gpu devices for both options in particular the peer to peer data transfer feature is extremely efficient for ground point detection which takes less than 0 01 of the total processing time the speedup ratio is slightly lower than 2x due to the additional time costs of launching threads distributing data to the gpu device and gathering data from multiple devices the additional costs may take a larger portion of the overall processing time when running simple functions e g clipping on smaller files further optimizations are necessary to reduce additional time costs as well as minimize the waiting period of synchronization of multiple gpu devices to achieve better performance with low computational overhead in summary both groups of tests illustrate the ability of the toolset to support large file processing and geoprocessing on multiple gpu devices group a shows the toolset s capability of handling large files with data decomposition strategies the toolset can process files with sizes up to 8 gb this feature makes it possible to process lidar products for large spatial regions and support high resolution modeling for such regions group b shows the toolset can scale to work with multiple gpu devices on the same machine with the functions of the configurator the toolset can fully utilize the computational power of both gpu devices of the instance while adding low overhead however this feature is not equivalent to the scalability of gpu clusters which allows the execution of geoprocessing tasks on multiple machines 4 conclusion this paper reports an effort in developing a cuda enabled parallel processing toolset for lidar data points this toolset addresses the needs for providing efficient and accessible geoprocessing capabilities for lidar point clouds and derived produces in environmental science research this toolset reduces computation costs for handling lidar datasets by using accessible gpu computing techniques this enhanced processing efficiency is especially useful when compared to traditional cpu only methods of processing e g in serial as they are not optimized for time efficiency since the toolset imposes minimal requirements on devices and delivers significant speedup ratios it can help environmental sciences researchers accelerate studies involving intensive lidar data processing preliminary results figs 7 9 show that significant speedup ratios from cpu to gpu are achieved for most of the functions we have developed this toolkit includes additional modules to ensure the capabilities of the functions when dealing with varying size files in different computing environments figs 10 and 11 the toolset provides multiple fundamental processing functions for terrain analysis e g basic measures of terrain features which can serve as the basis for other advanced environmental applications the solution also provides a complete processing workflow starting from data preparation to data analysis with the toolset environmental scientists can perform efficient processing on lidar datasets to support advanced modeling and analysis missions scientists with different levels of technical proficiency can deploy use evaluate and customize the library through manipulating source codes and executables despite the progress we have made with the toolset it has the following deficiencies that must be addressed the above experiments demonstrate the toolset can handle files up to 8 gb since lidar datasets can be larger than this it can be argued that the toolset does not have the ability to process large files in conventional computing environments additionally the toolset uses cuda c and many dependent libraries producing a significant barrier to users who wish to contribute to the library finally the toolset currently cannot run in a cluster environment further limiting its utility we recognize the importance of addressing these critiques in future research to improve the performance of the toolset future work will include the generation of additional advanced geoprocessing functions such as hydrological modeling murphy et al 2008 solar panel installation lukač and alik 2013 in cities and feature detection homm et al 2010 secondly there is a need to expand the library to gpu cluster environments the toolset currently works with conventional computing environments for users who do not have access to high end computing facilities e g desktop laptops the expansion will be critical for the toolset so it can allow any input function to process almost any dataset regardless of size exceeding the constraints of the main memory of a single machine thirdly providing a high level programming language wrapper e g python for the toolset will be critical for expanding its customizability and utility for a greater number of researchers additionally we plan on building easy deployment scripts which will allow users to configure the toolsets with multiple configuration steps fifthly we will explore the usage of openacc as a way to enhance the toolset s compatibility and reduce development efforts to further improve its scalability to a cluster environment finally providing a set of features such as a python wrapper and guis would enable user friendly processing and open the tools up to users who do not have experience with c or command line software availability our gpu enabled high performance lidar data processing toolset is available at https lidarlib github io gpulidarproj it was developed by a team at the university of denver led by dr jing li this toolset was built using c and requires a windows or linux machine with an nvidia gpu the toolset was first available in 2018 dr jing li developed all functions except the functions to create triangular irregular networks tin and contour lines laura atkinson integrated the tin function you xu developed the contour function hailey macrander tested functions and prepared the documentation tabris thomas configured the subprojects dr mario a lopez provided guidance and consulted on the project dataset availability all lidar datasets used for testing or to derive other testing datasets were from the united states geological survey usgs 3d elevation program 3dep the datasets were located in north carolina usgs lidar datasets can be found at https viewer nationalmap gov basic funding this material is based upon work supported by the u s geological survey under grant cooperative agreement no g16ac00152 disclaimer the views and conclusions contained in this document are those of the authors and should not be interpreted as representing the opinions or policies of the u s geological survey mention of trade names or commercial products does not constitute their endorsement by the u s geological survey this manuscript is submitted for publication with the understanding that the united states government is authorized to reproduce and distribute reprints for governmental purposes acknowledgements we are grateful to nvidia for providing a hardware donation and to amazon web services for offering cloud credits we also thank the reviewers of the manuscript for providing valuable suggestions in the improvement of the manuscript and how to further increase the toolset s capabilities in the future appendix a supplementary data the following is the supplementary data to this article multimedia component 1 multimedia component 1 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2019 03 014 appendix a time cost comparisons table a1 time cost comparisons between arcgis clip and lasclip table a1 file function time cost seconds function time cost seconds a 1 lasclip 0 006 arcgis clip 0 66 a 2 0 02 2 72 a 3 0 04 5 49 a 4 0 078 11 22 table a2 time cost comparisons between arcgis summary statistics and lasstats table a2 file function time cost seconds function time cost seconds a 1 lasstats 0 036 arcgis summary statistics 0 34 a 2 0 102 1 21 a 3 0 243 2 41 a 4 0 446 5 31 table a3 time cost comparison between point cloud library and lasground table a3 file function time cost seconds function time cost seconds b 1 lasground 32 72 point cloud library 385 974 b 2 51 44 593 59 b 3 87 923 1013 041 b 4 170 79 1956 table a4 time cost comparisons between arcgis kernel density and lasdensity table a4 file function time cost seconds function time cost seconds b 1 lasdensity 0 664 kernel density arcgis 32 38 b 2 1 038 57 8 b 3 2 086 107 b 4 3 623 221 table a5 time cost comparisons between arcgis slope and aspect and lasslope aspect table a5 file function time cost seconds function time cost seconds c 1 lasslope aspect 0 032 arcgis slope and aspect total 1 69 c 2 0 125 6 52 c 3 0 271 13 88 c 4 0 414 29 73 c 5 0 772 58 32 table a6 time cost comparisons between arcgis contour and lasgridcontour table a6 file function time cost seconds function time cost seconds c 1 lasgridcontour marching square 1 044 arcgis contour 1 63 c 2 3 493 3 92 c 3 7 949 10 35 c 4 16 899 18 48 c 5 33 544 29 71 appendix b detailed breakdown time costs for contour function the contour function consists of multiple steps including contour line detection using the marching square function contour line connection and final file creation only the marching square algorithm is implemented in parallel we have tested the breakdown time costs of performing the tests for one round of contour line creation and summarized the test results a the breakdown time of performing marching square function using gpu the breakdown time costs include three parts the time cost of transferring data from cpu to gpu the time cost of the kernel execution and the time cost of transferring data from gpu to cpu according to the test results data transfer takes a significantly longer time than the actual kernel execution table b1 time costs of gpu functions table b1 time cost units ms c 1 c 2 c 3 c 4 c 5 cpu to gpu 20 111 291 481 746 gpu kernel 1 3 5 11 28 gpu to cpu 49 214 484 962 2028 b performance between cpu serial and gpu parallel we further tested running the two versions of the marching square algorithm serial cpu and parallel gpu the data transfer is included as part of the total time cost of the gpu parallel algorithm results show that the speedup ratio is around 1 5x given this result is based on the serial version of cpu versus the parallel version of gpu parallel algorithms on multi core cpu can deliver better performance in this case table b2 time costs of cpu serial and gpu parallel functions table b2 time cost units ms c 1 c 2 c 3 c 4 c 5 cpu serial 103 468 1113 1405 4802 gpu parallel 70 328 780 1454 2802 ratio 1 47 1 43 1 43 0 9663 1 71 
26214,the iowa flood center ifc developed a pilot infrastructure to explore rainfall metadata descriptive statistics and generate rainfall products over the iowa domain based on the nexrad level ii data directly accessible through cloud storage e g amazon web services known as ifc cloud nexrad it resembles the hydro nexrad portal that provided researchers with ready access to nexrad radar data taking advantage of the cloud storage benefits unlimited storage and instant access ifc cloud nexrad reduces the common challenges of most data exploration systems which often lead to massive data acquisition ingestion and rapid filling of limited system storage its map based interface allows researchers to select a space time domain of interest retrieve and visualize pre calculated rainfall metadata and generate radar derived rainfall products because the system provides generalized approaches to compute metadata and process data for rainfall estimation the framework presented in this study would be readily transferrable to other geographic regions and larger scale applications keywords nexrad rainfall cloud computing level ii data hydrology 1 introduction precipitation is a primary driving factor of numerous environmental processes and reliable rainfall information with proper space and time scale is a key element in the successful management and prediction of these processes for decades forecasters and researchers have used weather radar for severe weather monitoring and quantitative rainfall information with high space and time resolutions over large areas e g berne and krajewski 2013 these quantitative precipitation estimates qpe are often used to drive models that describe hydrologic e g rainfall runoff and environmental e g agricultural non point source pollution processes the network of u s weather surveillance radar 1988 doppler wsr 88d radars nexrad has improved its observational capabilities based on enhanced resolution and higher accuracy base data known as level ii from this network are deposited in the national centers for environmental information ncei archive the level ii storage format and data distribution have evolved with the advancement of information technology e g crum et al 1993 kelleher et al 2007 we designed hydro nexrad to address some practical aspects of using the level ii data and level iii products and thus to support better utilization of nexrad data within the hydrologic research community krajewski et al 2011 kruger et al 2011 seo et al 2011 the hydro nexrad system provided pre calculated descriptive rainfall statistics defined as metadata in this study for broad space and time domains in addition the system processed level ii data and delivered radar rainfall products based on the user s custom selections regarding space and time domain final product resolution and qpe algorithmic options the system contained data from about 40 radar sites around the united states and included innovations such as an efficient data format relational database of comprehensive metadata map based interface and custom algorithms designed with the hydrologic user in mind unfortunately hydro nexrad was lost due to a fatal system crash and lack of funding to fully recover and maintain it using the recent advent of nexrad level ii data available through cloud storage ansari et al 2017 the iowa flood center ifc built a pilot infrastructure system ifc cloud nexrad based on hydro nexrad cloud infrastructure offers instant search and access for to the archived level ii data which is a significant enhancement compared to a conventional data request through the ncei this new opportunity reduces prior challenges faced with hydro nexrad mainly its data acquisition process and rapid depletion of its limited storage the ifc cloud nerad system demonstrates this improvement but we limit its spatial domain because the system only illustrates what could be done at the national scale which is outside of the scope of our capabilities and funding because we implemented generalized approaches in the metadata computation and processing algorithm structure one could easily transfer extend the framework reported here to a different region or larger scale the generalized approaches imply that the same data sources procedures and algorithms can be used for a different region or scale application however the algorithms may require a different set of adaptable parameter values that vary with different geographic locations e g radar basin spatial extent and rain rate estimator for a different climatic regime the main functionalities of ifc cloud nexrad are metadata search and visualization as well as rainfall product generation researchers can search for an interesting rainfall event through metadata display and request processed rainfall products for the selected event and area in this paper we describe the main features of the ifc cloud nexrad system and its detailed architecture metadata processing algorithms and graphic user interface 2 features of ifc cloud nexrad both the real time and full historic nexrad level ii data over the entire united states have recently became available through noaa s big data partnership with cloud service providers such as amazon web services aws google and microsoft ansari et al 2017 this allowed us to immediately and repeatedly retrieve level ii data from cloud storage whereas we previously had to wait for several hours to days depending on the amount of data we requested after submitting a data request through the ncei furthermore the link provided by ncei that allowed us to download the requested data would be available only within next few days real time level ii data acquisition required more vigorous efforts such as specific software e g unidata local data manager installation and configuration as well as data feed management e g kelleher et al 2007 the instant and unlimited access to level ii data on the cloud helped the ifc cloud nexrad system reduce substantial challenges attributable to ncei s antiquated methods of data storage and management e g tape based archive systems the ifc cloud nexrad system does not have limitations on the time and space domain because it does not have to retain the level ii data in the system storage this also removed the need for procedures of past and real time data ingestion and data format conversion to reduce data volume in the system storage although we limit the ifc cloud nexrad system domain to iowa because it is a pilot system we used a national mosaic of radar reflectivity maps for the computation of rainfall metadata the use of the national map enhances flexibility for future spatial extension and computational efficiency of the system in general rainfall metadata calculation requires processed radar products maps the generation of this intermediate product takes significant time particularly for large basins that require additional data processing to merge information from multiple radars after the individual radar data pre processing the use of a national coverage map for metadata computation removes or simplifies many intermediate procedures and saves significant computer resources and computation time we note that the rainfall metadata in ifc cloud nexrad covers the period from 1995 to the present which is a significant enhancement compared to the period encompassed by hydro nexrad e g 2002 to 2008 on the other hand this created an issue in use of the ready made product e g the national coverage map for metadata calculation the effects of which we will discuss in the next section estimation algorithms for rainfall product generation maintain a modular structure for the flexible combination and upgrade of algorithm modules besides major elements e g rain rate estimator the modules include a few selective schemes to correct negative effects e g non meteorological radar echoes arising from ground clutter anomalous radar beam propagation and wind turbine effects from radar error sources villarini and krajewski 2010 and merging procedures of multiple radar data those schemes and procedures include radar data quality control removal of the non precipitation echoes using polarimetric measurements seo et al 2015 synchronization of different observation times among involved radars and spatial merging of individual radar domains for a larger basin coverage we also appended a procedure to correct temporal radar sampling error seo and krajewski 2015 that often manifests as discontinuous banded rain patterns in an accumulation map for a specific period while the sequence and each processing procedure of estimation algorithms are generalized i e not site specific likely change of some adaptable parameters e g coefficients of rain rate estimator may reduce concerns regarding potential uncertainties for different regions or weather regimes a portable database called sqlite e g owens 2003 stores some of the required information used for estimation input parameters e g radar site latitude longitude radar id that covers a specific basin and latitude longitude boundary of a specific basin this stand alone database can avoid dependency on the main database server e g for metadata and thus the subsystems of metadata and data processing for product generation are fully independent we describe the details of system architecture in the next section 3 system architecture 3 1 system overview we limit the spatial domain of this pilot study to iowa where the ifc s abundant hydrologic resources see krajewski et al 2017 are readily accessible fig 1 shows the system domain covered by the seven wsr 88d radars and numerous basins within the domain in table 1 we present detailed information on the radars the number of usgs hydrologic unit code huc seaber et al 1987 basins 6 to 10 digit and the names of representative basins in the domain most of the basins shown in fig 1 belong to the two major river basins in the u s midwest i e the mississippi and the missouri rivers the system covers the period from 1995 to the present we ingest new rainfall metadata once a day and there is almost no latency with the latest level ii data reception in cloud storage the system consists of the three main elements as illustrated in fig 2 1 rainfall metadata descriptive statistics e g rainfall intensity and areal coverage for a domain of radar or basin and database 2 rainfall estimation algorithms and 3 graphical user interface gui we calculate the rainfall metadata using the national composite map and store them in a relational database e g codd 1970 the metadata computation modules are written in c and acquire necessary information for the computation from text format lookup tables e g spatial indices for basins python scripts execute the modules when new national maps are available and ingest the calculated metadata into the database through the gui researchers can query and retrieve specific metadata to meet their needs e g radar or basin of interest date and time and metadata item which enables them to find a significant rainfall event for the area of their interest the gui color codes the retrieved metadata and displays them in a calendar view after navigating daily and hourly metadata one can place an order for rainfall products through the gui once the data request is submitted specifying the spatial radar basin and time domains desired the processing manager e g python scripts starts acquiring the level ii data for the requested period from the cloud upon receiving the data the manager runs a series of processing modules e g rainfall estimation algorithms organizes input and output files generates the final products and notifies the researcher by email that the requested products are ready to download 3 2 rainfall metadata and database we calculated the rainfall metadata using the national composite map of radar reflectivity both the unidata and iowa environmental mesonet iem combine the level iii base reflectivity products see klazura and imy 1993 from all available wsr 88d radars and offer the national coverage map every 5 min while the unidata combines the base reflectivity product referred to as n0r in real time only there is no archive both the real time and past maps are available in the iem archive with an additional option for higher resolution product referred to as n0q we distinguish between n0q and n0r using the product spatial resolution 0 5 vs 1 km and quantized reflectivity intervals 0 5 vs 5 dbz associated with the provided image format 8 vs 4 bit png because of the higher resolution and precision we decided to use the iem n0q national map for our real time metadata computation we used the iem archived maps for metadata computation from the past period the archives of n0r and n0q cover different periods january 1995 november 2010 and november 2010 present respectively although the use of national coverage map provides flexibility and computational efficiency we discovered an issue related to the accuracy of calculated metadata fig 3 shows certain echo patterns that were not caused by rain we confirmed that this error arose in part from missing level ii data for which the level iii base products had been erroneously created this demonstrates that the ready made product might yield metadata errors in some cases we note that the error shown in fig 3 is not a common one and occurs only rarely e g less than 0 1 in the entire data set the ifc cloud nexrad system provides two types of rainfall metadata i e radar and basin centric fig 1 shows the seven radars and 1363 huc basins the numbers of huc basins for different digits are presented in table 1 for which we calculate the rainfall metadata in this study we initially calculate both types of metadata based on the 5 min temporal scale and then integrate them over the daily scale we calculate basin centric metadata for 10 digit huc basins and integrate them for larger basins e g 6 and 8 digit hucs because of the nested feature among different digit huc basins the radar centric metadata calculated at the 5 min scale include maximum reflectivity dbz and rainfall coverage within a corresponding radar domain defined as the 230 km range around the radar site several threshold values e g 20 30 40 and 50 dbz used in the rainfall coverage calculation can provide information on rainfall intensity or severity e g light or heavy rain daily scale integration for maximum reflectivity is relatively simple and straightforward we choose the maximum of all 5 min maximum reflectivity values over a daily time window for rainfall coverage integration we use a simple average of the 5 min rainfall coverage values within a day which does not represent actual daily rainfall spatial coverage based on our multi year monitoring experience e g http ifis iowafloodcenter org ifis demir and krajewski 2013 the spatial coverage increases as temporal scale grows longer and daily rainfall coverage in particular should approach to 100 on most rainy days therefore our definition of daily spatial coverage delivers a reasonable sense of the information rather than reporting the actual daily coverage about 100 for almost rainy days additional radar centric metadata at the daily scale include temporal fraction of rain events and data missing rate we use the 5 min rainfall coverage values as a binary indicator and integrate them over a specific day for the calculation of temporal fraction we define the missing rate as a proportion of the number of 5 min observations acquired to the expected number of 5 min observations within a daily time window e g 288 we present the detailed parameters e g thresholds and formulations of the radar centric metadata in table 2 the basin centric metadata calculated at the 5 min scale include maximum rainfall rate and location mean areal rainfall rate conditional mean areal rainfall rate and basin rainfall coverage in this metadata calculation we estimate rainfall rate mm h using the nexrad relation of radar reflectivity rain rate e g fulton et al 1998 z 300r 1 4 the term conditional indicates that the areal average is estimated solely for the rainy area the daily integration procedures for maximum rainfall rate and basin rainfall coverage are the same as those for the radar centric ones spatial integration from 10 to 6 and 8 digit hucs of metadata regarding mean areal rainfall rate requires an additional metadata component e g the number of rainy pixels within 10 digit huc basins because mean areal rainfall between nested basins at different scale cannot be linearly averaged or integrated in space table 3 shows detailed formulation on the metadata computation and their spatial integration currently the total capacity of the rainfall metadata stored in the ifc cloud nexrad database is about 223 gb and the database slowly grows as we ingest new metadata every day approximately 29 mb day for the entire domain shown in fig 1 in table 4 we provide the current database volume size for all radar and basin centric metadata the ifc cloud nexrad database also contains general information on the involved radars and basins 1 radar radar id and site location city state and latitude longitude and 2 basin huc id and the name of basin the map interface displays this information when a user selects one of the radars or basins 3 3 rainfall estimation algorithms rainfall product generation upon data request involves a variety of radar data processing algorithms including data quality control rain rate estimation rainfall accumulation and conversion of spherical polar to geographic coordinates these modular algorithms are written in c and the processing manager python scripts connects and sequentially executes them by arranging the input and output files of each algorithm module a product request for a large basin covered by multiple radars requires exclusive merging procedures that synchronize different observation times and combine different spatial coverages among the involved radars for detailed procedures see fig 4 because the data processing sequences for all individual radars are identical before feeding processed individual data to the merging modules we perform parallel like data processing using a python package called multi processing to reduce data processing time we did not apply this method to the hydro nexrad data processing stream in which all algorithm modules processed data in a sequential order only the parallel like processing can also split data for an entire period requested into several subperiods based on the random selection of files feeding each split group of data into the same sequential processing stream using multiple cores processors we note that rainfall accumulation cannot take advantage of parallel processing because the temporal order of input data is a critical factor of the accumulation procedure seo et al 2011 documents most individual rainfall estimation algorithm modules implemented in ifc cloud nexrad while hydro nexrad offered users selective options for algorithm combinations depending on product quality and processing time we fix and employ a unique algorithm combination for simplicity in ifc cloud nexrad the unique combination is identical to the one used for the ifc qpe see krajewski et al 2017 the ifc algorithms encompass data processing elements associated with data quality control using polarimetric features seo et al 2015 hybrid scan e g cappi product generation radar data merging rain rate conversion using the nexrad z r relation e g z 300 r1 4 and rainfall accumulation with advection correction seo and krajewski 2015 the advection scheme fills a radar temporal scanning span e g about 5 min with a 1 min interval by using a calculated storm velocity vector between two consecutive rain rate maps and corrects rainfall accumulation errors closely associated with the radar scanning frequency storm velocity and rainfall product spatial resolution this correction is more effective for fast moving rain systems that often generate spatially discontinuous rainfall accumulation patterns fig 4 shows the sequential processing procedures of the ifc cloud nexrad rainfall estimation once a product request has arrived the processing manager starts to download the level ii data files from the cloud and communicates with the sqlite database to prepare the input parameters required to run the processing modules the parameters stored in sqlite include radar site information e g site elevation and radar tower height and radar basin spatial extent the system uses the information on site elevation and tower height when building the cappi product the spatial extent of a specific basin is represented as a rectangular bounding box e g upper left and lower right corners which the processing manager uses for multiple radar data merging and coordinate conversion e g polar to geographic coordinates 3 4 graphic user interface we developed a web based gui that interacts with researchers and fulfils their requests which can include rainfall metadata display rainfall product generation and more researchers can define space and time domains and retrieve metadata that are graphically presented through the gui as shown in fig 5 the gui involves three elements 1 map interface 2 rainfall metadata browser and 3 rainfall product request the map interface integrates google maps api to support flexible navigation and visual selection among the domains of involved radars and huc basins because of the nested and hierarchical structure basin selection requires the user to confirm how he or she wants to view the metadata users can choose the initially selected scale e g 6 digit huc basin or move to smaller scale basins e g 8 digit hucs for convenience the interface also provides functionality that automatically adjusts a zoom level for the selected scale basin metadata browsing begins with specifying the domain radar basin of interest year and metadata listed in the browser some metadata elements e g rainfall coverage require the user to indicate the parameters e g threshold values to be used for metadata calculation daily metadata are color coded and displayed in a calendar view while the 5 min metadata time series for a selected day is also available in the panel below the calendar as seen in fig 5 the calendar interface allows a user to examine entire days in a selected year and pick a specific day with rainfall activity to request a rainfall product a researcher needs to specify the period e g start and end dates and product type e g rain rate and rainfall accumulation the information on the spatial domain is automatically transferred from the rainfall metadata browser once the researcher pushes the submit request button after inserting his her email address the product request specification is transmitted to the database as an active job the rainfall processing manager regularly checks the job list in the database and activates processing algorithms for rainfall product generation for any active requests the manager sends an email notification when the requested products are ready with a link to download the products the product delivery time may vary with the domain size and period length the average time for data processing and product delivery is approximately 5 h including data downloading time of about 20 min for a request of single radar products for a rainy month e g more than 7000 level ii files we note that the processing time for a period after june 2016 takes about three additional hours because of the level ii data format change this format change requires an additional file compression format conversion process that is compatible with the processing modules we plan to develop and add a pre processing module that can directly read the format to accelerate the data processing 4 summary and discussion based on the recent advent of nexrad level ii data archive in cloud storage ansari et al 2017 we built ifc cloud nexrad with major features of rainfall metadata and rainfall product generation because the level ii data in the cloud are immediately accessible for any past time period or real time and all wsr 88d locations ifc cloud nexrad can reduce significant challenges such as waiting time for data availability and the limited system domain space and time the ifc cloud nexrad system provides radar and basin centric rainfall metadata descriptive statistics over the iowa domain we used the national composite reflectivity map and calculated the metadata for the 5 min and daily scale from 1995 to the present the relational database manages radar basin information as well as the calculated metadata and communicates with the gui to efficiently retrieve required data information for visualization researchers can also request rainfall products for their search domain and period through the gui based on the specification of the request the system executes processing algorithms and delivers the final products via an internet link the modular structure and generalized processing procedures of the ifc cloud nexrad system enable flexible upgrade of rainfall estimation algorithms the successful applications of the hydro nexrad algorithms to a variety of different regions in the united states e g lin et al 2010 seo and krajewski 2011 yang et al 2014 verify that the capability of our estimation algorithms is not limited to iowa for more reliable rainfall estimation we will soon implement a state of the art radar rainfall estimation algorithm e g ryzhkov et al 2014 that promotes scientific enhancements using weather radar s polarimetric capabilities e g istok et al 2009 we note that the quantitative estimation of snow is still a challenging issue and thus the estimation algorithm in the system may not capture well some snow cases other observational e g satellites or model based e g numerical weather predictions resources can also be incorporated into the algorithm stream to improve the accuracy of rainfall estimates once those resources are made available online while we limit the current system s spatial domain to iowa the framework developed here both the computational configuration and scientific algorithms can be readily transferrable to some research organizations for regional or national scale applications perhaps the consortium of universities for the advancement of hydrologic science inc cuahsi https www cuahsi org should be a candidate to manage a large scale application such as this providing the water science community with a wide range of online services and support the additional resources required for the domain extension are the spatial coverage information and lookup tables e g for added radars and basins for metadata computation and algorithm processing software availability ifc cloud nexrad is available at http s iihr52 iihr uiowa edu ifc cloud nexrad we initially built the system on aws e g ec2 instances we fully tested the network communications and relevant computations as well as gui functionalities on the cloud environment upon the completion of the unidata supported project we moved the system to our local server at the university of iowa currently rainfall product request is available from 2008 at which time the wsr 88d sampling resolution was upgraded e g super resolution seo and krajewski 2010 we note that the maximum length of period for product request is limited to 30 days to protect the system from unwieldy bulk orders acknowledgments this work was supported by the iowa flood center at the university of iowa and the unidata ucar community equipment awards program under grant no z16 21958 the authors are grateful to radoslaw goska who helped design and build the database and gui for the ifc cloud nexrad system appendix a supplementary data the following is the supplementary data to this article data profile data profile appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2019 03 008 
26214,the iowa flood center ifc developed a pilot infrastructure to explore rainfall metadata descriptive statistics and generate rainfall products over the iowa domain based on the nexrad level ii data directly accessible through cloud storage e g amazon web services known as ifc cloud nexrad it resembles the hydro nexrad portal that provided researchers with ready access to nexrad radar data taking advantage of the cloud storage benefits unlimited storage and instant access ifc cloud nexrad reduces the common challenges of most data exploration systems which often lead to massive data acquisition ingestion and rapid filling of limited system storage its map based interface allows researchers to select a space time domain of interest retrieve and visualize pre calculated rainfall metadata and generate radar derived rainfall products because the system provides generalized approaches to compute metadata and process data for rainfall estimation the framework presented in this study would be readily transferrable to other geographic regions and larger scale applications keywords nexrad rainfall cloud computing level ii data hydrology 1 introduction precipitation is a primary driving factor of numerous environmental processes and reliable rainfall information with proper space and time scale is a key element in the successful management and prediction of these processes for decades forecasters and researchers have used weather radar for severe weather monitoring and quantitative rainfall information with high space and time resolutions over large areas e g berne and krajewski 2013 these quantitative precipitation estimates qpe are often used to drive models that describe hydrologic e g rainfall runoff and environmental e g agricultural non point source pollution processes the network of u s weather surveillance radar 1988 doppler wsr 88d radars nexrad has improved its observational capabilities based on enhanced resolution and higher accuracy base data known as level ii from this network are deposited in the national centers for environmental information ncei archive the level ii storage format and data distribution have evolved with the advancement of information technology e g crum et al 1993 kelleher et al 2007 we designed hydro nexrad to address some practical aspects of using the level ii data and level iii products and thus to support better utilization of nexrad data within the hydrologic research community krajewski et al 2011 kruger et al 2011 seo et al 2011 the hydro nexrad system provided pre calculated descriptive rainfall statistics defined as metadata in this study for broad space and time domains in addition the system processed level ii data and delivered radar rainfall products based on the user s custom selections regarding space and time domain final product resolution and qpe algorithmic options the system contained data from about 40 radar sites around the united states and included innovations such as an efficient data format relational database of comprehensive metadata map based interface and custom algorithms designed with the hydrologic user in mind unfortunately hydro nexrad was lost due to a fatal system crash and lack of funding to fully recover and maintain it using the recent advent of nexrad level ii data available through cloud storage ansari et al 2017 the iowa flood center ifc built a pilot infrastructure system ifc cloud nexrad based on hydro nexrad cloud infrastructure offers instant search and access for to the archived level ii data which is a significant enhancement compared to a conventional data request through the ncei this new opportunity reduces prior challenges faced with hydro nexrad mainly its data acquisition process and rapid depletion of its limited storage the ifc cloud nerad system demonstrates this improvement but we limit its spatial domain because the system only illustrates what could be done at the national scale which is outside of the scope of our capabilities and funding because we implemented generalized approaches in the metadata computation and processing algorithm structure one could easily transfer extend the framework reported here to a different region or larger scale the generalized approaches imply that the same data sources procedures and algorithms can be used for a different region or scale application however the algorithms may require a different set of adaptable parameter values that vary with different geographic locations e g radar basin spatial extent and rain rate estimator for a different climatic regime the main functionalities of ifc cloud nexrad are metadata search and visualization as well as rainfall product generation researchers can search for an interesting rainfall event through metadata display and request processed rainfall products for the selected event and area in this paper we describe the main features of the ifc cloud nexrad system and its detailed architecture metadata processing algorithms and graphic user interface 2 features of ifc cloud nexrad both the real time and full historic nexrad level ii data over the entire united states have recently became available through noaa s big data partnership with cloud service providers such as amazon web services aws google and microsoft ansari et al 2017 this allowed us to immediately and repeatedly retrieve level ii data from cloud storage whereas we previously had to wait for several hours to days depending on the amount of data we requested after submitting a data request through the ncei furthermore the link provided by ncei that allowed us to download the requested data would be available only within next few days real time level ii data acquisition required more vigorous efforts such as specific software e g unidata local data manager installation and configuration as well as data feed management e g kelleher et al 2007 the instant and unlimited access to level ii data on the cloud helped the ifc cloud nexrad system reduce substantial challenges attributable to ncei s antiquated methods of data storage and management e g tape based archive systems the ifc cloud nexrad system does not have limitations on the time and space domain because it does not have to retain the level ii data in the system storage this also removed the need for procedures of past and real time data ingestion and data format conversion to reduce data volume in the system storage although we limit the ifc cloud nexrad system domain to iowa because it is a pilot system we used a national mosaic of radar reflectivity maps for the computation of rainfall metadata the use of the national map enhances flexibility for future spatial extension and computational efficiency of the system in general rainfall metadata calculation requires processed radar products maps the generation of this intermediate product takes significant time particularly for large basins that require additional data processing to merge information from multiple radars after the individual radar data pre processing the use of a national coverage map for metadata computation removes or simplifies many intermediate procedures and saves significant computer resources and computation time we note that the rainfall metadata in ifc cloud nexrad covers the period from 1995 to the present which is a significant enhancement compared to the period encompassed by hydro nexrad e g 2002 to 2008 on the other hand this created an issue in use of the ready made product e g the national coverage map for metadata calculation the effects of which we will discuss in the next section estimation algorithms for rainfall product generation maintain a modular structure for the flexible combination and upgrade of algorithm modules besides major elements e g rain rate estimator the modules include a few selective schemes to correct negative effects e g non meteorological radar echoes arising from ground clutter anomalous radar beam propagation and wind turbine effects from radar error sources villarini and krajewski 2010 and merging procedures of multiple radar data those schemes and procedures include radar data quality control removal of the non precipitation echoes using polarimetric measurements seo et al 2015 synchronization of different observation times among involved radars and spatial merging of individual radar domains for a larger basin coverage we also appended a procedure to correct temporal radar sampling error seo and krajewski 2015 that often manifests as discontinuous banded rain patterns in an accumulation map for a specific period while the sequence and each processing procedure of estimation algorithms are generalized i e not site specific likely change of some adaptable parameters e g coefficients of rain rate estimator may reduce concerns regarding potential uncertainties for different regions or weather regimes a portable database called sqlite e g owens 2003 stores some of the required information used for estimation input parameters e g radar site latitude longitude radar id that covers a specific basin and latitude longitude boundary of a specific basin this stand alone database can avoid dependency on the main database server e g for metadata and thus the subsystems of metadata and data processing for product generation are fully independent we describe the details of system architecture in the next section 3 system architecture 3 1 system overview we limit the spatial domain of this pilot study to iowa where the ifc s abundant hydrologic resources see krajewski et al 2017 are readily accessible fig 1 shows the system domain covered by the seven wsr 88d radars and numerous basins within the domain in table 1 we present detailed information on the radars the number of usgs hydrologic unit code huc seaber et al 1987 basins 6 to 10 digit and the names of representative basins in the domain most of the basins shown in fig 1 belong to the two major river basins in the u s midwest i e the mississippi and the missouri rivers the system covers the period from 1995 to the present we ingest new rainfall metadata once a day and there is almost no latency with the latest level ii data reception in cloud storage the system consists of the three main elements as illustrated in fig 2 1 rainfall metadata descriptive statistics e g rainfall intensity and areal coverage for a domain of radar or basin and database 2 rainfall estimation algorithms and 3 graphical user interface gui we calculate the rainfall metadata using the national composite map and store them in a relational database e g codd 1970 the metadata computation modules are written in c and acquire necessary information for the computation from text format lookup tables e g spatial indices for basins python scripts execute the modules when new national maps are available and ingest the calculated metadata into the database through the gui researchers can query and retrieve specific metadata to meet their needs e g radar or basin of interest date and time and metadata item which enables them to find a significant rainfall event for the area of their interest the gui color codes the retrieved metadata and displays them in a calendar view after navigating daily and hourly metadata one can place an order for rainfall products through the gui once the data request is submitted specifying the spatial radar basin and time domains desired the processing manager e g python scripts starts acquiring the level ii data for the requested period from the cloud upon receiving the data the manager runs a series of processing modules e g rainfall estimation algorithms organizes input and output files generates the final products and notifies the researcher by email that the requested products are ready to download 3 2 rainfall metadata and database we calculated the rainfall metadata using the national composite map of radar reflectivity both the unidata and iowa environmental mesonet iem combine the level iii base reflectivity products see klazura and imy 1993 from all available wsr 88d radars and offer the national coverage map every 5 min while the unidata combines the base reflectivity product referred to as n0r in real time only there is no archive both the real time and past maps are available in the iem archive with an additional option for higher resolution product referred to as n0q we distinguish between n0q and n0r using the product spatial resolution 0 5 vs 1 km and quantized reflectivity intervals 0 5 vs 5 dbz associated with the provided image format 8 vs 4 bit png because of the higher resolution and precision we decided to use the iem n0q national map for our real time metadata computation we used the iem archived maps for metadata computation from the past period the archives of n0r and n0q cover different periods january 1995 november 2010 and november 2010 present respectively although the use of national coverage map provides flexibility and computational efficiency we discovered an issue related to the accuracy of calculated metadata fig 3 shows certain echo patterns that were not caused by rain we confirmed that this error arose in part from missing level ii data for which the level iii base products had been erroneously created this demonstrates that the ready made product might yield metadata errors in some cases we note that the error shown in fig 3 is not a common one and occurs only rarely e g less than 0 1 in the entire data set the ifc cloud nexrad system provides two types of rainfall metadata i e radar and basin centric fig 1 shows the seven radars and 1363 huc basins the numbers of huc basins for different digits are presented in table 1 for which we calculate the rainfall metadata in this study we initially calculate both types of metadata based on the 5 min temporal scale and then integrate them over the daily scale we calculate basin centric metadata for 10 digit huc basins and integrate them for larger basins e g 6 and 8 digit hucs because of the nested feature among different digit huc basins the radar centric metadata calculated at the 5 min scale include maximum reflectivity dbz and rainfall coverage within a corresponding radar domain defined as the 230 km range around the radar site several threshold values e g 20 30 40 and 50 dbz used in the rainfall coverage calculation can provide information on rainfall intensity or severity e g light or heavy rain daily scale integration for maximum reflectivity is relatively simple and straightforward we choose the maximum of all 5 min maximum reflectivity values over a daily time window for rainfall coverage integration we use a simple average of the 5 min rainfall coverage values within a day which does not represent actual daily rainfall spatial coverage based on our multi year monitoring experience e g http ifis iowafloodcenter org ifis demir and krajewski 2013 the spatial coverage increases as temporal scale grows longer and daily rainfall coverage in particular should approach to 100 on most rainy days therefore our definition of daily spatial coverage delivers a reasonable sense of the information rather than reporting the actual daily coverage about 100 for almost rainy days additional radar centric metadata at the daily scale include temporal fraction of rain events and data missing rate we use the 5 min rainfall coverage values as a binary indicator and integrate them over a specific day for the calculation of temporal fraction we define the missing rate as a proportion of the number of 5 min observations acquired to the expected number of 5 min observations within a daily time window e g 288 we present the detailed parameters e g thresholds and formulations of the radar centric metadata in table 2 the basin centric metadata calculated at the 5 min scale include maximum rainfall rate and location mean areal rainfall rate conditional mean areal rainfall rate and basin rainfall coverage in this metadata calculation we estimate rainfall rate mm h using the nexrad relation of radar reflectivity rain rate e g fulton et al 1998 z 300r 1 4 the term conditional indicates that the areal average is estimated solely for the rainy area the daily integration procedures for maximum rainfall rate and basin rainfall coverage are the same as those for the radar centric ones spatial integration from 10 to 6 and 8 digit hucs of metadata regarding mean areal rainfall rate requires an additional metadata component e g the number of rainy pixels within 10 digit huc basins because mean areal rainfall between nested basins at different scale cannot be linearly averaged or integrated in space table 3 shows detailed formulation on the metadata computation and their spatial integration currently the total capacity of the rainfall metadata stored in the ifc cloud nexrad database is about 223 gb and the database slowly grows as we ingest new metadata every day approximately 29 mb day for the entire domain shown in fig 1 in table 4 we provide the current database volume size for all radar and basin centric metadata the ifc cloud nexrad database also contains general information on the involved radars and basins 1 radar radar id and site location city state and latitude longitude and 2 basin huc id and the name of basin the map interface displays this information when a user selects one of the radars or basins 3 3 rainfall estimation algorithms rainfall product generation upon data request involves a variety of radar data processing algorithms including data quality control rain rate estimation rainfall accumulation and conversion of spherical polar to geographic coordinates these modular algorithms are written in c and the processing manager python scripts connects and sequentially executes them by arranging the input and output files of each algorithm module a product request for a large basin covered by multiple radars requires exclusive merging procedures that synchronize different observation times and combine different spatial coverages among the involved radars for detailed procedures see fig 4 because the data processing sequences for all individual radars are identical before feeding processed individual data to the merging modules we perform parallel like data processing using a python package called multi processing to reduce data processing time we did not apply this method to the hydro nexrad data processing stream in which all algorithm modules processed data in a sequential order only the parallel like processing can also split data for an entire period requested into several subperiods based on the random selection of files feeding each split group of data into the same sequential processing stream using multiple cores processors we note that rainfall accumulation cannot take advantage of parallel processing because the temporal order of input data is a critical factor of the accumulation procedure seo et al 2011 documents most individual rainfall estimation algorithm modules implemented in ifc cloud nexrad while hydro nexrad offered users selective options for algorithm combinations depending on product quality and processing time we fix and employ a unique algorithm combination for simplicity in ifc cloud nexrad the unique combination is identical to the one used for the ifc qpe see krajewski et al 2017 the ifc algorithms encompass data processing elements associated with data quality control using polarimetric features seo et al 2015 hybrid scan e g cappi product generation radar data merging rain rate conversion using the nexrad z r relation e g z 300 r1 4 and rainfall accumulation with advection correction seo and krajewski 2015 the advection scheme fills a radar temporal scanning span e g about 5 min with a 1 min interval by using a calculated storm velocity vector between two consecutive rain rate maps and corrects rainfall accumulation errors closely associated with the radar scanning frequency storm velocity and rainfall product spatial resolution this correction is more effective for fast moving rain systems that often generate spatially discontinuous rainfall accumulation patterns fig 4 shows the sequential processing procedures of the ifc cloud nexrad rainfall estimation once a product request has arrived the processing manager starts to download the level ii data files from the cloud and communicates with the sqlite database to prepare the input parameters required to run the processing modules the parameters stored in sqlite include radar site information e g site elevation and radar tower height and radar basin spatial extent the system uses the information on site elevation and tower height when building the cappi product the spatial extent of a specific basin is represented as a rectangular bounding box e g upper left and lower right corners which the processing manager uses for multiple radar data merging and coordinate conversion e g polar to geographic coordinates 3 4 graphic user interface we developed a web based gui that interacts with researchers and fulfils their requests which can include rainfall metadata display rainfall product generation and more researchers can define space and time domains and retrieve metadata that are graphically presented through the gui as shown in fig 5 the gui involves three elements 1 map interface 2 rainfall metadata browser and 3 rainfall product request the map interface integrates google maps api to support flexible navigation and visual selection among the domains of involved radars and huc basins because of the nested and hierarchical structure basin selection requires the user to confirm how he or she wants to view the metadata users can choose the initially selected scale e g 6 digit huc basin or move to smaller scale basins e g 8 digit hucs for convenience the interface also provides functionality that automatically adjusts a zoom level for the selected scale basin metadata browsing begins with specifying the domain radar basin of interest year and metadata listed in the browser some metadata elements e g rainfall coverage require the user to indicate the parameters e g threshold values to be used for metadata calculation daily metadata are color coded and displayed in a calendar view while the 5 min metadata time series for a selected day is also available in the panel below the calendar as seen in fig 5 the calendar interface allows a user to examine entire days in a selected year and pick a specific day with rainfall activity to request a rainfall product a researcher needs to specify the period e g start and end dates and product type e g rain rate and rainfall accumulation the information on the spatial domain is automatically transferred from the rainfall metadata browser once the researcher pushes the submit request button after inserting his her email address the product request specification is transmitted to the database as an active job the rainfall processing manager regularly checks the job list in the database and activates processing algorithms for rainfall product generation for any active requests the manager sends an email notification when the requested products are ready with a link to download the products the product delivery time may vary with the domain size and period length the average time for data processing and product delivery is approximately 5 h including data downloading time of about 20 min for a request of single radar products for a rainy month e g more than 7000 level ii files we note that the processing time for a period after june 2016 takes about three additional hours because of the level ii data format change this format change requires an additional file compression format conversion process that is compatible with the processing modules we plan to develop and add a pre processing module that can directly read the format to accelerate the data processing 4 summary and discussion based on the recent advent of nexrad level ii data archive in cloud storage ansari et al 2017 we built ifc cloud nexrad with major features of rainfall metadata and rainfall product generation because the level ii data in the cloud are immediately accessible for any past time period or real time and all wsr 88d locations ifc cloud nexrad can reduce significant challenges such as waiting time for data availability and the limited system domain space and time the ifc cloud nexrad system provides radar and basin centric rainfall metadata descriptive statistics over the iowa domain we used the national composite reflectivity map and calculated the metadata for the 5 min and daily scale from 1995 to the present the relational database manages radar basin information as well as the calculated metadata and communicates with the gui to efficiently retrieve required data information for visualization researchers can also request rainfall products for their search domain and period through the gui based on the specification of the request the system executes processing algorithms and delivers the final products via an internet link the modular structure and generalized processing procedures of the ifc cloud nexrad system enable flexible upgrade of rainfall estimation algorithms the successful applications of the hydro nexrad algorithms to a variety of different regions in the united states e g lin et al 2010 seo and krajewski 2011 yang et al 2014 verify that the capability of our estimation algorithms is not limited to iowa for more reliable rainfall estimation we will soon implement a state of the art radar rainfall estimation algorithm e g ryzhkov et al 2014 that promotes scientific enhancements using weather radar s polarimetric capabilities e g istok et al 2009 we note that the quantitative estimation of snow is still a challenging issue and thus the estimation algorithm in the system may not capture well some snow cases other observational e g satellites or model based e g numerical weather predictions resources can also be incorporated into the algorithm stream to improve the accuracy of rainfall estimates once those resources are made available online while we limit the current system s spatial domain to iowa the framework developed here both the computational configuration and scientific algorithms can be readily transferrable to some research organizations for regional or national scale applications perhaps the consortium of universities for the advancement of hydrologic science inc cuahsi https www cuahsi org should be a candidate to manage a large scale application such as this providing the water science community with a wide range of online services and support the additional resources required for the domain extension are the spatial coverage information and lookup tables e g for added radars and basins for metadata computation and algorithm processing software availability ifc cloud nexrad is available at http s iihr52 iihr uiowa edu ifc cloud nexrad we initially built the system on aws e g ec2 instances we fully tested the network communications and relevant computations as well as gui functionalities on the cloud environment upon the completion of the unidata supported project we moved the system to our local server at the university of iowa currently rainfall product request is available from 2008 at which time the wsr 88d sampling resolution was upgraded e g super resolution seo and krajewski 2010 we note that the maximum length of period for product request is limited to 30 days to protect the system from unwieldy bulk orders acknowledgments this work was supported by the iowa flood center at the university of iowa and the unidata ucar community equipment awards program under grant no z16 21958 the authors are grateful to radoslaw goska who helped design and build the database and gui for the ifc cloud nexrad system appendix a supplementary data the following is the supplementary data to this article data profile data profile appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2019 03 008 
