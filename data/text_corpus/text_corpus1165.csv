index,text
5825,stochastic simulation has been employed for producing long term records and assessing the impact of climate change on hydrological and climatological variables in the future however traditional stochastic simulation of hydroclimatological variables often underestimates the variability and correlation structure of larger timescale due to the preservation of long term memory however the long short term memory lstm model one type of recurrent neural network rnn employed in different fields exhibits a remarkable long term memory characteristic owing to the recursive hidden and cell states the current study therefore applied the lstm model to the stochastic simulation of hydroclimatological variables to examine how good the lstm model can preserve the long term memory and overcome the drawbacks of conventional time series models the simulation involved a trigonometric function and the rössler system as well as real case studies for hydrological and climatological variables results showed that the lstm model reproduced the variability and correlation structure of the larger timescale as well as the key statistics of the original time domain better than the traditional models the hidden and cell states of the lstm containing the long memory and oscillation structure following the observations allows better performance compared to the other tested conventional models this better representation of the long term variability can be critical in water manager since future water resources planning and management is highly related with this long term variability thus it is concluded that the lstm model can be a potential alternative for the stochastic simulation of hydroclimatological variables also note that another long term memory model such as gated recurrent unit can be also applicable keywords deep learning stochastic simulation hydroclimate long term memory streamflow climate index 1 introduction long term historical data on hydroclimatological variables are needed in many environmental and hydrological studies stochastic simulation has been frequently employed to assess water resources systems and its influences from climatic variables using time series models including parametric models such as autoregressive ar model lee 2016 or nonparametric models lall and sharma 1996 prairie et al 2005 lee et al 2010 recently it has been found that long term climatic variability significantly represented with climate indices such as pacific decadal oscillation el nino and northern atlantic oscillation influences the hydrological variables therefore it is important to reproduce such long term evolution in hydroclimate variables however one of the drawbacks of conventional time series models is their deficiency to reproduce the variability and correlation structure of the upper timescale for example when monthly streamflow is modeled with a parametric model such as ar model or a nonparametric model the simulated data underestimate the observed variability and lagged correlation at the annual scale chen et al 2010a in order to overcome this drawback many enhanced models have been proposed such as a markov switching model k nearest neighbor with the genetic algorithm and hidden markov chain model sveinsson et al 2005 salas and lee 2010 however those models entail complex procedures due to some constraints and requires many assumptions to be implemented for example linearity normality and stationary additionally those models can be applied to a specific type of time series data due to limitations of each model meanwhile machine learning tools such as neural networks nns have been used to simulate hydroclimatological variables tongal and booij 2018 to memorize past information that is important in hydroclimatology advanced recurrent nns rnns have been developed for hydrometeorological applications besaw et al 2010 however as the time interval increases the rnn model is not able to learn to connect information further in the past to remedy this loss of memory hochreiter and schmidhuber 1997 proposed a long short term memory lstm model and showed that this model is capable of performing complex and long time lag tasks that rnns are not capable of the rnn model has the vanishing gradient problem in training using the data sets with many lags gradients of weights for the data at long lags become very small in the rnn since lstm can deal with the vanishing gradient problem adopting a new unit concept composed of a cell input gate an output gate and a forget gate the lstm can give prediction conserving long term memory unlike the rnn model in the stochastic simulation a long term record of data is generated by the stochastic time series models it is critical to reproduce the statistical characteristics of time series data instead of predicting specific values modeling a long term record without the loss of original statistical characteristics is very difficult due to the dissipation of information by lead time the statistical characteristics of hydrological time series data are governed by a complex interaction of atmospheric and hydrologic phenomena the conventional neural networks i e multi perceptron layer model may not be able to emulate complexity in the data a deeper network may be a good remedy for a time series model of hydrological data because deep neural networks have complex and deep hidden layers they can account more complex interaction as compared to the conventional machine learning algorithms shen 2018 shen et al 2018 in the current study the lstm is tested for a time series model in a stochastic simulation as a deep neural network since the publication of the hochreiter and schmidhuber 1997 paper many researchers have reported excellent results in a wide range of sequential applications wang et al 2017 recently rnns especially lstm hochreiter and schmidhuber 1997 and the gated recurrent unit gru cho et al 2014 are used as strong sequences in various tasks including machine translation language modeling voice text conversion health informatics and time series vlachas et al 2018 they have adopted used for core algorithm in modeling hydro and hydroclimatological variables and some case studies have been reported wang et al 2018 yuan et al 2018 bowes et al 2019 yuan et al 2018 employed the lstm model to predict monthly runoff and found that it was more accurate than other models wang et al 2018 showed that the lstm model performed better than nn models for predicting water quality bowes et al 2019 compared two types of neural networks rnn and lstm for their ability to predict groundwater table response to storm events in a coastal environment and the lstm has better performance than rnn many machine learning algorithms such as support vector regression extreme learning machine random forest and artificial neuro fuzzy inference system were used for univariate time series modeling of hydro meteorological variables using machine learning methods particularly to forecasting hipni 2013 huang et al 2014 karran et al 2013 li et al 2016 yaseen 2016 their performance and suitability differed from the characteristics of data sets papacharalampous et al 2018 and papacharalampous et al 2019 compared performances of machine learning algorithms and stochastic time series methods for forecasting univariate hydrological variables via extensive simulation and case studies they claimed that the machine learning and stochastic methods provided equally useful forecasts since the machine learning and stochastic methods inherently cannot capture complex the characteristics of the time series data such as multiple oscillations and long term correlation their performances for forecast would be similar subsequently their performances may be similar in a stochastic simulation of hydroclimatological variables unlike these methods the lstm has the potential to reproduce the long term behavior in hydroclimatological variables due to its special structure however little attention has been given to the lstm model for stochastic simulation in hydroclimatological studies therefore this study applied the lstm to stochastic simulations and examined whether long term behavior of observed hydroclimatological variables can be reproduced note that the current study is focused on building a stochastic simulation model with lstm rather than the traditional prediction since its object is different from the traditional prediction the performance testing is focused on whether the model is to reproduce the statistical characteristics of observational data especially long term evolution the proposed simulation models can contribute to enhancing our capacity to reproduce characteristics of hydroclimatological data this will lead to improvements in the long term modeling of rainfall runoff and reservoir operation the paper is organized as follows in the next section a mathematical background of traditional stochastic simulation models and neural networks is reviewed section 3 discusses the lstm model and its simulation strategy the methodology for application and results is presented in section 4 two real case studies that apply the lstm model to simulate observations of a hydrological variable and a climatological variable are discussed in sections 5 and 6 respectively section 7 presents a discussion followed by the summary and conclusions in section 8 2 mathematical background 2 1 stochastic time series models 2 1 1 fundamentals of time series models stochastic models attempt to reproduce statistical characteristics of observed data and can be expressed as 1 y t f x t ε t where x t x t 1 x t 2 x t d is a set consisting of d number of predictor or explanatory variables at time t and εt is the independent white noise term regarded as εt n 0 σε 2 while yt is the predictand at time t n and σε 2 are normal distribution and variance of independent white noise term respectively autoregressive time series models employ the lagged terms of the predictand variables for predictors as x t y t 1 y t 2 y t d the traditional autoregressive ar model is a linear combination of these predictors while a nonlinear ar nar model develops a nonlinear combination of the predictors the nonlinear relation can be developed with neural networks or deep learning algorithms such as rnn convolutional neural network and lstm stochastic simulation is done by repeatedly employing eq 1 with random noise generated from a normal distribution 2 1 2 autoregressive ar model to model time series of hydrological and hydroclimatological variables many time series models have been proposed and tested salas et al 1980 2006 the autoregressive moving average arma model is commonly used to take into consideration random shocks also known as innovations for time series modeling autoregressive integrated moving average arima model was proposed for modeling data that have linear or non linear trends for taking into account seasonality in data seasonal ar arma and arima models have widely employed in the modeling of hydrological variables for a variable yt the autoregressive model of order p ar p can be described as 2 y t j 1 p φ j y t j ε t where yt is the time dependent series with zero mean and εt is the time independent series as εt n 0 σε 2 the target time series must follow the assumption that there is no long term trend or cyclicity and the time series variable yt is assumed to be normally distributed to meet this assumption of normality and no cycles in a target variable the target time series must be transformed for each season the autocorrelation function or lagged cross correlation ρ k of the ar model is geometrically bounded as 3 ρ k c r k k 1 2 where c 0 and 0 r 1 while c is a constant value the infinite sum k ρ k is defined as short term memory and the case k ρ k is long term memory brockwell and davis 1988 generally the ar model has a short memory structure there are many transformation methods such as power log standardization and copula normalization to meet the normality condition of the ar model lee and salas 2011 proposed a copula based transformation a variable can be transformed into standard normal distribution as 4 y t f φ 1 f y θ where f y is the cumulative distribution fitted to the original variable y t and θ is the distribution parameter in general gamma distribution has been employed for hydroclimatic variables f φ 1 is the inverse of standard normal distribution also seasonal cycles can be abstracted by applying this transformation in each season when observations follow a normal distribution but with seasonal mean and standard deviation only standardization can be applied to remove the seasonality by 5 y t y t μ y σ y where y t and y t are the original and transformed explanatory variables respectively μ y and σ y are mean and standard deviation of the original variable y t 2 2 neural network models 2 2 1 feed forward neural network ffnn a neural network is a computing system inspired by the human nervous system it is a network of adaptive and highly interconnected processing units called neurons a simple feed forward neural network ffnn is composed of three layers as input hidden and output with many neurons connected with nodes as shown in fig 1 a this network also is called a multi layer perceptron model in this network the source nodes of the input layer are projected onto computational nodes in the hidden layer through the connection weights and then to neurons of the output layer the learning process of the ffnn is to adjust weights to minimize the difference between the values predicted by the model and real data consider an output vector with two dimensions y y 1 y 2 y n t with y t y t 1 y t 2 y t no and an input vector or called feature matrix x x 1 x 2 x n t in which x t x t 1 x t 2 x t d is a vector with d features note that 1 no is the number of output variables and n is the record length and 2 in the current study the number of output variables is one no 1 y t y t and only the same variable as the output but time lag is employed i e x t y t 1 y t 2 y t d the architecture of the network employed in the current study is illustrated in fig 1 a the output prediction yt can be expressed as 6 y t g ho w ho h t b ho 7 h t g ih w ih x t b ih where gho and gih are activation functions from hidden to output and from input to hidden layers respectively a few activation functions are available such as tanh and sigmoid also w ho and w ih denote the weight parameter matrices and bho and bih denote their bias and ht contains the nh number of hidden units these parameters can be estimated with a certain predefined criterion such as minimizing a mean square error between the predicted value and observed output one of the popular learning methods for estimating parameters is backpropagation bp which searches for a minimum of error function by applying the gradient descent method and iteratively tunes the weights based on the error derivative with respect to each weight 2 2 2 recurrent neural network rnn recurrent neural networks rnns are one type of extended neural networks that exhibit not just the current data state but also the previous hidden state rnns contain a feedback loop so that the current hidden state serves as an additional input to estimate the subsequent hidden state shown in fig 1 b for rnns the hidden state vector can be modified from the traditional nn in eq 7 as 8 h t g ih w ih x t w h h t 1 b ih where w h is the weight parameter matrix for the previous hidden state to estimate the parameter set composed of weights and biases backpropagation through time which is similar to bp in traditional nns is employed recurrent networks use their understanding of past events to process the input vector however rnns have a strong weakness on called gradient vanishing problem such that the gradients back propagating through the hidden units shrink exponentially when there is a long range 3 long short term memory lstm and its stochastic simulation 3 1 lstm lstm was proposed by hochreiter and schmidhuber 1997 to solve the vanishing problem in rnns and has since been improved stollenga et al 2015 greff et al 2017 they introduced multiplicative gate units that learn to open and close access to the information flow illustrated in fig 1 c this deep learning algorithm has been employed in various fields due to its exceptional long term memory characteristic the lstm model contains three gates as input output and forget gates denoted as it ot and ft respectively controlling the amount of information flow within the cell which is responsible for memorizing values over a number of time intervals as shown in fig 1 c these three gates compute an activation of a weighted sum with the sigmoid function between zero and one these gates are described as 9 f t σ w fx x t w fh h t 1 b f 10 i t σ w ix x t w ih h t 1 b i 11 o t σ w ox x t w oh h t 1 b o where σ z indicates the sigmoid function of σ z 1 1 exp z and w and b are the weight parameter matrix and bias the candidate cell state c t is defined with the tanh activation function ranged as 1 1 as 12 c t tan h w c x x t w c h h t 1 b c the cell state is defined as a combination of 1 how much the previous cell state information has flown in and 2 how much the current update information is in this can be denoted as 13 c t f t c t 1 i t c t where indicates the element multiplication refer to fig 1 c note that the question of how much can be answered with the gates here forget gate f t and input gate i t in eqs 9 and 10 respectively for example if the forget gate is zero then the previous cell information is lost and only the current new cell information is considered i e c t i t c t finally the hidden state is defined by how much the current cell state information outflows to the hidden state 14 h t o t tan h c t the scheme illustrated in fig 1 c presents that the output yt is made of a linear combination of the hidden state h t as shown in eq 6 the hidden state is the cell state filtered with the tanh function whose consideration is controlled with the output gate o t the cell state is a linear combination of the current updated cell information c t and the inflowed previous cell state ct this deep learning algorithm can sequentially deliver the long term memory through the cell state as stated earlier if the forget gate is zero then the previous cell information is lost and only the current new cell information is considered i e c t i t c t conversely the previous cell state information is completely delivered as c t c t 1 when i t 0 and ft 1 note that those three gates are defined with the previous hidden state h t 1 and the current input x t here yt 1 the reason that this algorithm has the long term memory ability is due to particular information delivery system with the cell state and three control gates in addition to the hidden state an example of the lstm is given in supplementary a to further illustrate the algorithm 3 2 stochastic simulation with machine learning algorithms as mentioned in eq 1 the stochastic simulation with machine learning algorithms including rnn and lstm can be done by setting the explanatory variables as the same variables but with time lagged as x t y t 1 y t 2 y t d the functional relation i e f x t in eq 1 between x t and yt is nonlinearly created from the rnn and lstms for consistency of comparison input data sets for all employed models are the preprocessed data sets described in section 2 1 2 note that we only tested x t y t 1 for the lstm model to investigate the model performance because the lstm model with one lagged data leads to comparable performance to those with different lagged data and further tests in the discussion section the modeling procedure seems a forecasting scheme because the model requires to preserve the lagged short and long term correlation structures it is straightforward to simulate with machine learning algorithms after building their architecture and fitting them the random number is generated from a normal distribution for εt in eq 1 and is multiplied with the error standard deviation σ ε i e εt n 0 σ ε 2 the error variance σ ε 2 can be estimated with the difference between the values predicted by machine learning algorithms and observed values the datasets were separated with training and validation periods only i e no testing data since the objective of the current study is to build the stochastic simulation model instead of forecasting not specific values k fold cross validation kfcv was used to find the lstm model structure the performance of the lstm model was checked from how well the key statistics such as mean and standard deviation and short and long term correlations are preserved the model overfitting and underfitting can be judged from its reproduction capability of those statistics the dataset for estimating the error variance must be obtained from the validation data not included in the training data since the real error for unknown input values is with the testing data otherwise the variance of the final simulated data can be underestimated in order to cover the entire dataset in estimating the error variance k fold cross validation type approach can be applied as 15 σ 2 1 n t 1 n ε t 2 1 n t 1 n f val x t y t obs 2 where f val x t is the estimates for the validation dataset t and y t obs is the corresponding observed data zhang and yang 2015 the detailed information for the procedure is presented in the methodology subsection corresponding to the section this modeling process is different from traditional time series model such as ar in that ar model employs its own parameter estimation method such as forward backward employed in the current study burg s method maximum likelihood and method of moments brockwell and davis 2003 4 simulation study for synthetic data sets many hydrological and hydroclimatological variables have multiple oscillatory characteristics such as daily seasonal and long term cycles for evaluation purposes synthetic data sets can reproduce multiple oscillatory characteristics the trigonometric functions and rössler system satisfy this condition furthermore the rössler system can reproduce non linear oscillatory characteristics therefore the trigonometric functions and rössler system are selected to generate synthetic data in testing the performance of lstm for a stochastic simulation model of hydrological and hydroclimatological variables 4 1 trigonometric function in order to test the performance of the lstm stochastic simulation a synthetic dataset was generated from a combination of trigonometric functions as 16 y t c o s a t 0 5 s i n 5 t 0 3 ε t here the error term ε t is a random noise following a standard gaussian distribution the value 0 3 is multiplied so that the error term is not too much dominant in the simulated dataset the two trigonometric functions varied from 1 to 1 and the random noise was between 1 96 0 3 and 1 96 0 3 for 95 percent a realization of eq 16 is presented in fig 2 with a length n 2000 four different values of a in eq 16 were tested as a 1 2 3 and 4 as shown in the middle panel the sine function represents a relatively high frequency with around 360 5 per cycle see the black line in the top panel while a cosine function represents a relatively low frequency with 360 per cycle for a 1 see the blue dash dotted line in the top panel and 90 per cycle for a 4 the two signals realized with a 1 and a 4 see the dotted and dash dotted lines in the bottom panel indicate that the low frequency case a 1 clearly shows the oscillation of the sine function conversely for the high frequency case a 4 the sine signal is somewhat vague because their frequency is similar to each other with the dataset of four different values of a realized the lstm model was fitted the simulation with eq 16 was performed by generating 100 series and only one lagged term was considered for the predictor variable i e x t y t 1 for the lstm model for a simulation model the accurate prediction is not its interest but reproducing the statistical characteristics of observational data further data separation is not needed to confirm since its performance is checked through the reproductivity of the key and long term statistics in observation in our study the ten fold cross validation with all data points was employed only to find the architecture of lstm the number of hidden units nh for lstm was investigated with k fold cross validation and not much difference can be found in the root mean square error rmse after four therefore its hidden units were fixed as four the same number of hidden units was employed for the following case studies basic statistics of the data simulated by the lstm model such as mean standard deviation skewness lag 1 correlation maximum and minimum are presented in fig 3 for the data realized with a 1 the results are displayed using boxplots in which the box shows the interquartile range iqr when the extrema are higher than 1 5iqr the whiskers extend up to 1 5iqr and the excess values are displayed with crosses and are considered outliers otherwise the whiskers extend to the extrema and feature horizontal lines at their ends the horizontal line inside the box depicts the median of the data in addition the value of the statistic corresponding to the historical data is represented by a cross connected line with dots as a result the simulation model well reproduced the basic statistics of the realized data and the fitted lstm model was suitable for stochastic simulation the long range correlation structure was investigated as shown in fig 4 results of the realized data with a 1 are shown in the top panel whose cycle was 360 the lagged correlation of the simulated data was well reproduced only up to 30 lags the observed long term oscillation pattern of the correlation was not preserved for the case of a 2 the simulated data reproduced the correlation up to 60 lags the simulated data for the case of a 4 well reproduced the historical correlation structure while a slight deviation was seen in the case of a 3 these results show that the lstm model has a good potential for reproducing long term patterns of observed data however it also has the limitation that the model can capture it is quite surprising that the lstm architecture with lag 1 term as an explanatory variable i e x t y t 1 can deliver this complex correlation structure even with this limitation the spectral densities shown in fig 5 coincided with the result of lagged correlations the simulated data for the realized data with smaller a values i e a 1 and 2 in eq 16 could not reproduce the lower frequency part of the observed spectral densities while the simulated data with higher values i e a 3 and 4 are well reproduced 4 2 rössler system in this study an example of a non linear dynamic system rössler attractor rössler 1976 which is one of the most famous and chaotic attractors that have been tested in the field of climatology and physics huang et al 1998 smith et al 1999 kijewski correa and kareem 2007 he et al 2016 were selected the attractor is the solution to a system of three non linear ordinary differential equations as x y z y x α y and z β z x δ z where x y z r 3 are dynamical variables defining the phase space with time t and α β δ r 3 are parameters this example was selected for its chaotic behavior with long term persistency rössler 1976 the generated series is presented in fig 6 the x variable of the generated series was selected in the current study including the random error as x t ε t and ε t n 0 0 5σ x the model was fitted to the lstm model with the hidden units of 16 from 10 fold cross validation results not shown the ar model were also fitted to this dataset for comparison the order p of the ar model was checked up to p 100 with akaike information criterion aic the result presented that the order of 53 showed the best with aic however the simulation performance was not reasonable and the order of 5 presents the best performance among the orders tested the result presented is with the order of 5 i e p 5 in addition many different structures in the rnn model including the hidden unit were exhaustively tested with 10 fold cross validation none of the rnn model structures present reliable performance to use and no correlation structure was presented therefore the results of the rnn model were omitted the lagged correlations of the observed and simulated rössler x variable from the fitted ar and fitted lstm models are presented in fig 7 the observed lagged correlation presents the long memory i e slow decay to around lag 40 and oscillation structure the ar model shows the fast decay of the lagged correlation structure meanwhile the lstm model presents strong preservation of the lagged correlation and its oscillation component is reproduced up to nearly lag 100 the spectral density presents that the behavior of the simulated data from the ar model is different from the observation especially the observed spectral density in the left side presenting low frequency is not reproduced properly from the ar model while the lstm better preserves the spectral density this indicates that the lstm have the good capability to reproduce the long term persistency as well as long term oscillation 5 hydrological application for streamflow 5 1 study area the colorado river is one of the important river systems in north america which is an indispensable source of water supply for seven states in the western united states and mexico the colorado river system has been divided into two large systems as the upper colorado river and the lower colorado river where the division is at lees ferry the system has been regulated with a number of within the year and over the year reservoirs the system was analyzed by performing simulation studies using potential streamflow scenarios that may occur in the future monthly streamflow data from colorado river at lees ferry was used because the system was regulated on a monthly basis to assess model performance in addition simulated data must also preserve the interannual variability of the observed data since the streamflow time series of the colorado river at lees ferry shows a significant interannual variability with several wet and dry periods the historical data of the colorado river has been naturalized and extended to 1906 2003 lee and salas 2006 5 2 application three models were compared to the lstm model for the lees ferry monthly streamflow these models were ar p ffnn and rnn with sequences transformed to gamma distribution for each month because monthly streamflow data has a strong seasonality and to meet the normality condition of the time series model to eliminate strong seasonality and normalize the dataset a copula based transformation was applied as in lee and salas 2011 for each month defined as 17 y t f φ 1 f y y t θ where f y is the cumulative distribution function cdf of the original domain variable and f φ 1 is the inverse cdf of the standard normal variable yt is the standard normal variable and gamma distribution was applied for f y since this distribution has been widely fitted to hydroclimatological variables including streamflow lee et al 2010 salas and lee 2010 this normalization procedure was applied in the current study since the distributional characteristics of the employed data is dominant and ruin the other statistical characteristics such as lagged correlation this normalization has been commonly employed in hydrologic time series modeling salas 1993a also twelve times longer record is achieved from this standardization without it a separate lstm model should be applied at each month the standardized streamflow is shown in fig 8 a its time series illustrates that there is a strong long term oscillatory structure especially in the last 300 months note that it has been known that conventional stochastic simulation models for hydrological variables can reproduce short term dependence structure but not long term dependent structure for example monthly simulated data of hydrological variables from stochastic simulation models can preserve key statistics at the monthly scale such as mean standard deviation and lagged correlations however when the monthly simulated data is accumulated into the annual series the annual data cannot reproduce the key statistics of the observed annual data 5 3 results as shown in supplementary a for the example of the lstm model the number of hidden units must be selected different numbers of the hidden unit were tested using the root mean square error rmse result shown in fig 9 indicate that four hidden units provided the smallest rmse for the case with one step ahead prediction of the standardized data see the bottom panel also the original rmse and the long term prediction forecasting up to 30 months ahead were evaluated as shown in the top and middle panel of fig 9 respectively both results showed that two hidden units were the best however the current study simulated the sequence by employing only lag 1 time delayed variable i e xt yt 1 and strong seasonality ruined the rmse in the original domain therefore four hidden units were employed for the lstm model the scatterplot of observed versus the one step ahead prediction is shown in fig 8 c which represents that the lstm model well fitted monthly streamflow data it also tested how the lstm model behaved for the last 10 sequence around 10 years used as the testing period the one step ahead and long term prediction as enlarged in fig 8 b are presented with the dash dotted red line with circle marker and dotted green line with cross markers respectively the prediction results indicate that the lstm model well predicted the one step forward surprisingly the long oscillatory pattern in the observed data was also well presented furthermore the long term prediction showed the excellent performance without losing the memory both predictions allow to anticipate that the long term patterns can be well simulated as well as the short term memory the key statistics of the simulated 100 sequences of monthly streamflow are presented in fig 10 with boxplots as well as observed data with the dash dotted line with cross markers results show that the observed mean and standard deviation of monthly streamflow were well reproduced by the lstm model but skewness was not much preserved note that the distribution applied for transformation was a two parameter gamma distribution as in 17 which cannot specifically take higher moments like skewness into account the lag 1 correlations showed similar ranges of observed lag 1 correlations but overall lag 1 correlations were not reproduced well for an individual month the extrema seemed fairly reproduced even if some bias was seen especially in minima no better or worse performance was observed with the ar 1 model than the lstm model for the key statistics of monthly streamflow see supplementary fig s1 in fig 11 the observed key statistics of annual series were well reproduced by the lstm model including the standard deviation and lag 1 correlation it is quite surprising that the lstm model can reproduce these statistics at the annual scale with only the lag 1 delayed input xt yt 1 in eq 1 it is quite comparable to ar 1 model which uses the same lag 1 delayed input as in 2 shown in fig 12 the observed standard deviation and lag 1 correlation of the annual data were underestimated by the data simulated from the ar 1 model a quite complex model was required to preserve those annual statistics the ar 5 model selected as the best model was tested according to the akaike information criterion aic shown in supplementary fig s2 and ar 12 model the annual standard deviation was still significantly underestimated and its correlation was overestimated in both cases see supplementary figs s2 and s3 note that the lag 1 correlation of monthly data was highly underestimated by the ar 5 and ar 12 models see supplementary fig s7 this is because modeling the long term correlation structure weakens the preservation of short term memory conversely the long term memory was preserved by the lstm model without any weakening of the short memory structure furthermore ffnn and rnn models see supplementary figs s5 and s6 also are tested results showed that those models could not reproduce the key statistics of observed monthly streamflow at all multiple model variations were for ffnn and rnn were tried without any success it was concluded that the ffnn and rnn models did not seem to have the stochastic simulation capability for the considered data sets for water resources managers reproduction of drought related statistics is critical for stochastic simulation in fig 13 the consecutive length and sum for deficit and surplus as well as storage capacity were presented with the boxplots for data simulated by the lstm model and the observed data circle the storage capacity was the minimum capacity that a reservoir could deliver a certain demand through the time period salas 1993b the lstm model reasonably reproduced the storage capacity while some biases were observed for other statistics for the ar 1 model shown in fig 14 these statistics were not very well preserved notably significant underestimation was observed in the storage capacity produced by the ar 1 model 6 climatological application for pacific decadal oscillation 6 1 data description and application methodology one of the most representative of climate indices as pacific decadal oscillation pdo a long lived el nino like pattern of pacific climate variability mantua et al 1997 was tested this index has been proven to be teleconnected with many hydrometeorological variables nigam et al 1999 the variability of climate indices has been employed in stochastic simulation of hydrometeorological variables teleconnecting climate indices and variables lee and ouarda 2012 ar p was compared with the lstm model furthermore the shifting mean sm model developed by salas and boes 1980 and improved by sveinsson et al 2003a was also tested its fundamental model description is shown at the supporting information a the sm model was developed to model the hydroclimatic process that shifted abruptly from one stationary state to another the dataset was downloaded from mantuna 2017 and standardized by subtracting mean and dividing by standard deviation as in eq 5 especially since only a little seasonality could be observed in the mean 6 2 results the monthly key statistics were well preserved by the lstm model as shown in fig 15 as mentioned before seasonality was present in mean and standard deviation and was well reproduced the seasonality in skewness and lagged correlation was not reproduced because the model did not take them into account note that the transformation has been made only to mean and standard deviation as in eq 5 to preserve their seasonality but no consideration has been made to skewness and lagged correlation the extrema of simulated data slightly followed the seasonality due to the effect of standardization i e subtracting mean and dividing by the standard deviation the ar 1 and sm models showed a result similar to the lstm model result not shown the other ar 4 presenting the best order with aic and ar 12 models were tested and showed significant underestimation in lag 1 correlation results not shown which was the same as in the colorado river case study the observed annual statistics were fairly preserved by the simulated data from the lstm model except for the underestimation of lag 1 correlation results not shown in the ar 1 model much significant underestimation was observed for the standard deviation and lag 1 correlation see fig s8 the higher order ar 4 and ar 12 models overestimated lag 1 correlation but significantly underestimated the standard deviation at the annual scale results not shown moreover ffnn and rnn models were also tested see supplementary figs s9 and s10 results showed that those models could not reproduce the key statistics of observed pdo series the lagged correlation structures of the tested models as in fig 16 were further investigated the correlation structure of the observed monthly pdo presented with the dotted line with circles seemed to have a short memory pattern but also slightly long term fluctuation around 80 lags the ar 1 and sm models exhibited an exponentially decaying memory loss in the early time lags see the panels a and b of fig 16 respectively conversely the lagged correlation structure shown by the lstm model was similar to that by the observed data as in the panel c of fig 16 the examples of two correlation plots see two blue dash dotted lines with and markers in the panel c showed a long term oscillatory pattern similar to the observed data the other ar 4 and ar 12 models showed the fluctuation of the correlations even with positive and negative correlations crossing see supplementary fig s11 the higher order ar p models sacrificed short term memory structure to preserve the long term structure the spectral density salas et al 1980 describing the distribution of power into frequency components was presented for the observed and simulated data with log scale shown in fig 17 the ar 1 and sm models showed a different shape of the spectral densities from those of the lstm model the lstm model showed a straight decrease for the log scale which coincided with the observation the other two models showed a decreasing shape of second or third order polynomial functions so that in some parts of ranges the observed spectral density was outside of the 95 percentile range of the 100 simulated series hurst coefficient h hurst 1951 has been interpreted as a long memory model if h 1 2 and vice versa this coefficient was estimated for each model as shown in fig s12 the observed data showed that h was about 0 85 circle implying a high long memory structure while the ar 1 and sm models significantly underestimated the coefficient the lstm model showed better but not perfect preservation the other ar model showed a result to similar to that of the lstm model result not shown 7 discussion the lstm has been developed to store information over long extended time intervals this up to date deep learning model was tested and compared with the conventional time series models to determine whether the lstm model could solve the long term memory problems in stochastic simulation of hydro climatological variables results indicated that the lstm model not only reproduced the variance and lagged correlation structure of the larger time scale but also preserved at the current time scale this is due to the ability to memorize long term patterns of observed data sets as the lstm model was originally devised hochreiter and schmidhuber 1997 discussed that the lstm could learn to bridge time intervals in excess of exceptionally long time steps without loss of short time lag capabilities the current study shows that the model can have a good potential for simulating hydroclimatological variables by reproducing long term memory i e annual as well as preserving key statistics of the original timescale i e monthly surprisingly all these modeling features of the lstm are acquired with only lag 1 input i e x t y t 1 in eq 1 one of the remaining questions is whether it is possible to improve the model performance with higher time lagged inputs x t y t 1 y t d where d is the considered time lags further tests were made by increasing the number of time lagged inputs for monthly pdo data results showed that better preservation could be made with higher time lagged inputs as shown in fig s13 for example the annual lag 1 correlation can be better preserved with higher time lagged inputs such as d 4 for the lstm model while all the other useful features still stood e g reproduction of observed key statistics of monthly and annual data however the hurst coefficient cannot be improved even with higher time lagged inputs as much as k 12 results not shown from the lstm results the authors wondered which of the model feature made the lstm so special subsequently the structure of the lstm was further investigated by looking into the inside of the model the hidden state ht in eq 14 and the cell state ct in eq 13 were analyzed in detail the time series of both states see supplementary fig s14 showed long term persistency the third and fourth hidden states were highly related to each other the same for the cell state this is shown clearly with the scatterplot in supplementary figs s12 and s13 a significant relation was observed between four hidden states and cell states rather a linear relationship between each cell state was observed see supplementary fig s16 while a nonlinear relation was seen between the hidden states see supplementary fig s15 in fig 18 the relations between the simulated pdo and the hidden states or cell states are presented with four cell and hidden components it revealed that the first top left panel and second top right panel cell states has negative relation with the simulated pdo while the third bottom left panel and fourth bottom right panel cell states has the negative relation this relation is similar in the hidden states with the conditional relations of the first third and fourth hidden states this conditional relation is that the hidden state has around 1 when the simulated output yt is positive physically the highly positive value of the pdo is likely to be in el nino phase corrêa et al 2017 when the simulated pdo is negative the third and fourth hidden states present nearly 1 note that the la nina phase is more intense when the pdo is strongly negative this seems to be a very critical structure to have a long term persistency in the simulated data the hidden state with near 1 value allows constantly staying in one phase with even small autocorrelation of the simulated pdo since the hidden state recursively affects itself and the simulated pdo the time series in fig s14 illustrates that the long term negative phase of the simulated pdo bottom panel between the year 1960 1990 is the effect of the third and fourth hidden states ht 3 and ht 4 in this period the hidden states remain around 1 the highly positive value around the year 1950 of the simulated pdo i e el nino is also related with ht 1 indicating the long term persistency of near 1 this particular structure of the 1 value in the hidden states provide the ability of the long term persistency the lagged cross correlations of the hidden and cell states in supplementary fig s17 showed a long memory structure until 15 20 time lags the longer persistent structure was shown by the lstm model with multiple lagged input as x t y t 1 y t 12 as compared with the original lstm input x t y t 1 as shown in supplementary fig s18 the memory was delayed until 20 30 as well as presented an oscillation pattern in the second hidden and cell states this persistent structure allowed the lstm model to have an exceptional capability to reproduce the long memory structure without the loss of short term memory the long term autocorrelation of the observed pdo is compared with the ones of the cell and hidden states as well as the simulated data shown in fig 19 the short term autocorrelation up to lag 20 is decreased quickly for the observed pdo then the long term oscillation is followed between 80 and 180 this short term autocorrelation behavior and a long term oscillation of the observed pdo is well reproduced in the simulated pdo dashed line with cross marker all the cell and hidden states contain the short and long term memory information the crosscorrelation between the simulated pdo and hidden or cell states presented in fig 20 shows that two hidden and cell states are negatively correlated and the other two are positively correlated with exponential decrease up to lag 20 the crosswavelets between the simulated pdo and the hidden and cell states in figs s16 and s17 are shown the power between 32 and 64 periods are dominant for both of the first and second hidden and cell states see the top panels of figs s19 and s20 while the significant power above 128 is shown in the third and fourth hidden state as well as the cell states see the bottom panels of figs s19 and s20 in this study the long term autocorrelation structure of the hidden and cell states was further investigated up to 500 lags in fig s21 the long term autocorrelations of the hidden and cell states are similar to each other the critical point is that the first hidden and cell states oscillates within 60 lags where this result coincides with the result of the crosswavelets above meanwhile the third and fourth states shows slow decrease up to lag 50 indicating the long term persistency the combined structure of the long term oscillation and slow decrease of lagged correlation in the hidden and cell states allows preserving long term variability and correlation of the pdo furthermore the scatterplots between the hidden state components as well as the cell state components in supplementary figs s22 and s23 illustrated a much weakening relationship compared to the lstm model with one lagged input as shown in figs s15 and s16 this was well stated by the correlation values see supplementary table s1 the weakened relationship between hidden states and cell states can be induced by more substantial lagged inputs that bring in more information this weak relationship is because each hidden and cell component requires more independent information to be passed on to each other also the histogram shows the hidden states are highly skewed see supplementary figs s15 and s22 while the cell states are not much skewed this skewness might be induced from the fact that a particular hidden state among the four components might prefer a high or low state consistently 8 summary and conclusions the long short term memory lstm model one of the most modern deep learning algorithms has been proposed and developed in the neural computations to memorize long time interval information the lstm model was tested for this useful feature in the current study on whether or not the long term memory structure can be preserved without losing the short term memory structure of hydroclimatological variables simulations with a trigonometric function and the rössler system as well as two case studies for hydroclimatological variables as the colorado river streamflow and the pdo index were performed the results indicated that the lstm model had an excellent ability to reproduce key statistics from the upper time domain e g yearly for streamflow case and the original time domain e g monthly for streamflow case while the traditional ar models and the other tested models such as ffnn rnn and sm did preserve the variance and correlation at the larger time scale i e annual it has been shown that the lagged inputs further improved to reproduce the statistics by the lstm model at the larger time scale it is concluded that the lstm model has a strong potential to be an excellent alternative for stochastic simulation for hydroclimatological variables while preserving key statistics at different time scales the lstm model can be tested not only for short term memory but also for prediction of climate indices and flood forecasting where long term memory is required also note that another long term memory model such as gated recurrent unit can be also applicable but not tested in detail credit authorship contribution statement taesam lee conceptualization resources formal analysis ju young shin conceptualization methodology writing original draft jong suk kim writing original draft writing review editing vijay p singh writing review editing declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments this work was supported by the national research foundation of korea nrf grant funded by the korean government mest 2018r1a2b6001799 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j jhydrol 2019 124540 appendix a supplementary data the following are the supplementary data to this article supplementary data 1 
5825,stochastic simulation has been employed for producing long term records and assessing the impact of climate change on hydrological and climatological variables in the future however traditional stochastic simulation of hydroclimatological variables often underestimates the variability and correlation structure of larger timescale due to the preservation of long term memory however the long short term memory lstm model one type of recurrent neural network rnn employed in different fields exhibits a remarkable long term memory characteristic owing to the recursive hidden and cell states the current study therefore applied the lstm model to the stochastic simulation of hydroclimatological variables to examine how good the lstm model can preserve the long term memory and overcome the drawbacks of conventional time series models the simulation involved a trigonometric function and the rössler system as well as real case studies for hydrological and climatological variables results showed that the lstm model reproduced the variability and correlation structure of the larger timescale as well as the key statistics of the original time domain better than the traditional models the hidden and cell states of the lstm containing the long memory and oscillation structure following the observations allows better performance compared to the other tested conventional models this better representation of the long term variability can be critical in water manager since future water resources planning and management is highly related with this long term variability thus it is concluded that the lstm model can be a potential alternative for the stochastic simulation of hydroclimatological variables also note that another long term memory model such as gated recurrent unit can be also applicable keywords deep learning stochastic simulation hydroclimate long term memory streamflow climate index 1 introduction long term historical data on hydroclimatological variables are needed in many environmental and hydrological studies stochastic simulation has been frequently employed to assess water resources systems and its influences from climatic variables using time series models including parametric models such as autoregressive ar model lee 2016 or nonparametric models lall and sharma 1996 prairie et al 2005 lee et al 2010 recently it has been found that long term climatic variability significantly represented with climate indices such as pacific decadal oscillation el nino and northern atlantic oscillation influences the hydrological variables therefore it is important to reproduce such long term evolution in hydroclimate variables however one of the drawbacks of conventional time series models is their deficiency to reproduce the variability and correlation structure of the upper timescale for example when monthly streamflow is modeled with a parametric model such as ar model or a nonparametric model the simulated data underestimate the observed variability and lagged correlation at the annual scale chen et al 2010a in order to overcome this drawback many enhanced models have been proposed such as a markov switching model k nearest neighbor with the genetic algorithm and hidden markov chain model sveinsson et al 2005 salas and lee 2010 however those models entail complex procedures due to some constraints and requires many assumptions to be implemented for example linearity normality and stationary additionally those models can be applied to a specific type of time series data due to limitations of each model meanwhile machine learning tools such as neural networks nns have been used to simulate hydroclimatological variables tongal and booij 2018 to memorize past information that is important in hydroclimatology advanced recurrent nns rnns have been developed for hydrometeorological applications besaw et al 2010 however as the time interval increases the rnn model is not able to learn to connect information further in the past to remedy this loss of memory hochreiter and schmidhuber 1997 proposed a long short term memory lstm model and showed that this model is capable of performing complex and long time lag tasks that rnns are not capable of the rnn model has the vanishing gradient problem in training using the data sets with many lags gradients of weights for the data at long lags become very small in the rnn since lstm can deal with the vanishing gradient problem adopting a new unit concept composed of a cell input gate an output gate and a forget gate the lstm can give prediction conserving long term memory unlike the rnn model in the stochastic simulation a long term record of data is generated by the stochastic time series models it is critical to reproduce the statistical characteristics of time series data instead of predicting specific values modeling a long term record without the loss of original statistical characteristics is very difficult due to the dissipation of information by lead time the statistical characteristics of hydrological time series data are governed by a complex interaction of atmospheric and hydrologic phenomena the conventional neural networks i e multi perceptron layer model may not be able to emulate complexity in the data a deeper network may be a good remedy for a time series model of hydrological data because deep neural networks have complex and deep hidden layers they can account more complex interaction as compared to the conventional machine learning algorithms shen 2018 shen et al 2018 in the current study the lstm is tested for a time series model in a stochastic simulation as a deep neural network since the publication of the hochreiter and schmidhuber 1997 paper many researchers have reported excellent results in a wide range of sequential applications wang et al 2017 recently rnns especially lstm hochreiter and schmidhuber 1997 and the gated recurrent unit gru cho et al 2014 are used as strong sequences in various tasks including machine translation language modeling voice text conversion health informatics and time series vlachas et al 2018 they have adopted used for core algorithm in modeling hydro and hydroclimatological variables and some case studies have been reported wang et al 2018 yuan et al 2018 bowes et al 2019 yuan et al 2018 employed the lstm model to predict monthly runoff and found that it was more accurate than other models wang et al 2018 showed that the lstm model performed better than nn models for predicting water quality bowes et al 2019 compared two types of neural networks rnn and lstm for their ability to predict groundwater table response to storm events in a coastal environment and the lstm has better performance than rnn many machine learning algorithms such as support vector regression extreme learning machine random forest and artificial neuro fuzzy inference system were used for univariate time series modeling of hydro meteorological variables using machine learning methods particularly to forecasting hipni 2013 huang et al 2014 karran et al 2013 li et al 2016 yaseen 2016 their performance and suitability differed from the characteristics of data sets papacharalampous et al 2018 and papacharalampous et al 2019 compared performances of machine learning algorithms and stochastic time series methods for forecasting univariate hydrological variables via extensive simulation and case studies they claimed that the machine learning and stochastic methods provided equally useful forecasts since the machine learning and stochastic methods inherently cannot capture complex the characteristics of the time series data such as multiple oscillations and long term correlation their performances for forecast would be similar subsequently their performances may be similar in a stochastic simulation of hydroclimatological variables unlike these methods the lstm has the potential to reproduce the long term behavior in hydroclimatological variables due to its special structure however little attention has been given to the lstm model for stochastic simulation in hydroclimatological studies therefore this study applied the lstm to stochastic simulations and examined whether long term behavior of observed hydroclimatological variables can be reproduced note that the current study is focused on building a stochastic simulation model with lstm rather than the traditional prediction since its object is different from the traditional prediction the performance testing is focused on whether the model is to reproduce the statistical characteristics of observational data especially long term evolution the proposed simulation models can contribute to enhancing our capacity to reproduce characteristics of hydroclimatological data this will lead to improvements in the long term modeling of rainfall runoff and reservoir operation the paper is organized as follows in the next section a mathematical background of traditional stochastic simulation models and neural networks is reviewed section 3 discusses the lstm model and its simulation strategy the methodology for application and results is presented in section 4 two real case studies that apply the lstm model to simulate observations of a hydrological variable and a climatological variable are discussed in sections 5 and 6 respectively section 7 presents a discussion followed by the summary and conclusions in section 8 2 mathematical background 2 1 stochastic time series models 2 1 1 fundamentals of time series models stochastic models attempt to reproduce statistical characteristics of observed data and can be expressed as 1 y t f x t ε t where x t x t 1 x t 2 x t d is a set consisting of d number of predictor or explanatory variables at time t and εt is the independent white noise term regarded as εt n 0 σε 2 while yt is the predictand at time t n and σε 2 are normal distribution and variance of independent white noise term respectively autoregressive time series models employ the lagged terms of the predictand variables for predictors as x t y t 1 y t 2 y t d the traditional autoregressive ar model is a linear combination of these predictors while a nonlinear ar nar model develops a nonlinear combination of the predictors the nonlinear relation can be developed with neural networks or deep learning algorithms such as rnn convolutional neural network and lstm stochastic simulation is done by repeatedly employing eq 1 with random noise generated from a normal distribution 2 1 2 autoregressive ar model to model time series of hydrological and hydroclimatological variables many time series models have been proposed and tested salas et al 1980 2006 the autoregressive moving average arma model is commonly used to take into consideration random shocks also known as innovations for time series modeling autoregressive integrated moving average arima model was proposed for modeling data that have linear or non linear trends for taking into account seasonality in data seasonal ar arma and arima models have widely employed in the modeling of hydrological variables for a variable yt the autoregressive model of order p ar p can be described as 2 y t j 1 p φ j y t j ε t where yt is the time dependent series with zero mean and εt is the time independent series as εt n 0 σε 2 the target time series must follow the assumption that there is no long term trend or cyclicity and the time series variable yt is assumed to be normally distributed to meet this assumption of normality and no cycles in a target variable the target time series must be transformed for each season the autocorrelation function or lagged cross correlation ρ k of the ar model is geometrically bounded as 3 ρ k c r k k 1 2 where c 0 and 0 r 1 while c is a constant value the infinite sum k ρ k is defined as short term memory and the case k ρ k is long term memory brockwell and davis 1988 generally the ar model has a short memory structure there are many transformation methods such as power log standardization and copula normalization to meet the normality condition of the ar model lee and salas 2011 proposed a copula based transformation a variable can be transformed into standard normal distribution as 4 y t f φ 1 f y θ where f y is the cumulative distribution fitted to the original variable y t and θ is the distribution parameter in general gamma distribution has been employed for hydroclimatic variables f φ 1 is the inverse of standard normal distribution also seasonal cycles can be abstracted by applying this transformation in each season when observations follow a normal distribution but with seasonal mean and standard deviation only standardization can be applied to remove the seasonality by 5 y t y t μ y σ y where y t and y t are the original and transformed explanatory variables respectively μ y and σ y are mean and standard deviation of the original variable y t 2 2 neural network models 2 2 1 feed forward neural network ffnn a neural network is a computing system inspired by the human nervous system it is a network of adaptive and highly interconnected processing units called neurons a simple feed forward neural network ffnn is composed of three layers as input hidden and output with many neurons connected with nodes as shown in fig 1 a this network also is called a multi layer perceptron model in this network the source nodes of the input layer are projected onto computational nodes in the hidden layer through the connection weights and then to neurons of the output layer the learning process of the ffnn is to adjust weights to minimize the difference between the values predicted by the model and real data consider an output vector with two dimensions y y 1 y 2 y n t with y t y t 1 y t 2 y t no and an input vector or called feature matrix x x 1 x 2 x n t in which x t x t 1 x t 2 x t d is a vector with d features note that 1 no is the number of output variables and n is the record length and 2 in the current study the number of output variables is one no 1 y t y t and only the same variable as the output but time lag is employed i e x t y t 1 y t 2 y t d the architecture of the network employed in the current study is illustrated in fig 1 a the output prediction yt can be expressed as 6 y t g ho w ho h t b ho 7 h t g ih w ih x t b ih where gho and gih are activation functions from hidden to output and from input to hidden layers respectively a few activation functions are available such as tanh and sigmoid also w ho and w ih denote the weight parameter matrices and bho and bih denote their bias and ht contains the nh number of hidden units these parameters can be estimated with a certain predefined criterion such as minimizing a mean square error between the predicted value and observed output one of the popular learning methods for estimating parameters is backpropagation bp which searches for a minimum of error function by applying the gradient descent method and iteratively tunes the weights based on the error derivative with respect to each weight 2 2 2 recurrent neural network rnn recurrent neural networks rnns are one type of extended neural networks that exhibit not just the current data state but also the previous hidden state rnns contain a feedback loop so that the current hidden state serves as an additional input to estimate the subsequent hidden state shown in fig 1 b for rnns the hidden state vector can be modified from the traditional nn in eq 7 as 8 h t g ih w ih x t w h h t 1 b ih where w h is the weight parameter matrix for the previous hidden state to estimate the parameter set composed of weights and biases backpropagation through time which is similar to bp in traditional nns is employed recurrent networks use their understanding of past events to process the input vector however rnns have a strong weakness on called gradient vanishing problem such that the gradients back propagating through the hidden units shrink exponentially when there is a long range 3 long short term memory lstm and its stochastic simulation 3 1 lstm lstm was proposed by hochreiter and schmidhuber 1997 to solve the vanishing problem in rnns and has since been improved stollenga et al 2015 greff et al 2017 they introduced multiplicative gate units that learn to open and close access to the information flow illustrated in fig 1 c this deep learning algorithm has been employed in various fields due to its exceptional long term memory characteristic the lstm model contains three gates as input output and forget gates denoted as it ot and ft respectively controlling the amount of information flow within the cell which is responsible for memorizing values over a number of time intervals as shown in fig 1 c these three gates compute an activation of a weighted sum with the sigmoid function between zero and one these gates are described as 9 f t σ w fx x t w fh h t 1 b f 10 i t σ w ix x t w ih h t 1 b i 11 o t σ w ox x t w oh h t 1 b o where σ z indicates the sigmoid function of σ z 1 1 exp z and w and b are the weight parameter matrix and bias the candidate cell state c t is defined with the tanh activation function ranged as 1 1 as 12 c t tan h w c x x t w c h h t 1 b c the cell state is defined as a combination of 1 how much the previous cell state information has flown in and 2 how much the current update information is in this can be denoted as 13 c t f t c t 1 i t c t where indicates the element multiplication refer to fig 1 c note that the question of how much can be answered with the gates here forget gate f t and input gate i t in eqs 9 and 10 respectively for example if the forget gate is zero then the previous cell information is lost and only the current new cell information is considered i e c t i t c t finally the hidden state is defined by how much the current cell state information outflows to the hidden state 14 h t o t tan h c t the scheme illustrated in fig 1 c presents that the output yt is made of a linear combination of the hidden state h t as shown in eq 6 the hidden state is the cell state filtered with the tanh function whose consideration is controlled with the output gate o t the cell state is a linear combination of the current updated cell information c t and the inflowed previous cell state ct this deep learning algorithm can sequentially deliver the long term memory through the cell state as stated earlier if the forget gate is zero then the previous cell information is lost and only the current new cell information is considered i e c t i t c t conversely the previous cell state information is completely delivered as c t c t 1 when i t 0 and ft 1 note that those three gates are defined with the previous hidden state h t 1 and the current input x t here yt 1 the reason that this algorithm has the long term memory ability is due to particular information delivery system with the cell state and three control gates in addition to the hidden state an example of the lstm is given in supplementary a to further illustrate the algorithm 3 2 stochastic simulation with machine learning algorithms as mentioned in eq 1 the stochastic simulation with machine learning algorithms including rnn and lstm can be done by setting the explanatory variables as the same variables but with time lagged as x t y t 1 y t 2 y t d the functional relation i e f x t in eq 1 between x t and yt is nonlinearly created from the rnn and lstms for consistency of comparison input data sets for all employed models are the preprocessed data sets described in section 2 1 2 note that we only tested x t y t 1 for the lstm model to investigate the model performance because the lstm model with one lagged data leads to comparable performance to those with different lagged data and further tests in the discussion section the modeling procedure seems a forecasting scheme because the model requires to preserve the lagged short and long term correlation structures it is straightforward to simulate with machine learning algorithms after building their architecture and fitting them the random number is generated from a normal distribution for εt in eq 1 and is multiplied with the error standard deviation σ ε i e εt n 0 σ ε 2 the error variance σ ε 2 can be estimated with the difference between the values predicted by machine learning algorithms and observed values the datasets were separated with training and validation periods only i e no testing data since the objective of the current study is to build the stochastic simulation model instead of forecasting not specific values k fold cross validation kfcv was used to find the lstm model structure the performance of the lstm model was checked from how well the key statistics such as mean and standard deviation and short and long term correlations are preserved the model overfitting and underfitting can be judged from its reproduction capability of those statistics the dataset for estimating the error variance must be obtained from the validation data not included in the training data since the real error for unknown input values is with the testing data otherwise the variance of the final simulated data can be underestimated in order to cover the entire dataset in estimating the error variance k fold cross validation type approach can be applied as 15 σ 2 1 n t 1 n ε t 2 1 n t 1 n f val x t y t obs 2 where f val x t is the estimates for the validation dataset t and y t obs is the corresponding observed data zhang and yang 2015 the detailed information for the procedure is presented in the methodology subsection corresponding to the section this modeling process is different from traditional time series model such as ar in that ar model employs its own parameter estimation method such as forward backward employed in the current study burg s method maximum likelihood and method of moments brockwell and davis 2003 4 simulation study for synthetic data sets many hydrological and hydroclimatological variables have multiple oscillatory characteristics such as daily seasonal and long term cycles for evaluation purposes synthetic data sets can reproduce multiple oscillatory characteristics the trigonometric functions and rössler system satisfy this condition furthermore the rössler system can reproduce non linear oscillatory characteristics therefore the trigonometric functions and rössler system are selected to generate synthetic data in testing the performance of lstm for a stochastic simulation model of hydrological and hydroclimatological variables 4 1 trigonometric function in order to test the performance of the lstm stochastic simulation a synthetic dataset was generated from a combination of trigonometric functions as 16 y t c o s a t 0 5 s i n 5 t 0 3 ε t here the error term ε t is a random noise following a standard gaussian distribution the value 0 3 is multiplied so that the error term is not too much dominant in the simulated dataset the two trigonometric functions varied from 1 to 1 and the random noise was between 1 96 0 3 and 1 96 0 3 for 95 percent a realization of eq 16 is presented in fig 2 with a length n 2000 four different values of a in eq 16 were tested as a 1 2 3 and 4 as shown in the middle panel the sine function represents a relatively high frequency with around 360 5 per cycle see the black line in the top panel while a cosine function represents a relatively low frequency with 360 per cycle for a 1 see the blue dash dotted line in the top panel and 90 per cycle for a 4 the two signals realized with a 1 and a 4 see the dotted and dash dotted lines in the bottom panel indicate that the low frequency case a 1 clearly shows the oscillation of the sine function conversely for the high frequency case a 4 the sine signal is somewhat vague because their frequency is similar to each other with the dataset of four different values of a realized the lstm model was fitted the simulation with eq 16 was performed by generating 100 series and only one lagged term was considered for the predictor variable i e x t y t 1 for the lstm model for a simulation model the accurate prediction is not its interest but reproducing the statistical characteristics of observational data further data separation is not needed to confirm since its performance is checked through the reproductivity of the key and long term statistics in observation in our study the ten fold cross validation with all data points was employed only to find the architecture of lstm the number of hidden units nh for lstm was investigated with k fold cross validation and not much difference can be found in the root mean square error rmse after four therefore its hidden units were fixed as four the same number of hidden units was employed for the following case studies basic statistics of the data simulated by the lstm model such as mean standard deviation skewness lag 1 correlation maximum and minimum are presented in fig 3 for the data realized with a 1 the results are displayed using boxplots in which the box shows the interquartile range iqr when the extrema are higher than 1 5iqr the whiskers extend up to 1 5iqr and the excess values are displayed with crosses and are considered outliers otherwise the whiskers extend to the extrema and feature horizontal lines at their ends the horizontal line inside the box depicts the median of the data in addition the value of the statistic corresponding to the historical data is represented by a cross connected line with dots as a result the simulation model well reproduced the basic statistics of the realized data and the fitted lstm model was suitable for stochastic simulation the long range correlation structure was investigated as shown in fig 4 results of the realized data with a 1 are shown in the top panel whose cycle was 360 the lagged correlation of the simulated data was well reproduced only up to 30 lags the observed long term oscillation pattern of the correlation was not preserved for the case of a 2 the simulated data reproduced the correlation up to 60 lags the simulated data for the case of a 4 well reproduced the historical correlation structure while a slight deviation was seen in the case of a 3 these results show that the lstm model has a good potential for reproducing long term patterns of observed data however it also has the limitation that the model can capture it is quite surprising that the lstm architecture with lag 1 term as an explanatory variable i e x t y t 1 can deliver this complex correlation structure even with this limitation the spectral densities shown in fig 5 coincided with the result of lagged correlations the simulated data for the realized data with smaller a values i e a 1 and 2 in eq 16 could not reproduce the lower frequency part of the observed spectral densities while the simulated data with higher values i e a 3 and 4 are well reproduced 4 2 rössler system in this study an example of a non linear dynamic system rössler attractor rössler 1976 which is one of the most famous and chaotic attractors that have been tested in the field of climatology and physics huang et al 1998 smith et al 1999 kijewski correa and kareem 2007 he et al 2016 were selected the attractor is the solution to a system of three non linear ordinary differential equations as x y z y x α y and z β z x δ z where x y z r 3 are dynamical variables defining the phase space with time t and α β δ r 3 are parameters this example was selected for its chaotic behavior with long term persistency rössler 1976 the generated series is presented in fig 6 the x variable of the generated series was selected in the current study including the random error as x t ε t and ε t n 0 0 5σ x the model was fitted to the lstm model with the hidden units of 16 from 10 fold cross validation results not shown the ar model were also fitted to this dataset for comparison the order p of the ar model was checked up to p 100 with akaike information criterion aic the result presented that the order of 53 showed the best with aic however the simulation performance was not reasonable and the order of 5 presents the best performance among the orders tested the result presented is with the order of 5 i e p 5 in addition many different structures in the rnn model including the hidden unit were exhaustively tested with 10 fold cross validation none of the rnn model structures present reliable performance to use and no correlation structure was presented therefore the results of the rnn model were omitted the lagged correlations of the observed and simulated rössler x variable from the fitted ar and fitted lstm models are presented in fig 7 the observed lagged correlation presents the long memory i e slow decay to around lag 40 and oscillation structure the ar model shows the fast decay of the lagged correlation structure meanwhile the lstm model presents strong preservation of the lagged correlation and its oscillation component is reproduced up to nearly lag 100 the spectral density presents that the behavior of the simulated data from the ar model is different from the observation especially the observed spectral density in the left side presenting low frequency is not reproduced properly from the ar model while the lstm better preserves the spectral density this indicates that the lstm have the good capability to reproduce the long term persistency as well as long term oscillation 5 hydrological application for streamflow 5 1 study area the colorado river is one of the important river systems in north america which is an indispensable source of water supply for seven states in the western united states and mexico the colorado river system has been divided into two large systems as the upper colorado river and the lower colorado river where the division is at lees ferry the system has been regulated with a number of within the year and over the year reservoirs the system was analyzed by performing simulation studies using potential streamflow scenarios that may occur in the future monthly streamflow data from colorado river at lees ferry was used because the system was regulated on a monthly basis to assess model performance in addition simulated data must also preserve the interannual variability of the observed data since the streamflow time series of the colorado river at lees ferry shows a significant interannual variability with several wet and dry periods the historical data of the colorado river has been naturalized and extended to 1906 2003 lee and salas 2006 5 2 application three models were compared to the lstm model for the lees ferry monthly streamflow these models were ar p ffnn and rnn with sequences transformed to gamma distribution for each month because monthly streamflow data has a strong seasonality and to meet the normality condition of the time series model to eliminate strong seasonality and normalize the dataset a copula based transformation was applied as in lee and salas 2011 for each month defined as 17 y t f φ 1 f y y t θ where f y is the cumulative distribution function cdf of the original domain variable and f φ 1 is the inverse cdf of the standard normal variable yt is the standard normal variable and gamma distribution was applied for f y since this distribution has been widely fitted to hydroclimatological variables including streamflow lee et al 2010 salas and lee 2010 this normalization procedure was applied in the current study since the distributional characteristics of the employed data is dominant and ruin the other statistical characteristics such as lagged correlation this normalization has been commonly employed in hydrologic time series modeling salas 1993a also twelve times longer record is achieved from this standardization without it a separate lstm model should be applied at each month the standardized streamflow is shown in fig 8 a its time series illustrates that there is a strong long term oscillatory structure especially in the last 300 months note that it has been known that conventional stochastic simulation models for hydrological variables can reproduce short term dependence structure but not long term dependent structure for example monthly simulated data of hydrological variables from stochastic simulation models can preserve key statistics at the monthly scale such as mean standard deviation and lagged correlations however when the monthly simulated data is accumulated into the annual series the annual data cannot reproduce the key statistics of the observed annual data 5 3 results as shown in supplementary a for the example of the lstm model the number of hidden units must be selected different numbers of the hidden unit were tested using the root mean square error rmse result shown in fig 9 indicate that four hidden units provided the smallest rmse for the case with one step ahead prediction of the standardized data see the bottom panel also the original rmse and the long term prediction forecasting up to 30 months ahead were evaluated as shown in the top and middle panel of fig 9 respectively both results showed that two hidden units were the best however the current study simulated the sequence by employing only lag 1 time delayed variable i e xt yt 1 and strong seasonality ruined the rmse in the original domain therefore four hidden units were employed for the lstm model the scatterplot of observed versus the one step ahead prediction is shown in fig 8 c which represents that the lstm model well fitted monthly streamflow data it also tested how the lstm model behaved for the last 10 sequence around 10 years used as the testing period the one step ahead and long term prediction as enlarged in fig 8 b are presented with the dash dotted red line with circle marker and dotted green line with cross markers respectively the prediction results indicate that the lstm model well predicted the one step forward surprisingly the long oscillatory pattern in the observed data was also well presented furthermore the long term prediction showed the excellent performance without losing the memory both predictions allow to anticipate that the long term patterns can be well simulated as well as the short term memory the key statistics of the simulated 100 sequences of monthly streamflow are presented in fig 10 with boxplots as well as observed data with the dash dotted line with cross markers results show that the observed mean and standard deviation of monthly streamflow were well reproduced by the lstm model but skewness was not much preserved note that the distribution applied for transformation was a two parameter gamma distribution as in 17 which cannot specifically take higher moments like skewness into account the lag 1 correlations showed similar ranges of observed lag 1 correlations but overall lag 1 correlations were not reproduced well for an individual month the extrema seemed fairly reproduced even if some bias was seen especially in minima no better or worse performance was observed with the ar 1 model than the lstm model for the key statistics of monthly streamflow see supplementary fig s1 in fig 11 the observed key statistics of annual series were well reproduced by the lstm model including the standard deviation and lag 1 correlation it is quite surprising that the lstm model can reproduce these statistics at the annual scale with only the lag 1 delayed input xt yt 1 in eq 1 it is quite comparable to ar 1 model which uses the same lag 1 delayed input as in 2 shown in fig 12 the observed standard deviation and lag 1 correlation of the annual data were underestimated by the data simulated from the ar 1 model a quite complex model was required to preserve those annual statistics the ar 5 model selected as the best model was tested according to the akaike information criterion aic shown in supplementary fig s2 and ar 12 model the annual standard deviation was still significantly underestimated and its correlation was overestimated in both cases see supplementary figs s2 and s3 note that the lag 1 correlation of monthly data was highly underestimated by the ar 5 and ar 12 models see supplementary fig s7 this is because modeling the long term correlation structure weakens the preservation of short term memory conversely the long term memory was preserved by the lstm model without any weakening of the short memory structure furthermore ffnn and rnn models see supplementary figs s5 and s6 also are tested results showed that those models could not reproduce the key statistics of observed monthly streamflow at all multiple model variations were for ffnn and rnn were tried without any success it was concluded that the ffnn and rnn models did not seem to have the stochastic simulation capability for the considered data sets for water resources managers reproduction of drought related statistics is critical for stochastic simulation in fig 13 the consecutive length and sum for deficit and surplus as well as storage capacity were presented with the boxplots for data simulated by the lstm model and the observed data circle the storage capacity was the minimum capacity that a reservoir could deliver a certain demand through the time period salas 1993b the lstm model reasonably reproduced the storage capacity while some biases were observed for other statistics for the ar 1 model shown in fig 14 these statistics were not very well preserved notably significant underestimation was observed in the storage capacity produced by the ar 1 model 6 climatological application for pacific decadal oscillation 6 1 data description and application methodology one of the most representative of climate indices as pacific decadal oscillation pdo a long lived el nino like pattern of pacific climate variability mantua et al 1997 was tested this index has been proven to be teleconnected with many hydrometeorological variables nigam et al 1999 the variability of climate indices has been employed in stochastic simulation of hydrometeorological variables teleconnecting climate indices and variables lee and ouarda 2012 ar p was compared with the lstm model furthermore the shifting mean sm model developed by salas and boes 1980 and improved by sveinsson et al 2003a was also tested its fundamental model description is shown at the supporting information a the sm model was developed to model the hydroclimatic process that shifted abruptly from one stationary state to another the dataset was downloaded from mantuna 2017 and standardized by subtracting mean and dividing by standard deviation as in eq 5 especially since only a little seasonality could be observed in the mean 6 2 results the monthly key statistics were well preserved by the lstm model as shown in fig 15 as mentioned before seasonality was present in mean and standard deviation and was well reproduced the seasonality in skewness and lagged correlation was not reproduced because the model did not take them into account note that the transformation has been made only to mean and standard deviation as in eq 5 to preserve their seasonality but no consideration has been made to skewness and lagged correlation the extrema of simulated data slightly followed the seasonality due to the effect of standardization i e subtracting mean and dividing by the standard deviation the ar 1 and sm models showed a result similar to the lstm model result not shown the other ar 4 presenting the best order with aic and ar 12 models were tested and showed significant underestimation in lag 1 correlation results not shown which was the same as in the colorado river case study the observed annual statistics were fairly preserved by the simulated data from the lstm model except for the underestimation of lag 1 correlation results not shown in the ar 1 model much significant underestimation was observed for the standard deviation and lag 1 correlation see fig s8 the higher order ar 4 and ar 12 models overestimated lag 1 correlation but significantly underestimated the standard deviation at the annual scale results not shown moreover ffnn and rnn models were also tested see supplementary figs s9 and s10 results showed that those models could not reproduce the key statistics of observed pdo series the lagged correlation structures of the tested models as in fig 16 were further investigated the correlation structure of the observed monthly pdo presented with the dotted line with circles seemed to have a short memory pattern but also slightly long term fluctuation around 80 lags the ar 1 and sm models exhibited an exponentially decaying memory loss in the early time lags see the panels a and b of fig 16 respectively conversely the lagged correlation structure shown by the lstm model was similar to that by the observed data as in the panel c of fig 16 the examples of two correlation plots see two blue dash dotted lines with and markers in the panel c showed a long term oscillatory pattern similar to the observed data the other ar 4 and ar 12 models showed the fluctuation of the correlations even with positive and negative correlations crossing see supplementary fig s11 the higher order ar p models sacrificed short term memory structure to preserve the long term structure the spectral density salas et al 1980 describing the distribution of power into frequency components was presented for the observed and simulated data with log scale shown in fig 17 the ar 1 and sm models showed a different shape of the spectral densities from those of the lstm model the lstm model showed a straight decrease for the log scale which coincided with the observation the other two models showed a decreasing shape of second or third order polynomial functions so that in some parts of ranges the observed spectral density was outside of the 95 percentile range of the 100 simulated series hurst coefficient h hurst 1951 has been interpreted as a long memory model if h 1 2 and vice versa this coefficient was estimated for each model as shown in fig s12 the observed data showed that h was about 0 85 circle implying a high long memory structure while the ar 1 and sm models significantly underestimated the coefficient the lstm model showed better but not perfect preservation the other ar model showed a result to similar to that of the lstm model result not shown 7 discussion the lstm has been developed to store information over long extended time intervals this up to date deep learning model was tested and compared with the conventional time series models to determine whether the lstm model could solve the long term memory problems in stochastic simulation of hydro climatological variables results indicated that the lstm model not only reproduced the variance and lagged correlation structure of the larger time scale but also preserved at the current time scale this is due to the ability to memorize long term patterns of observed data sets as the lstm model was originally devised hochreiter and schmidhuber 1997 discussed that the lstm could learn to bridge time intervals in excess of exceptionally long time steps without loss of short time lag capabilities the current study shows that the model can have a good potential for simulating hydroclimatological variables by reproducing long term memory i e annual as well as preserving key statistics of the original timescale i e monthly surprisingly all these modeling features of the lstm are acquired with only lag 1 input i e x t y t 1 in eq 1 one of the remaining questions is whether it is possible to improve the model performance with higher time lagged inputs x t y t 1 y t d where d is the considered time lags further tests were made by increasing the number of time lagged inputs for monthly pdo data results showed that better preservation could be made with higher time lagged inputs as shown in fig s13 for example the annual lag 1 correlation can be better preserved with higher time lagged inputs such as d 4 for the lstm model while all the other useful features still stood e g reproduction of observed key statistics of monthly and annual data however the hurst coefficient cannot be improved even with higher time lagged inputs as much as k 12 results not shown from the lstm results the authors wondered which of the model feature made the lstm so special subsequently the structure of the lstm was further investigated by looking into the inside of the model the hidden state ht in eq 14 and the cell state ct in eq 13 were analyzed in detail the time series of both states see supplementary fig s14 showed long term persistency the third and fourth hidden states were highly related to each other the same for the cell state this is shown clearly with the scatterplot in supplementary figs s12 and s13 a significant relation was observed between four hidden states and cell states rather a linear relationship between each cell state was observed see supplementary fig s16 while a nonlinear relation was seen between the hidden states see supplementary fig s15 in fig 18 the relations between the simulated pdo and the hidden states or cell states are presented with four cell and hidden components it revealed that the first top left panel and second top right panel cell states has negative relation with the simulated pdo while the third bottom left panel and fourth bottom right panel cell states has the negative relation this relation is similar in the hidden states with the conditional relations of the first third and fourth hidden states this conditional relation is that the hidden state has around 1 when the simulated output yt is positive physically the highly positive value of the pdo is likely to be in el nino phase corrêa et al 2017 when the simulated pdo is negative the third and fourth hidden states present nearly 1 note that the la nina phase is more intense when the pdo is strongly negative this seems to be a very critical structure to have a long term persistency in the simulated data the hidden state with near 1 value allows constantly staying in one phase with even small autocorrelation of the simulated pdo since the hidden state recursively affects itself and the simulated pdo the time series in fig s14 illustrates that the long term negative phase of the simulated pdo bottom panel between the year 1960 1990 is the effect of the third and fourth hidden states ht 3 and ht 4 in this period the hidden states remain around 1 the highly positive value around the year 1950 of the simulated pdo i e el nino is also related with ht 1 indicating the long term persistency of near 1 this particular structure of the 1 value in the hidden states provide the ability of the long term persistency the lagged cross correlations of the hidden and cell states in supplementary fig s17 showed a long memory structure until 15 20 time lags the longer persistent structure was shown by the lstm model with multiple lagged input as x t y t 1 y t 12 as compared with the original lstm input x t y t 1 as shown in supplementary fig s18 the memory was delayed until 20 30 as well as presented an oscillation pattern in the second hidden and cell states this persistent structure allowed the lstm model to have an exceptional capability to reproduce the long memory structure without the loss of short term memory the long term autocorrelation of the observed pdo is compared with the ones of the cell and hidden states as well as the simulated data shown in fig 19 the short term autocorrelation up to lag 20 is decreased quickly for the observed pdo then the long term oscillation is followed between 80 and 180 this short term autocorrelation behavior and a long term oscillation of the observed pdo is well reproduced in the simulated pdo dashed line with cross marker all the cell and hidden states contain the short and long term memory information the crosscorrelation between the simulated pdo and hidden or cell states presented in fig 20 shows that two hidden and cell states are negatively correlated and the other two are positively correlated with exponential decrease up to lag 20 the crosswavelets between the simulated pdo and the hidden and cell states in figs s16 and s17 are shown the power between 32 and 64 periods are dominant for both of the first and second hidden and cell states see the top panels of figs s19 and s20 while the significant power above 128 is shown in the third and fourth hidden state as well as the cell states see the bottom panels of figs s19 and s20 in this study the long term autocorrelation structure of the hidden and cell states was further investigated up to 500 lags in fig s21 the long term autocorrelations of the hidden and cell states are similar to each other the critical point is that the first hidden and cell states oscillates within 60 lags where this result coincides with the result of the crosswavelets above meanwhile the third and fourth states shows slow decrease up to lag 50 indicating the long term persistency the combined structure of the long term oscillation and slow decrease of lagged correlation in the hidden and cell states allows preserving long term variability and correlation of the pdo furthermore the scatterplots between the hidden state components as well as the cell state components in supplementary figs s22 and s23 illustrated a much weakening relationship compared to the lstm model with one lagged input as shown in figs s15 and s16 this was well stated by the correlation values see supplementary table s1 the weakened relationship between hidden states and cell states can be induced by more substantial lagged inputs that bring in more information this weak relationship is because each hidden and cell component requires more independent information to be passed on to each other also the histogram shows the hidden states are highly skewed see supplementary figs s15 and s22 while the cell states are not much skewed this skewness might be induced from the fact that a particular hidden state among the four components might prefer a high or low state consistently 8 summary and conclusions the long short term memory lstm model one of the most modern deep learning algorithms has been proposed and developed in the neural computations to memorize long time interval information the lstm model was tested for this useful feature in the current study on whether or not the long term memory structure can be preserved without losing the short term memory structure of hydroclimatological variables simulations with a trigonometric function and the rössler system as well as two case studies for hydroclimatological variables as the colorado river streamflow and the pdo index were performed the results indicated that the lstm model had an excellent ability to reproduce key statistics from the upper time domain e g yearly for streamflow case and the original time domain e g monthly for streamflow case while the traditional ar models and the other tested models such as ffnn rnn and sm did preserve the variance and correlation at the larger time scale i e annual it has been shown that the lagged inputs further improved to reproduce the statistics by the lstm model at the larger time scale it is concluded that the lstm model has a strong potential to be an excellent alternative for stochastic simulation for hydroclimatological variables while preserving key statistics at different time scales the lstm model can be tested not only for short term memory but also for prediction of climate indices and flood forecasting where long term memory is required also note that another long term memory model such as gated recurrent unit can be also applicable but not tested in detail credit authorship contribution statement taesam lee conceptualization resources formal analysis ju young shin conceptualization methodology writing original draft jong suk kim writing original draft writing review editing vijay p singh writing review editing declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments this work was supported by the national research foundation of korea nrf grant funded by the korean government mest 2018r1a2b6001799 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j jhydrol 2019 124540 appendix a supplementary data the following are the supplementary data to this article supplementary data 1 
5826,changes in canadian prairie streamflow particularly trends over time have not been well studied but are particularly relevant for food and water security in this vast agricultural region streamflow records for this region are often unsuitable for conventional trend analysis streams are often intermittent and have only a few days per year with flow and stations operate only during the warm season because of a lack of flow during the very cold prairie winter this study takes an alternative approach streamflow data for the period from march to october for individual years between 1910 and 2015 from 169 hydrometric stations from the prairie and adjacent areas in canada were converted to annual cumulative runoff series these 5895 individual station years were then clustered based upon their shape using dynamic time warping three clusters of cumulative annual runoff were found the first and most common type has infrequent days with flow and low total annual runoff 0 50 mm the second has more days with flow and slightly greater runoff 48 175 mm and the least common third type has the fewest days without flow includes perennial streams and has much greater annual runoff 173 mm for each hydrometric station a time series of annual cluster memberships was created trends in the fractions of cluster types were determined using logistic regression with spatial groupings of these time series over five year periods trends in the fractions of types within an ecoregion indicate spatially consistent and organized changes in the pattern of runoff over the region in the western canadian prairies particularly in the mixed grassland and cypress upland ecoregions drying is occurring as indicated by the increased frequency of the dry type in the northern and eastern canadian prairies conditions are shifting to greater runoffs particularly in the aspen parkland where the wet types are increasing in frequency keywords prairie streams trend assessment dynamic time warping logistic regression ecoregion 1 introduction changes in the hydrology of the canadian prairies have been chronically understudied because of lack of continuous streamflow records and statistical methods suitable for zero rich partial year records many studies of trends and changes in hydrology have avoided stations from this region because the streams often do not have continuous streamflow records the available records contain large numbers of days having zero discharge and many sites are operated only during the warm season because of a lack of streamflow during the cold winters many studies have assessed annual trends and seasonal changes in rivers across canada but excluded the prairies for the above reasons e g gan 1998 whitfield and cannon 2000 zhang et al 2001 yue et al 2003 ehsanzadeh and adamowski 2007 statistical methods suitable for perennial streams are not always applicable to temporary streams court 1952 buttle et al 2012 perennial rivers on the prairies generally originate in the canadian rockies are exotic to the region they cross such that their hydrological characteristics are vastly different than those that originate locally macculloch and whitfield 2012 daily streamflows in semi arid and arid regions are characterized by extended sequences of days with zero flow seasonality high autoregressions and extreme events such as floods and droughts ghahramani et al 2012 many prairie basins have non contributing areas areas within the basin that only infrequently generate stream flows stichling and blackwell 1957 spence et al 2010 shook et al 2015 because of low precipitation and relief and the presence of depressions in the post glacial landscape many prairie streams have no baseflow because of the presence of low permeability glacial tills in their basins shook et al 2015 very few of the catchments across the prairies can be considered as reference sites whitfield et al 2012 because of widespread landscape and wetland drainage changes associated with agriculture conly and van der kamp 2001 dumanski et al 2015 separating the effects of climate variation from those of land use change and drainage impact is difficult a valid study of hydrologic change across the prairies must address these issues the prairie ecozone exists because of aridity mcginn 2010 and is covered by farmland 94 in a similar fashion to the northern us great plains little of the original native grassland or woodland vegetation remains and many of the wetlands have been lost the vegetation pattern of the north american grasslands an area of predominantly grass and parkland east of the rocky mountains consists of the steppe with shortgrass on the west extending into the palliser triangle in canada and the prairie with tallgrass on the east exist because of climatic gradients borchert 1950 the prairie climate is characterized by variability swings in temperature and precipitation occur within and between seasons as the result of the interactions between air masses wheaton 1998 the climatic gradients are largely in precipitation but temperature particularly the location of the 0 c isotherm also plays an important role precipitation gradients exist because of prevailing airflows from the west consequent rain shadows in the lee of the canadian rockies and convergence and frontal penetration from the south and suggest that changes in the regional westerly airflow have affected the boundary between forest and grassland over time borchert 1950 rosenberg 1987 edwards et al 2008 the west to east increasing precipitation gradient across alberta saskatchewan and manitoba longley 1953 ferguson and storr 1969 lemmen et al 1998 bonsal et al 1999 was attributed to storm tracks longley 1974 asong et al 2015 2016 delineated the prairie into five homogenous precipitation regions southeast prairies northern south central prairies central west prairies and foothills of the canadian rockies tracking the precipitation gradient the spatial and temporal variability of precipitation are not consistent over the prairies correlation between stations is small from may to september as local convection plays a prominent role in generating summer precipitation longley 1974 drought in the prairies is frequent mcginn 2010 lemmen et al 1998 in the adjacent u s northern great plains the situation is similar the climate is extreme and variable weather conditions change within a day between days between seasons and between years rosenberg 1987 extended wet and drought events which can vary greatly with season and sub regions are common to the great plains and their results suggest that the recent wet period may be a part of natural variability on a very long time scale ryberg et al 2016 ecological structure and water availability across the prairies is spatially complex and changes may not be discernable at large scales from 1890 to 1990 annual precipitation increased in southern canada by 13 but predominantly in eastern canada groisman and easterling 1993 precipitation increases in the prairies over the 20th century were the smallest in canada zhang et al 2000 the seasonality of precipitation in canada changed between 1976 and 1995 summer precipitation in the northern portion of the prairies increased whitfield et al 2002 precipitation increased by 49 mm or about 9 and the west east gradient in precipitation across the prairies steepened with drying in the west and the east becoming wetter millett et al 2009 while precipitation intensities have not increased the number of events greater than 0 5 mm have increased by 16 per year over the past 75 years and include an increased number of days with rain akinremi et al 1999 summer single day rainfall events have not decreased in frequency but the number of multiday events have increased in the prairies over the 20th century shook and pomeroy 2012 the number of multiday rain events has increased by 50 in eastern saskatchewan dumanski et al 2015 the increases in the numbers and lengths of multiday rain events suggest that the spatial extent of storms has increased the temporal variability of daily snowfalls over the 20th century has also increased which affects snowfall accumulation shook and pomeroy 2010 the fraction of precipitation falling as rain rather than as snow in non summer months increased significantly by 0 095 yr 1901 2000 and 0 17 yr 1951 2000 shook and pomeroy 2012 changes in precipitation across the arid prairies are expected to drive hydrological changes for example multiday extreme events rather than individual events were responsible for recent flooding in the saskatchewan and athabasca river basins brimelow et al 2014 annual mean temperatures in canada increased by between 0 5 and 1 5 c between 1900 and 1998 and the greatest warmings took place in the prairies zhang et al 2000 and have continued debeer et al 2016 in the prairie pothole region north south temperature gradients have changed minimum temperatures have increased by 1 0 c more in winter than in summer and maximum temperatures have cooled by 0 15 c between 1906 and 2000 millett et al 2009 using common trend detection techniques ehsanzadeh et al 2016 investigated the effects of precipitation change and landscape changes on streams in the assiniboine and saskatchewan river basins examining trends and change points did not find detectable changes in the streamflow metrics commonly used for continuously flowing streams statistical analyses of daily data are problematic for intermittent streams because of the preponderance of zero flows rendering basic statistical analyses inappropriate alternative approaches are needed annual cumulative flow curves and cumulative annual curves are a useful method for comparing records between stations and between years within stations amongst perennial streams the shapes and timings of the annual cumulative flow curves are remarkably consistent for prairie streams the shapes of these curves are consistent but not their timing macculloch and whitfield 2012 the number of days with zero flow and the duration of zero flow periods for prairie intermittent streams were affected by basin size and precipitation large basins had the fewest zero flow days the shortest periods of continuous flow and a negative relationship between annual precipitation and zero flow days costigan et al 2015 therefore the shape of the annual cumulative runoff curves is a distinguishing feature that can be classified the methodology using dynamic time warping for classifying the shapes of annual runoff curves and clustering of them into types is discussed in the section on methods the dynamic time warping algorithm dtw wang and gasser 1997 keogh and ratanamahatana 2005 was used to measure the distance between two time series dtw exploits the pattern of shifts between two time series to measure their alignment as a complementary tool sakoe and chiba 1978 proposed the method for speech recognition and it has since become widely used for measuring the similarity between two time series berndt and clifford 1994 wang and gasser 1997 an example of aligning two time series is shown in fig 1 where two non overlapping ecg time series are compared unlike euclidean distance where series are compared point to point dtw compares the curves based on their features and aligns the series giorgino 2009 traditional trend analysis methods examine annual series of one or more streamflow attributes such as non parametric trend tests of mean annual flow the method presented here examines changes in the frequency of types the objectives of this paper are to demonstrate this methodology and to apply it to assess regional differences and temporal changes in the frequency of classified runoff regimes of prairie basins trends are determined based on changes in frequency of annual classifications in spatially grouped stations and temporal blocks five years of particular patterns in annual cumulative streamflow ecologically defined areas are used to demonstrate the methodology and trends in frequency in ecoregions by examining changes occurring in different areas of the prairies 2 methods 2 1 data the discharge of a stream is expressed as a runoff depth per basin unit area in mm to remove basin size effects the data for these stations was obtained from ecdataexplorer https www canada ca en environment climate change services water overview quantity monitoring survey data products services explorer html for natural streams to exclude any that were regulated the study region includes the prairie ecoregion and adjacent ecoregions in canada the latter are included so the pattern of accumulated runoff in these areas are known within the clustering this was based upon the view that changes along the margins are likely to be similar to existing patterns in adjacent areas this selection of stations provides the maximum number of sample catchments within the prairies and the surrounding regions the intention was that hydrological types from adjacent non prairie catchments would include sample streams along the margin of the prairies 2 2 analyses all the analyses were done using r r development core team 2014 the r package flowscreen dierauer et al 2017 was used to assess data quality stations determined to have significant change points were eliminated as they would not be suitable for trend analysis the r package dtwclust sarda espinosa 2017 was used to perform dynamic time warping as described below to cluster curves based upon their shapes and inflections the method used here builds from the work of macculloch and whitfield 2012 the annual cumulative runoff time series for each individual year for which there exists a record between 1910 and 2015 from 169 hydrometric stations on unregulated streams from prairies and adjacent regions a total of 5895 individual station years are clustered based upon their shape into a type following the algorithm 1 calculate the daily depth of runoff dividing the daily flows by basin area 2 calculate the cumulative runoff time for the interval of day 60 to day 300 these day numbers correspond to march 1st to october 31 in non leap years for each station and each year these dates span the annual operating period for seasonal gauges only the data available for this interval were used for all streams including perennial streams each case is considered a station year 3 standardize each annual cumulative runoff series to have a mean of zero and a variance of 1 4 cluster each series using dynamic time warping to classify each station year as one of type 1 2 or 3 creating a time series of annual types 5 group aggregate the individual station time series of types across a spatial domain over five year periods this results in a time series of the frequencies of the three types 6 determine trends in the fraction of stations in each type by logistic regression over five year periods for each spatial domain for each station the annual cumulative runoff series for each complete year was generated as is plotted in fig 2 for four sample stations one distinguishing feature of prairie streams fig 2a c is that the annual cumulative runoff plots have a highly variable step like temporal structure due to the intermittency of flow and the dominance of spring snowmelt runoff or extreme event rainfall runoff processes unlike the smoother curves associated with perennial streams which have baseflow and runoff has a wider variety of sources fig 2d macculloch and whitfield 2012 each annual cumulative runoff series was standardized to z scores which have a zero mean and unit variance this is done so that patterns rather than variations in the magnitude of runoff would be captured by the analysis only those years with no missing data in the march to october period were included the clustering of annual cumulative streamflow time series was done using dynamic time warping dtw wang and gasser 1997 keogh and ratanamahatana 2005 with the r package dtwclust sarda espinosa 2017 the outputs are clusters of annual cumulative runoff curves based on partial year records as described above the 169 stations used do not have records for all of the same years non overlapping and inconsistent periods of records may falsely induce trends in mann kendall type climate change studies spatial binning creates a grouping of stations similar in location and for which common responses to climate and landuse should exist since the individual records are being converted to a type which can be counted and converted to a fraction some of the issues of periods of record are of a lesser concern there is a trade off between the temporal block size and the number of station years of data for large areas where there could be many stations such as the entire prairies small block sizes would be possible but for smaller areas with fewer stations such as for several ecoregions the number of available sites is smaller and the block size needs to be larger five year blocks were chosen as a compromise to get large enough counts to determine frequency and to be consistent across different spatial groupings the result of clustering converts the daily runoff values to annual types and the spatial grouping and temporal blocking results in time series of the fraction of each type the trends in cluster types in those series were determined using logistic regression which is an appropriate method to assess trends in discrete count or proportion data frei and schär 2001 schmidli and frei 2005 the r package trend frei 2013 was used to perform the trend tests and produce the odds ratio the constant effect where ratio values 1 are increasing and values 1 are decreasing over time the probability and the estimate of over dispersion which is a measure of inflated variance for the poisson model of count data large values of the overdispersion would indicate that the poisson model is not a good fit of the data 3 results several clustering solutions were evaluated using between five and fifteen clusters the solution chosen used the minimum five clusters the clustering patterns from all solutions considered were quite stable in each solution large numbers of cases were placed in clusters 1 and 2 and the balance of being placed into other small clusters that provided more separation within this complex mixture of accumulation curves fig 3 c clusters 1 and 2 are considered to be of types 1 and 2 respectively the smaller clusters 3 5 were combined into a single type 3 three types of annual cumulative runoff are shown in fig 3 the first has early and low total runoffs 0 50 mm fig 3a the second has a short period of sustained runoffs and greater totals 48 175 mm fig 3b while the third type has both early and late runoff events and annual runoff totals that are very large 173 mm fig 3c the shapes of the type 1 and 2 curves are very simple with small numbers of inflections the clusters regrouped into type 3 have accumulation curves with greater complexity the differences among the curves types are shown by the plots in fig 3c where the three types are rescaled and the 2011 accumulation curves are plotted in black the year 2011 was chosen as it was particularly wet on the prairies and resulted in flooding in some areas shook and pomeroy 2016 stadnyk et al 2016 blais et al 2016 the type 1 curves have only a few inflection points and during 2011 most take place around day 100 fig 3a type 2 curves have more inflection points and are more variable in time but in 2011 they flatten after day 200 fig 3b type 3 curves are more complex and diverse the 2011 cases highlight this fig 3c the number of data sets in each type cluster has changed dramatically over time as is plotted in fig 4 the maximum number of data sets occurred in 1985 140 stations 700 station years prior to 1960 the number of stations ranged between one 1910 1914 and twelve 1955 1959 the fraction of time series assigned to type 1 exceeded 0 6 except in 2010 type 2 is common to all time periods except 1935 1939 since 1950 type 3 is increasingly prevalent before 1950 this type was not observed in the few stations for which records were available trend analyses the records of individual series for each type of the stations plotted in fig 5 are a sample of the records for one spatial domain the aspen parkland ecoregion see fig 6 for location of ecozone 7 in fig 5 the period prior to 1960 is omitted for the purpose of clarity and because that period is data sparse as shown in fig 4 fig 5 shows that the record of each site are unique but that there is some evident structure among the series where types 2 and 3 become common such as around 1980 and again in 1989 in the examples analyzed below spatial groupings of time series similar to the example in fig 5 are the basic input but they differ between spatial groupings logistic regression is used to determine if the fraction of each type changes over five year periods using this methodology it is possible to determine trends in individual regions at differing spatial scales although five year periods were used other temporal groupings could be used five years was chosen as the temporal grouping because the number of available stations is small in some years an ecozone is a broad biogeographic division of the earth surface the prairie ecozone spans the southern portion of the prairie provinces canadian council on ecological areas 2018 the location of all 169 hydrometric stations in this study are shown within the prairie and adjacent ecozones in fig 6 as described above the trend analyses used logistic regression of the fraction of stations in each type against the year for each spatial grouping at this largest scale the trends in the fractions of events in five year periods for the 3 different types for the prairie ecozone are compared to those in non prairie ecozones fig 7 and summarized in table 1 only the type 2 patterns have significant trends declining in both prairie and non prairie ecozones p 0 05 fig 7 table 1 the prairie ecozone is not uniform and there is a strong link between ecology and climate using the ecoregion divisions of the prairie ecozone we can examine trends in types spatially ecoregions are smaller regions nested within ecozones characterized by distinctive landforms or assemblages similar climate vegetation soils water and human uses marshall et al 1999 the stations were categorized into groups based upon ecoregion boundaries both within the prairies and in the adjacent ecoregions significant trends p 0 05 were detected in all three types table 2 fig 8 table 2 provides a numbered cross reference to the ecoregions shown in fig 6 type 1 increased cyan in the southwest portion of the prairies cypress uplands and mixed grasslands and decreased red in the aspen parkland which arcs across the northern edge of the prairie ecozone fig 8a type 2 significantly decreased in the cypress uplands and mixed grassland and also in the boreal transition which lies to the north of the aspen parkland fig 8b significant increases in type 3 occur in the aspen parkland interlake plain and mid boreal lowland and decreases in the lake manitoba plain fig 8c in many ecoregions there were no trends in some cases this could simply be because the number of stations is too small 4 discussion at the broadest spatial scale i e the prairie and non prairie ecozones the trend tests shown in fig 7 and summarized in table 1 show overall increases in the fraction of type 1 station years in both the prairie ecozone and the non prairie areas but the increases while large are not significant type 2 station years are significantly decreasing in both the prairie and non prairie areas while there are no changes in the frequency of type 3 in either area fig 7 table 1 examining trends at this coarse level across such a region with strong precipitation gradients and landscape variations pools data in a way that does not capture the structure of the area such as reported by asong et al 2015 2016 using ecoregions a logical spatial disaggregation of the ecozone the changes detected are not uniform as shown by the summary statistics listed in table 2 significant increases in type 1 station years occurred in the cypress and western alberta uplands and decreases in the aspen parkland significant increases in type 2 station years occurred in the aspen parkland and decreases in cypress uplands mixed grasslands interlake plain and boreal transition ecoregions type 3 station years increased in aspen parkland interlake plain and mid boreal lowland ecoregions and decreased the southwest manitoba uplands the spatial structures of these significant changes in the fraction of these types are shown in fig 8 in some ecoregions there is a change in fraction between one type and another and in others two types contribute to a significant change in the third only in the aspen parkland are there significant changes in the fractions of all three types the changes in type 1 in fig 8 suggest that the western ecoregions are becoming dryer while the decreased fraction of type 1 in the aspen parkland suggests a shift to wetter conditions the changes in type 2 in fig 8 show a band of decreasing type 2 to the north of the aspen parkland where type 2 is increasing the decrease in type 2 in the cypress uplands and mixed grasslands in fig 8 reflects a shift to more type 1 and type 3 station years the increase in type 2 and type 3 station years in the aspen parkland and type 3 in other ecoregions reflects an increase in wetter conditions in the northeast these results indicate that the ecoregions are affected by the west east gradient in precipitation across the prairie and the steepening of drying in the west and wetting in the east millett et al 2009 and reflect other changes in precipitation patterns that have been reported the method however does not allow quantification of the change in terms of commonly used streamflow metrics in addition to changes in precipitation variations in winds likely contribute to the drying of the western prairies warm dry chinook winds frequently descend the east slope of the rocky mountains in southern alberta in late fall and winter longley 1967 nkemdirim 1996 these winds cause substantial redistribution of the seasonal snowpack by wind transport in transit blowing snow sublimation losses in situ melt and evaporation from ponded meltwater without recharging soil moisture or contributing to winter or spring runoff macdonald et al 2018 the effects of these westerly winds frequently extend eastward and are at a maximum at 50 n nkemdirim 1996 chinooks directly affect the fescue grassland but changes in frequency or magnitude could impact the mixed grassland and cypress uplands as warm days and dry winds result in increased sublimation of winter snow macdonald et al 2018 the aspen parkland is the northern boundary of the prairie ecozone and represents a transition in climatic patterns from dry to wetter conditions and from prairie to forest anderson 1983 ireson et al 2015 identified the boreal plains ecozone as an area of ecological sensitivity in the 21st century changes in climate will result in drier conditions with frequent disturbances and shifts in vegetation the forest will contract to the north warming and the drying will affect many processes ireson et al 2015 the results shown here reflect this projection in the western part of the boreal plains as the increasing frequency of type 1 station years in the west and a decreasing frequency of type 2 station years in the boreal transition ecoregion but an increased frequency of type 3 station years in the eastern portion of the boreal plains fig 8 precipitation has increased in parts of the study area where a shift to wetter hydrographs was observed this is consistent with the extreme shifts to wetter hydrographs found in smith creek in the eastern portion of this region where runoff efficiency increased 14 fold over 30 years dumanski et al 2015 significant increases in precipitation were found in basins near the cypress uplands and in clusters of basins in the mid boreal uplands and interlake plains reflecting modelled higher precipitation armstrong et al 2015 in the eastern and northwest portion of the aspen parkland consistent with the trend to wetter types of years presented here lakes particularly those in closed basins across the prairies are sensitive indicators of changes in climate mason et al 1994 sixteen closed basin lakes in the prairie pothole region demonstrated regional differences in the annual series of water levels van der kamp et al 2008 increases in water level were greater in the eastern portion of the arc of the aspen parkland ecoregion than the western portion van der kamp et al 2008 paralleling the trends reported here for streamflow types the methodology presented here only determines the shifts in type and not their cause changes in hydrology can result from changes in precipitation and or from changes in land use regional scale statistical studies of prairie wetlands can be used to relate to regional climate data but would have limited predictive capability when both climate and land use are changing conly and van der kamp 2001 in the conversion of a prairie basin from cultivation to grass wetlands became dry van der kamp et al 2003 smith creek an aspen parkland catchment that had undergone drainage of half of its depressional storage saw a shift in runoff generating processes from predominately snowmelt to predominately rainfall and a 50 increase in the number and magnitude of multiple day rainfall events despite having no trends or change points in annual precipitation depth dumanski et al 2015 the annual streamflow volume increased 14 fold but due to the complex set of changes occurring in the basin this could not be fully attributed to either land use or climate change in the american great plains recent increases in precipitation led to a disproportionately large increase in streamflow in ten basins in oklahoma kansas and nebraska that have mostly natural unregulated streamflow garbrecht et al 2004 trends in water storage based on gravity recovery and climate experiment grace satellites identified a progression from a dry to a wet period in the eastern prairies great plains and an area of surface water drying in the west rodell et al 2018 the methods presented here allow trend assessments of prairie streams that flow only intermittently that have data only for a portion of the year and that may not have consistent observation periods other studies of trends in streamflow in canada have focused on sites where the flow is perennial and for which suitable methods exist whitfield and cannon 2000 zhang et al 2001 yue et al 2003 ehsanzadeh and adamowski 2007 the results obtained from different spatial domains indicate that there is considerable spatial structure to the changes the results are different from the methods used for perennial streams but are consistent with other studies that have examined changes in portions of the prairies such as van der kamp et al 2008 the overall changes are also consistent with understanding of the precipitation gradients of the prairies becoming larger both west to east and south to north over the period examined the observed changes do not attribute the cause of the wetter hydrographs in the parkland zone but it can be noted that this is also in the prairie pothole region which has substantial wetland coverage and a high rate of recent wetland drainage modelling results in this region suggest that wetland drainage can increase streamflow volumes substantially pomeroy et al 2014 5 conclusions streamflow records in the canadian prairies are not well suited for common trend assessment methods that gap is partially filled using a novel alternative approach to trend assessment that overcomes the data availability issues associated with the conventional methods that have been developed for perennial streams the method uses logistic regression of spatial groupings of stations that have been classified into types using dynamic time warping these streamflow records may only have records for a portion of the year and may contain days with zero flow the method also allows inclusion of records from stations which do not have an overlapping record and with missing years of observations the method can detect trends in the fraction of each type but it does not yield trends in magnitude or timing or other common streamflow metrics but it does identify changes in cumulative runoff at a coarse scale the method marks an advance in statistical analysis of hydrological change by providing trends in runoff patterns in groups of basins and so is suitable for detecting changes that occur in a spatially coherent group in that it is based upon runoff patterns rather than the usual point metrics the results illustrate that trends in runoff patterns across the prairies exist at the ecozone and ecoregion levels the changes that are detected reflect the climatology of the prairies by reflecting the strong west to east precipitation gradient reported in the literature and the trends in precipitation reported by others this analysis confirms that the hydrographic wetness gradient appears to be increasing the results also confirm studies of climate driven changes in the aspen parkland where increasing water levels in closed basins have been reported van der kamp et al 2008 overall from 1920 to 2015 the streams in the west and south of the prairies have shifted to drier conditions while those in the east and north have shifted to wetter conditions a result consistent with many scenarios of climate change for the region the methods can be applied to any spatial delineations that can be used to group stations here ecological boundaries have been used the method could be applied to provinces drainage basins or arbitrary spatial areas it may be possible to extend this method to examine changes in agricultural practices by adapting the method so that basin grouping is made based on land use status over time to make it possible to better understand impacts of changes to agricultural water storage e g mcgee et al 2012 shook et al 2015 cultivation e g van der kamp et al 2003 or wetland drainage e g dumanski et al 2015 credit authorship contribution statement paul h whitfield conceptualization software visualization writing review editing k r shook conceptualization writing review editing j w pomeroy conceptualization writing review editing declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements funding was provided by the natural science and engineering research council of canada through discovery grants and through the changing cold regions network and by the canada research chairs the canada excellence research chairs programs and the global water futures program streamflow values were obtained from water survey of canada environment and climate change canada we appreciate being able to use the r package trend which was provided by chrisoph frei the manuscript has benefited from the comments and suggestions of dr geoff pegram and other anonymous reviewers 
5826,changes in canadian prairie streamflow particularly trends over time have not been well studied but are particularly relevant for food and water security in this vast agricultural region streamflow records for this region are often unsuitable for conventional trend analysis streams are often intermittent and have only a few days per year with flow and stations operate only during the warm season because of a lack of flow during the very cold prairie winter this study takes an alternative approach streamflow data for the period from march to october for individual years between 1910 and 2015 from 169 hydrometric stations from the prairie and adjacent areas in canada were converted to annual cumulative runoff series these 5895 individual station years were then clustered based upon their shape using dynamic time warping three clusters of cumulative annual runoff were found the first and most common type has infrequent days with flow and low total annual runoff 0 50 mm the second has more days with flow and slightly greater runoff 48 175 mm and the least common third type has the fewest days without flow includes perennial streams and has much greater annual runoff 173 mm for each hydrometric station a time series of annual cluster memberships was created trends in the fractions of cluster types were determined using logistic regression with spatial groupings of these time series over five year periods trends in the fractions of types within an ecoregion indicate spatially consistent and organized changes in the pattern of runoff over the region in the western canadian prairies particularly in the mixed grassland and cypress upland ecoregions drying is occurring as indicated by the increased frequency of the dry type in the northern and eastern canadian prairies conditions are shifting to greater runoffs particularly in the aspen parkland where the wet types are increasing in frequency keywords prairie streams trend assessment dynamic time warping logistic regression ecoregion 1 introduction changes in the hydrology of the canadian prairies have been chronically understudied because of lack of continuous streamflow records and statistical methods suitable for zero rich partial year records many studies of trends and changes in hydrology have avoided stations from this region because the streams often do not have continuous streamflow records the available records contain large numbers of days having zero discharge and many sites are operated only during the warm season because of a lack of streamflow during the cold winters many studies have assessed annual trends and seasonal changes in rivers across canada but excluded the prairies for the above reasons e g gan 1998 whitfield and cannon 2000 zhang et al 2001 yue et al 2003 ehsanzadeh and adamowski 2007 statistical methods suitable for perennial streams are not always applicable to temporary streams court 1952 buttle et al 2012 perennial rivers on the prairies generally originate in the canadian rockies are exotic to the region they cross such that their hydrological characteristics are vastly different than those that originate locally macculloch and whitfield 2012 daily streamflows in semi arid and arid regions are characterized by extended sequences of days with zero flow seasonality high autoregressions and extreme events such as floods and droughts ghahramani et al 2012 many prairie basins have non contributing areas areas within the basin that only infrequently generate stream flows stichling and blackwell 1957 spence et al 2010 shook et al 2015 because of low precipitation and relief and the presence of depressions in the post glacial landscape many prairie streams have no baseflow because of the presence of low permeability glacial tills in their basins shook et al 2015 very few of the catchments across the prairies can be considered as reference sites whitfield et al 2012 because of widespread landscape and wetland drainage changes associated with agriculture conly and van der kamp 2001 dumanski et al 2015 separating the effects of climate variation from those of land use change and drainage impact is difficult a valid study of hydrologic change across the prairies must address these issues the prairie ecozone exists because of aridity mcginn 2010 and is covered by farmland 94 in a similar fashion to the northern us great plains little of the original native grassland or woodland vegetation remains and many of the wetlands have been lost the vegetation pattern of the north american grasslands an area of predominantly grass and parkland east of the rocky mountains consists of the steppe with shortgrass on the west extending into the palliser triangle in canada and the prairie with tallgrass on the east exist because of climatic gradients borchert 1950 the prairie climate is characterized by variability swings in temperature and precipitation occur within and between seasons as the result of the interactions between air masses wheaton 1998 the climatic gradients are largely in precipitation but temperature particularly the location of the 0 c isotherm also plays an important role precipitation gradients exist because of prevailing airflows from the west consequent rain shadows in the lee of the canadian rockies and convergence and frontal penetration from the south and suggest that changes in the regional westerly airflow have affected the boundary between forest and grassland over time borchert 1950 rosenberg 1987 edwards et al 2008 the west to east increasing precipitation gradient across alberta saskatchewan and manitoba longley 1953 ferguson and storr 1969 lemmen et al 1998 bonsal et al 1999 was attributed to storm tracks longley 1974 asong et al 2015 2016 delineated the prairie into five homogenous precipitation regions southeast prairies northern south central prairies central west prairies and foothills of the canadian rockies tracking the precipitation gradient the spatial and temporal variability of precipitation are not consistent over the prairies correlation between stations is small from may to september as local convection plays a prominent role in generating summer precipitation longley 1974 drought in the prairies is frequent mcginn 2010 lemmen et al 1998 in the adjacent u s northern great plains the situation is similar the climate is extreme and variable weather conditions change within a day between days between seasons and between years rosenberg 1987 extended wet and drought events which can vary greatly with season and sub regions are common to the great plains and their results suggest that the recent wet period may be a part of natural variability on a very long time scale ryberg et al 2016 ecological structure and water availability across the prairies is spatially complex and changes may not be discernable at large scales from 1890 to 1990 annual precipitation increased in southern canada by 13 but predominantly in eastern canada groisman and easterling 1993 precipitation increases in the prairies over the 20th century were the smallest in canada zhang et al 2000 the seasonality of precipitation in canada changed between 1976 and 1995 summer precipitation in the northern portion of the prairies increased whitfield et al 2002 precipitation increased by 49 mm or about 9 and the west east gradient in precipitation across the prairies steepened with drying in the west and the east becoming wetter millett et al 2009 while precipitation intensities have not increased the number of events greater than 0 5 mm have increased by 16 per year over the past 75 years and include an increased number of days with rain akinremi et al 1999 summer single day rainfall events have not decreased in frequency but the number of multiday events have increased in the prairies over the 20th century shook and pomeroy 2012 the number of multiday rain events has increased by 50 in eastern saskatchewan dumanski et al 2015 the increases in the numbers and lengths of multiday rain events suggest that the spatial extent of storms has increased the temporal variability of daily snowfalls over the 20th century has also increased which affects snowfall accumulation shook and pomeroy 2010 the fraction of precipitation falling as rain rather than as snow in non summer months increased significantly by 0 095 yr 1901 2000 and 0 17 yr 1951 2000 shook and pomeroy 2012 changes in precipitation across the arid prairies are expected to drive hydrological changes for example multiday extreme events rather than individual events were responsible for recent flooding in the saskatchewan and athabasca river basins brimelow et al 2014 annual mean temperatures in canada increased by between 0 5 and 1 5 c between 1900 and 1998 and the greatest warmings took place in the prairies zhang et al 2000 and have continued debeer et al 2016 in the prairie pothole region north south temperature gradients have changed minimum temperatures have increased by 1 0 c more in winter than in summer and maximum temperatures have cooled by 0 15 c between 1906 and 2000 millett et al 2009 using common trend detection techniques ehsanzadeh et al 2016 investigated the effects of precipitation change and landscape changes on streams in the assiniboine and saskatchewan river basins examining trends and change points did not find detectable changes in the streamflow metrics commonly used for continuously flowing streams statistical analyses of daily data are problematic for intermittent streams because of the preponderance of zero flows rendering basic statistical analyses inappropriate alternative approaches are needed annual cumulative flow curves and cumulative annual curves are a useful method for comparing records between stations and between years within stations amongst perennial streams the shapes and timings of the annual cumulative flow curves are remarkably consistent for prairie streams the shapes of these curves are consistent but not their timing macculloch and whitfield 2012 the number of days with zero flow and the duration of zero flow periods for prairie intermittent streams were affected by basin size and precipitation large basins had the fewest zero flow days the shortest periods of continuous flow and a negative relationship between annual precipitation and zero flow days costigan et al 2015 therefore the shape of the annual cumulative runoff curves is a distinguishing feature that can be classified the methodology using dynamic time warping for classifying the shapes of annual runoff curves and clustering of them into types is discussed in the section on methods the dynamic time warping algorithm dtw wang and gasser 1997 keogh and ratanamahatana 2005 was used to measure the distance between two time series dtw exploits the pattern of shifts between two time series to measure their alignment as a complementary tool sakoe and chiba 1978 proposed the method for speech recognition and it has since become widely used for measuring the similarity between two time series berndt and clifford 1994 wang and gasser 1997 an example of aligning two time series is shown in fig 1 where two non overlapping ecg time series are compared unlike euclidean distance where series are compared point to point dtw compares the curves based on their features and aligns the series giorgino 2009 traditional trend analysis methods examine annual series of one or more streamflow attributes such as non parametric trend tests of mean annual flow the method presented here examines changes in the frequency of types the objectives of this paper are to demonstrate this methodology and to apply it to assess regional differences and temporal changes in the frequency of classified runoff regimes of prairie basins trends are determined based on changes in frequency of annual classifications in spatially grouped stations and temporal blocks five years of particular patterns in annual cumulative streamflow ecologically defined areas are used to demonstrate the methodology and trends in frequency in ecoregions by examining changes occurring in different areas of the prairies 2 methods 2 1 data the discharge of a stream is expressed as a runoff depth per basin unit area in mm to remove basin size effects the data for these stations was obtained from ecdataexplorer https www canada ca en environment climate change services water overview quantity monitoring survey data products services explorer html for natural streams to exclude any that were regulated the study region includes the prairie ecoregion and adjacent ecoregions in canada the latter are included so the pattern of accumulated runoff in these areas are known within the clustering this was based upon the view that changes along the margins are likely to be similar to existing patterns in adjacent areas this selection of stations provides the maximum number of sample catchments within the prairies and the surrounding regions the intention was that hydrological types from adjacent non prairie catchments would include sample streams along the margin of the prairies 2 2 analyses all the analyses were done using r r development core team 2014 the r package flowscreen dierauer et al 2017 was used to assess data quality stations determined to have significant change points were eliminated as they would not be suitable for trend analysis the r package dtwclust sarda espinosa 2017 was used to perform dynamic time warping as described below to cluster curves based upon their shapes and inflections the method used here builds from the work of macculloch and whitfield 2012 the annual cumulative runoff time series for each individual year for which there exists a record between 1910 and 2015 from 169 hydrometric stations on unregulated streams from prairies and adjacent regions a total of 5895 individual station years are clustered based upon their shape into a type following the algorithm 1 calculate the daily depth of runoff dividing the daily flows by basin area 2 calculate the cumulative runoff time for the interval of day 60 to day 300 these day numbers correspond to march 1st to october 31 in non leap years for each station and each year these dates span the annual operating period for seasonal gauges only the data available for this interval were used for all streams including perennial streams each case is considered a station year 3 standardize each annual cumulative runoff series to have a mean of zero and a variance of 1 4 cluster each series using dynamic time warping to classify each station year as one of type 1 2 or 3 creating a time series of annual types 5 group aggregate the individual station time series of types across a spatial domain over five year periods this results in a time series of the frequencies of the three types 6 determine trends in the fraction of stations in each type by logistic regression over five year periods for each spatial domain for each station the annual cumulative runoff series for each complete year was generated as is plotted in fig 2 for four sample stations one distinguishing feature of prairie streams fig 2a c is that the annual cumulative runoff plots have a highly variable step like temporal structure due to the intermittency of flow and the dominance of spring snowmelt runoff or extreme event rainfall runoff processes unlike the smoother curves associated with perennial streams which have baseflow and runoff has a wider variety of sources fig 2d macculloch and whitfield 2012 each annual cumulative runoff series was standardized to z scores which have a zero mean and unit variance this is done so that patterns rather than variations in the magnitude of runoff would be captured by the analysis only those years with no missing data in the march to october period were included the clustering of annual cumulative streamflow time series was done using dynamic time warping dtw wang and gasser 1997 keogh and ratanamahatana 2005 with the r package dtwclust sarda espinosa 2017 the outputs are clusters of annual cumulative runoff curves based on partial year records as described above the 169 stations used do not have records for all of the same years non overlapping and inconsistent periods of records may falsely induce trends in mann kendall type climate change studies spatial binning creates a grouping of stations similar in location and for which common responses to climate and landuse should exist since the individual records are being converted to a type which can be counted and converted to a fraction some of the issues of periods of record are of a lesser concern there is a trade off between the temporal block size and the number of station years of data for large areas where there could be many stations such as the entire prairies small block sizes would be possible but for smaller areas with fewer stations such as for several ecoregions the number of available sites is smaller and the block size needs to be larger five year blocks were chosen as a compromise to get large enough counts to determine frequency and to be consistent across different spatial groupings the result of clustering converts the daily runoff values to annual types and the spatial grouping and temporal blocking results in time series of the fraction of each type the trends in cluster types in those series were determined using logistic regression which is an appropriate method to assess trends in discrete count or proportion data frei and schär 2001 schmidli and frei 2005 the r package trend frei 2013 was used to perform the trend tests and produce the odds ratio the constant effect where ratio values 1 are increasing and values 1 are decreasing over time the probability and the estimate of over dispersion which is a measure of inflated variance for the poisson model of count data large values of the overdispersion would indicate that the poisson model is not a good fit of the data 3 results several clustering solutions were evaluated using between five and fifteen clusters the solution chosen used the minimum five clusters the clustering patterns from all solutions considered were quite stable in each solution large numbers of cases were placed in clusters 1 and 2 and the balance of being placed into other small clusters that provided more separation within this complex mixture of accumulation curves fig 3 c clusters 1 and 2 are considered to be of types 1 and 2 respectively the smaller clusters 3 5 were combined into a single type 3 three types of annual cumulative runoff are shown in fig 3 the first has early and low total runoffs 0 50 mm fig 3a the second has a short period of sustained runoffs and greater totals 48 175 mm fig 3b while the third type has both early and late runoff events and annual runoff totals that are very large 173 mm fig 3c the shapes of the type 1 and 2 curves are very simple with small numbers of inflections the clusters regrouped into type 3 have accumulation curves with greater complexity the differences among the curves types are shown by the plots in fig 3c where the three types are rescaled and the 2011 accumulation curves are plotted in black the year 2011 was chosen as it was particularly wet on the prairies and resulted in flooding in some areas shook and pomeroy 2016 stadnyk et al 2016 blais et al 2016 the type 1 curves have only a few inflection points and during 2011 most take place around day 100 fig 3a type 2 curves have more inflection points and are more variable in time but in 2011 they flatten after day 200 fig 3b type 3 curves are more complex and diverse the 2011 cases highlight this fig 3c the number of data sets in each type cluster has changed dramatically over time as is plotted in fig 4 the maximum number of data sets occurred in 1985 140 stations 700 station years prior to 1960 the number of stations ranged between one 1910 1914 and twelve 1955 1959 the fraction of time series assigned to type 1 exceeded 0 6 except in 2010 type 2 is common to all time periods except 1935 1939 since 1950 type 3 is increasingly prevalent before 1950 this type was not observed in the few stations for which records were available trend analyses the records of individual series for each type of the stations plotted in fig 5 are a sample of the records for one spatial domain the aspen parkland ecoregion see fig 6 for location of ecozone 7 in fig 5 the period prior to 1960 is omitted for the purpose of clarity and because that period is data sparse as shown in fig 4 fig 5 shows that the record of each site are unique but that there is some evident structure among the series where types 2 and 3 become common such as around 1980 and again in 1989 in the examples analyzed below spatial groupings of time series similar to the example in fig 5 are the basic input but they differ between spatial groupings logistic regression is used to determine if the fraction of each type changes over five year periods using this methodology it is possible to determine trends in individual regions at differing spatial scales although five year periods were used other temporal groupings could be used five years was chosen as the temporal grouping because the number of available stations is small in some years an ecozone is a broad biogeographic division of the earth surface the prairie ecozone spans the southern portion of the prairie provinces canadian council on ecological areas 2018 the location of all 169 hydrometric stations in this study are shown within the prairie and adjacent ecozones in fig 6 as described above the trend analyses used logistic regression of the fraction of stations in each type against the year for each spatial grouping at this largest scale the trends in the fractions of events in five year periods for the 3 different types for the prairie ecozone are compared to those in non prairie ecozones fig 7 and summarized in table 1 only the type 2 patterns have significant trends declining in both prairie and non prairie ecozones p 0 05 fig 7 table 1 the prairie ecozone is not uniform and there is a strong link between ecology and climate using the ecoregion divisions of the prairie ecozone we can examine trends in types spatially ecoregions are smaller regions nested within ecozones characterized by distinctive landforms or assemblages similar climate vegetation soils water and human uses marshall et al 1999 the stations were categorized into groups based upon ecoregion boundaries both within the prairies and in the adjacent ecoregions significant trends p 0 05 were detected in all three types table 2 fig 8 table 2 provides a numbered cross reference to the ecoregions shown in fig 6 type 1 increased cyan in the southwest portion of the prairies cypress uplands and mixed grasslands and decreased red in the aspen parkland which arcs across the northern edge of the prairie ecozone fig 8a type 2 significantly decreased in the cypress uplands and mixed grassland and also in the boreal transition which lies to the north of the aspen parkland fig 8b significant increases in type 3 occur in the aspen parkland interlake plain and mid boreal lowland and decreases in the lake manitoba plain fig 8c in many ecoregions there were no trends in some cases this could simply be because the number of stations is too small 4 discussion at the broadest spatial scale i e the prairie and non prairie ecozones the trend tests shown in fig 7 and summarized in table 1 show overall increases in the fraction of type 1 station years in both the prairie ecozone and the non prairie areas but the increases while large are not significant type 2 station years are significantly decreasing in both the prairie and non prairie areas while there are no changes in the frequency of type 3 in either area fig 7 table 1 examining trends at this coarse level across such a region with strong precipitation gradients and landscape variations pools data in a way that does not capture the structure of the area such as reported by asong et al 2015 2016 using ecoregions a logical spatial disaggregation of the ecozone the changes detected are not uniform as shown by the summary statistics listed in table 2 significant increases in type 1 station years occurred in the cypress and western alberta uplands and decreases in the aspen parkland significant increases in type 2 station years occurred in the aspen parkland and decreases in cypress uplands mixed grasslands interlake plain and boreal transition ecoregions type 3 station years increased in aspen parkland interlake plain and mid boreal lowland ecoregions and decreased the southwest manitoba uplands the spatial structures of these significant changes in the fraction of these types are shown in fig 8 in some ecoregions there is a change in fraction between one type and another and in others two types contribute to a significant change in the third only in the aspen parkland are there significant changes in the fractions of all three types the changes in type 1 in fig 8 suggest that the western ecoregions are becoming dryer while the decreased fraction of type 1 in the aspen parkland suggests a shift to wetter conditions the changes in type 2 in fig 8 show a band of decreasing type 2 to the north of the aspen parkland where type 2 is increasing the decrease in type 2 in the cypress uplands and mixed grasslands in fig 8 reflects a shift to more type 1 and type 3 station years the increase in type 2 and type 3 station years in the aspen parkland and type 3 in other ecoregions reflects an increase in wetter conditions in the northeast these results indicate that the ecoregions are affected by the west east gradient in precipitation across the prairie and the steepening of drying in the west and wetting in the east millett et al 2009 and reflect other changes in precipitation patterns that have been reported the method however does not allow quantification of the change in terms of commonly used streamflow metrics in addition to changes in precipitation variations in winds likely contribute to the drying of the western prairies warm dry chinook winds frequently descend the east slope of the rocky mountains in southern alberta in late fall and winter longley 1967 nkemdirim 1996 these winds cause substantial redistribution of the seasonal snowpack by wind transport in transit blowing snow sublimation losses in situ melt and evaporation from ponded meltwater without recharging soil moisture or contributing to winter or spring runoff macdonald et al 2018 the effects of these westerly winds frequently extend eastward and are at a maximum at 50 n nkemdirim 1996 chinooks directly affect the fescue grassland but changes in frequency or magnitude could impact the mixed grassland and cypress uplands as warm days and dry winds result in increased sublimation of winter snow macdonald et al 2018 the aspen parkland is the northern boundary of the prairie ecozone and represents a transition in climatic patterns from dry to wetter conditions and from prairie to forest anderson 1983 ireson et al 2015 identified the boreal plains ecozone as an area of ecological sensitivity in the 21st century changes in climate will result in drier conditions with frequent disturbances and shifts in vegetation the forest will contract to the north warming and the drying will affect many processes ireson et al 2015 the results shown here reflect this projection in the western part of the boreal plains as the increasing frequency of type 1 station years in the west and a decreasing frequency of type 2 station years in the boreal transition ecoregion but an increased frequency of type 3 station years in the eastern portion of the boreal plains fig 8 precipitation has increased in parts of the study area where a shift to wetter hydrographs was observed this is consistent with the extreme shifts to wetter hydrographs found in smith creek in the eastern portion of this region where runoff efficiency increased 14 fold over 30 years dumanski et al 2015 significant increases in precipitation were found in basins near the cypress uplands and in clusters of basins in the mid boreal uplands and interlake plains reflecting modelled higher precipitation armstrong et al 2015 in the eastern and northwest portion of the aspen parkland consistent with the trend to wetter types of years presented here lakes particularly those in closed basins across the prairies are sensitive indicators of changes in climate mason et al 1994 sixteen closed basin lakes in the prairie pothole region demonstrated regional differences in the annual series of water levels van der kamp et al 2008 increases in water level were greater in the eastern portion of the arc of the aspen parkland ecoregion than the western portion van der kamp et al 2008 paralleling the trends reported here for streamflow types the methodology presented here only determines the shifts in type and not their cause changes in hydrology can result from changes in precipitation and or from changes in land use regional scale statistical studies of prairie wetlands can be used to relate to regional climate data but would have limited predictive capability when both climate and land use are changing conly and van der kamp 2001 in the conversion of a prairie basin from cultivation to grass wetlands became dry van der kamp et al 2003 smith creek an aspen parkland catchment that had undergone drainage of half of its depressional storage saw a shift in runoff generating processes from predominately snowmelt to predominately rainfall and a 50 increase in the number and magnitude of multiple day rainfall events despite having no trends or change points in annual precipitation depth dumanski et al 2015 the annual streamflow volume increased 14 fold but due to the complex set of changes occurring in the basin this could not be fully attributed to either land use or climate change in the american great plains recent increases in precipitation led to a disproportionately large increase in streamflow in ten basins in oklahoma kansas and nebraska that have mostly natural unregulated streamflow garbrecht et al 2004 trends in water storage based on gravity recovery and climate experiment grace satellites identified a progression from a dry to a wet period in the eastern prairies great plains and an area of surface water drying in the west rodell et al 2018 the methods presented here allow trend assessments of prairie streams that flow only intermittently that have data only for a portion of the year and that may not have consistent observation periods other studies of trends in streamflow in canada have focused on sites where the flow is perennial and for which suitable methods exist whitfield and cannon 2000 zhang et al 2001 yue et al 2003 ehsanzadeh and adamowski 2007 the results obtained from different spatial domains indicate that there is considerable spatial structure to the changes the results are different from the methods used for perennial streams but are consistent with other studies that have examined changes in portions of the prairies such as van der kamp et al 2008 the overall changes are also consistent with understanding of the precipitation gradients of the prairies becoming larger both west to east and south to north over the period examined the observed changes do not attribute the cause of the wetter hydrographs in the parkland zone but it can be noted that this is also in the prairie pothole region which has substantial wetland coverage and a high rate of recent wetland drainage modelling results in this region suggest that wetland drainage can increase streamflow volumes substantially pomeroy et al 2014 5 conclusions streamflow records in the canadian prairies are not well suited for common trend assessment methods that gap is partially filled using a novel alternative approach to trend assessment that overcomes the data availability issues associated with the conventional methods that have been developed for perennial streams the method uses logistic regression of spatial groupings of stations that have been classified into types using dynamic time warping these streamflow records may only have records for a portion of the year and may contain days with zero flow the method also allows inclusion of records from stations which do not have an overlapping record and with missing years of observations the method can detect trends in the fraction of each type but it does not yield trends in magnitude or timing or other common streamflow metrics but it does identify changes in cumulative runoff at a coarse scale the method marks an advance in statistical analysis of hydrological change by providing trends in runoff patterns in groups of basins and so is suitable for detecting changes that occur in a spatially coherent group in that it is based upon runoff patterns rather than the usual point metrics the results illustrate that trends in runoff patterns across the prairies exist at the ecozone and ecoregion levels the changes that are detected reflect the climatology of the prairies by reflecting the strong west to east precipitation gradient reported in the literature and the trends in precipitation reported by others this analysis confirms that the hydrographic wetness gradient appears to be increasing the results also confirm studies of climate driven changes in the aspen parkland where increasing water levels in closed basins have been reported van der kamp et al 2008 overall from 1920 to 2015 the streams in the west and south of the prairies have shifted to drier conditions while those in the east and north have shifted to wetter conditions a result consistent with many scenarios of climate change for the region the methods can be applied to any spatial delineations that can be used to group stations here ecological boundaries have been used the method could be applied to provinces drainage basins or arbitrary spatial areas it may be possible to extend this method to examine changes in agricultural practices by adapting the method so that basin grouping is made based on land use status over time to make it possible to better understand impacts of changes to agricultural water storage e g mcgee et al 2012 shook et al 2015 cultivation e g van der kamp et al 2003 or wetland drainage e g dumanski et al 2015 credit authorship contribution statement paul h whitfield conceptualization software visualization writing review editing k r shook conceptualization writing review editing j w pomeroy conceptualization writing review editing declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements funding was provided by the natural science and engineering research council of canada through discovery grants and through the changing cold regions network and by the canada research chairs the canada excellence research chairs programs and the global water futures program streamflow values were obtained from water survey of canada environment and climate change canada we appreciate being able to use the r package trend which was provided by chrisoph frei the manuscript has benefited from the comments and suggestions of dr geoff pegram and other anonymous reviewers 
5827,quantifying bacteria fluxes and contaminants from the point and nonpoint sources in a watershed are important for the management of water quality and safeguard public health therefore the appropriate characterization of bacteria from different sources is necessary for understanding of fate and transport of bacteria in watersheds however it is challenging to simulate the effects of ph on bacteria such as escherichia coli e coli in the original version of soil and water assessment tool swat this study aimed to augment swat bacteria module to evaluate the potential effect of ph on e coli concentrations we modified swat bacteria module to incorporate ph factor and to check e coli observations from four sites of athabasca river basin the modified swat bacteria model demonstrated a linear relationship between observed and simulated daily e coli data with r2 values found between 0 70 and 0 80 nse 0 59 and 0 68 pbias 7 94 and 17 85 during calibration for all monitoring sites 2010 2012 while during the validation 2013 2014 the performance statistics found to be r2 0 59 0 72 nse 0 55 0 66 pbias 10 22 the results of the sensitivity analysis confirmed that ph is one of the most significant fate factors of e coli the modified swat bacteria module provides an improved estimate of e coli concentration from the river basin this study contributes new insight to e coli modelling therefore the modified swat bacteria model could be a powerful tool for the future regional to global scale model of e coli concentrations thus significantly contribute for the application of effective river basin management keywords modified soil and water assessment tool ph escherichia coli water quality bacteria fluxes 1 introduction surface waters can be contaminated by various rural faecal sources including agricultural fertilizers septic leakage and livestock and wildlife excreta kot et al 2015 marcheggiani et al 2015 sherchand 2013 the urban untreated wastewater discharge and runoff from anthropogenic activities also contribute to surface water contamination and nutrient enrichment ferreira et al 2017 álvarez et al 2017 sanches fernandes et al 2018 waterborne microbial diseases caused by faecal contamination of surface waters are a significant threat to the water quality and biodiversity of aquatic ecosystems wolf et al 2007 marcheggiani et al 2015 whelan et al 2018 the risk that these bacteria cause to human health is an impetus to advance our knowhow of their characteristics sources and environmental factors governing the life cycles therefore quantifying the fluxes of bacteria and contaminants from both point and nonpoint sources in a watershed is significantly important for the management of water resources thus helping to reduce impacts of various faecal sources on surface waters and protect aquatic ecosystems and human health ge et al 2012a b niu et al 2015 in situ monitoring of microbial water quality of river network is spatially not continuous and expensive and time consuming at large scale remote river basin there are only scant monitoring stations in some remote areas especially in semiarid and arid environments or cold climate regions because of difficulties of transport and monitoring as a result proper microbial water quality management and decision making are impeded considerably by the scarcity of data integrated watershed modelling could be useful to generate spatially and temporally continuous concentrations if a modelling is in conjunction with observations and laboratory experiments it can help to improve the understanding of the sources fate and transport of bacteria in the water and deliver effective decision support information for handling large scale treatment processes to achieve sustainable public health management benham et al 2006 dorner et al 2006 baffaut and sadeghi 2010 cho et al 2012 2016a b therefore several models have been developed for simulating fate and transport of bacteria at watersheds such as the hydrologic simulation program fortran hspf benham et al 2006 sadeghi and arnold 2002 the visualising pathogen environmental risk viper model oliver et al 2009 2012 2018 and the soil and water assessment tool swat arnold and fohrer 2005 kim et al 2010 cho et al 2012 2016a b the hspf resolves some degree of spatial heterogeneity in simulation but must be calibrated and validated due to many empirical relationships benham et al 2006 the viper is an empirical model constructed by using biological parameters of faecal excretion and the e coli shedding and die off rate the model accounts dynamically for the accumulation and depletion of e coli burden to land at daily time steps but lack of representation of soil and vegetation heterogeneity the swat is a watershed scale based model that operates on a continuous daily time step neitsch et al 2005 pandey et al 2017 himanshu et al 2017 it has been widely used to simulate the hydrological process nutrient loss runoff and sediment yield into the surface groundwater because it considers explicitly soil land use cover and climate the fate and transport of bacteria in watersheds are very complex processes due to interaction among water soil bacteria vegetation and weather condition it has been found that the e coli concentrations are significantly affected by meteorological conditions marsalek and rochfort 2004 paul et al 2004 petersen et al 2005 kim et al 2007 cho et al 2010 won et al 2010 the effects of solar radiation on the survival of e coli in the natural waters sinton et al 2007 won et al 2010 cho et al 2010 and temperature ross et al 2003 pietikainen et al 2005 ishii et al 2007 therefore the swat model was suitable for the simulation of fecal coliforms bacteria i e e coli concentration transport growth and die off by adding the bacteria module in watersheds sadeghi and arnold 2002 the swat has been widely used to simulate the fate and transport of bacteria in many watersheds across the globe parajuli et al 2009 simulated source specific fecal bacteria in the upper wakarusa watershed in kansas usa coffey et al 2010 estimated pathogen sources and transport in an irish watershed cho et al 2012 modified swat model for predicting fecal coliforms in wachusett reservoir watershed in massachusetts similarly frey et al 2013 simulated waterborne pathogen occurrence in payne river watershed in ontario canada niazi et al 2015 studied pathogen transport and fate modeling in upper salem river watershed in new jersey qiu 2018 modeled fecal indicator bacteria fate and transport in the neshanic river watershed by giving emphasis on temperature solar radiation and sediment on e coli characterization despite many successes the swat model does not consider the effect of ph parameter although ph is a static input parameter therefore no swat based studies have been reported on the role of ph in microbial growth and survival many studies have shown direct ph effects on enzymatic activities and indirect ph effects on activity via substrate speciation ma et al 2017 šimek and cooper 2002 hipsey et al 2008 akkermans et al 2018 showed the combined effect of ph and acetic acid on the microbial growth rate paule mercado et al 2016 found the dependence of e coli growth rate on ph level fumasoli et al 2015 found that the ph can limit any bacterium growing near its thermodynamic limit antoniou et al 1990 indicated that temperature and ph can affect the effective maximum specific growth rate of nitrifying bacteria furthermore the above swat studies have not been applied for the fate and transport of bacteria at large scale river basin in cold climate regions such as the athabasca river basin arb it is found that glacial retreat snow melt and freeze thaw cycles can affect significantly hydrological processes such as stream flow shrestha et al 2017 sediment shrestha and wang 2018 stream temperature du et al 2018 du et al 2019 and nutrient runoff shrestha and wang 2020 therefore cold regions require to specifically be treated due to effects of snowmelt glacial retreat permafrost and freeze thaw cycles on the fate and transport of bacteria given the importance of ph factor and cold climate regions it is critical to incorporate the variety of ph factors on bacterial persistence and growth in these cold environments into a watershed scale swat model the objectives are to provide the ability to predict effect of ph level on e coli bacteria concentrations in the cold climate river basin we developed a new physically based subroutine of ph dynamics within the swat and modified swat bacteria module to couple with ph dynamics hereafter denoted as swat bacteria and test its prediction efficiency using observed e coli data from different sites in the arb 2 materials and methods 2 1 study area the study was conducted in the athabasca river basin arb the study area is shown in fig 1 athabasca river basin is known by cold climate areas it is found in the center of alberta province in canada which have significant contribution to the economy of the province by supplying dependable water provision to various industries and local communities awc 2011 forest is the main landcover accounting for about 82 of the whole basin area followed by agricultural land 9 5 in general agriculture forest coal mining pulp mills traditional oil and gas extraction and oil sand mining are the major industries in the area awc 2013 the mean annual precipitation of the basin ranges from 300 mm from the downstream to over 1000 mm in the headwater whereas the mean annual temperature is recorded 1 8 c in the upper 5 1 c middle and 3 5 c in the lower basin dibike et al 2018 2 2 escherichia coli e coli data the escherichia coli e coli data was observed from four sites of the athabasca river basin january 2010 december 2014 the samples were collected daily in each month at a gaging station from athabasca river d s of devils elbow at winter road crossing ab07dd0105 a athabasca river at old entrance town site left bank ab07ad0100 b athabasca river at town of athabasca ab07be0010 c and athabasca river u s fort mcmurray 100 m above the confluence with horse river left bank ab07cc0030 d as shown in the fig 1 the records of the data are obtained from the government of alberta environment and parks http aep alberta ca water reports data surface water quality data default aspx for the area the weather data are downloaded from https globalweather tamu edu and government of alberta agriculture and forester https agriculture alberta ca acis alberta weather data viewer jsp 2 3 swat model soil and water assessment tool swat is a watershed scale based model that operates on a continuous daily time step neitsch et al 2005 it used to simulate the hydrological process nutrient loss runoff and sediment yield into the surface groundwater it also used to simulate the impacts of landuse and management practices on water quality and quantity in the model the watershed is divided into multiple sub basins based on homogenous landuse slopes and soil types of hydrological response unit hru during the simulation process point and non point source loads from the hru are assumed and the resulting loads are routed through streams to the watershed in the swat version 2000 microbial survival and transport sub model have been added and modified considerably in the swat version 2012 neitsch et al 2005 in this study we incorporated the swat bacteria model subroutine to examine the ability of to predict e coli concentrations by coupling ph dynamics the subroutine predicts e coli concentrations at the hydrological response units based on homogenous landuse soil type and slope class 2 4 modified swat bacteria module for dynamic ph parameter the original version of the swat model available for bacteria modelling operates the overall net rate by considering the growth and die off rates kim et al 2010 however the existing swat model does not consider the dynamic of ph as a factor to provide the ability to predict ph on e coli concentrations we modified a new subroutine for the swat model version 2012 therefore we used the modified swat bacteria module by adding new parameters into the original swat version 2012 rev 664 which has an association with bacteria characteristics bacteria f in order to reproduce ph associated e coli it used to improve the overall prediction accuracy fig 2 from kim et al 2010 the detailed description related to bacteria module can be found pathogens such as e coli are a high tolerance to less level of ph this low ph tolerance could be due to the outbreaks of e coli infections resulted in acidic foods occur presser et al 1998 according to ross et al 2003 and john and rose 2005 the effect of ph on the survival of e coli bacteria has been observed in both saline and freshwater in our model we adopted the computational aquatic ecosystem dynamics model caedym equation to simulate the dynamics of ph effect on e coli concentrations suggested by hipsey et al 2008 and incorporated into the swat module sources code to evaluate the effect of ph on bacteria the following equation has been adopted the caedym equation from hipsey et al 2008 1 k d 1 c phm p h δ m k phm δ m p h δ m the function of ph was developed from simek and cooper 2002 2 f ph 0 001 for p h 3 5 ph 3 5 3 for 3 5 p h 6 5 1 for p h 6 5 where kd is dark death rate at 20 c in the fresh water cphm shows maximum ph toxicity effect can have on mortality effect kphm and δm shows used to mediate sensitivity of mortality to the change of ph and ph indicate the magnitude of ph departure from the neutral ph 7 2 4 1 bacteria survival based on the chick s law soil and water assessment tool swat model used to define e coli and fecal coliform survival stored in the soil stream manure and other places over time using the following equation 3 n n 0 exp μ t where n is the number of bacteria indicators in a given time t cfu 100 ml n0 original number of bacteria indicator cfu μ is bacterial die off rate constant h 1 and t represents time h the die off rates of bacteria are hindered by several factors for instance an increase in the moisture resulted a decrease in the die off rate constant the decay rates are specified each bacteria population for the adsorbed and solution phases for various locations in different landscape soil ponds reservoirs and streams therefore to obtain from each landscape location bacteria decay is exponentially derived with time baffaut and sadeghi 2010 in swat model temperature is one of the variables that determines the die off rate and can be obtained using the following equation 4 μ μ 20 θ t 20 where μ20 refers die off rate at 20 h 1 θ is the temperature correction parameter unitless for the first order decay t shows the temperature in c the value of θ is constant with the relative value is 1 07 while the value of μ20 is considerably varies crane and moore 1986 the water temperature tw was estimated from both average daily minimum and maximum air temperature using the following equation adopted from stefan and preud homme 1993 5 t w 5 0 0 75 x t air where tw is water temperature and tair is average air temperature c 2 4 2 bacteria transport the indicator of bacterial transport is simulated with the assumption of dissolved constituents the algorithm used for bacteria flux in the model swat model can be obtained using the following equation 6 q v c where q is used to show the flux of bacteria per unit cross sectional area cfu cm 2h 1 v shows the velocity of flow cm h 1 and c indicates concentration of bacteria cfu cm 3 2 5 model sensitivity analysis the model sensitivity analysis has been performed to identify the most sensitive parameters prior to the model calibration and validation process the goal of this process was used to estimate the rate of change in the model result with respect to the changes in the inputs the whole simulation process was carried out with the daily time step from january 2010 to december 2014 after model parametrization an automatic calibration was undertaken with a sequential uncertainty fitting sufi2 tool including in the swat calibration and uncertainty programs swat cup for e coli fate and transport sequential uncertainty fitting 2 sufi2 algorithm was employed for e coli concentrations fate and transport also provided the sensitivity of different parameters governing e coli the uncertainty and goodness fit have been assessed by r factor and p factors the value of r factor shows the average thickness of 95ppu band divided by the standard deviation of the measured and observed data its value ranges from 0 to abbaspour et al 2004 abbaspour et al 2007 abbaspour et al 2015 while p factor is a percentage of measured and observed data bracketed by 95 of prediction uncertainty theoretically its value ranges from 0 to 100 worku et al 2017 melaku and wang 2019 hence the p factor of 1 and r factor of 0 indicates the perfection of simulation which corresponds to measured and observed data 2 6 metrics of model performance the modified e coli module was quantified by the coefficient of determination r2 root mean square error rmse nash sutcliffe efficiency nse 1970 and percentage bias pbais the linearity relationship between measured and simulated e coli is indicated by r2 the value ranges from 0 0 to 1 0 goodness of fit was used for the performances assessment of the swat bacteria model table 1 we targeted at least satisfactory performance with the value of r2 higher than 0 50 for the daily e coli prediction on the other hand nse shows the degree to what extent the observed and simulated value of e coli values fit 1 1 line coefficient of determination r2 obtained using the following equation 7 r 2 i 1 n o i o avr p i p avr i 1 n o i o avr 2 i 1 n p i p avr 2 2 where oi is the ith observed value oavr is average observed value pi is the ith modeled value pavr is average modelled value nash sutcliffe efficiency nse is obtained using the following equation 8 nse 1 i 1 n o i p i 2 i 1 n o i o avr 2 where n shows the number of time step days in our case based on nse 1970 the value ranges from to 1 we target the value of nse values for daily e coli prediction was set to better than 0 35 the value closer to 0 shows poor model performance while the value closer to 1 shows perfect model performance according to moriasi et al 2007 the percentage bias is used to indicate the average tendency of observed data smaller or larger than modelled data the optimal value is 0 0 lower value shows accurate model performance it can be obtained using the following equation 9 pbias i 1 n ob s i s i m i 100 i 1 n ob s i negative value indicates the model overestimation bias whereas positive value indicates model underestimation bias 3 results 3 1 performance of the modified bacteria module in swat model in order to test the modified bacteria module the swat model should be calibrated and validated for the stream flow temperature sediment and nutrient runoff these have been done in our previous work the interested readers can refer to references such as stream flow shrestha et al 2017 temperature du et al 2018 sediment shrestha and wang 2018 and nutrient runoff shrestha and wang 2020 in this paper we therefore focus on the calibration and validation of bacteria fate and transport 3 1 1 sensitivity analysis for model performance assessment the whole dataset was divided into the period of calibration 2010 2012 and validation 2013 2014 before the swat model calibration and validation process initially we tested the sensitivity analysis to identify bacteria associated most sensitive parameters a model sensitivity analysis can be done to find the relative response of the model to changes in the values of specific model parameters therefore sensitivity analysis is used to decide the most important parameters that govern system processes the most sensitive bacteria related parameters for the model are shown in table 2 the range of selected parameters are shown in table 3 and swat calibration and uncertainty programs swat cup were performed for calibrating e coli associated most sensitive parameters of the modified swat module in order to calibrate the updated bacteria module associated sensitive parameters latin hypercube one factor at a time lh oat was employed the results of the most sensitive analysis specify that all the measured input parameters have a significant influence on the e coli model prediction therefore during simulation periods 8 most sensitive e coli associated parameters were selected as the model inputs 3 1 2 model calibration and validation based on sensitivity analysis shown in table 3 eight bacteria associated parameters with the highest ranks in the process of simulations were selected for calibration and validation the remaining parameter was set at the default swat model values temperature adjustment factor thbac 1 07 shown in table 3 unlike the existing swat model the calibration results noticed from the modified swat illustrate higher sensitivity values the higher values of the parameters denote that ph significantly influences the concentration of e coli the predicted and observed daily e coli cfu 100 ml calibration period was from 2010 to 2012 and the validation period was from 2013 to 2014 are compared in fig 3 plotting the concentrations of e coli over the given periods of time confirms the oscillatory nature of e coli variation generally fig 3 summarizes the evaluation of statistical parameters on the daily e coli cfu 100 ml simulations for the four water quality monitoring sites the original swat model could not capture the distribution of e coli whereas the modified swat model results was quite evident with successful model performance evaluation for the monitoring sites the comparison of the original and modified swat model performance can be found from fig 3 and table 4 the value of r factor and p factor were found to be 0 51 and 0 62 at ab07dd0105 a monitoring station 0 46 and 0 60 at ab07ad0100 b monitoring station 0 32 and 0 56 at ab07be0010 monitoring c station and 0 34 and 0 67 at ab07cc0030 d monitoring station during calibration respectively the predictive power of the model at the daily time step was therefore successful over the periods as per the guidelines suggested in moriasi et al 2015 and indicates the satisfactory thresholds abbaspour et al 2015 the simulated e coli shows successful in terms of model evaluation statistics in the process of calibration for all the studied monitoring stations fig 3 the daily basis coefficient of determination r2 was found to be 0 78 0 70 0 80 and 0 71 at a b c and d monitoring stations respectively during calibration similarly the value for nash sutcliffe efficiency nse were 0 68 0 62 0 59 and 0 68 for the same monitoring sites respectively on the other hand the percentage bias pbias for the same monitoring stations found to be 7 94 11 79 17 85 and 16 90 respectively indicated in fig 3 and table 4 based on the results shown in fig 3 the modified swat model generally captured e coli cfu 100 ml concentrations for the periods of calibration similarly the value of the r factor and p factor were found to be 0 31 and 0 47 at ab07dd0105 a monitoring station 0 48 and 0 68 at ab07ad0100 b monitoring station 0 33 and 0 63 at ab07be0010 c monitoring station and 0 35 and 0 58 at ab07cc0030 d monitoring station respectively during validation in terms of model performance evaluation during validation periods the simulated results of e coli cfu 100 ml concentrations confirmed successful as shown in fig 3 the value of r2 was found to be 0 72 0 73 0 59 and 0 66 at a b c and d monitoring stations respectively for the period of validation similarly the value of nse was estimated to be 0 65 0 66 0 55 and 0 66 respectively table 4 while the value of pbias was found to be 10 18 22 and 14 respectively during validation for the same monitoring stations therefore the results indicate satisfactory thresholds even though the accuracy of the overall simulations was improved the model tended to overestimate the observed e coli concentrations at some point for all stations given the daily time steps over the validation intervals the model ability to simulate the e coli concentration deemed acceptable the overestimations of e coli cfu 100 ml concentrations in this study were probably due to the limited availability of observations to validate the model at some point during the dry season the model performance indicates underestimated the observed e coli cfu 100 ml in each monitoring site compared to the winter season during summer season access to livestock wildlife and human activity to the stream and leaks become common and consequently contribute to the disturbance of streambed with e coli cfu 100 ml release to the lakes and stream water increases the results of our study were set to the same as presented in thilakarathne et al 2018 in which the concentrations of e coli were found to be high in the summer a report by dibike et al 2018 during winter the concentration of ph reduced due to spring snowmelt which reflects lower ph level of snowpack and hinders snowmelt runoff has on flows in spring therefore the record of relatively low accuracy of the model performance during calibration and validation periods attributed to the uncertainty of observed e coli cfu 100 ml number in lakes and streamed sediment and the input from wildlife based on the results obtained from the comparison between the original and the modified swat model the modified swat model results displayed good model predictive performance than the original swat model in e coli distribution table 4 the model performance was evaluated by scatter plot analysis at each e coli cfu 100 ml monitoring sites fig 4 the slopes for all sites were significantly far from zero inferring that the prediction accuracy was enhanced by the modified e coli module therefore the results of the 1 1 line fitting diagram prove that the modified swat model showed successful predictive power for e coli cfu 100 ml concentration for each monitoring site during model calibration and validation fig 4 the scatters are found to be closer to the 1 1 line during the whole study periods except at some points in each monitoring site which is subject probably due to insufficient observations available for simulation generally it is worth to conclude the modified swat model gives better efficiency for ph associated daily e coli concentrations by adding the new parameters during calibration and validation 3 2 effect of ph on e coli distribution to assess the potential effects of ph on e coli concentrations we plotted ph versus observed and simulated e coli concentrations the e coli concentrations in the river basin shows an inverse relationship with ph dynamics fig 5 the swat bacteria module used only the first decay equation considering temperature as a function i e more e coli die off at a higher temperature can be found in sadeghi and arnold 2002 cho et al 2012 but not for ph dynamics this study presented the need of swat model modifications of the original bacteria module to simulate the effect of ph dynamic on e coli for the river basin the count of e coli shows increasing trend as of decreasing in the ph level when ph level is lowest e coli survival and growth was observed and vice versa when ph level was high e coli growth level was not observed for instance from the fig 5a and b observation at ph level of 4 very little concentrations of e coli was observed however at ph level of about 5 lower level of acid more e coli concentration was observed when 5 ph 8 e coli concentration increased as ph level increased up to a largest value therefore it is worth to say at ph level of about 5 lower level of acid e coli could survive to grow and more e coli concentrations was observed fig 5 like the other controlling factors i e temperature and solar radiation pietikäinen et al 2005 ishii et al 2007 the results of this study prove that ph was one of a major significant controlling factors for e coli growth and the overall concentrations our study was in lined with other scholars presser et al 1998 ross et al 2003 in which presented in their report rapid decline in e coli distribution has been observed at highest ph therefore the rate of e coli growth was high at the minimum ph level 4 discussion the observed data ranges from 5 e coli 100 ml to 240 e coli 100 ml on the other hand the model predicted value was a maximum of 175 e coli 100 ml these may have happened probably due to various influencing factors these unaccountable factors could include the high degree of observed e coli measurements and the challenging nature of trying to realistically emulate e coli spatiotemporal loading patterns in the model for the river basin the absence of consideration for e coli fate and transport within the existing swat model is also a potential source of error for the model results oliver et al 2009 the results confirm the variability and uncertainty that influence e coli modelling and demonstrate how the single unaccountable e coli cfu 100 ml concentrations input can lead to large inconsistencies in model results due to these and other factors modelling e coli is one of the toughest and may lead to error therefore less confidence compared with nutrients sediment and runoff novotny 2002 parajuli 2007 parajuli et al 2009 a reasonable level of e coli cfu 100 ml simulation was achievable nonetheless the inability to justify for the unknown spatiotemporal contaminate sources within and around the river basins affects the model accuracy in theory the accuracy of the model results should infer the reliability and scope of the observed e coli cfu 100 ml data for calibration and validation however scant monitoring stations might limit the data availability to directly evaluate what happens in small scale tributaries for the entire catchment some small variations might have been smoothed or hidden when only the mainstream is monitored given uncertainties in representing e coli cfu 100 ml sources spatial as well as temporal and small number of grasp samples from four monitoring stations for the modified swat model calibration and validation developing suitable functions and parameters with an inclusive of the characteristics as well as the behavior of ph associated e coli into the modified swat model presents an interesting path of the applied research modified swat model successfully simulates measured and modelled e coli cfu 100 ml in the athabasca river basin during the calibration and validation periods 2010 2014 at four monitoring stations based on the swat model predictive efficiency classification moriasi et al 2007 the capability of modified swat model calibration and validation evidenced that the model could capture the e coli cfu 100 ml concentration fate and transport on the daily basis the magnitude of the coefficient of determination r2 confirms the association between measured and modelled e coli cfu 100 ml which indicates the linearity relationship between measured and simulated ph based e coli cfu 100 ml characterization similarly the percentage bias and nse results of this study assure how the tendency of model prediction efficiency to the reality elsewhere previous researchers have reported the fate and transport of temperature associated e coli cfu 100 ml kim et al 2010 2017 chao et al 2016a b which employed the older version of swat model did not consider the hyporheic exchange of e coli transport across the water interface during the in stream process mawdsley et al 1995 and muirhead et al 2006 showed that rainfall in combination with the application of manure has more influence on bacteria transport consequently affect model accuracy some other scholars upgraded swat model by adding the solar radiation associated die off rate but none of them are considered ph as fate factor for bacteria using modified swat bacteria model modified swat model which integrates the complete understanding of the e coli characterization process would help to improve the model performance the original swat model allows the only temperature associated bacteria characterization in this study unlike other studies we adapted a modified version of the swat model to simulate ph associated e coli cfu 100 ml for the athabasca river basin considering the results demonstrates in figs 3 and 4 and prove that modified swat model can be employed in order to predict the potential e coli concentration within the river basin however given a limited number of measured data points attention would be given in the process of interpreting the results assuming the unpredictability that surrounds various temporal moments as well as spatial sources of river basin contamination model performance evaluations recommend that capability to accurately predict e coli cfu 100 ml in water is uncertain quantifying effect of ph on bacteria fluxes and contaminants in a large scale watershed is a challenge for water resources management and help to protect aquatic ecosystems and human health therefore this paper addresses one of aquatic ecosystems and human health management quantifying the effect of ph on bacteria fate and transport in cold river basin analysis results confirms that the ph associated e coli concentrations in different areas like agricultural practices surface runoff human and animal waste as a real changeable to which further individual study could be focused in order to brief detail and accurate information for use of the river basin modelling the present understanding of e coli concentration dynamics in a water is expanding but still is insufficient to develop a swat bacteria model pachepsky and shelton 2011 this is a motive why the observed rather than simulated e coli concentrations in the water bodies were used as the inputs for the effects of ph algorithms in this work testing of such model is necessary to advance modelling dynamics of bacteria concentrations within the water bodies which is important development to advance microbiological water quality modelling the results of this study show that the indicator and sources of microorganism concentrations should be determined where bacteria modelling is used to assist in stream and watershed management decisions 5 conclusions the swat model as a tool for modelling bacteria at the river basin and watershed scale was modified in this study as swat bacteria module to incorporate ph associated e coli fate and transport and concentrations practices distracting water quality the modified swat model was the first attempt to incorporate ph into the swat model in simulating the effects on the e coli concentrations and was applied for the athabasca river basin the modified swat bacteria model predictions for the daily ph based e coli prove there was a linear relationship between measured and simulated data however uncertainty exists as the modified swat bacteria model tended to underestimate the e coli concentrations compared to those observed at the dry season and overestimate the e coli concentrations compared to those of the observed data however it should be clear that it was not the only reason for the over and under estimations the sources of the model error included the limited availability of observed e coli data for model calibration and validation the model prediction capability could be increased by having reliable river basin input data from different sources which is significant to refine the critical source area for e coli overall the e coli based module considering ph as a factor provides a new capacity to improve e coli algorithm of the swat model and can be applied in other catchments and river basins although the uncertainty of e coli considering ph as a factor probably hindered the performance of the modified swat bacteria module this study could be a useful tool for effective decision support information for policymakers and catchment manager for water quality management credit authorship contribution statement tesfa worku meshesha conceptualization data curation formal analysis investigation methodology project administration resources software validation visualization writing original draft writing review editing junye wang conceptualization data curation formal analysis funding acquisition investigation methodology project administration resources software supervision validation writing review editing nigus demelash melaku data curation formal analysis software validation visualization writing review editing declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements the authors would like to thank the alberta economic development and trade for the campus innovates program research chair for the financial support no rcp 12 001 bcaip the water quality data product can be found at http aep alberta ca water reports data surface water quality data default aspx and landuse digital elevation map and other necessary data used for this study can found at http www agr gc ca atlas landu and http nowdata rcc acis org 
5827,quantifying bacteria fluxes and contaminants from the point and nonpoint sources in a watershed are important for the management of water quality and safeguard public health therefore the appropriate characterization of bacteria from different sources is necessary for understanding of fate and transport of bacteria in watersheds however it is challenging to simulate the effects of ph on bacteria such as escherichia coli e coli in the original version of soil and water assessment tool swat this study aimed to augment swat bacteria module to evaluate the potential effect of ph on e coli concentrations we modified swat bacteria module to incorporate ph factor and to check e coli observations from four sites of athabasca river basin the modified swat bacteria model demonstrated a linear relationship between observed and simulated daily e coli data with r2 values found between 0 70 and 0 80 nse 0 59 and 0 68 pbias 7 94 and 17 85 during calibration for all monitoring sites 2010 2012 while during the validation 2013 2014 the performance statistics found to be r2 0 59 0 72 nse 0 55 0 66 pbias 10 22 the results of the sensitivity analysis confirmed that ph is one of the most significant fate factors of e coli the modified swat bacteria module provides an improved estimate of e coli concentration from the river basin this study contributes new insight to e coli modelling therefore the modified swat bacteria model could be a powerful tool for the future regional to global scale model of e coli concentrations thus significantly contribute for the application of effective river basin management keywords modified soil and water assessment tool ph escherichia coli water quality bacteria fluxes 1 introduction surface waters can be contaminated by various rural faecal sources including agricultural fertilizers septic leakage and livestock and wildlife excreta kot et al 2015 marcheggiani et al 2015 sherchand 2013 the urban untreated wastewater discharge and runoff from anthropogenic activities also contribute to surface water contamination and nutrient enrichment ferreira et al 2017 álvarez et al 2017 sanches fernandes et al 2018 waterborne microbial diseases caused by faecal contamination of surface waters are a significant threat to the water quality and biodiversity of aquatic ecosystems wolf et al 2007 marcheggiani et al 2015 whelan et al 2018 the risk that these bacteria cause to human health is an impetus to advance our knowhow of their characteristics sources and environmental factors governing the life cycles therefore quantifying the fluxes of bacteria and contaminants from both point and nonpoint sources in a watershed is significantly important for the management of water resources thus helping to reduce impacts of various faecal sources on surface waters and protect aquatic ecosystems and human health ge et al 2012a b niu et al 2015 in situ monitoring of microbial water quality of river network is spatially not continuous and expensive and time consuming at large scale remote river basin there are only scant monitoring stations in some remote areas especially in semiarid and arid environments or cold climate regions because of difficulties of transport and monitoring as a result proper microbial water quality management and decision making are impeded considerably by the scarcity of data integrated watershed modelling could be useful to generate spatially and temporally continuous concentrations if a modelling is in conjunction with observations and laboratory experiments it can help to improve the understanding of the sources fate and transport of bacteria in the water and deliver effective decision support information for handling large scale treatment processes to achieve sustainable public health management benham et al 2006 dorner et al 2006 baffaut and sadeghi 2010 cho et al 2012 2016a b therefore several models have been developed for simulating fate and transport of bacteria at watersheds such as the hydrologic simulation program fortran hspf benham et al 2006 sadeghi and arnold 2002 the visualising pathogen environmental risk viper model oliver et al 2009 2012 2018 and the soil and water assessment tool swat arnold and fohrer 2005 kim et al 2010 cho et al 2012 2016a b the hspf resolves some degree of spatial heterogeneity in simulation but must be calibrated and validated due to many empirical relationships benham et al 2006 the viper is an empirical model constructed by using biological parameters of faecal excretion and the e coli shedding and die off rate the model accounts dynamically for the accumulation and depletion of e coli burden to land at daily time steps but lack of representation of soil and vegetation heterogeneity the swat is a watershed scale based model that operates on a continuous daily time step neitsch et al 2005 pandey et al 2017 himanshu et al 2017 it has been widely used to simulate the hydrological process nutrient loss runoff and sediment yield into the surface groundwater because it considers explicitly soil land use cover and climate the fate and transport of bacteria in watersheds are very complex processes due to interaction among water soil bacteria vegetation and weather condition it has been found that the e coli concentrations are significantly affected by meteorological conditions marsalek and rochfort 2004 paul et al 2004 petersen et al 2005 kim et al 2007 cho et al 2010 won et al 2010 the effects of solar radiation on the survival of e coli in the natural waters sinton et al 2007 won et al 2010 cho et al 2010 and temperature ross et al 2003 pietikainen et al 2005 ishii et al 2007 therefore the swat model was suitable for the simulation of fecal coliforms bacteria i e e coli concentration transport growth and die off by adding the bacteria module in watersheds sadeghi and arnold 2002 the swat has been widely used to simulate the fate and transport of bacteria in many watersheds across the globe parajuli et al 2009 simulated source specific fecal bacteria in the upper wakarusa watershed in kansas usa coffey et al 2010 estimated pathogen sources and transport in an irish watershed cho et al 2012 modified swat model for predicting fecal coliforms in wachusett reservoir watershed in massachusetts similarly frey et al 2013 simulated waterborne pathogen occurrence in payne river watershed in ontario canada niazi et al 2015 studied pathogen transport and fate modeling in upper salem river watershed in new jersey qiu 2018 modeled fecal indicator bacteria fate and transport in the neshanic river watershed by giving emphasis on temperature solar radiation and sediment on e coli characterization despite many successes the swat model does not consider the effect of ph parameter although ph is a static input parameter therefore no swat based studies have been reported on the role of ph in microbial growth and survival many studies have shown direct ph effects on enzymatic activities and indirect ph effects on activity via substrate speciation ma et al 2017 šimek and cooper 2002 hipsey et al 2008 akkermans et al 2018 showed the combined effect of ph and acetic acid on the microbial growth rate paule mercado et al 2016 found the dependence of e coli growth rate on ph level fumasoli et al 2015 found that the ph can limit any bacterium growing near its thermodynamic limit antoniou et al 1990 indicated that temperature and ph can affect the effective maximum specific growth rate of nitrifying bacteria furthermore the above swat studies have not been applied for the fate and transport of bacteria at large scale river basin in cold climate regions such as the athabasca river basin arb it is found that glacial retreat snow melt and freeze thaw cycles can affect significantly hydrological processes such as stream flow shrestha et al 2017 sediment shrestha and wang 2018 stream temperature du et al 2018 du et al 2019 and nutrient runoff shrestha and wang 2020 therefore cold regions require to specifically be treated due to effects of snowmelt glacial retreat permafrost and freeze thaw cycles on the fate and transport of bacteria given the importance of ph factor and cold climate regions it is critical to incorporate the variety of ph factors on bacterial persistence and growth in these cold environments into a watershed scale swat model the objectives are to provide the ability to predict effect of ph level on e coli bacteria concentrations in the cold climate river basin we developed a new physically based subroutine of ph dynamics within the swat and modified swat bacteria module to couple with ph dynamics hereafter denoted as swat bacteria and test its prediction efficiency using observed e coli data from different sites in the arb 2 materials and methods 2 1 study area the study was conducted in the athabasca river basin arb the study area is shown in fig 1 athabasca river basin is known by cold climate areas it is found in the center of alberta province in canada which have significant contribution to the economy of the province by supplying dependable water provision to various industries and local communities awc 2011 forest is the main landcover accounting for about 82 of the whole basin area followed by agricultural land 9 5 in general agriculture forest coal mining pulp mills traditional oil and gas extraction and oil sand mining are the major industries in the area awc 2013 the mean annual precipitation of the basin ranges from 300 mm from the downstream to over 1000 mm in the headwater whereas the mean annual temperature is recorded 1 8 c in the upper 5 1 c middle and 3 5 c in the lower basin dibike et al 2018 2 2 escherichia coli e coli data the escherichia coli e coli data was observed from four sites of the athabasca river basin january 2010 december 2014 the samples were collected daily in each month at a gaging station from athabasca river d s of devils elbow at winter road crossing ab07dd0105 a athabasca river at old entrance town site left bank ab07ad0100 b athabasca river at town of athabasca ab07be0010 c and athabasca river u s fort mcmurray 100 m above the confluence with horse river left bank ab07cc0030 d as shown in the fig 1 the records of the data are obtained from the government of alberta environment and parks http aep alberta ca water reports data surface water quality data default aspx for the area the weather data are downloaded from https globalweather tamu edu and government of alberta agriculture and forester https agriculture alberta ca acis alberta weather data viewer jsp 2 3 swat model soil and water assessment tool swat is a watershed scale based model that operates on a continuous daily time step neitsch et al 2005 it used to simulate the hydrological process nutrient loss runoff and sediment yield into the surface groundwater it also used to simulate the impacts of landuse and management practices on water quality and quantity in the model the watershed is divided into multiple sub basins based on homogenous landuse slopes and soil types of hydrological response unit hru during the simulation process point and non point source loads from the hru are assumed and the resulting loads are routed through streams to the watershed in the swat version 2000 microbial survival and transport sub model have been added and modified considerably in the swat version 2012 neitsch et al 2005 in this study we incorporated the swat bacteria model subroutine to examine the ability of to predict e coli concentrations by coupling ph dynamics the subroutine predicts e coli concentrations at the hydrological response units based on homogenous landuse soil type and slope class 2 4 modified swat bacteria module for dynamic ph parameter the original version of the swat model available for bacteria modelling operates the overall net rate by considering the growth and die off rates kim et al 2010 however the existing swat model does not consider the dynamic of ph as a factor to provide the ability to predict ph on e coli concentrations we modified a new subroutine for the swat model version 2012 therefore we used the modified swat bacteria module by adding new parameters into the original swat version 2012 rev 664 which has an association with bacteria characteristics bacteria f in order to reproduce ph associated e coli it used to improve the overall prediction accuracy fig 2 from kim et al 2010 the detailed description related to bacteria module can be found pathogens such as e coli are a high tolerance to less level of ph this low ph tolerance could be due to the outbreaks of e coli infections resulted in acidic foods occur presser et al 1998 according to ross et al 2003 and john and rose 2005 the effect of ph on the survival of e coli bacteria has been observed in both saline and freshwater in our model we adopted the computational aquatic ecosystem dynamics model caedym equation to simulate the dynamics of ph effect on e coli concentrations suggested by hipsey et al 2008 and incorporated into the swat module sources code to evaluate the effect of ph on bacteria the following equation has been adopted the caedym equation from hipsey et al 2008 1 k d 1 c phm p h δ m k phm δ m p h δ m the function of ph was developed from simek and cooper 2002 2 f ph 0 001 for p h 3 5 ph 3 5 3 for 3 5 p h 6 5 1 for p h 6 5 where kd is dark death rate at 20 c in the fresh water cphm shows maximum ph toxicity effect can have on mortality effect kphm and δm shows used to mediate sensitivity of mortality to the change of ph and ph indicate the magnitude of ph departure from the neutral ph 7 2 4 1 bacteria survival based on the chick s law soil and water assessment tool swat model used to define e coli and fecal coliform survival stored in the soil stream manure and other places over time using the following equation 3 n n 0 exp μ t where n is the number of bacteria indicators in a given time t cfu 100 ml n0 original number of bacteria indicator cfu μ is bacterial die off rate constant h 1 and t represents time h the die off rates of bacteria are hindered by several factors for instance an increase in the moisture resulted a decrease in the die off rate constant the decay rates are specified each bacteria population for the adsorbed and solution phases for various locations in different landscape soil ponds reservoirs and streams therefore to obtain from each landscape location bacteria decay is exponentially derived with time baffaut and sadeghi 2010 in swat model temperature is one of the variables that determines the die off rate and can be obtained using the following equation 4 μ μ 20 θ t 20 where μ20 refers die off rate at 20 h 1 θ is the temperature correction parameter unitless for the first order decay t shows the temperature in c the value of θ is constant with the relative value is 1 07 while the value of μ20 is considerably varies crane and moore 1986 the water temperature tw was estimated from both average daily minimum and maximum air temperature using the following equation adopted from stefan and preud homme 1993 5 t w 5 0 0 75 x t air where tw is water temperature and tair is average air temperature c 2 4 2 bacteria transport the indicator of bacterial transport is simulated with the assumption of dissolved constituents the algorithm used for bacteria flux in the model swat model can be obtained using the following equation 6 q v c where q is used to show the flux of bacteria per unit cross sectional area cfu cm 2h 1 v shows the velocity of flow cm h 1 and c indicates concentration of bacteria cfu cm 3 2 5 model sensitivity analysis the model sensitivity analysis has been performed to identify the most sensitive parameters prior to the model calibration and validation process the goal of this process was used to estimate the rate of change in the model result with respect to the changes in the inputs the whole simulation process was carried out with the daily time step from january 2010 to december 2014 after model parametrization an automatic calibration was undertaken with a sequential uncertainty fitting sufi2 tool including in the swat calibration and uncertainty programs swat cup for e coli fate and transport sequential uncertainty fitting 2 sufi2 algorithm was employed for e coli concentrations fate and transport also provided the sensitivity of different parameters governing e coli the uncertainty and goodness fit have been assessed by r factor and p factors the value of r factor shows the average thickness of 95ppu band divided by the standard deviation of the measured and observed data its value ranges from 0 to abbaspour et al 2004 abbaspour et al 2007 abbaspour et al 2015 while p factor is a percentage of measured and observed data bracketed by 95 of prediction uncertainty theoretically its value ranges from 0 to 100 worku et al 2017 melaku and wang 2019 hence the p factor of 1 and r factor of 0 indicates the perfection of simulation which corresponds to measured and observed data 2 6 metrics of model performance the modified e coli module was quantified by the coefficient of determination r2 root mean square error rmse nash sutcliffe efficiency nse 1970 and percentage bias pbais the linearity relationship between measured and simulated e coli is indicated by r2 the value ranges from 0 0 to 1 0 goodness of fit was used for the performances assessment of the swat bacteria model table 1 we targeted at least satisfactory performance with the value of r2 higher than 0 50 for the daily e coli prediction on the other hand nse shows the degree to what extent the observed and simulated value of e coli values fit 1 1 line coefficient of determination r2 obtained using the following equation 7 r 2 i 1 n o i o avr p i p avr i 1 n o i o avr 2 i 1 n p i p avr 2 2 where oi is the ith observed value oavr is average observed value pi is the ith modeled value pavr is average modelled value nash sutcliffe efficiency nse is obtained using the following equation 8 nse 1 i 1 n o i p i 2 i 1 n o i o avr 2 where n shows the number of time step days in our case based on nse 1970 the value ranges from to 1 we target the value of nse values for daily e coli prediction was set to better than 0 35 the value closer to 0 shows poor model performance while the value closer to 1 shows perfect model performance according to moriasi et al 2007 the percentage bias is used to indicate the average tendency of observed data smaller or larger than modelled data the optimal value is 0 0 lower value shows accurate model performance it can be obtained using the following equation 9 pbias i 1 n ob s i s i m i 100 i 1 n ob s i negative value indicates the model overestimation bias whereas positive value indicates model underestimation bias 3 results 3 1 performance of the modified bacteria module in swat model in order to test the modified bacteria module the swat model should be calibrated and validated for the stream flow temperature sediment and nutrient runoff these have been done in our previous work the interested readers can refer to references such as stream flow shrestha et al 2017 temperature du et al 2018 sediment shrestha and wang 2018 and nutrient runoff shrestha and wang 2020 in this paper we therefore focus on the calibration and validation of bacteria fate and transport 3 1 1 sensitivity analysis for model performance assessment the whole dataset was divided into the period of calibration 2010 2012 and validation 2013 2014 before the swat model calibration and validation process initially we tested the sensitivity analysis to identify bacteria associated most sensitive parameters a model sensitivity analysis can be done to find the relative response of the model to changes in the values of specific model parameters therefore sensitivity analysis is used to decide the most important parameters that govern system processes the most sensitive bacteria related parameters for the model are shown in table 2 the range of selected parameters are shown in table 3 and swat calibration and uncertainty programs swat cup were performed for calibrating e coli associated most sensitive parameters of the modified swat module in order to calibrate the updated bacteria module associated sensitive parameters latin hypercube one factor at a time lh oat was employed the results of the most sensitive analysis specify that all the measured input parameters have a significant influence on the e coli model prediction therefore during simulation periods 8 most sensitive e coli associated parameters were selected as the model inputs 3 1 2 model calibration and validation based on sensitivity analysis shown in table 3 eight bacteria associated parameters with the highest ranks in the process of simulations were selected for calibration and validation the remaining parameter was set at the default swat model values temperature adjustment factor thbac 1 07 shown in table 3 unlike the existing swat model the calibration results noticed from the modified swat illustrate higher sensitivity values the higher values of the parameters denote that ph significantly influences the concentration of e coli the predicted and observed daily e coli cfu 100 ml calibration period was from 2010 to 2012 and the validation period was from 2013 to 2014 are compared in fig 3 plotting the concentrations of e coli over the given periods of time confirms the oscillatory nature of e coli variation generally fig 3 summarizes the evaluation of statistical parameters on the daily e coli cfu 100 ml simulations for the four water quality monitoring sites the original swat model could not capture the distribution of e coli whereas the modified swat model results was quite evident with successful model performance evaluation for the monitoring sites the comparison of the original and modified swat model performance can be found from fig 3 and table 4 the value of r factor and p factor were found to be 0 51 and 0 62 at ab07dd0105 a monitoring station 0 46 and 0 60 at ab07ad0100 b monitoring station 0 32 and 0 56 at ab07be0010 monitoring c station and 0 34 and 0 67 at ab07cc0030 d monitoring station during calibration respectively the predictive power of the model at the daily time step was therefore successful over the periods as per the guidelines suggested in moriasi et al 2015 and indicates the satisfactory thresholds abbaspour et al 2015 the simulated e coli shows successful in terms of model evaluation statistics in the process of calibration for all the studied monitoring stations fig 3 the daily basis coefficient of determination r2 was found to be 0 78 0 70 0 80 and 0 71 at a b c and d monitoring stations respectively during calibration similarly the value for nash sutcliffe efficiency nse were 0 68 0 62 0 59 and 0 68 for the same monitoring sites respectively on the other hand the percentage bias pbias for the same monitoring stations found to be 7 94 11 79 17 85 and 16 90 respectively indicated in fig 3 and table 4 based on the results shown in fig 3 the modified swat model generally captured e coli cfu 100 ml concentrations for the periods of calibration similarly the value of the r factor and p factor were found to be 0 31 and 0 47 at ab07dd0105 a monitoring station 0 48 and 0 68 at ab07ad0100 b monitoring station 0 33 and 0 63 at ab07be0010 c monitoring station and 0 35 and 0 58 at ab07cc0030 d monitoring station respectively during validation in terms of model performance evaluation during validation periods the simulated results of e coli cfu 100 ml concentrations confirmed successful as shown in fig 3 the value of r2 was found to be 0 72 0 73 0 59 and 0 66 at a b c and d monitoring stations respectively for the period of validation similarly the value of nse was estimated to be 0 65 0 66 0 55 and 0 66 respectively table 4 while the value of pbias was found to be 10 18 22 and 14 respectively during validation for the same monitoring stations therefore the results indicate satisfactory thresholds even though the accuracy of the overall simulations was improved the model tended to overestimate the observed e coli concentrations at some point for all stations given the daily time steps over the validation intervals the model ability to simulate the e coli concentration deemed acceptable the overestimations of e coli cfu 100 ml concentrations in this study were probably due to the limited availability of observations to validate the model at some point during the dry season the model performance indicates underestimated the observed e coli cfu 100 ml in each monitoring site compared to the winter season during summer season access to livestock wildlife and human activity to the stream and leaks become common and consequently contribute to the disturbance of streambed with e coli cfu 100 ml release to the lakes and stream water increases the results of our study were set to the same as presented in thilakarathne et al 2018 in which the concentrations of e coli were found to be high in the summer a report by dibike et al 2018 during winter the concentration of ph reduced due to spring snowmelt which reflects lower ph level of snowpack and hinders snowmelt runoff has on flows in spring therefore the record of relatively low accuracy of the model performance during calibration and validation periods attributed to the uncertainty of observed e coli cfu 100 ml number in lakes and streamed sediment and the input from wildlife based on the results obtained from the comparison between the original and the modified swat model the modified swat model results displayed good model predictive performance than the original swat model in e coli distribution table 4 the model performance was evaluated by scatter plot analysis at each e coli cfu 100 ml monitoring sites fig 4 the slopes for all sites were significantly far from zero inferring that the prediction accuracy was enhanced by the modified e coli module therefore the results of the 1 1 line fitting diagram prove that the modified swat model showed successful predictive power for e coli cfu 100 ml concentration for each monitoring site during model calibration and validation fig 4 the scatters are found to be closer to the 1 1 line during the whole study periods except at some points in each monitoring site which is subject probably due to insufficient observations available for simulation generally it is worth to conclude the modified swat model gives better efficiency for ph associated daily e coli concentrations by adding the new parameters during calibration and validation 3 2 effect of ph on e coli distribution to assess the potential effects of ph on e coli concentrations we plotted ph versus observed and simulated e coli concentrations the e coli concentrations in the river basin shows an inverse relationship with ph dynamics fig 5 the swat bacteria module used only the first decay equation considering temperature as a function i e more e coli die off at a higher temperature can be found in sadeghi and arnold 2002 cho et al 2012 but not for ph dynamics this study presented the need of swat model modifications of the original bacteria module to simulate the effect of ph dynamic on e coli for the river basin the count of e coli shows increasing trend as of decreasing in the ph level when ph level is lowest e coli survival and growth was observed and vice versa when ph level was high e coli growth level was not observed for instance from the fig 5a and b observation at ph level of 4 very little concentrations of e coli was observed however at ph level of about 5 lower level of acid more e coli concentration was observed when 5 ph 8 e coli concentration increased as ph level increased up to a largest value therefore it is worth to say at ph level of about 5 lower level of acid e coli could survive to grow and more e coli concentrations was observed fig 5 like the other controlling factors i e temperature and solar radiation pietikäinen et al 2005 ishii et al 2007 the results of this study prove that ph was one of a major significant controlling factors for e coli growth and the overall concentrations our study was in lined with other scholars presser et al 1998 ross et al 2003 in which presented in their report rapid decline in e coli distribution has been observed at highest ph therefore the rate of e coli growth was high at the minimum ph level 4 discussion the observed data ranges from 5 e coli 100 ml to 240 e coli 100 ml on the other hand the model predicted value was a maximum of 175 e coli 100 ml these may have happened probably due to various influencing factors these unaccountable factors could include the high degree of observed e coli measurements and the challenging nature of trying to realistically emulate e coli spatiotemporal loading patterns in the model for the river basin the absence of consideration for e coli fate and transport within the existing swat model is also a potential source of error for the model results oliver et al 2009 the results confirm the variability and uncertainty that influence e coli modelling and demonstrate how the single unaccountable e coli cfu 100 ml concentrations input can lead to large inconsistencies in model results due to these and other factors modelling e coli is one of the toughest and may lead to error therefore less confidence compared with nutrients sediment and runoff novotny 2002 parajuli 2007 parajuli et al 2009 a reasonable level of e coli cfu 100 ml simulation was achievable nonetheless the inability to justify for the unknown spatiotemporal contaminate sources within and around the river basins affects the model accuracy in theory the accuracy of the model results should infer the reliability and scope of the observed e coli cfu 100 ml data for calibration and validation however scant monitoring stations might limit the data availability to directly evaluate what happens in small scale tributaries for the entire catchment some small variations might have been smoothed or hidden when only the mainstream is monitored given uncertainties in representing e coli cfu 100 ml sources spatial as well as temporal and small number of grasp samples from four monitoring stations for the modified swat model calibration and validation developing suitable functions and parameters with an inclusive of the characteristics as well as the behavior of ph associated e coli into the modified swat model presents an interesting path of the applied research modified swat model successfully simulates measured and modelled e coli cfu 100 ml in the athabasca river basin during the calibration and validation periods 2010 2014 at four monitoring stations based on the swat model predictive efficiency classification moriasi et al 2007 the capability of modified swat model calibration and validation evidenced that the model could capture the e coli cfu 100 ml concentration fate and transport on the daily basis the magnitude of the coefficient of determination r2 confirms the association between measured and modelled e coli cfu 100 ml which indicates the linearity relationship between measured and simulated ph based e coli cfu 100 ml characterization similarly the percentage bias and nse results of this study assure how the tendency of model prediction efficiency to the reality elsewhere previous researchers have reported the fate and transport of temperature associated e coli cfu 100 ml kim et al 2010 2017 chao et al 2016a b which employed the older version of swat model did not consider the hyporheic exchange of e coli transport across the water interface during the in stream process mawdsley et al 1995 and muirhead et al 2006 showed that rainfall in combination with the application of manure has more influence on bacteria transport consequently affect model accuracy some other scholars upgraded swat model by adding the solar radiation associated die off rate but none of them are considered ph as fate factor for bacteria using modified swat bacteria model modified swat model which integrates the complete understanding of the e coli characterization process would help to improve the model performance the original swat model allows the only temperature associated bacteria characterization in this study unlike other studies we adapted a modified version of the swat model to simulate ph associated e coli cfu 100 ml for the athabasca river basin considering the results demonstrates in figs 3 and 4 and prove that modified swat model can be employed in order to predict the potential e coli concentration within the river basin however given a limited number of measured data points attention would be given in the process of interpreting the results assuming the unpredictability that surrounds various temporal moments as well as spatial sources of river basin contamination model performance evaluations recommend that capability to accurately predict e coli cfu 100 ml in water is uncertain quantifying effect of ph on bacteria fluxes and contaminants in a large scale watershed is a challenge for water resources management and help to protect aquatic ecosystems and human health therefore this paper addresses one of aquatic ecosystems and human health management quantifying the effect of ph on bacteria fate and transport in cold river basin analysis results confirms that the ph associated e coli concentrations in different areas like agricultural practices surface runoff human and animal waste as a real changeable to which further individual study could be focused in order to brief detail and accurate information for use of the river basin modelling the present understanding of e coli concentration dynamics in a water is expanding but still is insufficient to develop a swat bacteria model pachepsky and shelton 2011 this is a motive why the observed rather than simulated e coli concentrations in the water bodies were used as the inputs for the effects of ph algorithms in this work testing of such model is necessary to advance modelling dynamics of bacteria concentrations within the water bodies which is important development to advance microbiological water quality modelling the results of this study show that the indicator and sources of microorganism concentrations should be determined where bacteria modelling is used to assist in stream and watershed management decisions 5 conclusions the swat model as a tool for modelling bacteria at the river basin and watershed scale was modified in this study as swat bacteria module to incorporate ph associated e coli fate and transport and concentrations practices distracting water quality the modified swat model was the first attempt to incorporate ph into the swat model in simulating the effects on the e coli concentrations and was applied for the athabasca river basin the modified swat bacteria model predictions for the daily ph based e coli prove there was a linear relationship between measured and simulated data however uncertainty exists as the modified swat bacteria model tended to underestimate the e coli concentrations compared to those observed at the dry season and overestimate the e coli concentrations compared to those of the observed data however it should be clear that it was not the only reason for the over and under estimations the sources of the model error included the limited availability of observed e coli data for model calibration and validation the model prediction capability could be increased by having reliable river basin input data from different sources which is significant to refine the critical source area for e coli overall the e coli based module considering ph as a factor provides a new capacity to improve e coli algorithm of the swat model and can be applied in other catchments and river basins although the uncertainty of e coli considering ph as a factor probably hindered the performance of the modified swat bacteria module this study could be a useful tool for effective decision support information for policymakers and catchment manager for water quality management credit authorship contribution statement tesfa worku meshesha conceptualization data curation formal analysis investigation methodology project administration resources software validation visualization writing original draft writing review editing junye wang conceptualization data curation formal analysis funding acquisition investigation methodology project administration resources software supervision validation writing review editing nigus demelash melaku data curation formal analysis software validation visualization writing review editing declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements the authors would like to thank the alberta economic development and trade for the campus innovates program research chair for the financial support no rcp 12 001 bcaip the water quality data product can be found at http aep alberta ca water reports data surface water quality data default aspx and landuse digital elevation map and other necessary data used for this study can found at http www agr gc ca atlas landu and http nowdata rcc acis org 
5828,sufficient hydrological data such as streamflow are important to represent the long term characteristics of a watershed in order to support decision making policy and management lack of data remains one of the main challenges of hydrological analyses therefore the main goal of this study is to extend streamflow records using a proposed approach where the wavelet transform wt is incorporated as a pre processing method into eight existing record extension techniques namely the ordinary least square regression ols maintenance of variance move types 1 4 kendall theil robust line ktrl ktrl2 and robust line of organic correlation rloc the performance of the wt based methods in estimating individual data values means and standard deviations of the extended records and a series of percentiles of the extended records was then compared with that of the respective conventional methods i e without the wt the data used in the analysis consisted of 67 pairs of target index stations that were obtained from canada s reference hydrometric basin network database all of which contain outliers as such the results and discussions obtained are applicable for cases where the data contain outliers the wt based methods particularly the ktrl wt ktrl2 wt and rloc wt demonstrated consistent improvements in precision and accuracy compared with their conventional counterparts especially in estimating the means and standard deviations of the extended records in estimating individual data values the wt based methods showed inconsistent improvements finally in terms of percentiles greater improvements were seen in the estimation of higher percentiles more specifically for the move1 wt move2 wt ktrl2 wt and rloc wt compared with their conventional counterparts keywords streamflow hydrologic time series record extension methods wavelet transform wavelet decomposition 1 introduction the availability and reliability of sufficiently detailed watershed information including hydro meteorological data such as streamflow and precipitation is required for effective and sustainable water resources management panu et al 2000 horne 2015 in practice however the availability of data for long periods is often lacking despite there being no universal guidelines for the minimum length of data required for optimal planning and management of water resources raman et al 1995 argues that a minimum of 35 years of data are required to produce reliable results burn et al 2010 only uses stations with a minimum of 50 years of streamflow data to ensure statistical validity when studying the trends in hydrological extremes for canadian watersheds partal 2010 recommends a minimum of 40 years a length also used in many other studies e g nalley et al 2012 2013 zhang et al 2013 tan and shao 2017 other studies have used even longer datasets for example de jongh et al 2006 zhang et al 2007 and sauchyn et al 2015 use more than 100 years of data to study temperature trends in belgium investigate the long term reliability of the athabasca river as a freshwater source and the long term variability in the annual maximum flow of the yangtze river in china respectively continuous 100 year datasets are however extremely rare the present study focuses on addressing the issue of short data lengths using data from stations in canada by proposing a new approach that incorporates the wavelet transform wt into existing streamflow record extension methods there are a variety of reasons why sufficient data may not be available in canada for example hydrometric stations are not evenly and well distributed across the country most stations with extensive data sets are located in southern canada resulting in short data or data gaps for the rest of the country equipment malfunctions and seasonal shutdowns can also result in missing observations in canada the reference hydrometric basin network rhbn contains a network of stations that record data that are considered ideal for hydrological and climatic related studies monk et al 2011 out of over 200 rhbn stations there are only 73 that have at least 40 years of continuous streamflow data nalley et al 2019 similarly when studying the changes in the annual mean daily streamflow over several decades 156 rhbn stations have 30 years of records and that number decreases significantly to 25 for 60 years of records khaliq et al 2009 consequently the development of new approaches to improve data lengths in streamflow is needed commonly used stochastic record extension methods include the ordinary least square ols maintenance of variance move types 1 2 3 and 4 and kendall theil robust line ktrl more recently the ktrl2 proposed by khalil et al 2012 and the robust line of organic correlation rloc proposed by khalil and adamowski 2012 were found to be useful in extending hydrological records due to their robustness in the presence of outliers and their ability to maintain the variance of the extended records khalil and adamowski 2014a the ols technique is a parametric method which assumes normally distributed and independent residuals one main deficiency of the ols method is that it can be sensitive to the presence of outliers which are often present in hydrological data causing the variance in the produced records to be underestimated hirsch et al 1982 vogel and stedinger 1985 helsel and hirsch 2002 khalil et al 2010 for data that have extreme values or outliers and depart from normality the ktrl has been shown to be an effective non parametric estimator thus it is more efficient than the ols hirsch et al 1991 nevitt and tam 1998 helsel and hirsch 2002 the ktrl however may still produce estimated records with underestimated variance khalil and adamowski 2014b also helsel and hirsch 2002 have shown that move techniques are advantageous as they can maintain the variance of estimated records as such move techniques have been applied in many hydrological data extension and generation studies e g moog et al 1999 hernández henríquez et al 2010 ryu et al 2010 duan et al 2013 although these studies demonstrate the usefulness of move techniques these techniques are also sensitive to the presence of outliers khalil and adamowski 2014a b in order to manage the presence of outliers and the underestimation of the variance of extended records the ktrl2 method modifies the intercept and slope of the regression model in the ktrl in order to minimize percentile estimation errors the ktrl2 aims to produce records with a cumulative distribution function cdf that approximates the actual records well therefore the ktrl2 essentially has the advantages of the ktrl method with the added advantage of maintaining the variance of the percentile estimations of extended records khalil et al 2012 similarly khalil and adamowski 2012 propose the rloc as a new regression model which is a modified move type 1 method also referred to as line of organic correlation loc that can maintain the variance of the extended records and remain robust in the presence of outliers the rloc uses the inter quartile range of concurrent records when estimating the slope allowing for an estimation of the intercept it is commonly recognized that noise and other random components are parts of the makeup of hydrologic time series minville et al 2008 sang et al 2009 rai et al 2010 cleaner signals which may be obtained by separating the deterministic and noise components have been used to improve the performances of many hydrologic models wu et al 2009 rajaee et al 2011 sang et al 2013 signal pre processing using data decomposition methods such as wavelet analysis has been demonstrated to be a useful approach in obtaining cleaner signals liu et al 2013 santos and silva 2014 nourani et al 2017 hadi and tombul 2018 the information obtained through data decomposition is then used as inputs in another data driven model this approach has been applied in areas such as streamflow forecasting e g kisi et al 2011 belayneh et al 2014 djerbouai and souag gamane 2016 honorato et al 2018 fouchal and souag gamane 2019 but not in streamflow record extension streamflow record extension studies that used techniques such as the ols move ktrl and rloc do not generally pre process or decompose the data before using these record extension techniques in light of this the present study aims to pre process the streamflow data using the wavelet analysis approach in order to obtain cleaner signals prior to inputting these cleaner signals into the record extension methods the aim of using wavelet analysis is to capture hidden information in the data which helps provide a more concise representation of the original time series shoaib et al 2014 shafaei et al 2016 quilty and adamowski 2018 the applications of the discrete wavelet transform dwt in various aspects of hydrological research have been wide ranging including trend detection short and long term forecasting and synthetic data generation hybrid models which combine dwt and data driven or artificial intelligence ai methods have led to significant improvements in hydrological forecasting examples of the most common data driven or ai methods used in hydrological forecasting include artificial neural network support vector regression genetic programming and fuzzy logic although these methods are not discussed in detail here as they are not within the scope of the current work the contribution of the dwt in improving these models performance is notable and motivated this study to combine dwt with record extension methods dwt has improved various forecasting models by i providing the time frequency and physical structure of the analyzed signal ii extracting features of the analyzed signals prior to using them as inputs and iii de noising the original signal which results in cleaner signals for use as inputs into ai or data driven models nourani et al 2014 shafaei et al 2016 fahimi et al 2017 despite the potential uses and benefits of the dwt approach e g in forecasting researchers have not explored in any detail the use of dwt in streamflow record extension therefore the main objective of the present study is to propose and develop a method that couples the dwt using the à trous algorithm with eight record extension techniques namely the ols move1 move2 move3 move4 ktrl ktrl2 and rloc to extend streamflow records more specifically the wt based and conventional methods performances were assessed and compared with the estimations of individual data values means and standard deviations of the extended records and a series of percentiles 5th 95th of the extended records to the best of the authors knowledge this study is the first to incorporate the use of dwt via the à trous algorithm into the aforementioned eight existing record extension techniques to extend streamflow records the data used were from 67 rhbn stations in canada the estimations of the extended records were computed using various lengths of concurrent and extended records and levels of correlations 2 materials and methods 2 1 study sites and streamflow data monthly streamflow averages from rhbn stations in canada were used for this study these stations are a sub set of the national network monitoring stations with high quality data that make them ideal for hydroclimatic studies the accuracy of this data is checked by local experts who consider the hydraulic condition the stage discharge relationship channel geometry and ice condition zhang et al 2001 coulibaly and burn 2004 for each pair of target and index stations a minimum of 35 years 420 months of continuous concurrent records were sought continuous records were used because the computation of the wt requires that datasets have consistent time intervals there were 67 pairs of target index rhbn stations that qualified in which each pair had common monthly records of 35 years 420 months fig 1 indicates the locations and the climatic ecozone of each of the rhbn stations used in this study 2 2 record extension procedures and theoretical background this study compared the performance of eight conventional record extension methods with their respective wt based versions which is the new approach proposed in this study the analytical framework of the wt based record extension method is given in fig 2 b both sets of methods were applied to the monthly streamflow averages of the 67 pairs of the target index stations the time series of the index and target stations were represented by xt and yt respectively the target station had a shorter number of records represented by n1 where t 1 2 3 n1 which was extended up to the data length of the index station n1 n2 where t 1 2 3 n1 n1 1 n1 2 n1 3 n1 n2 thus n1 and n2 represent the lengths of concurrent records and extended records respectively the data analysis was carried out in matlab according to the following step by step procedure fig 2a 1 an index station x was determined for each target station y based on spearman s rank correlation r the data analysis was then conducted using three categories of r values low around 0 5 medium around 0 7 and high around 0 90 and for each category there were 67 pairs of target index stations 2 from the 35 years 420 months of common data three lengths of concurrent records n1 were chosen to be included in the analysis 10 years 120 months 20 years 240 months and 30 years 360 months as such the applications and conclusions obtained from this study should be applicable not only for concurrent records of 30 years but also for shorter data i e 20 years for each n1 the lengths of extended records n2 were 12 months 24 months 36 months 48 months and 60 months therefore there were a total of 45 combinations 3 r values 3 lengths of n1 and 5 lengths of n2 3 for each combination record extensions using the conventional methods i e the ols move types 1 4 ktrl ktrl2 and rloc were applied to each of the 67 pairs of the target index stations 4 record extensions using the wt based methods were performed fig 2b as follows a for each length of n1 the dwt was computed separately on the original streamflow data of the target and index stations the dwt computations produced the respective detail d and approximation a subseries of the target and index stations b for each n1 the d and a subseries were then used as inputs in the computation of the ols move types 1 4 ktrl ktrl2 and rloc to produce the extended records for the various lengths of n2 more detailed information on the procedure of constructing extended data using d and a subseries is found in section 2 5 c the performances of the conventional and the wt based models were evaluated and compared based on the different r values categories lengths of n1 and lengths of n2 three performance measures were used i relative bias rbias ii relative root mean square error rrmse and iii the ratio of the predicted mean and standard deviation over the respective actual mean and standard deviation 2 3 detection for outliers most record extension techniques e g the parametric ols and move types 1 4 are sensitive to the presence of outliers khalil and adamowski 2014a b for example estimations produced using the ols method are based on the means and standard deviations of the data thus this method is less robust when confronted by extreme values or outliers helsel and hirsch 2002 khalil and adamowski 2014a b similarly the slopes and intercepts used in the move techniques are also based on the mean and standard deviation of the data which are also influenced by the presence of extreme values or outliers helsel and hirsch 2002 khalil and adamowski 2012 in the present study the presence of outliers was first assessed on the n1 of the target and index stations using box plots it was found that for all lengths of n1 all pairs of target index stations had outliers figures not shown the data analysis conducted included the outliers they were not treated nor removed because this study aims to compare the performance of the conventional methods against the wt based approach when they are confronted with the presence of outliers additionally since the main use of the wt is to pre process the original streamflow data the outliers if they exist should be included therefore the results of this study are applicable for cases where outliers are present in the data 2 4 record extension methods a review of the literature showed that the performance of record extension methods is dependent on the conditions of the data parametric approaches e g the ols and move are highly influenced by factors such as a skewed distribution and the presence of outliers on the contrary non parametric approaches e g the ktrl ktrl2 and rloc are considered more robust in the presence of outliers the rloc also assumes symmetrical distributions the record extension methods used i e ols move ktrl ktrl2 and rloc intend to determine the best intercept c and slope m estimators for the regression equation 1 y m x c 2 4 1 ordinary least square regression ols the ols describes the covariation between the response y and predictor variables x the values of the intercept and slope in the linear regression are obtained to minimize the squared error of the estimated values the ols has the following form draper and smith 1966 2 y t y r r sd y r sd x r x t x r y t represents the estimated value for the target station using the corresponding observed value of the index station at x t for the concurrent period t 1 n1 the mean values of the flow data during the concurrent period are y r and x r sd y r and sd x r are the standard deviations during the concurrent period as described in eq 2 the estimated values are based on the mean and standard deviation of the data which are more easily influenced by the presence of extreme values 2 4 2 maintenance of variance extension move move techniques are more advantageous than the ols as they maintain the variance of the extended records hirsch 1982 vogel and stedinger 1985 helsel and hirsch 2002 in move type 1 the slope and intercept are determined so that the entire estimated values of y t for t 1 2 n1 n2 have the mean and standard deviation of the concurrent period y r and sd y r respectively move type 1 is written as hirsch 1982 3 y t y r s i g n r sd y r sd x r x t x r move type 1 is similar to the ols except it uses the sign of r instead of the value of r while the ols produces records with underestimated variance move type 1 produces extended records having the properties of the records they intend to present hirsch 1982 helsel and hirsch 2002 koutsoyiannis and langousis 2011 in move type 2 the slope and intercept are determined so that the entire estimated values of y t for t 1 2 n1 n2 have an unbiased estimate of the mean μ y and variance σ y 2 as proposed by matalas and jacob 1964 4 μ y y r n 2 n 1 n 2 β x e x r 5 σ y 2 β 2 sd x 2 1 n 1 n 2 3 n 1 3 n 1 n 2 1 n 1 1 n 1 2 sd y r 2 β sd x r 2 where vogel and stedinger 1985 6 β t 1 n 1 x t x r y t y r t 1 n 1 x t x r x e represents the mean value of xt for t n1 1 n1 2 n1 n2 the equation for move type 2 is hirsch 1982 7 y t μ y σ y sd x x t x move types 1 and 2 are used to produce records at t n1 1 n1 2 n1 n2 however vogel and stedinger 1985 found that slope and intercept estimators for the two move techniques were not able to achieve the intended purpose for move type 3 vogel and stedinger 1985 proposed slope m and intercept c estimators that can be used to produce extended records so that the n1 n2 records of the target station have a mean and variance according to those proposed by matalas and jacobs 1964 in eqs 4 and 5 respectively the estimators proposed by vogel and stedinger 1985 are 8 m n 1 n 2 1 σ y 2 n 1 1 sd y r 2 n 1 y r μ y 2 n 2 a μ 2 n 2 1 sd x e 2 9 c n 1 n 2 μ y n 1 y r n 2 the mean and variance estimators used in move types 2 and 3 are not ideal for small samples vogel and stedinger 1985 therefore vogel and stedinger 1985 proposed improved versions of the mean μ y and variance σ y 2 estimators 10 μ y y r θ 1 n 2 n 1 n 2 β x e x r 11 σ y 2 1 θ 2 sd y r 2 θ 2 β 2 sd x 2 1 n 1 n 2 3 n 1 3 n 1 n 2 1 n 1 1 n 1 2 sd y r 2 β sd x r 2 where θ 1 and θ 2 are vogel and stedinger 1985 12 θ 1 n 1 1 ρ 2 n 1 4 ρ 2 1 13 θ 2 n 1 4 ρ 2 n 1 8 5 ρ 2 4 5 the estimated cross correlation of the concurrent flow time series between the index xt and target yt stations is represented by ρ vogel and stedinger 1985 14 ρ β sd x r sd y r using the mean and variance estimators given in eqs 10 and 11 respectively vogel and stedinger 1985 proposed move type 4 the main goal of move type 4 is to find slope m and intercept c estimators so that the sequence of records n 1 n 2 produced for the target station would have a mean of μ y and variance of σ y 2 to achieve that the μ y and σ y 2 in eqs 4 and 5 are replaced with μ y eq 10 and σ y 2 eq 11 respectively again it can be seen that all four types of move methods involve the use of the mean and variance making these techniques sensitive to the presence of extreme values or outliers khalil et al 2012 2 4 3 the kendall theil robust line ktrl and ktrl2 the ktrl and ktrl2 methods are more robust than the move methods in the presence of outliers unlike the move methods which use the mean and variance the ktrl methods are based on the kendall rank correlation coefficient helsel and hirsch 2002 khalil et al 2012 khalil and adamowski 2014a b the ktrl robust slope estimator is calculated in a pair wise manner by comparing each pair of records to all other pairs for a pair of data xt and yt in a total of n pairs there will be n n 1 2 pair wise comparisons for each pair wise comparison a slope of δyt δxt is calculated and the median of all the pair wise slopes is used as the nonparametric slope estimate m kt theil 1950 15 m kt m e d i a n y t q y t k x t q x t k f o r a l l k q k 1 2 n 1 1 q 2 3 4 n 1 for all k q k 1 2 n 1 1 q 2 3 4 n 1 the intercept c k t is calculated as 16 c kt m e d i a n y t m kt m e d i a n x t the line of the ktrl is fitted to go through the median x and median y helsel and hirsch 2002 in the ktrl2 the modification of the slope and intercept was done to create extended records with a cumulative distribution function cdf as close as possible to the cdf of the actual records thus the ktrl2 more specifically aims to reduce the error related to the estimation of the percentiles of the extended records the modified slope m k t 2 and intercept c k t 2 are calculated as follows khalil et al 2012 17 m k t 2 m e d i a n y t q y t k x t q x t k for all k q k 5 t h 10 t h 15 t h 90 t h q 10 t h 15 t h 20 t h 95 t h 18 c k t 2 m e d i a n y t m k t 2 m e d i a n x t y t q and x t q represent the percentiles of y r and x r respectively during the concurrent period since the percentiles are taken at increments of 5 from the 5th to the 95th percentile there are a total of 19 x t q y t q pairs of percentiles the computation n n 1 2 19 19 1 2 produces 171 pair wise comparisons for each comparison a slope δyt δxt is determined and the median of the 171 pair wise slopes is used as the slope estimate 2 4 4 robust line of organic correlation rloc the rloc method was proposed by khalil and adamowski 2012 as a modification of move type 1 that aims to overcome its sensitivity to outliers the presence of outliers may cause the variance produced to be greater than it is supposed to be since calculating variance involves squared deviations from the mean the rloc method is less sensitive to outliers since its slope and intercept estimators are based on the inter quartile range iqr which focuses on the central half of the data and thus is not affected by either extremely high or low values khalil and adamowski 2012 the slope m r l and intercept c r l estimators used in the rloc are computed using khalil and adamowski 2012 19 m rl y t 75 y t 25 x t 75 x t 25 20 c rl m e d i a n y r m rl m e d i a n x r y t 75 a n d y t 25 represent the 75th and 25th of yt during the concurrent period and x t 75 a n d x t 25 are the counterparts for the xt 2 5 wt based model coupling dwt with existing record extension methods in this study dwt using the à trous haar algorithm was used to decompose the streamflow time series of target and index stations studies such as quilty and adamowski 2018 and du et al 2017 have indicated that despite the growing popularity of the dwt approach many studies have incorrectly applied the method in hydrological forecasting and prediction models for example common dwt approaches e g the dwt multiresolution analysis dwt mra and maximal overlap dwt multiresolution analysis modwt mra that have been used in data forecasting which are relevant to record extension require the existence of future data quilty and adamowski 2018 to obtain the reconstruction coefficients at a specific time e g time t data ahead of a time t are required in the case of dwt mra and modwt mra to calculate the scaling approximation series and wavelet detail series coefficients as these approaches use an iterative reconstruction process where some future data are necessary quilty and adamowski 2018 therefore these methods are unusable for real life forecasting scenarios because future data would be unavailable on the contrary using the à trous algorithm in the decomposition process does not require future data because the scaling coefficients used to obtain the reconstruction coefficients which are the summation of the wavelet and scaling coefficients at any time t are obtained using data at time t renaud et al 2005 quilty and adamowski 2018 as such the decomposition using the à trous dwt method is applicable for real life situations which is the main reason for the use of this approach in the present study furthermore the decimation process used in the traditional pyramid dwt approach decreases the lengths of wavelet coefficients down to half as the scale increases to the next level shensa 1992 gonzález audícana et al 2005 li and cheng 2014 this is not ideal for predicting or forecasting purposes because there is not enough information available li and cheng 2014 this issue can be overcome by using the à trous wavelet decomposition where the lengths of the wavelet coefficients are always equal to that of the original signal this is a non decimated process where zeros are inserted instead of decimating the lengths of the wavelet coefficients shensa 1992 li and cheng 2014 the empirical datasets used in this study had the extended future data which were used to validate the results and subsequently evaluate the performances of the conventional record extension methods and their respective wt based versions the convolution of an original signal e g for xt t 1 n1 n1 1 n1 n2 using the dwt with the à trous algorithm at a scale a and translation factor b a and b are integers can be expressed as renaud et al 2005 li and cheng 2014 21 a a t l l x b 1 t 2 a 1 l a a t represents the scaling coefficient approximation component at the scale a the computation in eq 21 using the haar wavelet with a low pass filter l consisting of 1 2 1 2 at a resolution e g the following is for level 1 can be written as renaud et al 2005 22 a a 1 t 0 5 a a t 2 a a a t the detail component at the scale a is then obtained by computing the difference between two consecutive scaling coefficients renaud et al 2005 li and cheng 2014 23 d a t a a 1 t a a t for each target index station pair the à trous dwt was applied separately to the concurrent records only not to the whole available data since in real life scenarios we would not have the extended or future data available for the index xt and target yt data therefore these dwt decompositions on the n1 would produce two d subseries and two a subseries all of which have lengths that are equal to n1 and they would be used as inputs into the eight record extension methods the two d subseries were first used as inputs into the eight methods and the same process was repeated for the two a subseries the final results for each wt based version were obtained by summing the results or values produced from using the d and a sets in the respective conventional record extension method for each combination of r value n1 and n2 2 6 evaluation of performance the performance of the conventional methods and their respective dwt based versions were compared and evaluated based on uncertainty which was quantified in the form of accuracy and precision measures accuracy refers to the ability of the model to correctly estimate the true value of interest leading to either an underestimation or overestimation of the true value whereas precision refers to the amount of variation in the estimates themselves or the measure of the repeatability of an estimate walther and moore 2005 kennard et al 2010 the uncertainty measurements were assessed using rbias rrmse and the ratios of the predicted mean and standard deviation of the extended records over their respective observed values rbias and rrmse reflected the accuracy and precision measures respectively the ratios of means and standard deviations also indicated accuracy and precision and were assessed using boxplots the accuracy was reflected by the closeness of the median shown in the boxplot to 1 and the symmetry of the boxplots around 1 the precision was reflected by the amount of dispersion of the box and whiskers of the boxplots khalil et al 2010 khalil and adamowski 2014a the rbias and rrmse were calculated as follows 24 rbias 1 n 2 t n 1 1 n 1 n 2 y t y t y t 25 rrmse 1 n 2 t n 1 1 n 1 n 2 y t y t y t 2 the rbias and rrmse were calculated for each target index pair the final rbias and rrmse values of all 67 target index pairs were averaged to obtain the final rbias or rrms value the rbias and rrmse of the extended records were assessed on i the individual data values ii means and standard deviations and iii percentiles ranging from the 5th to the 95th percentiles the performance evaluations were conducted for the various lengths of n1 120 240 and 360 months and n2 12 24 36 48 and 60 months and using the r values from the low r 0 50 medium r 0 70 and high r 0 90 categories 3 results and discussion the results of the empirical experiment in this study reflect scenarios where outliers are present in the data due to space limitations only selected figures were presented to illustrate the findings obtained i e not all figures are presented furthermore in each of the result tables tables 1 6 only a subset of the extended record lengths are presented i e n2 12 and 48 months for each length of concurrent record n1 3 1 relative rmse rrmse and relative bias rbias 3 1 1 individual data values the rrmse and rbias for the estimations of the individual data are shown in tables 1 and 2 respectively in terms of precision improvements in the wt based methods were most noticeable for the concurrent record length n1 of 360 months where most rrmse values were lower for the wt based methods except for the ktrl wt table 1 this could indicate that large concurrent records are needed in order for wt based methods to produce higher accuracy it appears that for data 240 months 20 years the conventional versions performed better than the wt based version but at 360 months 30 years the opposite occurred this improved performance of the wt based methods was particularly evident for the low and high r value categories regardless of the lengths of n1 and n2 and the values of r the only wt based version that did not show improvements in precision compared to its conventional version was the ktrl wt in terms of accuracy improvements in the wt based methods were mostly observed for the low r value category where almost all rbias values were lower for the wt based versions than for their respective conventional methods except for the ktrl wt table 2 for the higher r value category the improvements in the wt based versions were scattered it was noted that the ktrl wt never showed higher accuracy performance than the conventional version in estimating the individual data values for the extension of individual records the performance of the wt based methods appears inconsistent and the better performance of the wt based methods was mostly seen when r values were low the ktrl wt however never outperformed the conventional ktrl regardless of the lengths of concurrent and extended records and levels of correlation 3 1 2 means and standard deviations 3 1 2 1 means the rrmse and rbias values for the estimations of the means of the extended records are shown in tables 3 and 4 respectively generally the wt based methods led to an improvement in precision compared with the conventional methods in particular the ktrl2 wt and rloc wt outperformed their respective conventional versions in most cases furthermore the ktrl wt consistently performed better than its conventional version under any lengths of n1 and n2 table 3 the wt based versions of ols and move types 1 4 were less consistent in showing improved precision as a number of their rrmse values were equal to those of their respective conventional version though at high r values the precision of move3 wt and move4 wt were better than their respective conventional versions therefore it appears that the lengths of n1 and n2 were not a factor in determining the improved performance of the wt based methods when estimating the mean of the extended records for smaller sample sizes the performance of move4 both versions should be better compared to move2 both versions and move3 both versions since the mean and variance estimators for move2 and move3 are not ideal for small sample size in this present study there were no patterns indicating that the performance of move4 was better compared to that of move2 and move3 even for the smallest n1 of 240 months 20 years this might be because the monthly time series used in the study that spanned a period of 10 or 20 years still resulted in relatively large sample sizes finally the level of correlation increased the precision of only move3 wt and move4 wt compared with their conventional versions similarly higher accuracy in the wt based methods was consistently observed for the ktrl wt as its rbias values were always better than those of the conventional version in estimating the means of the extended records table 4 additionally most rbias values of the ktrl2 wt and rloc wt were better than their respective conventional version particularly for the medium 0 7 and high 0 90 r value categories table 5 r values also had stronger positive effects compared with the lengths on n1 and n2 on the accuracy performances of the wt based methods in ols wt and move wt types 1 to 4 at the low and medium r value categories move1 wt and move2 wt displayed better accuracy than their respective conventional versions conversely move3 wt and move4 wt showed improved accuracy at high r values table 4 it can be seen that when estimating the means of the extended records among the wt based methods the ktrl2 wt rloc wt and especially the ktrl wt showed the most improvement both in precision and accuracy over their conventional counterparts the ktrl ktrl2 and rloc methods were developed to be more robust in the presence of extreme values and outliers as they use outlier resistant estimators e g the medians and iqr when estimating the slope and subsequently calculating the intercept of the line this study demonstrated that for these outlier resistant methods their wt based versions produced even more accurate and precise results for data containing outliers 3 1 2 2 standard deviations the rrmse and rbias values for the estimations of the standard deviations of the extended records are shown in tables 5 and 6 respectively improved precision was consistently seen for the ktrl wt and ktrl2 wt compared with their respective conventional versions regardless of the lengths of n1 and n2 and the levels of correlations table 5 additionally in most cases the rloc wt also showed better precision than its conventional version it was noted that the lowest rrmse values for each case among all methods wt based and conventional were mostly observed for either move1 move1 wt or move2 move2 wt reflecting the ability of the move methods to maintain the variance of the estimated records more precisely in terms of accuracy in estimating the standard deviation of the extended records the wt based methods ols wt ktrl wt ktrl2 wt and rloc wt outperformed their conventional versions table 6 for move wt types 1 4 in most cases the wt based methods also performed better than their respective conventional versions table 6 the lowest rbias values were generally observed for one of the move or move wt methods again reflecting the ability of move to maintain variance of all the methods showing the lowest rbias values for estimating the standard deviation of the extended records the wt based version was superior more accurate to the respective conventional method in all cases except two out of 45 cases for medium r values at n1 240 n2 24 and at n1 360 n2 48 table 6 finally there were no patterns that could indicate if a single parameter i e lengths of concurrent records and extended records or the strengths of correlations was the cause of the better performances of the wt based methods in estimating the means and standard deviations of the extended records improvements in measures of precision and accuracy in the wt based methods were more apparent in the estimations of means and standard deviations of the extended records compared with the estimations of individual extended data values indeed when the purpose is to extend records rather than filling in missing values the means and standard deviations of the extended records provide more valuable information than the individual records themselves this is because the statistical moments rather than individual records are more useful for obtaining information on probabilities and probability distributions khalil and adamowski 2014a which may be useful for certain assessments such as for low and high flows 3 1 3 percentiles for all lengths of n1 and n2 and various r values lower percentiles the 5th 20th percentiles figs 3 4 displayed larger ranges of rrmse and rrbias values compared to higher percentiles the 75th 95th percentiles figs 5 6 this indicates that for all parameters used n1 n2 and r values the amount of overestimation in the precision measure and the amount of over underestimation in the accuracy measure were higher at lower percentiles than at higher percentiles furthermore the performance was noticeably more competitive across the different methods and versions at higher percentiles compared to lower percentiles 3 1 3 1 lower percentiles the 5th 20th at lower percentiles competitive performance of methods having high precision and high accuracy was generally observed for move1 move1 wt move2 move2 wt ktrl2 ktrl2 wt rloc and rloc wt the worst performance both in terms of precision and accuracy was consistently seen for move3 move3 wt and move4 move4 wt some of their values were beyond the cut off shown in figs 3 4 the performance of the ols and ols wt was influenced by the strength of correlations both versions performed poorly in the low r value category but performed well in the medium and high r value categories figs 3 4 specifically the ols wt was more precise and more accurate when compared to its conventional version for the medium and high r value categories fig 3b c 4b c the lengths of n1 and n2 did not appear to be factors affecting the performance of either version but r values may have been excluding the ktrl wt the wt based versions generally performed better higher in both precision and accuracy when estimating the lower percentiles this was mostly noticeable for the ols wt move1 wt and move2 wt figs 3 4 for the high r value category despite the ktrl2 wt and rloc wt having had better accuracy compared to their respective conventional versions they did not always have better precision fig 3c and 4c additionally the conventional ktrl mostly outperformed its wt based version the better performance generally observed for the wt based methods in estimating lower percentiles implies that the wt based methods except for the ktrl wt move3 wt and move4 wt might serve as better methods in determining future low flow values which could indicate dry periods in summary at low percentiles the methods that performed well were move1 move1 wt move2 move2 wt ktrl2 ktrl wt and rloc rloc wt but the wt version generally showed better performances it was not surprising that the ktrl2 and ktrl2 wt were among the best performing methods as these methods were designed to find the best slope and intercept in estimating the percentiles of extended records khalil et al 2012 furthermore the ktlr2 and ktrl2 wt were robust in the presence of outliers as median values were used when estimating the slope of the line 3 1 3 2 higher percentiles the 75th 95th similar to the estimation of the lower percentiles the worst performance for precision was typically observed for move3 move3 wt and move4 move4 wt though the wt based versions tended to have better precision especially at higher lengths of n1 for the ols ols wt move1 move1 wt and move2 move2 wt the precision performances between the two versions were competitive but in many cases the wt based methods outperformed even if only slightly their respective conventional version e g figs 5 6 finally the wt based versions in the ktrl wt ktrl2 wt and rloc wt consistently showed increased precision compared to their respective conventional versions figs 5 6 again this study demonstrated the ability of the ktrl2 to minimize the error in percentile estimations while remaining robust in the presence of outliers khalil et al 2012 and the ktrl2 wt did even better by outperforming its conventional version in terms of accuracy the ols ols wt ktrl ktrl wt move3 move3 wt and move4 move4 wt typically showed the lowest accuracy the ols ols wt and ktrl ktrl wt consistently underestimated the values of the higher percentiles which could be due to the presence of outliers that might have been significant in all datasets used even so the ktrl wt consistently outperformed the conventional version the ols and ols wt showed very similar rbias values and thus their performances were considered comparable fig 6 for move3 move3 wt and move4 move4 wt both overestimations and underestimations in higher percentiles were observed for move3 move3 wt and move4 move4 wt the conventional versions tended to show better accuracy at lower n1 120 and 240 months while the wt based versions were better at higher n1 360 months regardless of the lengths of n2 and values of r this could indicate that for the wt based versions in move3 wt and move4 wt to produce higher accuracy than their respective conventional versions longer concurrent data are needed in estimating higher percentiles low rbias values high accuracy were observed in the move1 move1 wt move2 move2 wt ktrl2 ktrl2 wt and rloc rloc wt for move1 move1 wt and move2 move2 wt the wt based versions generally outperformed the conventional versions though not for all estimates in the higher percentiles for example for the 75th 80th percentiles the move1 wt was more accurate than the move1 but for the 80th 95th this was reversed fig 6 for the ktrl2 ktrl2 wt and rloc rloc wt the wt based versions generally outperformed their respective conventional versions figs 5 6 as can be seen the wt based methods may serve as better techniques for determining future high flow values which could indicate wet periods methods with the highest performance both in precision and accuracy in the estimations of higher percentiles were the move1 wt move2 wt ktrl2 wt and rloc wt for percentile estimations the superior performance of the wt based methods was more evident for estimations of higher percentiles than for lower percentiles 3 2 ratio of the estimated means and standard deviations of the extended records over their respective observed values the ratio of the estimated means and standard deviations of the extended records over their respective observed values were represented graphically using box plots the accuracy of each method was reflected by the closeness of the median value of the box plot to 1 and the symmetry of the box and whiskers around 1 the precision of each method was reflected by the amount of dispersion of the box and whiskers due to space limitations only selected box plots are presented based on the low medium and high r value categories 3 2 1 means of the extended records selected figures representing the low medium and high r value categories are shown in figs 7 8 and 9 respectively all methods except for the ktrl and ktrl wt had median values near 1 for the estimations of the means of the extended records the ktrl and ktrl wt highly underestimated the means particularly in the low and medium r value categories and the two versions had poor symmetry around 1 figs 7 8 in some cases over 75 of the mean values were underestimated however the performance of the ktrl wt was generally better than its conventional version fig 7a b in other cases the ktrl wt had only slight underestimations with medians very close to 1 while noticeable underestimations were seen in the corresponding conventional ktrl fig 9 for the ols and ols wt in the low r value category overestimations or underestimations were observed and both versions tended to show similar performances for example fig 7a in the medium and high r value categories the medians of the box plots of both versions were very close to 1 indicating good accuracy figs 8 9 furthermore the symmetry of the box and whiskers around 1 for the ols and ols wt was very similar thus the performances of both versions of the ols were comparable at higher r values slight overestimations and underestimations were also observed in both versions in move3 move3 wt and move4 move4 wt generally the closeness of the medians to 1 between the two versions was comparable in the low r value category fig 7 but the wt based versions had medians that were closer to 1 in the medium and high r value categories figs 8 9 indicating a higher accuracy which is supported by findings of the rbias values shown in table 4 for move1 move1 wt and move2 move2 wt regardless of the parameters used almost all medians were close to 1 the box and whisker plots were also relatively symmetric around 1 figs 7 9 overestimations and underestimations were rarely observed for these methods and when they occurred they were minimal overall move1 move1 wt and move2 move2 wt had good accuracy and the performances of both versions were comparable finally for the ktrl2 ktrl2 wt and rloc rloc wt the medians were also close to 1 and the box and whisker plots had relatively good symmetry around 1 indicating good accuracy in both versions figs 7 9 minimal overestimations and underestimations were seen for both versions of these methods and when they occurred both versions showed similar amounts as can be seen in terms of the accuracy measure the closeness of the medians to 1 was generally comparable between the two versions except for the ktrl ktrl wt where underestimations of the median values were consistently observed even so the ktrl wt consistently had less underestimation compared to its conventional version reflecting a more accurate performance similarly in terms of box plot symmetry both versions of most methods generally showed similar symmetry around 1 again except for the ktrl and ktrl wt in terms of precision for the move methods it was generally observed that move3 move3 wt and move4 move4 wt showed the highest degree of dispersion i e least precise figs 7 9 but there was no clear pattern to indicate the more precise version it is also noted that in a number of cases the presence of outliers was more noticeable in move3 move4 move3 wt and move4 wt compared not only to the other move methods but also the rest of the methods this is seen both in the estimation of the means and standard deviations in the following subsection of the extended records e g fig 7b 9a 10a 11b 12a this observation indicates poor performances by move3 move3 wt and move4 move4 wt in estimating the means and standard deviations of the extended records for data with outliers the poor performance by these methods is also reflected in their rrmse and rbias values tables 3 6 where in most cases the highest values are observed for move3 move4 move3 wt and move4 wt the ktrl2 ktrl2 wt and rloc rloc wt also showed high degrees of dispersion and often were even more dispersed than move3 move3 wt and move4 move4 wt e g figs 7 8b making them among the least precise methods even so in the majority of cases the ktrl2 wt and rloc wt were less dispersed higher precision compared to their conventional counterparts figs 7 9 similarly the ktrl wt was generally less dispersed than its conventional version again implying a higher precision fig 7 8a 9b finally the ols ols wt move1 move1 wt and move2 move2 wt consistently showed the highest precision i e least dispersion additionally both versions had similar degrees of dispersion indicating similar precision performances figs 7 9 it was also noted that despite the ols ols wt highly overestimating or underestimating the estimated means of the extended records at low r values low accuracy the methods maintained good precision this is supported by their rrmse values that were often the lowest i e high precision but they did not necessarily have the lowest rbias values tables 4 and 5 based on the results of the box plots for estimating the means of the extended records it appeared that among the wt based methods the ktrl wt ktrl2 wt and rloc wt showed the most improvement in performance compared with their conventional versions 3 2 2 standard deviations selected figures representing the low medium and high r value categories are shown in figs 10 11 and 12 respectively the median values seen in the box plots of the ratio of standard deviations showed that both underestimations and overestimations of the standard deviation of the extended records occurred however underestimations were larger and more prevalent figs 10 12 there were no methods that showed high amounts of overestimation while high underestimations were consistently observed in the ols ols wt and ktrl ktrl wt close to or even higher than 75 of the standard deviations were underestimated even for the high r value category figs 10 12 underestimating the variance of the extended records is indeed the main weakness of the ktrl method khalil and adamowski 2014a the ols also produces extended records with an underestimated variance in the presence of outliers khalil and adamowski 2014a as such highly underestimated standard deviations observed in the current study could indicate that the outliers in the datasets used are significant similarly in terms of the symmetry around 1 both versions of methods showed relatively good performance except for the ols ols wt and ktrl ktrl wt where the poorest symmetry was observed i e the worst accuracy performance for estimating the standard deviations of the extended records figs 10 12 poor performances are also reflected in the rbias values of the ols ols wt and ktrl ktrl wt where in most cases the values were the highest i e lowest accuracy table 6 for move types 1 4 the closeness of the medians to 1 produced was almost always comparable between the two versions of each move method figs 10 12 for the ktrl2 ktrl2 wt and rloc rloc wt the median values produced were relatively close to 1 however in many cases the wt based versions particularly in the rloc wt tended to show medians that were closer to 1 indicating better accuracy e g figs 10 12 based on the closeness of the medians to 1 and the symmetry of the box and whiskers around 1 among the wt based methods the ktrl2 wt and rloc wt showed the highest potential in producing more accurate estimations of the standard deviations compared to their respective conventional versions this observation was also supported by the rbias values table 6 where the ktrl2 wt rloc wt and ktrl wt were the most consistent wt based methods for producing lower rbias values compared with their respective conventional version in terms of the degree of dispersion of the box and whiskers both versions in the ols ols wt move1 move wt and move2 move2 wt consistently had comparable performances across the different parameters n1 n2 and r values figs 10 12 moreover they consistently displayed the least degree of dispersion implying the highest precision figs 10 12 additionally the rrmse values for move1 move wt and move2 move2 wt were often the lowest i e highest precision table 6 however there were no clear patterns that could be used to determine if one version was consistently more precise than the other and often the rrmse values between the two versions of move1 move wt and move2 move2 wt were equal table 6 finally although the ols and ols wt had poor accuracy performance these methods had good precision among the move methods move3 move3 wt and move4 move4 wt showed the highest degree of dispersion i e the least precision and often were the most dispersed among all the methods however there were no consistent patterns to indicate if one version was more precise than the other in addition to move3 move3 wt and move4 move4 wt the ktrl2 ktrl2 wt and rloc rloc wt often had a high degree of dispersion low precision and the wt based methods appeared to be more precise particularly for the medium and high r value categories figs 11 12 supported by the rrmse values at all high and most medium r values the ktrl2 wt and rloc wt were more precise than their respective conventional version table 6 similarly the ktrl wt was consistently less dispersed than its conventional version at the high r value categories fig 12 which was also reflected by their rrmse values table 6 this suggests that in estimating the standard deviations of the extended records higher r values appear to be a factor in the improved precision performance of the ktrl wt and rloc wt over their conventional counterparts while the ktrl2 wt outperformed its conventional version at all r value categories from the results of the boxplots it can be concluded that among the wt based methods the ktrl2 wt and rloc wt showed the highest improvement in accuracy and precision performances compared to their conventional versions in estimating the standard deviations of the extended records 4 conclusions this study incorporated the dwt using the à trous algorithm to eight existing record extension methods namely the ols move types 1 4 ktrl ktrl2 and rloc to extend streamflow records using empirical data from a total of 67 pairs of rhbn stations that had at least 35 years of continuous data comparisons in performance were made between each conventional method and its respective wt based version the performances were assessed based on accuracy and precision measures in estimating individual data values means standard deviations and a series of percentiles of the extended records for the estimation of individual data values of the extended records improvement in the wt based methods over their respective conventional version was inconsistent for example the wt based versions generally showed improved precision except for the ktrl wt over their conventional counterparts for the low and high r value categories while for the medium r value category the improvement was minimal in terms of accuracy noticeable improvements in the wt based methods except for the ktrl wt were seen for the low r value category while for the medium and high r value categories the improvements were scattered with no discernable patterns for the estimations of the means of the extended records the improvements in terms of both precision and accuracy were most noticeable in the ktrl wt ktrl2 wt and rloc wt in comparison to their respective conventional versions the levels of correlation r values appeared to be a more significant factor in improving the performance compared to the lengths of the concurrent and extended records finally the most consistent improvement in the wt based methods was seen in the estimations of the standard deviations of the extended records based on the rrmse and rbias values and the boxplot results it was generally observed that the wt based methods were relatively consistent except for move3 wt and move4 wt in improving accuracy compared to their respective conventional version additionally the ktrl wt ktrl2 wt and rloc wt were the wt based versions that showed the most consistent improvements in precision over their respective conventional version this study demonstrated that the ktrl wt ktrl2 wt and rloc wt were the wt based methods that outperformed their conventional versions both in precision and accuracy measures in estimating the statistical moments means and standard deviations of the extended records khalil and adamowski 2014a showed that for extending water quality records the ktrl2 and rloc had a maximum performance when outliers were present as was the case in the present study the ktrl2 and rloc were also better able to maintain the characteristics of the entire distribution in the presence of outliers khalil and adamowski 2014b it should be noted that in estimating the means and standard deviations of the extended records for cases where the wt based methods did not outperform their conventional version underperformance is not necessarily implied in many cases the rbias and rrmse values between the two versions were equal this was noticeable mainly for the ols ols wt move1 move1 wt and move2 move2 wt finally although improvements in the wt based versions in move3 wt and move4 wt were observed in estimating the means and standard deviations of the extended records the improvements were inconsistent and lacked clear patterns for the estimations of the lower percentiles the 5th 20th the wt based methods except for the ktrl wt generally performed better at higher levels of correlations while at lower levels of correlations the conventional versions performed better when estimating the higher percentiles the 75th 95th the wt based methods generally showed better precision compared with their conventional versions in terms of accuracy at higher percentiles the ktrl wt ktrl2 wt and rloc wt outperformed their respective conventional versions while the rest of the wt based methods had comparable accuracy performances to those of their respective conventional versions this present study was thus able to show the advantage of the ktrl wt and ktrl2 wt methods which were designed to estimate percentiles over their respective conventional versions this study demonstrated how the wt based methods have the potential to be used instead of the conventional methods for extending streamflow records that contain outliers furthermore since there were no noticeable differences in the computational effort between the wt based and conventional versions the wt based methods are an improvement for streamflow record extensions particularly when estimating the means standard deviations and higher percentiles generally the improvements seen in the wt based methods were more affected by the levels of correlations rather than the lengths of concurrent or extended records since this present study showed the potential of incorporating wt into record extension methods future studies can further explore other applications of the proposed approach in this study the datasets used were all complete without missing values or data gaps this was due to the dwt used being unable to accommodate unequal sampling intervals with somewhat greater complexity some forms of wavelet analysis such as slepian wavelets can deal with data that have equal sampling intervals but contain missing observations i e irregular sampled data for such data slepian wavelets would assume that the sampling intervals are a product of a stationary sequence in order to estimate the overall energy and be able to associate it with a specific scale further information on these wavelets can be found in mondal and percival 2010 future studies should investigate how such wavelets could be used to extend records in hydrological data that contain gaps or scattered missing values alternatively other available techniques that can be used to estimate missing data points such as those that belong to empirical statistical and function fitting methods more detailed explanations on these various categories of methods are covered in studies such as kashani and dinpashoh 2012 lee and kang 2015 sattari et al 2017 could be explored these methods might be particularly useful for mountainous and forested areas where data scarcity is very often encountered kashani and dinpashoh 2012 after applying the appropriate techniques to infill the missing records or observations the resulting complete data may be processed using the wt based methods proposed in this present study future studies can also look into applying the proposed approach on cases where outliers are absent or if present how the magnitude and number of outliers may affect the performance of the wt based methods declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgement this research was funded by an nserc discovery grant an nserc accelerate grant and a william dawson scholar fund held by jan adamowski the first author was also financially supported by the liliane and david m stewart water resources management ph d scholarship at mcgill university 
5828,sufficient hydrological data such as streamflow are important to represent the long term characteristics of a watershed in order to support decision making policy and management lack of data remains one of the main challenges of hydrological analyses therefore the main goal of this study is to extend streamflow records using a proposed approach where the wavelet transform wt is incorporated as a pre processing method into eight existing record extension techniques namely the ordinary least square regression ols maintenance of variance move types 1 4 kendall theil robust line ktrl ktrl2 and robust line of organic correlation rloc the performance of the wt based methods in estimating individual data values means and standard deviations of the extended records and a series of percentiles of the extended records was then compared with that of the respective conventional methods i e without the wt the data used in the analysis consisted of 67 pairs of target index stations that were obtained from canada s reference hydrometric basin network database all of which contain outliers as such the results and discussions obtained are applicable for cases where the data contain outliers the wt based methods particularly the ktrl wt ktrl2 wt and rloc wt demonstrated consistent improvements in precision and accuracy compared with their conventional counterparts especially in estimating the means and standard deviations of the extended records in estimating individual data values the wt based methods showed inconsistent improvements finally in terms of percentiles greater improvements were seen in the estimation of higher percentiles more specifically for the move1 wt move2 wt ktrl2 wt and rloc wt compared with their conventional counterparts keywords streamflow hydrologic time series record extension methods wavelet transform wavelet decomposition 1 introduction the availability and reliability of sufficiently detailed watershed information including hydro meteorological data such as streamflow and precipitation is required for effective and sustainable water resources management panu et al 2000 horne 2015 in practice however the availability of data for long periods is often lacking despite there being no universal guidelines for the minimum length of data required for optimal planning and management of water resources raman et al 1995 argues that a minimum of 35 years of data are required to produce reliable results burn et al 2010 only uses stations with a minimum of 50 years of streamflow data to ensure statistical validity when studying the trends in hydrological extremes for canadian watersheds partal 2010 recommends a minimum of 40 years a length also used in many other studies e g nalley et al 2012 2013 zhang et al 2013 tan and shao 2017 other studies have used even longer datasets for example de jongh et al 2006 zhang et al 2007 and sauchyn et al 2015 use more than 100 years of data to study temperature trends in belgium investigate the long term reliability of the athabasca river as a freshwater source and the long term variability in the annual maximum flow of the yangtze river in china respectively continuous 100 year datasets are however extremely rare the present study focuses on addressing the issue of short data lengths using data from stations in canada by proposing a new approach that incorporates the wavelet transform wt into existing streamflow record extension methods there are a variety of reasons why sufficient data may not be available in canada for example hydrometric stations are not evenly and well distributed across the country most stations with extensive data sets are located in southern canada resulting in short data or data gaps for the rest of the country equipment malfunctions and seasonal shutdowns can also result in missing observations in canada the reference hydrometric basin network rhbn contains a network of stations that record data that are considered ideal for hydrological and climatic related studies monk et al 2011 out of over 200 rhbn stations there are only 73 that have at least 40 years of continuous streamflow data nalley et al 2019 similarly when studying the changes in the annual mean daily streamflow over several decades 156 rhbn stations have 30 years of records and that number decreases significantly to 25 for 60 years of records khaliq et al 2009 consequently the development of new approaches to improve data lengths in streamflow is needed commonly used stochastic record extension methods include the ordinary least square ols maintenance of variance move types 1 2 3 and 4 and kendall theil robust line ktrl more recently the ktrl2 proposed by khalil et al 2012 and the robust line of organic correlation rloc proposed by khalil and adamowski 2012 were found to be useful in extending hydrological records due to their robustness in the presence of outliers and their ability to maintain the variance of the extended records khalil and adamowski 2014a the ols technique is a parametric method which assumes normally distributed and independent residuals one main deficiency of the ols method is that it can be sensitive to the presence of outliers which are often present in hydrological data causing the variance in the produced records to be underestimated hirsch et al 1982 vogel and stedinger 1985 helsel and hirsch 2002 khalil et al 2010 for data that have extreme values or outliers and depart from normality the ktrl has been shown to be an effective non parametric estimator thus it is more efficient than the ols hirsch et al 1991 nevitt and tam 1998 helsel and hirsch 2002 the ktrl however may still produce estimated records with underestimated variance khalil and adamowski 2014b also helsel and hirsch 2002 have shown that move techniques are advantageous as they can maintain the variance of estimated records as such move techniques have been applied in many hydrological data extension and generation studies e g moog et al 1999 hernández henríquez et al 2010 ryu et al 2010 duan et al 2013 although these studies demonstrate the usefulness of move techniques these techniques are also sensitive to the presence of outliers khalil and adamowski 2014a b in order to manage the presence of outliers and the underestimation of the variance of extended records the ktrl2 method modifies the intercept and slope of the regression model in the ktrl in order to minimize percentile estimation errors the ktrl2 aims to produce records with a cumulative distribution function cdf that approximates the actual records well therefore the ktrl2 essentially has the advantages of the ktrl method with the added advantage of maintaining the variance of the percentile estimations of extended records khalil et al 2012 similarly khalil and adamowski 2012 propose the rloc as a new regression model which is a modified move type 1 method also referred to as line of organic correlation loc that can maintain the variance of the extended records and remain robust in the presence of outliers the rloc uses the inter quartile range of concurrent records when estimating the slope allowing for an estimation of the intercept it is commonly recognized that noise and other random components are parts of the makeup of hydrologic time series minville et al 2008 sang et al 2009 rai et al 2010 cleaner signals which may be obtained by separating the deterministic and noise components have been used to improve the performances of many hydrologic models wu et al 2009 rajaee et al 2011 sang et al 2013 signal pre processing using data decomposition methods such as wavelet analysis has been demonstrated to be a useful approach in obtaining cleaner signals liu et al 2013 santos and silva 2014 nourani et al 2017 hadi and tombul 2018 the information obtained through data decomposition is then used as inputs in another data driven model this approach has been applied in areas such as streamflow forecasting e g kisi et al 2011 belayneh et al 2014 djerbouai and souag gamane 2016 honorato et al 2018 fouchal and souag gamane 2019 but not in streamflow record extension streamflow record extension studies that used techniques such as the ols move ktrl and rloc do not generally pre process or decompose the data before using these record extension techniques in light of this the present study aims to pre process the streamflow data using the wavelet analysis approach in order to obtain cleaner signals prior to inputting these cleaner signals into the record extension methods the aim of using wavelet analysis is to capture hidden information in the data which helps provide a more concise representation of the original time series shoaib et al 2014 shafaei et al 2016 quilty and adamowski 2018 the applications of the discrete wavelet transform dwt in various aspects of hydrological research have been wide ranging including trend detection short and long term forecasting and synthetic data generation hybrid models which combine dwt and data driven or artificial intelligence ai methods have led to significant improvements in hydrological forecasting examples of the most common data driven or ai methods used in hydrological forecasting include artificial neural network support vector regression genetic programming and fuzzy logic although these methods are not discussed in detail here as they are not within the scope of the current work the contribution of the dwt in improving these models performance is notable and motivated this study to combine dwt with record extension methods dwt has improved various forecasting models by i providing the time frequency and physical structure of the analyzed signal ii extracting features of the analyzed signals prior to using them as inputs and iii de noising the original signal which results in cleaner signals for use as inputs into ai or data driven models nourani et al 2014 shafaei et al 2016 fahimi et al 2017 despite the potential uses and benefits of the dwt approach e g in forecasting researchers have not explored in any detail the use of dwt in streamflow record extension therefore the main objective of the present study is to propose and develop a method that couples the dwt using the à trous algorithm with eight record extension techniques namely the ols move1 move2 move3 move4 ktrl ktrl2 and rloc to extend streamflow records more specifically the wt based and conventional methods performances were assessed and compared with the estimations of individual data values means and standard deviations of the extended records and a series of percentiles 5th 95th of the extended records to the best of the authors knowledge this study is the first to incorporate the use of dwt via the à trous algorithm into the aforementioned eight existing record extension techniques to extend streamflow records the data used were from 67 rhbn stations in canada the estimations of the extended records were computed using various lengths of concurrent and extended records and levels of correlations 2 materials and methods 2 1 study sites and streamflow data monthly streamflow averages from rhbn stations in canada were used for this study these stations are a sub set of the national network monitoring stations with high quality data that make them ideal for hydroclimatic studies the accuracy of this data is checked by local experts who consider the hydraulic condition the stage discharge relationship channel geometry and ice condition zhang et al 2001 coulibaly and burn 2004 for each pair of target and index stations a minimum of 35 years 420 months of continuous concurrent records were sought continuous records were used because the computation of the wt requires that datasets have consistent time intervals there were 67 pairs of target index rhbn stations that qualified in which each pair had common monthly records of 35 years 420 months fig 1 indicates the locations and the climatic ecozone of each of the rhbn stations used in this study 2 2 record extension procedures and theoretical background this study compared the performance of eight conventional record extension methods with their respective wt based versions which is the new approach proposed in this study the analytical framework of the wt based record extension method is given in fig 2 b both sets of methods were applied to the monthly streamflow averages of the 67 pairs of the target index stations the time series of the index and target stations were represented by xt and yt respectively the target station had a shorter number of records represented by n1 where t 1 2 3 n1 which was extended up to the data length of the index station n1 n2 where t 1 2 3 n1 n1 1 n1 2 n1 3 n1 n2 thus n1 and n2 represent the lengths of concurrent records and extended records respectively the data analysis was carried out in matlab according to the following step by step procedure fig 2a 1 an index station x was determined for each target station y based on spearman s rank correlation r the data analysis was then conducted using three categories of r values low around 0 5 medium around 0 7 and high around 0 90 and for each category there were 67 pairs of target index stations 2 from the 35 years 420 months of common data three lengths of concurrent records n1 were chosen to be included in the analysis 10 years 120 months 20 years 240 months and 30 years 360 months as such the applications and conclusions obtained from this study should be applicable not only for concurrent records of 30 years but also for shorter data i e 20 years for each n1 the lengths of extended records n2 were 12 months 24 months 36 months 48 months and 60 months therefore there were a total of 45 combinations 3 r values 3 lengths of n1 and 5 lengths of n2 3 for each combination record extensions using the conventional methods i e the ols move types 1 4 ktrl ktrl2 and rloc were applied to each of the 67 pairs of the target index stations 4 record extensions using the wt based methods were performed fig 2b as follows a for each length of n1 the dwt was computed separately on the original streamflow data of the target and index stations the dwt computations produced the respective detail d and approximation a subseries of the target and index stations b for each n1 the d and a subseries were then used as inputs in the computation of the ols move types 1 4 ktrl ktrl2 and rloc to produce the extended records for the various lengths of n2 more detailed information on the procedure of constructing extended data using d and a subseries is found in section 2 5 c the performances of the conventional and the wt based models were evaluated and compared based on the different r values categories lengths of n1 and lengths of n2 three performance measures were used i relative bias rbias ii relative root mean square error rrmse and iii the ratio of the predicted mean and standard deviation over the respective actual mean and standard deviation 2 3 detection for outliers most record extension techniques e g the parametric ols and move types 1 4 are sensitive to the presence of outliers khalil and adamowski 2014a b for example estimations produced using the ols method are based on the means and standard deviations of the data thus this method is less robust when confronted by extreme values or outliers helsel and hirsch 2002 khalil and adamowski 2014a b similarly the slopes and intercepts used in the move techniques are also based on the mean and standard deviation of the data which are also influenced by the presence of extreme values or outliers helsel and hirsch 2002 khalil and adamowski 2012 in the present study the presence of outliers was first assessed on the n1 of the target and index stations using box plots it was found that for all lengths of n1 all pairs of target index stations had outliers figures not shown the data analysis conducted included the outliers they were not treated nor removed because this study aims to compare the performance of the conventional methods against the wt based approach when they are confronted with the presence of outliers additionally since the main use of the wt is to pre process the original streamflow data the outliers if they exist should be included therefore the results of this study are applicable for cases where outliers are present in the data 2 4 record extension methods a review of the literature showed that the performance of record extension methods is dependent on the conditions of the data parametric approaches e g the ols and move are highly influenced by factors such as a skewed distribution and the presence of outliers on the contrary non parametric approaches e g the ktrl ktrl2 and rloc are considered more robust in the presence of outliers the rloc also assumes symmetrical distributions the record extension methods used i e ols move ktrl ktrl2 and rloc intend to determine the best intercept c and slope m estimators for the regression equation 1 y m x c 2 4 1 ordinary least square regression ols the ols describes the covariation between the response y and predictor variables x the values of the intercept and slope in the linear regression are obtained to minimize the squared error of the estimated values the ols has the following form draper and smith 1966 2 y t y r r sd y r sd x r x t x r y t represents the estimated value for the target station using the corresponding observed value of the index station at x t for the concurrent period t 1 n1 the mean values of the flow data during the concurrent period are y r and x r sd y r and sd x r are the standard deviations during the concurrent period as described in eq 2 the estimated values are based on the mean and standard deviation of the data which are more easily influenced by the presence of extreme values 2 4 2 maintenance of variance extension move move techniques are more advantageous than the ols as they maintain the variance of the extended records hirsch 1982 vogel and stedinger 1985 helsel and hirsch 2002 in move type 1 the slope and intercept are determined so that the entire estimated values of y t for t 1 2 n1 n2 have the mean and standard deviation of the concurrent period y r and sd y r respectively move type 1 is written as hirsch 1982 3 y t y r s i g n r sd y r sd x r x t x r move type 1 is similar to the ols except it uses the sign of r instead of the value of r while the ols produces records with underestimated variance move type 1 produces extended records having the properties of the records they intend to present hirsch 1982 helsel and hirsch 2002 koutsoyiannis and langousis 2011 in move type 2 the slope and intercept are determined so that the entire estimated values of y t for t 1 2 n1 n2 have an unbiased estimate of the mean μ y and variance σ y 2 as proposed by matalas and jacob 1964 4 μ y y r n 2 n 1 n 2 β x e x r 5 σ y 2 β 2 sd x 2 1 n 1 n 2 3 n 1 3 n 1 n 2 1 n 1 1 n 1 2 sd y r 2 β sd x r 2 where vogel and stedinger 1985 6 β t 1 n 1 x t x r y t y r t 1 n 1 x t x r x e represents the mean value of xt for t n1 1 n1 2 n1 n2 the equation for move type 2 is hirsch 1982 7 y t μ y σ y sd x x t x move types 1 and 2 are used to produce records at t n1 1 n1 2 n1 n2 however vogel and stedinger 1985 found that slope and intercept estimators for the two move techniques were not able to achieve the intended purpose for move type 3 vogel and stedinger 1985 proposed slope m and intercept c estimators that can be used to produce extended records so that the n1 n2 records of the target station have a mean and variance according to those proposed by matalas and jacobs 1964 in eqs 4 and 5 respectively the estimators proposed by vogel and stedinger 1985 are 8 m n 1 n 2 1 σ y 2 n 1 1 sd y r 2 n 1 y r μ y 2 n 2 a μ 2 n 2 1 sd x e 2 9 c n 1 n 2 μ y n 1 y r n 2 the mean and variance estimators used in move types 2 and 3 are not ideal for small samples vogel and stedinger 1985 therefore vogel and stedinger 1985 proposed improved versions of the mean μ y and variance σ y 2 estimators 10 μ y y r θ 1 n 2 n 1 n 2 β x e x r 11 σ y 2 1 θ 2 sd y r 2 θ 2 β 2 sd x 2 1 n 1 n 2 3 n 1 3 n 1 n 2 1 n 1 1 n 1 2 sd y r 2 β sd x r 2 where θ 1 and θ 2 are vogel and stedinger 1985 12 θ 1 n 1 1 ρ 2 n 1 4 ρ 2 1 13 θ 2 n 1 4 ρ 2 n 1 8 5 ρ 2 4 5 the estimated cross correlation of the concurrent flow time series between the index xt and target yt stations is represented by ρ vogel and stedinger 1985 14 ρ β sd x r sd y r using the mean and variance estimators given in eqs 10 and 11 respectively vogel and stedinger 1985 proposed move type 4 the main goal of move type 4 is to find slope m and intercept c estimators so that the sequence of records n 1 n 2 produced for the target station would have a mean of μ y and variance of σ y 2 to achieve that the μ y and σ y 2 in eqs 4 and 5 are replaced with μ y eq 10 and σ y 2 eq 11 respectively again it can be seen that all four types of move methods involve the use of the mean and variance making these techniques sensitive to the presence of extreme values or outliers khalil et al 2012 2 4 3 the kendall theil robust line ktrl and ktrl2 the ktrl and ktrl2 methods are more robust than the move methods in the presence of outliers unlike the move methods which use the mean and variance the ktrl methods are based on the kendall rank correlation coefficient helsel and hirsch 2002 khalil et al 2012 khalil and adamowski 2014a b the ktrl robust slope estimator is calculated in a pair wise manner by comparing each pair of records to all other pairs for a pair of data xt and yt in a total of n pairs there will be n n 1 2 pair wise comparisons for each pair wise comparison a slope of δyt δxt is calculated and the median of all the pair wise slopes is used as the nonparametric slope estimate m kt theil 1950 15 m kt m e d i a n y t q y t k x t q x t k f o r a l l k q k 1 2 n 1 1 q 2 3 4 n 1 for all k q k 1 2 n 1 1 q 2 3 4 n 1 the intercept c k t is calculated as 16 c kt m e d i a n y t m kt m e d i a n x t the line of the ktrl is fitted to go through the median x and median y helsel and hirsch 2002 in the ktrl2 the modification of the slope and intercept was done to create extended records with a cumulative distribution function cdf as close as possible to the cdf of the actual records thus the ktrl2 more specifically aims to reduce the error related to the estimation of the percentiles of the extended records the modified slope m k t 2 and intercept c k t 2 are calculated as follows khalil et al 2012 17 m k t 2 m e d i a n y t q y t k x t q x t k for all k q k 5 t h 10 t h 15 t h 90 t h q 10 t h 15 t h 20 t h 95 t h 18 c k t 2 m e d i a n y t m k t 2 m e d i a n x t y t q and x t q represent the percentiles of y r and x r respectively during the concurrent period since the percentiles are taken at increments of 5 from the 5th to the 95th percentile there are a total of 19 x t q y t q pairs of percentiles the computation n n 1 2 19 19 1 2 produces 171 pair wise comparisons for each comparison a slope δyt δxt is determined and the median of the 171 pair wise slopes is used as the slope estimate 2 4 4 robust line of organic correlation rloc the rloc method was proposed by khalil and adamowski 2012 as a modification of move type 1 that aims to overcome its sensitivity to outliers the presence of outliers may cause the variance produced to be greater than it is supposed to be since calculating variance involves squared deviations from the mean the rloc method is less sensitive to outliers since its slope and intercept estimators are based on the inter quartile range iqr which focuses on the central half of the data and thus is not affected by either extremely high or low values khalil and adamowski 2012 the slope m r l and intercept c r l estimators used in the rloc are computed using khalil and adamowski 2012 19 m rl y t 75 y t 25 x t 75 x t 25 20 c rl m e d i a n y r m rl m e d i a n x r y t 75 a n d y t 25 represent the 75th and 25th of yt during the concurrent period and x t 75 a n d x t 25 are the counterparts for the xt 2 5 wt based model coupling dwt with existing record extension methods in this study dwt using the à trous haar algorithm was used to decompose the streamflow time series of target and index stations studies such as quilty and adamowski 2018 and du et al 2017 have indicated that despite the growing popularity of the dwt approach many studies have incorrectly applied the method in hydrological forecasting and prediction models for example common dwt approaches e g the dwt multiresolution analysis dwt mra and maximal overlap dwt multiresolution analysis modwt mra that have been used in data forecasting which are relevant to record extension require the existence of future data quilty and adamowski 2018 to obtain the reconstruction coefficients at a specific time e g time t data ahead of a time t are required in the case of dwt mra and modwt mra to calculate the scaling approximation series and wavelet detail series coefficients as these approaches use an iterative reconstruction process where some future data are necessary quilty and adamowski 2018 therefore these methods are unusable for real life forecasting scenarios because future data would be unavailable on the contrary using the à trous algorithm in the decomposition process does not require future data because the scaling coefficients used to obtain the reconstruction coefficients which are the summation of the wavelet and scaling coefficients at any time t are obtained using data at time t renaud et al 2005 quilty and adamowski 2018 as such the decomposition using the à trous dwt method is applicable for real life situations which is the main reason for the use of this approach in the present study furthermore the decimation process used in the traditional pyramid dwt approach decreases the lengths of wavelet coefficients down to half as the scale increases to the next level shensa 1992 gonzález audícana et al 2005 li and cheng 2014 this is not ideal for predicting or forecasting purposes because there is not enough information available li and cheng 2014 this issue can be overcome by using the à trous wavelet decomposition where the lengths of the wavelet coefficients are always equal to that of the original signal this is a non decimated process where zeros are inserted instead of decimating the lengths of the wavelet coefficients shensa 1992 li and cheng 2014 the empirical datasets used in this study had the extended future data which were used to validate the results and subsequently evaluate the performances of the conventional record extension methods and their respective wt based versions the convolution of an original signal e g for xt t 1 n1 n1 1 n1 n2 using the dwt with the à trous algorithm at a scale a and translation factor b a and b are integers can be expressed as renaud et al 2005 li and cheng 2014 21 a a t l l x b 1 t 2 a 1 l a a t represents the scaling coefficient approximation component at the scale a the computation in eq 21 using the haar wavelet with a low pass filter l consisting of 1 2 1 2 at a resolution e g the following is for level 1 can be written as renaud et al 2005 22 a a 1 t 0 5 a a t 2 a a a t the detail component at the scale a is then obtained by computing the difference between two consecutive scaling coefficients renaud et al 2005 li and cheng 2014 23 d a t a a 1 t a a t for each target index station pair the à trous dwt was applied separately to the concurrent records only not to the whole available data since in real life scenarios we would not have the extended or future data available for the index xt and target yt data therefore these dwt decompositions on the n1 would produce two d subseries and two a subseries all of which have lengths that are equal to n1 and they would be used as inputs into the eight record extension methods the two d subseries were first used as inputs into the eight methods and the same process was repeated for the two a subseries the final results for each wt based version were obtained by summing the results or values produced from using the d and a sets in the respective conventional record extension method for each combination of r value n1 and n2 2 6 evaluation of performance the performance of the conventional methods and their respective dwt based versions were compared and evaluated based on uncertainty which was quantified in the form of accuracy and precision measures accuracy refers to the ability of the model to correctly estimate the true value of interest leading to either an underestimation or overestimation of the true value whereas precision refers to the amount of variation in the estimates themselves or the measure of the repeatability of an estimate walther and moore 2005 kennard et al 2010 the uncertainty measurements were assessed using rbias rrmse and the ratios of the predicted mean and standard deviation of the extended records over their respective observed values rbias and rrmse reflected the accuracy and precision measures respectively the ratios of means and standard deviations also indicated accuracy and precision and were assessed using boxplots the accuracy was reflected by the closeness of the median shown in the boxplot to 1 and the symmetry of the boxplots around 1 the precision was reflected by the amount of dispersion of the box and whiskers of the boxplots khalil et al 2010 khalil and adamowski 2014a the rbias and rrmse were calculated as follows 24 rbias 1 n 2 t n 1 1 n 1 n 2 y t y t y t 25 rrmse 1 n 2 t n 1 1 n 1 n 2 y t y t y t 2 the rbias and rrmse were calculated for each target index pair the final rbias and rrmse values of all 67 target index pairs were averaged to obtain the final rbias or rrms value the rbias and rrmse of the extended records were assessed on i the individual data values ii means and standard deviations and iii percentiles ranging from the 5th to the 95th percentiles the performance evaluations were conducted for the various lengths of n1 120 240 and 360 months and n2 12 24 36 48 and 60 months and using the r values from the low r 0 50 medium r 0 70 and high r 0 90 categories 3 results and discussion the results of the empirical experiment in this study reflect scenarios where outliers are present in the data due to space limitations only selected figures were presented to illustrate the findings obtained i e not all figures are presented furthermore in each of the result tables tables 1 6 only a subset of the extended record lengths are presented i e n2 12 and 48 months for each length of concurrent record n1 3 1 relative rmse rrmse and relative bias rbias 3 1 1 individual data values the rrmse and rbias for the estimations of the individual data are shown in tables 1 and 2 respectively in terms of precision improvements in the wt based methods were most noticeable for the concurrent record length n1 of 360 months where most rrmse values were lower for the wt based methods except for the ktrl wt table 1 this could indicate that large concurrent records are needed in order for wt based methods to produce higher accuracy it appears that for data 240 months 20 years the conventional versions performed better than the wt based version but at 360 months 30 years the opposite occurred this improved performance of the wt based methods was particularly evident for the low and high r value categories regardless of the lengths of n1 and n2 and the values of r the only wt based version that did not show improvements in precision compared to its conventional version was the ktrl wt in terms of accuracy improvements in the wt based methods were mostly observed for the low r value category where almost all rbias values were lower for the wt based versions than for their respective conventional methods except for the ktrl wt table 2 for the higher r value category the improvements in the wt based versions were scattered it was noted that the ktrl wt never showed higher accuracy performance than the conventional version in estimating the individual data values for the extension of individual records the performance of the wt based methods appears inconsistent and the better performance of the wt based methods was mostly seen when r values were low the ktrl wt however never outperformed the conventional ktrl regardless of the lengths of concurrent and extended records and levels of correlation 3 1 2 means and standard deviations 3 1 2 1 means the rrmse and rbias values for the estimations of the means of the extended records are shown in tables 3 and 4 respectively generally the wt based methods led to an improvement in precision compared with the conventional methods in particular the ktrl2 wt and rloc wt outperformed their respective conventional versions in most cases furthermore the ktrl wt consistently performed better than its conventional version under any lengths of n1 and n2 table 3 the wt based versions of ols and move types 1 4 were less consistent in showing improved precision as a number of their rrmse values were equal to those of their respective conventional version though at high r values the precision of move3 wt and move4 wt were better than their respective conventional versions therefore it appears that the lengths of n1 and n2 were not a factor in determining the improved performance of the wt based methods when estimating the mean of the extended records for smaller sample sizes the performance of move4 both versions should be better compared to move2 both versions and move3 both versions since the mean and variance estimators for move2 and move3 are not ideal for small sample size in this present study there were no patterns indicating that the performance of move4 was better compared to that of move2 and move3 even for the smallest n1 of 240 months 20 years this might be because the monthly time series used in the study that spanned a period of 10 or 20 years still resulted in relatively large sample sizes finally the level of correlation increased the precision of only move3 wt and move4 wt compared with their conventional versions similarly higher accuracy in the wt based methods was consistently observed for the ktrl wt as its rbias values were always better than those of the conventional version in estimating the means of the extended records table 4 additionally most rbias values of the ktrl2 wt and rloc wt were better than their respective conventional version particularly for the medium 0 7 and high 0 90 r value categories table 5 r values also had stronger positive effects compared with the lengths on n1 and n2 on the accuracy performances of the wt based methods in ols wt and move wt types 1 to 4 at the low and medium r value categories move1 wt and move2 wt displayed better accuracy than their respective conventional versions conversely move3 wt and move4 wt showed improved accuracy at high r values table 4 it can be seen that when estimating the means of the extended records among the wt based methods the ktrl2 wt rloc wt and especially the ktrl wt showed the most improvement both in precision and accuracy over their conventional counterparts the ktrl ktrl2 and rloc methods were developed to be more robust in the presence of extreme values and outliers as they use outlier resistant estimators e g the medians and iqr when estimating the slope and subsequently calculating the intercept of the line this study demonstrated that for these outlier resistant methods their wt based versions produced even more accurate and precise results for data containing outliers 3 1 2 2 standard deviations the rrmse and rbias values for the estimations of the standard deviations of the extended records are shown in tables 5 and 6 respectively improved precision was consistently seen for the ktrl wt and ktrl2 wt compared with their respective conventional versions regardless of the lengths of n1 and n2 and the levels of correlations table 5 additionally in most cases the rloc wt also showed better precision than its conventional version it was noted that the lowest rrmse values for each case among all methods wt based and conventional were mostly observed for either move1 move1 wt or move2 move2 wt reflecting the ability of the move methods to maintain the variance of the estimated records more precisely in terms of accuracy in estimating the standard deviation of the extended records the wt based methods ols wt ktrl wt ktrl2 wt and rloc wt outperformed their conventional versions table 6 for move wt types 1 4 in most cases the wt based methods also performed better than their respective conventional versions table 6 the lowest rbias values were generally observed for one of the move or move wt methods again reflecting the ability of move to maintain variance of all the methods showing the lowest rbias values for estimating the standard deviation of the extended records the wt based version was superior more accurate to the respective conventional method in all cases except two out of 45 cases for medium r values at n1 240 n2 24 and at n1 360 n2 48 table 6 finally there were no patterns that could indicate if a single parameter i e lengths of concurrent records and extended records or the strengths of correlations was the cause of the better performances of the wt based methods in estimating the means and standard deviations of the extended records improvements in measures of precision and accuracy in the wt based methods were more apparent in the estimations of means and standard deviations of the extended records compared with the estimations of individual extended data values indeed when the purpose is to extend records rather than filling in missing values the means and standard deviations of the extended records provide more valuable information than the individual records themselves this is because the statistical moments rather than individual records are more useful for obtaining information on probabilities and probability distributions khalil and adamowski 2014a which may be useful for certain assessments such as for low and high flows 3 1 3 percentiles for all lengths of n1 and n2 and various r values lower percentiles the 5th 20th percentiles figs 3 4 displayed larger ranges of rrmse and rrbias values compared to higher percentiles the 75th 95th percentiles figs 5 6 this indicates that for all parameters used n1 n2 and r values the amount of overestimation in the precision measure and the amount of over underestimation in the accuracy measure were higher at lower percentiles than at higher percentiles furthermore the performance was noticeably more competitive across the different methods and versions at higher percentiles compared to lower percentiles 3 1 3 1 lower percentiles the 5th 20th at lower percentiles competitive performance of methods having high precision and high accuracy was generally observed for move1 move1 wt move2 move2 wt ktrl2 ktrl2 wt rloc and rloc wt the worst performance both in terms of precision and accuracy was consistently seen for move3 move3 wt and move4 move4 wt some of their values were beyond the cut off shown in figs 3 4 the performance of the ols and ols wt was influenced by the strength of correlations both versions performed poorly in the low r value category but performed well in the medium and high r value categories figs 3 4 specifically the ols wt was more precise and more accurate when compared to its conventional version for the medium and high r value categories fig 3b c 4b c the lengths of n1 and n2 did not appear to be factors affecting the performance of either version but r values may have been excluding the ktrl wt the wt based versions generally performed better higher in both precision and accuracy when estimating the lower percentiles this was mostly noticeable for the ols wt move1 wt and move2 wt figs 3 4 for the high r value category despite the ktrl2 wt and rloc wt having had better accuracy compared to their respective conventional versions they did not always have better precision fig 3c and 4c additionally the conventional ktrl mostly outperformed its wt based version the better performance generally observed for the wt based methods in estimating lower percentiles implies that the wt based methods except for the ktrl wt move3 wt and move4 wt might serve as better methods in determining future low flow values which could indicate dry periods in summary at low percentiles the methods that performed well were move1 move1 wt move2 move2 wt ktrl2 ktrl wt and rloc rloc wt but the wt version generally showed better performances it was not surprising that the ktrl2 and ktrl2 wt were among the best performing methods as these methods were designed to find the best slope and intercept in estimating the percentiles of extended records khalil et al 2012 furthermore the ktlr2 and ktrl2 wt were robust in the presence of outliers as median values were used when estimating the slope of the line 3 1 3 2 higher percentiles the 75th 95th similar to the estimation of the lower percentiles the worst performance for precision was typically observed for move3 move3 wt and move4 move4 wt though the wt based versions tended to have better precision especially at higher lengths of n1 for the ols ols wt move1 move1 wt and move2 move2 wt the precision performances between the two versions were competitive but in many cases the wt based methods outperformed even if only slightly their respective conventional version e g figs 5 6 finally the wt based versions in the ktrl wt ktrl2 wt and rloc wt consistently showed increased precision compared to their respective conventional versions figs 5 6 again this study demonstrated the ability of the ktrl2 to minimize the error in percentile estimations while remaining robust in the presence of outliers khalil et al 2012 and the ktrl2 wt did even better by outperforming its conventional version in terms of accuracy the ols ols wt ktrl ktrl wt move3 move3 wt and move4 move4 wt typically showed the lowest accuracy the ols ols wt and ktrl ktrl wt consistently underestimated the values of the higher percentiles which could be due to the presence of outliers that might have been significant in all datasets used even so the ktrl wt consistently outperformed the conventional version the ols and ols wt showed very similar rbias values and thus their performances were considered comparable fig 6 for move3 move3 wt and move4 move4 wt both overestimations and underestimations in higher percentiles were observed for move3 move3 wt and move4 move4 wt the conventional versions tended to show better accuracy at lower n1 120 and 240 months while the wt based versions were better at higher n1 360 months regardless of the lengths of n2 and values of r this could indicate that for the wt based versions in move3 wt and move4 wt to produce higher accuracy than their respective conventional versions longer concurrent data are needed in estimating higher percentiles low rbias values high accuracy were observed in the move1 move1 wt move2 move2 wt ktrl2 ktrl2 wt and rloc rloc wt for move1 move1 wt and move2 move2 wt the wt based versions generally outperformed the conventional versions though not for all estimates in the higher percentiles for example for the 75th 80th percentiles the move1 wt was more accurate than the move1 but for the 80th 95th this was reversed fig 6 for the ktrl2 ktrl2 wt and rloc rloc wt the wt based versions generally outperformed their respective conventional versions figs 5 6 as can be seen the wt based methods may serve as better techniques for determining future high flow values which could indicate wet periods methods with the highest performance both in precision and accuracy in the estimations of higher percentiles were the move1 wt move2 wt ktrl2 wt and rloc wt for percentile estimations the superior performance of the wt based methods was more evident for estimations of higher percentiles than for lower percentiles 3 2 ratio of the estimated means and standard deviations of the extended records over their respective observed values the ratio of the estimated means and standard deviations of the extended records over their respective observed values were represented graphically using box plots the accuracy of each method was reflected by the closeness of the median value of the box plot to 1 and the symmetry of the box and whiskers around 1 the precision of each method was reflected by the amount of dispersion of the box and whiskers due to space limitations only selected box plots are presented based on the low medium and high r value categories 3 2 1 means of the extended records selected figures representing the low medium and high r value categories are shown in figs 7 8 and 9 respectively all methods except for the ktrl and ktrl wt had median values near 1 for the estimations of the means of the extended records the ktrl and ktrl wt highly underestimated the means particularly in the low and medium r value categories and the two versions had poor symmetry around 1 figs 7 8 in some cases over 75 of the mean values were underestimated however the performance of the ktrl wt was generally better than its conventional version fig 7a b in other cases the ktrl wt had only slight underestimations with medians very close to 1 while noticeable underestimations were seen in the corresponding conventional ktrl fig 9 for the ols and ols wt in the low r value category overestimations or underestimations were observed and both versions tended to show similar performances for example fig 7a in the medium and high r value categories the medians of the box plots of both versions were very close to 1 indicating good accuracy figs 8 9 furthermore the symmetry of the box and whiskers around 1 for the ols and ols wt was very similar thus the performances of both versions of the ols were comparable at higher r values slight overestimations and underestimations were also observed in both versions in move3 move3 wt and move4 move4 wt generally the closeness of the medians to 1 between the two versions was comparable in the low r value category fig 7 but the wt based versions had medians that were closer to 1 in the medium and high r value categories figs 8 9 indicating a higher accuracy which is supported by findings of the rbias values shown in table 4 for move1 move1 wt and move2 move2 wt regardless of the parameters used almost all medians were close to 1 the box and whisker plots were also relatively symmetric around 1 figs 7 9 overestimations and underestimations were rarely observed for these methods and when they occurred they were minimal overall move1 move1 wt and move2 move2 wt had good accuracy and the performances of both versions were comparable finally for the ktrl2 ktrl2 wt and rloc rloc wt the medians were also close to 1 and the box and whisker plots had relatively good symmetry around 1 indicating good accuracy in both versions figs 7 9 minimal overestimations and underestimations were seen for both versions of these methods and when they occurred both versions showed similar amounts as can be seen in terms of the accuracy measure the closeness of the medians to 1 was generally comparable between the two versions except for the ktrl ktrl wt where underestimations of the median values were consistently observed even so the ktrl wt consistently had less underestimation compared to its conventional version reflecting a more accurate performance similarly in terms of box plot symmetry both versions of most methods generally showed similar symmetry around 1 again except for the ktrl and ktrl wt in terms of precision for the move methods it was generally observed that move3 move3 wt and move4 move4 wt showed the highest degree of dispersion i e least precise figs 7 9 but there was no clear pattern to indicate the more precise version it is also noted that in a number of cases the presence of outliers was more noticeable in move3 move4 move3 wt and move4 wt compared not only to the other move methods but also the rest of the methods this is seen both in the estimation of the means and standard deviations in the following subsection of the extended records e g fig 7b 9a 10a 11b 12a this observation indicates poor performances by move3 move3 wt and move4 move4 wt in estimating the means and standard deviations of the extended records for data with outliers the poor performance by these methods is also reflected in their rrmse and rbias values tables 3 6 where in most cases the highest values are observed for move3 move4 move3 wt and move4 wt the ktrl2 ktrl2 wt and rloc rloc wt also showed high degrees of dispersion and often were even more dispersed than move3 move3 wt and move4 move4 wt e g figs 7 8b making them among the least precise methods even so in the majority of cases the ktrl2 wt and rloc wt were less dispersed higher precision compared to their conventional counterparts figs 7 9 similarly the ktrl wt was generally less dispersed than its conventional version again implying a higher precision fig 7 8a 9b finally the ols ols wt move1 move1 wt and move2 move2 wt consistently showed the highest precision i e least dispersion additionally both versions had similar degrees of dispersion indicating similar precision performances figs 7 9 it was also noted that despite the ols ols wt highly overestimating or underestimating the estimated means of the extended records at low r values low accuracy the methods maintained good precision this is supported by their rrmse values that were often the lowest i e high precision but they did not necessarily have the lowest rbias values tables 4 and 5 based on the results of the box plots for estimating the means of the extended records it appeared that among the wt based methods the ktrl wt ktrl2 wt and rloc wt showed the most improvement in performance compared with their conventional versions 3 2 2 standard deviations selected figures representing the low medium and high r value categories are shown in figs 10 11 and 12 respectively the median values seen in the box plots of the ratio of standard deviations showed that both underestimations and overestimations of the standard deviation of the extended records occurred however underestimations were larger and more prevalent figs 10 12 there were no methods that showed high amounts of overestimation while high underestimations were consistently observed in the ols ols wt and ktrl ktrl wt close to or even higher than 75 of the standard deviations were underestimated even for the high r value category figs 10 12 underestimating the variance of the extended records is indeed the main weakness of the ktrl method khalil and adamowski 2014a the ols also produces extended records with an underestimated variance in the presence of outliers khalil and adamowski 2014a as such highly underestimated standard deviations observed in the current study could indicate that the outliers in the datasets used are significant similarly in terms of the symmetry around 1 both versions of methods showed relatively good performance except for the ols ols wt and ktrl ktrl wt where the poorest symmetry was observed i e the worst accuracy performance for estimating the standard deviations of the extended records figs 10 12 poor performances are also reflected in the rbias values of the ols ols wt and ktrl ktrl wt where in most cases the values were the highest i e lowest accuracy table 6 for move types 1 4 the closeness of the medians to 1 produced was almost always comparable between the two versions of each move method figs 10 12 for the ktrl2 ktrl2 wt and rloc rloc wt the median values produced were relatively close to 1 however in many cases the wt based versions particularly in the rloc wt tended to show medians that were closer to 1 indicating better accuracy e g figs 10 12 based on the closeness of the medians to 1 and the symmetry of the box and whiskers around 1 among the wt based methods the ktrl2 wt and rloc wt showed the highest potential in producing more accurate estimations of the standard deviations compared to their respective conventional versions this observation was also supported by the rbias values table 6 where the ktrl2 wt rloc wt and ktrl wt were the most consistent wt based methods for producing lower rbias values compared with their respective conventional version in terms of the degree of dispersion of the box and whiskers both versions in the ols ols wt move1 move wt and move2 move2 wt consistently had comparable performances across the different parameters n1 n2 and r values figs 10 12 moreover they consistently displayed the least degree of dispersion implying the highest precision figs 10 12 additionally the rrmse values for move1 move wt and move2 move2 wt were often the lowest i e highest precision table 6 however there were no clear patterns that could be used to determine if one version was consistently more precise than the other and often the rrmse values between the two versions of move1 move wt and move2 move2 wt were equal table 6 finally although the ols and ols wt had poor accuracy performance these methods had good precision among the move methods move3 move3 wt and move4 move4 wt showed the highest degree of dispersion i e the least precision and often were the most dispersed among all the methods however there were no consistent patterns to indicate if one version was more precise than the other in addition to move3 move3 wt and move4 move4 wt the ktrl2 ktrl2 wt and rloc rloc wt often had a high degree of dispersion low precision and the wt based methods appeared to be more precise particularly for the medium and high r value categories figs 11 12 supported by the rrmse values at all high and most medium r values the ktrl2 wt and rloc wt were more precise than their respective conventional version table 6 similarly the ktrl wt was consistently less dispersed than its conventional version at the high r value categories fig 12 which was also reflected by their rrmse values table 6 this suggests that in estimating the standard deviations of the extended records higher r values appear to be a factor in the improved precision performance of the ktrl wt and rloc wt over their conventional counterparts while the ktrl2 wt outperformed its conventional version at all r value categories from the results of the boxplots it can be concluded that among the wt based methods the ktrl2 wt and rloc wt showed the highest improvement in accuracy and precision performances compared to their conventional versions in estimating the standard deviations of the extended records 4 conclusions this study incorporated the dwt using the à trous algorithm to eight existing record extension methods namely the ols move types 1 4 ktrl ktrl2 and rloc to extend streamflow records using empirical data from a total of 67 pairs of rhbn stations that had at least 35 years of continuous data comparisons in performance were made between each conventional method and its respective wt based version the performances were assessed based on accuracy and precision measures in estimating individual data values means standard deviations and a series of percentiles of the extended records for the estimation of individual data values of the extended records improvement in the wt based methods over their respective conventional version was inconsistent for example the wt based versions generally showed improved precision except for the ktrl wt over their conventional counterparts for the low and high r value categories while for the medium r value category the improvement was minimal in terms of accuracy noticeable improvements in the wt based methods except for the ktrl wt were seen for the low r value category while for the medium and high r value categories the improvements were scattered with no discernable patterns for the estimations of the means of the extended records the improvements in terms of both precision and accuracy were most noticeable in the ktrl wt ktrl2 wt and rloc wt in comparison to their respective conventional versions the levels of correlation r values appeared to be a more significant factor in improving the performance compared to the lengths of the concurrent and extended records finally the most consistent improvement in the wt based methods was seen in the estimations of the standard deviations of the extended records based on the rrmse and rbias values and the boxplot results it was generally observed that the wt based methods were relatively consistent except for move3 wt and move4 wt in improving accuracy compared to their respective conventional version additionally the ktrl wt ktrl2 wt and rloc wt were the wt based versions that showed the most consistent improvements in precision over their respective conventional version this study demonstrated that the ktrl wt ktrl2 wt and rloc wt were the wt based methods that outperformed their conventional versions both in precision and accuracy measures in estimating the statistical moments means and standard deviations of the extended records khalil and adamowski 2014a showed that for extending water quality records the ktrl2 and rloc had a maximum performance when outliers were present as was the case in the present study the ktrl2 and rloc were also better able to maintain the characteristics of the entire distribution in the presence of outliers khalil and adamowski 2014b it should be noted that in estimating the means and standard deviations of the extended records for cases where the wt based methods did not outperform their conventional version underperformance is not necessarily implied in many cases the rbias and rrmse values between the two versions were equal this was noticeable mainly for the ols ols wt move1 move1 wt and move2 move2 wt finally although improvements in the wt based versions in move3 wt and move4 wt were observed in estimating the means and standard deviations of the extended records the improvements were inconsistent and lacked clear patterns for the estimations of the lower percentiles the 5th 20th the wt based methods except for the ktrl wt generally performed better at higher levels of correlations while at lower levels of correlations the conventional versions performed better when estimating the higher percentiles the 75th 95th the wt based methods generally showed better precision compared with their conventional versions in terms of accuracy at higher percentiles the ktrl wt ktrl2 wt and rloc wt outperformed their respective conventional versions while the rest of the wt based methods had comparable accuracy performances to those of their respective conventional versions this present study was thus able to show the advantage of the ktrl wt and ktrl2 wt methods which were designed to estimate percentiles over their respective conventional versions this study demonstrated how the wt based methods have the potential to be used instead of the conventional methods for extending streamflow records that contain outliers furthermore since there were no noticeable differences in the computational effort between the wt based and conventional versions the wt based methods are an improvement for streamflow record extensions particularly when estimating the means standard deviations and higher percentiles generally the improvements seen in the wt based methods were more affected by the levels of correlations rather than the lengths of concurrent or extended records since this present study showed the potential of incorporating wt into record extension methods future studies can further explore other applications of the proposed approach in this study the datasets used were all complete without missing values or data gaps this was due to the dwt used being unable to accommodate unequal sampling intervals with somewhat greater complexity some forms of wavelet analysis such as slepian wavelets can deal with data that have equal sampling intervals but contain missing observations i e irregular sampled data for such data slepian wavelets would assume that the sampling intervals are a product of a stationary sequence in order to estimate the overall energy and be able to associate it with a specific scale further information on these wavelets can be found in mondal and percival 2010 future studies should investigate how such wavelets could be used to extend records in hydrological data that contain gaps or scattered missing values alternatively other available techniques that can be used to estimate missing data points such as those that belong to empirical statistical and function fitting methods more detailed explanations on these various categories of methods are covered in studies such as kashani and dinpashoh 2012 lee and kang 2015 sattari et al 2017 could be explored these methods might be particularly useful for mountainous and forested areas where data scarcity is very often encountered kashani and dinpashoh 2012 after applying the appropriate techniques to infill the missing records or observations the resulting complete data may be processed using the wt based methods proposed in this present study future studies can also look into applying the proposed approach on cases where outliers are absent or if present how the magnitude and number of outliers may affect the performance of the wt based methods declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgement this research was funded by an nserc discovery grant an nserc accelerate grant and a william dawson scholar fund held by jan adamowski the first author was also financially supported by the liliane and david m stewart water resources management ph d scholarship at mcgill university 
5829,recently the use of heat as a tracer to evaluate the process of hyporheic exchange in riparian zones has attracted wide attention a more accurate flow heat coupling model of riparian zones could help us understand the patterns of water flow and heat transfer in riparian zones and provide a scientific basis for the comprehensive management and efficient utilization of these zones in this paper a flow heat coupling model of a riparian zone based on thermal conductivity empirical models tcems was built by customizing partial differential equations pdes based on the simulation of soil water flow using porous media and the subsurface flow module in comsol combined with the data collected from the field heat tracer test in the riparian zone the flow heat coupling model of the riparian zone under 12 types of tcems was validated and compared the results show that the use of the pde module instead of the heat transfer in porous media htpm module is very effective for modeling the pde module could basically replace the htpm module in the heat transfer calculation the performance of the flow heat coupling model varies under different types of tcems and the chung and horton 1987 model shows better simulation effects with a root mean square error rmse coefficient of determination r2 mean absolute error mae and mean relative error mre ranging from 1 37 to 2 48 0 73 0 94 1 06 2 08 and 11 94 15 79 respectively therefore this model could better reflect the dynamic temperature variations in the riparian zone the sensitivity analysis results illustrate that the hydraulic conductivity k s van genuchten parameter β volumetric heat capacity of dry solids c s and porosity n of the flow heat coupling model greatly influence riparian zone temperature variations and the parameters β k s and residual water content θ r significantly affect the lateral hyporheic exchange rate in the riparian zone of which β has the largest effect keywords riparian zone flow heat coupling model thermal conductivity empirical models comsol 1 introduction the hyporheic zone hz which is known as the liver of a river is a transitional zone of dynamic interaction between groundwater and surface water gw sw in rivers conant et al 2019 it undergoes frequent dynamic exchange and active material circulation and it is an important breeding baiting and fattening site for fish shrimp crabs and shellfish kawanishi et al 2013 tonina and buffington 2007 zhang et al 2018 in recent years many scholars have focused their studies on hyporheic exchange in the riverbed including vertical exchange mechanisms main functions and solute cycles fox et al 2018 graham et al 2018 liu et al 2017 zheng et al 2016 however hyporheic exchange not only occurs below the riverbed but also in the riparian zone gerecht et al 2011 watson et al 2018 busato et al 2019 the riparian zone which is one of the main types of hz is formed by sediment deposition in the bay and plays an indispensable role in processes related to river ecosystem balance and stability koiter et al 2013 conant et al 2019 due to the particularity of the location and structure of the riparian hz it is different from the riverbed hz in terms of the exchange mechanism ecological process and material transport busato et al 2019 peralta maraver et al 2018 ren et al 2019a the riparian zone plays an important role in protecting rivers and groundwater especially in effectively regulating and storing floods reducing pollution and protecting the water and soil environments chen et al 2017 ding et al 2017 these abundant functions of the riparian zone have made it a hot spot of eco hydrological research schwab et al 2016 lind et al 2019 the gw sw interaction is always accompanied by heat transfer thus observing the temporal and spatial distribution characteristics of temperature in the hz is an effective means of qualitatively or quantitatively analyzing the hyporheic exchange rate and process patterns constantz 2008 engelhardt et al 2011 2013 in particular the improvement in automatic temperature monitoring equipment signal analysis and processing methods computer numerical calculation and simulation analysis capabilities in recent years has provided a broad space for the application of the heat tracing method for hyporheic exchange and has gradually made the heat tracing method one of the important means of studying the problem of water and heat exchange in the hz ren et al 2018a for example ren et al 2018b developed a 2 d riparian zone laboratory test and evaluated the water flow and thermal dynamics in riparian zones using the injection of a heat tracer test through a field test watson et al 2018 determined how the temperature patterns are related to lateral hyporheic exchange and fluid flow and groundwater surface water interactions in general busato et al 2019 performed hydrogeophysical characterization of the hyporheic and riparian zones at an experimental site in the adige catchment northern italy by means of electrical resistivity tomography ert distributed temperature sensing dts and hydrological modeling therefore the accurate evaluation of riparian temperature variations will promote the development and use of heat tracers to study issues related to the riparian zone although there have been some studies on hyporheic exchange and temperature distribution within the riparian zone these studies have been field based and relatively costly to conduct further studies are hindered by the lack of an effective flow heat coupling model and the best way to fully exploit hydrogeophysical data is through the development of a hydrological model busato et al 2019 therefore further attempts are required to identify a suitable model coupling approach thermal conductivity is the most important parameter affecting the temperature distribution in soil during heat transfer zhang and wang 2017 therefore it can determine the simulation accuracy of the flow and heat coupling model however the thermal conductivity of soils is assumed to be a fixed value formula in some software for simulating heat transfer in geological bodies healy and ronan 1996 koch et al 2016 theoretically speaking this assumption is not accurate because the thermal conductivity of soil is related to the soil type particle size distribution soil structure porosity saturation and temperature among which porosity and saturation have the greatest influence zhang and wang 2017 these considerations are particularly important for the study of flow heat coupling of soil in the unsaturated riparian zone at present there are many models for predicting the thermal conductivity of soils these models could be divided into empirical models and theoretical models and some empirical models have been embedded in water and heat transfer simulation software for example the value of thermal conductivity in hydrus software is predicted according to the chung and horton 1987 model ren et al 2018b in hst3d software the volume average model is used to estimate the value of thermal conductivity bravo et al 2002 comsol software provides users with three choices the volume average model the reciprocal average model and the power law model however with the continuous development of tcems more reasonable and efficient tcems might be neglected therefore it is necessary to carry out secondary development of relevant software so that the different tcems could be taken into account in the flow heat coupling model to compare and analyze their application effects in practical engineering comsol is a finite element analysis and solver software package for various physics and engineering applications especially coupled phenomena or multiphysics li et al 2009 in the field of geoscience comsol could simulate groundwater flow solute transport in soil and geothermal and land subsidence calcutt and anissimov 2019 song et al 2018 jayeoba et al 2019 comsol provides the basic module of heat transfer in porous media but only three thermal conductivity empirical models are considered therefore the performance of different tcems in simulating temperature variations in riparian zones cannot be evaluated however it supports the solution of customized pdes in which users can freely define the required equations and achieve coupling between them li et al 2009 moreover in the modeling process the material attributes source terms and boundary conditions could employ for example piecewise functions interpolation and analysis these functions can be time dependent or space dependent wissmeier and barry 2011 with these advantages secondary development of physical problems can be carried out in comsol without additional programming which has great advantages for coupling different tcems with the flow heat coupling model in this study the porous media and subsurface flow module in comsol was adopted to simulate the soil water flow and based on the tcems and heat transfer theory of porous media the flow heat coupling model of the riparian zone was built using a customized pde module combined with the collected data from the field heat tracer test of the riparian zone the performance of the flow heat coupling model under 12 types of tcems in simulating the temperature variations is analyzed which could provide a reference for the rational selection of tcems in the flow heat coupling model of the riparian zone in addition the orthogonal test method was used to determine the main influencing factors of the flow heat coupling parameters on the temperature and lateral hyporheic exchange rate in the riparian zone to reduce the workload of model calibration and provide a reference for related research 2 flow heat coupling mathematical model of the riparian zone 2 1 flow model the saturated unsaturated transient seepage field of the riparian zone can be described by the richards equation richards 1931 1 ρ w c m ρ w g s e s s p t ρ w k s k r θ μ t p ρ w gz q m where ρ w is the water density kg m3 c m is the specific water capacity m 1 g is the acceleration of gravity m s2 s e is the relative saturation of the soil s s is the elastic water storage rate pa 1 p is the pressure pa t is the time s is the laplace operator θ is the volumetric water content m3 m3 k s is the saturated hydraulic conductivity of the soil m s k r θ is the relative hydraulic conductivity of the unsaturated zone m s which is a function of the volumetric water content μ is the dynamic viscosity of water pa s μ t 0 00002424 10 247 8 t 133 16 kipp 1987 z is the calculation point elevation m and q m is the flow source term kg m3 s the soil hydraulic function is described by the van genuchten model van genuchten 1980 2 θ θ r s e θ s θ r 3 s e 1 1 α h p β m 4 c m α m 1 m θ s θ r s e 1 m 1 s e 1 m m 5 k r θ s e 1 2 1 1 s e 1 m m 2 where θ r is the residual water content m3 m3 θ s is the saturated water content m3 m3 h p is the pressure head m α is the reciprocal of the intake value of the moisture characteristic curve m 1 and β is the parameter indicative of the gradient of the soil moisture characteristic curve obtained by fitting the soil moisture characteristic curve m 1 1 β 2 2 heat transfer model the heat transfer of the riparian zone can be expressed by the advection dispersion equation which is derived by balancing the changes in the energy stored within a volume of porous media and can be written with temperature as the dependent variable healy and ronan 1996 6 θ c w 1 n c s t t k eff t θ c w d h t θ c w u t q s where c w and c s are the volumetric heat capacity of water and the dry solid in j m3 c respectively t is the temperature c k eff is the effective thermal conductivity w m3 c d h is the hydrodynamic dispersion coefficient m2 s and u is the average water velocity calculated by u v θ v is the darcy seepage velocity m s the left side of eq 6 represents the temperature change in the variably saturated volume over time which is a time varying term the first term on the right side of eq 6 represents the heat conduction term the second term is the heat dispersion term the third term represents the heat advection term and the final term q s is the heat source term w m3 the hydrodynamic dispersion coefficient d h is a parameter that characterizes the ability of a porous media to diffuse a contaminant at a certain flow velocity it macroscopically reflects the influence of the groundwater flow process and void structure characteristics on the solute transport process in porous media the advection and dispersion components of heat transfer are analogous to those in the solute transport process healy and ronan 1996 so the hydrodynamic dispersion coefficient could be calculated by the following formula healy 1990 7 d h i j α t v δ ij α l α t v i v j v where α t is the transverse dispersivity of the soil m v is the magnitude of the velocity vector m s δij is the kronecker delta which equals 1 if i j and otherwise equals 0 α l is the longitudinal dispersivity m and vi and vj are the i th and j th components of the velocity vector respectively 2 3 thermal conductivity empirical models 2 3 1 johansen 1975 model johansen 1975 first proposed the concept of normalized thermal conductivity k e he believes that the relationship between the normalized thermal conductivity k e and the degree of saturation s r could simultaneously reflect the influence of the soil type porosity water content and mineral composition on the thermal conductivity of the soil the proposed normalized thermal conductivity is expressed as 8 k e k eff k dry k sat k dry where k e is the normalized thermal conductivity and k sat and k dry are the thermal conductivity of the soil in saturated and dry states respectively in the calculation of the thermal conductivity of saturated soil the formula proposed by sass et al 1971 has been widely used and can be expressed as follows 9 k sat k w n k s 1 n where k s is the thermal conductivity of soil solid particles obtained from quartz q and its thermal conductivity k q 7 7 w m3 c and the thermal conductivity of other minerals k o i e k s k q q k o 1 q among them k o 2 0 w m3 c q 0 2 k o 3 0 w m3 c q 0 2 k w is the thermal conductivity of water k w 0 594 w m3 c at 20 c and n is the porosity by improving the de vries 1963 thermal conductivity model johansen 1975 proposed the thermal conductivity formula for dry soil 10 k dry 0 137 ρ b 64 7 ρ s 0 947 ρ b where ρ b is the bulk density of the soil kg m3 and ρ s is the soil particle density kg m3 in addition johansen 1975 proposed several k e s r relationships for different types of soils by fitting the experimental data of kersten 1949 11 k e 0 7 lg 10 s r 1 s r 0 05 a lg 10 s r 1 s r 0 10 b where s r is the degree of saturation s r θ n eq 11 a and b are for medium fine sands and fine textured soils in unfrozen states therefore the johansen 1975 model can ultimately be summarized as follows 12 k eff k w n k s 1 n 0 137 ρ b 64 7 ρ s 0 947 ρ b k e 0 137 ρ b 64 7 ρ s 0 947 ρ b 2 3 2 campbell 1985 model considering the effects of soil texture bulk density and volumetric water content on thermal conductivity campbell 1985 proposed an empirical model for calculating the soil effective thermal conductivity as follows 13 k eff a b θ a d exp c θ e where the parameters a b c and d could be obtained according to the soil bulk density and clay content which can be expressed as 14 a 0 65 0 78 ρ b 0 60 ρ b 2 b 1 06 ρ b c 1 2 6 m clay 0 5 d 0 03 0 1 ρ b 2 e 4 where m clay is the clay content and ρ b is the bulk density of the soil in g cm3 2 3 3 chung and horton 1987 model the chung and horton 1987 model is the default thermal conductivity empirical formula in hydrus software hydrus is one of the mainstream finite element software programs for simulating water heat and solute transport in variable saturated porous media and due to its outstanding performance it has been widely used by scholars of relevant research in various countries the chung and horton 1987 model can be expressed as follows 15 k eff b 1 b 2 θ b 3 θ 0 5 where b 1 b 2 and b 3 are the regression parameters in the present study the values of b 1 b 2 and b 3 were taken from the chung and horton 1987 and they are 0 228 2 406 and 4 909 respectively 2 3 4 ewen and thomas 1987 model based on the research of johansen 1975 ewen and thomas 1987 proposed an exponential function to describe the relationship between k e and s r 16 k e 1 exp ξ s r where ξ is a fitted parameter and ξ 8 9 the rest of the unknown parameters are calculated in the same way as those of the johansen 1975 model 2 3 5 côté and konrad 2005 model to simplify the calculation of the logarithmic function eq 11 in the johansen 1975 model côté and konrad 2005 proposed a new k e s r relationship with κas the variable based on the normalized thermal conductivity model proposed by johansen 1975 the value of κcould reflect the influence of the soil types i e coarse sand fine sand silt and clay on the k e s r relationships and the effective thermal conductivity of soil 17 k e κ s r 1 κ 1 s r where κis an empirical parameter used to account for the different soil types in the unfrozen and frozen states the suggested values for gravel and coarse sand medium fine sand and silt and clay are 4 66 3 55 and 1 90 respectively côté and konrad 2005 also analyzed the relationships between the thermal conductivity and porosity of dry soil based on a large amount of data from the literature and proposed a new formula for calculating the thermal conductivity of dry soil 18 k dry χ 10 η n where χ and η are the parameters affected by the soil texture where χ 1 70 w m3 c and η 1 80 for crushed rocks χ 0 75 w m3 c and η 1 20 for mineral soils and χ 0 30 w m3 c and η 0 87 for organic fibrous soils the thermal conductivity empirical model proposed by côté and konrad 2005 can finally be summarized as follows 19 k eff k w n k s 1 n χ 10 η n κ s r 1 κ 1 s r χ 10 η n it should be emphasized that the calculation of k s here is slightly different from that in the johansen 1975 model the value of k s was calculated by using the geometric mean method based on the thermal conductivity and volumetric proportion of rock forming minerals therefore a complete soil mineral composition was required in this study the value of k s is taken from the sandy loam in he et al 2017 and it is set to 7 5 w m3 c 2 3 6 lu et al 2007 model to make the johansen 1975 model more suitable for the calculation of the thermal conductivity under low water content conditions lu et al 2007 used the thermo tdr probe to test a large number of thermal conductivities of twelve different soils under various water content conditions he believes that the k e s r relationships are affected by the soil types especially for fine grained soil thus a new exponential function expression of k e and s r was proposed 20 k e exp γ 1 s r γ 1 33 where γ is a parameter determined by the soil texture γ is 0 96 and 0 27 for coarse textured and fine textured soils respectively and 1 33 is a shape parameter furthermore they obtained a simpler linear relationship between the thermal conductivity of the dry soil and the porosity by fitting the experimental data 21 k dry 0 56 n 0 51 other unknown parameters are obtained using the procedure from johansen 1975 lu et al 2007 did not measure the quartz contents of soils but assumed that the quartz content was equal to the sand fraction here we also make this assumption for the estimation of the quartz content 2 3 7 lu et al 2014 model based on lu et al 2007 2014 proposed an exponential function to express the nonlinear behavior of k e as related to soil volumetric water content texture and bulk density 22 k eff k dry exp ϕ θ μ θ 0 where μ and φ are shape factors of the k eff θ curve which are related to the soil texture and bulk density ρ b and can be expressed as 23 μ 0 67 m c 0 24 24 ϕ 1 97 m sand 1 87 ρ b 1 36 m sand ρ b 0 95 where m clay and m sand are the clay and sand contents respectively 2 3 8 nikoosokhan et al 2015 model based on the normalized thermal conductivity proposed by johansen 1975 nikoosokhan et al 2015 proposed a set of line relationships for k sat and k dry 25 k sat 0 53 m clay 0 1 γ d 26 k dry 0 087 m sand 0 019 γ d where γ d is the dry density kn m3 γ d ρb g the relationship between k e and s r is obtained from the côté and konrad 2005 model the value of κcan be calculated from m clay by 27 κ 4 4 m clay 0 4 therefore the model proposed by nikoosokhan et al 2015 could ultimately be expressed as 28 k eff 0 53 m clay 0 1 γ d 0 087 m sand 0 019 γ d κ s r 1 κ 1 s r 0 087 m sand 0 019 γ d 2 3 9 ren et al 2019b model based on previous studies ren et al 2019b considered the influence of organic matter content and particle composition on thermal conductivity and proposed a new relationship between k e and s r based on the previous models within the entire range of water content 29 k e k dry exp φ θ ψ where ψ and ϕ are the shape factors of k eff θ furthermore they believed that the relationships among ψ ϕ and soil texture should be considered based on the larger amount of collected data they proposed the following linear relationship 30 ψ 0 493 m sand 0 86 m silt 0 014 m om 0 778 31 φ 0 736 m clay 0 006 m om 0 222 in this model k sat and k dry are calculated from eqs 25 and 26 and the other unknown parameters are calculated using the formula from the johansen 1975 model 2 3 10 comsol built in models the thermal conductivity empirical models were also considered in comsol software therefore three models are provided for users in the heat transfer in porous media module of comsol the volume average model the reciprocal average model and the power law model the formulas are as follows 32 k eff n k w 1 n k s 33 1 k eff 1 n k s n k w 34 k eff k w n k s 1 n where the thermal conductivities of water k w and soil solid particles k s are 0 594 w m3 c and 7 5 w m3 c respectively 3 development of the flow heat coupling model of the riparian zone in comsol 3 1 secondary development environment in comsol comsol provides users with different levels of secondary development interfaces and allows users to flexibly choose the appropriate interface type according to their needs the secondary development methods in comsol could be divided into the following four types 1 use matlab for secondary development users could use the data processing functions in matlab as a data conversion interface between comsol and other software to achieve coupling between different software 2 use the application program interface api for secondary development users could use the api functions provided by comsol to write the interface environment and running instructions and build their own finite element analysis software 3 use a physical model creator for secondary development comsol directly attributes different physical phenomena to different mathematical physics equations after the user derives the corresponding weak solution formal equation for the physical phenomenon of interest the user could use the creator provided by comsol to develop a custom physics model that can directly call the comsol pre and postprocessing and solver functions this secondary development method is powerful but requires the user to give a weak solution to the mathematical equation which is difficult 4 modify the built in equations of comsol for secondary development the material properties and boundary conditions involved in the comsol mathematical physics equation could be a constant arbitrary variable function or logical expression these parameters and even the coefficients of the pdes could be modified through the comsol interface in addition users could add custom pdes and specify their relationship with other equations to achieve the purpose of coupling calculation this method is relatively simple and flexible and it is used in this paper to carry out secondary development of the flow heat coupling model of the riparian zone 3 2 implementation method of the flow heat coupling model in this paper the richards equation in porous media and subsurface flow module of comsol was used to describe soil water flow however the physical problem of heat transfer in porous media will be achieved through custom pdes the coefficient type pdes provided in comsol are as follows 35 e a 2 u t 2 d a u t c u α u γ β u a u f u ω n c u α u γ g q u u ω hu r u ω where u is the dependent variable e a d a c α γ β a q and h are the custom coefficients f g and r are the source terms on the solution domain and the boundary respectively ω is the solution domain ω is the outer boundary of ω and n is the outer normal direction of ω the coefficients in the above equations have different physical meanings in different problems but essentially all physical phenomena could be expressed by eq 35 which can be defined as different types of functions according to actual physical problems these functions could be derivatives of different orders and they can be time dependent or spatially related first the governing equation of heat transfer is arranged according to the form of eq 35 then each coefficient is defined parts without a corresponding item could be achieved by defining the coefficient f instead of solving programmatically in terms of heat transfer equation modeling in this paper d a u t can constitute the time varying term of the equation c u αu γ could be set as the combination of the advection conduction and dispersion term and f could be viewed as the heat source terms the heat transfer equation is coupled with the richards equation by the soil volumetric water content θ and coupling between the thermal conductivity empirical model and the heat transfer equation is achieved by the effective thermal conductivity k eff to avoid unit confusion in the analysis the variable unit and geometric model size are adapted in international units the node temperature t is taken as the basic unknown variable and the time term in the governing equation is discretized by the implicit euler backward difference method the nonlinear iterative modified damped newton method is used to solve the equation in terms of meshing comsol comes with a predefined triangle mesh unit division method users need only to select a reasonable mesh size to mesh the research objects the denser the mesh size is the more accurate the calculation but the longer the required calculation time for the study of flow heat coupling in the riparian zone the normal mesh size is a reasonable mesh size but when analyzing the unsaturated seepage problem it is necessary to select a particularly refined mesh to calculate the convergence fig 1 shows the flow chart of the comsol solution 4 field experimental data collection and model setup 4 1 data collection to verify the built flow heat coupling model of the riparian zone and to compare and analyze the application performance of different thermal conductivity empirical models in simulating the temperature variation in the riparian zone this paper obtained temperature and water level data from a field test conducted by the u s geological survey nevada water science center carson city nevada within the walker river basin between march 2012 and december 2012 naranjo and smith 2016 to analyze the modeling results u s geological survey researchers installed water level and temperature sensors in rivers and riparian zones to achieve long term dynamic monitoring of the water levels and temperatures first polyvinyl chloride pvc pipes wrapped with sieves were driven into the sediment layer of the riparian zone and then a string of three to four independent water resistant temperature sensors alpha mach inc ibcod type z accuracy 0 1 c resolution 0 01 c were placed inside the pvc pipes to monitor the temperature created by lateral hyporheic exchange and a similar type of sensor alpha mach inc ibcod type l accuracy 0 5 c resolution 0 5 c was installed at the base of the pvc pipes slightly below the land surface to monitor the air temperature the data monitored during the test were recorded and stored by a data recorder every 1 h fig 2 shows the canal water level air and water temperature time series data monitored by the sensors more detailed information about the study area and data collection methods are provided by naranjo and smith 2016 4 2 conceptual model setup based on the aforementioned mathematical models and the collected field experimental data the finite element solution of the saturated unsaturated flow and heat coupling model of the riparian zone could be realized by comsol software through the combination of the richards equation and the custom coefficient type pde module depending on the permeability the hydrostratigraphy could be simplified to three zones fig 3 in terms of the boundary conditions set up in the conceptual model for the saturated unsaturated seepage field the no flow boundary conditions were set at the left fa right bc and atmospheric soil contact interfaces cd and ef the variable head boundary condition was set at the water soil contact interface de and the permeable boundary was set at the bottom ab for the temperature field the adiabatic boundary conditions neumann boundary condition achieved by the second formula in eq 35 were set at the left fa right bc and bottom ab the air and water temperature boundary conditions dirchlet boundary condition achieved by the third formula in eq 35 were set at the atmospheric soil and water soil contact interfaces cd ef and de respectively and the initial temperatures of the three zones were set based on the mean value of the sensor measurements at the corresponding zones at the initial time referring to the relevant literature table 1 gives the hydraulic and thermal parameters to evaluate the flow heat coupling model of the riparian zone where the values of k s θ r α β n m om c s and c w were obtained from naranjo and smith 2016 and the values of m clay m sand m silt and θ s were taken from the sandy loam of carsel and parrish 1988 the values of ρ s and ρ b were obtained from the soils with closer soil textures porosity and particle size distributions of he et al 2017 and keller and håkansson 2010 the values of longitudinal dispersivity α l and transverse dispersivity α t in the text were considered to be 0 01 m naranjo and smith 2016 4 3 criterion for model performance evaluation to quantitatively evaluate the simulation performance of the models four different types of standard statistical methods were considered as statistical performance evaluations the root mean squared error rmse coefficient of determination r2 mean absolute error mae and mean relative error mre were used the four performance evaluation criteria used in this study could be calculated by utilizing the following equations vafakhah 2013 9 rmse 1 n i 1 n o i s i 2 10 r 2 1 i 1 n o i s i 2 i 1 n o i o 2 11 mae 1 n i 1 n o i s i 12 mre 1 n i 1 n o i s i o i 100 where n is the number of data oi and si are the observed and simulated temperatures at moment i respectively and o is the average measured temperature the rmse is a nonnegative value and a low rmse indicates good consistency between the observed and simulated values r2 ranges from to 1 and a larger r2 value indicates a better match of the simulated and observed data a value of r2 0 6 is considered good while a value of 0 indicates that the estimations are no better than those derived from taking the average of the observed data while values less than 0 represent increasingly poor model performance zounemat kermani et al 2016 lower values of the mae and mre indicate less deviation between the observed and simulated data 4 4 sensitivity analysis in previous studies of riparian zones many scholars have focused on temperature variations or lateral hyporheic exchange rate flux changes in riparian zones dick et al 2015 liu et al 2018 therefore in this section the temperature t and the absolute value of the lateral hyporheic exchange rate vx were selected as the main test indexes for sensitivity analysis the parameter sensitivity of the coupled model is determined by quantitative analysis of the changes in the temperature and the lateral hyporheic exchange rate in the riparian zone caused by the changes in each parameter considering the range of the riparian zone we limit the object area of the sensitivity analysis to 5 13 m x 6 14 m and 2 90 m y 1 10 m in fig 3 the difference between the temperature field or lateral hyporheic exchange rate and the rated temperature field or rated lateral hyporheic exchange rate due to changes in the parameters was determined for each finite element node then the average change was used to represent the change in temperature lateral hyporheic exchange rate the selected sensitivity analysis objects are seven flow heat coupling model parameters that affect the model s temperature variations and lateral hyporheic exchange rate i e hydraulic conductivity k s saturated water content θ s residual water content θ r van genuchten parameters α and β porosity n and specific heat capacity of dry solids c s three factor levels were selected for each parameter one acting as the reference value and then an increase and decrease in the reference value by 10 the level of each factor in the parameter sensitivity analysis is shown in table 2 according to the number and the level of test factors shown in table 2 and combined with the principle of the orthogonal test method in appendix 1 the l 18 2 37 orthogonal table is selected to arrange the test the first column is set as a blank column the test factors are randomly assigned to the last seven columns of the orthogonal table and each element in the table is replaced by the corresponding design parameters according to its corresponding factors and levels then the orthogonal test table for the parameter sensitivity analysis of the flow heat coupling model of the riparian zone could be obtained 5 results and discussion 5 1 verification of pde module modeling effectiveness in previous studies on flow heat coupling some scholars mostly used the heat transfer porous media htpm module to simulate the internal heat transfer of the research object anderson 2005 ren et al 2019a however this modeling method could use only the default empirical formula of the software to predict the thermal conductivity which may ignore some new and more accurate tcems thus to verify the effectiveness of replacing the htpm module with the pde module the heat transfer model is built based on the pde module and the htpm module to make the two modeling methods comparable the formula of thermal conductivity in the pde module is written according to the three tcems of the htpm module i e the volume average model the power law model and the reciprocal average model in this way we could verify the effectiveness of the pde module by comparing the consistency of two different modeling methods in calculating the temperature variations in the riparian zone thus providing a basis for the subsequent use of the pde module to compare the performance of different tcems in simulating temperature variations in the riparian zone fig 4 shows a comparison of the riparian zone temperature variations simulated by two different modeling methods using three comsol built in tcems at different monitoring points it can be seen from fig 4 that regardless of which tcem is built in comsol the temperature variations in the riparian zone simulated by the two modeling methods are basically consistent to quantitatively evaluate the performance of the pde module in this section the result of the htpm based method is considered as observed data and the pde based method is considered as simulated data in this way we could quantitatively evaluate the difference in the calculation results based on two different modeling methods through the four abovementioned performance evaluation indicators table 3 shows the results of the four performance evaluation indicators of the pde based method from the results of four evaluation indicators the ranges of the rmse r2 mae and mre at 0 16 0 53 c 0 99 1 00 0 13 0 46 c and 0 87 3 41 respectively indicating that the results calculated by the two modeling methods are in good agreement with small deviations in general the rmse mae and the degree of the rmse greater than the mae could reflect the change in the difference between the observed and simulated values or in a certain sense the degree of the maximum difference ramos et al 2011 fig 5 shows the difference between the rmse and mae at different monitoring points under the three tcems and it can be seen that the rmse is always larger than the mae and that the difference between them is very small and basically remains at approximately 0 04 c in addition compared with other evaluation indicators of the three models it can be seen that the variation in the performance evaluation indicators under different tcems is also very small these results indicate that in the modeling process of the pde module the replacement of the tcem will not affect the stability of the model and the pde module adopted in this paper is effective in modeling the heat transfer model and could basically replace the htpm module for heat transfer calculation therefore in subsequent research we could modify the formula of thermal conductivity in the pde module to compare and evaluate the practical application performance of the existing tcems in engineering 5 2 model validation and comparative analysis fig 6 shows the comparison between the observed and simulated temperature values at eight monitoring points in the riparian zone based on 12 types of tcems and the performance is evaluated by four indicators introduced in section 4 4 table 4 presents the statistical results of four model evaluation indicators of different tcems at the eight monitoring points as shown in table 4 there are differences in various indicators obtained by using the different tcems and the performance is also different at different monitoring points to reflect the investigation more intuitively the statistical results in table 4 are presented in the form of radar charts fig 7 that can intuitively reflect the performance of the different tcems and be used to compare the evaluation indicators at different monitoring points fig 7 reveals that the values of the rmse r2 mae and mre of the campbell 1985 model are the worst of all the tcems which indicates that the performance of the campbell 1985 model is poor in the application of this engineering case in addition all the indicators simulated by the tcems perform poorly at the bp1 2 50 m and bp2 2 60 m monitoring points when we compare the observed and simulated values at different monitoring points of the same pvc pipe in fig 5 we find that the simulated temperatures at bp1 2 50 m and bp2 2 60 m are smaller than the observed temperature and the ranges of rmse r2 mae and mre are 2 81 6 89 c 0 75 0 75 2 44 6 09 c and 14 28 35 17 respectively the simulation error at these two points is large generally the temperature range in the riparian zone decreases with increasing depth in a certain range of depths and remains stable after reaching a certain depth anderson 2005 ren et al 2019a however the temperature curves at these two monitoring points are obviously higher than those at the upper monitoring points we suspect this phenomenon might be caused by an upwelling groundwater temperature influence at the bottom monitoring sensors in addition referring to naranjo and smith s 2016 report we found that the same issues were noted in their research which supports our hypothesis therefore to make the results more applicable to general situations the analysis of these two abnormal monitoring points should be removed when evaluating the performance of the tcems due to the different performances of different tcems at different monitoring points it is difficult to evaluate the performance of the tcems based on the existing results it could only be concluded that the performance of the campbell 1985 model is poor and the error is large at bp1 2 50 m and bp2 2 60 m therefore we used the global analysis method to compare the observed and simulated values of six monitoring points excluding bp1 2 50 m and bp2 2 60 m and the comparison results are shown in fig 8 as shown in fig 8 among the 12 types of tcems the symbols of the chung and horton 1987 model the ewen and thomas 1987 model the lu et al 2007 model and the côté and konrad 2005 model are in a narrower range along the 1 1 line while the trend line of these models is close to y x it also has a high r2 indicating that they have good performance in simulating the temperature variations in the riparian zone in contrast the performance of the campbell 1985 model and the volume average model are relatively poor to quantitatively evaluate the performance of 12 types of tcems in simulating temperature variations in the riparian zone fig 9 provides a global comparison of four performance evaluation indicators for different tcems and it can be found that the performance varies with the models among all the tcems the chung and horton 1987 model performs best with rmse r2 mae and mre values of 2 11 c 0 84 1 70 c and 13 88 respectively this result may explain why hydrus software has excellent performance in the simulation of water flow and heat transport and has been widely favored by researchers in related fields among the three tcems built in comsol the volume average model performed poorly on various indicators while the power law model performed relatively well however their performance is obviously worse than that of the chung and horton 1987 model the ewen and thomas 1987 model the lu et al 2007 model and the côté and konrad 2005 model which also explains the necessity of secondary development of the htpm module in comsol therefore the modeling method of the flow heat coupling model of the riparian zone in this paper might provide some references for scholars in related fields 5 3 parameter sensitivity analysis of the flow heat coupling model table 5 shows the simulation schemes and results statistics of the orthogonal test and the horizontal combination of the corresponding factors in each row in table 5 is a test scheme according to the test scheme designed in table 5 the test indexes t and vx under each scheme are calculated and the results are listed in the last two columns of table 5 then according to the calculation results of each scheme the sensitivity of each parameter to the test indexes are analyzed by the range analysis method described in appendix 1 table 6 shows the results of the range analysis on the influencing factors of the index t and the results showed that the order of the sensitivities of the factors to the index t from large to small was as follows k s c s β n θ r θ s and α table 7 presents the results of the range analysis on the influencing factors of the index vx and the results showed that the order of the sensitivities of the factors to the index vx from large to small was as follows β k s θ r θ s c s n and α fig 10 shows the results of the range values of each factor of the two test indexes according to the principle of range analysis described in appendix 1 the greatest factor of the range value is the most sensitive and has the greatest influence on the test indexes therefore according to fig 10 we can conclude that the parameters k s β c s and n of the flow heat coupling model have a great influence on riparian zone temperature variations and the parameters β k s and θ r have a significant effect on the lateral hyporheic exchange rate in the riparian zone of which β is the largest 6 conclusions comsol is very flexible in setting material properties source terms and boundary conditions and supports the solution of custom pdes based on these advantages to simulate soil water flow with a porous media and subsurface flow module a flow heat coupling model of the riparian zone based on tcem was built through the custom pde module in comsol and the model was verified by the htpm module built in comsol the results show that the heat transfer model based on the pde module is very effective and could basically replace the htpm module for heat transfer calculation the field heat tracer test data in the riparian zone were adopted to compare and analyze different flow heat coupling models the results show that the performance of the flow heat coupling model under different types of tcems is different in simulating the temperature variations in the riparian zone among them the chung and horton 1987 model the ewen and thomas 1987 model the lu et al 2007 model and the côté and konrad 2005 model have good simulation effects in contrast the campbell 1985 model performs poorly and has large calculation errors the sensitivity analysis results show that the parameters k s β c s and n of the flow heat coupling model have a great influence on the temperature variations in the riparian zone among which k s is the largest followed by c s the parameters β k s and θ r have a significant influence on the lateral hyporheic exchange rate in the riparian zone and the sensitivity of parameter β is the greatest therefore in the analysis of the temperature or lateral hyporheic exchange rate in the riparian zone by using the flow heat coupling model parameters with high sensitivity should be taken as the key points of parameter determination these parameters could be determined by experiments or inversion analyses and parameters with low sensitivity can be simplified appropriately as in the engineering analogy method in this way even if those minor factors are neglected a numerical model that is close to the actual phenomenon could be obtained credit authorship contribution statement wenbing zhang conceptualization data curation software methodology validation writing review editing zhenzhong shen funding acquisition supervision writing review editing jie ren funding acquisition formal analysis resources software methodology supervision writing review editing lei gan writing review editing fei wang writing review editing bihan yu writing review editing chenglin li writing review editing declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments we are grateful to the u s geological survey of nevada for sharing the data needed in this study we thank the precious suggestions by anonymous reviewers and editors which have greatly helped the improvement of the paper this work was supported by the national natural science foundation of china grant no 51679194 and the planning project of science and technology of water resources of shaanxi grant no 2019slkj 12 and the state key laboratory of eco hydraulics in northwest arid region xi an university of technology grant no 2019kjcxtd 10 and the project of national natural science foundation of china yalong river joint fund grant no u1765205 and the priority academic program development of jiangsu higher education institutions papd appendix 1 orthogonal test method orthogonal test design orthogonal test method is based on mathematical statistics and orthogonality principle and select appropriate representative points from a large number of test points and then arrange the test according to the orthogonal table due to the orthogonal table has the construction principle of balanced dispersion and neat comparability the number of experiments designed according to this method is less and it can reflect the changing rule of objective things in the orthogonal test the results we investigate usually called indexes the parameters that may affect the test indexes are called factors and the specific test conditions that each factor should be compared in the test are called levels the orthogonal table is expressed by ln tc where l is the code name of the orthogonal table n is the total number of tests t is the level number of factors and c is the number of columns of the orthogonal table that is the maximum number of factors that can be arranged orthogonal table is the key of orthogonal experimental design it must satisfy the following two conditions 1 the different levels of each factor appeared the same number of times in the test to ensure its uniformity 2 in order to ensure the uniformity of the distribution of test points the number of pairs of different levels of combinations of any two columns of factors occurs the same number of times in the test only by satisfying these two conditions can the test results be conveniently and comprehensively reflected table 8 shows the eight factors orthogonal table of l 18 2 37 where the first factor is two levels and the last seven factors are three levels orthogonal test results analysis method according to the test design of orthogonal table the sensitivity of each factor to the index could be judged by calculating the index value of each test in this paper the range analysis method is used to analyze the data of orthogonal test results suppose a and b represent the different influencing factors in the test t is the level of influencing factors ai represents the i th level of influencing factor a i 1 2 t xij represents the i th level of influencing factor j i 1 2 pij j a b and pij tests were performed at xij to obtain pij tests results yk k 1 2 pij the formula is 36 k ij 1 p ij k 1 p ij y k y where kij is the average value of the test results of factor j at the i th level pij is the number of test of factor j at the i th level yk is the k th test index value y is the average value of all test results the range value rj is the criterion for evaluating the sensitivity of range analysis factors and its formula is expressed as follows 37 r j max k 1 j k 2 j min k 1 j k 2 j the larger the value of rj the greater the influence of the change in the factor on the test index and means that the more sensitive the influencing factor is on the contrary the smaller the value of rj the less sensitive the influencing factor is 
5829,recently the use of heat as a tracer to evaluate the process of hyporheic exchange in riparian zones has attracted wide attention a more accurate flow heat coupling model of riparian zones could help us understand the patterns of water flow and heat transfer in riparian zones and provide a scientific basis for the comprehensive management and efficient utilization of these zones in this paper a flow heat coupling model of a riparian zone based on thermal conductivity empirical models tcems was built by customizing partial differential equations pdes based on the simulation of soil water flow using porous media and the subsurface flow module in comsol combined with the data collected from the field heat tracer test in the riparian zone the flow heat coupling model of the riparian zone under 12 types of tcems was validated and compared the results show that the use of the pde module instead of the heat transfer in porous media htpm module is very effective for modeling the pde module could basically replace the htpm module in the heat transfer calculation the performance of the flow heat coupling model varies under different types of tcems and the chung and horton 1987 model shows better simulation effects with a root mean square error rmse coefficient of determination r2 mean absolute error mae and mean relative error mre ranging from 1 37 to 2 48 0 73 0 94 1 06 2 08 and 11 94 15 79 respectively therefore this model could better reflect the dynamic temperature variations in the riparian zone the sensitivity analysis results illustrate that the hydraulic conductivity k s van genuchten parameter β volumetric heat capacity of dry solids c s and porosity n of the flow heat coupling model greatly influence riparian zone temperature variations and the parameters β k s and residual water content θ r significantly affect the lateral hyporheic exchange rate in the riparian zone of which β has the largest effect keywords riparian zone flow heat coupling model thermal conductivity empirical models comsol 1 introduction the hyporheic zone hz which is known as the liver of a river is a transitional zone of dynamic interaction between groundwater and surface water gw sw in rivers conant et al 2019 it undergoes frequent dynamic exchange and active material circulation and it is an important breeding baiting and fattening site for fish shrimp crabs and shellfish kawanishi et al 2013 tonina and buffington 2007 zhang et al 2018 in recent years many scholars have focused their studies on hyporheic exchange in the riverbed including vertical exchange mechanisms main functions and solute cycles fox et al 2018 graham et al 2018 liu et al 2017 zheng et al 2016 however hyporheic exchange not only occurs below the riverbed but also in the riparian zone gerecht et al 2011 watson et al 2018 busato et al 2019 the riparian zone which is one of the main types of hz is formed by sediment deposition in the bay and plays an indispensable role in processes related to river ecosystem balance and stability koiter et al 2013 conant et al 2019 due to the particularity of the location and structure of the riparian hz it is different from the riverbed hz in terms of the exchange mechanism ecological process and material transport busato et al 2019 peralta maraver et al 2018 ren et al 2019a the riparian zone plays an important role in protecting rivers and groundwater especially in effectively regulating and storing floods reducing pollution and protecting the water and soil environments chen et al 2017 ding et al 2017 these abundant functions of the riparian zone have made it a hot spot of eco hydrological research schwab et al 2016 lind et al 2019 the gw sw interaction is always accompanied by heat transfer thus observing the temporal and spatial distribution characteristics of temperature in the hz is an effective means of qualitatively or quantitatively analyzing the hyporheic exchange rate and process patterns constantz 2008 engelhardt et al 2011 2013 in particular the improvement in automatic temperature monitoring equipment signal analysis and processing methods computer numerical calculation and simulation analysis capabilities in recent years has provided a broad space for the application of the heat tracing method for hyporheic exchange and has gradually made the heat tracing method one of the important means of studying the problem of water and heat exchange in the hz ren et al 2018a for example ren et al 2018b developed a 2 d riparian zone laboratory test and evaluated the water flow and thermal dynamics in riparian zones using the injection of a heat tracer test through a field test watson et al 2018 determined how the temperature patterns are related to lateral hyporheic exchange and fluid flow and groundwater surface water interactions in general busato et al 2019 performed hydrogeophysical characterization of the hyporheic and riparian zones at an experimental site in the adige catchment northern italy by means of electrical resistivity tomography ert distributed temperature sensing dts and hydrological modeling therefore the accurate evaluation of riparian temperature variations will promote the development and use of heat tracers to study issues related to the riparian zone although there have been some studies on hyporheic exchange and temperature distribution within the riparian zone these studies have been field based and relatively costly to conduct further studies are hindered by the lack of an effective flow heat coupling model and the best way to fully exploit hydrogeophysical data is through the development of a hydrological model busato et al 2019 therefore further attempts are required to identify a suitable model coupling approach thermal conductivity is the most important parameter affecting the temperature distribution in soil during heat transfer zhang and wang 2017 therefore it can determine the simulation accuracy of the flow and heat coupling model however the thermal conductivity of soils is assumed to be a fixed value formula in some software for simulating heat transfer in geological bodies healy and ronan 1996 koch et al 2016 theoretically speaking this assumption is not accurate because the thermal conductivity of soil is related to the soil type particle size distribution soil structure porosity saturation and temperature among which porosity and saturation have the greatest influence zhang and wang 2017 these considerations are particularly important for the study of flow heat coupling of soil in the unsaturated riparian zone at present there are many models for predicting the thermal conductivity of soils these models could be divided into empirical models and theoretical models and some empirical models have been embedded in water and heat transfer simulation software for example the value of thermal conductivity in hydrus software is predicted according to the chung and horton 1987 model ren et al 2018b in hst3d software the volume average model is used to estimate the value of thermal conductivity bravo et al 2002 comsol software provides users with three choices the volume average model the reciprocal average model and the power law model however with the continuous development of tcems more reasonable and efficient tcems might be neglected therefore it is necessary to carry out secondary development of relevant software so that the different tcems could be taken into account in the flow heat coupling model to compare and analyze their application effects in practical engineering comsol is a finite element analysis and solver software package for various physics and engineering applications especially coupled phenomena or multiphysics li et al 2009 in the field of geoscience comsol could simulate groundwater flow solute transport in soil and geothermal and land subsidence calcutt and anissimov 2019 song et al 2018 jayeoba et al 2019 comsol provides the basic module of heat transfer in porous media but only three thermal conductivity empirical models are considered therefore the performance of different tcems in simulating temperature variations in riparian zones cannot be evaluated however it supports the solution of customized pdes in which users can freely define the required equations and achieve coupling between them li et al 2009 moreover in the modeling process the material attributes source terms and boundary conditions could employ for example piecewise functions interpolation and analysis these functions can be time dependent or space dependent wissmeier and barry 2011 with these advantages secondary development of physical problems can be carried out in comsol without additional programming which has great advantages for coupling different tcems with the flow heat coupling model in this study the porous media and subsurface flow module in comsol was adopted to simulate the soil water flow and based on the tcems and heat transfer theory of porous media the flow heat coupling model of the riparian zone was built using a customized pde module combined with the collected data from the field heat tracer test of the riparian zone the performance of the flow heat coupling model under 12 types of tcems in simulating the temperature variations is analyzed which could provide a reference for the rational selection of tcems in the flow heat coupling model of the riparian zone in addition the orthogonal test method was used to determine the main influencing factors of the flow heat coupling parameters on the temperature and lateral hyporheic exchange rate in the riparian zone to reduce the workload of model calibration and provide a reference for related research 2 flow heat coupling mathematical model of the riparian zone 2 1 flow model the saturated unsaturated transient seepage field of the riparian zone can be described by the richards equation richards 1931 1 ρ w c m ρ w g s e s s p t ρ w k s k r θ μ t p ρ w gz q m where ρ w is the water density kg m3 c m is the specific water capacity m 1 g is the acceleration of gravity m s2 s e is the relative saturation of the soil s s is the elastic water storage rate pa 1 p is the pressure pa t is the time s is the laplace operator θ is the volumetric water content m3 m3 k s is the saturated hydraulic conductivity of the soil m s k r θ is the relative hydraulic conductivity of the unsaturated zone m s which is a function of the volumetric water content μ is the dynamic viscosity of water pa s μ t 0 00002424 10 247 8 t 133 16 kipp 1987 z is the calculation point elevation m and q m is the flow source term kg m3 s the soil hydraulic function is described by the van genuchten model van genuchten 1980 2 θ θ r s e θ s θ r 3 s e 1 1 α h p β m 4 c m α m 1 m θ s θ r s e 1 m 1 s e 1 m m 5 k r θ s e 1 2 1 1 s e 1 m m 2 where θ r is the residual water content m3 m3 θ s is the saturated water content m3 m3 h p is the pressure head m α is the reciprocal of the intake value of the moisture characteristic curve m 1 and β is the parameter indicative of the gradient of the soil moisture characteristic curve obtained by fitting the soil moisture characteristic curve m 1 1 β 2 2 heat transfer model the heat transfer of the riparian zone can be expressed by the advection dispersion equation which is derived by balancing the changes in the energy stored within a volume of porous media and can be written with temperature as the dependent variable healy and ronan 1996 6 θ c w 1 n c s t t k eff t θ c w d h t θ c w u t q s where c w and c s are the volumetric heat capacity of water and the dry solid in j m3 c respectively t is the temperature c k eff is the effective thermal conductivity w m3 c d h is the hydrodynamic dispersion coefficient m2 s and u is the average water velocity calculated by u v θ v is the darcy seepage velocity m s the left side of eq 6 represents the temperature change in the variably saturated volume over time which is a time varying term the first term on the right side of eq 6 represents the heat conduction term the second term is the heat dispersion term the third term represents the heat advection term and the final term q s is the heat source term w m3 the hydrodynamic dispersion coefficient d h is a parameter that characterizes the ability of a porous media to diffuse a contaminant at a certain flow velocity it macroscopically reflects the influence of the groundwater flow process and void structure characteristics on the solute transport process in porous media the advection and dispersion components of heat transfer are analogous to those in the solute transport process healy and ronan 1996 so the hydrodynamic dispersion coefficient could be calculated by the following formula healy 1990 7 d h i j α t v δ ij α l α t v i v j v where α t is the transverse dispersivity of the soil m v is the magnitude of the velocity vector m s δij is the kronecker delta which equals 1 if i j and otherwise equals 0 α l is the longitudinal dispersivity m and vi and vj are the i th and j th components of the velocity vector respectively 2 3 thermal conductivity empirical models 2 3 1 johansen 1975 model johansen 1975 first proposed the concept of normalized thermal conductivity k e he believes that the relationship between the normalized thermal conductivity k e and the degree of saturation s r could simultaneously reflect the influence of the soil type porosity water content and mineral composition on the thermal conductivity of the soil the proposed normalized thermal conductivity is expressed as 8 k e k eff k dry k sat k dry where k e is the normalized thermal conductivity and k sat and k dry are the thermal conductivity of the soil in saturated and dry states respectively in the calculation of the thermal conductivity of saturated soil the formula proposed by sass et al 1971 has been widely used and can be expressed as follows 9 k sat k w n k s 1 n where k s is the thermal conductivity of soil solid particles obtained from quartz q and its thermal conductivity k q 7 7 w m3 c and the thermal conductivity of other minerals k o i e k s k q q k o 1 q among them k o 2 0 w m3 c q 0 2 k o 3 0 w m3 c q 0 2 k w is the thermal conductivity of water k w 0 594 w m3 c at 20 c and n is the porosity by improving the de vries 1963 thermal conductivity model johansen 1975 proposed the thermal conductivity formula for dry soil 10 k dry 0 137 ρ b 64 7 ρ s 0 947 ρ b where ρ b is the bulk density of the soil kg m3 and ρ s is the soil particle density kg m3 in addition johansen 1975 proposed several k e s r relationships for different types of soils by fitting the experimental data of kersten 1949 11 k e 0 7 lg 10 s r 1 s r 0 05 a lg 10 s r 1 s r 0 10 b where s r is the degree of saturation s r θ n eq 11 a and b are for medium fine sands and fine textured soils in unfrozen states therefore the johansen 1975 model can ultimately be summarized as follows 12 k eff k w n k s 1 n 0 137 ρ b 64 7 ρ s 0 947 ρ b k e 0 137 ρ b 64 7 ρ s 0 947 ρ b 2 3 2 campbell 1985 model considering the effects of soil texture bulk density and volumetric water content on thermal conductivity campbell 1985 proposed an empirical model for calculating the soil effective thermal conductivity as follows 13 k eff a b θ a d exp c θ e where the parameters a b c and d could be obtained according to the soil bulk density and clay content which can be expressed as 14 a 0 65 0 78 ρ b 0 60 ρ b 2 b 1 06 ρ b c 1 2 6 m clay 0 5 d 0 03 0 1 ρ b 2 e 4 where m clay is the clay content and ρ b is the bulk density of the soil in g cm3 2 3 3 chung and horton 1987 model the chung and horton 1987 model is the default thermal conductivity empirical formula in hydrus software hydrus is one of the mainstream finite element software programs for simulating water heat and solute transport in variable saturated porous media and due to its outstanding performance it has been widely used by scholars of relevant research in various countries the chung and horton 1987 model can be expressed as follows 15 k eff b 1 b 2 θ b 3 θ 0 5 where b 1 b 2 and b 3 are the regression parameters in the present study the values of b 1 b 2 and b 3 were taken from the chung and horton 1987 and they are 0 228 2 406 and 4 909 respectively 2 3 4 ewen and thomas 1987 model based on the research of johansen 1975 ewen and thomas 1987 proposed an exponential function to describe the relationship between k e and s r 16 k e 1 exp ξ s r where ξ is a fitted parameter and ξ 8 9 the rest of the unknown parameters are calculated in the same way as those of the johansen 1975 model 2 3 5 côté and konrad 2005 model to simplify the calculation of the logarithmic function eq 11 in the johansen 1975 model côté and konrad 2005 proposed a new k e s r relationship with κas the variable based on the normalized thermal conductivity model proposed by johansen 1975 the value of κcould reflect the influence of the soil types i e coarse sand fine sand silt and clay on the k e s r relationships and the effective thermal conductivity of soil 17 k e κ s r 1 κ 1 s r where κis an empirical parameter used to account for the different soil types in the unfrozen and frozen states the suggested values for gravel and coarse sand medium fine sand and silt and clay are 4 66 3 55 and 1 90 respectively côté and konrad 2005 also analyzed the relationships between the thermal conductivity and porosity of dry soil based on a large amount of data from the literature and proposed a new formula for calculating the thermal conductivity of dry soil 18 k dry χ 10 η n where χ and η are the parameters affected by the soil texture where χ 1 70 w m3 c and η 1 80 for crushed rocks χ 0 75 w m3 c and η 1 20 for mineral soils and χ 0 30 w m3 c and η 0 87 for organic fibrous soils the thermal conductivity empirical model proposed by côté and konrad 2005 can finally be summarized as follows 19 k eff k w n k s 1 n χ 10 η n κ s r 1 κ 1 s r χ 10 η n it should be emphasized that the calculation of k s here is slightly different from that in the johansen 1975 model the value of k s was calculated by using the geometric mean method based on the thermal conductivity and volumetric proportion of rock forming minerals therefore a complete soil mineral composition was required in this study the value of k s is taken from the sandy loam in he et al 2017 and it is set to 7 5 w m3 c 2 3 6 lu et al 2007 model to make the johansen 1975 model more suitable for the calculation of the thermal conductivity under low water content conditions lu et al 2007 used the thermo tdr probe to test a large number of thermal conductivities of twelve different soils under various water content conditions he believes that the k e s r relationships are affected by the soil types especially for fine grained soil thus a new exponential function expression of k e and s r was proposed 20 k e exp γ 1 s r γ 1 33 where γ is a parameter determined by the soil texture γ is 0 96 and 0 27 for coarse textured and fine textured soils respectively and 1 33 is a shape parameter furthermore they obtained a simpler linear relationship between the thermal conductivity of the dry soil and the porosity by fitting the experimental data 21 k dry 0 56 n 0 51 other unknown parameters are obtained using the procedure from johansen 1975 lu et al 2007 did not measure the quartz contents of soils but assumed that the quartz content was equal to the sand fraction here we also make this assumption for the estimation of the quartz content 2 3 7 lu et al 2014 model based on lu et al 2007 2014 proposed an exponential function to express the nonlinear behavior of k e as related to soil volumetric water content texture and bulk density 22 k eff k dry exp ϕ θ μ θ 0 where μ and φ are shape factors of the k eff θ curve which are related to the soil texture and bulk density ρ b and can be expressed as 23 μ 0 67 m c 0 24 24 ϕ 1 97 m sand 1 87 ρ b 1 36 m sand ρ b 0 95 where m clay and m sand are the clay and sand contents respectively 2 3 8 nikoosokhan et al 2015 model based on the normalized thermal conductivity proposed by johansen 1975 nikoosokhan et al 2015 proposed a set of line relationships for k sat and k dry 25 k sat 0 53 m clay 0 1 γ d 26 k dry 0 087 m sand 0 019 γ d where γ d is the dry density kn m3 γ d ρb g the relationship between k e and s r is obtained from the côté and konrad 2005 model the value of κcan be calculated from m clay by 27 κ 4 4 m clay 0 4 therefore the model proposed by nikoosokhan et al 2015 could ultimately be expressed as 28 k eff 0 53 m clay 0 1 γ d 0 087 m sand 0 019 γ d κ s r 1 κ 1 s r 0 087 m sand 0 019 γ d 2 3 9 ren et al 2019b model based on previous studies ren et al 2019b considered the influence of organic matter content and particle composition on thermal conductivity and proposed a new relationship between k e and s r based on the previous models within the entire range of water content 29 k e k dry exp φ θ ψ where ψ and ϕ are the shape factors of k eff θ furthermore they believed that the relationships among ψ ϕ and soil texture should be considered based on the larger amount of collected data they proposed the following linear relationship 30 ψ 0 493 m sand 0 86 m silt 0 014 m om 0 778 31 φ 0 736 m clay 0 006 m om 0 222 in this model k sat and k dry are calculated from eqs 25 and 26 and the other unknown parameters are calculated using the formula from the johansen 1975 model 2 3 10 comsol built in models the thermal conductivity empirical models were also considered in comsol software therefore three models are provided for users in the heat transfer in porous media module of comsol the volume average model the reciprocal average model and the power law model the formulas are as follows 32 k eff n k w 1 n k s 33 1 k eff 1 n k s n k w 34 k eff k w n k s 1 n where the thermal conductivities of water k w and soil solid particles k s are 0 594 w m3 c and 7 5 w m3 c respectively 3 development of the flow heat coupling model of the riparian zone in comsol 3 1 secondary development environment in comsol comsol provides users with different levels of secondary development interfaces and allows users to flexibly choose the appropriate interface type according to their needs the secondary development methods in comsol could be divided into the following four types 1 use matlab for secondary development users could use the data processing functions in matlab as a data conversion interface between comsol and other software to achieve coupling between different software 2 use the application program interface api for secondary development users could use the api functions provided by comsol to write the interface environment and running instructions and build their own finite element analysis software 3 use a physical model creator for secondary development comsol directly attributes different physical phenomena to different mathematical physics equations after the user derives the corresponding weak solution formal equation for the physical phenomenon of interest the user could use the creator provided by comsol to develop a custom physics model that can directly call the comsol pre and postprocessing and solver functions this secondary development method is powerful but requires the user to give a weak solution to the mathematical equation which is difficult 4 modify the built in equations of comsol for secondary development the material properties and boundary conditions involved in the comsol mathematical physics equation could be a constant arbitrary variable function or logical expression these parameters and even the coefficients of the pdes could be modified through the comsol interface in addition users could add custom pdes and specify their relationship with other equations to achieve the purpose of coupling calculation this method is relatively simple and flexible and it is used in this paper to carry out secondary development of the flow heat coupling model of the riparian zone 3 2 implementation method of the flow heat coupling model in this paper the richards equation in porous media and subsurface flow module of comsol was used to describe soil water flow however the physical problem of heat transfer in porous media will be achieved through custom pdes the coefficient type pdes provided in comsol are as follows 35 e a 2 u t 2 d a u t c u α u γ β u a u f u ω n c u α u γ g q u u ω hu r u ω where u is the dependent variable e a d a c α γ β a q and h are the custom coefficients f g and r are the source terms on the solution domain and the boundary respectively ω is the solution domain ω is the outer boundary of ω and n is the outer normal direction of ω the coefficients in the above equations have different physical meanings in different problems but essentially all physical phenomena could be expressed by eq 35 which can be defined as different types of functions according to actual physical problems these functions could be derivatives of different orders and they can be time dependent or spatially related first the governing equation of heat transfer is arranged according to the form of eq 35 then each coefficient is defined parts without a corresponding item could be achieved by defining the coefficient f instead of solving programmatically in terms of heat transfer equation modeling in this paper d a u t can constitute the time varying term of the equation c u αu γ could be set as the combination of the advection conduction and dispersion term and f could be viewed as the heat source terms the heat transfer equation is coupled with the richards equation by the soil volumetric water content θ and coupling between the thermal conductivity empirical model and the heat transfer equation is achieved by the effective thermal conductivity k eff to avoid unit confusion in the analysis the variable unit and geometric model size are adapted in international units the node temperature t is taken as the basic unknown variable and the time term in the governing equation is discretized by the implicit euler backward difference method the nonlinear iterative modified damped newton method is used to solve the equation in terms of meshing comsol comes with a predefined triangle mesh unit division method users need only to select a reasonable mesh size to mesh the research objects the denser the mesh size is the more accurate the calculation but the longer the required calculation time for the study of flow heat coupling in the riparian zone the normal mesh size is a reasonable mesh size but when analyzing the unsaturated seepage problem it is necessary to select a particularly refined mesh to calculate the convergence fig 1 shows the flow chart of the comsol solution 4 field experimental data collection and model setup 4 1 data collection to verify the built flow heat coupling model of the riparian zone and to compare and analyze the application performance of different thermal conductivity empirical models in simulating the temperature variation in the riparian zone this paper obtained temperature and water level data from a field test conducted by the u s geological survey nevada water science center carson city nevada within the walker river basin between march 2012 and december 2012 naranjo and smith 2016 to analyze the modeling results u s geological survey researchers installed water level and temperature sensors in rivers and riparian zones to achieve long term dynamic monitoring of the water levels and temperatures first polyvinyl chloride pvc pipes wrapped with sieves were driven into the sediment layer of the riparian zone and then a string of three to four independent water resistant temperature sensors alpha mach inc ibcod type z accuracy 0 1 c resolution 0 01 c were placed inside the pvc pipes to monitor the temperature created by lateral hyporheic exchange and a similar type of sensor alpha mach inc ibcod type l accuracy 0 5 c resolution 0 5 c was installed at the base of the pvc pipes slightly below the land surface to monitor the air temperature the data monitored during the test were recorded and stored by a data recorder every 1 h fig 2 shows the canal water level air and water temperature time series data monitored by the sensors more detailed information about the study area and data collection methods are provided by naranjo and smith 2016 4 2 conceptual model setup based on the aforementioned mathematical models and the collected field experimental data the finite element solution of the saturated unsaturated flow and heat coupling model of the riparian zone could be realized by comsol software through the combination of the richards equation and the custom coefficient type pde module depending on the permeability the hydrostratigraphy could be simplified to three zones fig 3 in terms of the boundary conditions set up in the conceptual model for the saturated unsaturated seepage field the no flow boundary conditions were set at the left fa right bc and atmospheric soil contact interfaces cd and ef the variable head boundary condition was set at the water soil contact interface de and the permeable boundary was set at the bottom ab for the temperature field the adiabatic boundary conditions neumann boundary condition achieved by the second formula in eq 35 were set at the left fa right bc and bottom ab the air and water temperature boundary conditions dirchlet boundary condition achieved by the third formula in eq 35 were set at the atmospheric soil and water soil contact interfaces cd ef and de respectively and the initial temperatures of the three zones were set based on the mean value of the sensor measurements at the corresponding zones at the initial time referring to the relevant literature table 1 gives the hydraulic and thermal parameters to evaluate the flow heat coupling model of the riparian zone where the values of k s θ r α β n m om c s and c w were obtained from naranjo and smith 2016 and the values of m clay m sand m silt and θ s were taken from the sandy loam of carsel and parrish 1988 the values of ρ s and ρ b were obtained from the soils with closer soil textures porosity and particle size distributions of he et al 2017 and keller and håkansson 2010 the values of longitudinal dispersivity α l and transverse dispersivity α t in the text were considered to be 0 01 m naranjo and smith 2016 4 3 criterion for model performance evaluation to quantitatively evaluate the simulation performance of the models four different types of standard statistical methods were considered as statistical performance evaluations the root mean squared error rmse coefficient of determination r2 mean absolute error mae and mean relative error mre were used the four performance evaluation criteria used in this study could be calculated by utilizing the following equations vafakhah 2013 9 rmse 1 n i 1 n o i s i 2 10 r 2 1 i 1 n o i s i 2 i 1 n o i o 2 11 mae 1 n i 1 n o i s i 12 mre 1 n i 1 n o i s i o i 100 where n is the number of data oi and si are the observed and simulated temperatures at moment i respectively and o is the average measured temperature the rmse is a nonnegative value and a low rmse indicates good consistency between the observed and simulated values r2 ranges from to 1 and a larger r2 value indicates a better match of the simulated and observed data a value of r2 0 6 is considered good while a value of 0 indicates that the estimations are no better than those derived from taking the average of the observed data while values less than 0 represent increasingly poor model performance zounemat kermani et al 2016 lower values of the mae and mre indicate less deviation between the observed and simulated data 4 4 sensitivity analysis in previous studies of riparian zones many scholars have focused on temperature variations or lateral hyporheic exchange rate flux changes in riparian zones dick et al 2015 liu et al 2018 therefore in this section the temperature t and the absolute value of the lateral hyporheic exchange rate vx were selected as the main test indexes for sensitivity analysis the parameter sensitivity of the coupled model is determined by quantitative analysis of the changes in the temperature and the lateral hyporheic exchange rate in the riparian zone caused by the changes in each parameter considering the range of the riparian zone we limit the object area of the sensitivity analysis to 5 13 m x 6 14 m and 2 90 m y 1 10 m in fig 3 the difference between the temperature field or lateral hyporheic exchange rate and the rated temperature field or rated lateral hyporheic exchange rate due to changes in the parameters was determined for each finite element node then the average change was used to represent the change in temperature lateral hyporheic exchange rate the selected sensitivity analysis objects are seven flow heat coupling model parameters that affect the model s temperature variations and lateral hyporheic exchange rate i e hydraulic conductivity k s saturated water content θ s residual water content θ r van genuchten parameters α and β porosity n and specific heat capacity of dry solids c s three factor levels were selected for each parameter one acting as the reference value and then an increase and decrease in the reference value by 10 the level of each factor in the parameter sensitivity analysis is shown in table 2 according to the number and the level of test factors shown in table 2 and combined with the principle of the orthogonal test method in appendix 1 the l 18 2 37 orthogonal table is selected to arrange the test the first column is set as a blank column the test factors are randomly assigned to the last seven columns of the orthogonal table and each element in the table is replaced by the corresponding design parameters according to its corresponding factors and levels then the orthogonal test table for the parameter sensitivity analysis of the flow heat coupling model of the riparian zone could be obtained 5 results and discussion 5 1 verification of pde module modeling effectiveness in previous studies on flow heat coupling some scholars mostly used the heat transfer porous media htpm module to simulate the internal heat transfer of the research object anderson 2005 ren et al 2019a however this modeling method could use only the default empirical formula of the software to predict the thermal conductivity which may ignore some new and more accurate tcems thus to verify the effectiveness of replacing the htpm module with the pde module the heat transfer model is built based on the pde module and the htpm module to make the two modeling methods comparable the formula of thermal conductivity in the pde module is written according to the three tcems of the htpm module i e the volume average model the power law model and the reciprocal average model in this way we could verify the effectiveness of the pde module by comparing the consistency of two different modeling methods in calculating the temperature variations in the riparian zone thus providing a basis for the subsequent use of the pde module to compare the performance of different tcems in simulating temperature variations in the riparian zone fig 4 shows a comparison of the riparian zone temperature variations simulated by two different modeling methods using three comsol built in tcems at different monitoring points it can be seen from fig 4 that regardless of which tcem is built in comsol the temperature variations in the riparian zone simulated by the two modeling methods are basically consistent to quantitatively evaluate the performance of the pde module in this section the result of the htpm based method is considered as observed data and the pde based method is considered as simulated data in this way we could quantitatively evaluate the difference in the calculation results based on two different modeling methods through the four abovementioned performance evaluation indicators table 3 shows the results of the four performance evaluation indicators of the pde based method from the results of four evaluation indicators the ranges of the rmse r2 mae and mre at 0 16 0 53 c 0 99 1 00 0 13 0 46 c and 0 87 3 41 respectively indicating that the results calculated by the two modeling methods are in good agreement with small deviations in general the rmse mae and the degree of the rmse greater than the mae could reflect the change in the difference between the observed and simulated values or in a certain sense the degree of the maximum difference ramos et al 2011 fig 5 shows the difference between the rmse and mae at different monitoring points under the three tcems and it can be seen that the rmse is always larger than the mae and that the difference between them is very small and basically remains at approximately 0 04 c in addition compared with other evaluation indicators of the three models it can be seen that the variation in the performance evaluation indicators under different tcems is also very small these results indicate that in the modeling process of the pde module the replacement of the tcem will not affect the stability of the model and the pde module adopted in this paper is effective in modeling the heat transfer model and could basically replace the htpm module for heat transfer calculation therefore in subsequent research we could modify the formula of thermal conductivity in the pde module to compare and evaluate the practical application performance of the existing tcems in engineering 5 2 model validation and comparative analysis fig 6 shows the comparison between the observed and simulated temperature values at eight monitoring points in the riparian zone based on 12 types of tcems and the performance is evaluated by four indicators introduced in section 4 4 table 4 presents the statistical results of four model evaluation indicators of different tcems at the eight monitoring points as shown in table 4 there are differences in various indicators obtained by using the different tcems and the performance is also different at different monitoring points to reflect the investigation more intuitively the statistical results in table 4 are presented in the form of radar charts fig 7 that can intuitively reflect the performance of the different tcems and be used to compare the evaluation indicators at different monitoring points fig 7 reveals that the values of the rmse r2 mae and mre of the campbell 1985 model are the worst of all the tcems which indicates that the performance of the campbell 1985 model is poor in the application of this engineering case in addition all the indicators simulated by the tcems perform poorly at the bp1 2 50 m and bp2 2 60 m monitoring points when we compare the observed and simulated values at different monitoring points of the same pvc pipe in fig 5 we find that the simulated temperatures at bp1 2 50 m and bp2 2 60 m are smaller than the observed temperature and the ranges of rmse r2 mae and mre are 2 81 6 89 c 0 75 0 75 2 44 6 09 c and 14 28 35 17 respectively the simulation error at these two points is large generally the temperature range in the riparian zone decreases with increasing depth in a certain range of depths and remains stable after reaching a certain depth anderson 2005 ren et al 2019a however the temperature curves at these two monitoring points are obviously higher than those at the upper monitoring points we suspect this phenomenon might be caused by an upwelling groundwater temperature influence at the bottom monitoring sensors in addition referring to naranjo and smith s 2016 report we found that the same issues were noted in their research which supports our hypothesis therefore to make the results more applicable to general situations the analysis of these two abnormal monitoring points should be removed when evaluating the performance of the tcems due to the different performances of different tcems at different monitoring points it is difficult to evaluate the performance of the tcems based on the existing results it could only be concluded that the performance of the campbell 1985 model is poor and the error is large at bp1 2 50 m and bp2 2 60 m therefore we used the global analysis method to compare the observed and simulated values of six monitoring points excluding bp1 2 50 m and bp2 2 60 m and the comparison results are shown in fig 8 as shown in fig 8 among the 12 types of tcems the symbols of the chung and horton 1987 model the ewen and thomas 1987 model the lu et al 2007 model and the côté and konrad 2005 model are in a narrower range along the 1 1 line while the trend line of these models is close to y x it also has a high r2 indicating that they have good performance in simulating the temperature variations in the riparian zone in contrast the performance of the campbell 1985 model and the volume average model are relatively poor to quantitatively evaluate the performance of 12 types of tcems in simulating temperature variations in the riparian zone fig 9 provides a global comparison of four performance evaluation indicators for different tcems and it can be found that the performance varies with the models among all the tcems the chung and horton 1987 model performs best with rmse r2 mae and mre values of 2 11 c 0 84 1 70 c and 13 88 respectively this result may explain why hydrus software has excellent performance in the simulation of water flow and heat transport and has been widely favored by researchers in related fields among the three tcems built in comsol the volume average model performed poorly on various indicators while the power law model performed relatively well however their performance is obviously worse than that of the chung and horton 1987 model the ewen and thomas 1987 model the lu et al 2007 model and the côté and konrad 2005 model which also explains the necessity of secondary development of the htpm module in comsol therefore the modeling method of the flow heat coupling model of the riparian zone in this paper might provide some references for scholars in related fields 5 3 parameter sensitivity analysis of the flow heat coupling model table 5 shows the simulation schemes and results statistics of the orthogonal test and the horizontal combination of the corresponding factors in each row in table 5 is a test scheme according to the test scheme designed in table 5 the test indexes t and vx under each scheme are calculated and the results are listed in the last two columns of table 5 then according to the calculation results of each scheme the sensitivity of each parameter to the test indexes are analyzed by the range analysis method described in appendix 1 table 6 shows the results of the range analysis on the influencing factors of the index t and the results showed that the order of the sensitivities of the factors to the index t from large to small was as follows k s c s β n θ r θ s and α table 7 presents the results of the range analysis on the influencing factors of the index vx and the results showed that the order of the sensitivities of the factors to the index vx from large to small was as follows β k s θ r θ s c s n and α fig 10 shows the results of the range values of each factor of the two test indexes according to the principle of range analysis described in appendix 1 the greatest factor of the range value is the most sensitive and has the greatest influence on the test indexes therefore according to fig 10 we can conclude that the parameters k s β c s and n of the flow heat coupling model have a great influence on riparian zone temperature variations and the parameters β k s and θ r have a significant effect on the lateral hyporheic exchange rate in the riparian zone of which β is the largest 6 conclusions comsol is very flexible in setting material properties source terms and boundary conditions and supports the solution of custom pdes based on these advantages to simulate soil water flow with a porous media and subsurface flow module a flow heat coupling model of the riparian zone based on tcem was built through the custom pde module in comsol and the model was verified by the htpm module built in comsol the results show that the heat transfer model based on the pde module is very effective and could basically replace the htpm module for heat transfer calculation the field heat tracer test data in the riparian zone were adopted to compare and analyze different flow heat coupling models the results show that the performance of the flow heat coupling model under different types of tcems is different in simulating the temperature variations in the riparian zone among them the chung and horton 1987 model the ewen and thomas 1987 model the lu et al 2007 model and the côté and konrad 2005 model have good simulation effects in contrast the campbell 1985 model performs poorly and has large calculation errors the sensitivity analysis results show that the parameters k s β c s and n of the flow heat coupling model have a great influence on the temperature variations in the riparian zone among which k s is the largest followed by c s the parameters β k s and θ r have a significant influence on the lateral hyporheic exchange rate in the riparian zone and the sensitivity of parameter β is the greatest therefore in the analysis of the temperature or lateral hyporheic exchange rate in the riparian zone by using the flow heat coupling model parameters with high sensitivity should be taken as the key points of parameter determination these parameters could be determined by experiments or inversion analyses and parameters with low sensitivity can be simplified appropriately as in the engineering analogy method in this way even if those minor factors are neglected a numerical model that is close to the actual phenomenon could be obtained credit authorship contribution statement wenbing zhang conceptualization data curation software methodology validation writing review editing zhenzhong shen funding acquisition supervision writing review editing jie ren funding acquisition formal analysis resources software methodology supervision writing review editing lei gan writing review editing fei wang writing review editing bihan yu writing review editing chenglin li writing review editing declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments we are grateful to the u s geological survey of nevada for sharing the data needed in this study we thank the precious suggestions by anonymous reviewers and editors which have greatly helped the improvement of the paper this work was supported by the national natural science foundation of china grant no 51679194 and the planning project of science and technology of water resources of shaanxi grant no 2019slkj 12 and the state key laboratory of eco hydraulics in northwest arid region xi an university of technology grant no 2019kjcxtd 10 and the project of national natural science foundation of china yalong river joint fund grant no u1765205 and the priority academic program development of jiangsu higher education institutions papd appendix 1 orthogonal test method orthogonal test design orthogonal test method is based on mathematical statistics and orthogonality principle and select appropriate representative points from a large number of test points and then arrange the test according to the orthogonal table due to the orthogonal table has the construction principle of balanced dispersion and neat comparability the number of experiments designed according to this method is less and it can reflect the changing rule of objective things in the orthogonal test the results we investigate usually called indexes the parameters that may affect the test indexes are called factors and the specific test conditions that each factor should be compared in the test are called levels the orthogonal table is expressed by ln tc where l is the code name of the orthogonal table n is the total number of tests t is the level number of factors and c is the number of columns of the orthogonal table that is the maximum number of factors that can be arranged orthogonal table is the key of orthogonal experimental design it must satisfy the following two conditions 1 the different levels of each factor appeared the same number of times in the test to ensure its uniformity 2 in order to ensure the uniformity of the distribution of test points the number of pairs of different levels of combinations of any two columns of factors occurs the same number of times in the test only by satisfying these two conditions can the test results be conveniently and comprehensively reflected table 8 shows the eight factors orthogonal table of l 18 2 37 where the first factor is two levels and the last seven factors are three levels orthogonal test results analysis method according to the test design of orthogonal table the sensitivity of each factor to the index could be judged by calculating the index value of each test in this paper the range analysis method is used to analyze the data of orthogonal test results suppose a and b represent the different influencing factors in the test t is the level of influencing factors ai represents the i th level of influencing factor a i 1 2 t xij represents the i th level of influencing factor j i 1 2 pij j a b and pij tests were performed at xij to obtain pij tests results yk k 1 2 pij the formula is 36 k ij 1 p ij k 1 p ij y k y where kij is the average value of the test results of factor j at the i th level pij is the number of test of factor j at the i th level yk is the k th test index value y is the average value of all test results the range value rj is the criterion for evaluating the sensitivity of range analysis factors and its formula is expressed as follows 37 r j max k 1 j k 2 j min k 1 j k 2 j the larger the value of rj the greater the influence of the change in the factor on the test index and means that the more sensitive the influencing factor is on the contrary the smaller the value of rj the less sensitive the influencing factor is 
