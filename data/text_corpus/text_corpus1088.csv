index,text
5440,the overall quality of groundwater gw is important primarily because it determines the suitability of water for drinking irrigation and domestic purposes in this study the adaptive fuzzy interface system anfis support vector machines svms and artificial neural network ann models were employed for predicting the total dissolved solids of aquifers the moth flam optimization cat swarm optimization cso particle swarm optimization pso shark algorithm sa grey wolf optimization gwo and gravitational search algorithm gsa were used to train the anfis svm and ann models the data were collected from yazd plain iran to predict the total dissolved solids tds the principal component analysis was used to determine the most appropriate inputs for predicting tds the hybrid anfis mfo improved the accuracy of rmse roo mean square error over the ann mfo and svm mfo models by 1 4 and 3 8 respectively it was also observed that the svm model had the least nse nash sutcliffe efficiency value among all the models unlike the standalone anfis the multilayer perceptron mlp and svms models the hybrid anfis ann and svm demonstrated high accuracy in the training and testing phase so that in the optimal hybrid model anfis mfo values of mean absolute error mae nash sutcliff efficiency nse and percent bias pbias were 2 21 mg lit 0 94 0 15 2 981 mg lit 0 93 and 0 18 respectively the anfis mfo was also seen to further enhance the rmse by approximately 3 and 7 as compared to the ann mfo and svm mfo this study also aims to investigate the temporal variability tds using innovative trend analysis ita the tds value of 1800 mg lit indicates a decreasing trend while a medium tds value 2000 mg lit tds 2800 mg lit does not have a significant trend the high tds values tds 3000 mg lit indicate an increasing trend in this study the anfis mfo and anfis cso models showed superior performance over the other models hence indicates significant implication in their application for other water resources and hydrological variables keywords total dissolved solids artificial intelligence model optimization algorithms groundwater quality 1 introduction groundwater gw among the most important natural water resources has many uses in both agriculture and domestic consumption banadkooki et al 2020 ehteram et al 2019a the overall gw quality is evaluated allowing for the gw to be used in an optimal manner banadkooki et al 2019 decision makers can manage gw resources in a more beneficial manner if they have a clear knowledge of the overall quality and quantity of the gw asadollahfardi et al 2011 a number of researchers have investigated the gw of aquifers in order to evaluate its appropriateness for various uses asadollahfardi et al 2018 the water demand is constantly increasing but freshwater is restricted therefore it is necessary to evaluate the gw quality not only for its utilization but also as a potential source of water for future consumption ehteram et al 2020 the quality of gw is important primarily due to the suitability of water for different aims yahya et al 2019 houatmia et al 2016 reported that the interaction between evaporation and rock water has a significant effect on the gw quality in the northeastern part of tunisia li et al 2016 observed that gw chemistry is affected by rock water interaction furthermore it has been reported that rock water interaction affects the quality of gw adimalla and venkatayogi 2017 thus finding an efficient way to evaluate the quality of gw is necessary for monitoring purposes a large number of studies have made an effort to evaluate a wide range of water quality parameters by means of numerical stochastic and deterministic models the gw quality is often described by total dissolved solids tds studies on the accurate estimation of tds is essential for the protection of freshwater gw becomes saline at a high tds concentration gw with a high tds concentration is generally regarded as problematic for agriculture and irrigation additionally modelling tds concentration and consequently predicting it is essential for hydrological and agricultural management studies kisi et al 2013 deterministic and numerical models have been widely applied for water quality modelling zounemat kermani et al 2019 previous works in the literature have reported that numerical and deterministic models might often provide inaccurate results due to the complicated hydraulic condition of aquifers in recent years soft computing models have been widely used for predicting hydrological variables the artificial neural network ann model adaptive neuro fuzzy interface system anfis support vector machines svms model genetic programming gp gene expression programming gp and decision tree model dtm have been widely used for predicting rainfall yaseen et al 2018 chang and tsai 2016 evaporation piri et al 2019 adnan et al 2019 malik et al 2018 runoff kumar et al 2019 ashrafi et al 2019 as well as drought kisi et al 2019a b malik et al 2019 fung et al 2019 the anfis and ann models have some advantages such as high adaption capability and rapid learning capacity asadollahfardi et al 2011 applied two different kinds of ann models to predict tds the results demonstrate that the accuracy of the recurrent neural network was better than the multilayer perceptron mlp model kisi and ay 2012 used the mlp radial basis function neural network rbfnn and multilinear regression model to evaluate dissolved oxygen do their results indicate that the mlp model had an acceptable performance compared to rbfnn and multilayer regression mlr verma and singh 2013 used an ann model to predict biochemical oxygen demand bdo using do ph and total suspended solids tss as the model s input the output reveals that the ann model had an acceptable accuracy in the prediction of ann ghavidel and montaseri 2014 employed the ann genetic express programming model gep and anfis model to predict tds parameters for a basin in iran they found that the gep model presented more accurate predictions than the anfis and ann models rajaee and shahabi 2016 compared wavelet ann and wavelet genetic programming in the prediction of nitrogen concentration the results demonstrate that the hybrid ann and genetic programming exhibited better performance compared to the standalone ann and genetic programming models arabgol et al 2016 used a svms model to estimate nitrate concentration in gw in arak plain their findings indicate that the svm model estimated nitrate concentration in the training and testing phase with a reasonably high correlation isazadeh et al 2017 used svms and ann models to simulate the spatial variability of the gw qualitative parameters the results reveal that the svm model had less uncertainty than the ann model asadollahfardi et al 2018 used mlp and time series to estimate the tds in a basin the calcium chloride and bicarbonate were used as an input to the models the results indicate that the mlp was more reliable than the time series model najafzadeh et al 2019 used gene express programming tree model and evolutionary polynomial regression model to estimate chemical oxygen demand their findings indicate the superiority of the evolutionary polynomial regression model compared to the gene express programming and tree models zounema kermani et al 2019 used an ann model for predicting nitrogen concentration the results indicate that the mlp model was superior in the prediction of nitrogen concentration over other models jafari et al 2019 predicted tds of the gw aquifer in the tabriz plain the anfis svm and mlp models were used to predict tds according to results the anfis mlp and svm model could be employed successfully in estimating tds aryafar et al 2019 used ann anfis and genetic programming to estimate ec and tds the anfis and ann produced accurate results for the estimation of the water quality parameters in another study several optimization algorithms including particle swarm optimization genetic algorithm and ant colony optimization were employed to estimate gw quality variables kisi et al 2019a b the results revealed that the all of the considered optimization algorithms enhanced the accuracy of the standalone anfis model in the modelling of ec and tds another study applied ann and multiple linear regression mlr to estimate the fitness of gw quality kadam et al 2019 the results reveal that predictions of ann were satisfactory for the prediction of water quality index maroufpoor et al 2019 used the anfis and ann models for predicting the spatial distribution of gw electrical conductivity ec the ann model revealed the most accurate outputs with the lowest rmse and mae although soft computing models have been successfully applied in the prediction of water quality parameters they have some weaknesses the ann svm anfis and knn models have unknown parameters the ann svm and anfis parameters have a significant influence on the accuracy of the models the literature review commonly uses classical training algorithms for training ann svm and anfis models however a reoccuring issue is that these algorithms may be trapped in a local optimum thus robust algorithms such as optimization algorithms can be used to fine tune the ann svm and anfis parameters among the relatively new optimization algorithms is the moth flame optimization mfo algorithm mirjalili 2015 the mfo mimics the behaviour of moths for navigating during the night the moths keep a fixed angle to the moon when flying the literature review has proved that this algorithm can potentially lead to a faster and robust method thus the mfo was selected for the current study due to the mentioned advantages in this study mfo was used to fine tune the ann svm and anfis parameters for predicting tds additionally the anfis cso gwo pso sa and gsa svms cso gwo pso sa and gsa and ann cso gwo pso sa and gsa models were used in an empirical comparison with three new hybrid models ann mfo anfis mfo and svm mfo for predicting tds the structure of this paper laid out as follows 1 section 2 explains the structure of the anfis ann and svm models afterwards the structure of the optimization algorithm is explained this is followed by a demonstration of how the anfis svm and ann models are combined with the optimization algorithms 2 secondly the principal component analysis pca method is demonstrated to select the best inputs to the anfis svm and ann models 3 thirdly the statistical indexes such as root mean square error rmse mean absolute error mae nash sutcliff efficiency nse and perecent bias pbias are used to evaluate the ability of the models an uncertainty analysis is also performed to have a more comprehensive comparison of difficult soft computing models 4 finally innovative trend analysis ita is used to determine the temporal changes of the tds series during the 2002 2015 period 2 method 2 1 anfis model the anfis model is widely applied for predicting hydrological variables and water quality parameters yaseen et al 2018 the anfis model contains an adaptable takagaki sugeno fuzzy model structurally the anfis model comprises five layers each of these layers is used for different purposes a classical anfis model has two inputs m n and a single output each layer consists of a number of neurons mekanik et al 2016 two fuzzy if then rules are used as follows if m is a1 and n is b1 then f1 p1m q1n r1 rule 1 if m is a2 and n is b2 then f2 p2m q2n r2 rule 2 where a1 a2 b1 and b2 membership functions for inputs m and n and p1 q1 r1 p2 q2 and r2 the output function parameters layer 1 there exist adaptive nodes in the first layer the fuzzy membership functions are the output of this layer 1 o 1 i μ a i x 2 o 1 i μ b i y where and y input nodes a and b linguistic labels and b i y symbolize the mfs membership fuzzy function layer 2 the final output of this layer o2 i shows the firing strength of its corresponding fuzzy rule as follows 3 o 2 i ω i μ a i x μ b i y where ω i the firing strength of rule and o 2 i the output of the second layer layer 3 each neuron in the third layer is shown by a circle and is labelled as n the ith rule s firing strength to the sum of all rules firing strength is computed by this layer as follows 4 o 3 i ω i ω i ω i where ω i the normalized firing strength and o 3 i the output of the third layer layer 4 every node in the fourth layer corresponds to adaptive processing units as follows 5 o 4 i ω i f i ω p i x q i y r where p i q i and r consequent parameters and o 4 i the output of the fourth layer layer 5 the overall output is computed as the sum of all incoming signals as follows 6 o i 5 i ω i f i i ω i f i i ω i where o i 5 the output of the fifth layer fig 1 shows the anfis structure the following membership function is used for this study 7 μ x e x c σ 2 where c and σ premise parameters the backpropagation gradient descent bgd is widely used to train the anfis models the bgd may be trapped in a local optimum in this study 25 optimization algorithms were used to fine tune the consequent and premise parameter 2 2 multilayer perceptron mlp model the mlp model is widely used for predicting various parameters such as meteorological hydrological and water quality parameters mehr and nourani 2017 the mlp models are a class under the ann model the mlp model consists of three layers each of these layers consists of several processing units the weighted connections connect each unit to the units in the next layers acharya et al 2019 the first layer receives the input neurons the second layer is known as the middle layer or hidden layer the third layer is the output layer to compute the output of the jth neuron in a middle layer all the inputs are entered into the jth neurons to be multiplied by the corresponding connection weight to jth neuron yaseen et al 2019 8 h j f i 1 n x i w ij b where f activation function b bias parameter w ij the weight of connection h j the output of jth neuron and xi the received input from the ith neuron the backpropagation algorithm is widely used for training the ann models the backpropagation algorithm may not have a fast convergence rate thus the optimization algorithms can be used to fine tune the ann parameters such as weight and bias parameters fig 2 shows the structure of the mlp model fig 3 2 3 svms model the svms model is widely used for classification and regression analysis this model uses the following equation to show the relationship between input and outputs pham et al 2020 9 f x η t ϕ x b where f x the output value η weighting coefficient of input data b bias of η t t transpose symbol and ϕ x the nonlinear mapping function the svms attempts to minimize the difference between measured data and estimated data samantaray et al 2020 thus it is used to minimize an optimization problem whose objective function is decreasing the error function the parameters η and b are computed as follows samantaray et al 2020 10 mi n η b ξ ξ 1 2 η η t c i 1 n ξ i ξ i 11 y i η t ϕ x i b ε ξ i y i η t ϕ x i b ε ξ i ξ i ξ i i 1 2 n where r ε the empirical risk c regulation parameter ξ i the errors above ε and ξ i the errors below ε finally the following equation is used for the svms model samantaray et al 2020 12 f x i 1 n β i β l k x i x j b where β i and β i the lagrangian coefficients and k x i x j the kernel function the radial basis function is widely used as a kernel function as follows 13 k x i x j exp x i x j 2 2 γ 2 where γ the gaussian parameter the parameters c and γ have a significant effect on the accuracy of the svms model in this study the optimization algorithms were used to find the optimal value of the svms parameters 2 4 optimization algorithms this section explains the considered optimization algorithms additionally the advantages and disadvantages of the optimization algorithms are demonstrated then the mathematical model of each of the algorithms is explained 2 4 1 moth flame optimization algorithm mirjalili 2015 introduced the moth flame optimization algorithm mfoa the mfoa is widely used for different optimization problems including the optimization of water resource systems li et al 2018 solving numerical optimization problems khalilpourazari and khalilpourazary 2019 data clustering shah et al 2018 and thresholding image segmentation el aziz et al 2017 the behaviour of moths for navigating during the night is simulated by the mfo the moths fly based on a mechanism referred to as transverse orientation the moths keep a fixed angle to the moon when flying as shown in fig 4 a this mechanism helps moths to fly in a straight line in the mfo the candidate solutions are the moths and the decision variables are the position of the moths mfo has high flexibility and robustness the disadvantage of mfo is that it involves a complicated implementation each moth flame is regarded as a value of d dimensional vectors as a candidate solution thus the position of moths is defined in a matrix as follows mirjalili 2015 14 m m 1 1 m 1 2 m 1 d m n 1 m n 2 m nd where n the number of moths d the number of dimensions mnd the position of the nth moth in the dth dimension and m the population of search agents the fitness values of moth flames are saved as follows mirjalili 2015 15 om o m 1 o m 2 o m n where o m n the fitness value of nth moth s position and om the fitness value of the moths the set of corresponding objective function values is saved as follows 16 f f 1 1 f 1 2 f 1 d f n 1 f n 2 f nd where fnd the position of nth moth in the dth dimension the fitness value of flames is shown as follows 17 of o f 1 o f 2 o f n where o f n the fitness value of nth flame s position in mfo moths search for solutions in the search space and the flames are the best solutions found by each moth thus the flames are regarded as flags in the search space a moth searches for better solutions around a flame three functions are used by the mfo to initialize the random position of moths i move moths in the the search pace p space p m m and finish the search space t the function i is considered as follows 18 m i j u b i l b j r a n d l b j where u b i upper bound of decision variable l b j lower bound of the decision variable rand random number and m i j the position of the ith moth with respect to the jth flame there are two points about a logarithmic spiral 1 spiral s initial location should start from the moth 2 spiral s final location should be the position of the flame the transverse orientation is used by the moth to search around the flames thus the p function is shown as follows mirjalili 2015 19 m i s m i f j d ij e bt cos 2 π t f j 20 d ij f j m j where b a constant to define the logarithmic shape spiral t random number f i the position of ith flame mj the position of jth moth and d ij the distance between the ith flame and jth moth the spiral motion of moths around the flames adjusts the balancing between exploration and exploitation ability finally the following equation is used to enhance the exploitation of the mfo 21 f l a m e number r o u n d n i n i t where i current number of iterations t maximum number of iterations and n maximum number of flames fig 4 shows the flowchart of the mfo 2 4 2 cat swarm optimization cso cat swarm optimization cso was initially introduced by chu et al 2006 the algorithm has been extensively utilized for different applications such as solving global optimization problems kumar and singh 2018 robotics problems karpenko and leshchev 2019 the manufacturing design cell problems soto et al 2019 and cloud computing gabi et al 2018 the cso mimics the developed behaviour of cats each cat shows a solution the cso contains two different processing modes namely tracing and seeking modes the cats in the seeking mode rest and keep eyes on their surroundings the tracing model simulates the cat chasing prey when the cats finds the prey they determine the speed and direction of their movement based on the prey s location the seeking mode consists of four parameters all for the seeking process which are i seeking memory pool smp ii seeking the range of the selected dimension srd iii counts of dimension to change cdc and iiii self position consideration spc the number of copies provided in the seeking mode is shown by smp therefore the difference between the new and the old values in the dimension space are investigated and the maximum difference is selected as the mutation hence referred to as the srd parameter the spc parameter shows the current position of cats chu et al 2006 finally the cdc expresses how many dimensions should be mutated the seeking mode consists of the following levels 1 provide j copies of the current location cat position k with j smp if the spc value is true then keep it as a possible solution 2 provide a random value for srd 3 for the new positions compute the objective function if the values of the objective function are identical then re adjust for all solutions a probability value equal to 1 otherwise recalculate the seeking probability of each solution chu et al 2006 as follows 22 p i f s i f s b f s max f s min where p i the probability of current candidate solutions f s i the objective function value of ith cat f s max maximum value of the objective function f s min minimum value of the objective function f s b f s max for maximization problem and f s b f s min for minimization problem 4 carryout mutation and replace the position of cats the tracing mode indicates cat with a quick movement the following equation is used to update the velocity of cats chu et al 2006 23 v k d v k d r 1 c 1 x best d x k d where v k d the new velocity of cats v k d the velocity of the kth cat r 1 random number c 1 a constant x best d the location of the optimal solution and x k d kth location of cat finally the new position of cats is updated as follows chu et al 2006 24 x k d n e w x k d o l d v k d where x k d n e w the new position of kth cat and x k d o l d the old position of kth cat the whole procedure of the cso is presented in the flowchart in fig 5 2 4 3 grey wolf among the robust optimization algorithms is the grey wolf optimization gwo algorithm gwo is widely used for various optimization fields such as the prediction of friction capacity of driven piles moayedi et al 2019 feature selection for big data manoj et al 2019 classification of kidney images raju et al 2018 predicting compressive strength golafshani et al 2020 and monthly streamflow forecasting tikhamarine et al 2019 the gwo mimics the exploring behaviour of grey wolves the grey wolves strictly follow a collective hierarchy the grey wolves in the leader group are referred to as alpha wolves a couple of males and females in the leader group make all the important decisions the other groups beta wolves at the subsequent level assist the grey wolves in the leader group the beta wolves are used to adjust the flock while the alpha wolves may die the older alpha wolves are substituted by the beta wolves the other group of wolves is referred to as delta δ they engage in the hunting process finally the last group of wolves is referred to as omega w the omega group plays the role of babysitters fig 6 shows the diagram for gwo the advantages of gwo are fast seeking speed and high accuracy fig 7 shows the flowchart of the gwo in order to mathematically simulate the social behaviour of wolves when designing gwo the fittest solution is considered as the alpha consequently the second and third best solutions are called beta and delta respectively the remainder of the solutions are hypothesized as omega a prey is encircled by the wolves during the hunt in order to mathematically simulate the encircling behaviour the following equations are suggested moayedi et al 2019 25 d c x p x t 26 x t 1 x p t b d where t the current iteration x p t the location of prey and x t 1 the position of wolves at iteration t 1 the parameters b and c are computed as follows moayedi et al 2019 27 b 2 a r 1 a c 2 r 2 where r1 and r2 random values and a a decreasing coefficient from 2 to 0 assuming the alpha beta and delta wolves are nearest to the prey the location of these kinds of wolves are used to estimate the prey s position the position of other agents is updated according to the location of the alpha beta and delta wolves moayedi et al 2019 as follows 28 d α c 1 x α x d β c 1 x β x d δ c 3 x δ x 29 x 1 x α b 1 d α x 2 x β b 2 d β x 3 x δ b 3 d δ 30 x t 1 x 1 x 2 x 3 3 where x α the location of alpha wolves x β the location of beta wolves x δ the location of delta wolves 2 4 4 shark algorithm sa abedinia et al 2016 introduced the shark optimization algorithm soa the algorithm is widely used for different optimization problems such as optimizing dams and reservoirs ehteram et al 2019a b cloud jobs scheduling suliman et al 2019 and determining optimal parameters of fuel cells han et al 2019 the sa uses a shark s distinct smelling ability for finding prey a random population of sharks is initialized as the initial position of sharks ehteram et al 2019a b the fitness function of each position shows its closeness to the prey the position and velocity of sharks are updated by receiving the density of the odour particles the velocity of particles is updated using eq 31 ehteram et al 2019b 31 v i j k min μ k r 1 of x j x i j k α k r 2 v i j k 1 β k v i j k 1 where β k the velocity limiter r 1 random value o α k a coefficient between zero and one r 2 a random value of objective function μ k a factor between zero and one v i j k 1 the current velocity of ith shark in the jth dimension and k number of stages the new position for sharks is computed as follows 32 y i k 1 y i k v i k δ t k where y i k 1 shark s new location y i k current shark location and δ t k the time step the sharks use the rotational movement for local search as follows 33 z i k 1 m y i k 1 r 3 y i k 1 where z i k 1 m the position of the shark after rotational movement and m number of points during the local search for a minimization problem the position of sharks is computed as follows 34 y i k 1 arg min o f y i k 1 o f z i k 1 1 o f z i k 1 m fig 8 shows the flowchart of sa 2 4 5 particle swarm optimization pso the pso is widely used for various optimization problems such as predicting compressive strength qi et al 2018 building energy performance marini and walczak 2015 and image contrast enhancement paul et al 2019 the disadvantages of pso are that it may fall into a local optimum in this regard the velocity and position of particles are computed as follows 35 v new ω v c 1 r 1 p best x c 2 r 2 g best x 36 x new x v new where v new the new velocity of particles ω inertia coefficient r 2 random number c 1 acceleration coefficient p best best personal position g best most appropriate global position x new the new position of particles and v new the new velocity of particles 2 4 6 gravitational search algorithm gsa rashedi et al 2009 introduced the gravitational search algorithm gsa the gsa has the advantages of easy implementation and low computational cost the objectives with masses are considered as the agents in the gsa agents use gravitational force to attract each other the gsa is an optimization algorithm inspired by the theory of newtonian gravity in physics the process of gsa is described as follows 1 a gravitational constant is initialized at the beginning of the algorithm equation 37 37 g t g 0 e κ t t where g 0 initial value of the gravitational coefficient t total number of iterations t number of iterations and κ the acceleration coefficient 2 the gsa starts with some initial solutions initial population and tries to enhance performance toward some best solutions equation 38 as follows 38 x i x i 1 x i n where xi n the position of the ith object in the nth dimension and x i the position of the ith object in the search space 3 in gsa each mass has two types of mas i gravitational mass and ii inertia mass assuming the equality of the gravitational and inertial mass the mass is computed according to equations 39 and 40 the gravitational mass contains active gravitational mass ma and the passive gravitational mass mp the strength of the gravitational field due to the current mass is measured by the active gravitational mass the strength of the interaction between an object and the gravitational field is measured by the passive gravitational mass as follows 39 m ai m pi m ii m i m i t fi t i t w o r s t t b e s t t w o r s t t 40 m i t m i t j 1 n m j t where fi t i t the fitness value of the agent the active gravitational mass i m pi the passive gravitational mass i m i t the mass of each agent b e s t t w o r s t t the minimum and the maximum fitness values and m i t the mass j at iteration t respectively a more efficient agent shows a heavier mass 4 once the fitness function value of each individual in the population has been computed the algorithm process begins then the force acting on mass i from mass j is computed as follows 41 f ij k t g c t m i t m j t x i t x j t 2 ε x i k t x j k t where m i t the passive gravitational mass of agent i m j t the active gravitational mass of agent j g c t the gravitational constant ε small constant x i t x j t the distance between two agents i and j euclidean distance x i k t the position of ith agent in the kth dimension and x j k t the position of jth agent in the kth dimension 5 the acceleration of the agent i at iteration t in the direction kth κ i k t is given as follows 42 κ i k t j i j i n rand f ij k t m i t 6 finally the position and velocity of agents are computed as follows 43 v i k t 1 r a n d v i k t κ i k t 44 x i k t 1 x i k t v i k t 1 where v i k t 1 the new velocity of kth dimension ith agent at iteration t 1 and x i k t 1 the new position of kth dimension at iteration t 1 fig 9 shows the flowchart of the gsa 2 5 hybrid anfis and optimization algorithms the anfis model has the premise and consequent parameters the values of consequent and premise parameters have a great influence on the outputs of the anfis model in this study the optimization algorithms are used to fine tune the anfis parameters the optimization algorithms are preferred to estimate the anfis parameters because the classical training algorithms frequently arrive at suboptimal solutions in the determination of anfis parameters the first level in the hybrid anfis and optimization algorithms classifies data into training data and testing data then the second level is to train the anfis based on the available training data in the next level the premise and consequent parameters are inserted into the optimization algorithms as the initial population the consequent and premise parameters are considered as decision variables the position of the agents of the optimization algorithms indicates the value of the premise and consequent parameters then the fitness function for each agent is computed the root mean square error rmse is widely used as an objective function the computation of the fitness function shows the quality of solutions next the operator of optimization algorithms is applied to the agents to update their values finally the stop criterion is checked if the stop criterion is satisfied the simulation and optimization process finishes otherwise the simulation process repeats 2 6 hybrid ann and optimization algorithms the overall accuracy of the ann models is heavily dependent on the weight and bias values thus it is crucial to fine tune the ann parameters the model procedure typically initiates with the colocation of random agents i e particles wolves cats sharks moths and objectives in the next level the position of agents shows the ann parameters bias and weight values in the level after that having the initial position of agents the hybrid ann optimization models are trained the subsequent stage investigates the convergence of the trained model the error function rmse is used to calculate the error between the observed data and the estimated data the calculated error is reduced through updating positions of the agent for each stage the new error is calculated which is expected to be lower than the previous stage 2 7 hybrid svms and optimization algorithms a major problem encountered in establishing the svms model is in the selection of its parameters the parameters c and γ have the key roles in the svms model first the testing and training data are randomly selected next the initial parameters of the optimization algorithms are randomly initialized the svms parameters are initialized the optimization algorithms are adopted to find the optimal solution of agents i e value of c and γ parameters in the search space next the training data are put into the svms model with the optimal parameters to obtain the trained svms optimization model finally the testing data are used to investigate the prediction ability of the trained svm optimization model 2 8 principal component analysis pca in this study principal component analysis was used to select the optimal input combination for predicting tds the pca is a statistical method that transforms a number of correlated variables into a number of uncorrelated variables referred to as principal components lu et al 2019 the principal components are used by the pca to identify the input data that have the highest effect on the tds output additionally pca is used to reduce the number of input data some inputs are eliminated from the original input data while the most salient parameters are detected by the pca zhao et al 2019 first the standardization of input data is performed then the correlation matrix is computed finally the eigenvalues and vectors are computed as follows 45 r λ i i p 0 where r correlation matrix λ i eigenvalues and ip unit matrix the first principal method has the highest eigenvalues in this study the following indexes are used to evaluate the accuracy of hybrid models maroufpoor et al 2019 46 rmse 1 n i 1 n y i y i 2 47 mae 1 n i 1 n y i y i 48 nse 1 t 1 t y i y i 2 t 1 t y i y i 2 49 pbias i 1 n y i y i 100 i 1 n y i where rmse root mean square error mae mean absolute error nse nash sutcliff efficiency y i simulated data and y observed data the lower values of rmse nse and pbias show better accuracy of models maroufpoor et al 2019 the higher value of nse shows more accuracy of models maroufpoor et al 2019 one of the most important steps in developing the model is the procedure taken to split the data for training and testing the training data would be used to identify the model structure and the value of the internal parameters and hence the testing data is used to evaluate the performance of the model which in this case could be considered as unseen data for the model this step usually is carried out using trial and error procedure in order to assure that the model could achieve the optimal performance in fact the most important reason for the successfulness of the model is that the training data should include most of the possible patterns that could be occurred in the whole data and hence the model could achieve accurate forecasting during testing unfortunately there is no particular procedure could be used to identify the best splitting percentage therefore in this study different splitting percentages have been examined the data splitting percentages are 80 70 and 60 for training and consequently 20 30 and 40 for the testing however there is a need to evaluate the performance to attain which one of these data splitting percentages is the optimal one therefore sensitivity analysis within each data splitting percentage has been carried out utilizing root mean square error rmse as an objective function to evaluate the error progressing it should be noted here that other data splitting percentage could achieve better results however as long as one of the examined data splitting percentages has been proved to satisfactory achieve acceptable objective function it could be considered as a stopping criterion for the trial and error procedure it should be noticed here that it is not necessary that if certain data splitting percentage has been successful for a particular data pattern it could be applied for other data pattern in fact this procedure for the best selection of the data splitting should be carried out when developing any similar model and applied for other data at different case studies 3 case study the study area is situated in the yazd province in iran fig 10 which extends from the longitude of 54 20 to 55 05 e to the latitude of 31 15 to 32 40 n the area of the basin is 2 491 km2 here the data related to 12 observed wells in yazd plain recorded between 2002 and 2015 are used to predict the tds value table 1 provides the statistics of water quality data used in this study according to this table among all input parameters sodium na 63 97 mg lit train and 63 57 mg lit test stood at the maximum value of concentration whereas the minimum value 3 75 mg lit and 3 72 mg lit was attributed to calcium ca the standard deviation value of na was spread over a wider range of values in comparison to other inputs the standard deviation value for ca indicates that the ca values were closer to its mean different sizes of data for the training and testing level are selected about 80 of training data and of testing 20 data account for the lowest error function rmse as mentioned above a sensitivity analysis is required to identify the best size of the data used for training and testing in this study three sizes were selected as observed in fig 11 as it could be depicted from fig 11 three different sizes of data splitting percentage have been carried out 60 70 and 80 for training and 40 30 20 for testing in order to evaluate the performance of each data splitting percentage the objective function value rmse has computed for different sizes in addition fig 11 illustrates the value of the attained objective function value for different sizes and models it could be observed that the objective function value experienced the minimal value for all models when using data splitting percentage equal to 80 for training and 20 for testing as a result all the models have been developed based on this optimal data splitting percentage in order to evaluate and analyze the performance of all proposed models within this research this study attempts to contribute to the analysis of tds data by conducting a trend analysis using a graphical method the trend analysis shows the distribution of the tds values for better monitoring of the water quality of the basin thus if the tds values have significant increasing trends the decision makers implement policies to control the tds values the application of ita includes changes observed in trends in tds it is crucial to identify possible trends to better understand the temporal changes in tds the knowledge of temporal changes in tds is of particular importance in the optimal management of water resources during recent years the increasing demand for monitoring water quality of aquifers has been observed in the ita method the time series is first divided into two equal groups the first half of tds time series is placed on the horizontal axis while the second half is placed on the vertical axis şen 2012 this technique identifies the trends of low medium and high tds data in fig 12 a graphical representation of the innovative trend method ita is observed if data are observed on a 45 line there is no trend in the time series if the data are collected on the upper triangular area of the 45 line an increasing trend is observed in the time series i e tds value is increasing if the data are accumulated in the lower triangular area of the 45 line there is a decreasing trend in the time series i e tds value is decreasing 4 results and discussion in this section the results are listed as follows 1 first the results of the taguchi model are listed the optimal values of random parameters are selected using the taguchi model 2 the results of the pca are listed to select the appropriate inputs to the models 3 the results of statistical indexes are presented to select the best modes 4 the scatterplots are used to evaluate the accuracy of the models 5 an uncertainty analysis is used to quantify the uncertainty of the models 6 finally innovative analysis is used to detect the tds trends 4 1 selection of random parameters of optimization algorithms when the optimization algorithms are applied to the optimization problems at hand the effect of the random parameters on the performance of the algorithms should be considered thus it is essential to determine the optimal values of the random parameters zhang and wang 2019 in this study a robust taguchi model was used to find the appropriate values of the random parameters of the optimization algorithms the taguchi model involves the detection of proper random parameters to obtain the optimum results of the process zhang and wang 2019 the orthogonal arrays were used by the taguchi method the minimum number of designed experiments is computed as follows 50 n taguchi 1 l i 1 n v where n taguchi minimum number of required experiments for the taguchi model l number of levels and nv number of parameters table 2 shows the level of each parameter the taguchi model determines the relative value of each parameter with respect to its main effect on the performance of the optimization algorithm when the taguchi model investigates the impact of a specific parameter on the performance of the optimization algorithms other parameters of the optimization algorithm are kept fixed the signal to noise s n ratio is used to determine the appropriate values of random parameters as follows 51 s n r a t i o 10 log 10 o b j e c t i v e function 2 the signal to noise ratio is computed with respect to the variation of the value of a parameter the levels are also categorized according to the range of changeability of the random parameters the mean of s n ratio of each parameter at a defined level is computed the detailed results are observed in table 2 the maximum value of the s n ratio shows the optimal values of the random parameters the optimal values of population size and the maximum number of iterations of mfo are 200 s n ratio 1 27 and 300 s n ratio 1 25 respectively regarding similar interpretation the optimal values of random parameters are obtained 4 2 results of pca the orthogonal transformation is used by the pca model to obtain the principal components pca reduces the number of data without using much information the pca was used because a smaller data set can be easily analysed first the input data are standardized then the correlation matrix is formed with a dimension of 6 6 equivalent to the number of input data table 3 shows the results of the pca model the results indicate that the first three pcs included 87 of the input data variance in addition the hco3 ca and mg had the most impact on the first three pcs thus the first three pcs were selected for the current article 4 3 the analysis results for the soft computing models table 4 shows the results of the soft computing models in the training level the rmse mae pbias and nse calculated for the hybrid anfis ann and svm models were significantly different from those of the standalone svm anfis and ann models among the soft computing models the anfis mfo indicated the highest accuracy in the training level rmse 2 25 mae 2 21 nse 0 94 and pbias 0 12 among the hybrid soft computing models the rmse mae and pbias of the svm pso had the highest value compared to those of other hybrid models rmse 2 69 mae 2 67 pbias 0 32 the results indicate the superiority of the mfo followed by cso in comparison to pso gsa sa and gwo the outputs indicate the superiority of the anfis model followed by ann in comparison to the svm model regarding nse mae pbias and rmse the svm mfo was found to be the best model compared to the svm gwo svm cso svm pso and svm gsa models according to the findings from table 4 the results indicate that the ann mfo had better performance than the ann pso ann cso ann gsa ann sa ann gwo and ann models table 5 indicates the results for the soft computing models in the testing level table 5 shows that none of the standalone anfis ann and svm models obtain better performance than the hybrid anfis svm and ann models in quantitative analysis the hybrid anfis mfo reduced the rmse over the ann mfo and svm mfo models by 1 4 and 3 8 respectively it was observed that the svm model had the least nse value among the models a detailed comparison of models indicates that the ann mfo shows the highest nse value and lowest mae rmse and pbias values among the ann models however the highest values of rmse mae and pbias were observed for the svms model the results indicate the superiority of the mfo followed by cso in comparison to pso gsa sa and gwo a separate analysis was used to determine the cpu time taken by each of the soft computing models to predict the tds values the training time of the models indicate that the performance of the anfis mfo is three and four seconds faster than the ann mfo and svm mfo respectively the testing results of models indicate that the anfis ann and svms models take 152 157 and 158 s respectively the results reveal that the svms model is the slowest model among all the models in fig 13 and fig 14 the scatterplots were depicted for different soft computing models the results reveal that the anfis mfo indicted the highest r2 among all the models a detailed comparison of the values of r2 indicates that the standalone ann anfis and svms models had worse performance than the hybrid ann anfis and svm models according to the findings from fig 13 and fig 14 the results indicate that the pso obtained worse performance than the mfo sa gwo gsa and cso there are different sources of uncertainty for soft computing models such as models parameters and input data gholami et al 2019 when the classical training algorithms with low accuracy are used to train the soft computing models the uncertainty of model parameters increases the current study uses optimization algorithms as an alternative to classical training algorithms to decrease the uncertainty of model parameters the input data was among the most important sources of uncertainty of the soft computing models in this study the uncertainty of input data was regarded thus it is necessary to quantify the uncertainty of soft computing models gholami et al 2019 in this study the sequential uncertainty fitting method sufi 2 was used to quantify the uncertainty of the soft computing models to apply the sufi 2 algorithm to the soft computing models the rmse was selected as an objective function for minimizing error values then the input parameter ranges were defined next the latin hypercube methods were used as the sampling method the latin hypercube sampling lhs is a way of providing random samples of input parameters a cube with more than three dimensions is known as a hypercube the lhs is used to sample from the multiple dimensions the generated samples were used as an input to the soft computing models the percentage of observed data bracketed by 95 prediction uncertainty 95ppu was computed at the 2 5 and 97 5 levels of the cumulative distribution of the tds variable the 95ppu denotes as p parameter the r parameter is used as another parameter to quantity to the uncertainty results the average thickness of the 95ppu band divided by the standard deviation of the data is used to compute the r index the best model has the highest p and the lowest r values table 6 a shows the uncertainty results for the training level considering the obtained results of soft computing models in the table 6a the anfis mfo achieved the lowest and highest values of d and p respectively the results indicate that the standalone ann anfis and svms models had higher d than the hybrid ann anfis and svm models table 6a also confirmed that the mfo outperformed the other optimization algorithms table 6b showed the testing results of the uncertainty analysis of soft computing models the results indicate that the svms had the highest r and the lowest p among all the models the results indicate that the ann mfo had higher p and lower r than the ann pso ann wgo ann gsa and ann cso as observed in table 6b the results indicate that the mfo outperformed the other optimization algorithms the trends in tds value using ita are provided in fig 15 fig 15a illustrates the ita results for observed data the tds value 1800 mg lit indicates a decreasing trend while the medium tds values 2000 mg lit tds 2800 mg lit did not have a significant trend the high tds values tds 3000 mg lit indicate an increasing trend fig 15b shows the ita results for anfis mfo the low tds values tds 1800 indicate a decreasing trend in the medium cluster 2000 mg lit tds 2800 mg lit there is almost no trend in the high cluster tds 3000 mg lit the results indicate an increasing trend regarding the similar interoperation the ita results were analyzed for outputs of ann mfo and svm mfo models the high tds values tds 3000 mg lit of svm mfo indicate no significant trend similarly the medium tds values of svm mfo and ann mfo models 2000 mg lit tds 2800 mg lit indicate no significant trend 5 conclusion aquifers are among the largest freshwater resources in this research prominent soft computing models were used to estimate tds value in the yazd plain the anfis ann and svm were used for predicting tds although the anfis svm and ann are widely used for predicting the hydrological variables they require powerful training algorithms for finding optimal values of model parameters in this study the mfo cso gwo gsa pso and sa were used to train the ann anfis and svm models among the soft computing models the anfis mfo indicate the highest accuracy in the training level rmse 2 25 mae 2 21 nse 0 94 and pbias 0 12 the results indicate that the anfis mfo has the highest r2 among all the models a detailed comparison of the values of r2 indicates that the standalone ann anfis and svm models had worse performance than the hybrid ann anfis and svm models the results indicate that the svms had the highest r and the lowest p among all the models the trend of observed data outputs of the anfis mfo ann mfo and svm mfo were identified using the ita method the low tds value for observed data tds 1800 mg lit indicates a decreasing trend while the medium tds values 2000 mg lit tds 2800 mg lit did not have a significant trend the high tds values for observed data tds 3000 mg lit indicate an increasing trend the quality of gw is important because it determines the suitability of water for drinking irrigation and domestic purposes thus predictive models can be used by decision makers and policymakers future research can employ the multi objective mfo to further enhance the accuracy of the outputs in fact two objective functions can be defined for the mfo to determine the appropriate inputs and optimal parameters of the soft computing models if the multiobjective optimization algorithms are used to develop the soft computing models it may not be required to use the preprocessing methods such as pca to select the best inputs separately these developed models are also used for agro environmental policies because they can determine the effect of agriculture on some key parameters in gw chemistry credit authorship contribution statement fatemeh barzegari banadkooki writing original draft writing review editing mohammad ehteram conceptualization methodology software fatemeh panahi writing original draft writing review editing saad sh sammen writing original draft faridah binti othman writing original draft writing review editing ahmed el shafie conceptualization methodology software declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements the authors appreciate so much the facilities support by the civil engineering department faculty of engineering university of malaya malaysia and the financial support received from research grant coded gpf082a 2018 and gpf070a 2018 funded by the university of malaya appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j jhydrol 2020 124989 appendix a supplementary data the following are the supplementary data to this article supplementary data 1 
5440,the overall quality of groundwater gw is important primarily because it determines the suitability of water for drinking irrigation and domestic purposes in this study the adaptive fuzzy interface system anfis support vector machines svms and artificial neural network ann models were employed for predicting the total dissolved solids of aquifers the moth flam optimization cat swarm optimization cso particle swarm optimization pso shark algorithm sa grey wolf optimization gwo and gravitational search algorithm gsa were used to train the anfis svm and ann models the data were collected from yazd plain iran to predict the total dissolved solids tds the principal component analysis was used to determine the most appropriate inputs for predicting tds the hybrid anfis mfo improved the accuracy of rmse roo mean square error over the ann mfo and svm mfo models by 1 4 and 3 8 respectively it was also observed that the svm model had the least nse nash sutcliffe efficiency value among all the models unlike the standalone anfis the multilayer perceptron mlp and svms models the hybrid anfis ann and svm demonstrated high accuracy in the training and testing phase so that in the optimal hybrid model anfis mfo values of mean absolute error mae nash sutcliff efficiency nse and percent bias pbias were 2 21 mg lit 0 94 0 15 2 981 mg lit 0 93 and 0 18 respectively the anfis mfo was also seen to further enhance the rmse by approximately 3 and 7 as compared to the ann mfo and svm mfo this study also aims to investigate the temporal variability tds using innovative trend analysis ita the tds value of 1800 mg lit indicates a decreasing trend while a medium tds value 2000 mg lit tds 2800 mg lit does not have a significant trend the high tds values tds 3000 mg lit indicate an increasing trend in this study the anfis mfo and anfis cso models showed superior performance over the other models hence indicates significant implication in their application for other water resources and hydrological variables keywords total dissolved solids artificial intelligence model optimization algorithms groundwater quality 1 introduction groundwater gw among the most important natural water resources has many uses in both agriculture and domestic consumption banadkooki et al 2020 ehteram et al 2019a the overall gw quality is evaluated allowing for the gw to be used in an optimal manner banadkooki et al 2019 decision makers can manage gw resources in a more beneficial manner if they have a clear knowledge of the overall quality and quantity of the gw asadollahfardi et al 2011 a number of researchers have investigated the gw of aquifers in order to evaluate its appropriateness for various uses asadollahfardi et al 2018 the water demand is constantly increasing but freshwater is restricted therefore it is necessary to evaluate the gw quality not only for its utilization but also as a potential source of water for future consumption ehteram et al 2020 the quality of gw is important primarily due to the suitability of water for different aims yahya et al 2019 houatmia et al 2016 reported that the interaction between evaporation and rock water has a significant effect on the gw quality in the northeastern part of tunisia li et al 2016 observed that gw chemistry is affected by rock water interaction furthermore it has been reported that rock water interaction affects the quality of gw adimalla and venkatayogi 2017 thus finding an efficient way to evaluate the quality of gw is necessary for monitoring purposes a large number of studies have made an effort to evaluate a wide range of water quality parameters by means of numerical stochastic and deterministic models the gw quality is often described by total dissolved solids tds studies on the accurate estimation of tds is essential for the protection of freshwater gw becomes saline at a high tds concentration gw with a high tds concentration is generally regarded as problematic for agriculture and irrigation additionally modelling tds concentration and consequently predicting it is essential for hydrological and agricultural management studies kisi et al 2013 deterministic and numerical models have been widely applied for water quality modelling zounemat kermani et al 2019 previous works in the literature have reported that numerical and deterministic models might often provide inaccurate results due to the complicated hydraulic condition of aquifers in recent years soft computing models have been widely used for predicting hydrological variables the artificial neural network ann model adaptive neuro fuzzy interface system anfis support vector machines svms model genetic programming gp gene expression programming gp and decision tree model dtm have been widely used for predicting rainfall yaseen et al 2018 chang and tsai 2016 evaporation piri et al 2019 adnan et al 2019 malik et al 2018 runoff kumar et al 2019 ashrafi et al 2019 as well as drought kisi et al 2019a b malik et al 2019 fung et al 2019 the anfis and ann models have some advantages such as high adaption capability and rapid learning capacity asadollahfardi et al 2011 applied two different kinds of ann models to predict tds the results demonstrate that the accuracy of the recurrent neural network was better than the multilayer perceptron mlp model kisi and ay 2012 used the mlp radial basis function neural network rbfnn and multilinear regression model to evaluate dissolved oxygen do their results indicate that the mlp model had an acceptable performance compared to rbfnn and multilayer regression mlr verma and singh 2013 used an ann model to predict biochemical oxygen demand bdo using do ph and total suspended solids tss as the model s input the output reveals that the ann model had an acceptable accuracy in the prediction of ann ghavidel and montaseri 2014 employed the ann genetic express programming model gep and anfis model to predict tds parameters for a basin in iran they found that the gep model presented more accurate predictions than the anfis and ann models rajaee and shahabi 2016 compared wavelet ann and wavelet genetic programming in the prediction of nitrogen concentration the results demonstrate that the hybrid ann and genetic programming exhibited better performance compared to the standalone ann and genetic programming models arabgol et al 2016 used a svms model to estimate nitrate concentration in gw in arak plain their findings indicate that the svm model estimated nitrate concentration in the training and testing phase with a reasonably high correlation isazadeh et al 2017 used svms and ann models to simulate the spatial variability of the gw qualitative parameters the results reveal that the svm model had less uncertainty than the ann model asadollahfardi et al 2018 used mlp and time series to estimate the tds in a basin the calcium chloride and bicarbonate were used as an input to the models the results indicate that the mlp was more reliable than the time series model najafzadeh et al 2019 used gene express programming tree model and evolutionary polynomial regression model to estimate chemical oxygen demand their findings indicate the superiority of the evolutionary polynomial regression model compared to the gene express programming and tree models zounema kermani et al 2019 used an ann model for predicting nitrogen concentration the results indicate that the mlp model was superior in the prediction of nitrogen concentration over other models jafari et al 2019 predicted tds of the gw aquifer in the tabriz plain the anfis svm and mlp models were used to predict tds according to results the anfis mlp and svm model could be employed successfully in estimating tds aryafar et al 2019 used ann anfis and genetic programming to estimate ec and tds the anfis and ann produced accurate results for the estimation of the water quality parameters in another study several optimization algorithms including particle swarm optimization genetic algorithm and ant colony optimization were employed to estimate gw quality variables kisi et al 2019a b the results revealed that the all of the considered optimization algorithms enhanced the accuracy of the standalone anfis model in the modelling of ec and tds another study applied ann and multiple linear regression mlr to estimate the fitness of gw quality kadam et al 2019 the results reveal that predictions of ann were satisfactory for the prediction of water quality index maroufpoor et al 2019 used the anfis and ann models for predicting the spatial distribution of gw electrical conductivity ec the ann model revealed the most accurate outputs with the lowest rmse and mae although soft computing models have been successfully applied in the prediction of water quality parameters they have some weaknesses the ann svm anfis and knn models have unknown parameters the ann svm and anfis parameters have a significant influence on the accuracy of the models the literature review commonly uses classical training algorithms for training ann svm and anfis models however a reoccuring issue is that these algorithms may be trapped in a local optimum thus robust algorithms such as optimization algorithms can be used to fine tune the ann svm and anfis parameters among the relatively new optimization algorithms is the moth flame optimization mfo algorithm mirjalili 2015 the mfo mimics the behaviour of moths for navigating during the night the moths keep a fixed angle to the moon when flying the literature review has proved that this algorithm can potentially lead to a faster and robust method thus the mfo was selected for the current study due to the mentioned advantages in this study mfo was used to fine tune the ann svm and anfis parameters for predicting tds additionally the anfis cso gwo pso sa and gsa svms cso gwo pso sa and gsa and ann cso gwo pso sa and gsa models were used in an empirical comparison with three new hybrid models ann mfo anfis mfo and svm mfo for predicting tds the structure of this paper laid out as follows 1 section 2 explains the structure of the anfis ann and svm models afterwards the structure of the optimization algorithm is explained this is followed by a demonstration of how the anfis svm and ann models are combined with the optimization algorithms 2 secondly the principal component analysis pca method is demonstrated to select the best inputs to the anfis svm and ann models 3 thirdly the statistical indexes such as root mean square error rmse mean absolute error mae nash sutcliff efficiency nse and perecent bias pbias are used to evaluate the ability of the models an uncertainty analysis is also performed to have a more comprehensive comparison of difficult soft computing models 4 finally innovative trend analysis ita is used to determine the temporal changes of the tds series during the 2002 2015 period 2 method 2 1 anfis model the anfis model is widely applied for predicting hydrological variables and water quality parameters yaseen et al 2018 the anfis model contains an adaptable takagaki sugeno fuzzy model structurally the anfis model comprises five layers each of these layers is used for different purposes a classical anfis model has two inputs m n and a single output each layer consists of a number of neurons mekanik et al 2016 two fuzzy if then rules are used as follows if m is a1 and n is b1 then f1 p1m q1n r1 rule 1 if m is a2 and n is b2 then f2 p2m q2n r2 rule 2 where a1 a2 b1 and b2 membership functions for inputs m and n and p1 q1 r1 p2 q2 and r2 the output function parameters layer 1 there exist adaptive nodes in the first layer the fuzzy membership functions are the output of this layer 1 o 1 i μ a i x 2 o 1 i μ b i y where and y input nodes a and b linguistic labels and b i y symbolize the mfs membership fuzzy function layer 2 the final output of this layer o2 i shows the firing strength of its corresponding fuzzy rule as follows 3 o 2 i ω i μ a i x μ b i y where ω i the firing strength of rule and o 2 i the output of the second layer layer 3 each neuron in the third layer is shown by a circle and is labelled as n the ith rule s firing strength to the sum of all rules firing strength is computed by this layer as follows 4 o 3 i ω i ω i ω i where ω i the normalized firing strength and o 3 i the output of the third layer layer 4 every node in the fourth layer corresponds to adaptive processing units as follows 5 o 4 i ω i f i ω p i x q i y r where p i q i and r consequent parameters and o 4 i the output of the fourth layer layer 5 the overall output is computed as the sum of all incoming signals as follows 6 o i 5 i ω i f i i ω i f i i ω i where o i 5 the output of the fifth layer fig 1 shows the anfis structure the following membership function is used for this study 7 μ x e x c σ 2 where c and σ premise parameters the backpropagation gradient descent bgd is widely used to train the anfis models the bgd may be trapped in a local optimum in this study 25 optimization algorithms were used to fine tune the consequent and premise parameter 2 2 multilayer perceptron mlp model the mlp model is widely used for predicting various parameters such as meteorological hydrological and water quality parameters mehr and nourani 2017 the mlp models are a class under the ann model the mlp model consists of three layers each of these layers consists of several processing units the weighted connections connect each unit to the units in the next layers acharya et al 2019 the first layer receives the input neurons the second layer is known as the middle layer or hidden layer the third layer is the output layer to compute the output of the jth neuron in a middle layer all the inputs are entered into the jth neurons to be multiplied by the corresponding connection weight to jth neuron yaseen et al 2019 8 h j f i 1 n x i w ij b where f activation function b bias parameter w ij the weight of connection h j the output of jth neuron and xi the received input from the ith neuron the backpropagation algorithm is widely used for training the ann models the backpropagation algorithm may not have a fast convergence rate thus the optimization algorithms can be used to fine tune the ann parameters such as weight and bias parameters fig 2 shows the structure of the mlp model fig 3 2 3 svms model the svms model is widely used for classification and regression analysis this model uses the following equation to show the relationship between input and outputs pham et al 2020 9 f x η t ϕ x b where f x the output value η weighting coefficient of input data b bias of η t t transpose symbol and ϕ x the nonlinear mapping function the svms attempts to minimize the difference between measured data and estimated data samantaray et al 2020 thus it is used to minimize an optimization problem whose objective function is decreasing the error function the parameters η and b are computed as follows samantaray et al 2020 10 mi n η b ξ ξ 1 2 η η t c i 1 n ξ i ξ i 11 y i η t ϕ x i b ε ξ i y i η t ϕ x i b ε ξ i ξ i ξ i i 1 2 n where r ε the empirical risk c regulation parameter ξ i the errors above ε and ξ i the errors below ε finally the following equation is used for the svms model samantaray et al 2020 12 f x i 1 n β i β l k x i x j b where β i and β i the lagrangian coefficients and k x i x j the kernel function the radial basis function is widely used as a kernel function as follows 13 k x i x j exp x i x j 2 2 γ 2 where γ the gaussian parameter the parameters c and γ have a significant effect on the accuracy of the svms model in this study the optimization algorithms were used to find the optimal value of the svms parameters 2 4 optimization algorithms this section explains the considered optimization algorithms additionally the advantages and disadvantages of the optimization algorithms are demonstrated then the mathematical model of each of the algorithms is explained 2 4 1 moth flame optimization algorithm mirjalili 2015 introduced the moth flame optimization algorithm mfoa the mfoa is widely used for different optimization problems including the optimization of water resource systems li et al 2018 solving numerical optimization problems khalilpourazari and khalilpourazary 2019 data clustering shah et al 2018 and thresholding image segmentation el aziz et al 2017 the behaviour of moths for navigating during the night is simulated by the mfo the moths fly based on a mechanism referred to as transverse orientation the moths keep a fixed angle to the moon when flying as shown in fig 4 a this mechanism helps moths to fly in a straight line in the mfo the candidate solutions are the moths and the decision variables are the position of the moths mfo has high flexibility and robustness the disadvantage of mfo is that it involves a complicated implementation each moth flame is regarded as a value of d dimensional vectors as a candidate solution thus the position of moths is defined in a matrix as follows mirjalili 2015 14 m m 1 1 m 1 2 m 1 d m n 1 m n 2 m nd where n the number of moths d the number of dimensions mnd the position of the nth moth in the dth dimension and m the population of search agents the fitness values of moth flames are saved as follows mirjalili 2015 15 om o m 1 o m 2 o m n where o m n the fitness value of nth moth s position and om the fitness value of the moths the set of corresponding objective function values is saved as follows 16 f f 1 1 f 1 2 f 1 d f n 1 f n 2 f nd where fnd the position of nth moth in the dth dimension the fitness value of flames is shown as follows 17 of o f 1 o f 2 o f n where o f n the fitness value of nth flame s position in mfo moths search for solutions in the search space and the flames are the best solutions found by each moth thus the flames are regarded as flags in the search space a moth searches for better solutions around a flame three functions are used by the mfo to initialize the random position of moths i move moths in the the search pace p space p m m and finish the search space t the function i is considered as follows 18 m i j u b i l b j r a n d l b j where u b i upper bound of decision variable l b j lower bound of the decision variable rand random number and m i j the position of the ith moth with respect to the jth flame there are two points about a logarithmic spiral 1 spiral s initial location should start from the moth 2 spiral s final location should be the position of the flame the transverse orientation is used by the moth to search around the flames thus the p function is shown as follows mirjalili 2015 19 m i s m i f j d ij e bt cos 2 π t f j 20 d ij f j m j where b a constant to define the logarithmic shape spiral t random number f i the position of ith flame mj the position of jth moth and d ij the distance between the ith flame and jth moth the spiral motion of moths around the flames adjusts the balancing between exploration and exploitation ability finally the following equation is used to enhance the exploitation of the mfo 21 f l a m e number r o u n d n i n i t where i current number of iterations t maximum number of iterations and n maximum number of flames fig 4 shows the flowchart of the mfo 2 4 2 cat swarm optimization cso cat swarm optimization cso was initially introduced by chu et al 2006 the algorithm has been extensively utilized for different applications such as solving global optimization problems kumar and singh 2018 robotics problems karpenko and leshchev 2019 the manufacturing design cell problems soto et al 2019 and cloud computing gabi et al 2018 the cso mimics the developed behaviour of cats each cat shows a solution the cso contains two different processing modes namely tracing and seeking modes the cats in the seeking mode rest and keep eyes on their surroundings the tracing model simulates the cat chasing prey when the cats finds the prey they determine the speed and direction of their movement based on the prey s location the seeking mode consists of four parameters all for the seeking process which are i seeking memory pool smp ii seeking the range of the selected dimension srd iii counts of dimension to change cdc and iiii self position consideration spc the number of copies provided in the seeking mode is shown by smp therefore the difference between the new and the old values in the dimension space are investigated and the maximum difference is selected as the mutation hence referred to as the srd parameter the spc parameter shows the current position of cats chu et al 2006 finally the cdc expresses how many dimensions should be mutated the seeking mode consists of the following levels 1 provide j copies of the current location cat position k with j smp if the spc value is true then keep it as a possible solution 2 provide a random value for srd 3 for the new positions compute the objective function if the values of the objective function are identical then re adjust for all solutions a probability value equal to 1 otherwise recalculate the seeking probability of each solution chu et al 2006 as follows 22 p i f s i f s b f s max f s min where p i the probability of current candidate solutions f s i the objective function value of ith cat f s max maximum value of the objective function f s min minimum value of the objective function f s b f s max for maximization problem and f s b f s min for minimization problem 4 carryout mutation and replace the position of cats the tracing mode indicates cat with a quick movement the following equation is used to update the velocity of cats chu et al 2006 23 v k d v k d r 1 c 1 x best d x k d where v k d the new velocity of cats v k d the velocity of the kth cat r 1 random number c 1 a constant x best d the location of the optimal solution and x k d kth location of cat finally the new position of cats is updated as follows chu et al 2006 24 x k d n e w x k d o l d v k d where x k d n e w the new position of kth cat and x k d o l d the old position of kth cat the whole procedure of the cso is presented in the flowchart in fig 5 2 4 3 grey wolf among the robust optimization algorithms is the grey wolf optimization gwo algorithm gwo is widely used for various optimization fields such as the prediction of friction capacity of driven piles moayedi et al 2019 feature selection for big data manoj et al 2019 classification of kidney images raju et al 2018 predicting compressive strength golafshani et al 2020 and monthly streamflow forecasting tikhamarine et al 2019 the gwo mimics the exploring behaviour of grey wolves the grey wolves strictly follow a collective hierarchy the grey wolves in the leader group are referred to as alpha wolves a couple of males and females in the leader group make all the important decisions the other groups beta wolves at the subsequent level assist the grey wolves in the leader group the beta wolves are used to adjust the flock while the alpha wolves may die the older alpha wolves are substituted by the beta wolves the other group of wolves is referred to as delta δ they engage in the hunting process finally the last group of wolves is referred to as omega w the omega group plays the role of babysitters fig 6 shows the diagram for gwo the advantages of gwo are fast seeking speed and high accuracy fig 7 shows the flowchart of the gwo in order to mathematically simulate the social behaviour of wolves when designing gwo the fittest solution is considered as the alpha consequently the second and third best solutions are called beta and delta respectively the remainder of the solutions are hypothesized as omega a prey is encircled by the wolves during the hunt in order to mathematically simulate the encircling behaviour the following equations are suggested moayedi et al 2019 25 d c x p x t 26 x t 1 x p t b d where t the current iteration x p t the location of prey and x t 1 the position of wolves at iteration t 1 the parameters b and c are computed as follows moayedi et al 2019 27 b 2 a r 1 a c 2 r 2 where r1 and r2 random values and a a decreasing coefficient from 2 to 0 assuming the alpha beta and delta wolves are nearest to the prey the location of these kinds of wolves are used to estimate the prey s position the position of other agents is updated according to the location of the alpha beta and delta wolves moayedi et al 2019 as follows 28 d α c 1 x α x d β c 1 x β x d δ c 3 x δ x 29 x 1 x α b 1 d α x 2 x β b 2 d β x 3 x δ b 3 d δ 30 x t 1 x 1 x 2 x 3 3 where x α the location of alpha wolves x β the location of beta wolves x δ the location of delta wolves 2 4 4 shark algorithm sa abedinia et al 2016 introduced the shark optimization algorithm soa the algorithm is widely used for different optimization problems such as optimizing dams and reservoirs ehteram et al 2019a b cloud jobs scheduling suliman et al 2019 and determining optimal parameters of fuel cells han et al 2019 the sa uses a shark s distinct smelling ability for finding prey a random population of sharks is initialized as the initial position of sharks ehteram et al 2019a b the fitness function of each position shows its closeness to the prey the position and velocity of sharks are updated by receiving the density of the odour particles the velocity of particles is updated using eq 31 ehteram et al 2019b 31 v i j k min μ k r 1 of x j x i j k α k r 2 v i j k 1 β k v i j k 1 where β k the velocity limiter r 1 random value o α k a coefficient between zero and one r 2 a random value of objective function μ k a factor between zero and one v i j k 1 the current velocity of ith shark in the jth dimension and k number of stages the new position for sharks is computed as follows 32 y i k 1 y i k v i k δ t k where y i k 1 shark s new location y i k current shark location and δ t k the time step the sharks use the rotational movement for local search as follows 33 z i k 1 m y i k 1 r 3 y i k 1 where z i k 1 m the position of the shark after rotational movement and m number of points during the local search for a minimization problem the position of sharks is computed as follows 34 y i k 1 arg min o f y i k 1 o f z i k 1 1 o f z i k 1 m fig 8 shows the flowchart of sa 2 4 5 particle swarm optimization pso the pso is widely used for various optimization problems such as predicting compressive strength qi et al 2018 building energy performance marini and walczak 2015 and image contrast enhancement paul et al 2019 the disadvantages of pso are that it may fall into a local optimum in this regard the velocity and position of particles are computed as follows 35 v new ω v c 1 r 1 p best x c 2 r 2 g best x 36 x new x v new where v new the new velocity of particles ω inertia coefficient r 2 random number c 1 acceleration coefficient p best best personal position g best most appropriate global position x new the new position of particles and v new the new velocity of particles 2 4 6 gravitational search algorithm gsa rashedi et al 2009 introduced the gravitational search algorithm gsa the gsa has the advantages of easy implementation and low computational cost the objectives with masses are considered as the agents in the gsa agents use gravitational force to attract each other the gsa is an optimization algorithm inspired by the theory of newtonian gravity in physics the process of gsa is described as follows 1 a gravitational constant is initialized at the beginning of the algorithm equation 37 37 g t g 0 e κ t t where g 0 initial value of the gravitational coefficient t total number of iterations t number of iterations and κ the acceleration coefficient 2 the gsa starts with some initial solutions initial population and tries to enhance performance toward some best solutions equation 38 as follows 38 x i x i 1 x i n where xi n the position of the ith object in the nth dimension and x i the position of the ith object in the search space 3 in gsa each mass has two types of mas i gravitational mass and ii inertia mass assuming the equality of the gravitational and inertial mass the mass is computed according to equations 39 and 40 the gravitational mass contains active gravitational mass ma and the passive gravitational mass mp the strength of the gravitational field due to the current mass is measured by the active gravitational mass the strength of the interaction between an object and the gravitational field is measured by the passive gravitational mass as follows 39 m ai m pi m ii m i m i t fi t i t w o r s t t b e s t t w o r s t t 40 m i t m i t j 1 n m j t where fi t i t the fitness value of the agent the active gravitational mass i m pi the passive gravitational mass i m i t the mass of each agent b e s t t w o r s t t the minimum and the maximum fitness values and m i t the mass j at iteration t respectively a more efficient agent shows a heavier mass 4 once the fitness function value of each individual in the population has been computed the algorithm process begins then the force acting on mass i from mass j is computed as follows 41 f ij k t g c t m i t m j t x i t x j t 2 ε x i k t x j k t where m i t the passive gravitational mass of agent i m j t the active gravitational mass of agent j g c t the gravitational constant ε small constant x i t x j t the distance between two agents i and j euclidean distance x i k t the position of ith agent in the kth dimension and x j k t the position of jth agent in the kth dimension 5 the acceleration of the agent i at iteration t in the direction kth κ i k t is given as follows 42 κ i k t j i j i n rand f ij k t m i t 6 finally the position and velocity of agents are computed as follows 43 v i k t 1 r a n d v i k t κ i k t 44 x i k t 1 x i k t v i k t 1 where v i k t 1 the new velocity of kth dimension ith agent at iteration t 1 and x i k t 1 the new position of kth dimension at iteration t 1 fig 9 shows the flowchart of the gsa 2 5 hybrid anfis and optimization algorithms the anfis model has the premise and consequent parameters the values of consequent and premise parameters have a great influence on the outputs of the anfis model in this study the optimization algorithms are used to fine tune the anfis parameters the optimization algorithms are preferred to estimate the anfis parameters because the classical training algorithms frequently arrive at suboptimal solutions in the determination of anfis parameters the first level in the hybrid anfis and optimization algorithms classifies data into training data and testing data then the second level is to train the anfis based on the available training data in the next level the premise and consequent parameters are inserted into the optimization algorithms as the initial population the consequent and premise parameters are considered as decision variables the position of the agents of the optimization algorithms indicates the value of the premise and consequent parameters then the fitness function for each agent is computed the root mean square error rmse is widely used as an objective function the computation of the fitness function shows the quality of solutions next the operator of optimization algorithms is applied to the agents to update their values finally the stop criterion is checked if the stop criterion is satisfied the simulation and optimization process finishes otherwise the simulation process repeats 2 6 hybrid ann and optimization algorithms the overall accuracy of the ann models is heavily dependent on the weight and bias values thus it is crucial to fine tune the ann parameters the model procedure typically initiates with the colocation of random agents i e particles wolves cats sharks moths and objectives in the next level the position of agents shows the ann parameters bias and weight values in the level after that having the initial position of agents the hybrid ann optimization models are trained the subsequent stage investigates the convergence of the trained model the error function rmse is used to calculate the error between the observed data and the estimated data the calculated error is reduced through updating positions of the agent for each stage the new error is calculated which is expected to be lower than the previous stage 2 7 hybrid svms and optimization algorithms a major problem encountered in establishing the svms model is in the selection of its parameters the parameters c and γ have the key roles in the svms model first the testing and training data are randomly selected next the initial parameters of the optimization algorithms are randomly initialized the svms parameters are initialized the optimization algorithms are adopted to find the optimal solution of agents i e value of c and γ parameters in the search space next the training data are put into the svms model with the optimal parameters to obtain the trained svms optimization model finally the testing data are used to investigate the prediction ability of the trained svm optimization model 2 8 principal component analysis pca in this study principal component analysis was used to select the optimal input combination for predicting tds the pca is a statistical method that transforms a number of correlated variables into a number of uncorrelated variables referred to as principal components lu et al 2019 the principal components are used by the pca to identify the input data that have the highest effect on the tds output additionally pca is used to reduce the number of input data some inputs are eliminated from the original input data while the most salient parameters are detected by the pca zhao et al 2019 first the standardization of input data is performed then the correlation matrix is computed finally the eigenvalues and vectors are computed as follows 45 r λ i i p 0 where r correlation matrix λ i eigenvalues and ip unit matrix the first principal method has the highest eigenvalues in this study the following indexes are used to evaluate the accuracy of hybrid models maroufpoor et al 2019 46 rmse 1 n i 1 n y i y i 2 47 mae 1 n i 1 n y i y i 48 nse 1 t 1 t y i y i 2 t 1 t y i y i 2 49 pbias i 1 n y i y i 100 i 1 n y i where rmse root mean square error mae mean absolute error nse nash sutcliff efficiency y i simulated data and y observed data the lower values of rmse nse and pbias show better accuracy of models maroufpoor et al 2019 the higher value of nse shows more accuracy of models maroufpoor et al 2019 one of the most important steps in developing the model is the procedure taken to split the data for training and testing the training data would be used to identify the model structure and the value of the internal parameters and hence the testing data is used to evaluate the performance of the model which in this case could be considered as unseen data for the model this step usually is carried out using trial and error procedure in order to assure that the model could achieve the optimal performance in fact the most important reason for the successfulness of the model is that the training data should include most of the possible patterns that could be occurred in the whole data and hence the model could achieve accurate forecasting during testing unfortunately there is no particular procedure could be used to identify the best splitting percentage therefore in this study different splitting percentages have been examined the data splitting percentages are 80 70 and 60 for training and consequently 20 30 and 40 for the testing however there is a need to evaluate the performance to attain which one of these data splitting percentages is the optimal one therefore sensitivity analysis within each data splitting percentage has been carried out utilizing root mean square error rmse as an objective function to evaluate the error progressing it should be noted here that other data splitting percentage could achieve better results however as long as one of the examined data splitting percentages has been proved to satisfactory achieve acceptable objective function it could be considered as a stopping criterion for the trial and error procedure it should be noticed here that it is not necessary that if certain data splitting percentage has been successful for a particular data pattern it could be applied for other data pattern in fact this procedure for the best selection of the data splitting should be carried out when developing any similar model and applied for other data at different case studies 3 case study the study area is situated in the yazd province in iran fig 10 which extends from the longitude of 54 20 to 55 05 e to the latitude of 31 15 to 32 40 n the area of the basin is 2 491 km2 here the data related to 12 observed wells in yazd plain recorded between 2002 and 2015 are used to predict the tds value table 1 provides the statistics of water quality data used in this study according to this table among all input parameters sodium na 63 97 mg lit train and 63 57 mg lit test stood at the maximum value of concentration whereas the minimum value 3 75 mg lit and 3 72 mg lit was attributed to calcium ca the standard deviation value of na was spread over a wider range of values in comparison to other inputs the standard deviation value for ca indicates that the ca values were closer to its mean different sizes of data for the training and testing level are selected about 80 of training data and of testing 20 data account for the lowest error function rmse as mentioned above a sensitivity analysis is required to identify the best size of the data used for training and testing in this study three sizes were selected as observed in fig 11 as it could be depicted from fig 11 three different sizes of data splitting percentage have been carried out 60 70 and 80 for training and 40 30 20 for testing in order to evaluate the performance of each data splitting percentage the objective function value rmse has computed for different sizes in addition fig 11 illustrates the value of the attained objective function value for different sizes and models it could be observed that the objective function value experienced the minimal value for all models when using data splitting percentage equal to 80 for training and 20 for testing as a result all the models have been developed based on this optimal data splitting percentage in order to evaluate and analyze the performance of all proposed models within this research this study attempts to contribute to the analysis of tds data by conducting a trend analysis using a graphical method the trend analysis shows the distribution of the tds values for better monitoring of the water quality of the basin thus if the tds values have significant increasing trends the decision makers implement policies to control the tds values the application of ita includes changes observed in trends in tds it is crucial to identify possible trends to better understand the temporal changes in tds the knowledge of temporal changes in tds is of particular importance in the optimal management of water resources during recent years the increasing demand for monitoring water quality of aquifers has been observed in the ita method the time series is first divided into two equal groups the first half of tds time series is placed on the horizontal axis while the second half is placed on the vertical axis şen 2012 this technique identifies the trends of low medium and high tds data in fig 12 a graphical representation of the innovative trend method ita is observed if data are observed on a 45 line there is no trend in the time series if the data are collected on the upper triangular area of the 45 line an increasing trend is observed in the time series i e tds value is increasing if the data are accumulated in the lower triangular area of the 45 line there is a decreasing trend in the time series i e tds value is decreasing 4 results and discussion in this section the results are listed as follows 1 first the results of the taguchi model are listed the optimal values of random parameters are selected using the taguchi model 2 the results of the pca are listed to select the appropriate inputs to the models 3 the results of statistical indexes are presented to select the best modes 4 the scatterplots are used to evaluate the accuracy of the models 5 an uncertainty analysis is used to quantify the uncertainty of the models 6 finally innovative analysis is used to detect the tds trends 4 1 selection of random parameters of optimization algorithms when the optimization algorithms are applied to the optimization problems at hand the effect of the random parameters on the performance of the algorithms should be considered thus it is essential to determine the optimal values of the random parameters zhang and wang 2019 in this study a robust taguchi model was used to find the appropriate values of the random parameters of the optimization algorithms the taguchi model involves the detection of proper random parameters to obtain the optimum results of the process zhang and wang 2019 the orthogonal arrays were used by the taguchi method the minimum number of designed experiments is computed as follows 50 n taguchi 1 l i 1 n v where n taguchi minimum number of required experiments for the taguchi model l number of levels and nv number of parameters table 2 shows the level of each parameter the taguchi model determines the relative value of each parameter with respect to its main effect on the performance of the optimization algorithm when the taguchi model investigates the impact of a specific parameter on the performance of the optimization algorithms other parameters of the optimization algorithm are kept fixed the signal to noise s n ratio is used to determine the appropriate values of random parameters as follows 51 s n r a t i o 10 log 10 o b j e c t i v e function 2 the signal to noise ratio is computed with respect to the variation of the value of a parameter the levels are also categorized according to the range of changeability of the random parameters the mean of s n ratio of each parameter at a defined level is computed the detailed results are observed in table 2 the maximum value of the s n ratio shows the optimal values of the random parameters the optimal values of population size and the maximum number of iterations of mfo are 200 s n ratio 1 27 and 300 s n ratio 1 25 respectively regarding similar interpretation the optimal values of random parameters are obtained 4 2 results of pca the orthogonal transformation is used by the pca model to obtain the principal components pca reduces the number of data without using much information the pca was used because a smaller data set can be easily analysed first the input data are standardized then the correlation matrix is formed with a dimension of 6 6 equivalent to the number of input data table 3 shows the results of the pca model the results indicate that the first three pcs included 87 of the input data variance in addition the hco3 ca and mg had the most impact on the first three pcs thus the first three pcs were selected for the current article 4 3 the analysis results for the soft computing models table 4 shows the results of the soft computing models in the training level the rmse mae pbias and nse calculated for the hybrid anfis ann and svm models were significantly different from those of the standalone svm anfis and ann models among the soft computing models the anfis mfo indicated the highest accuracy in the training level rmse 2 25 mae 2 21 nse 0 94 and pbias 0 12 among the hybrid soft computing models the rmse mae and pbias of the svm pso had the highest value compared to those of other hybrid models rmse 2 69 mae 2 67 pbias 0 32 the results indicate the superiority of the mfo followed by cso in comparison to pso gsa sa and gwo the outputs indicate the superiority of the anfis model followed by ann in comparison to the svm model regarding nse mae pbias and rmse the svm mfo was found to be the best model compared to the svm gwo svm cso svm pso and svm gsa models according to the findings from table 4 the results indicate that the ann mfo had better performance than the ann pso ann cso ann gsa ann sa ann gwo and ann models table 5 indicates the results for the soft computing models in the testing level table 5 shows that none of the standalone anfis ann and svm models obtain better performance than the hybrid anfis svm and ann models in quantitative analysis the hybrid anfis mfo reduced the rmse over the ann mfo and svm mfo models by 1 4 and 3 8 respectively it was observed that the svm model had the least nse value among the models a detailed comparison of models indicates that the ann mfo shows the highest nse value and lowest mae rmse and pbias values among the ann models however the highest values of rmse mae and pbias were observed for the svms model the results indicate the superiority of the mfo followed by cso in comparison to pso gsa sa and gwo a separate analysis was used to determine the cpu time taken by each of the soft computing models to predict the tds values the training time of the models indicate that the performance of the anfis mfo is three and four seconds faster than the ann mfo and svm mfo respectively the testing results of models indicate that the anfis ann and svms models take 152 157 and 158 s respectively the results reveal that the svms model is the slowest model among all the models in fig 13 and fig 14 the scatterplots were depicted for different soft computing models the results reveal that the anfis mfo indicted the highest r2 among all the models a detailed comparison of the values of r2 indicates that the standalone ann anfis and svms models had worse performance than the hybrid ann anfis and svm models according to the findings from fig 13 and fig 14 the results indicate that the pso obtained worse performance than the mfo sa gwo gsa and cso there are different sources of uncertainty for soft computing models such as models parameters and input data gholami et al 2019 when the classical training algorithms with low accuracy are used to train the soft computing models the uncertainty of model parameters increases the current study uses optimization algorithms as an alternative to classical training algorithms to decrease the uncertainty of model parameters the input data was among the most important sources of uncertainty of the soft computing models in this study the uncertainty of input data was regarded thus it is necessary to quantify the uncertainty of soft computing models gholami et al 2019 in this study the sequential uncertainty fitting method sufi 2 was used to quantify the uncertainty of the soft computing models to apply the sufi 2 algorithm to the soft computing models the rmse was selected as an objective function for minimizing error values then the input parameter ranges were defined next the latin hypercube methods were used as the sampling method the latin hypercube sampling lhs is a way of providing random samples of input parameters a cube with more than three dimensions is known as a hypercube the lhs is used to sample from the multiple dimensions the generated samples were used as an input to the soft computing models the percentage of observed data bracketed by 95 prediction uncertainty 95ppu was computed at the 2 5 and 97 5 levels of the cumulative distribution of the tds variable the 95ppu denotes as p parameter the r parameter is used as another parameter to quantity to the uncertainty results the average thickness of the 95ppu band divided by the standard deviation of the data is used to compute the r index the best model has the highest p and the lowest r values table 6 a shows the uncertainty results for the training level considering the obtained results of soft computing models in the table 6a the anfis mfo achieved the lowest and highest values of d and p respectively the results indicate that the standalone ann anfis and svms models had higher d than the hybrid ann anfis and svm models table 6a also confirmed that the mfo outperformed the other optimization algorithms table 6b showed the testing results of the uncertainty analysis of soft computing models the results indicate that the svms had the highest r and the lowest p among all the models the results indicate that the ann mfo had higher p and lower r than the ann pso ann wgo ann gsa and ann cso as observed in table 6b the results indicate that the mfo outperformed the other optimization algorithms the trends in tds value using ita are provided in fig 15 fig 15a illustrates the ita results for observed data the tds value 1800 mg lit indicates a decreasing trend while the medium tds values 2000 mg lit tds 2800 mg lit did not have a significant trend the high tds values tds 3000 mg lit indicate an increasing trend fig 15b shows the ita results for anfis mfo the low tds values tds 1800 indicate a decreasing trend in the medium cluster 2000 mg lit tds 2800 mg lit there is almost no trend in the high cluster tds 3000 mg lit the results indicate an increasing trend regarding the similar interoperation the ita results were analyzed for outputs of ann mfo and svm mfo models the high tds values tds 3000 mg lit of svm mfo indicate no significant trend similarly the medium tds values of svm mfo and ann mfo models 2000 mg lit tds 2800 mg lit indicate no significant trend 5 conclusion aquifers are among the largest freshwater resources in this research prominent soft computing models were used to estimate tds value in the yazd plain the anfis ann and svm were used for predicting tds although the anfis svm and ann are widely used for predicting the hydrological variables they require powerful training algorithms for finding optimal values of model parameters in this study the mfo cso gwo gsa pso and sa were used to train the ann anfis and svm models among the soft computing models the anfis mfo indicate the highest accuracy in the training level rmse 2 25 mae 2 21 nse 0 94 and pbias 0 12 the results indicate that the anfis mfo has the highest r2 among all the models a detailed comparison of the values of r2 indicates that the standalone ann anfis and svm models had worse performance than the hybrid ann anfis and svm models the results indicate that the svms had the highest r and the lowest p among all the models the trend of observed data outputs of the anfis mfo ann mfo and svm mfo were identified using the ita method the low tds value for observed data tds 1800 mg lit indicates a decreasing trend while the medium tds values 2000 mg lit tds 2800 mg lit did not have a significant trend the high tds values for observed data tds 3000 mg lit indicate an increasing trend the quality of gw is important because it determines the suitability of water for drinking irrigation and domestic purposes thus predictive models can be used by decision makers and policymakers future research can employ the multi objective mfo to further enhance the accuracy of the outputs in fact two objective functions can be defined for the mfo to determine the appropriate inputs and optimal parameters of the soft computing models if the multiobjective optimization algorithms are used to develop the soft computing models it may not be required to use the preprocessing methods such as pca to select the best inputs separately these developed models are also used for agro environmental policies because they can determine the effect of agriculture on some key parameters in gw chemistry credit authorship contribution statement fatemeh barzegari banadkooki writing original draft writing review editing mohammad ehteram conceptualization methodology software fatemeh panahi writing original draft writing review editing saad sh sammen writing original draft faridah binti othman writing original draft writing review editing ahmed el shafie conceptualization methodology software declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements the authors appreciate so much the facilities support by the civil engineering department faculty of engineering university of malaya malaysia and the financial support received from research grant coded gpf082a 2018 and gpf070a 2018 funded by the university of malaya appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j jhydrol 2020 124989 appendix a supplementary data the following are the supplementary data to this article supplementary data 1 
5441,this study provides numerical simulation and experimental evidence of flow through a rough conduit from the perspective of energy loss we have chosen four different types of relative roughness based on the experiment of huang et al 2013b for the numerical simulations the corresponding experimental data were compared to verify the correctness of the numerical simulation the experimental and numerical simulation results both concur that the specific discharge q in the rough conduit is obviously affected by different roughness of conduit when q is somewhat the same a larger relative roughness will lead to a larger hydraulic gradient j however the q j curves obtained experimentally and numerically are both smooth do not show obvious dividing points of transition from darcian flow to non darcian flow we have quantified the energy loss of different rough conduits according to different flow regimes which appear to impose different impacts on energy loss in the laminar flow regime the relationship between q and the energy loss is linear while in the fully developed turbulent flow regime such a relationship becomes quadratic moreover we used the local head loss coefficient ς as an indicator to reflect the energy dissipation influenced by the relative roughness and found that ς decreased gradually with the increase of q and finally tended to an asymptotic constant in addition we have observed the high resolution velocity variation inside the rough conduit and the streamline distribution inside the recirculation region in detail in the laminar flow regime the streamlines deflect but are generally along directions somewhat parallel to the conduit wall when the relative roughness to the maximum relative roughness reaches 1 2 4 the streamline will deflect more sharply even if q is relatively small however in the fully developed turbulent flow regime the deflection of streamline and eddy will be more striking the recirculation region becomes clearly visible even for relatively small q values within the fully developed turbulent flow regime keywords numerical simulation energy loss local head loss inertial force 1 introduction the effect of relative roughness on conduit flow has been studied for more than a century and the subject becomes even more relevant in hydrological sciences because of subsurface flow in karst conduits cheng and chen 2005 jeannin 2001 and flows in long boreholes such as horizontal wells penmatcha et al 1997 su and gudmundsson 1993 during the past decades many scholars carried out relevant experiments to study the influence of relative roughness on fluid flow in conduits where the relative roughness is defined as the ratio of roughness to the conduit hydraulic diameter however the exact nature of the impact of relative roughness on fluid flow has not been completely understood taylor et al 2006 to understand a flow system better characterization of energy loss and surface roughness are two key points of concern fanning 1892 proposed a relationship between pressure drop and surface roughness however a systematic study of quantifying the relationship between surface roughness and pressure drop was conducted by nikuradse 1933 in the experiments reported in nikuradse 1933 he coated the inside walls of conduits with uniform sands and controlled the reynolds numbers re ranging from 600 to 106 in a series of fluid flow experiments according to the experimental results nikuradse 1933 mainly divided the flow regimes into three portions huang et al 2013b a laminar flow regime a transitional flow regime and a fully developed turbulent flow regime the experimental results of nikuradse 1933 have been widely cited subsequently for testing numerous flow theories involving flow in conduits and wellbores brkić and praks 2018 tzelepis et al 2015 woo and brater 1961 one unique result of nikuradse 1933 was that the critical reynolds number rec for the transaction of laminar to fully developed turbulent flow was around 2000 based on the classical work of nikuradse 1933 many scholars have conducted extensive experiments with values of the relative roughness greater than what has been used by nikuradse 1933 and some new phenomena have been discovered brackbill and kandlikar 2008 huang et al 2013a mala and li 1999 tang et al 2007 for instance an earlier transition between flow regimes was observed and the critical reynolds number rec was found to decrease either linearly or exponentially with the increase of relative roughness huang et al 2013b flow in porous media however has long been conceptualized as flow in a bundle of tortuous conduits meandering around solid grains bear 1975 bianchi et al 2011 yeh et al 2015 thus theoretical development of porous media flow is inspired and closely related to that of conduit flow for instance reynolds number re and moody chart which are used commonly for conduit flow phenomena have also been widely utilized in porous media flow such as for determining the deviation from linear to non linear flow regimes li et al 2017 2019 lopik et al 2017 2019 arbhabhirama and dinoy 1973 has systematically analyzed many experiment data and proposed empirical relationship between re and friction factor f in porous media where the friction factor is defined as the darcy friction factor which is a dimensionless term as the ratio of local shear stress to kinetic energy density comparing the results of arbhabhirama and dinoy 1973 with those of nikuradse 1933 we find that the characteristic of the moody chart associated with porous media flow is significantly different from that of conduit flow as reported by nikuradse 1933 for instance the relationship for re versus f for porous media flow is smoother and there are four distinctive stages of flow regimes darcian laminar non darcian laminar transitional and fully developed turbulent flow regimes huang et al 2013b therefore how to quantify the deviation of darcian flow from non darcian flow has become a great concern and different scholars have obtained various critical reynolds number rec arranging from 1 to 1000 in different kinds of porous media setting where rec is associated with the point of transition from darcian flow to non darcian flow bear 1975 bu et al 2014 dybbs and edwards 1984 latifi et al 1989 sedghi asl et al 2014 recently we have carried out a series of conduit and porous media flow experiments in a variety of structures huang et al 2013b li et al 2017 2019 and these experiments have confirmed that there was no obvious point of transition from darcian flow to non darcian flow however a quantitative analysis from an energy or hydraulic head loss perspective along the flow direction about those experimental results have not been done yet where the hydraulic head or head is the energy per unit weight of water a term broadly used for describing subsurface water flow in porous media in addition recirculation and eddy are commonly seen for flow near rough conduit walls chen et al 2017 qian et al 2012 and the influence of recirculation and eddy on the overall conduit flow is still largely unknown furthermore the difference between the laws of energy loss in porous media flow and conduit flow needs to be explained from a more fundamental physical perspective one way of addressing the above mentioned issues is to employ high resolution numerical simulations that are capable of understanding the hydrodynamics of conduit flow and porous media flow from the first principle of physics for instance croce and d agaro 2005 carried out a series of flow models in rough conduits to estimate the magnitude of roughness on microchannel heat transfer and pressure drop in the laminar flow regime where the roughness is randomly generated to mimic a more realistic rough conduit wall rawool et al 2006 placed different shapes of well defined obstacles on the inner surface of conduit walls to investigate the effect of roughness geometry and re on the friction factor f khadem et al 2009 conducted numerical simulations of fluid flow and heat transfer characteristics in rough microchannels with special consideration of flow slip and temperature jump near the rough walls recently chen et al 2017 conducted a serial of experiments and numerical simulations to study the effect of roughness on flow through a single rough fracture and proposed new equations for calculation of friction factor f and specific discharge q huang et al 2013b has carried out a series of comprehensive experiments on pressure drop and friction factor f through conduits with relative roughness ranging from 1 118 75 to 1 3 and eventually to 1 2 4 the experimental apparatus mainly consists of three parts an adjustment valve a test section rough conduit and measurement equipments water was supplied by a pump to the test conduit and the specific discharge was adjusted by the inflow valve after passing through the test conduit water was collected by a tank to minimize errors the specific discharge was observed three times for a certain head drop and the mean value was adopted three piezometer tubes were used to measure the water head drop piezometer tubes were 500 mm away from both ends of the test tube to minimize the inlet and outlet effects meanwhile water temperature was also measured by a thermometer to ensure the isothermal condition during the experiments if the relative roughness is large enough that roughness asperities are touched and connected to each other then the rough conduits model may be regarded as a somewhat simple porous media model specifically when the relative roughness reaches 1 2 4 the roughness asperities on opposite sides of the conduit make contact with each other thus this case is designated as the transaction from a rough conduit model into a simple porous media model the case associated with a relative roughness of 1 2 4 can be visualized directly from fig 3 of huang et al 2013b huang et al 2013b further concluded that the curvature and variable cross section of flow channels for this special case with a relative roughness of 1 2 4 would cause frequent changes and acceleration in the flow direction to generate sufficient inertial forces in porous media flow that may not be seen in conduit flow hellstrom et al 2010 moutsopoulos et al 2009 this explains why non darcian flow in porous media can occur even at relatively low specific discharge or re values as compared to conduit flow in which non darcian flow can only occur at relatively large re number the purpose of this investigation is to carry out a relevant high resolution numerical simulation based on huang et al 2013b experiment to reveal the relationship between the flow characteristics at the pore scale and the specific discharge with different relative roughness and to explore the law of energy loss in different flow regimes to meet the objectives we have firstly chosen four types of conduits with different relative roughness after that a finite volume method fvm was used for simulations whose results are compared with the experimental results of huang et al 2013b finally the specific discharge distribution with different relative roughness and the effects of different flow regimes on energy loss were discussed 2 numerical simulation methodology 2 1 characterization of surface roughness huang et al 2013b have conducted a series of flow experiments in organic glass conduits with different relative roughness δ d using identical acrylic spheres stuck to the inner walls of conduits to create uniform roughness throughout the conduits the roughness generated in huang et al 2013b is different from that in a previous study of huang et al 2013a in which sifted sands are stuck to the inner walls of conduits to generate roughness the problem of using sifted sands to generate roughness is that the conduit morphological difference will increase as the diameter of the increase of the sand as pointed out by huang et al 2013b since the sands are not uniform spheres the surface unevenness of sands increase with the increase of the particle size therefore replacing the sands with acrylic spheres can ensure that the relative roughness of the experimental tube segment remains consistent on the basis of the experiments of huang et al 2013b we chose five different types of δ d values including 1 4 86 1 4 22 1 3 62 1 3 and 1 2 4 in this study it is worth mentioning that the test conduits with δ d values equalling to 1 4 86 1 4 22 1 3 62 and 1 3 had the same inner diameter d of 19 mm while acrylic spheres of 10 mm in diameter and a conduit with an inner diameter of 24 mm were chosen to made test conduit with the maximum relative roughness equal to 1 2 4 because of the axisymmetry of the problem of concern here we simplify the problem to a two dimensional 2d cross section through the center of the conduit as shown in fig 1 a complete three dimensional 3d investigation is computationally extremely expensive and will be saved for a future investigation fig 1 illustrates schematically a circular conduit with periodic roughness placed on its inner wall where d is the hydraulic diameter of the conduit δ is the surface roughness and l is the length of the conduit and it is assumed to be sufficiently long to ensure fully developed flow regimes within the domain of interest 2 2 governing equation generally speaking flow through a conduit is governed by a mass conservation or continuity equation and navier stokes n s equations the n s equation is the law of fluid motion obtained by applying newton s second law and it is the basis for studying flow in conduits when a newtonian fluid with constant density and viscosity flows through a conduit with impermeable walls the n s equation can be written in a vector form as follow chen et al 2017 van golf racht 1982 1 ρ v v μ 2 v p 2 v 0 p γ h where ρ is the fluid density μ is the fluid viscosity p is the hydrodynamic pressure v is the velocity vector designed as v v x v y v z with three components of vx vy and vz along the x y and z directions respectively is the gradient operator γ is the fluid specific weight and h is the hydraulic head or energy per unit weight of the fluid of concern the three terms of the eq 1 represent the inertial forces the term on the left hand side the viscous forces the first term on the right hand side and the pressure forces the second term on the right hand side respectively brush and thomson 2003 assuming that the inertial forces are small enough to be ignored eq 1 can be written as 3 μ 2 v p 0 combining eq 2 with eq 3 forms a system of linear equation called the stokes equation 2 3 numerical simulation four kinds of physical models with different relative roughness values of 1 4 86 1 4 22 1 3 62 1 3 were selected for corresponding numerical models based on the n s equation among many different numerical schemes for solving the n s equation fvm is selected in this study and detailed information about fvm will not be given here but can be found elsewhere leveque 2002 toro 2013 fvm is based on an integral form of the n s equations and is easy to program for parallel computation with high degree of accuracy in this paper the computational region is divided into a series of non repetitive finite control volumes with each control volume represented by a node and the n s equation can be solved for each given control volume chen et al 2017 toro 2013 fvm was operated in fluent 18 0 for related operations which is widely used in computational fluid dynamics cfd to improve computational efficiency the length of the numerical model is less than the length of the laboratory test model but it is long enough to ensure the fully developed turbulent flow regimes within the domain of interest the following procedures are used to decide the conduit length in the simulations first simulations with different conduit lengths and surface roughness can be tried at first and the flow streamlines can be inspected graphically from such a graphic presentation of streamlines the influences of the inlet and outlet boundaries are clearly identifiable meanwhile the development of flow regimes from the inlet to the outlet is also graphically visible second after determining the legnths of influences of the inlet li and outlet lo boundaries and finding out the minimal length l required to encompass the complete evolution of flow from laminar flow to fully developed turbulent flow then the minimal length required in the simulation should be greater than li lo l the 2d flow grids were created using a pre processor gambit to better observe the streamline near the inner wall of the conduit the mesh near the rough inner wall is denser than that near the center of the conduit a quadrilateral mesh is selected and mesh sensitivity is checked for each model to ensure that the numerical simulation result is free from the influence of the chosen mesh sizes overall a total of 292 822 control volumes have been usually used in our simulations constant pressures at inlet and outlet boundaries were maintained in the simulations specifically the inlet and outlet pressures of the numerical model were obtained to ensure that the average hydraulic gradients used in the numerical simulations are the same as those used in experiments by doing so it is possible to compare the numerical simulation results with the experimental results if necessary after the generation of a grid mesh and determination of all the boundary and initial conditions we chose the standard k ε model to simulate the n s equation to obtain the flow fields the k ε model was a two equation model about fully developed turbulent flow proposed by launder and spalding 1972 which included a kinetic energy term k and a turbulent dissipation rate term of ε the equations of kinetic energy term and turbulent dissipation rate term are shown below 4 t ρ k x i ρ k u i x j μ μ t σ k k x j g k ρ ε 5 t ρ ε x i ρ ε u i x j μ μ t σ ε ε x j c 1 ε ε k g k c 2 ε ρ ε 2 k where μ t is the turbulent viscosity g k is the term of turbulent kinetic energy k caused by the average specific discharge gradient eq 5 is a steady state equation there is no time variable in the equation combining eq 4 with eq 5 we can eliminate the transient terms and get the governing equation of the k ε model which is suitable for the numerical simulation according to the recommended value of launder and spalding 1972 the k ε model constants are c 1 ε 1 44 c 2 ε 1 92 c μ 0 99 σ k 1 0 σ ε 1 3 3 results and discussion 3 1 simulation and experimental results comparison numerical simulations were conducted with four different relative roughness of 1 4 86 1 4 22 1 3 62 1 3 to check the accuracy of the simulation we need to compare the simulation results with the experimental results the relationship between q and j was plot in fig 2 to compare the simulation results of this study and the experimental results of huang et al 2013b where q is determined as the ratio of total discharge and the total cross sectional area of a conduit regardless of the surface roughness fig 2 shows that the simulation results fit well with the experimental results when q is somewhat the same a larger relative roughness will lead to a larger j this is because when the relative roughness gets larger the flow resistance becomes stronger thus a larger j is required to attain the same q the simulation results are slightly larger than those of the experimental results in fig 2 but in general they agree with each other very well besides the q j curves obtained experimentally and numerically are both smooth and there were no obvious dividing points of transition from darcian flow to non darcian flow the simulation results based on fluent software are consistent with the experimental results therefore we are confident that the developed simulation scheme is reliable and accurate for demonstrating the features of water flow through rough conduits with relative roughness values ranging from 1 4 86 to 1 3 3 2 effect of roughness on flow characteristics in this series of simulation the same relative roughness 1 3 with four different average specific discharges 0 001 m s 0 04 m s 0 1 m s 0 2 m s were used to observe the change of flow characteristics under different flow states or re values to observe the change of streamline more clearly we have enlarged several portions of streamline profiles between adjacent acrylic spheres stuck on the inner walls of conduits to examine the details see fig 3 we can find from fig 3 that the flow characteristics are obviously different under the same relative roughness due to different specific discharges we can see from the streamline profiles and the partially enlarged streamline profiles that when the average specific discharge is small q 0 001 m s the streamline is almost parallel with each other and there is almost no eddies the streamlines near the solid surfaces of acrylic spheres are somewhat along the surface curvations of those spheres reflecting the control of elements of roughness acrylic spheres on the flow field near those elements streamlines near the central axis of conduit are somewhat along the axis direction of conduit showing that they are only slightly influenced by the actual morphology of the rough elements there is a tendency to generate vortices or eddies at two adjacent rough elements with the increase of specific discharge as can be seen in fig 3 when the specific discharge increases to 0 04 m s one eddy is clearly visible between two adjacent roughness elements when the average specific discharge increases to 0 1 m s two eddies are clearly visible between two adjacent roughness elements when the average specific discharge reaches 0 2 m s two eddies are fully developed between two adjacent roughness elements and a third eddy appears to emerge near the point of contact of two adjacent roughness elements at the inner wall of the conduit a number of interesting observations can be made from fig 3 first it is evident from fig 3 that the magnitudes of flow velocities within the eddies decline when the specific discharges through the rough conduits increases blue color means a smaller velocity and red color means a larger velocity in fig 3 for instance when comparing the case of a larger average specific discharge of q 0 2 m s to that of a smaller average specific discharge of q 0 1 m s flow velocities observed within a specific eddy are obviously smaller than those observed within an eddy at a similar location second when the specific discharge increases the velocity contrastbetween the maximum velocity which occurs near the axis of conduit between two opposing rough element peaks when the flow channel becomes the narrowest and the minimum velocity which occurs near the center of the largest eddy also increases third flow velocity in the central portion of the conduit is non uniform along the axis of the conduit in fact it appears to be periodic controlled by the opposing rough elements closely more specifically if assuming the axis of the conduit is along the horizontal direction then when the vertical distance between two opposing rough elements is shorter the flow velocities are larger vice versa fourth if one examines the flow profile between two opposing rough element peaks closely the flow velocity along the horizontal direction appears to follow a parabolic shape along the vertical direction similar to the classical poiseuille flow profile within a smooth conduit without roughness to look at the flow profile from a different perspective fig 4 examines the influence of different relative roughness 1 4 86 1 4 22 1 3 62 1 3 at a relatively large given average specific discharge of q 0 2 m s this figure shows the detailed velocity variation inside the rough conduit and the streamline distribution inside the recirculation region or eddy a number of interesting observations can be made from fig 4 first this figure shows that the specific discharge in the rough conduit is obviously affected by different roughness and the influence becomes stronger when the asperity height is larger second there is a tendency to generate vortices within two adjacent rough elements when the relative roughness is 1 4 86 as the relative roughness increases to 1 4 22 one significant recirculation region or eddy can be found when the relative roughness further increases to 1 3 62 two recirculation regions or eddies are visible finally when the relative roughness increases to 1 3 three recirculation regions or eddies have been identified third due to the obstruction of roughness elements the streamlines close to the roughness elements would become curved according to the morphology of those elements 3 3 energy loss in different flow regimes observations made in figs 3 and 4 are very valuable for understanding the flow characteristics inside the rough conduit however those observations are qualitative in nature in this section we will quantify the energy loss associated with those observations we can use the local head loss coefficient ς as an evaluation indicator to reflect the energy dissipation influenced by relative roughness the local head loss coefficient depends on the flow geometry and is only related to the cross sectional shape of the conduit such as the ratio of changing cross sectional areas the radius of curvature of bend shape and size of the valve etc to get the local head loss coefficient we need to obtain the head loss of two sections these two sections must meet a specific requirement the selected section should be in fully developed flow regimes within the domain of interest to ensure the consistency and stability of the flow regime after choosing two sections we can obtain the water pressures and specific discharges corresponding to the selected two sections a schematic diagram is shown in fig 5 to illustrate the positions of the selected two sections in this study we selected the distance between the two sections as two complete rough elements or 4δ where δ is the radius of the spherical element shown in fig 5 meanwhile these two sections do not include any surface roughness so their cross sectional areas equal to the cross sectional area of a roughness free or smooth conduit the bernoulli equation which relates the pressure to the local specific discharge and gravity potential segletes and walters 2002 will be used in the following analysis 6 p 1 ρ g z 1 α q 1 2 2 g p 2 ρ g z 2 α q 2 2 2 g δ h where p 1 and p 2 are the average pressures of the selected sections z 1 and z 2 are the average elevations q 1 and q 2 are the average specific discharges of the selected sections α is a kinetic energy correction coefficient which is related to the specific discharge distribution when the specific discharge distribution is uniform α 1 when the specific discharge distribution is not uniform α 1 more detailed explanation about α can be found in li 2012 and osorio and steffe 1984 in general α 1 05 1 10 in the conduit flow li 2012 osorio and steffe 1984 without loss of generality α 1 is used in this study we have also confirmed that the use of α slightly greater than 1 in the range of 1 05 1 10 does not affect the conclusions at all it is interesting to point out that the three terms on the left side of eq 6 respectively represent the pressure head elevation head and kinetic head in section 1 while the first three terms on the right side of eq 6 respectively represent the pressure head elevation head and kinetic head in section 2 therefore δ h on the right side of eq 6 is the total energy loss or total hydraulic head loss between sections 1 and 2 more specifically δ h includes the frictional head loss h l between sections 1 and 2 and the local head loss h m which refers to head loss occurring when water flows around rough elements inside the conduit according to the bernoulli equation and considering that the average elevation of the selected sections is the same z 1 z 2 one can get the total energy loss formula as shown below 7 δ h p 1 p 2 ρ g q 1 2 q 2 2 2 g according to eq 7 we need to get the average pressures and the average specific discharges of the two sections to obtain the energy loss therefore we chose the area weighted average of the pressures and specific discharges of the two sections besides the frictional head loss h l can be ignored because the distance between the selected two sections is short enough the average specific discharge of the selected two sections is the same when the flow reaches stability which means q 1 q 2 therefore the total energy loss δ h is basically equal to the local head loss h m as follow 8 δ h h m p 1 p 2 ρ g conventionally the local head loss is often computed using the average specific discharge of the flow q as h m ς q 2 2 g where ς is termed the local head loss coefficient nicholl and detwiler 2001 sánchez et al 2008 for the problem of concern here the average specific discharges in these two sections are the same when the flow is stable q q 1 q 2 therefore the local head loss coefficient ς is shown as follow 9 ς 2 g h m q 2 2 p 1 p 2 ρ q 2 to investigate the effects of surface roughness on flow in conduits and the characteristics of the local head loss coefficient huang et al 2013b chose the moody chart to plot the experimental data the results are shown in fig 6 we can clearly see from this figure that the experimental curves can divide into three regimes a laminar flow regime a transitional flow regime and a fully developed turbulent flow regime for the illustration purpose we will calculate the energy loss of laminar flow and fully developed turbulent flow and plot the relationship between the energy loss and re for these two flow regimes it is worth mentioning that we chose the standard k ε model to simulate the n s equation to obtain the flow fields the transitional flow regime is more complex than those of laminar flow and fully developed turbulent flow regimes because it involves a continuously evolving state that requires many different approximations to quantify the flow therefore the laminar flow and fully developed turbulent flow are discussed in the following sections as for transitional flow we will conduct a detailed analysis in another study 3 3 1 energy loss in laminar flow according to the experimental data of huang et al 2013b we selected the relevant data of laminar flow and calculated the local head loss according to eq 8 after this step we obtained the relationship between the average specific discharge q and energy loss δ h as shown in fig 7 this figure shows that the energy loss caused by different relative roughness is obviously different in the laminar flow regime when q is somewhat the same a larger relative roughness will lead to more energy loss the relationship between q and δ h is linear shown as below 10 δ h m q where m is the coefficient related to different relative roughness which is shown in table 1 as the distance between two sections is 4δ where δ is the radius of the spherical element shown in fig 5 then the hydraulic gradient between the two sections is j δ h 4δ thus the physical significance of coefficient m in eq 10 is equivalent to a ratio of 4δ over the hydraulic conductivity in darcy s law for laminar flow the relationship between coefficient m and relative roughness was plotted in fig 8 which shows that m is proportional to the relative roughness and the relationship between coefficient m and relative roughness is given as m 2 69e 06 δ d 5 28e 07 meanwhile we can also directly visualize the velocity distribution patterns inside the conduits with different relative roughnesses in the laminar flow regimes in fig 9 in which the average specific discharge is about 0 015 m s from fig 9 we can see that different roughness leads to different velocity distribution patterns under the laminar flow conditions the streamlines are almost parallel to the central axis when the roughness is small 1 4 86 1 4 22 and the velocities at the wall are almost zero then the streamlines near the wall deflect but flow parallel to the wall when the roughness increases gradually 1 3 62 1 3 the continual deflection of the streamlines inevitably leads to additional local head loss therefore we can imagine that when the relative roughness reaches the peak relative roughness of 1 2 4 which can be regarded as a simple porous media model as the roughness asperities on opposite sides of the conduit make contact with each other the streamlines will deflect sharply even if the specific discharge is small leading to significant local head loss 3 3 2 energy loss in fully developed turbulent flow in this section we also chose the relevant data of fully developed turbulent flow and calculated the local head loss accordingly using eq 8 the relationship between q and δ h is shown in fig 10 which demonstrates that the energy loss caused by different relative roughness is different in a fully developed turbulent flow regime when q is somewhat the same a larger relative roughness will lead to more energy loss this is because with the increase of surface roughness δ the vortex generated by the deflection of streamline at the two adjacent rough elements will be larger moreover the curve of energy loss with specific discharge in the fully developed turbulent flow is completely different from that in the laminar flow regime we performed a quadratic best fitting exercise of the curve shown in fig 10 and found that the quadratic curve fits very well as follows 11 δ h a q b q 2 where a and b are coefficients related to different relative roughness which are shown in table 2 once again if needed one can calculate the hydraulic gradient between two sections as the ratio of δ h in eq 11 over the distance between two sections 4δ for fully developed turbulent flow the relationships between coefficients a and b and relative roughness are plotted in fig 11 and fig 12 respectively we can see that a is inversely proportional to the relative roughness while b is a quadratic function of the relative roughness the coefficients in front of the linear a and quadratic b terms can be respectively predicted as a 4 57e 06 δ d 8 83e 07 b 2 6e 03 δ d 2 1 1e 03 δ d 1 0e 04 we have also obtained the velocity distribution patterns of different relative roughness for the fully developed turbulent flow shown in fig 13 in which the specific discharge is about 0 2 m s this figure shows that the effect of relative roughness on velocity distribution is significant in the fully developed turbulent flow regime we find that the streamlines near the central axis deflect sharply with the increase of roughness and additional energy loss is generated during the process of continuous enlargement and reduction of cross section available for flow due to the increase of roughness of the conduit wall in addition we also see from fig 4 that eddies have been generated at two adjacent rough elements therefore the deflection of streamlines will be more intense in those eddies leading to more energy loss at last we used the local head loss coefficient ς serving as an evaluation indicator to reflect the energy dissipation influenced by relative roughness and eq 9 is used to quantify the effect of surface roughness on rough conduit flow the relationship between q and the local head loss coefficient ς is plotted in fig 14 this figure shows that ς decreases gradually with the increase of q and finally tends to a constant value fig 14 shows that the ς values of a rough conduit with different relative roughnesses are different in general when q is somewhat the same a larger relative roughness will lead to a larger ς value the ς value decreases rapidly when q is small with the relative roughnesses in the range of 1 3and1 3 62 then tends to approach an asymptotic constant however the ς value of a rough conduit with a relative roughness of 1 3 is larger than that of a rough conduit with a relative roughness of 1 3 62 therefore when q is somewhat the same a larger relative roughness will lead to larger energy loss 3 3 3 further analysis of energy loss webb et al 1971 observed that flow separated from the surface of the roughness elements and then reattached at a distance 6 8 times the height of the elements in this study we find that the flow pattern consists of two features for laminar flow in a rough conduit including relatively straight flow paths near the central axis area and curved flow paths near the roughness elements huang et al 2013b the curved flow paths will lead to velocity components in other directions rather than the axis direction which means that the inertial force term is not equal to zero however the inertial force term is usually ignored for laminar flow especially when the specific discharge is small therefore the linear or darcian flow is an approximate representation of ignoring the inertial force term li et al 2017 2019 when such an inertial force term becomes sufficiently large to be non negligible then the energy loss will start to deviate from the linear trend signaling the start of non darcain flow when the flow through a rough conduit with the maximum relative roughness of 1 2 4 which can be regarded as a simple porous media model as the roughness asperities on opposite sides of the conduit make contact with each other the curvature and variable cross section of flow channels would cause frequent changes and acceleration in flow directions because of the rough elements even in laminar flow regime which means that the flow could become nonlinear in porous media even at relatively low specific discharge 4 limitations and future study although this study represents a significant advancement of present knowledge of conduit flow with various roughness which may eventually lead to a better understanding of flow in karst aquifer systems and other porous media systems there are a number of limitations that should be pointed out and some future investigations are also highlighted firstly we have simplified the problem to a 2d cross sectional view through the center of the conduit this is based on a few considerations first the periodic arrangement of identical roughness elements used in the experiments of huang et al 2013b and this study makes the use of a 2d approach appealing second the n s equation is very costly to solve from a computational standpoint even for the 2d problem and the computational cost of solving the n s equation for a 3d problem is extremely costly and sometimes forbidden in this regard advanced numerical schemes such as parallel computation in a supercomputing facility may have to be used in the future for 3d flow problems in a rough conduit secondly if a robust 3d simulator can be developed we can then expand the present study for greater relative roughness values that have not been investigated in detail in this study for instance when the maximum relative roughness reaches more than 1 2 4 the roughness asperities on opposite sides of the conduit will completely touch with each other generating a pore system that is somewhat similar to a porous media for such a rough conduit system the 3d description of flow is more appropriate because flow pathways have to move around those touched or connected asperities furthermore random or irregular roughness elements instead of the identical roughness elements used in this study should be utilized to make the problem more relevant to the actual setting thirdly this study only involves a straight conduit with a constant diameter which is obviously for the sake of simplifying the problem and reducing the computational cost it will be very desirable to investigate a meandering conduit with variable cross sectional sizes and shapes in the future to mimic flow in a subsurface karst conduit system nevertheless additional energy loss terms must be taken into consideration when the conduit is meandering and its cross sectional sizes and shapes are changing furthermore if several rough conduits are connected then flow converging diverging and partition issues all have to be considered 5 summary and conclusions this study presents numerical simulation results of different types relative roughness conduits the specific discharge distribution with different relative roughness and the effects of different flow regimes on energy loss were discussed the main conclusions can be summarized as follows 1 the experimental results and numerical simulation results show that the specific discharge q in the rough conduit is obviously affected by different roughness when q is somewhat the same a larger relative roughness will lead to a larger hydraulic gradient j 2 different flow regimes have different effects on energy loss the relationship between the specific discharge and the energy loss is linear in the laminar flow regime and the coefficient m which is the slope of total head loss δ h versus q is proportional to the relative roughness for the experimental data of huang et al 2013b one has m 2 69e 06 δ d 5 28e 07 the relationship between δ h and q becomes quadratic in the fully developed turbulent flow regime moreover for the experimental data of huang et al 2013b the coefficients in front of the linear a and quadratic b terms can be predicted as a 4 57e 06 δ d 8 83e 07 b 2 6e 03 δ d 2 1 1e 03 δ d 1 0e 04 3 we used the local head loss coefficient ς serving as an evaluation indicator to reflect the energy dissipation influenced by relative roughness and the local head loss coefficient ς decreases gradually with the increase of specific discharge finally tends to be stable 4 we observed the detailed streamline distributions patterns including eddies inside the rough conduit in the laminar flow regime when the relative roughness to the maximum relative roughness of 1 2 4 which can be regarded as a simple porous media model as the roughness asperities on opposite sides of the conduit make contact with each other the streamline will deflect sharply even if the specific discharge is small in the fully developed turbulent flow regime the deflection of streamline and eddy will become even more dramatic than those in the laminar flow regime and the recirculation regions become more obvious credit authorship contribution statement zhongxia li conceptualization software data curation writing original draft junwei wan methodology conceptualization hongbin zhan methodology writing review editing validation linqing he methodology data curation kun huang funding acquisition investigation writing review editing declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgement this study was supported by the national natural science foundation of china grant nos 41402204 the national key research and development program of china no 2018yfc0604202 and the fundamental research funds for national universities china university of geosciences wuhan we would like to thank the associate editor and two anonymous reviewers for their critical and constructive comments which help us greatly improve the quality of the manuscript 
5441,this study provides numerical simulation and experimental evidence of flow through a rough conduit from the perspective of energy loss we have chosen four different types of relative roughness based on the experiment of huang et al 2013b for the numerical simulations the corresponding experimental data were compared to verify the correctness of the numerical simulation the experimental and numerical simulation results both concur that the specific discharge q in the rough conduit is obviously affected by different roughness of conduit when q is somewhat the same a larger relative roughness will lead to a larger hydraulic gradient j however the q j curves obtained experimentally and numerically are both smooth do not show obvious dividing points of transition from darcian flow to non darcian flow we have quantified the energy loss of different rough conduits according to different flow regimes which appear to impose different impacts on energy loss in the laminar flow regime the relationship between q and the energy loss is linear while in the fully developed turbulent flow regime such a relationship becomes quadratic moreover we used the local head loss coefficient ς as an indicator to reflect the energy dissipation influenced by the relative roughness and found that ς decreased gradually with the increase of q and finally tended to an asymptotic constant in addition we have observed the high resolution velocity variation inside the rough conduit and the streamline distribution inside the recirculation region in detail in the laminar flow regime the streamlines deflect but are generally along directions somewhat parallel to the conduit wall when the relative roughness to the maximum relative roughness reaches 1 2 4 the streamline will deflect more sharply even if q is relatively small however in the fully developed turbulent flow regime the deflection of streamline and eddy will be more striking the recirculation region becomes clearly visible even for relatively small q values within the fully developed turbulent flow regime keywords numerical simulation energy loss local head loss inertial force 1 introduction the effect of relative roughness on conduit flow has been studied for more than a century and the subject becomes even more relevant in hydrological sciences because of subsurface flow in karst conduits cheng and chen 2005 jeannin 2001 and flows in long boreholes such as horizontal wells penmatcha et al 1997 su and gudmundsson 1993 during the past decades many scholars carried out relevant experiments to study the influence of relative roughness on fluid flow in conduits where the relative roughness is defined as the ratio of roughness to the conduit hydraulic diameter however the exact nature of the impact of relative roughness on fluid flow has not been completely understood taylor et al 2006 to understand a flow system better characterization of energy loss and surface roughness are two key points of concern fanning 1892 proposed a relationship between pressure drop and surface roughness however a systematic study of quantifying the relationship between surface roughness and pressure drop was conducted by nikuradse 1933 in the experiments reported in nikuradse 1933 he coated the inside walls of conduits with uniform sands and controlled the reynolds numbers re ranging from 600 to 106 in a series of fluid flow experiments according to the experimental results nikuradse 1933 mainly divided the flow regimes into three portions huang et al 2013b a laminar flow regime a transitional flow regime and a fully developed turbulent flow regime the experimental results of nikuradse 1933 have been widely cited subsequently for testing numerous flow theories involving flow in conduits and wellbores brkić and praks 2018 tzelepis et al 2015 woo and brater 1961 one unique result of nikuradse 1933 was that the critical reynolds number rec for the transaction of laminar to fully developed turbulent flow was around 2000 based on the classical work of nikuradse 1933 many scholars have conducted extensive experiments with values of the relative roughness greater than what has been used by nikuradse 1933 and some new phenomena have been discovered brackbill and kandlikar 2008 huang et al 2013a mala and li 1999 tang et al 2007 for instance an earlier transition between flow regimes was observed and the critical reynolds number rec was found to decrease either linearly or exponentially with the increase of relative roughness huang et al 2013b flow in porous media however has long been conceptualized as flow in a bundle of tortuous conduits meandering around solid grains bear 1975 bianchi et al 2011 yeh et al 2015 thus theoretical development of porous media flow is inspired and closely related to that of conduit flow for instance reynolds number re and moody chart which are used commonly for conduit flow phenomena have also been widely utilized in porous media flow such as for determining the deviation from linear to non linear flow regimes li et al 2017 2019 lopik et al 2017 2019 arbhabhirama and dinoy 1973 has systematically analyzed many experiment data and proposed empirical relationship between re and friction factor f in porous media where the friction factor is defined as the darcy friction factor which is a dimensionless term as the ratio of local shear stress to kinetic energy density comparing the results of arbhabhirama and dinoy 1973 with those of nikuradse 1933 we find that the characteristic of the moody chart associated with porous media flow is significantly different from that of conduit flow as reported by nikuradse 1933 for instance the relationship for re versus f for porous media flow is smoother and there are four distinctive stages of flow regimes darcian laminar non darcian laminar transitional and fully developed turbulent flow regimes huang et al 2013b therefore how to quantify the deviation of darcian flow from non darcian flow has become a great concern and different scholars have obtained various critical reynolds number rec arranging from 1 to 1000 in different kinds of porous media setting where rec is associated with the point of transition from darcian flow to non darcian flow bear 1975 bu et al 2014 dybbs and edwards 1984 latifi et al 1989 sedghi asl et al 2014 recently we have carried out a series of conduit and porous media flow experiments in a variety of structures huang et al 2013b li et al 2017 2019 and these experiments have confirmed that there was no obvious point of transition from darcian flow to non darcian flow however a quantitative analysis from an energy or hydraulic head loss perspective along the flow direction about those experimental results have not been done yet where the hydraulic head or head is the energy per unit weight of water a term broadly used for describing subsurface water flow in porous media in addition recirculation and eddy are commonly seen for flow near rough conduit walls chen et al 2017 qian et al 2012 and the influence of recirculation and eddy on the overall conduit flow is still largely unknown furthermore the difference between the laws of energy loss in porous media flow and conduit flow needs to be explained from a more fundamental physical perspective one way of addressing the above mentioned issues is to employ high resolution numerical simulations that are capable of understanding the hydrodynamics of conduit flow and porous media flow from the first principle of physics for instance croce and d agaro 2005 carried out a series of flow models in rough conduits to estimate the magnitude of roughness on microchannel heat transfer and pressure drop in the laminar flow regime where the roughness is randomly generated to mimic a more realistic rough conduit wall rawool et al 2006 placed different shapes of well defined obstacles on the inner surface of conduit walls to investigate the effect of roughness geometry and re on the friction factor f khadem et al 2009 conducted numerical simulations of fluid flow and heat transfer characteristics in rough microchannels with special consideration of flow slip and temperature jump near the rough walls recently chen et al 2017 conducted a serial of experiments and numerical simulations to study the effect of roughness on flow through a single rough fracture and proposed new equations for calculation of friction factor f and specific discharge q huang et al 2013b has carried out a series of comprehensive experiments on pressure drop and friction factor f through conduits with relative roughness ranging from 1 118 75 to 1 3 and eventually to 1 2 4 the experimental apparatus mainly consists of three parts an adjustment valve a test section rough conduit and measurement equipments water was supplied by a pump to the test conduit and the specific discharge was adjusted by the inflow valve after passing through the test conduit water was collected by a tank to minimize errors the specific discharge was observed three times for a certain head drop and the mean value was adopted three piezometer tubes were used to measure the water head drop piezometer tubes were 500 mm away from both ends of the test tube to minimize the inlet and outlet effects meanwhile water temperature was also measured by a thermometer to ensure the isothermal condition during the experiments if the relative roughness is large enough that roughness asperities are touched and connected to each other then the rough conduits model may be regarded as a somewhat simple porous media model specifically when the relative roughness reaches 1 2 4 the roughness asperities on opposite sides of the conduit make contact with each other thus this case is designated as the transaction from a rough conduit model into a simple porous media model the case associated with a relative roughness of 1 2 4 can be visualized directly from fig 3 of huang et al 2013b huang et al 2013b further concluded that the curvature and variable cross section of flow channels for this special case with a relative roughness of 1 2 4 would cause frequent changes and acceleration in the flow direction to generate sufficient inertial forces in porous media flow that may not be seen in conduit flow hellstrom et al 2010 moutsopoulos et al 2009 this explains why non darcian flow in porous media can occur even at relatively low specific discharge or re values as compared to conduit flow in which non darcian flow can only occur at relatively large re number the purpose of this investigation is to carry out a relevant high resolution numerical simulation based on huang et al 2013b experiment to reveal the relationship between the flow characteristics at the pore scale and the specific discharge with different relative roughness and to explore the law of energy loss in different flow regimes to meet the objectives we have firstly chosen four types of conduits with different relative roughness after that a finite volume method fvm was used for simulations whose results are compared with the experimental results of huang et al 2013b finally the specific discharge distribution with different relative roughness and the effects of different flow regimes on energy loss were discussed 2 numerical simulation methodology 2 1 characterization of surface roughness huang et al 2013b have conducted a series of flow experiments in organic glass conduits with different relative roughness δ d using identical acrylic spheres stuck to the inner walls of conduits to create uniform roughness throughout the conduits the roughness generated in huang et al 2013b is different from that in a previous study of huang et al 2013a in which sifted sands are stuck to the inner walls of conduits to generate roughness the problem of using sifted sands to generate roughness is that the conduit morphological difference will increase as the diameter of the increase of the sand as pointed out by huang et al 2013b since the sands are not uniform spheres the surface unevenness of sands increase with the increase of the particle size therefore replacing the sands with acrylic spheres can ensure that the relative roughness of the experimental tube segment remains consistent on the basis of the experiments of huang et al 2013b we chose five different types of δ d values including 1 4 86 1 4 22 1 3 62 1 3 and 1 2 4 in this study it is worth mentioning that the test conduits with δ d values equalling to 1 4 86 1 4 22 1 3 62 and 1 3 had the same inner diameter d of 19 mm while acrylic spheres of 10 mm in diameter and a conduit with an inner diameter of 24 mm were chosen to made test conduit with the maximum relative roughness equal to 1 2 4 because of the axisymmetry of the problem of concern here we simplify the problem to a two dimensional 2d cross section through the center of the conduit as shown in fig 1 a complete three dimensional 3d investigation is computationally extremely expensive and will be saved for a future investigation fig 1 illustrates schematically a circular conduit with periodic roughness placed on its inner wall where d is the hydraulic diameter of the conduit δ is the surface roughness and l is the length of the conduit and it is assumed to be sufficiently long to ensure fully developed flow regimes within the domain of interest 2 2 governing equation generally speaking flow through a conduit is governed by a mass conservation or continuity equation and navier stokes n s equations the n s equation is the law of fluid motion obtained by applying newton s second law and it is the basis for studying flow in conduits when a newtonian fluid with constant density and viscosity flows through a conduit with impermeable walls the n s equation can be written in a vector form as follow chen et al 2017 van golf racht 1982 1 ρ v v μ 2 v p 2 v 0 p γ h where ρ is the fluid density μ is the fluid viscosity p is the hydrodynamic pressure v is the velocity vector designed as v v x v y v z with three components of vx vy and vz along the x y and z directions respectively is the gradient operator γ is the fluid specific weight and h is the hydraulic head or energy per unit weight of the fluid of concern the three terms of the eq 1 represent the inertial forces the term on the left hand side the viscous forces the first term on the right hand side and the pressure forces the second term on the right hand side respectively brush and thomson 2003 assuming that the inertial forces are small enough to be ignored eq 1 can be written as 3 μ 2 v p 0 combining eq 2 with eq 3 forms a system of linear equation called the stokes equation 2 3 numerical simulation four kinds of physical models with different relative roughness values of 1 4 86 1 4 22 1 3 62 1 3 were selected for corresponding numerical models based on the n s equation among many different numerical schemes for solving the n s equation fvm is selected in this study and detailed information about fvm will not be given here but can be found elsewhere leveque 2002 toro 2013 fvm is based on an integral form of the n s equations and is easy to program for parallel computation with high degree of accuracy in this paper the computational region is divided into a series of non repetitive finite control volumes with each control volume represented by a node and the n s equation can be solved for each given control volume chen et al 2017 toro 2013 fvm was operated in fluent 18 0 for related operations which is widely used in computational fluid dynamics cfd to improve computational efficiency the length of the numerical model is less than the length of the laboratory test model but it is long enough to ensure the fully developed turbulent flow regimes within the domain of interest the following procedures are used to decide the conduit length in the simulations first simulations with different conduit lengths and surface roughness can be tried at first and the flow streamlines can be inspected graphically from such a graphic presentation of streamlines the influences of the inlet and outlet boundaries are clearly identifiable meanwhile the development of flow regimes from the inlet to the outlet is also graphically visible second after determining the legnths of influences of the inlet li and outlet lo boundaries and finding out the minimal length l required to encompass the complete evolution of flow from laminar flow to fully developed turbulent flow then the minimal length required in the simulation should be greater than li lo l the 2d flow grids were created using a pre processor gambit to better observe the streamline near the inner wall of the conduit the mesh near the rough inner wall is denser than that near the center of the conduit a quadrilateral mesh is selected and mesh sensitivity is checked for each model to ensure that the numerical simulation result is free from the influence of the chosen mesh sizes overall a total of 292 822 control volumes have been usually used in our simulations constant pressures at inlet and outlet boundaries were maintained in the simulations specifically the inlet and outlet pressures of the numerical model were obtained to ensure that the average hydraulic gradients used in the numerical simulations are the same as those used in experiments by doing so it is possible to compare the numerical simulation results with the experimental results if necessary after the generation of a grid mesh and determination of all the boundary and initial conditions we chose the standard k ε model to simulate the n s equation to obtain the flow fields the k ε model was a two equation model about fully developed turbulent flow proposed by launder and spalding 1972 which included a kinetic energy term k and a turbulent dissipation rate term of ε the equations of kinetic energy term and turbulent dissipation rate term are shown below 4 t ρ k x i ρ k u i x j μ μ t σ k k x j g k ρ ε 5 t ρ ε x i ρ ε u i x j μ μ t σ ε ε x j c 1 ε ε k g k c 2 ε ρ ε 2 k where μ t is the turbulent viscosity g k is the term of turbulent kinetic energy k caused by the average specific discharge gradient eq 5 is a steady state equation there is no time variable in the equation combining eq 4 with eq 5 we can eliminate the transient terms and get the governing equation of the k ε model which is suitable for the numerical simulation according to the recommended value of launder and spalding 1972 the k ε model constants are c 1 ε 1 44 c 2 ε 1 92 c μ 0 99 σ k 1 0 σ ε 1 3 3 results and discussion 3 1 simulation and experimental results comparison numerical simulations were conducted with four different relative roughness of 1 4 86 1 4 22 1 3 62 1 3 to check the accuracy of the simulation we need to compare the simulation results with the experimental results the relationship between q and j was plot in fig 2 to compare the simulation results of this study and the experimental results of huang et al 2013b where q is determined as the ratio of total discharge and the total cross sectional area of a conduit regardless of the surface roughness fig 2 shows that the simulation results fit well with the experimental results when q is somewhat the same a larger relative roughness will lead to a larger j this is because when the relative roughness gets larger the flow resistance becomes stronger thus a larger j is required to attain the same q the simulation results are slightly larger than those of the experimental results in fig 2 but in general they agree with each other very well besides the q j curves obtained experimentally and numerically are both smooth and there were no obvious dividing points of transition from darcian flow to non darcian flow the simulation results based on fluent software are consistent with the experimental results therefore we are confident that the developed simulation scheme is reliable and accurate for demonstrating the features of water flow through rough conduits with relative roughness values ranging from 1 4 86 to 1 3 3 2 effect of roughness on flow characteristics in this series of simulation the same relative roughness 1 3 with four different average specific discharges 0 001 m s 0 04 m s 0 1 m s 0 2 m s were used to observe the change of flow characteristics under different flow states or re values to observe the change of streamline more clearly we have enlarged several portions of streamline profiles between adjacent acrylic spheres stuck on the inner walls of conduits to examine the details see fig 3 we can find from fig 3 that the flow characteristics are obviously different under the same relative roughness due to different specific discharges we can see from the streamline profiles and the partially enlarged streamline profiles that when the average specific discharge is small q 0 001 m s the streamline is almost parallel with each other and there is almost no eddies the streamlines near the solid surfaces of acrylic spheres are somewhat along the surface curvations of those spheres reflecting the control of elements of roughness acrylic spheres on the flow field near those elements streamlines near the central axis of conduit are somewhat along the axis direction of conduit showing that they are only slightly influenced by the actual morphology of the rough elements there is a tendency to generate vortices or eddies at two adjacent rough elements with the increase of specific discharge as can be seen in fig 3 when the specific discharge increases to 0 04 m s one eddy is clearly visible between two adjacent roughness elements when the average specific discharge increases to 0 1 m s two eddies are clearly visible between two adjacent roughness elements when the average specific discharge reaches 0 2 m s two eddies are fully developed between two adjacent roughness elements and a third eddy appears to emerge near the point of contact of two adjacent roughness elements at the inner wall of the conduit a number of interesting observations can be made from fig 3 first it is evident from fig 3 that the magnitudes of flow velocities within the eddies decline when the specific discharges through the rough conduits increases blue color means a smaller velocity and red color means a larger velocity in fig 3 for instance when comparing the case of a larger average specific discharge of q 0 2 m s to that of a smaller average specific discharge of q 0 1 m s flow velocities observed within a specific eddy are obviously smaller than those observed within an eddy at a similar location second when the specific discharge increases the velocity contrastbetween the maximum velocity which occurs near the axis of conduit between two opposing rough element peaks when the flow channel becomes the narrowest and the minimum velocity which occurs near the center of the largest eddy also increases third flow velocity in the central portion of the conduit is non uniform along the axis of the conduit in fact it appears to be periodic controlled by the opposing rough elements closely more specifically if assuming the axis of the conduit is along the horizontal direction then when the vertical distance between two opposing rough elements is shorter the flow velocities are larger vice versa fourth if one examines the flow profile between two opposing rough element peaks closely the flow velocity along the horizontal direction appears to follow a parabolic shape along the vertical direction similar to the classical poiseuille flow profile within a smooth conduit without roughness to look at the flow profile from a different perspective fig 4 examines the influence of different relative roughness 1 4 86 1 4 22 1 3 62 1 3 at a relatively large given average specific discharge of q 0 2 m s this figure shows the detailed velocity variation inside the rough conduit and the streamline distribution inside the recirculation region or eddy a number of interesting observations can be made from fig 4 first this figure shows that the specific discharge in the rough conduit is obviously affected by different roughness and the influence becomes stronger when the asperity height is larger second there is a tendency to generate vortices within two adjacent rough elements when the relative roughness is 1 4 86 as the relative roughness increases to 1 4 22 one significant recirculation region or eddy can be found when the relative roughness further increases to 1 3 62 two recirculation regions or eddies are visible finally when the relative roughness increases to 1 3 three recirculation regions or eddies have been identified third due to the obstruction of roughness elements the streamlines close to the roughness elements would become curved according to the morphology of those elements 3 3 energy loss in different flow regimes observations made in figs 3 and 4 are very valuable for understanding the flow characteristics inside the rough conduit however those observations are qualitative in nature in this section we will quantify the energy loss associated with those observations we can use the local head loss coefficient ς as an evaluation indicator to reflect the energy dissipation influenced by relative roughness the local head loss coefficient depends on the flow geometry and is only related to the cross sectional shape of the conduit such as the ratio of changing cross sectional areas the radius of curvature of bend shape and size of the valve etc to get the local head loss coefficient we need to obtain the head loss of two sections these two sections must meet a specific requirement the selected section should be in fully developed flow regimes within the domain of interest to ensure the consistency and stability of the flow regime after choosing two sections we can obtain the water pressures and specific discharges corresponding to the selected two sections a schematic diagram is shown in fig 5 to illustrate the positions of the selected two sections in this study we selected the distance between the two sections as two complete rough elements or 4δ where δ is the radius of the spherical element shown in fig 5 meanwhile these two sections do not include any surface roughness so their cross sectional areas equal to the cross sectional area of a roughness free or smooth conduit the bernoulli equation which relates the pressure to the local specific discharge and gravity potential segletes and walters 2002 will be used in the following analysis 6 p 1 ρ g z 1 α q 1 2 2 g p 2 ρ g z 2 α q 2 2 2 g δ h where p 1 and p 2 are the average pressures of the selected sections z 1 and z 2 are the average elevations q 1 and q 2 are the average specific discharges of the selected sections α is a kinetic energy correction coefficient which is related to the specific discharge distribution when the specific discharge distribution is uniform α 1 when the specific discharge distribution is not uniform α 1 more detailed explanation about α can be found in li 2012 and osorio and steffe 1984 in general α 1 05 1 10 in the conduit flow li 2012 osorio and steffe 1984 without loss of generality α 1 is used in this study we have also confirmed that the use of α slightly greater than 1 in the range of 1 05 1 10 does not affect the conclusions at all it is interesting to point out that the three terms on the left side of eq 6 respectively represent the pressure head elevation head and kinetic head in section 1 while the first three terms on the right side of eq 6 respectively represent the pressure head elevation head and kinetic head in section 2 therefore δ h on the right side of eq 6 is the total energy loss or total hydraulic head loss between sections 1 and 2 more specifically δ h includes the frictional head loss h l between sections 1 and 2 and the local head loss h m which refers to head loss occurring when water flows around rough elements inside the conduit according to the bernoulli equation and considering that the average elevation of the selected sections is the same z 1 z 2 one can get the total energy loss formula as shown below 7 δ h p 1 p 2 ρ g q 1 2 q 2 2 2 g according to eq 7 we need to get the average pressures and the average specific discharges of the two sections to obtain the energy loss therefore we chose the area weighted average of the pressures and specific discharges of the two sections besides the frictional head loss h l can be ignored because the distance between the selected two sections is short enough the average specific discharge of the selected two sections is the same when the flow reaches stability which means q 1 q 2 therefore the total energy loss δ h is basically equal to the local head loss h m as follow 8 δ h h m p 1 p 2 ρ g conventionally the local head loss is often computed using the average specific discharge of the flow q as h m ς q 2 2 g where ς is termed the local head loss coefficient nicholl and detwiler 2001 sánchez et al 2008 for the problem of concern here the average specific discharges in these two sections are the same when the flow is stable q q 1 q 2 therefore the local head loss coefficient ς is shown as follow 9 ς 2 g h m q 2 2 p 1 p 2 ρ q 2 to investigate the effects of surface roughness on flow in conduits and the characteristics of the local head loss coefficient huang et al 2013b chose the moody chart to plot the experimental data the results are shown in fig 6 we can clearly see from this figure that the experimental curves can divide into three regimes a laminar flow regime a transitional flow regime and a fully developed turbulent flow regime for the illustration purpose we will calculate the energy loss of laminar flow and fully developed turbulent flow and plot the relationship between the energy loss and re for these two flow regimes it is worth mentioning that we chose the standard k ε model to simulate the n s equation to obtain the flow fields the transitional flow regime is more complex than those of laminar flow and fully developed turbulent flow regimes because it involves a continuously evolving state that requires many different approximations to quantify the flow therefore the laminar flow and fully developed turbulent flow are discussed in the following sections as for transitional flow we will conduct a detailed analysis in another study 3 3 1 energy loss in laminar flow according to the experimental data of huang et al 2013b we selected the relevant data of laminar flow and calculated the local head loss according to eq 8 after this step we obtained the relationship between the average specific discharge q and energy loss δ h as shown in fig 7 this figure shows that the energy loss caused by different relative roughness is obviously different in the laminar flow regime when q is somewhat the same a larger relative roughness will lead to more energy loss the relationship between q and δ h is linear shown as below 10 δ h m q where m is the coefficient related to different relative roughness which is shown in table 1 as the distance between two sections is 4δ where δ is the radius of the spherical element shown in fig 5 then the hydraulic gradient between the two sections is j δ h 4δ thus the physical significance of coefficient m in eq 10 is equivalent to a ratio of 4δ over the hydraulic conductivity in darcy s law for laminar flow the relationship between coefficient m and relative roughness was plotted in fig 8 which shows that m is proportional to the relative roughness and the relationship between coefficient m and relative roughness is given as m 2 69e 06 δ d 5 28e 07 meanwhile we can also directly visualize the velocity distribution patterns inside the conduits with different relative roughnesses in the laminar flow regimes in fig 9 in which the average specific discharge is about 0 015 m s from fig 9 we can see that different roughness leads to different velocity distribution patterns under the laminar flow conditions the streamlines are almost parallel to the central axis when the roughness is small 1 4 86 1 4 22 and the velocities at the wall are almost zero then the streamlines near the wall deflect but flow parallel to the wall when the roughness increases gradually 1 3 62 1 3 the continual deflection of the streamlines inevitably leads to additional local head loss therefore we can imagine that when the relative roughness reaches the peak relative roughness of 1 2 4 which can be regarded as a simple porous media model as the roughness asperities on opposite sides of the conduit make contact with each other the streamlines will deflect sharply even if the specific discharge is small leading to significant local head loss 3 3 2 energy loss in fully developed turbulent flow in this section we also chose the relevant data of fully developed turbulent flow and calculated the local head loss accordingly using eq 8 the relationship between q and δ h is shown in fig 10 which demonstrates that the energy loss caused by different relative roughness is different in a fully developed turbulent flow regime when q is somewhat the same a larger relative roughness will lead to more energy loss this is because with the increase of surface roughness δ the vortex generated by the deflection of streamline at the two adjacent rough elements will be larger moreover the curve of energy loss with specific discharge in the fully developed turbulent flow is completely different from that in the laminar flow regime we performed a quadratic best fitting exercise of the curve shown in fig 10 and found that the quadratic curve fits very well as follows 11 δ h a q b q 2 where a and b are coefficients related to different relative roughness which are shown in table 2 once again if needed one can calculate the hydraulic gradient between two sections as the ratio of δ h in eq 11 over the distance between two sections 4δ for fully developed turbulent flow the relationships between coefficients a and b and relative roughness are plotted in fig 11 and fig 12 respectively we can see that a is inversely proportional to the relative roughness while b is a quadratic function of the relative roughness the coefficients in front of the linear a and quadratic b terms can be respectively predicted as a 4 57e 06 δ d 8 83e 07 b 2 6e 03 δ d 2 1 1e 03 δ d 1 0e 04 we have also obtained the velocity distribution patterns of different relative roughness for the fully developed turbulent flow shown in fig 13 in which the specific discharge is about 0 2 m s this figure shows that the effect of relative roughness on velocity distribution is significant in the fully developed turbulent flow regime we find that the streamlines near the central axis deflect sharply with the increase of roughness and additional energy loss is generated during the process of continuous enlargement and reduction of cross section available for flow due to the increase of roughness of the conduit wall in addition we also see from fig 4 that eddies have been generated at two adjacent rough elements therefore the deflection of streamlines will be more intense in those eddies leading to more energy loss at last we used the local head loss coefficient ς serving as an evaluation indicator to reflect the energy dissipation influenced by relative roughness and eq 9 is used to quantify the effect of surface roughness on rough conduit flow the relationship between q and the local head loss coefficient ς is plotted in fig 14 this figure shows that ς decreases gradually with the increase of q and finally tends to a constant value fig 14 shows that the ς values of a rough conduit with different relative roughnesses are different in general when q is somewhat the same a larger relative roughness will lead to a larger ς value the ς value decreases rapidly when q is small with the relative roughnesses in the range of 1 3and1 3 62 then tends to approach an asymptotic constant however the ς value of a rough conduit with a relative roughness of 1 3 is larger than that of a rough conduit with a relative roughness of 1 3 62 therefore when q is somewhat the same a larger relative roughness will lead to larger energy loss 3 3 3 further analysis of energy loss webb et al 1971 observed that flow separated from the surface of the roughness elements and then reattached at a distance 6 8 times the height of the elements in this study we find that the flow pattern consists of two features for laminar flow in a rough conduit including relatively straight flow paths near the central axis area and curved flow paths near the roughness elements huang et al 2013b the curved flow paths will lead to velocity components in other directions rather than the axis direction which means that the inertial force term is not equal to zero however the inertial force term is usually ignored for laminar flow especially when the specific discharge is small therefore the linear or darcian flow is an approximate representation of ignoring the inertial force term li et al 2017 2019 when such an inertial force term becomes sufficiently large to be non negligible then the energy loss will start to deviate from the linear trend signaling the start of non darcain flow when the flow through a rough conduit with the maximum relative roughness of 1 2 4 which can be regarded as a simple porous media model as the roughness asperities on opposite sides of the conduit make contact with each other the curvature and variable cross section of flow channels would cause frequent changes and acceleration in flow directions because of the rough elements even in laminar flow regime which means that the flow could become nonlinear in porous media even at relatively low specific discharge 4 limitations and future study although this study represents a significant advancement of present knowledge of conduit flow with various roughness which may eventually lead to a better understanding of flow in karst aquifer systems and other porous media systems there are a number of limitations that should be pointed out and some future investigations are also highlighted firstly we have simplified the problem to a 2d cross sectional view through the center of the conduit this is based on a few considerations first the periodic arrangement of identical roughness elements used in the experiments of huang et al 2013b and this study makes the use of a 2d approach appealing second the n s equation is very costly to solve from a computational standpoint even for the 2d problem and the computational cost of solving the n s equation for a 3d problem is extremely costly and sometimes forbidden in this regard advanced numerical schemes such as parallel computation in a supercomputing facility may have to be used in the future for 3d flow problems in a rough conduit secondly if a robust 3d simulator can be developed we can then expand the present study for greater relative roughness values that have not been investigated in detail in this study for instance when the maximum relative roughness reaches more than 1 2 4 the roughness asperities on opposite sides of the conduit will completely touch with each other generating a pore system that is somewhat similar to a porous media for such a rough conduit system the 3d description of flow is more appropriate because flow pathways have to move around those touched or connected asperities furthermore random or irregular roughness elements instead of the identical roughness elements used in this study should be utilized to make the problem more relevant to the actual setting thirdly this study only involves a straight conduit with a constant diameter which is obviously for the sake of simplifying the problem and reducing the computational cost it will be very desirable to investigate a meandering conduit with variable cross sectional sizes and shapes in the future to mimic flow in a subsurface karst conduit system nevertheless additional energy loss terms must be taken into consideration when the conduit is meandering and its cross sectional sizes and shapes are changing furthermore if several rough conduits are connected then flow converging diverging and partition issues all have to be considered 5 summary and conclusions this study presents numerical simulation results of different types relative roughness conduits the specific discharge distribution with different relative roughness and the effects of different flow regimes on energy loss were discussed the main conclusions can be summarized as follows 1 the experimental results and numerical simulation results show that the specific discharge q in the rough conduit is obviously affected by different roughness when q is somewhat the same a larger relative roughness will lead to a larger hydraulic gradient j 2 different flow regimes have different effects on energy loss the relationship between the specific discharge and the energy loss is linear in the laminar flow regime and the coefficient m which is the slope of total head loss δ h versus q is proportional to the relative roughness for the experimental data of huang et al 2013b one has m 2 69e 06 δ d 5 28e 07 the relationship between δ h and q becomes quadratic in the fully developed turbulent flow regime moreover for the experimental data of huang et al 2013b the coefficients in front of the linear a and quadratic b terms can be predicted as a 4 57e 06 δ d 8 83e 07 b 2 6e 03 δ d 2 1 1e 03 δ d 1 0e 04 3 we used the local head loss coefficient ς serving as an evaluation indicator to reflect the energy dissipation influenced by relative roughness and the local head loss coefficient ς decreases gradually with the increase of specific discharge finally tends to be stable 4 we observed the detailed streamline distributions patterns including eddies inside the rough conduit in the laminar flow regime when the relative roughness to the maximum relative roughness of 1 2 4 which can be regarded as a simple porous media model as the roughness asperities on opposite sides of the conduit make contact with each other the streamline will deflect sharply even if the specific discharge is small in the fully developed turbulent flow regime the deflection of streamline and eddy will become even more dramatic than those in the laminar flow regime and the recirculation regions become more obvious credit authorship contribution statement zhongxia li conceptualization software data curation writing original draft junwei wan methodology conceptualization hongbin zhan methodology writing review editing validation linqing he methodology data curation kun huang funding acquisition investigation writing review editing declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgement this study was supported by the national natural science foundation of china grant nos 41402204 the national key research and development program of china no 2018yfc0604202 and the fundamental research funds for national universities china university of geosciences wuhan we would like to thank the associate editor and two anonymous reviewers for their critical and constructive comments which help us greatly improve the quality of the manuscript 
5442,the use of diurnal or seasonal water table fluctuation wtf to estimate groundwater evapotranspiration etg at different land uses and climate conditions is increasing applied in ecohydrological studies in this study we applied the wtf method for a shallow aquifer in an urbanized area in senegal over the dry season 2000 2013 to analyze the applicability and validity of the wtf method for this site and to understand the impact of the parameters used in this method the unsaturated saturated system was first simulated using the hydrus 1d model the drawdown of the water table ranges from 18 1 to 113 2 cm and 10 4 to 101 9 cm for a bare soil and a perennial grass scenario and is highly related to the annual rainfall of the previous rainy season the results indicate that the mean daily fao pm reference evapotranspiration rates for this area ranged from 2 to 4 mmd 1 and that the estimated actual evapotranspiration eta from the hydrus 1d model ranged between 0 22 to 1 11 and 0 23 to 1 27 mmd 1 in bare soil and vegetative condition respectively eta and etg were well correlated for the vegetated scenario however the wtf method slightly overestimates eta fluxes in the bare soil scenario the study shows that the decline of eta with water table depth can be simulated by an exponential function the overall results indicate that higher eta values were observed when the water table is shallow suggesting that eta is mainly driven by the water table depth at this site keywords evapotranspiration water table fluctuation semi arid regions unsaturated zone 1 introduction sustainable management of groundwater resources requires detailed information of all components of the water budget such as precipitation storage changes recharge as well as actual evapotranspiration eta different approaches derived from climatic data are available to estimate potential reference evapotranspiration et0 such as the physically based penman monteith pm equation allen et al 1998 simplified pm equation and semi empirical approach introduced by thornthwaite 1948 makkink 1957 priestly taylor 1972 and hargreaves and samani 1985 unfortunately eta can only be calculated based on et0 using physically based models kollett et al 2009 on the other hand eta can be measured directly using the eddy correlation ec method whereby the ec data have to be often corrected e g twine et al 2000 wilson et al 2002 and gap filled e g moffat et al 2007 resulting in errors in the eta estimation additionally the ec method requires complex measurement systems which are often not available at sites under investigation weighable lysimeters as another direct technique for estimating eta are also feasible meissner et al 2007 schrader et al 2013 von unold and frank 2008 but they need complex installation and maintenance it is known that the shallow water table is directly influenced by eta through upward flow e g maxwell and kollet 2008 vanderborght et al 2010 and its temporal fluctuations in many arid semi arid regions express the response to evaporation and plant transpiration in the unsaturated zone loheide et al 2005 ridolfi et al 2007 in these arid and semi arid regions groundwater evapotranspiration etg can be a predominant mechanism of seasonal groundwater fluctuations nichols 1994 healy and cook 2002 accurate estimates of etg is difficult to obtain because of variability in local atmospheric conditions carlson mazur et al 2014 changing groundwater levels ridolfi et al 2006 and spatially heterogeneous land use effects sanderson and cooper 2008 especially the latter might have an important impact on etg estimates if the vegetation cover over the aquifer varies causing different transpiration amounts however this estimation is needed to understand the implications of climate change for water resource management and to develop adaptation strategies carlson mazur et al 2014 the water table fluctuation wtf method based on the premise that changes in the water table of unconfined aquifers are caused by evapotranspiration only healy and cook 2002 lautz 2008 has been widely used to estimate etg rates in arid and semi arid areas e g carlson mazur et al 2014 gribovszki et al 2008 soylu et al 2012 wang et al 2014a b the advantage using this method is that water loss due to evapotranspiration is directly measured through groundwater level changes and no additional instrumentation at the soil surface is therefore needed white 1932 meyboom 1965 gerla 1992 loheide et al 2005 the originally proposed wtf method as introduced by white 1932 was modified by several authors due to uncertainties and deficiencies from different sources e g meyboom 1965 engel et al 2005 gribovszki et al 2008 loheide 2008 whereby the introduction of a term accounting for the specific yield improved the performance of the white method substantially the specific yield which is defined as the average amount of water per unit volume of soil drained from a soil column extending from the water table to the ground surface per unit lowering of the water table sophocleous 1985 is highly variable in shallow water table aquifers and it depends on soil texture water table depth and rate of change duke 1972 sophocleous 1985 healy and cook 2002 loheide 2008 gribovszki et al 2010 in general a water level located below the rooting zone is a limitation for the application of the wtf method mould et al 2010 other factor influencing the accuracy of etg estimation by the wtf methods involves the inherent assumption that groundwater recovery rates are constant over time despite that changes might occur over the course of the day which are related to changing evapotranspirative demand troxell 1936 because large part of the eta occurs through transpiration root water uptake etg depends on the actual vegetation cover and its status bethenod et al 2000 hsiao and xu 2005 wilson et al 2000 finally application of the wtf method during rainy season is problematic because of the rapid rise in water table elevation and subsequent percolation losses to avoid storage changes within the unsaturated zone due to precipitation many authors have chosen the dry periods to estimate etg wilson et al 2000 mould et al 2010 as mentioned above etg under none influenced system is mainly related to soil evaporation and the direct water withdrawal by plants however in urbanized and agricultural zones daily and seasonal groundwater fluctuations may be impacted by human activities such as groundwater extraction for drinking or irrigation water this water extraction from the quaternary sandy aquifer located in the region of dakar had played a major role in supplying drinking and irrigation water in the past whereby the use of the groundwater started in the 1950s with periods of higher and lower exploitation ranging from 15 000 to 1 300 m3 per day within the last years pumping was abandoned due to nitrate pollution resulting from improper sanitation system in the urbanized area on the other hand the municipal authorities currently discuss to allow pumping again to satisfy the water supply of gardening in the peri urban area therefore the objective of this study is to estimate the daily evapotranspiration rates to get knowledge about potential groundwater recharge and potential sustainable future extraction capacities to do so the etg rates form the wtf method were calculated and compared with actual evapotranspiration data eta calculated by the physically based hydrus 1d model for different surface covers namely bare soil and savanna type grass vegetative in western senegal 2 materials and methods 2 1 study area the dakar region extends over 550 km2 between longitudes 16 55 and 17 30 west and latitudes 14 55 and 14 35 north fig 1 the region is characterized by a semi arid climate with a rainy season occurring from june to october annual rainfall varies strongly between the years being for example 150 mm in 1983 and 723 mm in 2009 while the long term mean is 410 mm 1961 1990 maximum air temperature is on average 29 5 c 1980 2010 and occurs from may to june and october to november corresponding to the beginning and the end of the rainy season minimum air temperature is observed during the period from december to february 18 5 c daily mean fao pm reference evapotranspiration estimated between 2000 and 2013 ranged between 2 and 4 mm d 1 geologically the dakar region belongs to the senegalese mauritanian basin the largest coastal basin of northwest africa castalain 1965 which is covered by quaternary sediments of sandy and sandy clay nature from alluvial and eolien deposits bellion 1987 this quaternary sandy formation constitutes the groundwater reservoir in the dakar region and it overlay the impermeable marly sediments of the eocene the top formations are made up by unleached tropical ferruginous soils locally called soil diors that represent 80 of the total area maignien 1959 additionally hydromorphic and holomorphic soils are often located around the coastal lagoons these soils are characterized by a significant presence of organic matter in the surface layer 2 2 soil sampling field data and laboratory measurements soil sampling was performed in the dakar area in may 2015 whereby undisturbed soil cores using kopecky rings of 250 cm3 height 5 cm diameter 8 4 cm were taken in the uppermost three soil horizons from the sampling depth of 0 25 with n 3 25 100 n 3 and 100 200 cm n 4 and transferred to the forschungszentrum jülich gmbh germany for analysis for the estimation of the hydraulic properties mualem van genuchten parameters van genuchten 1980 the hyprop meter group münchen germany method as described by schindler et al 2010 was used in combination with the wp4 dewpoint potentiometer decagon devices wa usa the saturated hydraulic conductivity was estimated independently at the same samples using the falling head method by the ksat device meter group münchen germany previously unsaturated zone characterization was investigated in this study area using a hand auger device up to a depth of 300 cm soil samples were collected at 0 25 cm interval down to 1 m depth and beyond in 50 cm intervals down to the water table which was at 250 cm depth samples were bulked and homogenized for each depth and gravimetric soil water contents were determined by drying 20 g of the sample at 110 c for 4 h additionally particle size distributions were performed via sieve analysis 50 μm fraction groundwater level data in piezometer p3 1 located close to the investigated site fig 1 were recorded using thalimede orpheus mini recorders ott hydromet gmbh germany during time period 2010 to 2012 in order to determine groundwater fluctuations over two dry seasons the vegetation surrounding the groundwater well was grass vegetation but also bare patches were detectable whereby the vegetation also depends on dry or wet season 2 3 wtf method the wtf method is usually applied in arid and semi arid environments to estimate actual evapotranspiration fluxes by analyzing groundwater level changes in unconfined aquifers white 1932 healy and cook 2002 the etg derived from this method can be expressed as wang et al 2014a b 1 et g s y δ h δ h δ t where s y is the specific yield δh is the change of the groundwater level over the corresponding period cm δh is the lateral flow cm d 1 and δt is the time interval day the overall change of the water table head δh is assumed to equal the overall water table change at the well between the beginning and the end of the dry season in general lateral flow δh can be estimated by using the slope of the groundwater table curve before and after the growing season in our region lateral flow δh can be neglected because the aquifer is located in a plain and because lateral flow is less important at the seasonal scale as pointed out by pozdniakov et al 2013 therefore eq 1 can be simplified to 2 et g s y δ z y δ t where δzy is the seasonal decline of the groundwater table crosbie et al 2005 introduced the apparent specific yield sya term to evaluate the dynamics of the specific yield the apparent specific yield can be calculated by the knowledge of the van genuchten parameters van genuchten 1980 by 3 s ya s yu s yu 1 z i z f 2 n 1 1 n s yu θ s θ r where θ s and θ r are the saturated and residual water contents cm3cm 3 z i and z f are the initial and final depth to the water table cm and α cm 1 and n are the van genuchten parameters eq 4 is used to calculate the average apparent specific yield s ya when the groundwater table fluctuates in m soil layers 4 s ya i 1 m δ h i s yi i 1 m δ h i i 1 2 m where s yi is the specific yield of the corresponding soil layer and δ h i is the amplitude of the groundwater level fluctuation in the i th layer cm 2 4 numerical modeling for the simulation of vertical water flow the one dimensional richards equation eq 5 was solved using the finite element code hydrus 1d šimůnek et al 2008 šimůnek and van genuchten 2008 5 θ h t z k h h z 1 s where θ is the volumetric water content cm3 cm 3 h is the pressure head cm z represents the vertical coordinate cm positive in the downward direction and k h is the unsaturated hydraulic conductivity as a function of pressure head cm d 1 the sink term s in eq 5 describes the volume of water removed from a unit volume of soil due to plant water uptake and is defined by feddes et al 1978 as 6 s h α h s p where α h is the root water uptake function and sp is the potential water uptake rate šimůnek et al 2008 the mualem van genuchten functions van genuchten 1980 were used to describe the relationship between θ k and h with 7 θ h θ r θ s θ r 1 α h n m 8 k h k s s e 0 5 1 1 s e 1 m m 2 9 s e θ θ r θ s θ r where se is the effective saturation cm3 cm 3 θ r and θ s cm3 cm 3 are the residual and saturated volumetric water contents respectively α cm 1 n and m m 1 1 n are shape parameters and ks cm d 1 is the saturated hydraulic conductivity for the inverse modeling of the hyprop data the model was setup in the same dimensions as the laboratory column and the flux over the upper surface normalized weight loss was imposed at top boundary condition initialization was in pressure head with hydrostatic equilibrium from the lowest node h bottom 0 cm for the inversion the implemented levenberg marquardt algorithm was used for the estimation of all hydraulic parameters of eqs 7 and 8 eta was simulated using hydrus 1d with the estimated hyprop hydraulic parameters for the dakar region whereby the simulation domain was assumed to be 2300 cm deep assuming that the largest fraction of the domain was fully water filled lowest 2000 cm and acts as a groundwater reservoir which will fluctuate over depth as a system response to infiltration and evapotranspiration at the upper boundary atmospheric inputs for daily precipitation and potential evapotranspiration were used from the years 2000 2013 the soil hydraulic parameters for each layer were taken from the inversion of the hyprop data the groundwater hydraulic parameters were assumed to be a continuation of layer 3 because we considered that eta is more influenced by the upper than the deepest layers the ratio of potential evaporation to transpiration was calculated based on the approach introduced by šimůnek et al 2008 where the potential transpiration tpot and the potential evaporation epot can be separated by the knowledge of the leaf area index lai 10 t pot e t 0 1 e k l a i 11 e pot e t 0 e k l a i where et0 is the reference evapotranspiration cm k is a parameter that governs the radiation extinction of the canopy which depends on the sun angle the distribution of plants and the arrangement of leaves here we used k 0 49 as a representative value for grassland šimůnek et al 2008 and lai was taken from bobée et al 2012 over the growing season an overview of the atmospheric forcing s precipitation potential evapotranspiration and lai is shown in figs 3 and 4 lai varies between 0 27 to 0 8 m2m 2 in natural herbaceous savanna type vegetation closed to open 40 rainfall records were characterized by high frequency and intensity events producing high flooded area in the years 2005 2008 2009 2010 and 2013 for example in august 1st to 2nd 13st to 19st 22nd to 25th and 28th to 30th of 2005 rainfall recorded equal 63 9 108 9 97 3 and 49 21 mm respectively this situation causes flooding of many districts in the peri urban area where groundwater level is highly related to rainfall events diouf et al 2013 maximum daily et0 calculated by fao pm was observed during the dry season when the study area is influenced by the ne sw hot and dry wind locally called harmattan minimum values occur during the rainy season characterized by high air humidity 70 root depth was set to be 100 cm according to february and higgins 2010 and due to the perennial grassland vegetation no root growth was assumed the entire domain was initialized in pressure head with an initial groundwater table at 250 cm and hydrostatic equilibrium with the groundwater table in the unsaturated zone in total the simulation time was 22 years and only the years 2000 to 2013 were taken for analysis to be independent from initialization this kind of spin up is generally used in many modelling exercises to be independent of the initial soil water conditions in the soil profile as shown by e g boesten 2007 or weihermüller et al 2011 first the eta was calculated using hydrus 1d values were then compared to etg based on the wtf method calculated from the groundwater fluctuations simulated by hydrus 1d for the modelling of the groundwater level fluctuations the van genuchten parameters obtained from inverse simulation of the hyprop data were used table 1 and the root water uptake by the plants was calibrated in way that observed and modeled groundwater fluctuations match each other as close as possible in this respect the root zone depth for the perennial grassland and the parameter of critical water stress index for root water uptake were adapted as proposed by šimůnek and hopmans 2009 in a second step modeled hydrus 1d derived etg were compared to calculations based on the wtf method from measurements of the piezometer p3 1 2 5 calculation of the groundwater decoupling depth the decoupling depth d defines the depth of the water table where the actual evaportranspiration eta becomes lower as the potential evapotranspiration et0 because the water demand of the atmosphere cannot be delivered totally from the groundwater reservoir anymore to estimate the decoupling depth d and the decay coefficient b of the exponential function the decline of the ratio eta et0 with the water table depth was fitted by eq 12 to the eta and water table depth pairs according to shah et al 2007 12 et a et 0 f a o p m 1 f o r w t d d y 0 e b w t d d f o r w t d d where eta and et0 fao pm are the actual and reference evapotranspiration cm respectively y0 is the so called correction factor and d and b are constants and wtd is the water table depth cm 2 6 statistical analysis statistical analysis was performed to compare measured and modeled groundwater levels and the corresponding calculated etg rates to indicate the goodness of the correlation the pearsons correlation coefficient r and the root mean squared error rmse were used the coefficient of determination r2 as the square of the pearsons correlation coefficient was used to describe the goodness of fit for the regression analysis in general the rmse can range from 0 to infinity and lower values indicate better agreement between the two data sources willmot 1981 regression analysis were used to explain the relationship between eta and water table depth wtd for dry seasons of the years 2001 to 2013 3 results and discussion 3 1 soil texture and profile water content the measured soil texture up to a depth of 300 cm is plotted in fig 2 as can be seen the texture is dominated by 57 of fine sand 100 200 μm and 14 of medium sand 200 250 μm diouf 2012 the measured gravimetric water contents are plotted in fig 2a and were nearly constant between 0 and 200 cm depth with only 4 to 6 and increase beyond to up to 18 at 300 cm depth the low gravimetric water contents between 0 and 200 cm could be expected because sampling took place in the dry season of the year 2008 additionally water content and percentage of fine particles exhibit a similar profile pattern fig 2 except in the top zone likely due to evapotranspiration losses of soil water 3 2 soil hydraulic properties and specific yield accurate knowledge of soil hydraulic properties is required in studying water flow in the unsaturated zone to obtain the hydraulic properties the hyprop data were inverted as described in the materials and methods section the best parameters of the inversion for the different soil layers are listed in table 1 as displayed the saturated hydraulic conductivity of the upper layer 0 25 cm is 570 cm d 1 and slightly decreases to 504 cm d 1 in the middle layer 25 100 cm and 461 cm d 1 in the third layer 100 200 cm respectively saturated and residual water content values ranged between 0 42 to 0 45 cm3 cm 3 and 0 0011 to 0 0062 cm3 cm 3 the shape parameters α and n were also in the same order of magnitude in the three layers it has to be noted that the soil layers were almost homogeneous without obvious small scale layering at least from visual inspection of the soil augers the estimating of water budget components such as groundwater recharge and evapotranspiration requires also accurate determination of the specific yield which is a crucial term in the wtf method chen et al 2010 fahle and dietrich 2014 the specific yield values calculated using eqs 3 and 4 varied between 0 29 and 0 33 depending on the years analyzed this is in good agreement with data presented by loheide et al 2005 who reported specific yields of 0 32 when the depth to the water table exceeds 1 m 3 3 temporal variability in groundwater level and et 3 3 1 seasonal groundwater level changes fig 5 shows the hydrus 1d simulated groundwater levels for two different scenarios where the soil was either assumed to be bare or covered by short perennial grass both states bare and vegetated are observed in the vicinity of the monitoring piezometer additionally water table fluctuations measured in the piezometer are plotted for the years 2010 to 2012 as can be seen from the hydrus 1d modelled groundwater table fluctuations each dry season leads to a pronounced drop of the groundwater table and a relaxation in the corresponding wet season whereby the drop nearly showed same levels for each year independently of the maximum level of the groundwater table during the previous rainy season looking at the match between the modelled and measured groundwater tables for the years 2010 to 2012 indicates that the model results based on the assumption that the soil was bare is less good compared to the vegetated scenario for better comparison the modelled versus measured groundwater levels for the dry season are plotted in fig 6 for the bare soil assumption the r is 0 96 with and rmse of 0 26 m on the other hand for the grassland vegetation a slightly higher r 0 98 and a smaller rmse of 0 12 m indicates that the grassland vegetation scenario is more adequate to describe the groundwater fluctuations in this area 3 3 2 simulated hydrus 1d eta and etg the actual et values obtained with the two methods namely the direct eta output from hydrus 1d and the etg calculated by the wtf method based on simulated hydrus 1d water table fluctuations were compared in a first step to analyze the applicability and validity of the wtf method for this site table 2 presents result for each individually simulated dry season with regard to the cumulative rainfall from the previous rainy season the total length of the dry season and the maximum difference in groundwater level between wet and dry season data reveal that precipitation amounts vary greatly with a minimum of 272 mm in 2007 and a maximum of 723 mm in 2009 with a mean of 475 mm over all years length of the dry season also varies between 145 2002 and 271 2013 days mean 229 days drawdown varies as well and ranges from 18 1 2002 to 113 2 2013 cm mean 60 cm for the bare soil scenario and from10 4 2002 and 101 9 2013 cm mean 54 4 cm for the perennial grass scenario whereby no significant correlation p 0 05 between length of the dry period and water table decline was observed the generally lower values of the drawdown for the perennial grass scenario are probably caused by the generally lower water table during the wet season due to root water uptake during the vegetation period and a deeper groundwater table during the dry season hydrus 1d based eta varies for the bare soil scenario between 0 22 2003 and 1 11 2013 mm d 1 with a mean of 0 60 mm d 1 and for the perennial grass vegetation scenario between 0 23 2002 and 1 28 2011 mm d 1 with a mean of 0 78 mm d 1 higher eta in this latter scenario can be explained by greater water extraction due to root water uptake especially from deeper soil layers because of variability in dry season length total cumulative eta mm varies accordingly the total evaporation loss during the dry season with respect to the rainfall recorded in the previous rainy season varies from 10 in 2002 to more than 47 in 2013 mean 28 suggesting potential groundwater recharge variability table 3 looking at the results of etg calculated by the wtf method reflects the same pattern as for the eta whereby slightly higher etg mean 0 23 mm d 1 compared to eta was calculated for the bare soil scenario on the other hand calculated eta and etg matched each other well for the vegetated scenario with only a very small underestimation of etg mean 0 04 mm d 1 mean etg over the entire dry periods was estimated to be 0 82 and 0 73 mm d 1 for the bare and vegetated case respectively which is 37 and 6 5 deviation from the direct hydrus 1d calculated eta the reason for the slightly larger differences between etg and eta for the bare soil scenario are unclear especially that measurement errors can be excluded in the synthetic case shown therefore one can question if the definition of the specific yield as shown in eq 3 and 4 can be used globally on the other hand one can question why the use of the same specific yield seems not to impact the vegetated scenario here one can speculate that the additional water extraction form the root zone by the plants somehow compensate the incorrectly defined specific yield for a better insight into the direct comparison between eta and etg both values were plotted versus each other in fig 7 for both scenarios overall a good agreement between both methods can be detected with high r2 values exceeding 0 98 whereby a general overestimation of the etg for the bare soil conditions is again visible resulting in an rmse of 0 23 mm d 1 in comparison and as already stated the etg and eta match well over the entire eta range for the vegetated scenario rmse 0 12 mm d 1 which gives confidence that the wtf method can be used to estimate actual evaporation during the dry season from piezometer data only in the study area when vegetation is present the calculated daily eta 0 23 1 28 mm d 1 and etg 0 17 1 20 mm d 1 values in our study area were lower than other etg estimates for vegetated areas where the wtf method was also applied for example mould et al 2010 estimated maximum etg rates of 5 91 mm d 1 in northern germany where the water table depth ranged only between 0 1 and 0 6 m indicating that not only the atmospheric demand gribovszki et al 2008 but also the total water table depth the soil properties of the vadose zone as well as vegetation type determine the absolute evapotranspiration cleverly et al 2006 cooper et al 2006 devitt et al 2011 however our evapotranspiration values are comparable with estimated etg rates in semi arid and arid environments where the groundwater table is in general much deeper pozdniakov et al 2013 and lautz 2008 obtained etg values from 0 1 to 2 mm d 1 and 0 0 to 3 1 mm d 1 for short grass prairie for water table depth ranged between 1 5 to 4 m and 1 to 3 m respectively in a hyper arid environment of northwestern china wang et al 2014a b and cheng et al 2017 obtained similar etg values 0 63 to 2 33 mm d 1 from 0 5 to 3 5 m water table depth the results presented in this study are consistent with those presented by gardner 1958 who show that evaporation demand is mainly controlled by external condition and water table depth 3 3 3 etg from piezometer data and hydrus 1d simulation daily etg values estimated from measured groundwater levels in the piezometer and the wtf method varies between 0 84 2012 and 1 34 mm d 1 2011 with a mean of 1 10 mm d 1 etg of the year 2011 is higher than the eta modeled in bare soil condition 29 mismatch but is in the same range as the eta modeled in the vegetated scenario assuming a perennial grass cover 5 mismatch however in 2012 the difference between eta and etg is much larger with nearly three fold etg compared to the bare soil eta and an overestimation of 42 for the vegetated eta the mismatch for the year 2012 can be attributed to the fact that the measured groundwater fluctuations were not recorded for the entire dry season and ended already at april 7th 2012 even if only one dry season was completely covered by the measurements confidence in the wtf method for this study area is given by the synthetic model study using the hydrus 1d simulations 3 3 4 relationship between eta et0 and wtd the daily potential evapotranspiration et0 is only poorly related to daily eta r2 0 08 for the two scenarios bare and vegetated but in general higher daily eta rates correspond to higher daily et0 and modeled eta equals the et0 fao pm when the groundwater table is shallow corresponding to the end of the rainy season october november the same has been already shown by kurc and small 2004 who found that in semi arid ecosystems the maximum eta rate during the dry season occurs immediately after the rainy season and then rapidly decreases within days of no additional rainfall in a next step simulated eta data were plotted against the water table depth wtd for each single year for further analysis fig 8 clearly shows that eta values are highly related to water table depth and could be described by an exponential function with an r2 0 80 over all years largest regression between eta and wtd occurred in the years 2001 r2 0 84 2006 r2 0 83 2009 r2 0 85 2010 r2 0 73 2011 r2 0 88 and 2013 r2 0 84 where the wtd was most shallow due to the large precipitation amounts in the previous rainy season but also spans a more or less wide range on the other hand the relationships was poor for the year 2002 r2 0 23 and 2003 r2 0 35 where the wtd was low and nearly no fluctuations were detectable in conclusion eta seems to be mainly driven by the water table depth at this site the influence of wtd in evapotranspiration demand is also noted by o connor et al 2019 and gao et al 2017 in a study conducted over different land covers fig 9 shows the plot of eta rates vs wtd for the simulation with perennial grass cover and for all years the simulated eta was normalized by the et0 fao pm therefore the ratio eta et0fao pm varied only between 0 and 1 as can be seen the eta is equal to potential evapotranspiration until the water table reaches a depth d defined as the decoupling depth by shah et al 2007 up to this critical depth the aquifer will provide most part of water for eta in our case the decoupling depth equal 1 46 m beyond this point eta shifts from atmospheric control eta is equal to et0 to soil moisture control of the vadose zone again below the transition depth eta decreased with decreasing wtd in an exponential way the same observations was shown in the previous works of shah et al 2007 who proposed an exponential relationship for evaporation from a shallow water table from different land covers and soils first eq 14 was used without tacking into account the correction factor y0 to simulate the decline of eta with decline in wtd in the fitting process the decoupling depth was assuming to equal 1 46 m as shown in fig 9 the exponential function fitted well the data for shallow water table but the fit was poor with an r2of 0 59 and a rmse of 0 121 m for wtd d introducing a correction factor enhanced the fitted model r2 0 78 rmse 0 035 m the correction factor y0 and the decay coefficient b values obtained in this study are different than those obtained by shah et al 2007 in grass cover and for sandy soils probably due to differences in decoupling depth and soil characteristics 4 conclusion daily groundwater level fluctuations were observed during the dry season in dakar area the modelled data showed groundwater table drawdown from 18 1 to 113 2 cm and 10 4 to 101 9 cm for bare soil and perennial grass scenario respectively the results also indicate that each dry season leads to a pronounced drop of the groundwater table and this drop nearly showed same levels for each year independently of the maximum level of the groundwater table during the previous rainy season additionally dry season eta as an important component of the annual water balance consumed 10 to 43 of annual rainfall for this site modeled daily groundwater levels were first used to quantify etg and to analyze the applicability and validity of the wtf method for this site modelled actual evapotranspiration eta ranged between 0 22 and 1 28 mm d 1 the comparison of eta and etg values obtained for the vegetated scenario show that the wtf method can be used to estimate actual evaporation during the dry season from piezometer data only in the study area furthermore daily eta is directly coupled to the water table depth wtd whereby shallower wtd increased eta irrespectively of the atmospheric demand over all years the dependence between eta and wtd could be well described r2 0 80 by an exponential function the simulated eta model shows that higher eta values occurred during the period from october to december and lowers values were observed at june corresponding respectively to the ends of the rainy and the dry season groundwater evapotranspiration estimation based on the modeling and wtf methods may be considered the simplest easiest and least expensive technique available but the method involves a number of sources of uncertainty soylu et al 2012 in addition a reliable quantification of actual evapotranspiration requires a comprehensive investigation that combines the modeling approach with traditional methods such as weighable lysimeters and eddy correlation method credit authorship contribution statement ousmane coly diouf writing original draft writing review editing data curation conceptualization methodology software lutz weihermüller conceptualization formal analysis writing review editing funding acquisition writing original draft software mathias diedhiou visualization harry vereecken supervision resources funding acquisition seynabou cissé faye visualization sérigne faye writing review editing funding acquisition samba ndao sylla visualization declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgement this study was financially supported by a post doctoral grant from the german academicexchange service daad to ousmane coly diouf at forschungszentrum jülich gmbh agrosphere institute ibg 3 the authors are grateful to the daad for this funding 
5442,the use of diurnal or seasonal water table fluctuation wtf to estimate groundwater evapotranspiration etg at different land uses and climate conditions is increasing applied in ecohydrological studies in this study we applied the wtf method for a shallow aquifer in an urbanized area in senegal over the dry season 2000 2013 to analyze the applicability and validity of the wtf method for this site and to understand the impact of the parameters used in this method the unsaturated saturated system was first simulated using the hydrus 1d model the drawdown of the water table ranges from 18 1 to 113 2 cm and 10 4 to 101 9 cm for a bare soil and a perennial grass scenario and is highly related to the annual rainfall of the previous rainy season the results indicate that the mean daily fao pm reference evapotranspiration rates for this area ranged from 2 to 4 mmd 1 and that the estimated actual evapotranspiration eta from the hydrus 1d model ranged between 0 22 to 1 11 and 0 23 to 1 27 mmd 1 in bare soil and vegetative condition respectively eta and etg were well correlated for the vegetated scenario however the wtf method slightly overestimates eta fluxes in the bare soil scenario the study shows that the decline of eta with water table depth can be simulated by an exponential function the overall results indicate that higher eta values were observed when the water table is shallow suggesting that eta is mainly driven by the water table depth at this site keywords evapotranspiration water table fluctuation semi arid regions unsaturated zone 1 introduction sustainable management of groundwater resources requires detailed information of all components of the water budget such as precipitation storage changes recharge as well as actual evapotranspiration eta different approaches derived from climatic data are available to estimate potential reference evapotranspiration et0 such as the physically based penman monteith pm equation allen et al 1998 simplified pm equation and semi empirical approach introduced by thornthwaite 1948 makkink 1957 priestly taylor 1972 and hargreaves and samani 1985 unfortunately eta can only be calculated based on et0 using physically based models kollett et al 2009 on the other hand eta can be measured directly using the eddy correlation ec method whereby the ec data have to be often corrected e g twine et al 2000 wilson et al 2002 and gap filled e g moffat et al 2007 resulting in errors in the eta estimation additionally the ec method requires complex measurement systems which are often not available at sites under investigation weighable lysimeters as another direct technique for estimating eta are also feasible meissner et al 2007 schrader et al 2013 von unold and frank 2008 but they need complex installation and maintenance it is known that the shallow water table is directly influenced by eta through upward flow e g maxwell and kollet 2008 vanderborght et al 2010 and its temporal fluctuations in many arid semi arid regions express the response to evaporation and plant transpiration in the unsaturated zone loheide et al 2005 ridolfi et al 2007 in these arid and semi arid regions groundwater evapotranspiration etg can be a predominant mechanism of seasonal groundwater fluctuations nichols 1994 healy and cook 2002 accurate estimates of etg is difficult to obtain because of variability in local atmospheric conditions carlson mazur et al 2014 changing groundwater levels ridolfi et al 2006 and spatially heterogeneous land use effects sanderson and cooper 2008 especially the latter might have an important impact on etg estimates if the vegetation cover over the aquifer varies causing different transpiration amounts however this estimation is needed to understand the implications of climate change for water resource management and to develop adaptation strategies carlson mazur et al 2014 the water table fluctuation wtf method based on the premise that changes in the water table of unconfined aquifers are caused by evapotranspiration only healy and cook 2002 lautz 2008 has been widely used to estimate etg rates in arid and semi arid areas e g carlson mazur et al 2014 gribovszki et al 2008 soylu et al 2012 wang et al 2014a b the advantage using this method is that water loss due to evapotranspiration is directly measured through groundwater level changes and no additional instrumentation at the soil surface is therefore needed white 1932 meyboom 1965 gerla 1992 loheide et al 2005 the originally proposed wtf method as introduced by white 1932 was modified by several authors due to uncertainties and deficiencies from different sources e g meyboom 1965 engel et al 2005 gribovszki et al 2008 loheide 2008 whereby the introduction of a term accounting for the specific yield improved the performance of the white method substantially the specific yield which is defined as the average amount of water per unit volume of soil drained from a soil column extending from the water table to the ground surface per unit lowering of the water table sophocleous 1985 is highly variable in shallow water table aquifers and it depends on soil texture water table depth and rate of change duke 1972 sophocleous 1985 healy and cook 2002 loheide 2008 gribovszki et al 2010 in general a water level located below the rooting zone is a limitation for the application of the wtf method mould et al 2010 other factor influencing the accuracy of etg estimation by the wtf methods involves the inherent assumption that groundwater recovery rates are constant over time despite that changes might occur over the course of the day which are related to changing evapotranspirative demand troxell 1936 because large part of the eta occurs through transpiration root water uptake etg depends on the actual vegetation cover and its status bethenod et al 2000 hsiao and xu 2005 wilson et al 2000 finally application of the wtf method during rainy season is problematic because of the rapid rise in water table elevation and subsequent percolation losses to avoid storage changes within the unsaturated zone due to precipitation many authors have chosen the dry periods to estimate etg wilson et al 2000 mould et al 2010 as mentioned above etg under none influenced system is mainly related to soil evaporation and the direct water withdrawal by plants however in urbanized and agricultural zones daily and seasonal groundwater fluctuations may be impacted by human activities such as groundwater extraction for drinking or irrigation water this water extraction from the quaternary sandy aquifer located in the region of dakar had played a major role in supplying drinking and irrigation water in the past whereby the use of the groundwater started in the 1950s with periods of higher and lower exploitation ranging from 15 000 to 1 300 m3 per day within the last years pumping was abandoned due to nitrate pollution resulting from improper sanitation system in the urbanized area on the other hand the municipal authorities currently discuss to allow pumping again to satisfy the water supply of gardening in the peri urban area therefore the objective of this study is to estimate the daily evapotranspiration rates to get knowledge about potential groundwater recharge and potential sustainable future extraction capacities to do so the etg rates form the wtf method were calculated and compared with actual evapotranspiration data eta calculated by the physically based hydrus 1d model for different surface covers namely bare soil and savanna type grass vegetative in western senegal 2 materials and methods 2 1 study area the dakar region extends over 550 km2 between longitudes 16 55 and 17 30 west and latitudes 14 55 and 14 35 north fig 1 the region is characterized by a semi arid climate with a rainy season occurring from june to october annual rainfall varies strongly between the years being for example 150 mm in 1983 and 723 mm in 2009 while the long term mean is 410 mm 1961 1990 maximum air temperature is on average 29 5 c 1980 2010 and occurs from may to june and october to november corresponding to the beginning and the end of the rainy season minimum air temperature is observed during the period from december to february 18 5 c daily mean fao pm reference evapotranspiration estimated between 2000 and 2013 ranged between 2 and 4 mm d 1 geologically the dakar region belongs to the senegalese mauritanian basin the largest coastal basin of northwest africa castalain 1965 which is covered by quaternary sediments of sandy and sandy clay nature from alluvial and eolien deposits bellion 1987 this quaternary sandy formation constitutes the groundwater reservoir in the dakar region and it overlay the impermeable marly sediments of the eocene the top formations are made up by unleached tropical ferruginous soils locally called soil diors that represent 80 of the total area maignien 1959 additionally hydromorphic and holomorphic soils are often located around the coastal lagoons these soils are characterized by a significant presence of organic matter in the surface layer 2 2 soil sampling field data and laboratory measurements soil sampling was performed in the dakar area in may 2015 whereby undisturbed soil cores using kopecky rings of 250 cm3 height 5 cm diameter 8 4 cm were taken in the uppermost three soil horizons from the sampling depth of 0 25 with n 3 25 100 n 3 and 100 200 cm n 4 and transferred to the forschungszentrum jülich gmbh germany for analysis for the estimation of the hydraulic properties mualem van genuchten parameters van genuchten 1980 the hyprop meter group münchen germany method as described by schindler et al 2010 was used in combination with the wp4 dewpoint potentiometer decagon devices wa usa the saturated hydraulic conductivity was estimated independently at the same samples using the falling head method by the ksat device meter group münchen germany previously unsaturated zone characterization was investigated in this study area using a hand auger device up to a depth of 300 cm soil samples were collected at 0 25 cm interval down to 1 m depth and beyond in 50 cm intervals down to the water table which was at 250 cm depth samples were bulked and homogenized for each depth and gravimetric soil water contents were determined by drying 20 g of the sample at 110 c for 4 h additionally particle size distributions were performed via sieve analysis 50 μm fraction groundwater level data in piezometer p3 1 located close to the investigated site fig 1 were recorded using thalimede orpheus mini recorders ott hydromet gmbh germany during time period 2010 to 2012 in order to determine groundwater fluctuations over two dry seasons the vegetation surrounding the groundwater well was grass vegetation but also bare patches were detectable whereby the vegetation also depends on dry or wet season 2 3 wtf method the wtf method is usually applied in arid and semi arid environments to estimate actual evapotranspiration fluxes by analyzing groundwater level changes in unconfined aquifers white 1932 healy and cook 2002 the etg derived from this method can be expressed as wang et al 2014a b 1 et g s y δ h δ h δ t where s y is the specific yield δh is the change of the groundwater level over the corresponding period cm δh is the lateral flow cm d 1 and δt is the time interval day the overall change of the water table head δh is assumed to equal the overall water table change at the well between the beginning and the end of the dry season in general lateral flow δh can be estimated by using the slope of the groundwater table curve before and after the growing season in our region lateral flow δh can be neglected because the aquifer is located in a plain and because lateral flow is less important at the seasonal scale as pointed out by pozdniakov et al 2013 therefore eq 1 can be simplified to 2 et g s y δ z y δ t where δzy is the seasonal decline of the groundwater table crosbie et al 2005 introduced the apparent specific yield sya term to evaluate the dynamics of the specific yield the apparent specific yield can be calculated by the knowledge of the van genuchten parameters van genuchten 1980 by 3 s ya s yu s yu 1 z i z f 2 n 1 1 n s yu θ s θ r where θ s and θ r are the saturated and residual water contents cm3cm 3 z i and z f are the initial and final depth to the water table cm and α cm 1 and n are the van genuchten parameters eq 4 is used to calculate the average apparent specific yield s ya when the groundwater table fluctuates in m soil layers 4 s ya i 1 m δ h i s yi i 1 m δ h i i 1 2 m where s yi is the specific yield of the corresponding soil layer and δ h i is the amplitude of the groundwater level fluctuation in the i th layer cm 2 4 numerical modeling for the simulation of vertical water flow the one dimensional richards equation eq 5 was solved using the finite element code hydrus 1d šimůnek et al 2008 šimůnek and van genuchten 2008 5 θ h t z k h h z 1 s where θ is the volumetric water content cm3 cm 3 h is the pressure head cm z represents the vertical coordinate cm positive in the downward direction and k h is the unsaturated hydraulic conductivity as a function of pressure head cm d 1 the sink term s in eq 5 describes the volume of water removed from a unit volume of soil due to plant water uptake and is defined by feddes et al 1978 as 6 s h α h s p where α h is the root water uptake function and sp is the potential water uptake rate šimůnek et al 2008 the mualem van genuchten functions van genuchten 1980 were used to describe the relationship between θ k and h with 7 θ h θ r θ s θ r 1 α h n m 8 k h k s s e 0 5 1 1 s e 1 m m 2 9 s e θ θ r θ s θ r where se is the effective saturation cm3 cm 3 θ r and θ s cm3 cm 3 are the residual and saturated volumetric water contents respectively α cm 1 n and m m 1 1 n are shape parameters and ks cm d 1 is the saturated hydraulic conductivity for the inverse modeling of the hyprop data the model was setup in the same dimensions as the laboratory column and the flux over the upper surface normalized weight loss was imposed at top boundary condition initialization was in pressure head with hydrostatic equilibrium from the lowest node h bottom 0 cm for the inversion the implemented levenberg marquardt algorithm was used for the estimation of all hydraulic parameters of eqs 7 and 8 eta was simulated using hydrus 1d with the estimated hyprop hydraulic parameters for the dakar region whereby the simulation domain was assumed to be 2300 cm deep assuming that the largest fraction of the domain was fully water filled lowest 2000 cm and acts as a groundwater reservoir which will fluctuate over depth as a system response to infiltration and evapotranspiration at the upper boundary atmospheric inputs for daily precipitation and potential evapotranspiration were used from the years 2000 2013 the soil hydraulic parameters for each layer were taken from the inversion of the hyprop data the groundwater hydraulic parameters were assumed to be a continuation of layer 3 because we considered that eta is more influenced by the upper than the deepest layers the ratio of potential evaporation to transpiration was calculated based on the approach introduced by šimůnek et al 2008 where the potential transpiration tpot and the potential evaporation epot can be separated by the knowledge of the leaf area index lai 10 t pot e t 0 1 e k l a i 11 e pot e t 0 e k l a i where et0 is the reference evapotranspiration cm k is a parameter that governs the radiation extinction of the canopy which depends on the sun angle the distribution of plants and the arrangement of leaves here we used k 0 49 as a representative value for grassland šimůnek et al 2008 and lai was taken from bobée et al 2012 over the growing season an overview of the atmospheric forcing s precipitation potential evapotranspiration and lai is shown in figs 3 and 4 lai varies between 0 27 to 0 8 m2m 2 in natural herbaceous savanna type vegetation closed to open 40 rainfall records were characterized by high frequency and intensity events producing high flooded area in the years 2005 2008 2009 2010 and 2013 for example in august 1st to 2nd 13st to 19st 22nd to 25th and 28th to 30th of 2005 rainfall recorded equal 63 9 108 9 97 3 and 49 21 mm respectively this situation causes flooding of many districts in the peri urban area where groundwater level is highly related to rainfall events diouf et al 2013 maximum daily et0 calculated by fao pm was observed during the dry season when the study area is influenced by the ne sw hot and dry wind locally called harmattan minimum values occur during the rainy season characterized by high air humidity 70 root depth was set to be 100 cm according to february and higgins 2010 and due to the perennial grassland vegetation no root growth was assumed the entire domain was initialized in pressure head with an initial groundwater table at 250 cm and hydrostatic equilibrium with the groundwater table in the unsaturated zone in total the simulation time was 22 years and only the years 2000 to 2013 were taken for analysis to be independent from initialization this kind of spin up is generally used in many modelling exercises to be independent of the initial soil water conditions in the soil profile as shown by e g boesten 2007 or weihermüller et al 2011 first the eta was calculated using hydrus 1d values were then compared to etg based on the wtf method calculated from the groundwater fluctuations simulated by hydrus 1d for the modelling of the groundwater level fluctuations the van genuchten parameters obtained from inverse simulation of the hyprop data were used table 1 and the root water uptake by the plants was calibrated in way that observed and modeled groundwater fluctuations match each other as close as possible in this respect the root zone depth for the perennial grassland and the parameter of critical water stress index for root water uptake were adapted as proposed by šimůnek and hopmans 2009 in a second step modeled hydrus 1d derived etg were compared to calculations based on the wtf method from measurements of the piezometer p3 1 2 5 calculation of the groundwater decoupling depth the decoupling depth d defines the depth of the water table where the actual evaportranspiration eta becomes lower as the potential evapotranspiration et0 because the water demand of the atmosphere cannot be delivered totally from the groundwater reservoir anymore to estimate the decoupling depth d and the decay coefficient b of the exponential function the decline of the ratio eta et0 with the water table depth was fitted by eq 12 to the eta and water table depth pairs according to shah et al 2007 12 et a et 0 f a o p m 1 f o r w t d d y 0 e b w t d d f o r w t d d where eta and et0 fao pm are the actual and reference evapotranspiration cm respectively y0 is the so called correction factor and d and b are constants and wtd is the water table depth cm 2 6 statistical analysis statistical analysis was performed to compare measured and modeled groundwater levels and the corresponding calculated etg rates to indicate the goodness of the correlation the pearsons correlation coefficient r and the root mean squared error rmse were used the coefficient of determination r2 as the square of the pearsons correlation coefficient was used to describe the goodness of fit for the regression analysis in general the rmse can range from 0 to infinity and lower values indicate better agreement between the two data sources willmot 1981 regression analysis were used to explain the relationship between eta and water table depth wtd for dry seasons of the years 2001 to 2013 3 results and discussion 3 1 soil texture and profile water content the measured soil texture up to a depth of 300 cm is plotted in fig 2 as can be seen the texture is dominated by 57 of fine sand 100 200 μm and 14 of medium sand 200 250 μm diouf 2012 the measured gravimetric water contents are plotted in fig 2a and were nearly constant between 0 and 200 cm depth with only 4 to 6 and increase beyond to up to 18 at 300 cm depth the low gravimetric water contents between 0 and 200 cm could be expected because sampling took place in the dry season of the year 2008 additionally water content and percentage of fine particles exhibit a similar profile pattern fig 2 except in the top zone likely due to evapotranspiration losses of soil water 3 2 soil hydraulic properties and specific yield accurate knowledge of soil hydraulic properties is required in studying water flow in the unsaturated zone to obtain the hydraulic properties the hyprop data were inverted as described in the materials and methods section the best parameters of the inversion for the different soil layers are listed in table 1 as displayed the saturated hydraulic conductivity of the upper layer 0 25 cm is 570 cm d 1 and slightly decreases to 504 cm d 1 in the middle layer 25 100 cm and 461 cm d 1 in the third layer 100 200 cm respectively saturated and residual water content values ranged between 0 42 to 0 45 cm3 cm 3 and 0 0011 to 0 0062 cm3 cm 3 the shape parameters α and n were also in the same order of magnitude in the three layers it has to be noted that the soil layers were almost homogeneous without obvious small scale layering at least from visual inspection of the soil augers the estimating of water budget components such as groundwater recharge and evapotranspiration requires also accurate determination of the specific yield which is a crucial term in the wtf method chen et al 2010 fahle and dietrich 2014 the specific yield values calculated using eqs 3 and 4 varied between 0 29 and 0 33 depending on the years analyzed this is in good agreement with data presented by loheide et al 2005 who reported specific yields of 0 32 when the depth to the water table exceeds 1 m 3 3 temporal variability in groundwater level and et 3 3 1 seasonal groundwater level changes fig 5 shows the hydrus 1d simulated groundwater levels for two different scenarios where the soil was either assumed to be bare or covered by short perennial grass both states bare and vegetated are observed in the vicinity of the monitoring piezometer additionally water table fluctuations measured in the piezometer are plotted for the years 2010 to 2012 as can be seen from the hydrus 1d modelled groundwater table fluctuations each dry season leads to a pronounced drop of the groundwater table and a relaxation in the corresponding wet season whereby the drop nearly showed same levels for each year independently of the maximum level of the groundwater table during the previous rainy season looking at the match between the modelled and measured groundwater tables for the years 2010 to 2012 indicates that the model results based on the assumption that the soil was bare is less good compared to the vegetated scenario for better comparison the modelled versus measured groundwater levels for the dry season are plotted in fig 6 for the bare soil assumption the r is 0 96 with and rmse of 0 26 m on the other hand for the grassland vegetation a slightly higher r 0 98 and a smaller rmse of 0 12 m indicates that the grassland vegetation scenario is more adequate to describe the groundwater fluctuations in this area 3 3 2 simulated hydrus 1d eta and etg the actual et values obtained with the two methods namely the direct eta output from hydrus 1d and the etg calculated by the wtf method based on simulated hydrus 1d water table fluctuations were compared in a first step to analyze the applicability and validity of the wtf method for this site table 2 presents result for each individually simulated dry season with regard to the cumulative rainfall from the previous rainy season the total length of the dry season and the maximum difference in groundwater level between wet and dry season data reveal that precipitation amounts vary greatly with a minimum of 272 mm in 2007 and a maximum of 723 mm in 2009 with a mean of 475 mm over all years length of the dry season also varies between 145 2002 and 271 2013 days mean 229 days drawdown varies as well and ranges from 18 1 2002 to 113 2 2013 cm mean 60 cm for the bare soil scenario and from10 4 2002 and 101 9 2013 cm mean 54 4 cm for the perennial grass scenario whereby no significant correlation p 0 05 between length of the dry period and water table decline was observed the generally lower values of the drawdown for the perennial grass scenario are probably caused by the generally lower water table during the wet season due to root water uptake during the vegetation period and a deeper groundwater table during the dry season hydrus 1d based eta varies for the bare soil scenario between 0 22 2003 and 1 11 2013 mm d 1 with a mean of 0 60 mm d 1 and for the perennial grass vegetation scenario between 0 23 2002 and 1 28 2011 mm d 1 with a mean of 0 78 mm d 1 higher eta in this latter scenario can be explained by greater water extraction due to root water uptake especially from deeper soil layers because of variability in dry season length total cumulative eta mm varies accordingly the total evaporation loss during the dry season with respect to the rainfall recorded in the previous rainy season varies from 10 in 2002 to more than 47 in 2013 mean 28 suggesting potential groundwater recharge variability table 3 looking at the results of etg calculated by the wtf method reflects the same pattern as for the eta whereby slightly higher etg mean 0 23 mm d 1 compared to eta was calculated for the bare soil scenario on the other hand calculated eta and etg matched each other well for the vegetated scenario with only a very small underestimation of etg mean 0 04 mm d 1 mean etg over the entire dry periods was estimated to be 0 82 and 0 73 mm d 1 for the bare and vegetated case respectively which is 37 and 6 5 deviation from the direct hydrus 1d calculated eta the reason for the slightly larger differences between etg and eta for the bare soil scenario are unclear especially that measurement errors can be excluded in the synthetic case shown therefore one can question if the definition of the specific yield as shown in eq 3 and 4 can be used globally on the other hand one can question why the use of the same specific yield seems not to impact the vegetated scenario here one can speculate that the additional water extraction form the root zone by the plants somehow compensate the incorrectly defined specific yield for a better insight into the direct comparison between eta and etg both values were plotted versus each other in fig 7 for both scenarios overall a good agreement between both methods can be detected with high r2 values exceeding 0 98 whereby a general overestimation of the etg for the bare soil conditions is again visible resulting in an rmse of 0 23 mm d 1 in comparison and as already stated the etg and eta match well over the entire eta range for the vegetated scenario rmse 0 12 mm d 1 which gives confidence that the wtf method can be used to estimate actual evaporation during the dry season from piezometer data only in the study area when vegetation is present the calculated daily eta 0 23 1 28 mm d 1 and etg 0 17 1 20 mm d 1 values in our study area were lower than other etg estimates for vegetated areas where the wtf method was also applied for example mould et al 2010 estimated maximum etg rates of 5 91 mm d 1 in northern germany where the water table depth ranged only between 0 1 and 0 6 m indicating that not only the atmospheric demand gribovszki et al 2008 but also the total water table depth the soil properties of the vadose zone as well as vegetation type determine the absolute evapotranspiration cleverly et al 2006 cooper et al 2006 devitt et al 2011 however our evapotranspiration values are comparable with estimated etg rates in semi arid and arid environments where the groundwater table is in general much deeper pozdniakov et al 2013 and lautz 2008 obtained etg values from 0 1 to 2 mm d 1 and 0 0 to 3 1 mm d 1 for short grass prairie for water table depth ranged between 1 5 to 4 m and 1 to 3 m respectively in a hyper arid environment of northwestern china wang et al 2014a b and cheng et al 2017 obtained similar etg values 0 63 to 2 33 mm d 1 from 0 5 to 3 5 m water table depth the results presented in this study are consistent with those presented by gardner 1958 who show that evaporation demand is mainly controlled by external condition and water table depth 3 3 3 etg from piezometer data and hydrus 1d simulation daily etg values estimated from measured groundwater levels in the piezometer and the wtf method varies between 0 84 2012 and 1 34 mm d 1 2011 with a mean of 1 10 mm d 1 etg of the year 2011 is higher than the eta modeled in bare soil condition 29 mismatch but is in the same range as the eta modeled in the vegetated scenario assuming a perennial grass cover 5 mismatch however in 2012 the difference between eta and etg is much larger with nearly three fold etg compared to the bare soil eta and an overestimation of 42 for the vegetated eta the mismatch for the year 2012 can be attributed to the fact that the measured groundwater fluctuations were not recorded for the entire dry season and ended already at april 7th 2012 even if only one dry season was completely covered by the measurements confidence in the wtf method for this study area is given by the synthetic model study using the hydrus 1d simulations 3 3 4 relationship between eta et0 and wtd the daily potential evapotranspiration et0 is only poorly related to daily eta r2 0 08 for the two scenarios bare and vegetated but in general higher daily eta rates correspond to higher daily et0 and modeled eta equals the et0 fao pm when the groundwater table is shallow corresponding to the end of the rainy season october november the same has been already shown by kurc and small 2004 who found that in semi arid ecosystems the maximum eta rate during the dry season occurs immediately after the rainy season and then rapidly decreases within days of no additional rainfall in a next step simulated eta data were plotted against the water table depth wtd for each single year for further analysis fig 8 clearly shows that eta values are highly related to water table depth and could be described by an exponential function with an r2 0 80 over all years largest regression between eta and wtd occurred in the years 2001 r2 0 84 2006 r2 0 83 2009 r2 0 85 2010 r2 0 73 2011 r2 0 88 and 2013 r2 0 84 where the wtd was most shallow due to the large precipitation amounts in the previous rainy season but also spans a more or less wide range on the other hand the relationships was poor for the year 2002 r2 0 23 and 2003 r2 0 35 where the wtd was low and nearly no fluctuations were detectable in conclusion eta seems to be mainly driven by the water table depth at this site the influence of wtd in evapotranspiration demand is also noted by o connor et al 2019 and gao et al 2017 in a study conducted over different land covers fig 9 shows the plot of eta rates vs wtd for the simulation with perennial grass cover and for all years the simulated eta was normalized by the et0 fao pm therefore the ratio eta et0fao pm varied only between 0 and 1 as can be seen the eta is equal to potential evapotranspiration until the water table reaches a depth d defined as the decoupling depth by shah et al 2007 up to this critical depth the aquifer will provide most part of water for eta in our case the decoupling depth equal 1 46 m beyond this point eta shifts from atmospheric control eta is equal to et0 to soil moisture control of the vadose zone again below the transition depth eta decreased with decreasing wtd in an exponential way the same observations was shown in the previous works of shah et al 2007 who proposed an exponential relationship for evaporation from a shallow water table from different land covers and soils first eq 14 was used without tacking into account the correction factor y0 to simulate the decline of eta with decline in wtd in the fitting process the decoupling depth was assuming to equal 1 46 m as shown in fig 9 the exponential function fitted well the data for shallow water table but the fit was poor with an r2of 0 59 and a rmse of 0 121 m for wtd d introducing a correction factor enhanced the fitted model r2 0 78 rmse 0 035 m the correction factor y0 and the decay coefficient b values obtained in this study are different than those obtained by shah et al 2007 in grass cover and for sandy soils probably due to differences in decoupling depth and soil characteristics 4 conclusion daily groundwater level fluctuations were observed during the dry season in dakar area the modelled data showed groundwater table drawdown from 18 1 to 113 2 cm and 10 4 to 101 9 cm for bare soil and perennial grass scenario respectively the results also indicate that each dry season leads to a pronounced drop of the groundwater table and this drop nearly showed same levels for each year independently of the maximum level of the groundwater table during the previous rainy season additionally dry season eta as an important component of the annual water balance consumed 10 to 43 of annual rainfall for this site modeled daily groundwater levels were first used to quantify etg and to analyze the applicability and validity of the wtf method for this site modelled actual evapotranspiration eta ranged between 0 22 and 1 28 mm d 1 the comparison of eta and etg values obtained for the vegetated scenario show that the wtf method can be used to estimate actual evaporation during the dry season from piezometer data only in the study area furthermore daily eta is directly coupled to the water table depth wtd whereby shallower wtd increased eta irrespectively of the atmospheric demand over all years the dependence between eta and wtd could be well described r2 0 80 by an exponential function the simulated eta model shows that higher eta values occurred during the period from october to december and lowers values were observed at june corresponding respectively to the ends of the rainy and the dry season groundwater evapotranspiration estimation based on the modeling and wtf methods may be considered the simplest easiest and least expensive technique available but the method involves a number of sources of uncertainty soylu et al 2012 in addition a reliable quantification of actual evapotranspiration requires a comprehensive investigation that combines the modeling approach with traditional methods such as weighable lysimeters and eddy correlation method credit authorship contribution statement ousmane coly diouf writing original draft writing review editing data curation conceptualization methodology software lutz weihermüller conceptualization formal analysis writing review editing funding acquisition writing original draft software mathias diedhiou visualization harry vereecken supervision resources funding acquisition seynabou cissé faye visualization sérigne faye writing review editing funding acquisition samba ndao sylla visualization declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgement this study was financially supported by a post doctoral grant from the german academicexchange service daad to ousmane coly diouf at forschungszentrum jülich gmbh agrosphere institute ibg 3 the authors are grateful to the daad for this funding 
5443,the schaake shuffle is a simple and effective method for re ordering calibrated ensemble forecasts it is widely used in forecast calibration methods where realistic spatial and temporal sequences are important we illustrate a previously unidentified problem with the application of the schaake shuffle when the autocorrelation of uncalibrated forecasts is markedly different from observations the schaake shuffle cannot guarantee that the calibrated ensemble is reliable when ensemble members are accumulated through time accumulations in time and space are particularly important for applications that integrate rainfall over these dimensions notably hydrological modelling we demonstrate that ensemble means of uncalibrated forecasts tend to be more autocorrelated than observations this can cause poor reliability if variables are accumulated across lead times under certain conditions even if forecasts are perfectly reliable at individual lead times specifically if the following conditions occur ensemble predictions of accumulated variables tend to be too wide 1 forecasts are more autocorrelated than observations 2 forecasts are skillful i e cross correlations between forecasts and observations are high we show with a real world case study that these conditions can readily occur we discuss potential solutions to this issue keywords schaake shuffle statistical calibration poor man s ensemble ensemble forecasts autocorrelation reliability 1 introduction the schaake shuffle clark et al 2004 uses an ingeniously simple idea to solve a complex problem forecasts from numerical weather prediction nwp models often require statistical calibration to correct biases reduce errors and reliably quantify forecast uncertainty calibration is usually broken into two steps first an independently calibrated univariate distribution is generated at each location and lead time from which samples can be drawn second the samples drawn from all the univariate distributions are linked in space and time so that for example a rainstorm sweeps across a region rather than rain appearing randomly at different locations this is the role the schaake shuffle plays by reordering the samples according to rank patterns taken from observations a major benefit of the schaake shuffle is that ensemble members can be accumulated or averaged across space and time and the ensemble of accumulations will also be reliable this is a prerequisite to many applications of rainfall forecasts notably streamflow forecasting the schaake shuffle has proven so effective that it is virtually ubiquitous in new calibration methods where space time correlations are important schefzik 2016 wu et al 2018 this includes the calibration of ensemble forecasts where calibration is usually applied to the mean of the ensemble wilks 2015 scheuerer et al 2017 khajehei et al 2018 in this technical note we identify a drawback of the schaake shuffle not yet described elsewhere that can occur when calibrating the mean of ensemble forecasts we show that the schaake shuffle assumes that the autocorrelation of the uncalibrated forecast is similar to the observations from which patterns are sampled this assumption is often violated when calibration is applied to the mean of an ensemble forecast we demonstrate this shortcoming by calibrating the mean of the poor man s ensemble pme ebert 2001 of nwp models that is used as the primary guidance for australian weather forecasts issued by the bureau of meteorology we will show that this drawback is observable in the reliability of accumulations of forecast rainfall we describe our case study calibration method and the schaake shuffle in section 2 we demonstrate that the problem occurs more generally with a synthetic example described in section 3 and describe our verification methods in section 4 we demonstrate the problem with results from our case study and synthetic example in section 5 and discuss prospects for avoiding this problem in section 6 2 case study 2 1 the poor man s ensemble the pme underpins the short to medium term official weather forecasts issued by the bureau of meteorology it is made up of 8 deterministic nwp models which vary in both horizontal resolution and forecast horizon http www bom gov au watl about about forecast rainfall shtml to guide bureau forecasts the bureau uses a method called probability matching ebert 2001 to summarise the information from the ensemble in a deterministic forecast termed the probability matched mean the probability matched mean of the pme forecasts is generally more skilful than the forecasts from its component members ebert 2001 for simplicity we calibrate the simple arithmetic mean of the pme at each lead time the arithmetic mean of the pme produces precipitation forecasts to a forecast horizon of 9 5 days at a 3 h time step we use forecasts issued once per day at 0000 utc archived pme forecasts are available from 1 august 2010 to 31 december 2013 for comparison we calibrate the deterministic access g australian community climate and earth system simulator nwp which offers 10 day forecasts at the 3 h time step note that access g forms part of the pme we do not calibrate the first 3 h of forecasts to avoid spinup of deterministic nwp models in this study 0 h lead time corresponds to 0300 utc 2 2 observations we use precipitation observations prepared for a pre operational ensemble streamflow forecasting system that uses semi distributed hydrological models bennett et al 2014 the semi distributed hydrological model divides the target catchment into smaller subareas in order to simulate spatially distinct runoff responses this requires rainfall observations at each subarea subarea rainfalls at a 3 h time step are constructed with inverse distance weighting shrestha et al 2015 of gauged hourly rainfall records supplied by the bureau of meteorology for this technical note we use observations prepared for the subtropical stanley river catchment in south east queensland near australia s central east coast further details of the observations used here are available in bennett et al 2014 and shrestha et al 2015 2 3 forecast calibration we use the calibration method previously described by robertson et al 2013 and shrestha et al 2015 we give only a brief description here and refer the reader to the source papers for more detail 2 3 1 calibration of marginal distributions we use a simplified variant of the bayesian joint probability modelling approach bjp wang and robertson 2011 to produce a calibrated univariate distribution at each location and lead time the bjp uses the log sinh transformation wang et al 2012 to allow the predictor x the pme mean and the predictand y observed rainfall to be treated as normally distributed 1 x 1 b x log sinh a x b x x y 1 b y log sinh a y b y y where x and y are the transformed predictor and predictand respectively and a and b are parameters x and y are assumed to follow a bivariate normal distribution 2 p x y n m σ where 3 m μ x μ y and 4 σ σ x 2 ρ x y σ x σ y ρ x y σ x σ y σ y 2 μ x and μ y are the means and σ x 2 and σ y 2 the variances of x and y respectively ρ x y is the correlation between x and y a total of 9 parameters are inferred for each lead time and location θ a x b x a y b y μ x μ y σ x 2 σ y 2 ρ x y zero values or values close to zero are handled by treating them as censored data following wang and robertson 2011 parameter fitting is cross validated using leave 1 month out cross validation for more details on data censoring and on parameter inference see robertson et al 2013 and shrestha et al 2015 to produce a forecast y f the conditional univariate normal distribution is given by 5 y f f y x n μ y x σ y x 2 where 6 μ y x μ y ρ x y σ y x μ x σ x and 7 σ y x 2 σ y 2 1 ρ x y 2 samples of y f can be drawn from eq 5 to produce an ensemble of the desired size n we use n 1000 and then back transformed by applying the inverse of eq 1 2 3 2 schaake shuffle clark et al 2004 s description of the schaake shuffle incorporated multiple variables but in this study we are concerned only with rainfall let a calibrated ensemble rainfall forecast be given by the matrix x i j l where i is the ensemble member j is a location e g a weather station and l is the forecast lead time we construct an identically shaped matrix y t j l from observations where t is an index of dates in the historical time series and j is a location as before l is equivalent to the lead time and follows the sequence of observed rainfalls starting at each t for ease of computation we randomly sample n 1000 observed patterns from the same 1220 dates available in the archive following clark et al 2004 for each location j and lead time l we can sort the vectors x x 1 x 2 x n and y y 1 y 2 y n by 8 χ x 1 x 2 x n x 1 x 2 x n 9 γ y 1 y 2 y n y 1 y 2 y n denote b as a vector of indices showing where the ranked values γ are located in y so that γ y b finally the forecast vector x ss x 1 ss x 2 ss x n ss is constructed by 10 x q ss x r r 1 2 n and 11 q b r where the parenthetical subscripts of x refer to the elements in the sorted vector χ 3 synthetic example in addition to the case study we can show that mismatches in autocorrelation will generally result in unreliable accumulations with a synthetic numerical example denote a multivariate normal distribution for 2 l variables by 12 p z o 1 z o 2 z o l z f 1 z f 2 z f l n m σ where z o l and z f l are observations and forecasts respectively l represents forecast lead time and l the total number of lead times for this synthetic example we set l 20 the vector of means is given by 13 m 1 2 l μ with 1 2 l a vector of all ones with 2 l elements and the covariance matrix is σ σ 2 r 14 r 1 ρ o f 2 1 ρ o f l 1 ϕ ρ c f 2 1 ρ c f l 1 ρ o f 1 2 1 ρ o f l 2 ρ c f 1 2 ϕ ρ c f l 2 ρ o f 1 l ρ o f 2 l 1 ρ c f 1 l ρ c f 2 l ϕ ϕ ρ c f 2 1 ρ c f l 1 1 ρ f f 2 1 ρ f f l 1 ρ c f 1 2 ϕ ρ c f l 2 ρ f f 1 2 1 ρ f f l 2 ρ c f 1 l ρ c f 2 l ϕ ρ f f 1 l ρ f f 2 l 1 where 15 f k m exp k m 1 d d is the parameter that controls decay of f ρ o ρ f and ρ c denote autocorrelation of observations forecasts and cross lag correlation between them respectively ϕ is the cross correlation between observations and forecasts analogous to ρ x y in eq 4 cross lag correlation is correlation between observations at k lead time and forecasts at m lead time where k m we generate 2000 samples of observations and forecasts for 20 lead times using eq 12 we assess the impact of 2 different parameters the cross correlation between forecasts and observations ϕ and the autocorrelation of the forecasts ρ f while holding all other parameters except ρ c fixed the fixed parameters are ρ o 0 5 μ 0 σ 1 and d l the experimental parameters are allowed the take the following values 1 the cross correlation between the forecasts and observations takes values representing total independence and a moderately high correlation i e ϕ 0 0 0 75 high cross correlations are analogous to cases where forecasts are very skillful low cross correlations are analogous to cases where forecasts are not skillful 2 the autocorrelation of the forecasts takes values that are less than equal to and greater than the autocorrelation of the observations i e ρ f 0 3 0 5 0 7 note that autocorrelation of observations is 0 5 we assume the cross lag correlation between the forecasts and observations behaves similarly to the case study data ρ c ϕ ρ f synthetic data are generated for all 6 possible combinations of parameters calibration methods described in section 2 3 1 are applied for each lead time independently to generate ensemble forecasts since the synthetic data is already in normal space we do not require data transformation the schaake shuffle is then applied to impose the rank correlation structure in the ensemble forecast we verify the sum of the calibrated forecasts over l lead times 4 verification we focus on reliability as defined by the well known probability integral transform pit given the cumulative distribution function cdf of a single ensemble forecast f t x the pit is given by 16 pit f t y y 0 u 0 1 f t 0 y 0 where y is the corresponding observation if predictions are reliable the set of pit values will be uniformly distributed e g gneiting and katzfuss 2014 the treatment of pit values at y 0 is necessary to allow reliable predictions to produce uniformly distributed pit values when many zero rainfalls occur wang and robertson 2011 the uniformity of the pit values is checked against the standard uniform variate we term these pit diagrams reliable forecasts yield a pit diagram close to the diagonal to show the effect of autocorrelation on reliability we present pit diagrams for i individual lead times and ii accumulated rainfall totals accumulated rainfall totals can also be verified with multivariate verification metrics such as average rank histograms or depth rank histograms thorarinsdottir et al 2016 but for simplicity and brevity we use pit diagrams pit is a stringent diagnostic for reliability e g gneiting and katzfuss 2014 and is widely used in the hydrometeorological forecasting community results shown in this case study are from leave one month out cross validated forecasts shrestha et al 2015 5 results 5 1 case study the calibration produces reliable forecast ensembles at individual lead times for both access g and pme forecasts fig 1 a c conversely forecasts of accumulated precipitation fig 1d f show clear differences in reliability between calibrated access g and calibrated pme forecasts calibrated pme forecasts are clearly much less reliable than calibrated access g forecasts the transposed s shape of the calibrated pme forecasts indicates an underconfident forecast i e the ensemble spread is too wide pit diagrams of 72 h rainfall accumulations show that calibrated pme forecasts become more reliable as lead time increases fig 1g i forecasts of 72 h accumulations are very reliable at the longest lead time i e 144 216 h as we have shown that calibrated pme ensembles at individual lead times are well calibrated this fault must arise when ensemble members are reordered with the schaake shuffle to understand why we need to consider the autocorrelation of observations and the uncalibrated forecasts shown in fig 2 autocorrelation of the uncalibrated access g forecasts is very close to that of the observations in contrast the autocorrelation of the uncalibrated pme forecasts is consistently higher than that of the observations for all lead times the high autocorrelation of the pme forecasts is a natural consequence of averaging over multiple nwp models each model will tend to rain at different times and averaging smooths the forecasts in time essentially the schaake shuffle is constrained in the range of values it can reorder from conditional distributions at each lead time when the means of the conditional distributions are highly correlated the cross correlation between the forecasts and observations becomes weaker at longer lead times as a consequence the values sampled from the predictive distributions become less constrained therefore calibrated pme forecasts for 72 h totals become more reliable at longer lead times fig 1g i the cross lag correlation of pme is also shown in fig 2 here we define the cross lag correlation of pme forecasts as the correlation of pme forecasts at lead time 3 h to observations at lead time 6 9 12 h and so on standardised by the cross correlation i e correlation between the pme forecasts at lead time 3 h and observations at lead time 3 h the decay of the cross lag correlation of pme forecasts with lead time is very similar to the decay of autocorrelation of the pme forecasts with lead time analysis of the synthetic example see next section reveals that calibrated forecasts are underconfident when the autocorrelation of the forecasts are higher than the observations 5 2 synthetic example at individual lead times all synthetic calibrated forecasts are perfectly reliable by construction not shown this is equivalent to the results from our case study shown in fig 1a c as expected calibrated ensemble forecasts of accumulations are reliable when uncalibrated forecasts and observations are independent ϕ 0 when uncalibrated forecasts and observations are independent this is analogous to an uncalibrated forecast that displays no skill in these cases calibration will return climatology forecasts which are reliable by construction the schaake shuffle introduces climatological rank correlation patterns leading to reliable calibrated ensemble forecasts of accumulations for moderately high values of ϕ calibrated ensemble forecasts of accumulations are reliable if the autocorrelation of uncalibrated forecasts and observations are similar fig 3 b this is consistent with the result of access g fig 1 however we see clear differences in reliability when the autocorrelation of the uncalibrated forecasts differs to that of the observations fig 3a and c when the uncalibrated forecasts are more autocorrelated than the observations ρ f 0 7 calibrated ensemble forecasts of accumulations are too wide equivalent to fig 1d f signified by a transposed s shape fig 3c when uncalibrated forecasts are less autocorrelated than observations ρ f 0 3 calibrated ensemble forecasts of accumulations that are too narrow signified by an s shape fig 3a 6 discussion and conclusions variables that are aggregated in time by either summing or averaging are often of central importance to applications of calibrated nwp forecasts perhaps the most obvious of these applications is streamflow forecasting hydrological models aggregate precipitation over time to simulate streamflow and therefore forecasts of precipitation accumulations have a profound bearing on streamflow predictions therefore the problem we describe in this study is likely to have wide reaching deleterious consequences the problem we have described here is broader than our case study we believe it will arise in any case where autocorrelation between forecasts is sufficiently dissimilar to observations as we have shown one source of such dissimilarities is averaging across deterministic nwp models but it could also occur when calibrating the mean of ensemble nwp models whether these ensembles are generated as burst ensembles e g access ge dare et al 2016 or time lagged ensembles e g ncep climate forecast system version 2 http www cpc ncep noaa gov products cfsv2 cfsv2seasonal shtml further dissimilarities in autocorrelation characteristics may occur even with deterministic nwp models for example when calibrating gridded nwp models to point observations e g cattoën et al 2020 in addition we have focused on mismatches in autocorrelations for a single variable but similar analyses could be performed for spatial correlations and inter variable correlations both of which the schaake shuffle was originally designed to re order clark et al 2004 in the case of the pme forecasts a potential solution is to calibrate the component nwp models independently because formal calibration methods ensure ensemble members are exchangeable the calibrated forecasts from all nwp models can then simply be combined into a superensemble note however that this approach assumes that the component nwp models all have similar autocorrelation properties to observations this solution is more computationally demanding in that it requires the calibration of 8 models not 1 but is likely to avoid the mismatch in autocorrelation similar reasoning can be extended to the calibration of ensemble nwp models in this study we use the schaake shuffle clark et al 2004 as a method to reorder ensemble forecasts since the problem comes from the uncalibrated forecasts other ensemble reordering methods e g schefzik et al 2013 may also face the issue of poor reliability in accumulated variables in cases where deterministic nwp forecasts exhibit substantial differences in autocorrelation or spatial correlations to observations there is no straightforward solution we recommend that practitioners and researchers test for such differences with analyses like those performed in fig 2 if substantial differences in correlation properties are identified new methods to reorder ensembles may need to be developed credit authorship contribution statement durga lal shrestha methodology investigation formal analysis visualization software writing original draft d e robertson supervision conceptualization methodology formal analysis validation writing review editing j c bennett methodology formal analysis validation writing original draft q j wang supervision conceptualization methodology validation writing review editing declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements this research has been supported by the water information research and development alliance wirada between the bureau of meteorology and csiro land water we would like to thank dr seline ng csiro for valuable suggestions on an earlier draft and two anonymous referees for their helpful and constructive comments 
5443,the schaake shuffle is a simple and effective method for re ordering calibrated ensemble forecasts it is widely used in forecast calibration methods where realistic spatial and temporal sequences are important we illustrate a previously unidentified problem with the application of the schaake shuffle when the autocorrelation of uncalibrated forecasts is markedly different from observations the schaake shuffle cannot guarantee that the calibrated ensemble is reliable when ensemble members are accumulated through time accumulations in time and space are particularly important for applications that integrate rainfall over these dimensions notably hydrological modelling we demonstrate that ensemble means of uncalibrated forecasts tend to be more autocorrelated than observations this can cause poor reliability if variables are accumulated across lead times under certain conditions even if forecasts are perfectly reliable at individual lead times specifically if the following conditions occur ensemble predictions of accumulated variables tend to be too wide 1 forecasts are more autocorrelated than observations 2 forecasts are skillful i e cross correlations between forecasts and observations are high we show with a real world case study that these conditions can readily occur we discuss potential solutions to this issue keywords schaake shuffle statistical calibration poor man s ensemble ensemble forecasts autocorrelation reliability 1 introduction the schaake shuffle clark et al 2004 uses an ingeniously simple idea to solve a complex problem forecasts from numerical weather prediction nwp models often require statistical calibration to correct biases reduce errors and reliably quantify forecast uncertainty calibration is usually broken into two steps first an independently calibrated univariate distribution is generated at each location and lead time from which samples can be drawn second the samples drawn from all the univariate distributions are linked in space and time so that for example a rainstorm sweeps across a region rather than rain appearing randomly at different locations this is the role the schaake shuffle plays by reordering the samples according to rank patterns taken from observations a major benefit of the schaake shuffle is that ensemble members can be accumulated or averaged across space and time and the ensemble of accumulations will also be reliable this is a prerequisite to many applications of rainfall forecasts notably streamflow forecasting the schaake shuffle has proven so effective that it is virtually ubiquitous in new calibration methods where space time correlations are important schefzik 2016 wu et al 2018 this includes the calibration of ensemble forecasts where calibration is usually applied to the mean of the ensemble wilks 2015 scheuerer et al 2017 khajehei et al 2018 in this technical note we identify a drawback of the schaake shuffle not yet described elsewhere that can occur when calibrating the mean of ensemble forecasts we show that the schaake shuffle assumes that the autocorrelation of the uncalibrated forecast is similar to the observations from which patterns are sampled this assumption is often violated when calibration is applied to the mean of an ensemble forecast we demonstrate this shortcoming by calibrating the mean of the poor man s ensemble pme ebert 2001 of nwp models that is used as the primary guidance for australian weather forecasts issued by the bureau of meteorology we will show that this drawback is observable in the reliability of accumulations of forecast rainfall we describe our case study calibration method and the schaake shuffle in section 2 we demonstrate that the problem occurs more generally with a synthetic example described in section 3 and describe our verification methods in section 4 we demonstrate the problem with results from our case study and synthetic example in section 5 and discuss prospects for avoiding this problem in section 6 2 case study 2 1 the poor man s ensemble the pme underpins the short to medium term official weather forecasts issued by the bureau of meteorology it is made up of 8 deterministic nwp models which vary in both horizontal resolution and forecast horizon http www bom gov au watl about about forecast rainfall shtml to guide bureau forecasts the bureau uses a method called probability matching ebert 2001 to summarise the information from the ensemble in a deterministic forecast termed the probability matched mean the probability matched mean of the pme forecasts is generally more skilful than the forecasts from its component members ebert 2001 for simplicity we calibrate the simple arithmetic mean of the pme at each lead time the arithmetic mean of the pme produces precipitation forecasts to a forecast horizon of 9 5 days at a 3 h time step we use forecasts issued once per day at 0000 utc archived pme forecasts are available from 1 august 2010 to 31 december 2013 for comparison we calibrate the deterministic access g australian community climate and earth system simulator nwp which offers 10 day forecasts at the 3 h time step note that access g forms part of the pme we do not calibrate the first 3 h of forecasts to avoid spinup of deterministic nwp models in this study 0 h lead time corresponds to 0300 utc 2 2 observations we use precipitation observations prepared for a pre operational ensemble streamflow forecasting system that uses semi distributed hydrological models bennett et al 2014 the semi distributed hydrological model divides the target catchment into smaller subareas in order to simulate spatially distinct runoff responses this requires rainfall observations at each subarea subarea rainfalls at a 3 h time step are constructed with inverse distance weighting shrestha et al 2015 of gauged hourly rainfall records supplied by the bureau of meteorology for this technical note we use observations prepared for the subtropical stanley river catchment in south east queensland near australia s central east coast further details of the observations used here are available in bennett et al 2014 and shrestha et al 2015 2 3 forecast calibration we use the calibration method previously described by robertson et al 2013 and shrestha et al 2015 we give only a brief description here and refer the reader to the source papers for more detail 2 3 1 calibration of marginal distributions we use a simplified variant of the bayesian joint probability modelling approach bjp wang and robertson 2011 to produce a calibrated univariate distribution at each location and lead time the bjp uses the log sinh transformation wang et al 2012 to allow the predictor x the pme mean and the predictand y observed rainfall to be treated as normally distributed 1 x 1 b x log sinh a x b x x y 1 b y log sinh a y b y y where x and y are the transformed predictor and predictand respectively and a and b are parameters x and y are assumed to follow a bivariate normal distribution 2 p x y n m σ where 3 m μ x μ y and 4 σ σ x 2 ρ x y σ x σ y ρ x y σ x σ y σ y 2 μ x and μ y are the means and σ x 2 and σ y 2 the variances of x and y respectively ρ x y is the correlation between x and y a total of 9 parameters are inferred for each lead time and location θ a x b x a y b y μ x μ y σ x 2 σ y 2 ρ x y zero values or values close to zero are handled by treating them as censored data following wang and robertson 2011 parameter fitting is cross validated using leave 1 month out cross validation for more details on data censoring and on parameter inference see robertson et al 2013 and shrestha et al 2015 to produce a forecast y f the conditional univariate normal distribution is given by 5 y f f y x n μ y x σ y x 2 where 6 μ y x μ y ρ x y σ y x μ x σ x and 7 σ y x 2 σ y 2 1 ρ x y 2 samples of y f can be drawn from eq 5 to produce an ensemble of the desired size n we use n 1000 and then back transformed by applying the inverse of eq 1 2 3 2 schaake shuffle clark et al 2004 s description of the schaake shuffle incorporated multiple variables but in this study we are concerned only with rainfall let a calibrated ensemble rainfall forecast be given by the matrix x i j l where i is the ensemble member j is a location e g a weather station and l is the forecast lead time we construct an identically shaped matrix y t j l from observations where t is an index of dates in the historical time series and j is a location as before l is equivalent to the lead time and follows the sequence of observed rainfalls starting at each t for ease of computation we randomly sample n 1000 observed patterns from the same 1220 dates available in the archive following clark et al 2004 for each location j and lead time l we can sort the vectors x x 1 x 2 x n and y y 1 y 2 y n by 8 χ x 1 x 2 x n x 1 x 2 x n 9 γ y 1 y 2 y n y 1 y 2 y n denote b as a vector of indices showing where the ranked values γ are located in y so that γ y b finally the forecast vector x ss x 1 ss x 2 ss x n ss is constructed by 10 x q ss x r r 1 2 n and 11 q b r where the parenthetical subscripts of x refer to the elements in the sorted vector χ 3 synthetic example in addition to the case study we can show that mismatches in autocorrelation will generally result in unreliable accumulations with a synthetic numerical example denote a multivariate normal distribution for 2 l variables by 12 p z o 1 z o 2 z o l z f 1 z f 2 z f l n m σ where z o l and z f l are observations and forecasts respectively l represents forecast lead time and l the total number of lead times for this synthetic example we set l 20 the vector of means is given by 13 m 1 2 l μ with 1 2 l a vector of all ones with 2 l elements and the covariance matrix is σ σ 2 r 14 r 1 ρ o f 2 1 ρ o f l 1 ϕ ρ c f 2 1 ρ c f l 1 ρ o f 1 2 1 ρ o f l 2 ρ c f 1 2 ϕ ρ c f l 2 ρ o f 1 l ρ o f 2 l 1 ρ c f 1 l ρ c f 2 l ϕ ϕ ρ c f 2 1 ρ c f l 1 1 ρ f f 2 1 ρ f f l 1 ρ c f 1 2 ϕ ρ c f l 2 ρ f f 1 2 1 ρ f f l 2 ρ c f 1 l ρ c f 2 l ϕ ρ f f 1 l ρ f f 2 l 1 where 15 f k m exp k m 1 d d is the parameter that controls decay of f ρ o ρ f and ρ c denote autocorrelation of observations forecasts and cross lag correlation between them respectively ϕ is the cross correlation between observations and forecasts analogous to ρ x y in eq 4 cross lag correlation is correlation between observations at k lead time and forecasts at m lead time where k m we generate 2000 samples of observations and forecasts for 20 lead times using eq 12 we assess the impact of 2 different parameters the cross correlation between forecasts and observations ϕ and the autocorrelation of the forecasts ρ f while holding all other parameters except ρ c fixed the fixed parameters are ρ o 0 5 μ 0 σ 1 and d l the experimental parameters are allowed the take the following values 1 the cross correlation between the forecasts and observations takes values representing total independence and a moderately high correlation i e ϕ 0 0 0 75 high cross correlations are analogous to cases where forecasts are very skillful low cross correlations are analogous to cases where forecasts are not skillful 2 the autocorrelation of the forecasts takes values that are less than equal to and greater than the autocorrelation of the observations i e ρ f 0 3 0 5 0 7 note that autocorrelation of observations is 0 5 we assume the cross lag correlation between the forecasts and observations behaves similarly to the case study data ρ c ϕ ρ f synthetic data are generated for all 6 possible combinations of parameters calibration methods described in section 2 3 1 are applied for each lead time independently to generate ensemble forecasts since the synthetic data is already in normal space we do not require data transformation the schaake shuffle is then applied to impose the rank correlation structure in the ensemble forecast we verify the sum of the calibrated forecasts over l lead times 4 verification we focus on reliability as defined by the well known probability integral transform pit given the cumulative distribution function cdf of a single ensemble forecast f t x the pit is given by 16 pit f t y y 0 u 0 1 f t 0 y 0 where y is the corresponding observation if predictions are reliable the set of pit values will be uniformly distributed e g gneiting and katzfuss 2014 the treatment of pit values at y 0 is necessary to allow reliable predictions to produce uniformly distributed pit values when many zero rainfalls occur wang and robertson 2011 the uniformity of the pit values is checked against the standard uniform variate we term these pit diagrams reliable forecasts yield a pit diagram close to the diagonal to show the effect of autocorrelation on reliability we present pit diagrams for i individual lead times and ii accumulated rainfall totals accumulated rainfall totals can also be verified with multivariate verification metrics such as average rank histograms or depth rank histograms thorarinsdottir et al 2016 but for simplicity and brevity we use pit diagrams pit is a stringent diagnostic for reliability e g gneiting and katzfuss 2014 and is widely used in the hydrometeorological forecasting community results shown in this case study are from leave one month out cross validated forecasts shrestha et al 2015 5 results 5 1 case study the calibration produces reliable forecast ensembles at individual lead times for both access g and pme forecasts fig 1 a c conversely forecasts of accumulated precipitation fig 1d f show clear differences in reliability between calibrated access g and calibrated pme forecasts calibrated pme forecasts are clearly much less reliable than calibrated access g forecasts the transposed s shape of the calibrated pme forecasts indicates an underconfident forecast i e the ensemble spread is too wide pit diagrams of 72 h rainfall accumulations show that calibrated pme forecasts become more reliable as lead time increases fig 1g i forecasts of 72 h accumulations are very reliable at the longest lead time i e 144 216 h as we have shown that calibrated pme ensembles at individual lead times are well calibrated this fault must arise when ensemble members are reordered with the schaake shuffle to understand why we need to consider the autocorrelation of observations and the uncalibrated forecasts shown in fig 2 autocorrelation of the uncalibrated access g forecasts is very close to that of the observations in contrast the autocorrelation of the uncalibrated pme forecasts is consistently higher than that of the observations for all lead times the high autocorrelation of the pme forecasts is a natural consequence of averaging over multiple nwp models each model will tend to rain at different times and averaging smooths the forecasts in time essentially the schaake shuffle is constrained in the range of values it can reorder from conditional distributions at each lead time when the means of the conditional distributions are highly correlated the cross correlation between the forecasts and observations becomes weaker at longer lead times as a consequence the values sampled from the predictive distributions become less constrained therefore calibrated pme forecasts for 72 h totals become more reliable at longer lead times fig 1g i the cross lag correlation of pme is also shown in fig 2 here we define the cross lag correlation of pme forecasts as the correlation of pme forecasts at lead time 3 h to observations at lead time 6 9 12 h and so on standardised by the cross correlation i e correlation between the pme forecasts at lead time 3 h and observations at lead time 3 h the decay of the cross lag correlation of pme forecasts with lead time is very similar to the decay of autocorrelation of the pme forecasts with lead time analysis of the synthetic example see next section reveals that calibrated forecasts are underconfident when the autocorrelation of the forecasts are higher than the observations 5 2 synthetic example at individual lead times all synthetic calibrated forecasts are perfectly reliable by construction not shown this is equivalent to the results from our case study shown in fig 1a c as expected calibrated ensemble forecasts of accumulations are reliable when uncalibrated forecasts and observations are independent ϕ 0 when uncalibrated forecasts and observations are independent this is analogous to an uncalibrated forecast that displays no skill in these cases calibration will return climatology forecasts which are reliable by construction the schaake shuffle introduces climatological rank correlation patterns leading to reliable calibrated ensemble forecasts of accumulations for moderately high values of ϕ calibrated ensemble forecasts of accumulations are reliable if the autocorrelation of uncalibrated forecasts and observations are similar fig 3 b this is consistent with the result of access g fig 1 however we see clear differences in reliability when the autocorrelation of the uncalibrated forecasts differs to that of the observations fig 3a and c when the uncalibrated forecasts are more autocorrelated than the observations ρ f 0 7 calibrated ensemble forecasts of accumulations are too wide equivalent to fig 1d f signified by a transposed s shape fig 3c when uncalibrated forecasts are less autocorrelated than observations ρ f 0 3 calibrated ensemble forecasts of accumulations that are too narrow signified by an s shape fig 3a 6 discussion and conclusions variables that are aggregated in time by either summing or averaging are often of central importance to applications of calibrated nwp forecasts perhaps the most obvious of these applications is streamflow forecasting hydrological models aggregate precipitation over time to simulate streamflow and therefore forecasts of precipitation accumulations have a profound bearing on streamflow predictions therefore the problem we describe in this study is likely to have wide reaching deleterious consequences the problem we have described here is broader than our case study we believe it will arise in any case where autocorrelation between forecasts is sufficiently dissimilar to observations as we have shown one source of such dissimilarities is averaging across deterministic nwp models but it could also occur when calibrating the mean of ensemble nwp models whether these ensembles are generated as burst ensembles e g access ge dare et al 2016 or time lagged ensembles e g ncep climate forecast system version 2 http www cpc ncep noaa gov products cfsv2 cfsv2seasonal shtml further dissimilarities in autocorrelation characteristics may occur even with deterministic nwp models for example when calibrating gridded nwp models to point observations e g cattoën et al 2020 in addition we have focused on mismatches in autocorrelations for a single variable but similar analyses could be performed for spatial correlations and inter variable correlations both of which the schaake shuffle was originally designed to re order clark et al 2004 in the case of the pme forecasts a potential solution is to calibrate the component nwp models independently because formal calibration methods ensure ensemble members are exchangeable the calibrated forecasts from all nwp models can then simply be combined into a superensemble note however that this approach assumes that the component nwp models all have similar autocorrelation properties to observations this solution is more computationally demanding in that it requires the calibration of 8 models not 1 but is likely to avoid the mismatch in autocorrelation similar reasoning can be extended to the calibration of ensemble nwp models in this study we use the schaake shuffle clark et al 2004 as a method to reorder ensemble forecasts since the problem comes from the uncalibrated forecasts other ensemble reordering methods e g schefzik et al 2013 may also face the issue of poor reliability in accumulated variables in cases where deterministic nwp forecasts exhibit substantial differences in autocorrelation or spatial correlations to observations there is no straightforward solution we recommend that practitioners and researchers test for such differences with analyses like those performed in fig 2 if substantial differences in correlation properties are identified new methods to reorder ensembles may need to be developed credit authorship contribution statement durga lal shrestha methodology investigation formal analysis visualization software writing original draft d e robertson supervision conceptualization methodology formal analysis validation writing review editing j c bennett methodology formal analysis validation writing original draft q j wang supervision conceptualization methodology validation writing review editing declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements this research has been supported by the water information research and development alliance wirada between the bureau of meteorology and csiro land water we would like to thank dr seline ng csiro for valuable suggestions on an earlier draft and two anonymous referees for their helpful and constructive comments 
5444,reliable prediction of influent time series has demonstrated importance in high efficiency performance of wastewater treatment and reuse plants computational models are powerful tools that has been used for predicting influent time series but one of the major drawbacks of developed models is uncertainty analysis in instantaneous and multi time step prediction of influent time series for this purpose a multi objective shark smell optimization mosso algorithm is hybridized with multilayer perceptron mlp neural network radial basis function rbf neural networks and support vector machine svm to optimize the models these hybrid models of mlp mosso rbf mosso and svm mosso are used for predicting different prediction horizons of immediate short term and long term of influent time series accordingly there are three objectives in mosso 1 minimize mean absolute error mae in different input vectors for best selection lag time influent time series 2 achieve the optimal architecture of forecasting models by minimization root mean square error rmse for model parameters and 3 select accurate activate functions or kernel function of models for finding high accuracy of estimating models the studied hybrid multi objective models are modified using taguchi method for determination of pareto optimal solution sets the proposed hybrid models and standalone models of mlp rbf and svm are evaluated using monte carlo uncertainty analysis common evaluation criteria and taylor diagram in evaluation and reliability assessments the results demonstrated that mlp mosso model produce better results than rbf mosso svm mosso and standalone models in all prediction horizons s1i4 immediate q t 5min q t 10min s2i16 short term q t 12hr q t 24hr s3i15 long term q t 2day q t 4day the mlp mosso model for immediate prediction horizon q t 5min q t 10min with one hidden layer and five neurons in hidden layer correlation coefficient r2 of 0 95 root mean square error rmse of 1 1 nash sutcliffe efficiency nse of 0 91 rmse observations standard deviation ratio rsr of 0 14 and percent bias pbias of 12 for testing phase was chosen as the final and best model for predicting influent time series furthermore it was found that all models have some uncertainties but the uncertainties in the mlp mosso model with immediate prediction horizon was less than other models for predicting influent time series finally based on the optimized results an explicit equation is derived for the best mlp mosso model to predict multi step influent time series keywords explicit equation hybrid model immediate prediction horizon multi objective shark smell optimization time series uncertainty analysis 1 introduction municipal wastewater treatment plants wwtps are very important in wastewater reuse managements because of the quick development of urbans growing population and the necessity of treating high volume of untreated sewages singh et al 2016 the scheduling and management of wwtps is important in environmental and water resource management zeng et al 2016 the most important condition changes such as influent time series variations and quality variations are important in performance of wwtps kim et al 2016 several operations such as optimal scheduling of sewage pump wei et al 2013 inter catchment wastewater transfer method for sewer overflow mitigation zhang et al 2017 and chemical injections are dependent on the inflow time series to the wwtp li et al 2019 in addition wastewater properties such as biochemical oxygen demand bod total suspended solids tss and ph are strongly correlated to the influent flow rate wei et al 2013 influent time series to the inlet of wwtp is an extended dynamic influent dataset martin and vanrolleghem 2014 therefore prediction of reliable high frequency influent data is necessary to develop and assess control strategies of wwtps langeveld et al 2017 also in order to improve the wwtps process efficiency instantaneous flows management and energy saving it is important to predict influent flow rates in the immediate and short term prediction horizons wei and kusiak 2015 the precise prediction of influent flow rate is an important issue in the wwtps and wastewater industry there are several approaches that have been proposed for analysis completing and forecasting influent inflow rate mostly categorized in physical models lin et al 2010 karpf and krebs 2011 gernaey et al 2011 flores alsina et al 2014 conventional statistical models abunama and othman 2017 ansari et al 2018 boyd et al 2019 li et al 2019 and artificial intelligence forecasting models physical models are suitable to predict physical process such as contaminant transport in conduits consider dynamic of influent as well as rain events langeveld et al 2017 the statistical models of regression model autoregressive ar autoregressive moving average arma and autoregressive integrated moving average arima models are history based models over past and current data boyd et al 2019 fortunately with the rapid growth of soft computing models several intelligent algorithms such as artificial neural networks anns el din and smith 2002 wei and kusiak 2015 kim et al 2016 szeląg et al 2017 support vector machines svms szeląg and gawdzik 2016 ansari et al 2018 and deep learning techniques lstm zhang et al 2017 are developed and can be applied for influent time series predictions however physical forecasting models cannot adequately reproduce the dynamic influent data as they require very detailed information about boundary conditions population and drainage of study area complicated parameter such as rainfall infiltration runoff and snowmelt that vary in time precise data for calibration of parameters and large scale monitoring these situations make it difficult to calibrate this type of models el din and smith 2002 bittelli et al 2010 kim et al 2016 the mentioned stochastic parameters that effect on influent flow rate are generally too complex to simulate also domestic water use affects influent flow rate which is difficult to be combined with such physical models in addition developing physical models for predicting influent flow rate is affected by the exhaustion of pipes and connections as influent structures that make it more difficult tan et al 1991 ashley et al 1999 zhang et al 2019 the statistical models in wet weather flow conditions with nonlinear complex variations that affect the dynamics of the influent time series lose their performance talebizadeh et al 2016 furthermore the efficiency of artificial intelligence models is highly affected by its structure and learning process also based on the previous studies and best knowledge of authors no conventional and single predictive model can perform with highest accuracy on all cases due to its disadvantages tian and hao 2018 hao and tian 2019 for instance the conventional learning algorithms such as gradient descent suffer from some major drawbacks such as slow convergence speed high dependency of the model results and convergence to the initial parameters and easily get stuck in a local optimum da and xiurun 2005 zhu et al 2005 by trapping algorithm in local optimum improving learning using application of more training iterations is failed cetin et al 1993 the most conventional intelligent methods lose population diversity during operation process that lead to parameter convergence and dissipation of computational resources lin et al 2016 due to drawbacks of single intelligence models poor selection of initial parameters and their values will result in unsatisfactory fitting errors wang et al 2016 also the individual forecasting models cannot always capture nonlinear characteristics of streamflow and influent flow time series because of the inherent deficiency of each model so in order to diminish negative effects of single models and to overcome these problems the hybrid models have developed to provide better predicting accuracy li et al 2011 generally the hybrid models combine different single forecasting models such as anns adaptive neuro fuzzy inference system anfis svm with optimization algorithms such as genetic algorithm ga particle swarm optimization pso firefly algorithms fa grey wolf optimization gwo dragonfly algorithm da shark machine learning algorithm smla to improve the effectiveness of forecasting model yuan et al 2014 abedinia and amjady 2015 garcía nieto et al 2016 yaseen et al 2017 aljarah et al 2018 barman et al 2018 maroufpoor et al 2019 zounemat kermani and mahdavi meymand 2019 dehghani et al 2019 shekofteh and masoudi 2019 chen et al 2019 bi and qiu 2019 adedeji et al 2020 li et al 2020 stochastic meta heuristic optimization algorithms are reliable alternative to alleviate conventional gradient based algorithm drawbacks aljarah et al 2018 recently dehghani et al 2019 proposed a hybrid model for short term to long term influent flow forecasting based on the hybrid anfis model with the gwo algorithm they concluded that hybrid anfis gwo model exhibit more preferable predictive performance compared to anfis model developing a robust model that is able to search global optimal solutions for influent time series is critical for this aim finding an accurate and more efficient optimization algorithm is important to overcome the drawbacks of other methods recently the meta heuristic techniques such as shark smell optimization sso algorithm have been successfully used to improve the performance of single objective function for real world engineering and optimization problems abedinia and amjady 2015 ehteram et al 2017 suliman et al 2019 wei and stanford 2019 rao et al 2019 the evolutionary techniques such as sso are easy and have simplicity advantage to handle non convex and non linear relationships due to their efficiency reddy and nagesh kumar 2007 and fast learning speed since the major purpose of optimization algorithms is to find the global optimum solution which the efficiency of algorithms is effective in global diverse exploration jaddi et al 2015 the sso technique has some merits over the conventional approaches deb 2001 reddy and nagesh kumar 2007 jaddi et al 2015 including a the sso applies a population set of solutions in each iteration and suggests different alternatives in a single run b the sso uses stochastic search and randomized initialization in search space and c the sso is able to find global optima in problems which is a step to balance exploitation and exploration of algorithms previous studies on streamflow and sewage flow forecasting taormina et al 2015 adnan et al 2018 dehghani et al 2019 are based on enhancing forecasting accuracy by single objective optimization algorithms that ignore the importance of prediction stability improvement and uncertainty of results however forecasting stability is also an important aspect when considering a forecasting model wang et al 2017 the average of quantile lichtendahl et al 2013 papacharalampous et al 2020 error variance wang et al 2017 wu et al 2020 and uncertainty zake et al 2019 of prediction offer the stability in performance of models the small uncertainty indicates small error variance of model forecasting the stability analysis of forecasting model is performed with the aim determining level of uncertainty zake et al 2019 in addition input feature selection is a necessary pre processing technique that makes model training faster and less complex also structure of network is a key factor that affects performance of network however former studies suppose pre defined input variables and a fixed structure for network training hence in order to achieve a forecasting method for higher levels of accuracy and stability and also larger forecasting horizons optimal selection of input variables ghadimi et al 2018 mafarja and mirjalili 2018 and model structure faris et al 2019 is necessary it should be noted that simultaneous optimization of the artificial intelligence ai model structures and weights of forecasting models can drastically increase the number of ai model parameters karaboga et al 2007 which make it a multi objective optimization problem as mentioned before heuristic algorithms are useful in real challenging problems solving but have many difficulties in optimization problems because of the existence of multiple objectives mirjalili et al 2017 2018 to deal with these issues multi objective optimization algorithms should be equipped with proper operators such as crowding distance leader selection strategy and taguchi method the combination of these operators is helpful for the multi objective optimization algorithms to effectively find true pareto optimal fronts reddy and nagesh kumar 2007 mirjalili et al 2017 multi objective optimization considers achieving solutions of problems with several objectives the role of a multi objective optimization model is to find the best trade offs of objectives as the pareto optimal solutions that map the objective space and is called the pareto front mirjalili et al 2017 and these results of optimization are evaluated based on the pareto dominance operator asrari et al 2015 in the current paper a hybrid multi objective model is established to obtain more stable and accurate influent time series prediction which mainly contains three modules optimization module forecasting module and evaluation of model accuracy and model uncertainty module a quick and new algorithm namely multi objective shark smell optimization algorithm mosso based on nature inspired shark smell optimization algorithm ehteram et al 2017 is developed as a solution for predicting immediate short term and long term horizons of influent time series that is able to select optimal feature and optimizing model structure the mosso is a multi objective method of meta heuristic shark smell optimization algorithm that have important properties this algorithm is including high quality non dominated solutions on the pareto front of problem that have proper diversity khalili damghani et al 2013 the main aim of mosso is to search accurate solutions of pareto optimal solutions with the maximum diversity zhou et al 2011 the quick movement to the pareto optimal front for accessing the best solution is one of the advantages of mosso over the conventional techniques mirjalili et al 2018 another drawback of artificial intelligent ai methods is their stochastic nature so no unique solution can be derived on different trials kasiviswanathan and sudheer 2013 in order to improve the consistency and reliability of ai model results the analysis of uncertainty in ai method results have the greatest importance in model development and evaluations srivastav et al 2007 uncertainty states the conditions that are not accurately quantifiable and can be illustrated as a defective in data processes models and information goodarzi and eslamian 2014 in this research uncertainty analysis based on monte carlo simulation method is done to evaluate and compare the validity of developed models monte carlo simulation is a numerical method that reproduce variables randomly based on the special statistical distribution goodarzi and eslamian 2014 due to the importance of influent as a water resource and wwpt operational dependency on influent flow rate also because there are not enough studies on influent flow prediction in immediate and short term horizons more researches are needed to determine the performance of intelligence and multi objective hybrid models in influent flow rate prediction and uncertainty analysis based on the aforementioned studies in the current study the mosso hybridized with single models of multilayer perceptron mlp neural network radial basis function rbf neural networks and svm models for optimizing inner parameters and to reach high forecasting accuracy moreover three hybrid models mlp mosso rbf mosso and svm mosso are developed and implemented for influent time series prediction in the forecasting module finally evaluation module uses evaluation criteria and uncertainty analysis to verify and compare forecasting effectiveness of mlp mosso rbf mosso and svm mosso models in influent flow rate forecasting for various input scenarios the major contribution and aims of the current study are 1 development of a new robust multi objective algorithm namely multi objective shark smell optimization algorithm mosso the technique uses leader selection strategy for selecting best solution among non dominated pareto optimal solutions the crowding distance criterion for enhancing diversity and appropriate distribution of solutions to reach true pareto optimal fronts and taguchi method for the best selection of mosso parameters values 2 development a feature selection technique based on mosso optimization to obtain minimum as well as powerful subset of lag time influent flow time series 3 using mosso algorithm to achieve optimal and brief structure of forecasting models 4 develop and evaluate hybrid models of mlp mosso rbf mosso and svm mosso for forecasting accurate influent time series especially in immediate lag times 5 using monte carlo simulation method to quantify uncertainties associated with the various forecasting models mlp mosso rbf mosso and svm mosso of influent time series at different prediction horizons 6 compare the accuracy of developed hybrid mlp mosso rbf mosso and svm mosso models with the standalone mlp rbf and svm models this paper is organized as follows section 2 provides description of study area data and the methods of the developed hybrid system applied in the study section 3 presents experiments analysis of the results and discussions in view the results finally conclusion of the study is provided in section 4 2 material and methods an estimating approach was developed for forecasting influent flow rate time series and to compare the simulation results in sewer network in iran the work aspects are including 1 gathering influent flow rate time series data set 2 selecting different input vectors from immediate to long term horizon 3 using multi objective optimization method to perform pareto fronts by evaluating model error statistical and to find best model parameters 4 assessing differences in performance of models and 5 investigating uncertainty analysis of the best models for selecting optimum result further details on the framework and modeling influent flow rate are described in following sections 2 1 time series data and preprocessing the present study investigates the prediction of influent flow time series based on the different time lags from 5 minutes very immediate short term up to 10 days long term by new hybrid optimization models for this purpose the recorded influent discharges time series data from a local sewer network station in isfahan city were used the isfahan city is located in the central of iran between 51 26 27 and 51 49 58 eastern longitude and 32 40 49 and 32 32 38 northern latitude the elevation of the study area is 1575 m above the sea level and have arid climate condition falahatkar et al 2011 annual minimum and maximum air temperature of this city 10 18 and 24 18 c respectively isfahan meteorological organization 2014 in this station the influent flow rate is measured using portable cross correlation ultrasonic flow meter pcm 4 model and is gathered in a data logger the pcm 4 is able to temporarily measure flow in pipes and channels the measurement approach is based on the ultrasound reflection principle the pcm 4 use a sensor which simultaneously measures flow velocity as well as flow level the time between transmission and reception of a signal that reflected from the water surface is measured and then the water level is calculated for measuring flow velocity two signal scans are taken from flow and its particles at two different time and are evaluated the flow velocity is determined based on the beam angle the interval time between both transmitted signals and the pattern of each single measurement window this device is able to measure water level in range 0 2 m and water velocity up to 6 m s 1 the location of sewer network station and the applied device for recording influent flow rate in different time are given in fig 1 a time series data set is a collection of observation that each one was recorded at a specific time t and with a special interval relative to other data makridakis et al 2008 the influent flow rate data are measured at 5 min intervals in this station the data from 9 may 2011 to 22 may 2011 are available the data set includes 3683 data points with 5 min lags that is divided into training and testing sets with 2000 equal to 67 of total data and 1000 equal to 33 of total data data points respectively to develop the hybrid models table 1 summarizes the characteristic of collected influent flow rate for all train and test data sets according to the table 1 the measured influent flow data varies from 28 1 to 181 94 l s 1 and the mean flow is about 101 3 l s 1 one of the most important issues to forecast and for applying time series is the length of time series if the time series contain sufficient length and covers high and low flows then it has forecasting capability in this paper the hurst coefficient hcoeff equation was used for investigating this problem hurst et al 1965 1 log r s log a h coeff log n where r is variation range of data s is the standard deviation of data a is constant coefficient and n is the number of data according to the hurst coefficient if hcoeff 0 5 the time series known as a random process with normal distribution that its mean value is predictable the hcoeff 0 5 shows an anti persistence or anti correlation process with long length time series when hcoeff 0 5 the time series length is adequate to study the long term characteristics of influent data and to develop models it classified as persistent time series doucoure et al 2016 in this study the value of hurst coefficient was computed as 0 783 for the studied influent flow time series this result confirms that the length of applied influent inflow time series data is adequate to develop models and also these series are predictable three scenarios from prediction horizon were considered in this study the horizon concept is defined as the time in advance that estimation is issued cortez 2010 since immediate lags are particularly useful for influent flow rate series these types of series were considered in this study forecasting influent flow rate based on the immediate horizons have a strong affect in several domain such as effluent plant planning finance issues agricultural and industrial supply the studied horizons in present study are described as table 2 1 immediate prediction horizon in this scenario the influent flow rate in very short time duration of 5 and 10 min are considered in input variable for modeling forecasting horizon predicts the influent flow rate q t based on the values of influent flow rate in periods t 5 q t 5min t 10 q t 10min and t 15 q t 15min q t f q t 5 m i n q t 10 m i n q t 15 m i n 2 short term prediction horizon the influent flow rate in short time duration from one hour to 72 h are used for predicting q t in fact forecasting horizon predicts q t based on the values of influent flow rate in periods t 1hr t 2hr t 6hr t 12hr t 24hr t 48hr and t 72hr q t f q t 1 h r q t 2 h r q t 6 h r q t 12 h r q t 24 h r q t 48 h r q t 72 h r the models that were used short term prediction horizon are known as hourly predictive models 3 long term prediction horizon the influent flow rate in long time duration from one day to 7 days are applied for forecasting q t forecasting horizon calculates q t based on the values of influent flow rate in periods t 1 day t 2 day t 3 day t 4 day t 5 day t 6 day and t 7 day q t f q t 1 d a y q t 2 d a y q t 3 d a y q t 4 d a y q t 5 d a y q t 6 d a y q t 7 d a y these models are known as daily predictive models in time series analysis proper selection of input variables with different lag times affects model capability for output forecasting and is a key problem to develop accurate forecasting models nowadays there are several different techniques for identifying the models input variables and selecting optimum subsets that have been used in this study the hybrid multi objective algorithm was applied to find the appropriate input output combination of influent flow rates at different lag times for this the input variable selection was performed dependently on the learning algorithm as wrapper method to automatically select the most appropriate inputs oliveira et al 2002 gonzález et al 2019 the svm rbf and mlp models optimized by mosso algorithm were used to make the best input selection and to find the optimal model parameters for excellent forecasting influent flow rate 2 2 artificial neural network ann models ann models are intelligent dynamic system that have been used commonly for modeling and forecasting purposes to solve engineering problems in water and wastewater resources ansari et al 2018 babaei et al 2019 a common ann usually includes three layers of input output and hidden layer each layer consists some neurons as the basic building blocks of the model the input layer comprised input variables that connect to output and hidden layer using weights the common relation between input and output layers in ann network can be written as fazeli et al 2013 2 y k f 0 j w kj f h i w ji x i b j b k where i is the input layer j is the hidden layer xi is the input value to node i wji is the connection weight of input nodes i to hidden node j yk is the output of node k fh is the transfer function in hidden layer f0 is the transfer function for output layer bj represents the bias of jth hidden layer bk is the threshold value in the kth output neuron and wkj is the connection weight between j th neuron in the hidden layer to the kth neuron in the output layer the most widely mlp network with a learning error back propagation bp algorithm has been applied as one of the ann models in this study the mlp networks are extremely flexible general and nonlinear model that their complexity can be adjusted by varying the number of hidden layers murayama 2012 the mlp network has one input layer one or several hidden layers and one output layer with linked nodes and functions guo et al 2015 in mlp each neuron in each layer is linked to all neurons in the prior layer and the output of each layer constitutes input vectors of the next layer fig 3 bozorg haddad et al 2018 using from one of transition functions of sigmoid hyperbolic tangents linear and sigmoid tangents for training hidden and output layers are common in mlp the optimum hidden layer size of mlp network was determined using optimized mosso algorithm from 5 to 30 in this paper the transition functions in hidden and output layers were determined by defining an objective function in modeling process and were employed for calibration the network for influent flow rate time series modeling by mlp the data in each horizon are introduced into mlp network the signal passes from neurons by weights and transition functions govindaraju 2000 and the bp train the network by adjusting weights to minimize the objective function guo et al 2015 another ann model that commonly was used in researches is rbf the rbf is a feed forward network and have only one hidden layer fig 3 the hidden layer in this network utilizes a nonlinear radial basis transfer function from input layer to the hidden space as activation function shen et al 2011 the output was determined according to radial distance from a center vector the numbers of neurons in hidden layer weights values biases values and activation functions for mlp and rbf networks were determined using applied multi objective algorithm 2 3 support vector machine svm model the svm is a supervised learning algorithm and one of the most useful tools that was used for solving complex nonlinear problems wei et al 2013 the svm was introduced by vapnik 1995 that can construct hyperplanes in a high dimensional feature space with a set of optimized nonlinear kernel functions wang et al 2005 therefore the nonlinear problem transformed into linear function problem odintsev and miletenko 2015 the forecasting accuracy would be enhanced by applying suitable kernel function meng et al 2019 the linear equation of svm decision function f x is given as 3 f x w tr x b where x is the input variable b is the bias value w is the weighting vector that used for variables vector and tr is the transpose symbol the svm involves solving the following optimization problem to minimize the error function minimize 1 2 w 2 c i 1 m ξ i ξ i 4 s u b j e c t t o w i x i b y i ε ξ i where c is the penalty factor is the symbol for vector length m is the number of training data ξ i and ξ i are the slack parameters of the model the structure of svm model is shown in fig 3 totally there are several kernel functions for svm including linear polynomial radial basis function rbf sigmoid and gaussian in the current paper gaussian hyperbolic tangents and laplace kernels are used as the kernel functions the best kernel function is determined based on the objective function in applied multi objective algorithm the equations of applied kernel function are presented as 5 gussian k e r n e l f u n c t i o n k x x i e x p x x i 2 2 γ 2 6 laplace k e r n e l f u n c t i o n k x x i e x p x x i 2 σ 7 hyperbolic t a n g e n t k e r n e l f u n c t i o n k x x i t a n h k x i x j c where γ σ c and k are the kernel functions parameters that were determined using optimization 2 4 shark smell optimization sso application of meta heuristic optimization algorithms have attracted widely considerations over the last two decades due to characteristics of avoiding from local optimal flexibility simplicity and derivation free mechanism mirjalili et al 2014 the sso algorithm is a meta heuristic nature inspired optimization algorithm that first introduced by abedinia et al 2016 this optimization algorithm mimics the superior hunting behavior of sharks which are able to sense the odor and local of prey the blood odor concentration of prey in water is an effective factor to move of shark toward prey abedinia and amjady 2015 the movement behavior of shark to the odor source is shown in fig 2 the fig 2 show the shark rotation on a closed contour red arrows and search path black arrow toward the prey the sso algorithm includes three phases of initialization of odor particles forward movement of sharks rotational movement and position update to obtain the location of prey during the initialization of odor particles for the optimization problem one odor source is created in the environment of shark search due to one injured fish prey abedinia et al 2016 the search process begins when the shark smells odor in order to model this process the sso algorithm generates a random population of initial solution odor particles within the feasible search space x x 1 1 x 2 1 x np 1 for a np population size to search the solution space for finding optimum solutions each solution represents the possible shark position based on the odor particle each individual in initial position vector has a primary velocity vector as v v 1 1 v 2 1 v np 1 to become closer to the prey with stronger odor particles the ith initial candidate solution that is ith probable shark position is written as x i 1 x i 1 1 x i 2 1 x i n d 1 i 1 n p consequently in forward movement of sharks the velocity of shark will be changed and increased by increasing the odor particles concentration v i 1 v i 1 1 v i 2 1 v i n d 1 i 1 n p the mathematical of objective function for shark s velocity at kth iteration is defined as 8 v ij m m i n μ m r 1 o f x j x i j m α m r 2 v i j m 1 γ m v i j m 1 i 1 n p m 1 m j 1 n d μ m 0 1 where γm is the upper bound of current velocity for the previous one µm is the gradient constant αm is the momentum effect between 0 and 1 m is the stage number of shark forward movement and r1 and r2 are random number between 0 and 1 based on the velocity vector the position of shark updates as 9 z i m 1 x i m v i m δ t m i 1 n p m 1 m where δtm is the time interval in mth stage in rotational movement phase shark moves toward the prey by rotational movements for finding stronger odor particles hence the rotational movements of shark are considered as local search process in sso algorithm also in position update phase the best point is selected by shark the local search and position update phases are modeled as 10 γ i m 1 l z i m 1 r 3 z i m 1 i 1 n p m 1 m l 1 l 11 x i k 1 a r g m a x o f z i m 1 o f γ i m 1 1 o f γ i m 1 l i 1 n p where r3 is a random number between 1 and 1 and l is the number of points in the local search the sso algorithm starts optimization with user defining some parameters including np m γm µm and αm a set of random solutions is generated as the initial population and each decision variable of xij 1 is randomly generated within the allowable ranges the stage counter is initialized and the velocity vectors are calculated after that the updated location of shark is obtained by onward movement eq 9 in next step the local search is performed eq 10 to obtain γi m 1 l then the position of shark is updated and the next shark position is selected between the positions obtained from forward movement and local search eq 11 finally the best location of shark as best solution with the highest objective function is selected throughout optimization when the end condition is satisfied 2 5 multi objective shark smell optimization algorithm mosso a general multi objective optimization mo problem includes several objective functions and associated constrains oliveira et al 2002 which have mostly confliction with each other the important aim of mo problems is finding optimal solution from a special optimization algorithm wu et al 2020 since obtaining an optimal solution to simultaneously optimize all objective functions is almost impossible in mo problems hans 1988 due to the nature of mo problems mo problems create a set of solutions to generate most appropriate trade offs between the objectives that called pareto optimal solutions set mirjalili and lewis 2015 mirjalili et al 2018 the pareto optimal front creates an opportunity to evaluate the solutions in a multi objective domain mirjalili et al 2018 behnood and golafshani 2018 if a solution performs better over another in all experiment criteria it is dominant the multi objective sso algorithm mosso is based on the sso algorithm that use hunting technique of shark for obtaining optimal solution actually the main aim of mosso algorithm is to obtain very precise estimation of the proper pareto optimal fronts in order to apply meta heuristic sso algorithm for solving mo problems the concept of archive was used to store the best approximations of pareto optimal solutions during optimization a pareto archive is a set of non dominated pareto optimal solutions the archive must update frequently in any iteration and it may become full through optimization therefore it is necessary to achieve a mechanism for managing the archive in the beginning of mosso algorithm the positions of the sharks are the possible solution of optimization problem and the archive is empty by running the optimization algorithm the solutions are non dominated add to the archive and built the pareto front of the optimization problem the archive may remain empty in different iterations it means the algorithm has not found non dominated solution yet in addition the archive should be control and update in each iteration by finding new non dominated solutions during iterations actually the archive is responsible for saving the obtained non dominated pareto optimal solutions in hybrid mlp mosso rbf mosso and svm mosso models triple objective functions including 1 select optimal input combination by minimizing mae criterion 2 select best values of model s parameters by minimizing rmse criterion and 3 select best activation or kernel function of models by minimizing obj criterion were optimized simultaneously due to triple objective functions were conflicting it was impossible to recognize one solution for satisfying all objective functions therefore the concept of the pareto optimal fronts or non dominated solutions was applied in this study typically to support the decision maker for finding the final solution and to prevent direct effect of decision maker on solving mo algorithms using from reference points was introduced as an attractive and popular method miettinen and mäkelä 1999 miettinen and mäkelä 2002 luque et al 2009 a reference point includes desirable values for each objective function actually the reference point is projected to the non dominated solutions set and the best non dominated solution is found based on the more closely point to reference point in this study several single reference points were determined by solving the sso algorithm as single objective for any objective function which provides the best non dominated solution based on the single objective optimization technique the previous studies shown selecting several reference points can improve the performance of mo algorithms li et al 2018 decision about selecting the best solution from the pareto optimal is not simply for mosso algorithm selecting the appropriate non dominated solution among the different alternatives in the set of pareto optimal solutions is important therefore leader selection strategy is applied to choose the best solution among the best non dominated solutions in archive for the mo optimization problems the technique for order preference by similarity to ideal solution topsis ranking approach was introduced as a practical and classical method for selecting the accurate non dominated solution tzeng and huang 2011 ge et al 2017 based on the topsis concept the best non dominated solution in mo problems should have the shortest distance from the positive ideal solution reference point d and the largest distance from the negative ideal solution the worst possible status d hwang yoon 1981 two characteristics of d and d are computed based on the three criteria of rmse mae and obj of each solution the relative closeness of each solution is calculated based on d and d as cc d d d the cc values are between 0 and 1 the larger cc value show the best solution of algorithm for each prediction horizon yu et al 2018 the description of topsis method provided in different researches chen 2000 shih et al 2007 tsou 2008 yu et al 2018 the best non dominated solution was chosen based on the reference points and topsis method each selected non dominated solution includes triple objective functions for forecasting influent flow rate as mentioned above the objective functions of hybrid mosso algorithm were considered as 1 minimizing mae value for determining optimal input variables vector 2 minimizing rmse value for determining optimum parameters of models and 3 minimizing obj criterion for determining the best activation or kernel function for estimating influent flow rate time series the crowding distance criterion is applied to enhance diversity and appropriate distribution of solutions the crowding distance chooses and offers the least crowded solutions of the search space the crowding distance was defined as the average distance between each solution and its nearest neighbors in the search space deb et al 2002 to calculate crowding distance the objective function values are firstly sorted in ascending a solution with larger crowding distance and lower rank is selected as the best answer reddy and nagesh kumar 2007 the selection of the best non dominated solutions among offered solutions by crowding distance criterion are done according to roulette wheel mechanism mirjalili et al 2018 behnood and golafshani 2018 the crowding distance cd and roulette wheel criteria are calculated by following equations lalehzari et al 2015 mirjalili et al 2018 12 cd i m 1 3 of m i 1 of m i 1 of i max of i min 13 p i c n i where of is the objective function m is the number of objective functions i is the solution rank in pareto front c is a constant number that is greater than 1 and n is the number of obtained pareto optimal solutions the performance of mosso algorithm depends remarkably to the accurate choices of algorithm parameters values a sensitivity analysis is required to identify how the random parameters for an optimization algorithm affect the objective function value when the objective function value is computed versus the variability in the random parameter values the optimal values of random parameters are computed a robust calibration approach which is named taguchi method is used for the best selection of mosso parameters values in this study the taguchi method provides an opportunity to recognize main factors and possible interaction between parameters as well as to determine the number of corresponding levels for parameters liu et al 2019 the taguchi method can considerably reduce experimental cost and time due to minimum number of trials zhang et al 2015 also this method is a simple and effective statistical tool to optimize the design parameters in complex process such as forecasting influent flow rate time series using mosso algorithm ben arfa et al 2016 the taguchi model performs a sensitivity analysis to determine how the signal to noise s n ratio is affected by the variability in the random parameter values therefore the best parameters values for optimization algorithms can be determined easily by comparing the mean of s n ratio liu et al 2019 in the present study the s n ratio with the properties of higher is better hb is used to evaluate the experimental results as the following equation taguchi 1990 14 s n 10 log 10 1 n i 1 n 1 y i 2 where n is the total number of experiments and yi is the ith observed value the developed mosso algorithm can be summarized in the following steps step 1 the parameters values of mosso algorithm including m αm γm maximum iteration and population size np are initialized according to the taguchi method step 2 the initial position of sharks is randomly initialized as a set of random solution step 3 the objective functions values for each individual in the current population is calculated step 4 the non dominated solutions are found and the pareto front optimal is stored to the archive step 5 the crowding distance is calculated for each solution of the archive step 6 the roulette wheel mechanism and crowding distance criteria are used for selecting the best solution step 7 the velocity of sharks is updated according to eq 8 step 8 the positions of sharks current solution is updated according to eq 9 step 9 the non dominated solutions are determined and the archive is updated with respect to them step 10 all solutions in archive are compared and the dominated solutions by the other solutions are removed from the archive step 12 if the convergence condition is not satisfied then it returns to step 5 otherwise output the non dominated solution set from the algorithm in order to evaluate the pareto fronts performance of hybrid mosso models in the present study three evaluation indices are used the first index is the number of non dominated solutions that stored in the archive obviously an algorithm that maintains a high value of non dominated solutions in the archive is preferred to another in mo optimization problems khalili damghani et al 2013 hamdy et al 2016 the second index for comparing obtained pareto optimal fronts is spacing metric the concept of spacing metric is measuring relative distance of consequent solutions when the solutions distribute uniformly throughout the non dominated pareto solutions set the pareto optimal front consists non dominated solutions with small value for spacing khalili damghani et al 2013 gupta et al 2019 hence based on the spacing metric a model with smaller spacing for non dominated solution is desirable finally the maximum spared or diversity index was used as third index to investigate the performance of pareto fronts of hybrid mosso models the maximum spared or diversity index is calculated based on the terminate values of non dominated solutions set in the solution space the greater values of maximum spared or diversity index indicate to better performance of model 2 6 hybrid artificial intelligence models with mosso algorithm in the present study three artificial intelligence models including mlp rbf and svm in combination with mosso algorithm mlp mosso rbf mosso and svm mosso are used for forecasting influent flow rate time series in different horizons the structure of the forecasting influent flow rate time series by hybrid mlp mosso rbf mosso and svm mosso models is summarized in three objective functions in the initial step an input database including effective input variables or the most important time lags has to be selected hence the first objective function in mosso is related to select the best input variables since each scenario in this study consist several inputs with different time lags the first objective function was defined as minimizing mean absolute error mae in different input vectors as mentioned before the mlp rbf and svm models include several parameters such as weights biases number of hidden layers number of neurons in hidden layer and kernel functions parameters that are unknown for modeling and have to be characterized the architecture of models is usually defined based on trial and error method achieving a simple architecture of mlp rbf and svm models with high accuracy is important for application and users for forecasting the influent flow rate in different horizons by mlp rbf and svm models the related parameters for each model should be accurately determined hence in this study these models are connected to mosso algorithm and the model s parameters are determined and optimized based on the optimization process by allocating the second objective function in mosso algorithm therefore the second objective function is minimization root mean square error rmse for model s parameters finally the third objective function is determined to select accurate activate functions or kernel function for mlp rbf and svm models this objective function is written to choose an optimal kernel among gaussian hyperbolic tangents and laplace kernels for svm model and to select best activation function among sigmoid hyperbolic tangents linear and sigmoid functions for mlp and rbf models in fact the third objective function selects the kernel and activation functions automatically and avoids from personal selection and error trial process in hybrid mlp mosso rbf mosso and svm mosso modeling the flowchart of hybrid mlp mosso rbf mosso and svm mosso models is given in fig 3 and the steps of the hybrid forecasting systems are described as following steps step 1 the data are collected step 2 a prediction horizon from scenarios is selected and the its input dataset is prepared step 3 one of the artificial intelligence models of mlp rbf and svm is chosen step 4 the initial parameters of selected model is initialized step 5 the model is trained step 6 the mae value is obtained based on the estimation and observation influent flow rate values during first objective function for determining optimum input vector step 7 the rmse value is obtained based on the estimation and observation influent flow rate values during second objective function for determining optimum models parameters step 8 the obj criterion is calculated for third objective function from eq 23 to consider the simultaneous effect of mae and rmse from steps 6 and 7 step 9 the convergence condition is controlled if the end condition is satisfied then mosso algorithm goes to the step 14 otherwise it goes to the step 10 step 10 the initial parameters of sso algorithm is initialized step 11 the parameters and input variables entered to sso algorithm and is optimized as decision variable and initial shark population step 12 all steps of mosso algorithm is repeated from step 1 to step 11 step 13 the termination condition is investigated if the termination condition is satisfied then mosso algorithm goes to the step 5 otherwise it goes to the step 10 step 14 the selected model is tested and the results is reported 2 7 uncertainty analysis of hybrid mlp rbf and svm mosso models the error functions in hybrid mlp mosso rbf mosso and svm mosso models are directly related to error in input dataset of models hence the error in models input dataset can be a source of some uncertainty in output results of models therefore it is necessary to evaluate performance of models and to determine the uncertainty of outputs in this study the monte carlo method is used to assess the uncertainty of hybrid models and to generate a set of random samples from influent flow rate noori et al 2015 riahi madvar and seifi 2018 for this aim the random samples values are generated based on the probability distribution functions pdfs the pdf of influent flow rate is needed for monte carlo simulation which affects the quality of simulation results scharffenberg and kavvas 2011 in this study the simlab v2 2 software was used for fitting the pdfs to the influent flow rate the kolmogorov smirnov test is used as the goodness of fit for pdfs with p value 0 05 scharffenberg and kavvas 2011 then a large random dataset of influent flow rate is generated based on the pdfs and monte carlo sampling procedure based on the results of the different experiments for determining appropriate number of samples the number of 2000 samples were satisfied the convergence condition therefore the data sampling process iteration is set to be 2000 times and influent flow rate is reproduced 2000 times by the pdfs pattern for each input variable then the input time series are extracted from the monte carlo generated pdf after that the 2000 samples are trained and a range of output is determined for different prediction horizon of influent flow rate time series to evaluate the uncertainty of models the upper and lower bound are determined by considering 97 5 and 2 5 percent of produced influent flow rate time series from the 2000 times forecasting process respectively the 95ppu bound and the degree of uncertainty d x are used to evaluate goodness of fit of models noori et al 2015 riahi madvar and seifi 2018 15 d x 1 k i 1 k x u x l i 16 d f a c t o r d x σ x 17 95 p p u c o u n t q x l q x u n 100 where k is the number of samples and σ x is the standard deviation of influent flow rate and d x is the average distance between upper and lower bound the best performance is obtained when 100 of the observations are bracketed by 95ppu and the d factor is close to zero 2 8 optimum selection and model evaluation criteria the performances of proposed models were evaluated using several statistical evaluation criteria such as rmse mae nash sutcliffe efficiency nse rmse observations standard deviation ratio rsr percent of bias pbias and objective function obj these evaluation criteria calculate as following equations 18 nse 1 i 1 n o i p i 2 i 1 n o i o 2 n s e 1 o p t i m a l v a l u e 1 19 rmse 1 n i 1 n o i p i 2 0 r m s e o p t i m a l v a l u e 0 20 mae 1 n i 1 n o i p i 0 m a e o p t i m a l v a l u e 0 21 rsr rmse stdev i 1 n o i p i 2 i 1 n o i o 2 0 r s r o p t i m a l v a l u e 0 22 pbias i 1 n o i p i i 1 n o i p b i a s o p t i m a l v a l u e 0 23 obj n train n test n train n test rmse m a e r train 1 0 o b j o p t i m a l v a l u e 0 where oi is the observed value pi is the estimated value ō is the mean of observed values n is the number of data ntrain is the number of training data ntest is the number of testing data and rtrain is the pearson determination coefficient for training phase taylor diagram is a precise graphical tool to compare the results of studied models taylor 2001 that is used in this study taylor diagram presents three appropriate criteria including standard deviation sd centered mean squared difference rmsd and r2 in a single diagram taylor diagram shows the matching status of estimations with the real output the models are programmed and developed in matlab 2016 coding environment and the forecasting parameters are calculated using the trained models with optimized constant and weights in the uncertainty analysis the training process is done using the monte carlo approach and model results are quantified by uncertainty indices and bounds all of the calculations are done using matlab 2016 programming environment 3 results and discussion in this section the obtained results from the purposed hybrid mosso models for forecasting influent flow rate based on the different prediction horizons are presented in the first part of the results the pareto optimal fronts are used to extract the three best predictive horizons by different hybrid mlp mosso rbf mosso and svm mosso models in the second part the performances of the selected predictive models are compared by using several evaluation criteria finally the results of the monte carlo uncertainty analysis are presented for selected models 3 1 optimal value selection of mosso process parameters by taguchi method in mosso algorithm the most desired values of parameters were determined by using taguchi method the parameters values of m αm γm maximum iteration and np were investigated since the number of investigated parameters n is 5 the degree of freedom df n n 1 was calculated as 5 5 1 20 hence the appropriate array at least must have twenty rows each parameter has five levels then the number of run for each parameter was calculated 25 hence 25 different combinations of control factors were considered for each trial and 10 replicates were performed therefore l25 orthogonal array was used to perform the taguchi experiment to obtain the optimal value of each parameter of mosso algorithm after that the obtained results were transferred into s n ratios and the mean values of s n values from 10 replicates were calculated at each level these values represented the optimum values of algorithm parameters in this study the s n ratio plot of mosso algorithm process parameters are given in fig 4 the effect of each mosso algorithm process parameter including αm np maximum iteration γm and m is divided into five levels of 0 1 0 3 0 5 0 7 0 9 100 200 300 400 500 150 300 450 600 750 1 2 3 4 5 and 50 100 150 200 250 respectively the mean value of s n ratios was calculated from 10 replications at each level in this study the higher is better hb mechanism for s n was considered and each value that had greater s n was selected as optimal and more important value for each process parameter as shown in fig 4 the γm parameter exhibits the largest variations in which it first decreases and then increases and finally it decreases where the m maximum iteration np and αm parameters show increasing trend mean value of np m maximum iteration αm and γm were found to vary from 0 23 to 0 06 0 15 to 0 36 0 03 to 0 57 0 38 to 0 38 and 0 53 to 0 73 respectively therefore based on the results in fig 4 the optimum conditions were αm 0 7 np 500 maximum iteration 600 γm 1 and m 100 in proper levels of 4 5 4 1 and 2 respectively as shown in fig 4 the γm parameter had the largest variations followed by the αm parameter it has been demonstrated that the mosso algorithm were mainly affected by γm and αm parameters 3 2 the pareto optimal fronts result for hybrid mosso models the obtained pareto optimal solution for hybrid mlp mosso rbf mosso and svm mosso methods for forecasting influent flow rate time series are given in figs 5 7 respectively the reference points are painted as stars in the pareto optimal solution plots as seen from the figures the reference points were placed around the best solution in the pareto fronts the pareto front of the mlp mosso scenario 1 model had 31 solutions the rmse mae and obj indices varied from 1 to 9 l s 1 1 5 to 4 l s 1 and 3 to 9 l s 1 respectively the values of rmse mae and obj varied from 1 to 9 l s 1 1 5 to 4 l s 1 and 3 to 9 l s 1 for second scenario of mlp mosso and from 1 to 9 l s 1 1 5 to 4 l s 1 and 3 to 8 l s 1 for mlp mosso scenario 3 respectively there are four reference points for each scenario of mlp mosso model the objective functions values rmse mae obj for these reference points were obtained as 2 4 5 7 2 5 7 2 6 and 5 3 5 for mlp mosso scenario 1 also these values were equal to 7 2 5 7 3 5 5 3 6 and 5 2 5 for mlp mosso scenario 2 and 8 3 3 7 2 3 5 2 3 and 5 3 3 for mlp mosso scenario 3 the pareto fronts of rbf mosso and svm mosso models were more extend than the mlp mosso model the results of fig 5 show the best hybrid mlp mosso model for immediate prediction horizon scenario 1 includes q t 5min and q t 10min input variables s1i4 in which the hidden layers number of neurons and activation function for this model were achieved equal to 1 5 and sigmoid respectively this result was also achieved for rbf mosso and svm mosso models the structure characterizes of rbf mosso model for s1i4 scenario are including sigmoid activation function in hidden layer tanh activation function for output layer one hidden layer and seven hidden neurons in hidden layer also the kernel parameters of γ 2 25 and c 58 and activation function of gaussian were produced best structure of svm mosso model in immediate prediction horizon the best solution of mlp mosso model had rmse 7 mae 3 and obj 4 for objective functions the rmse mae and obj values were obtained 4 6 and 7 for rbf mosso and 7 6 and 8 for svm mosso model respectively comparison of these values show that the best solution point of mlp mosso model can reduce more accurate prediction for immediate horizon than the other hybrid models in short term prediction horizon scenario 2 the best predictive mlp rbf and svm mosso models for forecasting influent flow rate were selected based on the time lags of t 12hr and t 24hr as input scenario of s2i16 from table 2 the activation function for hidden layer number of hidden layers and number of neurons in hidden layer were obtained tanh 1 and 3 for mlp mosso and sigmoid 1 and 5 for rbf mosso respectively the results of svm mosso indicated that the gaussian activation function can provide the best results in all prediction horizons this result is in agreement with sharifi garmdareh et al 2018 that showed gaussian membership function produced better performance than the other functions for regional flood frequency prediction in iran the criteria values of rmse mae and obj of objective functions were calculated as 5 3 2 and 5 for mlp mosso 7 5 and 6 for rbf mosso and 7 7 and 8 for svm mosso models respectively finally the best long term prediction horizon scenario 3 were included q t 2day and q t 4day input variables s3i15 for all applied hybrid models in this horizon the activation function of sigmoid were produced best optimal solution for best input combinations of mlp mosso and rbf mosso models in this context the objective functions criteria including mae rmsem and obj were obtained equal to 7 3 and 5 for mlp mosso 5 6 and 7 for rbf mosso and 5 6 and 7 for svm mosso models respectively the results of cc index of topsis method for selecting best solution are given in table 3 as seen from the table 3 the mlp mosso model in scenario 1 had largest value of cc equal to 0 93 after that the cc value of mlp mosso scenario 2 rbf mosso scenario 1 rbf mosso scenario 2 svm mosso scenario 1 and svm mosso scenario 2 were equal to 0 91 0 90 0 89 0 88 and 0 88 respectively therefore it is clear that the best results were obtained by mlp mosso model in immediate prediction horizon which it was placed in rank 1 the svm mosso model showed the worst results based on the topsis therefore the advantage of applied mosso algorithm for saving time by applying triple objective functions and providing simple system for selecting best solutions from pareto fronts using reference points and topsis is clearly specified in which the results are presented simultaneously the mosso approach proposed in this study was very effective for approximating true pareto solutions and had high capability to find the best solution the results of this study are in agreement with siade et al 2019 when applied multi objective swarm particle optimization mopso for finding solutions and predicting groundwater flow in reverse to the previous studies wei and kusiak 2015 dehghani et al 2019 boyd et al 2019 selecting the best input variables vector determining the optimal values of model process parameters and choosing the most accurate active and kernel functions were done automatically and intelligently in the present study dehghani et al 2019 applied anfis and anfis gwo for predicting influent flow rate however they did not used from multi objective optimization for selecting best input combination model structure and activation functions they used gamma test gt feature selection of input combinations also the anfis parameters optimized by using gwo table 3 provides the indices results of hybrid mosso models it can be concluded from table 3 that the proposed hybrid mlp mosso model is able to provide the best results on first index for all scenarios with higher number of non dominated solutions than the other models the number of non dominated solutions of mlp mosso model was obtained 311 for all scenarios where it was 309 for rbf mosso and svm mosso models so it can be stated that the hybrid mlp mosso algorithm has the superior optimal level to construct the pareto optimal fronts the rbf mosso and svm mosso algorithms had similar results for first index it can be seen from table 3 the spacing metric for immediate prediction horizon of mlp mosso rbf mosso and svm mosso models was calculated equal to 2 12 2 25 and 2 17 respectively therefore it can be found the generated solutions using mlp mosso model for scenario 1 were more uniformly distributed throughout the non dominated pareto solution set comparison with the rbf mosso and svm mosso models the lowest value of second index in short term prediction horizon scenario 2 occurred in svm mosso 2 14 model compared with mlp mosso 2 15 and rbf mosso 2 17 models also the finding was revealed that the lowest value of second index for long term prediction horizon was related to svm mosso equal to 2 10 whereas it was obtained equal to 2 11 and 2 16 for mlp mosso and rbf mosso models respectively the difference of second index value between svm mosso and mlp mosso models in scenarios 2 and 3 was low the percentage of difference between these models was almost 0 5 for scenarios 2 and 3 therefore it seems the both svm mosso and mlp mosso models are able to uniformly distribute throughout the non dominated pareto solution set comparison with the rbf mosso table 3 presents the third index values for selected models and the results show that mlp mosso model has the best performance according to table 3 between models in scenario 1 the diversity index value of mlp mosso 7 1 2 was higher than those in rbf mosso 7 1 1 and svm mosso 7 0 6 in short term prediction horizon the third index values of mlp mosso and rbf mosso models was equal to 710 where it was calculated equal to 702 for svm mosso model also the mlp mosso model had best result in scenario 3 with diversity value of 709 when compared with rbf mosso and rbf mosso models with values of 708 and 699 respectively the results of hybrid models for each scenario revealed that mlp mosso model had the capability to be good and robust predictive tools for estimating influent flow rate in term of yielding a set of diverse solutions along the pareto optimal fronts hence the mlp mosso model was able to face with the preliminary converging problem in the search space this result is in agreement with oliveira et al 2002 when used multi objective genetic algorithms for handwritten digit recognition also comparing results of indices between scenarios were indicated that scenario 1 of mlp mosso model had higher values of first 3 1 1 and third 7 1 2 indices however the performance results of second index shown that svm mosso scenario 3 mlp mosso scenario 3 and mlp mosso scenario 1 with values of 2 10 2 11 and 2 12 respectively had close agreement with together totally based on the three applied indices the results of mlp mosso model tend to be better than the rbf mosso and svm mosso models according to the evaluation results the best results of obtained pareto optimal fronts for different models and scenarios show that the pareto optimal front of scenario 1 immediate prediction horizon for mlp mosso model is better and more distributed than the other scenarios and models although the results obtained for the other models are close to the mlp mosso model 3 3 performance analysis of hybrid mosso models statistical comparison between different optimal algorithms show in table 4 it can be found that the highest accuracy for all scenarios is achieved by mlp mosso model in both training and testing phases also the prediction accuracy of rbf mosso model was better than the svm mosso model in both training and testing phases in testing phase the pbias values of 12 27 and 38 were calculated for mlp mosso rbf mosso and svm mosso models in scenario 1 respectively also based on the nse and rsr evaluation criteria in testing phase the highest accuracy was achieved by mlp mosso model with values of 0 91 and 0 14 respectively compared with mlp mosso in scenario 1 the nse 0 87 and psr 0 29 were calculated for rbf mosso and nse 0 82 and psr 0 42 for svm mosso model the similar results were achieved in training phase of scenario 1 therefore it is clear that the performance of mlp mosso model was better than the other hybrid models in addition by comparing the accuracy of hybrid models in scenario 2 in testing phase the mlp mosso model had best performance with nse 0 90 rsr 0 16 and pbias 16 than the rbf mosso with nse 0 85 rsr 0 32 and pbias 29 and svm mosso model with nse 0 0 84 rsr 0 45 and pbias 40 also in testing phase the nse rsr and pbias values of scenario 3 were calculated as 0 89 0 19 and 17 for mlp mosso 0 84 0 35 and 31 for rbf mosso and 0 82 0 53 and 47 for svm mosso models respectively on the other hand the svm mosso model in scenario 3 long term prediction horizon had the lowest performance for predicting influent flow rate in this study it can be due to selection of optimal non dominated solutions in the pareto solution fronts based on the statistical comparisons for different prediction horizons the best performances are obtained by scenario 1 s1i4 for all hybrid models however the best result was achieved by mlp mosso model this scenario has time lags smaller than one hour which called immediate forecasting models it is evidence that increasing lag time of prediction from minutes scenario 1 to hour scenario 2 and day scenario 3 decrease accuracy of hybrid models it seems using from input variables with hourly and daily lags for forecasting influent flow rate time series have lower accuracy than the immediate time lags in minutes dehghani et al 2019 compared anfis and hybrid anfis gwo for predicting influent flow rate in different horizons the results of that study showed the performance of models affected by prediction horizons and anfis gwo1 with input vector of 5 min ahead performed better than the other input combinations the predicted values of influent flow rate by hybrid models versus observed values for training and testing datasets are shown in figs 8 10 each model has closer value to 1 for determination coefficient r2 is more accurate it can be seen from figs 8 10 that determination coefficient values of mlp mosso model are greater than those for rbf mosso and svm mosso models in all scenarios the r2 values for immediate prediction horizon scenario 1 s1i4 of mlp mosso rbf mosso and svm mosso models were achieved equal to 0 95 0 88 and 0 85 respectively the determination coefficients of models for immediate prediction horizons are more than those for short term and long term prediction horizons the r2 values of best hybrid model i e mlp mosso during the testing phase are equal to 0 93 and 0 90 in scenarios 2 and 3 respectively while this statistic is calculated as 0 87 in scenarios 2 and 3 for rbf mosso model the svm mosso model show the worst performance with values of r2 equal to 0 82 and 0 78 in scenarios 2 and 3 respectively these results indicate the high accuracy of developed mlp mosso model for forecasting influent flow rate especially using immediate prediction horizon scenario 1 there are very few studies on predicting influent flow rate for comparing results of current study with other researches tan et al 1991 applied a direct k step predictor to predict wastewater flow rate and found this model is able to obtain predictions up to 2 hours ahead chen et al 2014 demonstrated that the arma model can accurately predict the flow rate of sewage stream entering to the pump station a few hours ahead the results of taylor diagram analysis for three best models in influent flow rate time series prediction for training and testing phase are given in fig 11 the observed point is determined based on the standard deviation which is plotted along the horizontal axis the distance of each model to this observed point shows the accuracy of that model where the closer point to the observed point has the best performance in training phase the models of mlp mosso scenario 1 mlp mosso scenario 2 and mlp mosso scenario 3 were grouped around rmse line of 0 4 that had largest correlation coefficient with the observed point the correlation coefficient r for mlp mosso scenario 1 is more than 0 99 where it is between 0 95 and 0 99 for mlp mosso scenario 2 and mlp mosso scenario 3 although the standard deviation of rbf mosso scenario 1 and svm mosso scenario 3 are obtained equal to 0 75 and 0 80 respectively although the standard deviation values of mlp mosso model in all scenarios for scenario 1 1 15 l s 1 for scenario 2 0 90 l s 1 and for scenario 3 1 1 l s 1 were larger than those rbf mosso scenario 1 and svm mosso scenario 3 but the mlp mosso scenario 1 2 and 3 models were closer to observed point the rmse of mlp mosso model in all scenarios was lower than 0 4 while it was between 0 4 and 0 8 for the other hybrid models the similar results were obtained for testing phase the point related to mlp mosso scenario 1 model was very close to observed point with r close to 1 the point of mlp mosso model in scenario 2 and 3 was placed between lines of 0 95 to 099 the r values of other hybrid models were between 0 75 and 0 95 lines the value of standard deviation was approximately 1 15 for mlp mosso scenario 1 0 9 for mlp mosso scenario 2 1 4 for mlp mosso scenario 3 0 8 for rbf mosso scenario 1 1 45 for rbf mosso scenario 2 1 24 for rbf mosso scenario 3 1 1 for svm mosso scenario 1 1 35 for svm mosso scenario 2 and 0 85 for svm mosso scenario 3 the results of rmse criterion in testing phase was similar to training stage according to the results of taylor diagram at both training and testing phases the hybrid mlp mosso model can strongly predict influent flow rate time series especially using immediate prediction horizon after that the accurate of rbf mosso model is higher than the svm mosso model previous studies have identified the mlp model is able to successfully predict short term influent flow rate wei et al 2013 used mlp neural network model to predict short term horizons of influent flow rate up to 180 min by applying influent flow rate rainfall rate and radar reflectivity data in 15 min intervals their results indicated that the prediction accuracy of mlp model reduce after t 30 min wei and kusiak 2015 investigated the capability of mlp neural network model to predict influent flow rate in the wwtp up to 300 min ahead their results showed that mlp provided good predictions up to 150 min ahead by increasing time horizons the accuracy of model decreased the explicit formulas for influent flow rate estimation as a function of input variables were derived for immediate short term and long term horizons from the best hybrid model as mlp mosso short term predictions of the influent flow rate would be important if real time control of different unit processes in a wwpts is to be utilized the previous applied models on influent flow rate prediction are black box and did not provide any mathematical equation for prediction short time influent flow rate in the current paper the explicit equations are provided that can make the results of artificial intelligence more applicable therefore these equations can be used to monitor and estimate of instantaneous influent flow rate at the investigated station in the mlp mosso model the input layer included nodes representing q t 5min and q t 10min for immediate prediction horizon q t 12hr and q t 24hr for short term prediction horizon and q t 2day and q t 4day for long term prediction horizon in the first scenario the best mlp architecture the best solution with one hidden layer four hidden neuron layers a sigmoid function for the second layer and a liner function for the output layer was obtained here regarding a similar explanation the best ann networks can be observed in the figs 5 7 the explicit expressions for the best mlp mosso architectures the best solutions are computed as follows first scenario s1i4 25 sigmoid f z 1 1 e z z 1 2 0 q t 5 m i n 3 1 q t 10 m i n 1 11 z 2 4 1 q t 5 m i n 1 12 q t 10 m i n 1 23 z 3 4 55 q t 5 m i n 9 67 q t 10 m i n 2 33 z 4 2 78 q t 5 m i n 1 23 q t 10 m i n 3 12 z 5 7 65 q t 5 m i n 1 67 q t 10 m i n 1 11 i n f l u e n t f l o w r a t e 18 98 1 e 2 24 1 e z 1 1 12 1 e z 2 2 24 1 e z 3 2 12 1 e z 4 1 14 1 e z 5 9 23 second scenario s2i16 26 tanh f z e z e z e z e z z 1 3 12 q t 12 h r 1 15 q t 24 h r 3 12 z 2 1 14 q t 12 h r 1 11 q t 24 h r 3 41 z 3 1 15 q t 12 h r 1 11 q t 24 h r 5 15 i n f l u e n t f l o w r a t e e 1 12 e z 1 e z 1 e z 1 e z 1 2 23 e z 2 e z 2 e z 2 e z 2 3 25 e z 3 e z 3 e z 3 e z 3 e 1 17 e z 1 e z 1 e z 1 e z 1 2 28 e z 2 e z 2 e z 2 e z 2 3 27 e z 3 e z 3 e z 3 e z 3 e 1 18 e z 1 e z 1 e z 1 e z 1 2 24 e z 2 e z 2 e z 2 e z 2 3 27 e z 3 e z 3 e z 3 e z 3 e 1 14 e z 1 e z 1 e z 1 e z 1 2 24 e z 2 e z 2 e z 2 e z 2 3 29 e z 3 e z 3 e z 3 e z 3 1 92 third scenario s3i15 27 sigmoid f z 1 1 e z z 1 9 21 q t 2 d a y 6 77 q t 4 d a y 9 25 z 2 6 77 q t 2 d a y 5 37 q t 4 d a y 7 16 z 3 7 79 q t 2 d a y 7 77 q t 4 d a y 9 14 z 4 9 78 q t 2 d a y 7 79 q t 4 d a y 6 15 i n f l u e n t f l o w r a t e 14 48 1 e 2 14 1 e z 1 1 89 1 e z 2 1 32 1 e z 3 1 54 1 e z 4 2 67 3 4 uncertainty assessment of hybrid mosso models the uncertainty results of three selected hybrid mosso models for forecasting influent flow rate time series for different prediction horizons are presented in this section as mentioned previously 2000 times of sampling iteration resulted by pdf for influent flow rate then these generated time series from monte carlo generated pdf are used to produce input time series the probability distributions of gamma that were used for estimating flow rate time series is given in eq 24 24 f x α β β α x α 1 e β x γ α α 2 β 4 the 95 interval of prediction uncertainty is calculated at the 2 5 and 97 5 percentage levels of output distribution of influent flow rate q t for determining 95ppu the results of 2 5 and 97 5 percentage levels are shown as a lower line and upper line of uncertainty bound respectively the plots 95ppu for forecasting influent flow rate during testing phase for mlp mosso rbf mosso and svm mosso models are given in figs 12 14 respectively also statistical summary of uncertainty results for three hybrid mosso models in training and testing phases are listed in table 5 it can also be seen clearly from figs 12 14 that ever three models have some uncertainties for forecasting influent flow rate but the uncertainties bound of rbf mosso and svm mosso models are wider than those for mlp mosso model it is means that the rbf mosso and svm mosso models involve high uncertainty for forecasting influent flow rate comparison of uncertainty results in table 5 declares that the mlp mosso model is superior to the rbf mosso and svm mosso models the lowest values of d factor were obtained in scenario 1 for all hybrid models that were equal to 0 11 0 19 and 0 25 in training phase and 0 12 0 21 and 0 27 in testing phase for mlp mosso rbf mosso and svm mosso models respectively the similar results were obtained for scenarios 2 and 3 in training and testing stages of hybrid models the d factor values of rbf mosso model were 0 14 and 0 17 in scenarios 2 and 3 respectively the values of d factor were increased for svm mosso model in scenarios 2 and 3 than the rbf mosso model the higher values of d factor for rbf mosso and svm mosso models compared to mlp mosso model imply that these two models do not perform well in the prediction influent flow rate it is also observable that the models uncertainty for immediate prediction horizon is lower than the short term and long term prediction horizons all models except mlp mosso scenario 1 show several un bracketed values that resulted from inability of models for estimating very high and very low values of influent flow rate these uncertainties show the superiority of estimation influent flow rate using immediate prediction horizon by mlp mosso model based on the results of table 5 the mlp mosso immediate prediction horizon s1i4 model as the best model bracketed 95 and 93 of the observed values in training and testing phases respectively which consists both very high and very low influent flow rates also in scenario 1 the rbf mosso and svm mosso models presented 6 5 and 14 reductions in bracketed observed values in testing phase compared to mlp mosso model respectively the percentage of bracketed observed values by hybrid models reduced in scenarios 2 and 3 compared to scenario 1 in the other words the mlp mosso model and immediate prediction horizon are more reliable than the rbf mosso and svm mosso models and short term and long term prediction horizons for influent flow rate prediction the results of monte carlo uncertainty analysis are in agreement well with taylor diagram and performance analysis 3 5 comparison of standalone mlp rbf and svm models with hybrid models in this section the comparison results of standalone mlp rbf and svm models with hybrid mlp mosso rbf mosso and svm mosso models is discussed the training and testing results of standalone models are given in table 4 figs 15 17 it is clear from the table 4 that the mlp model had the lowest error between standalone models in all scenarios in the training phase and scenario 1 the nse value of mlp rbf and svm models was computed equal to 0 82 0 80 and 0 78 respectively also the mlp model had the lowest rsr 0 44 and pbias 41 among standalone models for scenario 1 the similar results were obtained in testing phase of scenario 1 the nse rsr and pbias were found 0 8 0 46 and 41 for mlp model 0 77 0 49 and 46 for rbf model and 0 76 0 55 and 49 for svm model respectively therefore the rank of applied standalone models in the training and testing phase for scenario 1 are mlp rbf and svm also the accuracy of standalone models in the testing phase was compared for scenarios 2 and 3 the mlp models had better performance than the rbf and svm models for short term and long term prediction horizons the scatter plots of standalone models are given in figs 15 17 it is obviously seen from the scatter plots that the mlp estimations in scenario 1 with r2 0 88 are closer to observed influent flow rate values than the other scenarios and standalone models the comparison of the models reveals that after mlp scenario 1 the models of mlp scenario 2 r2 0 86 and rbf scenario 1 r2 0 85 had better results than the other models fig 18 demonstrate that the predicted values of influent flow rate using standalone models had not accurate predictions and were far away the reference point in the training phase it shows that mlp scenario 1 was placed between 0 4 and 0 8 lines of rmse and had the largest correlation coefficient r 0 81 in the testing phase the model of mlp scenario 1 was close to reference point and had r 0 8 and standard deviation equal to 1 the other models were grouped between rmse lines 0 8 and 1 2 and between 0 5 and 0 75 the results of uncertainty analysis are given in figs 19 21 and table 5 according to table 5 among the standalone models the mlp scenario 1 model present highest bracketed observed values in training 89 and testing 87 phases this result is also clear from the fig 19 for mlp models also the scenarios 2 and 3 had more uncertainty for predicting influent flow rate than the scenario 1 in all standalone models in testing phase the highest unbracketed observed values 20 and d factor 0 3 was obtained by svm scenario 3 model therefore the uncertainty rank of standalone models for predicting influent flow rate in testing phase are mlp scenario 1 p 0 87 d 0 21 rbf scenario 1 p 0 86 d 0 25 mlp scenario 2 p 0 84 d 0 23 mlp scenario 3 p 0 83 d 0 25 svm scenario 1 p 0 83 d 0 28 rbf scenario 2 p 0 82 d 0 27 rbf scenario 3 p 0 81 d 0 28 svm scenario 2 p 0 81 d 0 29 and svm scenario 3 p 0 80 d 0 30 wei et al 2013 compared the performance of mlp and svm models for predicting sewage influent up to 180 min ahead based on the influent flow rate radar reflectivity and rainfall rate data the results showed that mlp model had better accuracy and performance than the svm model the results of current study are in agreement with ansari et al 2018 when applied nonlinear autoregressive network nar and svm models to predict influent flow rate up to 44 weeks in their research the nar predictions of influent flow fit well to observed data and svm model produced high errors the general results indicated that the hybrid mlp mosso rbf mosso and svm mosso had better performance than the standalone mlp rbf and svm models the results of table 4 indicate that in testing phase the nse of scenario 1 increased from 0 82 for standalone mlp to 0 91 for mlp mosso similarly the rsr decreased from 0 46 for standalone mlp to 0 14 for mlp mosso model and pbias decreased from 46 for standalone mlp to 12 for mlp mosso model also the mlp mosso model for scenarios 2 and 3 had higher accuracy than the standalone mlp model comparison of svm and svm mosso indicated that the svm mosso model had better accuracy than the svm model in testing phase for all scenarios the svm mosso increased the accuracy of standalone svm by 7 3 5 1 and 6 7 for nse in the first second and third scenario of testing step respectively also the rsr and pbias values of svm mosso model were lower than the svm model for all scenarios it is clearly seen from the table 4 that the accuracy of rbf mosso model with nse 0 87 rsr 0 29 and pbias 27 was better than the rbf model with nse 0 77 rsr 0 49 and pbias 46 for scenario 1 in testing phase the proposed rbf mosso model showed relatively perfect prediction with nse rsr and pbias values of 0 85 0 32 and 29 in testing phase of scenario 2 when compared with the rbf model with values of 0 77 0 54 and 45 respectively the similar results obtained for rbf mosso model of scenario 3 the mlp mosso model had the highest r2 value of 0 95 0 93 and 0 90 than the other hybrid and standalone models for first second and third scenarios respectively the minimum r2 value were respectively found as 0 80 0 76 and 0 72 for rbf scenario 1 rbf scenario 2 and rbf scenario 3 models the uncertainty results of hybrid and standalone models in table 5 show that mlp mosso model in all scenarios included most bracketed observed data 93 for scenario 1 91 for scenario 2 and 90 for scenario 3 and had lowest d factor 0 12 for scenario 1 0 15 for scenario 2 and 0 21 for scenario 3 than the other models after mlp mosso models the rbf mosso 95ppu 87 and d factor 0 21 mlp 95ppu 87 and d factor 0 21 and rbf 95ppu 86 and d factor 0 25 models for scenario 1 show lowest uncertainty the highest uncertainty was obtained by svm scenario 3 svm mosso scenario 2 and svm mosso scenario 3 models in addition the taylor diagram had proved more accurate predictions of hybrid models than the standalone models another way to visualize models stability is applying box plots for evaluating spread of the error of prediction the hybrid and standalone models for the three basic prediction horizons were compared by box plot of error in fig 22 in training and testing phases as seen in fig 22 the box plot of mlp mosso model was relatively shorter than the rbf mosso and svm mosso models in training step for all scenarios then the data in this model was more compact than two others models also the extent of the simulation errors in standalone models of mlp rbf and svm are larger than the hybrid models in training phase the prediction errors in testing phase are smaller than those in training phase the similar results with training phase were obtained in testing phase where the prediction errors of hybrid models are more compact than the standalone models in all scenarios comparison of mlp rbf svm and svm mosso models with mlp mosso and rbf mosso error values in all scenarios show that these models include more errors for predicting high values of influent flow rate the comparisons of studied models in fig 22 revealed that the mlp mosso model was superior to the other models in scenarios 1 2 and 3 therefore by selecting mlp mosso as the best model for predicting influent flow rate the highest accuracy obtained especially in immediate prediction horizon figs 23 25 show the time series plots of predicted model outputs versus the observed values of influent flow rate in selected first second and third scenarios respectively it clearly indicates that the influent flow rate simulated by hybrid models of mlp mosso and rbf mosso were consistent with the real values these figures confirm the high reliability of hybrid models predictions overall the statistical indices values and uncertainty analysis proved that the mlp mosso model had better performance compared with the other hybrid and standalone models for predicting influent flow rate especially in immediate prediction horizon with input vector of q t 5min and q t 10min this model can satisfactorily predict the fluctuations in observed data of influent flow rate also the results evidence the merits of the proposed mosso model in solving multi objective problems it can be concluded that the proposed model is able to outperform the current well known standalone models 4 conclusion accurate prediction of influent flow rate is helpful to operate of sewage treatment plant to maintain stable effluent to supply cleaned water for industrial and agricultural and to make a decision about water reuse therefore accurate influent flow rate forecasting is absolutely fundamental for normal operations and scheduling of wastewater networks however reliable forecasting of immediate time lags is a challenge problem and limit in this study a powerful approach was developed using meta heuristic multi objective shark smell optimization mosso algorithm in combination with three networks of mlp rbf and svm mlp mosso rbf mosso and svm mosso the proposed hybrid models were used for forecasting influent flow rate in different prediction horizons of immediate q t 5min q t 10min q t 15min short term from q t 1hr to q t 72hr and long term from q t 1day to q t 7day the mosso algorithm was applied to optimize triple objective functions including input vectors model process parameters and activate or kernel functions the parameters of sso algorithm was set using taguchi method also the achieved optimum non dominated solutions from the mosso algorithm were reported as pareto optimal fronts in addition the robust decision making method of topsis was used to select accurate non dominated solution from pareto optimal fronts of mosso algorithm among the three hybrid mosso algorithms used in this paper the mlp mosso neural network performed better than the other applied models for all prediction horizons by increasing time horizon the values of pbias and rsr criteria increased while the r2 and nse values decreased also the monte carlo uncertainty analysis proved the superiority of mlp mosso model for forecasting influent flow rate time series especially for immediate prediction horizon with a input vector including q t 5min and q t 10min s1i4 variables two uncertainty indices of 95ppu and d factor showed that this model has highest accuracy but the other models had large uncertainty for influent flow rate predictions the uncertainty bound in comparison with observed influent flow rate values showed that except immediate prediction horizon s1i4 of mlp mosso model all applied models have several un bracketed values this result indicates to inability of models for estimating very high and very low values of influent flow rate especially using short term and long term prediction horizons overall the results revealed that the proposed intelligence and automatic mlp mosso model achieves a superior and reliable accuracy using immediate prediction horizon than the other compared models for forecasting influent flow rate time series the main advantage of proposed mlp mosso model is that it is easy for applying and is robust approach that is able to produce diverse and efficient pareto frontiers the explicit equations for estimating influent flow rate were derived for immediate short term and long term horizons from the best hybrid model as mlp mosso the results of this study can provide assistance for predicting different horizons of influent time series at wwtps and support engineers to forecast manage and use of the future influent flows hence the proposed approach is a practical and feasible tool for multi objective analysis and decision making the future studies can focus on determining more objective functions for reducing input vectors uncertainty and also investigating neural fuzzy models in combination with meta heuristic multi objective algorithms to enhance the accuracy of predictions credit authorship contribution statement akram seifi conceptualization data curation writing original draft writing review editing mohammad ehteram conceptualization methodology software fatemeh soroush writing original draft declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper appendix 1 the statistical characteristics of dataset train first scenario model mean l s 1 standard deviation minimum l s 1 maximum l s 1 mlp mosso 102 10 37 98 28 23 180 45 rbf mosso 103 45 38 12 28 45 180 23 svm mosso 104 12 39 23 28 67 190 25 mlp 105 11 39 99 28 23 193 32 rbf 106 12 40 91 28 23 199 45 svm 107 23 40 12 28 55 202 21 train second scenario mlp mosso 102 12 38 12 28 24 180 81 rbf mosso 103 45 39 10 28 46 181 23 svm mosso 104 12 39 45 29 12 203 12 mlp 105 11 40 55 28 56 204 10 rbf 106 12 41 12 28 78 204 20 svm 107 23 42 14 28 89 204 55 train second scenario mlp mosso 102 14 39 212 28 24 182 34 rbf mosso 103 47 39 20 28 56 199 23 svm mosso 104 55 39 66 29 78 200 12 mlp 105 21 40 78 28 67 202 45 rbf 105 67 41 34 28 79 206 78 svm 105 71 42 67 28 89 208 23 test first scenario model mean l s 1 standard deviation minimum l s 1 maximum l s 1 mlp mosso 99 89 38 78 47 70 181 93 rbf mosso 99 91 39 12 47 81 182 34 svm mosso 102 23 40 21 47 84 183 95 mlp 103 45 41 12 47 91 190 23 rbf 104 12 42 23 47 89 195 67 svm 105 23 43 24 47 89 197 12 test second scenario mlp mosso 99 92 38 82 48 73 181 91 rbf mosso 99 93 39 44 48 83 182 96 svm mosso 102 34 40 24 48 85 183 96 mlp 103 51 41 44 48 68 191 29 rbf 104 32 42 34 48 91 195 92 svm 105 26 43 55 48 93 197 14 test second scenario mlp mosso 99 96 40 23 47 96 181 93 rbf mosso 99 99 42 12 47 98 182 98 svm mosso 102 98 43 14 47 99 184 55 mlp 103 98 44 56 48 25 191 24 rbf 104 55 45 78 48 22 195 93 svm 105 78 45 98 48 23 197 16 
5444,reliable prediction of influent time series has demonstrated importance in high efficiency performance of wastewater treatment and reuse plants computational models are powerful tools that has been used for predicting influent time series but one of the major drawbacks of developed models is uncertainty analysis in instantaneous and multi time step prediction of influent time series for this purpose a multi objective shark smell optimization mosso algorithm is hybridized with multilayer perceptron mlp neural network radial basis function rbf neural networks and support vector machine svm to optimize the models these hybrid models of mlp mosso rbf mosso and svm mosso are used for predicting different prediction horizons of immediate short term and long term of influent time series accordingly there are three objectives in mosso 1 minimize mean absolute error mae in different input vectors for best selection lag time influent time series 2 achieve the optimal architecture of forecasting models by minimization root mean square error rmse for model parameters and 3 select accurate activate functions or kernel function of models for finding high accuracy of estimating models the studied hybrid multi objective models are modified using taguchi method for determination of pareto optimal solution sets the proposed hybrid models and standalone models of mlp rbf and svm are evaluated using monte carlo uncertainty analysis common evaluation criteria and taylor diagram in evaluation and reliability assessments the results demonstrated that mlp mosso model produce better results than rbf mosso svm mosso and standalone models in all prediction horizons s1i4 immediate q t 5min q t 10min s2i16 short term q t 12hr q t 24hr s3i15 long term q t 2day q t 4day the mlp mosso model for immediate prediction horizon q t 5min q t 10min with one hidden layer and five neurons in hidden layer correlation coefficient r2 of 0 95 root mean square error rmse of 1 1 nash sutcliffe efficiency nse of 0 91 rmse observations standard deviation ratio rsr of 0 14 and percent bias pbias of 12 for testing phase was chosen as the final and best model for predicting influent time series furthermore it was found that all models have some uncertainties but the uncertainties in the mlp mosso model with immediate prediction horizon was less than other models for predicting influent time series finally based on the optimized results an explicit equation is derived for the best mlp mosso model to predict multi step influent time series keywords explicit equation hybrid model immediate prediction horizon multi objective shark smell optimization time series uncertainty analysis 1 introduction municipal wastewater treatment plants wwtps are very important in wastewater reuse managements because of the quick development of urbans growing population and the necessity of treating high volume of untreated sewages singh et al 2016 the scheduling and management of wwtps is important in environmental and water resource management zeng et al 2016 the most important condition changes such as influent time series variations and quality variations are important in performance of wwtps kim et al 2016 several operations such as optimal scheduling of sewage pump wei et al 2013 inter catchment wastewater transfer method for sewer overflow mitigation zhang et al 2017 and chemical injections are dependent on the inflow time series to the wwtp li et al 2019 in addition wastewater properties such as biochemical oxygen demand bod total suspended solids tss and ph are strongly correlated to the influent flow rate wei et al 2013 influent time series to the inlet of wwtp is an extended dynamic influent dataset martin and vanrolleghem 2014 therefore prediction of reliable high frequency influent data is necessary to develop and assess control strategies of wwtps langeveld et al 2017 also in order to improve the wwtps process efficiency instantaneous flows management and energy saving it is important to predict influent flow rates in the immediate and short term prediction horizons wei and kusiak 2015 the precise prediction of influent flow rate is an important issue in the wwtps and wastewater industry there are several approaches that have been proposed for analysis completing and forecasting influent inflow rate mostly categorized in physical models lin et al 2010 karpf and krebs 2011 gernaey et al 2011 flores alsina et al 2014 conventional statistical models abunama and othman 2017 ansari et al 2018 boyd et al 2019 li et al 2019 and artificial intelligence forecasting models physical models are suitable to predict physical process such as contaminant transport in conduits consider dynamic of influent as well as rain events langeveld et al 2017 the statistical models of regression model autoregressive ar autoregressive moving average arma and autoregressive integrated moving average arima models are history based models over past and current data boyd et al 2019 fortunately with the rapid growth of soft computing models several intelligent algorithms such as artificial neural networks anns el din and smith 2002 wei and kusiak 2015 kim et al 2016 szeląg et al 2017 support vector machines svms szeląg and gawdzik 2016 ansari et al 2018 and deep learning techniques lstm zhang et al 2017 are developed and can be applied for influent time series predictions however physical forecasting models cannot adequately reproduce the dynamic influent data as they require very detailed information about boundary conditions population and drainage of study area complicated parameter such as rainfall infiltration runoff and snowmelt that vary in time precise data for calibration of parameters and large scale monitoring these situations make it difficult to calibrate this type of models el din and smith 2002 bittelli et al 2010 kim et al 2016 the mentioned stochastic parameters that effect on influent flow rate are generally too complex to simulate also domestic water use affects influent flow rate which is difficult to be combined with such physical models in addition developing physical models for predicting influent flow rate is affected by the exhaustion of pipes and connections as influent structures that make it more difficult tan et al 1991 ashley et al 1999 zhang et al 2019 the statistical models in wet weather flow conditions with nonlinear complex variations that affect the dynamics of the influent time series lose their performance talebizadeh et al 2016 furthermore the efficiency of artificial intelligence models is highly affected by its structure and learning process also based on the previous studies and best knowledge of authors no conventional and single predictive model can perform with highest accuracy on all cases due to its disadvantages tian and hao 2018 hao and tian 2019 for instance the conventional learning algorithms such as gradient descent suffer from some major drawbacks such as slow convergence speed high dependency of the model results and convergence to the initial parameters and easily get stuck in a local optimum da and xiurun 2005 zhu et al 2005 by trapping algorithm in local optimum improving learning using application of more training iterations is failed cetin et al 1993 the most conventional intelligent methods lose population diversity during operation process that lead to parameter convergence and dissipation of computational resources lin et al 2016 due to drawbacks of single intelligence models poor selection of initial parameters and their values will result in unsatisfactory fitting errors wang et al 2016 also the individual forecasting models cannot always capture nonlinear characteristics of streamflow and influent flow time series because of the inherent deficiency of each model so in order to diminish negative effects of single models and to overcome these problems the hybrid models have developed to provide better predicting accuracy li et al 2011 generally the hybrid models combine different single forecasting models such as anns adaptive neuro fuzzy inference system anfis svm with optimization algorithms such as genetic algorithm ga particle swarm optimization pso firefly algorithms fa grey wolf optimization gwo dragonfly algorithm da shark machine learning algorithm smla to improve the effectiveness of forecasting model yuan et al 2014 abedinia and amjady 2015 garcía nieto et al 2016 yaseen et al 2017 aljarah et al 2018 barman et al 2018 maroufpoor et al 2019 zounemat kermani and mahdavi meymand 2019 dehghani et al 2019 shekofteh and masoudi 2019 chen et al 2019 bi and qiu 2019 adedeji et al 2020 li et al 2020 stochastic meta heuristic optimization algorithms are reliable alternative to alleviate conventional gradient based algorithm drawbacks aljarah et al 2018 recently dehghani et al 2019 proposed a hybrid model for short term to long term influent flow forecasting based on the hybrid anfis model with the gwo algorithm they concluded that hybrid anfis gwo model exhibit more preferable predictive performance compared to anfis model developing a robust model that is able to search global optimal solutions for influent time series is critical for this aim finding an accurate and more efficient optimization algorithm is important to overcome the drawbacks of other methods recently the meta heuristic techniques such as shark smell optimization sso algorithm have been successfully used to improve the performance of single objective function for real world engineering and optimization problems abedinia and amjady 2015 ehteram et al 2017 suliman et al 2019 wei and stanford 2019 rao et al 2019 the evolutionary techniques such as sso are easy and have simplicity advantage to handle non convex and non linear relationships due to their efficiency reddy and nagesh kumar 2007 and fast learning speed since the major purpose of optimization algorithms is to find the global optimum solution which the efficiency of algorithms is effective in global diverse exploration jaddi et al 2015 the sso technique has some merits over the conventional approaches deb 2001 reddy and nagesh kumar 2007 jaddi et al 2015 including a the sso applies a population set of solutions in each iteration and suggests different alternatives in a single run b the sso uses stochastic search and randomized initialization in search space and c the sso is able to find global optima in problems which is a step to balance exploitation and exploration of algorithms previous studies on streamflow and sewage flow forecasting taormina et al 2015 adnan et al 2018 dehghani et al 2019 are based on enhancing forecasting accuracy by single objective optimization algorithms that ignore the importance of prediction stability improvement and uncertainty of results however forecasting stability is also an important aspect when considering a forecasting model wang et al 2017 the average of quantile lichtendahl et al 2013 papacharalampous et al 2020 error variance wang et al 2017 wu et al 2020 and uncertainty zake et al 2019 of prediction offer the stability in performance of models the small uncertainty indicates small error variance of model forecasting the stability analysis of forecasting model is performed with the aim determining level of uncertainty zake et al 2019 in addition input feature selection is a necessary pre processing technique that makes model training faster and less complex also structure of network is a key factor that affects performance of network however former studies suppose pre defined input variables and a fixed structure for network training hence in order to achieve a forecasting method for higher levels of accuracy and stability and also larger forecasting horizons optimal selection of input variables ghadimi et al 2018 mafarja and mirjalili 2018 and model structure faris et al 2019 is necessary it should be noted that simultaneous optimization of the artificial intelligence ai model structures and weights of forecasting models can drastically increase the number of ai model parameters karaboga et al 2007 which make it a multi objective optimization problem as mentioned before heuristic algorithms are useful in real challenging problems solving but have many difficulties in optimization problems because of the existence of multiple objectives mirjalili et al 2017 2018 to deal with these issues multi objective optimization algorithms should be equipped with proper operators such as crowding distance leader selection strategy and taguchi method the combination of these operators is helpful for the multi objective optimization algorithms to effectively find true pareto optimal fronts reddy and nagesh kumar 2007 mirjalili et al 2017 multi objective optimization considers achieving solutions of problems with several objectives the role of a multi objective optimization model is to find the best trade offs of objectives as the pareto optimal solutions that map the objective space and is called the pareto front mirjalili et al 2017 and these results of optimization are evaluated based on the pareto dominance operator asrari et al 2015 in the current paper a hybrid multi objective model is established to obtain more stable and accurate influent time series prediction which mainly contains three modules optimization module forecasting module and evaluation of model accuracy and model uncertainty module a quick and new algorithm namely multi objective shark smell optimization algorithm mosso based on nature inspired shark smell optimization algorithm ehteram et al 2017 is developed as a solution for predicting immediate short term and long term horizons of influent time series that is able to select optimal feature and optimizing model structure the mosso is a multi objective method of meta heuristic shark smell optimization algorithm that have important properties this algorithm is including high quality non dominated solutions on the pareto front of problem that have proper diversity khalili damghani et al 2013 the main aim of mosso is to search accurate solutions of pareto optimal solutions with the maximum diversity zhou et al 2011 the quick movement to the pareto optimal front for accessing the best solution is one of the advantages of mosso over the conventional techniques mirjalili et al 2018 another drawback of artificial intelligent ai methods is their stochastic nature so no unique solution can be derived on different trials kasiviswanathan and sudheer 2013 in order to improve the consistency and reliability of ai model results the analysis of uncertainty in ai method results have the greatest importance in model development and evaluations srivastav et al 2007 uncertainty states the conditions that are not accurately quantifiable and can be illustrated as a defective in data processes models and information goodarzi and eslamian 2014 in this research uncertainty analysis based on monte carlo simulation method is done to evaluate and compare the validity of developed models monte carlo simulation is a numerical method that reproduce variables randomly based on the special statistical distribution goodarzi and eslamian 2014 due to the importance of influent as a water resource and wwpt operational dependency on influent flow rate also because there are not enough studies on influent flow prediction in immediate and short term horizons more researches are needed to determine the performance of intelligence and multi objective hybrid models in influent flow rate prediction and uncertainty analysis based on the aforementioned studies in the current study the mosso hybridized with single models of multilayer perceptron mlp neural network radial basis function rbf neural networks and svm models for optimizing inner parameters and to reach high forecasting accuracy moreover three hybrid models mlp mosso rbf mosso and svm mosso are developed and implemented for influent time series prediction in the forecasting module finally evaluation module uses evaluation criteria and uncertainty analysis to verify and compare forecasting effectiveness of mlp mosso rbf mosso and svm mosso models in influent flow rate forecasting for various input scenarios the major contribution and aims of the current study are 1 development of a new robust multi objective algorithm namely multi objective shark smell optimization algorithm mosso the technique uses leader selection strategy for selecting best solution among non dominated pareto optimal solutions the crowding distance criterion for enhancing diversity and appropriate distribution of solutions to reach true pareto optimal fronts and taguchi method for the best selection of mosso parameters values 2 development a feature selection technique based on mosso optimization to obtain minimum as well as powerful subset of lag time influent flow time series 3 using mosso algorithm to achieve optimal and brief structure of forecasting models 4 develop and evaluate hybrid models of mlp mosso rbf mosso and svm mosso for forecasting accurate influent time series especially in immediate lag times 5 using monte carlo simulation method to quantify uncertainties associated with the various forecasting models mlp mosso rbf mosso and svm mosso of influent time series at different prediction horizons 6 compare the accuracy of developed hybrid mlp mosso rbf mosso and svm mosso models with the standalone mlp rbf and svm models this paper is organized as follows section 2 provides description of study area data and the methods of the developed hybrid system applied in the study section 3 presents experiments analysis of the results and discussions in view the results finally conclusion of the study is provided in section 4 2 material and methods an estimating approach was developed for forecasting influent flow rate time series and to compare the simulation results in sewer network in iran the work aspects are including 1 gathering influent flow rate time series data set 2 selecting different input vectors from immediate to long term horizon 3 using multi objective optimization method to perform pareto fronts by evaluating model error statistical and to find best model parameters 4 assessing differences in performance of models and 5 investigating uncertainty analysis of the best models for selecting optimum result further details on the framework and modeling influent flow rate are described in following sections 2 1 time series data and preprocessing the present study investigates the prediction of influent flow time series based on the different time lags from 5 minutes very immediate short term up to 10 days long term by new hybrid optimization models for this purpose the recorded influent discharges time series data from a local sewer network station in isfahan city were used the isfahan city is located in the central of iran between 51 26 27 and 51 49 58 eastern longitude and 32 40 49 and 32 32 38 northern latitude the elevation of the study area is 1575 m above the sea level and have arid climate condition falahatkar et al 2011 annual minimum and maximum air temperature of this city 10 18 and 24 18 c respectively isfahan meteorological organization 2014 in this station the influent flow rate is measured using portable cross correlation ultrasonic flow meter pcm 4 model and is gathered in a data logger the pcm 4 is able to temporarily measure flow in pipes and channels the measurement approach is based on the ultrasound reflection principle the pcm 4 use a sensor which simultaneously measures flow velocity as well as flow level the time between transmission and reception of a signal that reflected from the water surface is measured and then the water level is calculated for measuring flow velocity two signal scans are taken from flow and its particles at two different time and are evaluated the flow velocity is determined based on the beam angle the interval time between both transmitted signals and the pattern of each single measurement window this device is able to measure water level in range 0 2 m and water velocity up to 6 m s 1 the location of sewer network station and the applied device for recording influent flow rate in different time are given in fig 1 a time series data set is a collection of observation that each one was recorded at a specific time t and with a special interval relative to other data makridakis et al 2008 the influent flow rate data are measured at 5 min intervals in this station the data from 9 may 2011 to 22 may 2011 are available the data set includes 3683 data points with 5 min lags that is divided into training and testing sets with 2000 equal to 67 of total data and 1000 equal to 33 of total data data points respectively to develop the hybrid models table 1 summarizes the characteristic of collected influent flow rate for all train and test data sets according to the table 1 the measured influent flow data varies from 28 1 to 181 94 l s 1 and the mean flow is about 101 3 l s 1 one of the most important issues to forecast and for applying time series is the length of time series if the time series contain sufficient length and covers high and low flows then it has forecasting capability in this paper the hurst coefficient hcoeff equation was used for investigating this problem hurst et al 1965 1 log r s log a h coeff log n where r is variation range of data s is the standard deviation of data a is constant coefficient and n is the number of data according to the hurst coefficient if hcoeff 0 5 the time series known as a random process with normal distribution that its mean value is predictable the hcoeff 0 5 shows an anti persistence or anti correlation process with long length time series when hcoeff 0 5 the time series length is adequate to study the long term characteristics of influent data and to develop models it classified as persistent time series doucoure et al 2016 in this study the value of hurst coefficient was computed as 0 783 for the studied influent flow time series this result confirms that the length of applied influent inflow time series data is adequate to develop models and also these series are predictable three scenarios from prediction horizon were considered in this study the horizon concept is defined as the time in advance that estimation is issued cortez 2010 since immediate lags are particularly useful for influent flow rate series these types of series were considered in this study forecasting influent flow rate based on the immediate horizons have a strong affect in several domain such as effluent plant planning finance issues agricultural and industrial supply the studied horizons in present study are described as table 2 1 immediate prediction horizon in this scenario the influent flow rate in very short time duration of 5 and 10 min are considered in input variable for modeling forecasting horizon predicts the influent flow rate q t based on the values of influent flow rate in periods t 5 q t 5min t 10 q t 10min and t 15 q t 15min q t f q t 5 m i n q t 10 m i n q t 15 m i n 2 short term prediction horizon the influent flow rate in short time duration from one hour to 72 h are used for predicting q t in fact forecasting horizon predicts q t based on the values of influent flow rate in periods t 1hr t 2hr t 6hr t 12hr t 24hr t 48hr and t 72hr q t f q t 1 h r q t 2 h r q t 6 h r q t 12 h r q t 24 h r q t 48 h r q t 72 h r the models that were used short term prediction horizon are known as hourly predictive models 3 long term prediction horizon the influent flow rate in long time duration from one day to 7 days are applied for forecasting q t forecasting horizon calculates q t based on the values of influent flow rate in periods t 1 day t 2 day t 3 day t 4 day t 5 day t 6 day and t 7 day q t f q t 1 d a y q t 2 d a y q t 3 d a y q t 4 d a y q t 5 d a y q t 6 d a y q t 7 d a y these models are known as daily predictive models in time series analysis proper selection of input variables with different lag times affects model capability for output forecasting and is a key problem to develop accurate forecasting models nowadays there are several different techniques for identifying the models input variables and selecting optimum subsets that have been used in this study the hybrid multi objective algorithm was applied to find the appropriate input output combination of influent flow rates at different lag times for this the input variable selection was performed dependently on the learning algorithm as wrapper method to automatically select the most appropriate inputs oliveira et al 2002 gonzález et al 2019 the svm rbf and mlp models optimized by mosso algorithm were used to make the best input selection and to find the optimal model parameters for excellent forecasting influent flow rate 2 2 artificial neural network ann models ann models are intelligent dynamic system that have been used commonly for modeling and forecasting purposes to solve engineering problems in water and wastewater resources ansari et al 2018 babaei et al 2019 a common ann usually includes three layers of input output and hidden layer each layer consists some neurons as the basic building blocks of the model the input layer comprised input variables that connect to output and hidden layer using weights the common relation between input and output layers in ann network can be written as fazeli et al 2013 2 y k f 0 j w kj f h i w ji x i b j b k where i is the input layer j is the hidden layer xi is the input value to node i wji is the connection weight of input nodes i to hidden node j yk is the output of node k fh is the transfer function in hidden layer f0 is the transfer function for output layer bj represents the bias of jth hidden layer bk is the threshold value in the kth output neuron and wkj is the connection weight between j th neuron in the hidden layer to the kth neuron in the output layer the most widely mlp network with a learning error back propagation bp algorithm has been applied as one of the ann models in this study the mlp networks are extremely flexible general and nonlinear model that their complexity can be adjusted by varying the number of hidden layers murayama 2012 the mlp network has one input layer one or several hidden layers and one output layer with linked nodes and functions guo et al 2015 in mlp each neuron in each layer is linked to all neurons in the prior layer and the output of each layer constitutes input vectors of the next layer fig 3 bozorg haddad et al 2018 using from one of transition functions of sigmoid hyperbolic tangents linear and sigmoid tangents for training hidden and output layers are common in mlp the optimum hidden layer size of mlp network was determined using optimized mosso algorithm from 5 to 30 in this paper the transition functions in hidden and output layers were determined by defining an objective function in modeling process and were employed for calibration the network for influent flow rate time series modeling by mlp the data in each horizon are introduced into mlp network the signal passes from neurons by weights and transition functions govindaraju 2000 and the bp train the network by adjusting weights to minimize the objective function guo et al 2015 another ann model that commonly was used in researches is rbf the rbf is a feed forward network and have only one hidden layer fig 3 the hidden layer in this network utilizes a nonlinear radial basis transfer function from input layer to the hidden space as activation function shen et al 2011 the output was determined according to radial distance from a center vector the numbers of neurons in hidden layer weights values biases values and activation functions for mlp and rbf networks were determined using applied multi objective algorithm 2 3 support vector machine svm model the svm is a supervised learning algorithm and one of the most useful tools that was used for solving complex nonlinear problems wei et al 2013 the svm was introduced by vapnik 1995 that can construct hyperplanes in a high dimensional feature space with a set of optimized nonlinear kernel functions wang et al 2005 therefore the nonlinear problem transformed into linear function problem odintsev and miletenko 2015 the forecasting accuracy would be enhanced by applying suitable kernel function meng et al 2019 the linear equation of svm decision function f x is given as 3 f x w tr x b where x is the input variable b is the bias value w is the weighting vector that used for variables vector and tr is the transpose symbol the svm involves solving the following optimization problem to minimize the error function minimize 1 2 w 2 c i 1 m ξ i ξ i 4 s u b j e c t t o w i x i b y i ε ξ i where c is the penalty factor is the symbol for vector length m is the number of training data ξ i and ξ i are the slack parameters of the model the structure of svm model is shown in fig 3 totally there are several kernel functions for svm including linear polynomial radial basis function rbf sigmoid and gaussian in the current paper gaussian hyperbolic tangents and laplace kernels are used as the kernel functions the best kernel function is determined based on the objective function in applied multi objective algorithm the equations of applied kernel function are presented as 5 gussian k e r n e l f u n c t i o n k x x i e x p x x i 2 2 γ 2 6 laplace k e r n e l f u n c t i o n k x x i e x p x x i 2 σ 7 hyperbolic t a n g e n t k e r n e l f u n c t i o n k x x i t a n h k x i x j c where γ σ c and k are the kernel functions parameters that were determined using optimization 2 4 shark smell optimization sso application of meta heuristic optimization algorithms have attracted widely considerations over the last two decades due to characteristics of avoiding from local optimal flexibility simplicity and derivation free mechanism mirjalili et al 2014 the sso algorithm is a meta heuristic nature inspired optimization algorithm that first introduced by abedinia et al 2016 this optimization algorithm mimics the superior hunting behavior of sharks which are able to sense the odor and local of prey the blood odor concentration of prey in water is an effective factor to move of shark toward prey abedinia and amjady 2015 the movement behavior of shark to the odor source is shown in fig 2 the fig 2 show the shark rotation on a closed contour red arrows and search path black arrow toward the prey the sso algorithm includes three phases of initialization of odor particles forward movement of sharks rotational movement and position update to obtain the location of prey during the initialization of odor particles for the optimization problem one odor source is created in the environment of shark search due to one injured fish prey abedinia et al 2016 the search process begins when the shark smells odor in order to model this process the sso algorithm generates a random population of initial solution odor particles within the feasible search space x x 1 1 x 2 1 x np 1 for a np population size to search the solution space for finding optimum solutions each solution represents the possible shark position based on the odor particle each individual in initial position vector has a primary velocity vector as v v 1 1 v 2 1 v np 1 to become closer to the prey with stronger odor particles the ith initial candidate solution that is ith probable shark position is written as x i 1 x i 1 1 x i 2 1 x i n d 1 i 1 n p consequently in forward movement of sharks the velocity of shark will be changed and increased by increasing the odor particles concentration v i 1 v i 1 1 v i 2 1 v i n d 1 i 1 n p the mathematical of objective function for shark s velocity at kth iteration is defined as 8 v ij m m i n μ m r 1 o f x j x i j m α m r 2 v i j m 1 γ m v i j m 1 i 1 n p m 1 m j 1 n d μ m 0 1 where γm is the upper bound of current velocity for the previous one µm is the gradient constant αm is the momentum effect between 0 and 1 m is the stage number of shark forward movement and r1 and r2 are random number between 0 and 1 based on the velocity vector the position of shark updates as 9 z i m 1 x i m v i m δ t m i 1 n p m 1 m where δtm is the time interval in mth stage in rotational movement phase shark moves toward the prey by rotational movements for finding stronger odor particles hence the rotational movements of shark are considered as local search process in sso algorithm also in position update phase the best point is selected by shark the local search and position update phases are modeled as 10 γ i m 1 l z i m 1 r 3 z i m 1 i 1 n p m 1 m l 1 l 11 x i k 1 a r g m a x o f z i m 1 o f γ i m 1 1 o f γ i m 1 l i 1 n p where r3 is a random number between 1 and 1 and l is the number of points in the local search the sso algorithm starts optimization with user defining some parameters including np m γm µm and αm a set of random solutions is generated as the initial population and each decision variable of xij 1 is randomly generated within the allowable ranges the stage counter is initialized and the velocity vectors are calculated after that the updated location of shark is obtained by onward movement eq 9 in next step the local search is performed eq 10 to obtain γi m 1 l then the position of shark is updated and the next shark position is selected between the positions obtained from forward movement and local search eq 11 finally the best location of shark as best solution with the highest objective function is selected throughout optimization when the end condition is satisfied 2 5 multi objective shark smell optimization algorithm mosso a general multi objective optimization mo problem includes several objective functions and associated constrains oliveira et al 2002 which have mostly confliction with each other the important aim of mo problems is finding optimal solution from a special optimization algorithm wu et al 2020 since obtaining an optimal solution to simultaneously optimize all objective functions is almost impossible in mo problems hans 1988 due to the nature of mo problems mo problems create a set of solutions to generate most appropriate trade offs between the objectives that called pareto optimal solutions set mirjalili and lewis 2015 mirjalili et al 2018 the pareto optimal front creates an opportunity to evaluate the solutions in a multi objective domain mirjalili et al 2018 behnood and golafshani 2018 if a solution performs better over another in all experiment criteria it is dominant the multi objective sso algorithm mosso is based on the sso algorithm that use hunting technique of shark for obtaining optimal solution actually the main aim of mosso algorithm is to obtain very precise estimation of the proper pareto optimal fronts in order to apply meta heuristic sso algorithm for solving mo problems the concept of archive was used to store the best approximations of pareto optimal solutions during optimization a pareto archive is a set of non dominated pareto optimal solutions the archive must update frequently in any iteration and it may become full through optimization therefore it is necessary to achieve a mechanism for managing the archive in the beginning of mosso algorithm the positions of the sharks are the possible solution of optimization problem and the archive is empty by running the optimization algorithm the solutions are non dominated add to the archive and built the pareto front of the optimization problem the archive may remain empty in different iterations it means the algorithm has not found non dominated solution yet in addition the archive should be control and update in each iteration by finding new non dominated solutions during iterations actually the archive is responsible for saving the obtained non dominated pareto optimal solutions in hybrid mlp mosso rbf mosso and svm mosso models triple objective functions including 1 select optimal input combination by minimizing mae criterion 2 select best values of model s parameters by minimizing rmse criterion and 3 select best activation or kernel function of models by minimizing obj criterion were optimized simultaneously due to triple objective functions were conflicting it was impossible to recognize one solution for satisfying all objective functions therefore the concept of the pareto optimal fronts or non dominated solutions was applied in this study typically to support the decision maker for finding the final solution and to prevent direct effect of decision maker on solving mo algorithms using from reference points was introduced as an attractive and popular method miettinen and mäkelä 1999 miettinen and mäkelä 2002 luque et al 2009 a reference point includes desirable values for each objective function actually the reference point is projected to the non dominated solutions set and the best non dominated solution is found based on the more closely point to reference point in this study several single reference points were determined by solving the sso algorithm as single objective for any objective function which provides the best non dominated solution based on the single objective optimization technique the previous studies shown selecting several reference points can improve the performance of mo algorithms li et al 2018 decision about selecting the best solution from the pareto optimal is not simply for mosso algorithm selecting the appropriate non dominated solution among the different alternatives in the set of pareto optimal solutions is important therefore leader selection strategy is applied to choose the best solution among the best non dominated solutions in archive for the mo optimization problems the technique for order preference by similarity to ideal solution topsis ranking approach was introduced as a practical and classical method for selecting the accurate non dominated solution tzeng and huang 2011 ge et al 2017 based on the topsis concept the best non dominated solution in mo problems should have the shortest distance from the positive ideal solution reference point d and the largest distance from the negative ideal solution the worst possible status d hwang yoon 1981 two characteristics of d and d are computed based on the three criteria of rmse mae and obj of each solution the relative closeness of each solution is calculated based on d and d as cc d d d the cc values are between 0 and 1 the larger cc value show the best solution of algorithm for each prediction horizon yu et al 2018 the description of topsis method provided in different researches chen 2000 shih et al 2007 tsou 2008 yu et al 2018 the best non dominated solution was chosen based on the reference points and topsis method each selected non dominated solution includes triple objective functions for forecasting influent flow rate as mentioned above the objective functions of hybrid mosso algorithm were considered as 1 minimizing mae value for determining optimal input variables vector 2 minimizing rmse value for determining optimum parameters of models and 3 minimizing obj criterion for determining the best activation or kernel function for estimating influent flow rate time series the crowding distance criterion is applied to enhance diversity and appropriate distribution of solutions the crowding distance chooses and offers the least crowded solutions of the search space the crowding distance was defined as the average distance between each solution and its nearest neighbors in the search space deb et al 2002 to calculate crowding distance the objective function values are firstly sorted in ascending a solution with larger crowding distance and lower rank is selected as the best answer reddy and nagesh kumar 2007 the selection of the best non dominated solutions among offered solutions by crowding distance criterion are done according to roulette wheel mechanism mirjalili et al 2018 behnood and golafshani 2018 the crowding distance cd and roulette wheel criteria are calculated by following equations lalehzari et al 2015 mirjalili et al 2018 12 cd i m 1 3 of m i 1 of m i 1 of i max of i min 13 p i c n i where of is the objective function m is the number of objective functions i is the solution rank in pareto front c is a constant number that is greater than 1 and n is the number of obtained pareto optimal solutions the performance of mosso algorithm depends remarkably to the accurate choices of algorithm parameters values a sensitivity analysis is required to identify how the random parameters for an optimization algorithm affect the objective function value when the objective function value is computed versus the variability in the random parameter values the optimal values of random parameters are computed a robust calibration approach which is named taguchi method is used for the best selection of mosso parameters values in this study the taguchi method provides an opportunity to recognize main factors and possible interaction between parameters as well as to determine the number of corresponding levels for parameters liu et al 2019 the taguchi method can considerably reduce experimental cost and time due to minimum number of trials zhang et al 2015 also this method is a simple and effective statistical tool to optimize the design parameters in complex process such as forecasting influent flow rate time series using mosso algorithm ben arfa et al 2016 the taguchi model performs a sensitivity analysis to determine how the signal to noise s n ratio is affected by the variability in the random parameter values therefore the best parameters values for optimization algorithms can be determined easily by comparing the mean of s n ratio liu et al 2019 in the present study the s n ratio with the properties of higher is better hb is used to evaluate the experimental results as the following equation taguchi 1990 14 s n 10 log 10 1 n i 1 n 1 y i 2 where n is the total number of experiments and yi is the ith observed value the developed mosso algorithm can be summarized in the following steps step 1 the parameters values of mosso algorithm including m αm γm maximum iteration and population size np are initialized according to the taguchi method step 2 the initial position of sharks is randomly initialized as a set of random solution step 3 the objective functions values for each individual in the current population is calculated step 4 the non dominated solutions are found and the pareto front optimal is stored to the archive step 5 the crowding distance is calculated for each solution of the archive step 6 the roulette wheel mechanism and crowding distance criteria are used for selecting the best solution step 7 the velocity of sharks is updated according to eq 8 step 8 the positions of sharks current solution is updated according to eq 9 step 9 the non dominated solutions are determined and the archive is updated with respect to them step 10 all solutions in archive are compared and the dominated solutions by the other solutions are removed from the archive step 12 if the convergence condition is not satisfied then it returns to step 5 otherwise output the non dominated solution set from the algorithm in order to evaluate the pareto fronts performance of hybrid mosso models in the present study three evaluation indices are used the first index is the number of non dominated solutions that stored in the archive obviously an algorithm that maintains a high value of non dominated solutions in the archive is preferred to another in mo optimization problems khalili damghani et al 2013 hamdy et al 2016 the second index for comparing obtained pareto optimal fronts is spacing metric the concept of spacing metric is measuring relative distance of consequent solutions when the solutions distribute uniformly throughout the non dominated pareto solutions set the pareto optimal front consists non dominated solutions with small value for spacing khalili damghani et al 2013 gupta et al 2019 hence based on the spacing metric a model with smaller spacing for non dominated solution is desirable finally the maximum spared or diversity index was used as third index to investigate the performance of pareto fronts of hybrid mosso models the maximum spared or diversity index is calculated based on the terminate values of non dominated solutions set in the solution space the greater values of maximum spared or diversity index indicate to better performance of model 2 6 hybrid artificial intelligence models with mosso algorithm in the present study three artificial intelligence models including mlp rbf and svm in combination with mosso algorithm mlp mosso rbf mosso and svm mosso are used for forecasting influent flow rate time series in different horizons the structure of the forecasting influent flow rate time series by hybrid mlp mosso rbf mosso and svm mosso models is summarized in three objective functions in the initial step an input database including effective input variables or the most important time lags has to be selected hence the first objective function in mosso is related to select the best input variables since each scenario in this study consist several inputs with different time lags the first objective function was defined as minimizing mean absolute error mae in different input vectors as mentioned before the mlp rbf and svm models include several parameters such as weights biases number of hidden layers number of neurons in hidden layer and kernel functions parameters that are unknown for modeling and have to be characterized the architecture of models is usually defined based on trial and error method achieving a simple architecture of mlp rbf and svm models with high accuracy is important for application and users for forecasting the influent flow rate in different horizons by mlp rbf and svm models the related parameters for each model should be accurately determined hence in this study these models are connected to mosso algorithm and the model s parameters are determined and optimized based on the optimization process by allocating the second objective function in mosso algorithm therefore the second objective function is minimization root mean square error rmse for model s parameters finally the third objective function is determined to select accurate activate functions or kernel function for mlp rbf and svm models this objective function is written to choose an optimal kernel among gaussian hyperbolic tangents and laplace kernels for svm model and to select best activation function among sigmoid hyperbolic tangents linear and sigmoid functions for mlp and rbf models in fact the third objective function selects the kernel and activation functions automatically and avoids from personal selection and error trial process in hybrid mlp mosso rbf mosso and svm mosso modeling the flowchart of hybrid mlp mosso rbf mosso and svm mosso models is given in fig 3 and the steps of the hybrid forecasting systems are described as following steps step 1 the data are collected step 2 a prediction horizon from scenarios is selected and the its input dataset is prepared step 3 one of the artificial intelligence models of mlp rbf and svm is chosen step 4 the initial parameters of selected model is initialized step 5 the model is trained step 6 the mae value is obtained based on the estimation and observation influent flow rate values during first objective function for determining optimum input vector step 7 the rmse value is obtained based on the estimation and observation influent flow rate values during second objective function for determining optimum models parameters step 8 the obj criterion is calculated for third objective function from eq 23 to consider the simultaneous effect of mae and rmse from steps 6 and 7 step 9 the convergence condition is controlled if the end condition is satisfied then mosso algorithm goes to the step 14 otherwise it goes to the step 10 step 10 the initial parameters of sso algorithm is initialized step 11 the parameters and input variables entered to sso algorithm and is optimized as decision variable and initial shark population step 12 all steps of mosso algorithm is repeated from step 1 to step 11 step 13 the termination condition is investigated if the termination condition is satisfied then mosso algorithm goes to the step 5 otherwise it goes to the step 10 step 14 the selected model is tested and the results is reported 2 7 uncertainty analysis of hybrid mlp rbf and svm mosso models the error functions in hybrid mlp mosso rbf mosso and svm mosso models are directly related to error in input dataset of models hence the error in models input dataset can be a source of some uncertainty in output results of models therefore it is necessary to evaluate performance of models and to determine the uncertainty of outputs in this study the monte carlo method is used to assess the uncertainty of hybrid models and to generate a set of random samples from influent flow rate noori et al 2015 riahi madvar and seifi 2018 for this aim the random samples values are generated based on the probability distribution functions pdfs the pdf of influent flow rate is needed for monte carlo simulation which affects the quality of simulation results scharffenberg and kavvas 2011 in this study the simlab v2 2 software was used for fitting the pdfs to the influent flow rate the kolmogorov smirnov test is used as the goodness of fit for pdfs with p value 0 05 scharffenberg and kavvas 2011 then a large random dataset of influent flow rate is generated based on the pdfs and monte carlo sampling procedure based on the results of the different experiments for determining appropriate number of samples the number of 2000 samples were satisfied the convergence condition therefore the data sampling process iteration is set to be 2000 times and influent flow rate is reproduced 2000 times by the pdfs pattern for each input variable then the input time series are extracted from the monte carlo generated pdf after that the 2000 samples are trained and a range of output is determined for different prediction horizon of influent flow rate time series to evaluate the uncertainty of models the upper and lower bound are determined by considering 97 5 and 2 5 percent of produced influent flow rate time series from the 2000 times forecasting process respectively the 95ppu bound and the degree of uncertainty d x are used to evaluate goodness of fit of models noori et al 2015 riahi madvar and seifi 2018 15 d x 1 k i 1 k x u x l i 16 d f a c t o r d x σ x 17 95 p p u c o u n t q x l q x u n 100 where k is the number of samples and σ x is the standard deviation of influent flow rate and d x is the average distance between upper and lower bound the best performance is obtained when 100 of the observations are bracketed by 95ppu and the d factor is close to zero 2 8 optimum selection and model evaluation criteria the performances of proposed models were evaluated using several statistical evaluation criteria such as rmse mae nash sutcliffe efficiency nse rmse observations standard deviation ratio rsr percent of bias pbias and objective function obj these evaluation criteria calculate as following equations 18 nse 1 i 1 n o i p i 2 i 1 n o i o 2 n s e 1 o p t i m a l v a l u e 1 19 rmse 1 n i 1 n o i p i 2 0 r m s e o p t i m a l v a l u e 0 20 mae 1 n i 1 n o i p i 0 m a e o p t i m a l v a l u e 0 21 rsr rmse stdev i 1 n o i p i 2 i 1 n o i o 2 0 r s r o p t i m a l v a l u e 0 22 pbias i 1 n o i p i i 1 n o i p b i a s o p t i m a l v a l u e 0 23 obj n train n test n train n test rmse m a e r train 1 0 o b j o p t i m a l v a l u e 0 where oi is the observed value pi is the estimated value ō is the mean of observed values n is the number of data ntrain is the number of training data ntest is the number of testing data and rtrain is the pearson determination coefficient for training phase taylor diagram is a precise graphical tool to compare the results of studied models taylor 2001 that is used in this study taylor diagram presents three appropriate criteria including standard deviation sd centered mean squared difference rmsd and r2 in a single diagram taylor diagram shows the matching status of estimations with the real output the models are programmed and developed in matlab 2016 coding environment and the forecasting parameters are calculated using the trained models with optimized constant and weights in the uncertainty analysis the training process is done using the monte carlo approach and model results are quantified by uncertainty indices and bounds all of the calculations are done using matlab 2016 programming environment 3 results and discussion in this section the obtained results from the purposed hybrid mosso models for forecasting influent flow rate based on the different prediction horizons are presented in the first part of the results the pareto optimal fronts are used to extract the three best predictive horizons by different hybrid mlp mosso rbf mosso and svm mosso models in the second part the performances of the selected predictive models are compared by using several evaluation criteria finally the results of the monte carlo uncertainty analysis are presented for selected models 3 1 optimal value selection of mosso process parameters by taguchi method in mosso algorithm the most desired values of parameters were determined by using taguchi method the parameters values of m αm γm maximum iteration and np were investigated since the number of investigated parameters n is 5 the degree of freedom df n n 1 was calculated as 5 5 1 20 hence the appropriate array at least must have twenty rows each parameter has five levels then the number of run for each parameter was calculated 25 hence 25 different combinations of control factors were considered for each trial and 10 replicates were performed therefore l25 orthogonal array was used to perform the taguchi experiment to obtain the optimal value of each parameter of mosso algorithm after that the obtained results were transferred into s n ratios and the mean values of s n values from 10 replicates were calculated at each level these values represented the optimum values of algorithm parameters in this study the s n ratio plot of mosso algorithm process parameters are given in fig 4 the effect of each mosso algorithm process parameter including αm np maximum iteration γm and m is divided into five levels of 0 1 0 3 0 5 0 7 0 9 100 200 300 400 500 150 300 450 600 750 1 2 3 4 5 and 50 100 150 200 250 respectively the mean value of s n ratios was calculated from 10 replications at each level in this study the higher is better hb mechanism for s n was considered and each value that had greater s n was selected as optimal and more important value for each process parameter as shown in fig 4 the γm parameter exhibits the largest variations in which it first decreases and then increases and finally it decreases where the m maximum iteration np and αm parameters show increasing trend mean value of np m maximum iteration αm and γm were found to vary from 0 23 to 0 06 0 15 to 0 36 0 03 to 0 57 0 38 to 0 38 and 0 53 to 0 73 respectively therefore based on the results in fig 4 the optimum conditions were αm 0 7 np 500 maximum iteration 600 γm 1 and m 100 in proper levels of 4 5 4 1 and 2 respectively as shown in fig 4 the γm parameter had the largest variations followed by the αm parameter it has been demonstrated that the mosso algorithm were mainly affected by γm and αm parameters 3 2 the pareto optimal fronts result for hybrid mosso models the obtained pareto optimal solution for hybrid mlp mosso rbf mosso and svm mosso methods for forecasting influent flow rate time series are given in figs 5 7 respectively the reference points are painted as stars in the pareto optimal solution plots as seen from the figures the reference points were placed around the best solution in the pareto fronts the pareto front of the mlp mosso scenario 1 model had 31 solutions the rmse mae and obj indices varied from 1 to 9 l s 1 1 5 to 4 l s 1 and 3 to 9 l s 1 respectively the values of rmse mae and obj varied from 1 to 9 l s 1 1 5 to 4 l s 1 and 3 to 9 l s 1 for second scenario of mlp mosso and from 1 to 9 l s 1 1 5 to 4 l s 1 and 3 to 8 l s 1 for mlp mosso scenario 3 respectively there are four reference points for each scenario of mlp mosso model the objective functions values rmse mae obj for these reference points were obtained as 2 4 5 7 2 5 7 2 6 and 5 3 5 for mlp mosso scenario 1 also these values were equal to 7 2 5 7 3 5 5 3 6 and 5 2 5 for mlp mosso scenario 2 and 8 3 3 7 2 3 5 2 3 and 5 3 3 for mlp mosso scenario 3 the pareto fronts of rbf mosso and svm mosso models were more extend than the mlp mosso model the results of fig 5 show the best hybrid mlp mosso model for immediate prediction horizon scenario 1 includes q t 5min and q t 10min input variables s1i4 in which the hidden layers number of neurons and activation function for this model were achieved equal to 1 5 and sigmoid respectively this result was also achieved for rbf mosso and svm mosso models the structure characterizes of rbf mosso model for s1i4 scenario are including sigmoid activation function in hidden layer tanh activation function for output layer one hidden layer and seven hidden neurons in hidden layer also the kernel parameters of γ 2 25 and c 58 and activation function of gaussian were produced best structure of svm mosso model in immediate prediction horizon the best solution of mlp mosso model had rmse 7 mae 3 and obj 4 for objective functions the rmse mae and obj values were obtained 4 6 and 7 for rbf mosso and 7 6 and 8 for svm mosso model respectively comparison of these values show that the best solution point of mlp mosso model can reduce more accurate prediction for immediate horizon than the other hybrid models in short term prediction horizon scenario 2 the best predictive mlp rbf and svm mosso models for forecasting influent flow rate were selected based on the time lags of t 12hr and t 24hr as input scenario of s2i16 from table 2 the activation function for hidden layer number of hidden layers and number of neurons in hidden layer were obtained tanh 1 and 3 for mlp mosso and sigmoid 1 and 5 for rbf mosso respectively the results of svm mosso indicated that the gaussian activation function can provide the best results in all prediction horizons this result is in agreement with sharifi garmdareh et al 2018 that showed gaussian membership function produced better performance than the other functions for regional flood frequency prediction in iran the criteria values of rmse mae and obj of objective functions were calculated as 5 3 2 and 5 for mlp mosso 7 5 and 6 for rbf mosso and 7 7 and 8 for svm mosso models respectively finally the best long term prediction horizon scenario 3 were included q t 2day and q t 4day input variables s3i15 for all applied hybrid models in this horizon the activation function of sigmoid were produced best optimal solution for best input combinations of mlp mosso and rbf mosso models in this context the objective functions criteria including mae rmsem and obj were obtained equal to 7 3 and 5 for mlp mosso 5 6 and 7 for rbf mosso and 5 6 and 7 for svm mosso models respectively the results of cc index of topsis method for selecting best solution are given in table 3 as seen from the table 3 the mlp mosso model in scenario 1 had largest value of cc equal to 0 93 after that the cc value of mlp mosso scenario 2 rbf mosso scenario 1 rbf mosso scenario 2 svm mosso scenario 1 and svm mosso scenario 2 were equal to 0 91 0 90 0 89 0 88 and 0 88 respectively therefore it is clear that the best results were obtained by mlp mosso model in immediate prediction horizon which it was placed in rank 1 the svm mosso model showed the worst results based on the topsis therefore the advantage of applied mosso algorithm for saving time by applying triple objective functions and providing simple system for selecting best solutions from pareto fronts using reference points and topsis is clearly specified in which the results are presented simultaneously the mosso approach proposed in this study was very effective for approximating true pareto solutions and had high capability to find the best solution the results of this study are in agreement with siade et al 2019 when applied multi objective swarm particle optimization mopso for finding solutions and predicting groundwater flow in reverse to the previous studies wei and kusiak 2015 dehghani et al 2019 boyd et al 2019 selecting the best input variables vector determining the optimal values of model process parameters and choosing the most accurate active and kernel functions were done automatically and intelligently in the present study dehghani et al 2019 applied anfis and anfis gwo for predicting influent flow rate however they did not used from multi objective optimization for selecting best input combination model structure and activation functions they used gamma test gt feature selection of input combinations also the anfis parameters optimized by using gwo table 3 provides the indices results of hybrid mosso models it can be concluded from table 3 that the proposed hybrid mlp mosso model is able to provide the best results on first index for all scenarios with higher number of non dominated solutions than the other models the number of non dominated solutions of mlp mosso model was obtained 311 for all scenarios where it was 309 for rbf mosso and svm mosso models so it can be stated that the hybrid mlp mosso algorithm has the superior optimal level to construct the pareto optimal fronts the rbf mosso and svm mosso algorithms had similar results for first index it can be seen from table 3 the spacing metric for immediate prediction horizon of mlp mosso rbf mosso and svm mosso models was calculated equal to 2 12 2 25 and 2 17 respectively therefore it can be found the generated solutions using mlp mosso model for scenario 1 were more uniformly distributed throughout the non dominated pareto solution set comparison with the rbf mosso and svm mosso models the lowest value of second index in short term prediction horizon scenario 2 occurred in svm mosso 2 14 model compared with mlp mosso 2 15 and rbf mosso 2 17 models also the finding was revealed that the lowest value of second index for long term prediction horizon was related to svm mosso equal to 2 10 whereas it was obtained equal to 2 11 and 2 16 for mlp mosso and rbf mosso models respectively the difference of second index value between svm mosso and mlp mosso models in scenarios 2 and 3 was low the percentage of difference between these models was almost 0 5 for scenarios 2 and 3 therefore it seems the both svm mosso and mlp mosso models are able to uniformly distribute throughout the non dominated pareto solution set comparison with the rbf mosso table 3 presents the third index values for selected models and the results show that mlp mosso model has the best performance according to table 3 between models in scenario 1 the diversity index value of mlp mosso 7 1 2 was higher than those in rbf mosso 7 1 1 and svm mosso 7 0 6 in short term prediction horizon the third index values of mlp mosso and rbf mosso models was equal to 710 where it was calculated equal to 702 for svm mosso model also the mlp mosso model had best result in scenario 3 with diversity value of 709 when compared with rbf mosso and rbf mosso models with values of 708 and 699 respectively the results of hybrid models for each scenario revealed that mlp mosso model had the capability to be good and robust predictive tools for estimating influent flow rate in term of yielding a set of diverse solutions along the pareto optimal fronts hence the mlp mosso model was able to face with the preliminary converging problem in the search space this result is in agreement with oliveira et al 2002 when used multi objective genetic algorithms for handwritten digit recognition also comparing results of indices between scenarios were indicated that scenario 1 of mlp mosso model had higher values of first 3 1 1 and third 7 1 2 indices however the performance results of second index shown that svm mosso scenario 3 mlp mosso scenario 3 and mlp mosso scenario 1 with values of 2 10 2 11 and 2 12 respectively had close agreement with together totally based on the three applied indices the results of mlp mosso model tend to be better than the rbf mosso and svm mosso models according to the evaluation results the best results of obtained pareto optimal fronts for different models and scenarios show that the pareto optimal front of scenario 1 immediate prediction horizon for mlp mosso model is better and more distributed than the other scenarios and models although the results obtained for the other models are close to the mlp mosso model 3 3 performance analysis of hybrid mosso models statistical comparison between different optimal algorithms show in table 4 it can be found that the highest accuracy for all scenarios is achieved by mlp mosso model in both training and testing phases also the prediction accuracy of rbf mosso model was better than the svm mosso model in both training and testing phases in testing phase the pbias values of 12 27 and 38 were calculated for mlp mosso rbf mosso and svm mosso models in scenario 1 respectively also based on the nse and rsr evaluation criteria in testing phase the highest accuracy was achieved by mlp mosso model with values of 0 91 and 0 14 respectively compared with mlp mosso in scenario 1 the nse 0 87 and psr 0 29 were calculated for rbf mosso and nse 0 82 and psr 0 42 for svm mosso model the similar results were achieved in training phase of scenario 1 therefore it is clear that the performance of mlp mosso model was better than the other hybrid models in addition by comparing the accuracy of hybrid models in scenario 2 in testing phase the mlp mosso model had best performance with nse 0 90 rsr 0 16 and pbias 16 than the rbf mosso with nse 0 85 rsr 0 32 and pbias 29 and svm mosso model with nse 0 0 84 rsr 0 45 and pbias 40 also in testing phase the nse rsr and pbias values of scenario 3 were calculated as 0 89 0 19 and 17 for mlp mosso 0 84 0 35 and 31 for rbf mosso and 0 82 0 53 and 47 for svm mosso models respectively on the other hand the svm mosso model in scenario 3 long term prediction horizon had the lowest performance for predicting influent flow rate in this study it can be due to selection of optimal non dominated solutions in the pareto solution fronts based on the statistical comparisons for different prediction horizons the best performances are obtained by scenario 1 s1i4 for all hybrid models however the best result was achieved by mlp mosso model this scenario has time lags smaller than one hour which called immediate forecasting models it is evidence that increasing lag time of prediction from minutes scenario 1 to hour scenario 2 and day scenario 3 decrease accuracy of hybrid models it seems using from input variables with hourly and daily lags for forecasting influent flow rate time series have lower accuracy than the immediate time lags in minutes dehghani et al 2019 compared anfis and hybrid anfis gwo for predicting influent flow rate in different horizons the results of that study showed the performance of models affected by prediction horizons and anfis gwo1 with input vector of 5 min ahead performed better than the other input combinations the predicted values of influent flow rate by hybrid models versus observed values for training and testing datasets are shown in figs 8 10 each model has closer value to 1 for determination coefficient r2 is more accurate it can be seen from figs 8 10 that determination coefficient values of mlp mosso model are greater than those for rbf mosso and svm mosso models in all scenarios the r2 values for immediate prediction horizon scenario 1 s1i4 of mlp mosso rbf mosso and svm mosso models were achieved equal to 0 95 0 88 and 0 85 respectively the determination coefficients of models for immediate prediction horizons are more than those for short term and long term prediction horizons the r2 values of best hybrid model i e mlp mosso during the testing phase are equal to 0 93 and 0 90 in scenarios 2 and 3 respectively while this statistic is calculated as 0 87 in scenarios 2 and 3 for rbf mosso model the svm mosso model show the worst performance with values of r2 equal to 0 82 and 0 78 in scenarios 2 and 3 respectively these results indicate the high accuracy of developed mlp mosso model for forecasting influent flow rate especially using immediate prediction horizon scenario 1 there are very few studies on predicting influent flow rate for comparing results of current study with other researches tan et al 1991 applied a direct k step predictor to predict wastewater flow rate and found this model is able to obtain predictions up to 2 hours ahead chen et al 2014 demonstrated that the arma model can accurately predict the flow rate of sewage stream entering to the pump station a few hours ahead the results of taylor diagram analysis for three best models in influent flow rate time series prediction for training and testing phase are given in fig 11 the observed point is determined based on the standard deviation which is plotted along the horizontal axis the distance of each model to this observed point shows the accuracy of that model where the closer point to the observed point has the best performance in training phase the models of mlp mosso scenario 1 mlp mosso scenario 2 and mlp mosso scenario 3 were grouped around rmse line of 0 4 that had largest correlation coefficient with the observed point the correlation coefficient r for mlp mosso scenario 1 is more than 0 99 where it is between 0 95 and 0 99 for mlp mosso scenario 2 and mlp mosso scenario 3 although the standard deviation of rbf mosso scenario 1 and svm mosso scenario 3 are obtained equal to 0 75 and 0 80 respectively although the standard deviation values of mlp mosso model in all scenarios for scenario 1 1 15 l s 1 for scenario 2 0 90 l s 1 and for scenario 3 1 1 l s 1 were larger than those rbf mosso scenario 1 and svm mosso scenario 3 but the mlp mosso scenario 1 2 and 3 models were closer to observed point the rmse of mlp mosso model in all scenarios was lower than 0 4 while it was between 0 4 and 0 8 for the other hybrid models the similar results were obtained for testing phase the point related to mlp mosso scenario 1 model was very close to observed point with r close to 1 the point of mlp mosso model in scenario 2 and 3 was placed between lines of 0 95 to 099 the r values of other hybrid models were between 0 75 and 0 95 lines the value of standard deviation was approximately 1 15 for mlp mosso scenario 1 0 9 for mlp mosso scenario 2 1 4 for mlp mosso scenario 3 0 8 for rbf mosso scenario 1 1 45 for rbf mosso scenario 2 1 24 for rbf mosso scenario 3 1 1 for svm mosso scenario 1 1 35 for svm mosso scenario 2 and 0 85 for svm mosso scenario 3 the results of rmse criterion in testing phase was similar to training stage according to the results of taylor diagram at both training and testing phases the hybrid mlp mosso model can strongly predict influent flow rate time series especially using immediate prediction horizon after that the accurate of rbf mosso model is higher than the svm mosso model previous studies have identified the mlp model is able to successfully predict short term influent flow rate wei et al 2013 used mlp neural network model to predict short term horizons of influent flow rate up to 180 min by applying influent flow rate rainfall rate and radar reflectivity data in 15 min intervals their results indicated that the prediction accuracy of mlp model reduce after t 30 min wei and kusiak 2015 investigated the capability of mlp neural network model to predict influent flow rate in the wwtp up to 300 min ahead their results showed that mlp provided good predictions up to 150 min ahead by increasing time horizons the accuracy of model decreased the explicit formulas for influent flow rate estimation as a function of input variables were derived for immediate short term and long term horizons from the best hybrid model as mlp mosso short term predictions of the influent flow rate would be important if real time control of different unit processes in a wwpts is to be utilized the previous applied models on influent flow rate prediction are black box and did not provide any mathematical equation for prediction short time influent flow rate in the current paper the explicit equations are provided that can make the results of artificial intelligence more applicable therefore these equations can be used to monitor and estimate of instantaneous influent flow rate at the investigated station in the mlp mosso model the input layer included nodes representing q t 5min and q t 10min for immediate prediction horizon q t 12hr and q t 24hr for short term prediction horizon and q t 2day and q t 4day for long term prediction horizon in the first scenario the best mlp architecture the best solution with one hidden layer four hidden neuron layers a sigmoid function for the second layer and a liner function for the output layer was obtained here regarding a similar explanation the best ann networks can be observed in the figs 5 7 the explicit expressions for the best mlp mosso architectures the best solutions are computed as follows first scenario s1i4 25 sigmoid f z 1 1 e z z 1 2 0 q t 5 m i n 3 1 q t 10 m i n 1 11 z 2 4 1 q t 5 m i n 1 12 q t 10 m i n 1 23 z 3 4 55 q t 5 m i n 9 67 q t 10 m i n 2 33 z 4 2 78 q t 5 m i n 1 23 q t 10 m i n 3 12 z 5 7 65 q t 5 m i n 1 67 q t 10 m i n 1 11 i n f l u e n t f l o w r a t e 18 98 1 e 2 24 1 e z 1 1 12 1 e z 2 2 24 1 e z 3 2 12 1 e z 4 1 14 1 e z 5 9 23 second scenario s2i16 26 tanh f z e z e z e z e z z 1 3 12 q t 12 h r 1 15 q t 24 h r 3 12 z 2 1 14 q t 12 h r 1 11 q t 24 h r 3 41 z 3 1 15 q t 12 h r 1 11 q t 24 h r 5 15 i n f l u e n t f l o w r a t e e 1 12 e z 1 e z 1 e z 1 e z 1 2 23 e z 2 e z 2 e z 2 e z 2 3 25 e z 3 e z 3 e z 3 e z 3 e 1 17 e z 1 e z 1 e z 1 e z 1 2 28 e z 2 e z 2 e z 2 e z 2 3 27 e z 3 e z 3 e z 3 e z 3 e 1 18 e z 1 e z 1 e z 1 e z 1 2 24 e z 2 e z 2 e z 2 e z 2 3 27 e z 3 e z 3 e z 3 e z 3 e 1 14 e z 1 e z 1 e z 1 e z 1 2 24 e z 2 e z 2 e z 2 e z 2 3 29 e z 3 e z 3 e z 3 e z 3 1 92 third scenario s3i15 27 sigmoid f z 1 1 e z z 1 9 21 q t 2 d a y 6 77 q t 4 d a y 9 25 z 2 6 77 q t 2 d a y 5 37 q t 4 d a y 7 16 z 3 7 79 q t 2 d a y 7 77 q t 4 d a y 9 14 z 4 9 78 q t 2 d a y 7 79 q t 4 d a y 6 15 i n f l u e n t f l o w r a t e 14 48 1 e 2 14 1 e z 1 1 89 1 e z 2 1 32 1 e z 3 1 54 1 e z 4 2 67 3 4 uncertainty assessment of hybrid mosso models the uncertainty results of three selected hybrid mosso models for forecasting influent flow rate time series for different prediction horizons are presented in this section as mentioned previously 2000 times of sampling iteration resulted by pdf for influent flow rate then these generated time series from monte carlo generated pdf are used to produce input time series the probability distributions of gamma that were used for estimating flow rate time series is given in eq 24 24 f x α β β α x α 1 e β x γ α α 2 β 4 the 95 interval of prediction uncertainty is calculated at the 2 5 and 97 5 percentage levels of output distribution of influent flow rate q t for determining 95ppu the results of 2 5 and 97 5 percentage levels are shown as a lower line and upper line of uncertainty bound respectively the plots 95ppu for forecasting influent flow rate during testing phase for mlp mosso rbf mosso and svm mosso models are given in figs 12 14 respectively also statistical summary of uncertainty results for three hybrid mosso models in training and testing phases are listed in table 5 it can also be seen clearly from figs 12 14 that ever three models have some uncertainties for forecasting influent flow rate but the uncertainties bound of rbf mosso and svm mosso models are wider than those for mlp mosso model it is means that the rbf mosso and svm mosso models involve high uncertainty for forecasting influent flow rate comparison of uncertainty results in table 5 declares that the mlp mosso model is superior to the rbf mosso and svm mosso models the lowest values of d factor were obtained in scenario 1 for all hybrid models that were equal to 0 11 0 19 and 0 25 in training phase and 0 12 0 21 and 0 27 in testing phase for mlp mosso rbf mosso and svm mosso models respectively the similar results were obtained for scenarios 2 and 3 in training and testing stages of hybrid models the d factor values of rbf mosso model were 0 14 and 0 17 in scenarios 2 and 3 respectively the values of d factor were increased for svm mosso model in scenarios 2 and 3 than the rbf mosso model the higher values of d factor for rbf mosso and svm mosso models compared to mlp mosso model imply that these two models do not perform well in the prediction influent flow rate it is also observable that the models uncertainty for immediate prediction horizon is lower than the short term and long term prediction horizons all models except mlp mosso scenario 1 show several un bracketed values that resulted from inability of models for estimating very high and very low values of influent flow rate these uncertainties show the superiority of estimation influent flow rate using immediate prediction horizon by mlp mosso model based on the results of table 5 the mlp mosso immediate prediction horizon s1i4 model as the best model bracketed 95 and 93 of the observed values in training and testing phases respectively which consists both very high and very low influent flow rates also in scenario 1 the rbf mosso and svm mosso models presented 6 5 and 14 reductions in bracketed observed values in testing phase compared to mlp mosso model respectively the percentage of bracketed observed values by hybrid models reduced in scenarios 2 and 3 compared to scenario 1 in the other words the mlp mosso model and immediate prediction horizon are more reliable than the rbf mosso and svm mosso models and short term and long term prediction horizons for influent flow rate prediction the results of monte carlo uncertainty analysis are in agreement well with taylor diagram and performance analysis 3 5 comparison of standalone mlp rbf and svm models with hybrid models in this section the comparison results of standalone mlp rbf and svm models with hybrid mlp mosso rbf mosso and svm mosso models is discussed the training and testing results of standalone models are given in table 4 figs 15 17 it is clear from the table 4 that the mlp model had the lowest error between standalone models in all scenarios in the training phase and scenario 1 the nse value of mlp rbf and svm models was computed equal to 0 82 0 80 and 0 78 respectively also the mlp model had the lowest rsr 0 44 and pbias 41 among standalone models for scenario 1 the similar results were obtained in testing phase of scenario 1 the nse rsr and pbias were found 0 8 0 46 and 41 for mlp model 0 77 0 49 and 46 for rbf model and 0 76 0 55 and 49 for svm model respectively therefore the rank of applied standalone models in the training and testing phase for scenario 1 are mlp rbf and svm also the accuracy of standalone models in the testing phase was compared for scenarios 2 and 3 the mlp models had better performance than the rbf and svm models for short term and long term prediction horizons the scatter plots of standalone models are given in figs 15 17 it is obviously seen from the scatter plots that the mlp estimations in scenario 1 with r2 0 88 are closer to observed influent flow rate values than the other scenarios and standalone models the comparison of the models reveals that after mlp scenario 1 the models of mlp scenario 2 r2 0 86 and rbf scenario 1 r2 0 85 had better results than the other models fig 18 demonstrate that the predicted values of influent flow rate using standalone models had not accurate predictions and were far away the reference point in the training phase it shows that mlp scenario 1 was placed between 0 4 and 0 8 lines of rmse and had the largest correlation coefficient r 0 81 in the testing phase the model of mlp scenario 1 was close to reference point and had r 0 8 and standard deviation equal to 1 the other models were grouped between rmse lines 0 8 and 1 2 and between 0 5 and 0 75 the results of uncertainty analysis are given in figs 19 21 and table 5 according to table 5 among the standalone models the mlp scenario 1 model present highest bracketed observed values in training 89 and testing 87 phases this result is also clear from the fig 19 for mlp models also the scenarios 2 and 3 had more uncertainty for predicting influent flow rate than the scenario 1 in all standalone models in testing phase the highest unbracketed observed values 20 and d factor 0 3 was obtained by svm scenario 3 model therefore the uncertainty rank of standalone models for predicting influent flow rate in testing phase are mlp scenario 1 p 0 87 d 0 21 rbf scenario 1 p 0 86 d 0 25 mlp scenario 2 p 0 84 d 0 23 mlp scenario 3 p 0 83 d 0 25 svm scenario 1 p 0 83 d 0 28 rbf scenario 2 p 0 82 d 0 27 rbf scenario 3 p 0 81 d 0 28 svm scenario 2 p 0 81 d 0 29 and svm scenario 3 p 0 80 d 0 30 wei et al 2013 compared the performance of mlp and svm models for predicting sewage influent up to 180 min ahead based on the influent flow rate radar reflectivity and rainfall rate data the results showed that mlp model had better accuracy and performance than the svm model the results of current study are in agreement with ansari et al 2018 when applied nonlinear autoregressive network nar and svm models to predict influent flow rate up to 44 weeks in their research the nar predictions of influent flow fit well to observed data and svm model produced high errors the general results indicated that the hybrid mlp mosso rbf mosso and svm mosso had better performance than the standalone mlp rbf and svm models the results of table 4 indicate that in testing phase the nse of scenario 1 increased from 0 82 for standalone mlp to 0 91 for mlp mosso similarly the rsr decreased from 0 46 for standalone mlp to 0 14 for mlp mosso model and pbias decreased from 46 for standalone mlp to 12 for mlp mosso model also the mlp mosso model for scenarios 2 and 3 had higher accuracy than the standalone mlp model comparison of svm and svm mosso indicated that the svm mosso model had better accuracy than the svm model in testing phase for all scenarios the svm mosso increased the accuracy of standalone svm by 7 3 5 1 and 6 7 for nse in the first second and third scenario of testing step respectively also the rsr and pbias values of svm mosso model were lower than the svm model for all scenarios it is clearly seen from the table 4 that the accuracy of rbf mosso model with nse 0 87 rsr 0 29 and pbias 27 was better than the rbf model with nse 0 77 rsr 0 49 and pbias 46 for scenario 1 in testing phase the proposed rbf mosso model showed relatively perfect prediction with nse rsr and pbias values of 0 85 0 32 and 29 in testing phase of scenario 2 when compared with the rbf model with values of 0 77 0 54 and 45 respectively the similar results obtained for rbf mosso model of scenario 3 the mlp mosso model had the highest r2 value of 0 95 0 93 and 0 90 than the other hybrid and standalone models for first second and third scenarios respectively the minimum r2 value were respectively found as 0 80 0 76 and 0 72 for rbf scenario 1 rbf scenario 2 and rbf scenario 3 models the uncertainty results of hybrid and standalone models in table 5 show that mlp mosso model in all scenarios included most bracketed observed data 93 for scenario 1 91 for scenario 2 and 90 for scenario 3 and had lowest d factor 0 12 for scenario 1 0 15 for scenario 2 and 0 21 for scenario 3 than the other models after mlp mosso models the rbf mosso 95ppu 87 and d factor 0 21 mlp 95ppu 87 and d factor 0 21 and rbf 95ppu 86 and d factor 0 25 models for scenario 1 show lowest uncertainty the highest uncertainty was obtained by svm scenario 3 svm mosso scenario 2 and svm mosso scenario 3 models in addition the taylor diagram had proved more accurate predictions of hybrid models than the standalone models another way to visualize models stability is applying box plots for evaluating spread of the error of prediction the hybrid and standalone models for the three basic prediction horizons were compared by box plot of error in fig 22 in training and testing phases as seen in fig 22 the box plot of mlp mosso model was relatively shorter than the rbf mosso and svm mosso models in training step for all scenarios then the data in this model was more compact than two others models also the extent of the simulation errors in standalone models of mlp rbf and svm are larger than the hybrid models in training phase the prediction errors in testing phase are smaller than those in training phase the similar results with training phase were obtained in testing phase where the prediction errors of hybrid models are more compact than the standalone models in all scenarios comparison of mlp rbf svm and svm mosso models with mlp mosso and rbf mosso error values in all scenarios show that these models include more errors for predicting high values of influent flow rate the comparisons of studied models in fig 22 revealed that the mlp mosso model was superior to the other models in scenarios 1 2 and 3 therefore by selecting mlp mosso as the best model for predicting influent flow rate the highest accuracy obtained especially in immediate prediction horizon figs 23 25 show the time series plots of predicted model outputs versus the observed values of influent flow rate in selected first second and third scenarios respectively it clearly indicates that the influent flow rate simulated by hybrid models of mlp mosso and rbf mosso were consistent with the real values these figures confirm the high reliability of hybrid models predictions overall the statistical indices values and uncertainty analysis proved that the mlp mosso model had better performance compared with the other hybrid and standalone models for predicting influent flow rate especially in immediate prediction horizon with input vector of q t 5min and q t 10min this model can satisfactorily predict the fluctuations in observed data of influent flow rate also the results evidence the merits of the proposed mosso model in solving multi objective problems it can be concluded that the proposed model is able to outperform the current well known standalone models 4 conclusion accurate prediction of influent flow rate is helpful to operate of sewage treatment plant to maintain stable effluent to supply cleaned water for industrial and agricultural and to make a decision about water reuse therefore accurate influent flow rate forecasting is absolutely fundamental for normal operations and scheduling of wastewater networks however reliable forecasting of immediate time lags is a challenge problem and limit in this study a powerful approach was developed using meta heuristic multi objective shark smell optimization mosso algorithm in combination with three networks of mlp rbf and svm mlp mosso rbf mosso and svm mosso the proposed hybrid models were used for forecasting influent flow rate in different prediction horizons of immediate q t 5min q t 10min q t 15min short term from q t 1hr to q t 72hr and long term from q t 1day to q t 7day the mosso algorithm was applied to optimize triple objective functions including input vectors model process parameters and activate or kernel functions the parameters of sso algorithm was set using taguchi method also the achieved optimum non dominated solutions from the mosso algorithm were reported as pareto optimal fronts in addition the robust decision making method of topsis was used to select accurate non dominated solution from pareto optimal fronts of mosso algorithm among the three hybrid mosso algorithms used in this paper the mlp mosso neural network performed better than the other applied models for all prediction horizons by increasing time horizon the values of pbias and rsr criteria increased while the r2 and nse values decreased also the monte carlo uncertainty analysis proved the superiority of mlp mosso model for forecasting influent flow rate time series especially for immediate prediction horizon with a input vector including q t 5min and q t 10min s1i4 variables two uncertainty indices of 95ppu and d factor showed that this model has highest accuracy but the other models had large uncertainty for influent flow rate predictions the uncertainty bound in comparison with observed influent flow rate values showed that except immediate prediction horizon s1i4 of mlp mosso model all applied models have several un bracketed values this result indicates to inability of models for estimating very high and very low values of influent flow rate especially using short term and long term prediction horizons overall the results revealed that the proposed intelligence and automatic mlp mosso model achieves a superior and reliable accuracy using immediate prediction horizon than the other compared models for forecasting influent flow rate time series the main advantage of proposed mlp mosso model is that it is easy for applying and is robust approach that is able to produce diverse and efficient pareto frontiers the explicit equations for estimating influent flow rate were derived for immediate short term and long term horizons from the best hybrid model as mlp mosso the results of this study can provide assistance for predicting different horizons of influent time series at wwtps and support engineers to forecast manage and use of the future influent flows hence the proposed approach is a practical and feasible tool for multi objective analysis and decision making the future studies can focus on determining more objective functions for reducing input vectors uncertainty and also investigating neural fuzzy models in combination with meta heuristic multi objective algorithms to enhance the accuracy of predictions credit authorship contribution statement akram seifi conceptualization data curation writing original draft writing review editing mohammad ehteram conceptualization methodology software fatemeh soroush writing original draft declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper appendix 1 the statistical characteristics of dataset train first scenario model mean l s 1 standard deviation minimum l s 1 maximum l s 1 mlp mosso 102 10 37 98 28 23 180 45 rbf mosso 103 45 38 12 28 45 180 23 svm mosso 104 12 39 23 28 67 190 25 mlp 105 11 39 99 28 23 193 32 rbf 106 12 40 91 28 23 199 45 svm 107 23 40 12 28 55 202 21 train second scenario mlp mosso 102 12 38 12 28 24 180 81 rbf mosso 103 45 39 10 28 46 181 23 svm mosso 104 12 39 45 29 12 203 12 mlp 105 11 40 55 28 56 204 10 rbf 106 12 41 12 28 78 204 20 svm 107 23 42 14 28 89 204 55 train second scenario mlp mosso 102 14 39 212 28 24 182 34 rbf mosso 103 47 39 20 28 56 199 23 svm mosso 104 55 39 66 29 78 200 12 mlp 105 21 40 78 28 67 202 45 rbf 105 67 41 34 28 79 206 78 svm 105 71 42 67 28 89 208 23 test first scenario model mean l s 1 standard deviation minimum l s 1 maximum l s 1 mlp mosso 99 89 38 78 47 70 181 93 rbf mosso 99 91 39 12 47 81 182 34 svm mosso 102 23 40 21 47 84 183 95 mlp 103 45 41 12 47 91 190 23 rbf 104 12 42 23 47 89 195 67 svm 105 23 43 24 47 89 197 12 test second scenario mlp mosso 99 92 38 82 48 73 181 91 rbf mosso 99 93 39 44 48 83 182 96 svm mosso 102 34 40 24 48 85 183 96 mlp 103 51 41 44 48 68 191 29 rbf 104 32 42 34 48 91 195 92 svm 105 26 43 55 48 93 197 14 test second scenario mlp mosso 99 96 40 23 47 96 181 93 rbf mosso 99 99 42 12 47 98 182 98 svm mosso 102 98 43 14 47 99 184 55 mlp 103 98 44 56 48 25 191 24 rbf 104 55 45 78 48 22 195 93 svm 105 78 45 98 48 23 197 16 
