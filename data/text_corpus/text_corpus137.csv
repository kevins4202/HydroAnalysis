index,text
685,a large number of complex fluids commonly used in industry exhibit yield stress e g concentrated polymer solutions waxy crude oils emulsions colloid suspensions and foams yield stress fluids are frequently injected through unconsolidated porous media in many fields such as soil remediation and reservoir engineering so modelling their flow through this type of media is of great economic importance however obtaining macroscopic laws to model non newtonian flow poses a considerable challenge given the dependence of the viscosity of the fluid on pore velocity for this reason no macroscopic equation is currently available to predict the relationship between injection flow rate and the pressure drop generated during the flow of a yield stress fluid without using any adjustable parameter in this work a method to extend darcy s equation to the flow of yield stress fluids through model unconsolidated porous media consisting of packs of spherical beads is presented then the method is experimentally validated through comparison with a total of 572 experimental measurements obtained during the flow of a concentrated aqueous polymer solution through different packs of glass spheres with uniform size an improved prediction of the pressure drop flow rate relationship is achieved by taking into account the non linear relationship between apparent shear rate and average pore velocity 1 introduction the flow of complex fluids in unconsolidated porous media is involved in many economically important industrial applications e g remediation of polluted soils gastone et al 2014 enhanced oil recovery eor wang et al 2017 rock fracturing roustaei et al 2016 and liquid food engineering welti chanes et al 2005 numerous complex fluids are shear thinning showing a decrease in shear viscosity as the applied shear rate is increased shear thinning fluids are extensively used in petroleum engineering and soil remediation to improve the microscopic sweep of the reservoir through stabilization of the injection front lake 1989 silva et al 2012 wever et al 2011 in some cases fluids with shear rate dependent viscosity additionally present a yield stress i e a threshold value in terms of shear stress below which they do not flow in the specific field of petroleum engineering the drilling fluids injected into rocks for the drilling of wells are often designed so as to have a yield stress in order to prevent cutting from settling when circulation stops lavrov 2013 coussot 2014 some examples of yield stress fluids used in oil industry include emulsions drilling muds polymeric gels such as carbopol hevy oils and foams talon et al 2014 furthermore a number of fracturing fluids used in hydraulic fracturing as gelling agents exhibit a yield stress designed to enhance proppant transport roustaei et al 2016 among them guar hydroxypropyl guar carboxymethyl hydroxypropyl guar hydroxyethyl cellulose and polyacrylamide are of particular relevance belyadi et al 2016 predicting the pressure drop of a yield stress fluid flowing through unconsolidated porous media is especially important given that a great number of petroleum reservoirs are located in unconsolidated formations peng et al 2007 pang and liu 2013 the majority of laboratory experiments in this field have been performed using beds of spherical beads which represent an idealization of unconsolidated porous media rao and chhabra 1993 tiu et al 1997 basu 2001 the newtonian case is generally well understood in this type of porous media and allows process design calculations with acceptable levels of accuracy the extensive literature regarding the flow of fluids with complex rheology through unconsolidated packed beds was critically reviewed by chhabra et al 2001 and sochi 2010 analysed the available models for describing non newtonian single phase flow in porous media more recently rodríguez de castro and radilla 2017a extended forchheimer s law and ergun s equation to the flow of fluids with shear rate dependent viscosity through packs of glass spheres with uniform size the latter authors determined the accuracy of the extended laws under creeping and inertial regimes from comparison with a full set of experiments nevertheless the yield stress effect was not addressed in the preceding works and obtaining a macroscopic law for the flow of yield stress fluids has proved to be a stumbling block talon and bauer 2013 performed lattice boltzmann simulations to solve 2d flow of bingham yield stress fluids in porous media and distinguished three different flow regimes these regimes corresponded to 1 the flow of a single pore 2 progressive pores opening and 3 flow of all pores furthermore chevalier et al 2013 focused on obtaining a generic relationship between flow rate and pressure drop applicable to the darcian flow of yield stress fluids through packed beds these authors proposed a macroscopic equation in which only the parameters of the rheological law of the injected fluid the diameter of the beads and two coefficients related to the internal structure of the porous medium were used as inputs then chevalier et al 2014 conducted nmr experiments which contributed to elucidate the structural parameters appearing in this generic law however the determination of these coefficients is still unclear and the proposed formula presents the inconvenient of assuming linear relationship between shear rate and darcy velocity in definition of the apparent shear rate in the porous medium moreover the extension of darcy s equation to the flow of yield stress fluids proposed by chevalier et al 2013 still has to be confirmed by further laboratory experiments given the serious lack of reliable experimental data in the literature lavrov 2013 coussot 2014 inspired by the broad interest of extending darcy s law to the flow of yield stress fluids through model unconsolidated porous media the objective of this work is to provide a straightforward procedure to predict the relationship between pressure gradient and flow rate the proposed method is also evaluated through comparison with experimental data to do so a series of flow experiments through four different packs of mono size spherical glass beads were carried out using concentrated aqueous solutions of xanthan biopolymer presenting a yield stress the effect of beads size on the accuracy of the predictions is then assessed and discussed in the area of non newtonian flow in porous media still very open it is crucial to base the interpretations and the modelling on solid observations the experimental details concerning the injection of xanthan gum solutions in different types of porous media have been carefully evaluated and discussed in the past rodríguez de castro 2014 rodríguez de castro et al 2014 2016 2018 rodríguez de castro and radilla 2017b in all these preceding works the same aqueous polymer solution was used and the experimental aspects were thoroughly addressed including a discussion on the rheological model the existence of a plateau viscosity the capability of the fluid to emulate yield stress behaviour and the interactions between fluid and porous medium polymer retention mechanical degradation and polymer adsorption for this reason it was decided in the present work to capitalize the knowledge acquired from the preceding research by using the same extensively investigated xanthan gum solution 2 predicting the flow of yield stress fluids in packed beds 2 1 previous attempts to extend darcy s law to the flow of yield stress fluids herschel bulkley empirical law herschel and bulkley 1926 is commonly used to describe the rheological behaviour under shear of a large group of time independent yield stress fluids this law can be written as follows 1 τ τ 0 a γ n for τ τ 0 γ 0 for τ τ 0 where τ is the shear stress experienced by the fluid at a given shear rate γ τ0 is the yield stress a is the consistency and n is the flow index of the fluid in the case of shear thinning yield stress fluids n is inferior to unity the three parameters are generally obtained by fitting the data obtained by measuring the shear rate γ as a function of the applied shear stress τ with a rheometer several attempts have been made to obtain a macroscopic law linking the injection flow rate to the resulting pressure drop during the flow of yield stress fluids in porous media pascal 1981 pascal 1983 al fariss and pinder 1987 chase and dachavijit 2005 coussot 2014 pascal modified darcy s law by introducing a threshold pressure gradient pt to account for the yield stress pascal 1981 p t φ τ 0 k pt is directly proportional to τ0 and inversely proportional to the square root of the absolute permeability k however pascal s relationship presents the serios drawback of including a dimensionless constant φ that must be empirically determined for each fluid medium pair also it only applies to the case n 1 indeed the existence of experimentally adjustable parameters with no clear physical meaning as inputs which impedes direct computational predictions is a major drawback of most available macroscopic flow expressions in this regard shahsavari and mckinley 2016 conducted numerical simulations providing analytical expressions for such parameters in the particular case of fibrous materials without including any specific dependence of these coefficients on the injection velocity only a few experimental works exist for the flow of yield stress fluids in porous media al fariss and pinder 1987 chase and dachavijit 2005 chevalier et al 2013 chevalier et al 2014 rodríguez de castro 2016 and the ranges of variation of the flow rate are usually narrow these experimental works showed that the relationship between the absolute value of the pressure gradient p and the absolute value of darcy velocity u is of the same form as the constitutive equation of the fluid i e p pt cun with c being a parameter that depends on the porous medium and the boundary conditions more recently chevalier et al 2013 presented a simple approach to extend darcy s law to the flow of yield stress fluids this general law contains a yielding term which may be simply expressed as a function of the yield stress of the material and the bead size 2 p χ τ 0 d s ω a u d s n d s with δp being the absolute value of the pressure drop through the packed bed of length l p δ p l the magnitude of the pressure gradient q the volume flow rate a the cross sectional area u q a the absolute value of the darcy velocity and ds the diameter of the spherical beads the latter authors initially stated that χ and ω in eq 2 should be universal factors for the flow through spherical beads the first coefficient is related to the path of maximum width throughout the porous medium while the second coefficient reflects the pore size distribution however on the basis of the results obtained by nmr measurements it was subsequently shown that χ and ω are two dimensionless coefficients depending only on the distribution of shear rate intensity and on the coefficient n which are in turn fluid dependent chevalier et al 2014 also u ds was considered to be the apparent shear rate for the flow through such a porous medium which is a serious flaw of eq 2 indeed the apparent shear rate was shown not be proportional to u in the case of yield stress fluids flowing at low and moderate flow rates rodríguez de castro and radilla 2017b the first yielding term on the right hand side of eq 2 corresponds to the critical pressure gradient below which no flow occurs the second term is velocity dependent and expresses the additional viscous pressure drop above the yielding pressure once the fluid is flowing chevalier et al 2013 experimentally determined the values of χ and ω obtaining χ 12 for a carbopol aqueous solution and χ 5 5 for a water in oil emulsion which did not permit to validate the universality of this coefficient in contrast these authors found that ω 85 for both types of fluids 2 2 new approach to extend darcy s law to the flow of yield stress fluids as mentioned above despite the method presented by chevalier et al 2013 being a valid approach the choice of ds as characteristic length in the definition of the apparent shear rate and the non dependence on injection velocity remain debatable also the values of χ and ω are not easily predictable for these reasons the objective of this subsection is to present a method to simply predict the u vs p relation by properly defining the actual shear rate and the shear viscosity of the fluid in the porous medium darcy s law darcy 1856 describes the single phase flow of incompressible newtonian fluids through porous media at low values of reynolds number 3 p μ k q a μ k u where μ is the shear viscosity of the injected fluid and k is the intrinsic permeability moreover kozeny carman equation allows to predict k from the porosity ε of the bed and the diameter of the beads using hydraulic radius theory 4 k ε 3 d s 2 36 κ 1 ε 2 κ being the kozeny carman constant the value of which is generally set to κ 5 in packs of spheres kaviany 1995 previous works have shown that some concentrated polymer solutions are yield stress fluids song et al 2006 carnali 1991 withcomb and macosko 1978 economides and nolte 2000 khodja 2008 benmouffok benbelkacem et al 2010 the steady state shear flow of these solutions can be well described by the herschel bulkley law eq 1 a practical approach to study the flow of complex fluids through a porous medium consists in defining an equivalent viscosity μ eq as being the quantity that must replace the viscosity in darcy s law to result in the same pressure drop actually measured tosco et al 2013 5 μ e q k p u in order to predict μ eq from the constitutive equation of the fluid an apparent shear rate in the porous medium γ p m has to be determined first assuming a bundle of capillaries model γ p m is usually taken as four times the average pore velocity 4 u ε divided by the average pore throat radius r characteristic length of the microscopic flow chauveteau and zaitoun 1981 chauveteau 1982 sheng 2011 r can be estimated from the permeability k and the porosity ε of the porous medium as proposed by kozeny 1927 using a bundle of capillaries model 6 r 8 k ε according to the preceding definition γ p m can be expressed as 7 γ p m 4 α u ε r α 2 u k ε where α is an empirical shift factor known to be a function of both the bulk rheology of the fluid and the tortuosity of the packed bed chauveteau 1982 sorbie et al 1989 lópez et al 2003 lópez 2004 comba et al 2011 therefore γ p m corresponds to the wall shear rate in a pore section of radius r this definition of apparent shear rate is in contrast with the one used by chevalier et al 2013 in which ds is taken as characteristic length instead of k ε 2 for the creeping flow of herschel bulkley fluids μ eq can be obtained from eqs 1 and 7 8 μ e q τ 0 k ε α 2 u a α 2 u k ε n 1 keeping in mind the objective to propose a prediction method analytical expressions for the calculation of α must be provided in order to obtain such expressions let us focus now on the determination of the wall shear rate in circular channels for the steady flow of an incompressible fluid through a circular channel of radius r the wall shear stress τ w is related to the pressure gradient p as follows 9 τ w p r 2 using eqs 3 8 9 can be written as 10 τ w τ 0 γ p m a γ p m n 1 u k r 2 1 2 2 ε k r α τ 0 2 n 3 2 k n 1 2 ε n 1 2 r α 1 n a u n for a constant viscosity incompressible fluid the wall shear rate γ w n e w t o n i a n is given by γ w n e w t o n i a n α n 4 u ε r where α n is the shift factor for the injection of a newtonian fluid α n is related to the tortuosity of the fluid flow through the packed bed and its value was shown to be 0 69 for spherical beads christopher and middleman 1965 shenoy 1994 however this value αn 0 69 has been contested by some authors james and mclaren 1975 chaveteau 1982 for this reason in this work α n will be considered first as unknown and will be determined through fitting to the experimental u vs p data then the obtained α n will be compared to the values previously reported in the literature the wall shear rate for the flow of liquids with a shear rate dependent viscosity can be calculated by using the weissenberg rabinowitsch mooney equation rabinowitsch 1929 mooney 1931 11 γ p m γ w n e w t o n i a n 3 2 d ln γ w n e w t o n i a n d ln τ w γ w n e w t o n i a n 3 2 d ln γ w n e w t o n i a n d u d u ln τ w u d u ln τ w α d α γ w n e w t o n i a n 3 2 d ln γ w n e w t o n i a n d u ln τ w u ln τ w α d α d u where α is a function of u weissenberg rabinowitsch mooney equation is commonly used to calculate the wall shear rate of complex fluids with non parabolic velocity profiles including yield stress fluids macosko 1994 steffe 1996 pipe et al 2008 sochi 2015 the following assumptions are used in the derivation of eq 11 incompressible fluid steady state laminar flow regime no wall slip no end effects unidirectional flow temperature is constant and properties are not a function of time or pressure steffe 1996 for a herschel bulkley fluid eq 11 becomes 12 γ p m α n r 4 u 3 ε 2 α τ 0 2 n 2 a α u k ε n 2 n 2 a n α α u k ε n 2 u τ 0 2 n 2 a n 1 α u k ε n α u from eq 12 it can be deduced that α becomes the constant value α α n 3 2 1 n for very high values of u by combining eqs 7 and 12 the following differential equation is obtained which allows the determination of α as a function of u 13 α α n r 2 2 k 3 ε 2 α τ 0 2 n 2 a α u k ε n 2 n 2 a n α α u k ε n 2 u τ 0 2 n 2 a n 1 α u k ε n α u for the simpler case of a power law fluid τ0 0 eq 14 leads to α α n 3 2 1 n which becomes α α n for a newtonian fluid therefore α is a constant parameter only if τ0 0 eq 13 can be numerically solved within a given range of u to obtain the relation between α and u then the obtained relation can be used in eq 8 to obtain μ eq once μ eq has been determined it can be entered in eq 3 leading to the extension of darcy s law eq 14 14 p μ e q k u c 1 α c 2 α n 1 u n with c 1 τ 0 ε 2 k and c 2 a k n 1 2 2 ε n 1 2 it is reminded that the value of α n is considered first as unknown and must be obtained by fitting eq 14 to the experimental u i p i data this is achieved by finding the value of α n that minimizes the sum e i 1 n p i p u i p i with n being the number of experimental data by comparing the method presented in this subsection with the works of chevalier et al 2013 it can be deduced from eqs 2 and 14 that 15 χ ε 1 2 d s 2 1 2 k 1 2 α moreover eq 4 can be used together with eq 15 to express χ as a function of only ε and α obtaining 16 χ 2 1 2 κ 1 2 3 1 ε ε α 2 1 2 5 1 2 3 1 ε ε α it can be concluded from the preceding equation that χ is a constant at high injection flow rates given that the value of α is also constant α α therefore the first yielding term on the right hand side of eq 2 can be considered a constant at high flow rates however this term depends on u at low and moderate values of u which was not taken into account in the work of chevalier et al 2013 also the following relationship can be obtained from comparison between eqs 2 and 14 17 ω 2 n 1 2 ε n 1 2 k n 1 2 α n 1 d s n 1 analogously eq 4 can be used together with eq 17 to express ω as a function of only ε and α obtaining 18 ω 2 3 n 1 2 3 n 1 κ n 1 2 1 ε n 1 ε 2 n 1 α n 1 2 3 n 1 2 3 n 1 5 n 1 2 1 ε n 1 ε 2 n 1 α n 1 eq 18 shows that ω is also contant at high flow rates while being a function of u at moderate and low flow rates moreover ω is not a function of ds but depends on the fluid properties through α this is contrast to the claim of chevalier et al 2013 according to which χ and ω are universal factors for a porous medium composed of an assembly of spheres it should be kept in mind that elongational flows during the injection of solutions of polymers presenting a certain degree of flexibility through porous media are known to induce extra pressure losses with respect to pure shear flow rodríguez et al 1993 müller and sáez 1999 nguyen and kausch 1999 seright et al 2011 amundarain et al 2009 this is a result of the formation of transient entanglements of polymer molecules due to the action of the extensional component of the flow in the present work we first hypothesize that the deviation of the experimentally measured pressure drop with respect to the viscous pressure drop are negligible this hypothesis is then validated through analysis of the experimental results 3 experimental methods and materials experimental p vs u measurements were performed by injecting a xanthan gum aqueous solution yield stress fluid through four packs of spherical glass beads flow experiments with filtered water newtonian fluid were also performed by following the procedure presented by rodríguez de castro and radilla 2017a in order to determine the permeability of the packed beds the glass beads were first placed into transparent acrylic glass cylinders and then compactly packed by means of vibration with a sieve shaker the inner diameter of the acrylic glass cylinders was d 5 cm and the diameter of the glass spheres used in each of the four columns was uniform with ds 1 mm 3 mm 4 mm and 5 mm in each case the length of the column was l 20 cm two different configurations were used depending on the involved flow rates for 0 12 l h q 6 l h the injection circuit was open and the fluid was injected through the packed beds at the selected flow rate using a dual piston pump prep digital hpcl pump a i t france for 9 l h q 250 l h the fluid was injected through a closed circuit using a volumetric pump as performed by rodríguez de castro and radilla 2017a a photo showing the experimental setup is provided as supplementary material fig s1 details of the experimental setup and procedure including the working ranges of the instruments and the measurement uncertainties were provided by rodríguez de castro and radilla 2017a the ranges of u imposed during the experiments with each packed bed are listed in table 1 xanthan biopolymer is a microbial high molecular weight exo polysaccharide produced by fermentation of x campestris bacteria garcia ochoa et al 2000 palaniraj and javarman 2011 kumar et al 2018 in solution state an isolated macromolecule of this polymer is more or less rigid and with a typical contour length of 1 µm mongruel and cloitre 2003 and a transverse size of approximately 2 nm the stiffness of xanthan macromolecules leads to high levels of shear viscosity and highly shear thinning behaviour of semidilute solutions in water for this reason the shear rheology of xanthan gum solutions is well described by the herschel bulkley model eq 1 under steady state conditions garcía ochoa and casas 1994 song et al 2006 rodríguez de castro et al 2014 2016 2018 rodríguez de castro and radilla 2017b however rigorously speaking they should be referred to as pseudo yield stress fluids the capacity of xanthan gum solutions to emulate the shear rheology of a yield stress fluid and the effects of polymer concentration was experimentally assessed by rodríguez de castro et al 2018 concluding that concentrated solutions 7000 ppm behave similarly to a yield stress fluid due to high viscosity values at low shear rates sixty litres of aqueous solution were prepared with xanthan gum concentration cp 7000 ppm and the rheogram was obtained following the procedure presented by rodríguez de castro and radilla 2017b eq 1 was then used to fit the rheogram rodríguez de castro et al 2014 giving τ0 7 4 pa a 0 37 pa s n and n 0 52 the rheogram of the solution and the herschel bulkley fit are provided in fig 1 the dynamic viscosity of water solvent was measured to be 0 0011 pa s and the densities ρ of both the water and the xanthan gum solution were taken as 1000 kg m3 moreover the rheograms of several effluent fluid samples were characterized and compared to that of the inflowing fluid at the highest injection flow rates in order to assess polymer degradation and retention on the pore walls no significant difference was observed between the rheograms proving that polymer degradation and polymer retention can be neglected despite the used glass beads being quite coarse as compared to most natural granular media the explored sizes fall within the range of grain sizes reported for coarse sand and fine gravel which are widely investigated in hydrologic applications morris and johnson 1967 moreover these beads sizes are commonly used in previous research e g dukhan et al 2014 so this choice facilitates comparison with literature data furthermore the use of smaller beads may result in polymer retention which was not observed in the present experiments 4 results the flow experiments were conducted for both fluids water and yield stress fluid and were repeated four times the number of repetitions for yield stress fluid injection through each packed bed corresponds to 4 n ranging from 108 to 176 as listed in table 1 the 4 n measures for each packed bed were considered to be an experimental set a total of 572 measurements were performed during the flow experiments with the yield stress fluid 4 1 experimental determination of ε and k the weight of each packed bed was measured before and after saturation with water in order to determine ε from the difference in mass also the procedure followed by rodríguez de castro and radilla 2017a to determine k from injection experiments with water was applied to the present measurements the obtained values and for ε and k are listed in table 2 together with the associated uncertainties 95 confidence interval 4 2 shear viscosity of the yield stress fluid in the porous media eq 13 was numerically solved within the involved range of u for both all the investigated packed beds using an implicit runge kutta method the resulting α versus u functions are represented in fig 2 and the results obtained for α n are listed in table 3 as a function of ds it is noted that the value of α n was close to 0 68 average value in all the tested porous media for the polymer solution used in the present work this is in very good agreement with the results of christopher and middleman 1965 who obtained α n 0 69 as mentioned in section 2 2 α becomes the constant value α α n 3 2 1 n for very high values of u i e when u τ 0 1 n k 1 2 ε 1 2 2 1 2 a 1 n α in the case of herschel bulkley shear thinning fluids 0 n 1 in fig 2 it can be observed that α monotonically decreases as u increases so the condition u τ 0 1 n k 1 2 ε 1 2 2 1 2 a 1 n α will be satisfied if u u τ 0 1 n k 1 2 ε 1 2 2 1 2 a 1 n α consequently the boundary condition α u 105 u α was used to numerically solve eq 13 the obtained α values are also listed in table 3 and are all close to 0 88 average value regarding the sensitivity of α to the microstructure of the packed bed it can be deduced from fig 2 that higher values of ds coarser microstructure result in higher values of α the value of γ p m corresponding to each darcy velocity u was calculated with eq 7 following two different approaches first a constant value of α named α was determined for each porous medium by calculating the shift factor in terms of shear rate which led to the best superposition between the in situ μ eq vs γ p m data and the bulk rheological law eq 1 the obtained values for α are shown in table 3 the second approach consisted in using the α u function obtained from eq 13 the results of both approaches are presented in fig 3 together with the bulk rheological law eq 1 in this figure it can be observed that μ eq is close to eq 1 at high values of u for both the constant α and the variable α methods however this is not the case at low and moderate values of u for which μ eq approaches better eq 1 with the variable α method also μ eq is expected to be greater than the bulk viscosity at high values of u in the presence of important inertial effects tosco et al 2013 rodríguez de castro and radilla 2016 the fact that no important deviation of μ eq with respect to μ pm is observed in the present experiments reflects that inertial effects are not significant moreover fig 3 shows that the shear rates involved in the flow through all porous media are within the same range as those measured with the rheometer during characterization of the fluid s shear viscosity 4 3 previous attempts to extend darcy s law to the flow of yield stress fluids the values of χ and ω were determined by fitting the experimental results presented in this work to eq 2 through minimization of the sum of the absolute values of the differences between fit and experimental data the obtained values are listed in table 4 showing that χ and ω are porous medium dependent as experimentally determined also it is remarked that the values of these coefficients may depend on the range of imposed u as they are obtained through fitting to experimental data this dependence on u is taken into account by the new method proposed in the present work as explained in section 2 2 the results of fitting eq 2 to the experimental data are shown in fig 4 moreover the average errors of these fits are presented in table 5 for different ranges of u it is observed that the resulting fits are accurate within a large range of u however a major drawback of this method is that χ and ω need to be experimentally determined which impedes prediction of the u vs p relation it is worth mentioning that the errors obtained by using χ 5 5 and ω 85 as proposed in the work of chevalier et al 2013 are too big in the case of the present experiments and lead to very inaccurate predictions 4 4 experimental validation of the new prediction method eq 14 was used to predict the relation between p and u for the injection of the 7000 ppm solution through the four packed beds the obtained predictions are presented in fig 5 together with the experimental results of measurements performed in the present work in this figure the errors bars correspond to a 95 confidence interval as explained in section 3 from these results the accuracy of the proposed methods for the prediction of p as a function of u during the flow of yield stress fluids through packed beds of spherical beads can be assessed fig 5 shows that the variable α approach provides more accurate predictions within the low and moderate u regions which is in agreement with the arguments presented above however a less important difference is obtained between both methods for the highest values of u the average errors obtained with the variable α method and the fixed α method for different ranges of u are summarized in table 5 it is observed that the variable α method successfully predicts the p u relationship for the flow of the yield stress fluid through the four packed beds even though the obtained predictions are slightly less accurate in the case of ds 1 mm the overestimation of p reported in figs 4 and 5 for the lowest flow rates may be related to the longer times needed to achieve stationary measurements of pressure drop within this region this effect is similar as the one reported for rheological measurements at low shear rates as shown in fig 1 this is a consequence of the viscosity of the fluid continuously increasing over time as illustrated in fig s2 this effect will be discussed in section 5 as mentioned above α becomes the constant value α α n 3 2 1 n when u u τ 0 1 n k 1 2 ε 1 2 2 1 2 a 1 n α this means that eq 14 presents a constant yielding term of value c 1 α and a constant consistency term of value c 2α n 1 for u u in other words eq 14 has the same form as herschel bulkley empirical law eq 1 only if the preceding condition is met therefore the threshold reynolds number re above which the extended darcy s law for herschel bulkley eq 14 fluids has the same form as herschel bulkley equation is given by 19 r e ρ u k μ where μ is the shear viscosity of the fluid in the porous media at u re is represented as a function of k for the four packed beds in fig 6 showing linear relationship it is worth mentioning that in spite of the negligible influence of inertial effects on the pressure drop vs flow rate relationships in the case of the highly viscous xanthan gum solutions used in this work the procedure presented in section 2 2 is also valid to extend forchheimer equation forchheimer 1901 to the case of yield stress fluids this is explained by the fact that the inertial coefficient appearing in forchheimer equation does not depend on the shear rheology of the injected fluid as numerically firdaouss et al 1997 yadzchi and luding 2012 tosco et al 2013 and experimentally rodríguez de castro and radilla 2016 2017a 2017b proved in previous works however it must be noted that even for newtonian fluids the macroscopic transport equations governing inertial regime are still under debate in the literature in particular it was demonstrated that whereas forchheimer regime is always well identified for inertial flow in disordered porous media its appearance in ordered media is strongly dependent on the microstructure and the orientation of the pressure gradient lasseux et al 2011 agnaou et al 2017 therefore a non linear dependence of apparent viscosity on pore scale velocity is expected to increase the complexity of the problem 5 discussion the values of α n obtained for all the tested porous media were always very close to 0 69 which was the value theoretically predicted by christopher and middleman 1965 it should be noted that although α n 0 69 is valid for the present experiments this value must still be confirmed by further experiments in different yield stress fluid packed beds combinations before declaring that it is a universal constant nevertheless it can be firmly stated that the results reported in this work are a highly promising step in this direction it can be deduced from eq 14 that α can be considered a constant value α α n 3 2 1 n in the high flow rates region i e when u u τ 0 1 n k 1 2 ε 1 2 2 1 2 a 1 n α and eq 14 can be written as 20 p p 0 c u n with p 0 3 τ 0 ε 2 k α n 2 1 n and c a k n 1 2 2 ε n 1 2 α n 3 2 1 n n 1 this is in agreement with the results of talon et al 2014 who stated that u scales linearly as p p 0 in the case of a bingham fluid n 1 flowing at high u through a one dimensional channel also nash and rees 2017 showed that the manner in which flow begins once the threshold pressure gradient is exceeded strongly depends on the channel size distribution of the porous media the same authors talon et al 2014 nash and rees 2017 proved that p 0 is higher than the actual threshold pressure which is consistent with our results given that α increases as u tends to zero fig 2 a model to accurately predict the flow of yield stress and carreau fluids through rough walled fractures by using α n 1 was presented and experimentally validated in a previous work rodríguez de castro and radilla 2017b it is reminded here that α n is a tortuosity related factor so media with different tortuosity may lead to different values of α n indeed the effective average pore throat radius r e f f which takes into account the tortuosity of the medium can be defined as r e f f 8 k t ε r t r α n with α n 1 t and t being the tortuosity factor christopher and middleman 1965 chaveteau 1982 given that the tortuosity of the flow paths in a packed bed is higher than in a fracture a lower value of α n is expected for packed beds in the case porous media with more complex pore size distributions the flow is highly conditioned by the narrowest flow paths at low flow rates and the representative pore section should be smaller than r in this regard the full set of equations presented in section 2 2 should be reconsidered as the current method is not able to capture the influence of pore size distribution nevertheless the use of the present method with more complex porous media should still be useful to predict the relationships between p and u with higher accuracy than the existing methods which use a constant viscosity value the existence of yield stress was challenged by barnes and walters 1985 and has been discussed for more than 30 years as explained by møller et al 2009 the supporters of the existence of yield stress commonly argue that the viscosity increases very sharply in some materials as the stress decreases towards the yield stress however other researchers claim that only a finite and constant viscosity newtonian plateau of viscosity is observed below a certain stress in particular barnes and walters 1985 used stress controlled rheometers to show that at low enough shear rates viscosity reaches a newtonian plateau for carbopol and other fluids which had traditionally been considered to have a yield stress they argued that any material flows providing enough observation time and sufficiently sensitive measuring equipment in stark contrast with barnes and walters 1985 and møller et al 2009 experimentally showed that such newtonian plateau is the consequence of non steady state measurements they demonstrated that for stresses below the yield stress viscosity is a priori unbounded and increases continuously though slowly if enough time is allowed they effectively observed an increase in viscosity even after 100 s in other words they found that viscosity is time dependent and tends to infinity below the yield stress in the case of the present xanthan gum solutions the evolution of viscosity over time was measured for 1000 s under a shear stress of 0 5 pa below the yield stress using a rheometer equipped with cone plate geometry the results are provided as supplementary material fig s2 showing that viscosity does not attain a constant value and continues to increase after that time one may wonder whether the proposed procedure is simpler than performing a numerical solution to the actual flow equations without invoking a bundle of capillaries approximation in this sense it should be highlighted that performing a numerical solution to the actual flow equations would imply using the size distribution of the flow paths as an input for the model which is rarely available in real applications it is reminded that the objective of this work is to present a simple method to predict the pressure drop for the flow of yield stress fluids through packed beds therefore using hardly accessible inputs as needed to perform a numerical solution to the actual flow equations is not a valid approach it is noted that in our experiments with yield stress fluids the total pressure drop through the porous media was successfully predicted from the value of k obtained from water injection without any significant deviation therefore similarly to the case of previous flow experiments with shear thinning fluids without yield stress rodríguez de castro and radilla 2017a no appreciabe effect of elongational viscosity has been observed in the present work also wall effect issue during shear thinning creeping flow in packed beds was previously addressed in the literature on this subject rao and chhabra 1993 studied the effects of column walls and particle size distribution on the flow rate pressure drop relationship proposing a wall correction method and confirming the applicability of the mean hydraulic radius of the particles to characterize a bed of mixed size spheres the latter authors showed that wall effect is less significant in the case of shear thinning fluids than in the newtonian case in the present experiments the porosity of all packed beads is 0 35 0 01 and the experimentally measured permeability is very close to kozeny carman prediction for the largest beads 3 6 difference therefore there is no evidence of significant wall effect affecting pressure drop vs flow rate relationship 6 summary and conclusions a simple approach to extend darcy s law to the flow of yield stress fluids through packed beds has been presented in this work this method takes into account the non proportional relationship between the apparent shear rate in the porous medium γ p m and average pore velocity u only the porosity ε and the permeability k of the porous medium exclusively for high flow rates are used as inputs of the method together with the herschel bulkley parameters of the fluid τ0 a n the following procedure to predict p as a function of u is proposed 1 determine the shear rheology parameters of the fluid using a rheometer τ0 a n 2 measure the porosity ε of the packed beds e g from difference in mass before and after saturation with water note that the usual values are close to ε 35 3 measure k from newtonian flow experiments alternatively k can be estimated from kozeny carman equation eq 4 or determined by other techniques e g x ray tomography however the cited methods provide k estimates with very different accuracy which can be roughly estimated to 5 for experimental assessment 10 for kozeny carman and 20 for tomography 4 calculate the values of α u 4 1 when low and moderate values of u are involved solve the following differential equation eq 13 to obtain α u α α n r 2 2 k 3 ε 2 α τ 0 2 n 2 a α u k ε n 2 n 2 a n α α u k ε n 2 u τ 0 2 n 2 a n 1 α u k ε n α u a value of α n 0 68 is proposed based on the results of the present experiments and previous theoretical works when only high values of u are involved u u τ 0 1 n k 1 2 ε 1 2 2 1 2 a 1 n α use a constant value α α α n 3 2 1 n 4 2 use eq 8 to compute μ eq as a function of u ε k τ0 a and n μ e q τ 0 k ε α 2 u a α 2 u k ε n 1 5 use eq 14 to calculate p as a function of u p μ e q k u c 1 α c 2 α n 1 u n with c 1 τ 0 ε 2 k and c 2 a k n 1 2 2 ε n 1 2 flow experiments of yield stress fluids covering a wide range of u were performed in order to assess the accuracy of the predictions obtained using the proposed method showing good agreement between model and experiments and negligible inertial effects within the explored range of u consequently darcy s law provides accurate u p predictions in contrast to the case of less concentrated solutions with no yield stress in which inertial effects were significant rodríguez de castro and radilla 2017a as an important industrial application the extended darcy s law can be included in computational studies of large scale non newtonian flow in unconsolidated porous media the conclusions of this work have now to be assessed using real granular media also future numerical studies should be performed in order to provide deeper insight into the physical mechanisms governing the non proportional relationship between γ p m and u acknowledgments the author would like to acknowledge pr giovanni radilla for providing the equipment used in the experimental part of this work the author also wishes to thank frédéric bastien for his technical support throughout the experimental campaign supplementary materials supplementary material associated with this article can be found in the online version at doi 10 1016 j advwatres 2019 01 012 appendix supplementary materials supporting figures are included as three figures in the online resource 1 file image application 1 
685,a large number of complex fluids commonly used in industry exhibit yield stress e g concentrated polymer solutions waxy crude oils emulsions colloid suspensions and foams yield stress fluids are frequently injected through unconsolidated porous media in many fields such as soil remediation and reservoir engineering so modelling their flow through this type of media is of great economic importance however obtaining macroscopic laws to model non newtonian flow poses a considerable challenge given the dependence of the viscosity of the fluid on pore velocity for this reason no macroscopic equation is currently available to predict the relationship between injection flow rate and the pressure drop generated during the flow of a yield stress fluid without using any adjustable parameter in this work a method to extend darcy s equation to the flow of yield stress fluids through model unconsolidated porous media consisting of packs of spherical beads is presented then the method is experimentally validated through comparison with a total of 572 experimental measurements obtained during the flow of a concentrated aqueous polymer solution through different packs of glass spheres with uniform size an improved prediction of the pressure drop flow rate relationship is achieved by taking into account the non linear relationship between apparent shear rate and average pore velocity 1 introduction the flow of complex fluids in unconsolidated porous media is involved in many economically important industrial applications e g remediation of polluted soils gastone et al 2014 enhanced oil recovery eor wang et al 2017 rock fracturing roustaei et al 2016 and liquid food engineering welti chanes et al 2005 numerous complex fluids are shear thinning showing a decrease in shear viscosity as the applied shear rate is increased shear thinning fluids are extensively used in petroleum engineering and soil remediation to improve the microscopic sweep of the reservoir through stabilization of the injection front lake 1989 silva et al 2012 wever et al 2011 in some cases fluids with shear rate dependent viscosity additionally present a yield stress i e a threshold value in terms of shear stress below which they do not flow in the specific field of petroleum engineering the drilling fluids injected into rocks for the drilling of wells are often designed so as to have a yield stress in order to prevent cutting from settling when circulation stops lavrov 2013 coussot 2014 some examples of yield stress fluids used in oil industry include emulsions drilling muds polymeric gels such as carbopol hevy oils and foams talon et al 2014 furthermore a number of fracturing fluids used in hydraulic fracturing as gelling agents exhibit a yield stress designed to enhance proppant transport roustaei et al 2016 among them guar hydroxypropyl guar carboxymethyl hydroxypropyl guar hydroxyethyl cellulose and polyacrylamide are of particular relevance belyadi et al 2016 predicting the pressure drop of a yield stress fluid flowing through unconsolidated porous media is especially important given that a great number of petroleum reservoirs are located in unconsolidated formations peng et al 2007 pang and liu 2013 the majority of laboratory experiments in this field have been performed using beds of spherical beads which represent an idealization of unconsolidated porous media rao and chhabra 1993 tiu et al 1997 basu 2001 the newtonian case is generally well understood in this type of porous media and allows process design calculations with acceptable levels of accuracy the extensive literature regarding the flow of fluids with complex rheology through unconsolidated packed beds was critically reviewed by chhabra et al 2001 and sochi 2010 analysed the available models for describing non newtonian single phase flow in porous media more recently rodríguez de castro and radilla 2017a extended forchheimer s law and ergun s equation to the flow of fluids with shear rate dependent viscosity through packs of glass spheres with uniform size the latter authors determined the accuracy of the extended laws under creeping and inertial regimes from comparison with a full set of experiments nevertheless the yield stress effect was not addressed in the preceding works and obtaining a macroscopic law for the flow of yield stress fluids has proved to be a stumbling block talon and bauer 2013 performed lattice boltzmann simulations to solve 2d flow of bingham yield stress fluids in porous media and distinguished three different flow regimes these regimes corresponded to 1 the flow of a single pore 2 progressive pores opening and 3 flow of all pores furthermore chevalier et al 2013 focused on obtaining a generic relationship between flow rate and pressure drop applicable to the darcian flow of yield stress fluids through packed beds these authors proposed a macroscopic equation in which only the parameters of the rheological law of the injected fluid the diameter of the beads and two coefficients related to the internal structure of the porous medium were used as inputs then chevalier et al 2014 conducted nmr experiments which contributed to elucidate the structural parameters appearing in this generic law however the determination of these coefficients is still unclear and the proposed formula presents the inconvenient of assuming linear relationship between shear rate and darcy velocity in definition of the apparent shear rate in the porous medium moreover the extension of darcy s equation to the flow of yield stress fluids proposed by chevalier et al 2013 still has to be confirmed by further laboratory experiments given the serious lack of reliable experimental data in the literature lavrov 2013 coussot 2014 inspired by the broad interest of extending darcy s law to the flow of yield stress fluids through model unconsolidated porous media the objective of this work is to provide a straightforward procedure to predict the relationship between pressure gradient and flow rate the proposed method is also evaluated through comparison with experimental data to do so a series of flow experiments through four different packs of mono size spherical glass beads were carried out using concentrated aqueous solutions of xanthan biopolymer presenting a yield stress the effect of beads size on the accuracy of the predictions is then assessed and discussed in the area of non newtonian flow in porous media still very open it is crucial to base the interpretations and the modelling on solid observations the experimental details concerning the injection of xanthan gum solutions in different types of porous media have been carefully evaluated and discussed in the past rodríguez de castro 2014 rodríguez de castro et al 2014 2016 2018 rodríguez de castro and radilla 2017b in all these preceding works the same aqueous polymer solution was used and the experimental aspects were thoroughly addressed including a discussion on the rheological model the existence of a plateau viscosity the capability of the fluid to emulate yield stress behaviour and the interactions between fluid and porous medium polymer retention mechanical degradation and polymer adsorption for this reason it was decided in the present work to capitalize the knowledge acquired from the preceding research by using the same extensively investigated xanthan gum solution 2 predicting the flow of yield stress fluids in packed beds 2 1 previous attempts to extend darcy s law to the flow of yield stress fluids herschel bulkley empirical law herschel and bulkley 1926 is commonly used to describe the rheological behaviour under shear of a large group of time independent yield stress fluids this law can be written as follows 1 τ τ 0 a γ n for τ τ 0 γ 0 for τ τ 0 where τ is the shear stress experienced by the fluid at a given shear rate γ τ0 is the yield stress a is the consistency and n is the flow index of the fluid in the case of shear thinning yield stress fluids n is inferior to unity the three parameters are generally obtained by fitting the data obtained by measuring the shear rate γ as a function of the applied shear stress τ with a rheometer several attempts have been made to obtain a macroscopic law linking the injection flow rate to the resulting pressure drop during the flow of yield stress fluids in porous media pascal 1981 pascal 1983 al fariss and pinder 1987 chase and dachavijit 2005 coussot 2014 pascal modified darcy s law by introducing a threshold pressure gradient pt to account for the yield stress pascal 1981 p t φ τ 0 k pt is directly proportional to τ0 and inversely proportional to the square root of the absolute permeability k however pascal s relationship presents the serios drawback of including a dimensionless constant φ that must be empirically determined for each fluid medium pair also it only applies to the case n 1 indeed the existence of experimentally adjustable parameters with no clear physical meaning as inputs which impedes direct computational predictions is a major drawback of most available macroscopic flow expressions in this regard shahsavari and mckinley 2016 conducted numerical simulations providing analytical expressions for such parameters in the particular case of fibrous materials without including any specific dependence of these coefficients on the injection velocity only a few experimental works exist for the flow of yield stress fluids in porous media al fariss and pinder 1987 chase and dachavijit 2005 chevalier et al 2013 chevalier et al 2014 rodríguez de castro 2016 and the ranges of variation of the flow rate are usually narrow these experimental works showed that the relationship between the absolute value of the pressure gradient p and the absolute value of darcy velocity u is of the same form as the constitutive equation of the fluid i e p pt cun with c being a parameter that depends on the porous medium and the boundary conditions more recently chevalier et al 2013 presented a simple approach to extend darcy s law to the flow of yield stress fluids this general law contains a yielding term which may be simply expressed as a function of the yield stress of the material and the bead size 2 p χ τ 0 d s ω a u d s n d s with δp being the absolute value of the pressure drop through the packed bed of length l p δ p l the magnitude of the pressure gradient q the volume flow rate a the cross sectional area u q a the absolute value of the darcy velocity and ds the diameter of the spherical beads the latter authors initially stated that χ and ω in eq 2 should be universal factors for the flow through spherical beads the first coefficient is related to the path of maximum width throughout the porous medium while the second coefficient reflects the pore size distribution however on the basis of the results obtained by nmr measurements it was subsequently shown that χ and ω are two dimensionless coefficients depending only on the distribution of shear rate intensity and on the coefficient n which are in turn fluid dependent chevalier et al 2014 also u ds was considered to be the apparent shear rate for the flow through such a porous medium which is a serious flaw of eq 2 indeed the apparent shear rate was shown not be proportional to u in the case of yield stress fluids flowing at low and moderate flow rates rodríguez de castro and radilla 2017b the first yielding term on the right hand side of eq 2 corresponds to the critical pressure gradient below which no flow occurs the second term is velocity dependent and expresses the additional viscous pressure drop above the yielding pressure once the fluid is flowing chevalier et al 2013 experimentally determined the values of χ and ω obtaining χ 12 for a carbopol aqueous solution and χ 5 5 for a water in oil emulsion which did not permit to validate the universality of this coefficient in contrast these authors found that ω 85 for both types of fluids 2 2 new approach to extend darcy s law to the flow of yield stress fluids as mentioned above despite the method presented by chevalier et al 2013 being a valid approach the choice of ds as characteristic length in the definition of the apparent shear rate and the non dependence on injection velocity remain debatable also the values of χ and ω are not easily predictable for these reasons the objective of this subsection is to present a method to simply predict the u vs p relation by properly defining the actual shear rate and the shear viscosity of the fluid in the porous medium darcy s law darcy 1856 describes the single phase flow of incompressible newtonian fluids through porous media at low values of reynolds number 3 p μ k q a μ k u where μ is the shear viscosity of the injected fluid and k is the intrinsic permeability moreover kozeny carman equation allows to predict k from the porosity ε of the bed and the diameter of the beads using hydraulic radius theory 4 k ε 3 d s 2 36 κ 1 ε 2 κ being the kozeny carman constant the value of which is generally set to κ 5 in packs of spheres kaviany 1995 previous works have shown that some concentrated polymer solutions are yield stress fluids song et al 2006 carnali 1991 withcomb and macosko 1978 economides and nolte 2000 khodja 2008 benmouffok benbelkacem et al 2010 the steady state shear flow of these solutions can be well described by the herschel bulkley law eq 1 a practical approach to study the flow of complex fluids through a porous medium consists in defining an equivalent viscosity μ eq as being the quantity that must replace the viscosity in darcy s law to result in the same pressure drop actually measured tosco et al 2013 5 μ e q k p u in order to predict μ eq from the constitutive equation of the fluid an apparent shear rate in the porous medium γ p m has to be determined first assuming a bundle of capillaries model γ p m is usually taken as four times the average pore velocity 4 u ε divided by the average pore throat radius r characteristic length of the microscopic flow chauveteau and zaitoun 1981 chauveteau 1982 sheng 2011 r can be estimated from the permeability k and the porosity ε of the porous medium as proposed by kozeny 1927 using a bundle of capillaries model 6 r 8 k ε according to the preceding definition γ p m can be expressed as 7 γ p m 4 α u ε r α 2 u k ε where α is an empirical shift factor known to be a function of both the bulk rheology of the fluid and the tortuosity of the packed bed chauveteau 1982 sorbie et al 1989 lópez et al 2003 lópez 2004 comba et al 2011 therefore γ p m corresponds to the wall shear rate in a pore section of radius r this definition of apparent shear rate is in contrast with the one used by chevalier et al 2013 in which ds is taken as characteristic length instead of k ε 2 for the creeping flow of herschel bulkley fluids μ eq can be obtained from eqs 1 and 7 8 μ e q τ 0 k ε α 2 u a α 2 u k ε n 1 keeping in mind the objective to propose a prediction method analytical expressions for the calculation of α must be provided in order to obtain such expressions let us focus now on the determination of the wall shear rate in circular channels for the steady flow of an incompressible fluid through a circular channel of radius r the wall shear stress τ w is related to the pressure gradient p as follows 9 τ w p r 2 using eqs 3 8 9 can be written as 10 τ w τ 0 γ p m a γ p m n 1 u k r 2 1 2 2 ε k r α τ 0 2 n 3 2 k n 1 2 ε n 1 2 r α 1 n a u n for a constant viscosity incompressible fluid the wall shear rate γ w n e w t o n i a n is given by γ w n e w t o n i a n α n 4 u ε r where α n is the shift factor for the injection of a newtonian fluid α n is related to the tortuosity of the fluid flow through the packed bed and its value was shown to be 0 69 for spherical beads christopher and middleman 1965 shenoy 1994 however this value αn 0 69 has been contested by some authors james and mclaren 1975 chaveteau 1982 for this reason in this work α n will be considered first as unknown and will be determined through fitting to the experimental u vs p data then the obtained α n will be compared to the values previously reported in the literature the wall shear rate for the flow of liquids with a shear rate dependent viscosity can be calculated by using the weissenberg rabinowitsch mooney equation rabinowitsch 1929 mooney 1931 11 γ p m γ w n e w t o n i a n 3 2 d ln γ w n e w t o n i a n d ln τ w γ w n e w t o n i a n 3 2 d ln γ w n e w t o n i a n d u d u ln τ w u d u ln τ w α d α γ w n e w t o n i a n 3 2 d ln γ w n e w t o n i a n d u ln τ w u ln τ w α d α d u where α is a function of u weissenberg rabinowitsch mooney equation is commonly used to calculate the wall shear rate of complex fluids with non parabolic velocity profiles including yield stress fluids macosko 1994 steffe 1996 pipe et al 2008 sochi 2015 the following assumptions are used in the derivation of eq 11 incompressible fluid steady state laminar flow regime no wall slip no end effects unidirectional flow temperature is constant and properties are not a function of time or pressure steffe 1996 for a herschel bulkley fluid eq 11 becomes 12 γ p m α n r 4 u 3 ε 2 α τ 0 2 n 2 a α u k ε n 2 n 2 a n α α u k ε n 2 u τ 0 2 n 2 a n 1 α u k ε n α u from eq 12 it can be deduced that α becomes the constant value α α n 3 2 1 n for very high values of u by combining eqs 7 and 12 the following differential equation is obtained which allows the determination of α as a function of u 13 α α n r 2 2 k 3 ε 2 α τ 0 2 n 2 a α u k ε n 2 n 2 a n α α u k ε n 2 u τ 0 2 n 2 a n 1 α u k ε n α u for the simpler case of a power law fluid τ0 0 eq 14 leads to α α n 3 2 1 n which becomes α α n for a newtonian fluid therefore α is a constant parameter only if τ0 0 eq 13 can be numerically solved within a given range of u to obtain the relation between α and u then the obtained relation can be used in eq 8 to obtain μ eq once μ eq has been determined it can be entered in eq 3 leading to the extension of darcy s law eq 14 14 p μ e q k u c 1 α c 2 α n 1 u n with c 1 τ 0 ε 2 k and c 2 a k n 1 2 2 ε n 1 2 it is reminded that the value of α n is considered first as unknown and must be obtained by fitting eq 14 to the experimental u i p i data this is achieved by finding the value of α n that minimizes the sum e i 1 n p i p u i p i with n being the number of experimental data by comparing the method presented in this subsection with the works of chevalier et al 2013 it can be deduced from eqs 2 and 14 that 15 χ ε 1 2 d s 2 1 2 k 1 2 α moreover eq 4 can be used together with eq 15 to express χ as a function of only ε and α obtaining 16 χ 2 1 2 κ 1 2 3 1 ε ε α 2 1 2 5 1 2 3 1 ε ε α it can be concluded from the preceding equation that χ is a constant at high injection flow rates given that the value of α is also constant α α therefore the first yielding term on the right hand side of eq 2 can be considered a constant at high flow rates however this term depends on u at low and moderate values of u which was not taken into account in the work of chevalier et al 2013 also the following relationship can be obtained from comparison between eqs 2 and 14 17 ω 2 n 1 2 ε n 1 2 k n 1 2 α n 1 d s n 1 analogously eq 4 can be used together with eq 17 to express ω as a function of only ε and α obtaining 18 ω 2 3 n 1 2 3 n 1 κ n 1 2 1 ε n 1 ε 2 n 1 α n 1 2 3 n 1 2 3 n 1 5 n 1 2 1 ε n 1 ε 2 n 1 α n 1 eq 18 shows that ω is also contant at high flow rates while being a function of u at moderate and low flow rates moreover ω is not a function of ds but depends on the fluid properties through α this is contrast to the claim of chevalier et al 2013 according to which χ and ω are universal factors for a porous medium composed of an assembly of spheres it should be kept in mind that elongational flows during the injection of solutions of polymers presenting a certain degree of flexibility through porous media are known to induce extra pressure losses with respect to pure shear flow rodríguez et al 1993 müller and sáez 1999 nguyen and kausch 1999 seright et al 2011 amundarain et al 2009 this is a result of the formation of transient entanglements of polymer molecules due to the action of the extensional component of the flow in the present work we first hypothesize that the deviation of the experimentally measured pressure drop with respect to the viscous pressure drop are negligible this hypothesis is then validated through analysis of the experimental results 3 experimental methods and materials experimental p vs u measurements were performed by injecting a xanthan gum aqueous solution yield stress fluid through four packs of spherical glass beads flow experiments with filtered water newtonian fluid were also performed by following the procedure presented by rodríguez de castro and radilla 2017a in order to determine the permeability of the packed beds the glass beads were first placed into transparent acrylic glass cylinders and then compactly packed by means of vibration with a sieve shaker the inner diameter of the acrylic glass cylinders was d 5 cm and the diameter of the glass spheres used in each of the four columns was uniform with ds 1 mm 3 mm 4 mm and 5 mm in each case the length of the column was l 20 cm two different configurations were used depending on the involved flow rates for 0 12 l h q 6 l h the injection circuit was open and the fluid was injected through the packed beds at the selected flow rate using a dual piston pump prep digital hpcl pump a i t france for 9 l h q 250 l h the fluid was injected through a closed circuit using a volumetric pump as performed by rodríguez de castro and radilla 2017a a photo showing the experimental setup is provided as supplementary material fig s1 details of the experimental setup and procedure including the working ranges of the instruments and the measurement uncertainties were provided by rodríguez de castro and radilla 2017a the ranges of u imposed during the experiments with each packed bed are listed in table 1 xanthan biopolymer is a microbial high molecular weight exo polysaccharide produced by fermentation of x campestris bacteria garcia ochoa et al 2000 palaniraj and javarman 2011 kumar et al 2018 in solution state an isolated macromolecule of this polymer is more or less rigid and with a typical contour length of 1 µm mongruel and cloitre 2003 and a transverse size of approximately 2 nm the stiffness of xanthan macromolecules leads to high levels of shear viscosity and highly shear thinning behaviour of semidilute solutions in water for this reason the shear rheology of xanthan gum solutions is well described by the herschel bulkley model eq 1 under steady state conditions garcía ochoa and casas 1994 song et al 2006 rodríguez de castro et al 2014 2016 2018 rodríguez de castro and radilla 2017b however rigorously speaking they should be referred to as pseudo yield stress fluids the capacity of xanthan gum solutions to emulate the shear rheology of a yield stress fluid and the effects of polymer concentration was experimentally assessed by rodríguez de castro et al 2018 concluding that concentrated solutions 7000 ppm behave similarly to a yield stress fluid due to high viscosity values at low shear rates sixty litres of aqueous solution were prepared with xanthan gum concentration cp 7000 ppm and the rheogram was obtained following the procedure presented by rodríguez de castro and radilla 2017b eq 1 was then used to fit the rheogram rodríguez de castro et al 2014 giving τ0 7 4 pa a 0 37 pa s n and n 0 52 the rheogram of the solution and the herschel bulkley fit are provided in fig 1 the dynamic viscosity of water solvent was measured to be 0 0011 pa s and the densities ρ of both the water and the xanthan gum solution were taken as 1000 kg m3 moreover the rheograms of several effluent fluid samples were characterized and compared to that of the inflowing fluid at the highest injection flow rates in order to assess polymer degradation and retention on the pore walls no significant difference was observed between the rheograms proving that polymer degradation and polymer retention can be neglected despite the used glass beads being quite coarse as compared to most natural granular media the explored sizes fall within the range of grain sizes reported for coarse sand and fine gravel which are widely investigated in hydrologic applications morris and johnson 1967 moreover these beads sizes are commonly used in previous research e g dukhan et al 2014 so this choice facilitates comparison with literature data furthermore the use of smaller beads may result in polymer retention which was not observed in the present experiments 4 results the flow experiments were conducted for both fluids water and yield stress fluid and were repeated four times the number of repetitions for yield stress fluid injection through each packed bed corresponds to 4 n ranging from 108 to 176 as listed in table 1 the 4 n measures for each packed bed were considered to be an experimental set a total of 572 measurements were performed during the flow experiments with the yield stress fluid 4 1 experimental determination of ε and k the weight of each packed bed was measured before and after saturation with water in order to determine ε from the difference in mass also the procedure followed by rodríguez de castro and radilla 2017a to determine k from injection experiments with water was applied to the present measurements the obtained values and for ε and k are listed in table 2 together with the associated uncertainties 95 confidence interval 4 2 shear viscosity of the yield stress fluid in the porous media eq 13 was numerically solved within the involved range of u for both all the investigated packed beds using an implicit runge kutta method the resulting α versus u functions are represented in fig 2 and the results obtained for α n are listed in table 3 as a function of ds it is noted that the value of α n was close to 0 68 average value in all the tested porous media for the polymer solution used in the present work this is in very good agreement with the results of christopher and middleman 1965 who obtained α n 0 69 as mentioned in section 2 2 α becomes the constant value α α n 3 2 1 n for very high values of u i e when u τ 0 1 n k 1 2 ε 1 2 2 1 2 a 1 n α in the case of herschel bulkley shear thinning fluids 0 n 1 in fig 2 it can be observed that α monotonically decreases as u increases so the condition u τ 0 1 n k 1 2 ε 1 2 2 1 2 a 1 n α will be satisfied if u u τ 0 1 n k 1 2 ε 1 2 2 1 2 a 1 n α consequently the boundary condition α u 105 u α was used to numerically solve eq 13 the obtained α values are also listed in table 3 and are all close to 0 88 average value regarding the sensitivity of α to the microstructure of the packed bed it can be deduced from fig 2 that higher values of ds coarser microstructure result in higher values of α the value of γ p m corresponding to each darcy velocity u was calculated with eq 7 following two different approaches first a constant value of α named α was determined for each porous medium by calculating the shift factor in terms of shear rate which led to the best superposition between the in situ μ eq vs γ p m data and the bulk rheological law eq 1 the obtained values for α are shown in table 3 the second approach consisted in using the α u function obtained from eq 13 the results of both approaches are presented in fig 3 together with the bulk rheological law eq 1 in this figure it can be observed that μ eq is close to eq 1 at high values of u for both the constant α and the variable α methods however this is not the case at low and moderate values of u for which μ eq approaches better eq 1 with the variable α method also μ eq is expected to be greater than the bulk viscosity at high values of u in the presence of important inertial effects tosco et al 2013 rodríguez de castro and radilla 2016 the fact that no important deviation of μ eq with respect to μ pm is observed in the present experiments reflects that inertial effects are not significant moreover fig 3 shows that the shear rates involved in the flow through all porous media are within the same range as those measured with the rheometer during characterization of the fluid s shear viscosity 4 3 previous attempts to extend darcy s law to the flow of yield stress fluids the values of χ and ω were determined by fitting the experimental results presented in this work to eq 2 through minimization of the sum of the absolute values of the differences between fit and experimental data the obtained values are listed in table 4 showing that χ and ω are porous medium dependent as experimentally determined also it is remarked that the values of these coefficients may depend on the range of imposed u as they are obtained through fitting to experimental data this dependence on u is taken into account by the new method proposed in the present work as explained in section 2 2 the results of fitting eq 2 to the experimental data are shown in fig 4 moreover the average errors of these fits are presented in table 5 for different ranges of u it is observed that the resulting fits are accurate within a large range of u however a major drawback of this method is that χ and ω need to be experimentally determined which impedes prediction of the u vs p relation it is worth mentioning that the errors obtained by using χ 5 5 and ω 85 as proposed in the work of chevalier et al 2013 are too big in the case of the present experiments and lead to very inaccurate predictions 4 4 experimental validation of the new prediction method eq 14 was used to predict the relation between p and u for the injection of the 7000 ppm solution through the four packed beds the obtained predictions are presented in fig 5 together with the experimental results of measurements performed in the present work in this figure the errors bars correspond to a 95 confidence interval as explained in section 3 from these results the accuracy of the proposed methods for the prediction of p as a function of u during the flow of yield stress fluids through packed beds of spherical beads can be assessed fig 5 shows that the variable α approach provides more accurate predictions within the low and moderate u regions which is in agreement with the arguments presented above however a less important difference is obtained between both methods for the highest values of u the average errors obtained with the variable α method and the fixed α method for different ranges of u are summarized in table 5 it is observed that the variable α method successfully predicts the p u relationship for the flow of the yield stress fluid through the four packed beds even though the obtained predictions are slightly less accurate in the case of ds 1 mm the overestimation of p reported in figs 4 and 5 for the lowest flow rates may be related to the longer times needed to achieve stationary measurements of pressure drop within this region this effect is similar as the one reported for rheological measurements at low shear rates as shown in fig 1 this is a consequence of the viscosity of the fluid continuously increasing over time as illustrated in fig s2 this effect will be discussed in section 5 as mentioned above α becomes the constant value α α n 3 2 1 n when u u τ 0 1 n k 1 2 ε 1 2 2 1 2 a 1 n α this means that eq 14 presents a constant yielding term of value c 1 α and a constant consistency term of value c 2α n 1 for u u in other words eq 14 has the same form as herschel bulkley empirical law eq 1 only if the preceding condition is met therefore the threshold reynolds number re above which the extended darcy s law for herschel bulkley eq 14 fluids has the same form as herschel bulkley equation is given by 19 r e ρ u k μ where μ is the shear viscosity of the fluid in the porous media at u re is represented as a function of k for the four packed beds in fig 6 showing linear relationship it is worth mentioning that in spite of the negligible influence of inertial effects on the pressure drop vs flow rate relationships in the case of the highly viscous xanthan gum solutions used in this work the procedure presented in section 2 2 is also valid to extend forchheimer equation forchheimer 1901 to the case of yield stress fluids this is explained by the fact that the inertial coefficient appearing in forchheimer equation does not depend on the shear rheology of the injected fluid as numerically firdaouss et al 1997 yadzchi and luding 2012 tosco et al 2013 and experimentally rodríguez de castro and radilla 2016 2017a 2017b proved in previous works however it must be noted that even for newtonian fluids the macroscopic transport equations governing inertial regime are still under debate in the literature in particular it was demonstrated that whereas forchheimer regime is always well identified for inertial flow in disordered porous media its appearance in ordered media is strongly dependent on the microstructure and the orientation of the pressure gradient lasseux et al 2011 agnaou et al 2017 therefore a non linear dependence of apparent viscosity on pore scale velocity is expected to increase the complexity of the problem 5 discussion the values of α n obtained for all the tested porous media were always very close to 0 69 which was the value theoretically predicted by christopher and middleman 1965 it should be noted that although α n 0 69 is valid for the present experiments this value must still be confirmed by further experiments in different yield stress fluid packed beds combinations before declaring that it is a universal constant nevertheless it can be firmly stated that the results reported in this work are a highly promising step in this direction it can be deduced from eq 14 that α can be considered a constant value α α n 3 2 1 n in the high flow rates region i e when u u τ 0 1 n k 1 2 ε 1 2 2 1 2 a 1 n α and eq 14 can be written as 20 p p 0 c u n with p 0 3 τ 0 ε 2 k α n 2 1 n and c a k n 1 2 2 ε n 1 2 α n 3 2 1 n n 1 this is in agreement with the results of talon et al 2014 who stated that u scales linearly as p p 0 in the case of a bingham fluid n 1 flowing at high u through a one dimensional channel also nash and rees 2017 showed that the manner in which flow begins once the threshold pressure gradient is exceeded strongly depends on the channel size distribution of the porous media the same authors talon et al 2014 nash and rees 2017 proved that p 0 is higher than the actual threshold pressure which is consistent with our results given that α increases as u tends to zero fig 2 a model to accurately predict the flow of yield stress and carreau fluids through rough walled fractures by using α n 1 was presented and experimentally validated in a previous work rodríguez de castro and radilla 2017b it is reminded here that α n is a tortuosity related factor so media with different tortuosity may lead to different values of α n indeed the effective average pore throat radius r e f f which takes into account the tortuosity of the medium can be defined as r e f f 8 k t ε r t r α n with α n 1 t and t being the tortuosity factor christopher and middleman 1965 chaveteau 1982 given that the tortuosity of the flow paths in a packed bed is higher than in a fracture a lower value of α n is expected for packed beds in the case porous media with more complex pore size distributions the flow is highly conditioned by the narrowest flow paths at low flow rates and the representative pore section should be smaller than r in this regard the full set of equations presented in section 2 2 should be reconsidered as the current method is not able to capture the influence of pore size distribution nevertheless the use of the present method with more complex porous media should still be useful to predict the relationships between p and u with higher accuracy than the existing methods which use a constant viscosity value the existence of yield stress was challenged by barnes and walters 1985 and has been discussed for more than 30 years as explained by møller et al 2009 the supporters of the existence of yield stress commonly argue that the viscosity increases very sharply in some materials as the stress decreases towards the yield stress however other researchers claim that only a finite and constant viscosity newtonian plateau of viscosity is observed below a certain stress in particular barnes and walters 1985 used stress controlled rheometers to show that at low enough shear rates viscosity reaches a newtonian plateau for carbopol and other fluids which had traditionally been considered to have a yield stress they argued that any material flows providing enough observation time and sufficiently sensitive measuring equipment in stark contrast with barnes and walters 1985 and møller et al 2009 experimentally showed that such newtonian plateau is the consequence of non steady state measurements they demonstrated that for stresses below the yield stress viscosity is a priori unbounded and increases continuously though slowly if enough time is allowed they effectively observed an increase in viscosity even after 100 s in other words they found that viscosity is time dependent and tends to infinity below the yield stress in the case of the present xanthan gum solutions the evolution of viscosity over time was measured for 1000 s under a shear stress of 0 5 pa below the yield stress using a rheometer equipped with cone plate geometry the results are provided as supplementary material fig s2 showing that viscosity does not attain a constant value and continues to increase after that time one may wonder whether the proposed procedure is simpler than performing a numerical solution to the actual flow equations without invoking a bundle of capillaries approximation in this sense it should be highlighted that performing a numerical solution to the actual flow equations would imply using the size distribution of the flow paths as an input for the model which is rarely available in real applications it is reminded that the objective of this work is to present a simple method to predict the pressure drop for the flow of yield stress fluids through packed beds therefore using hardly accessible inputs as needed to perform a numerical solution to the actual flow equations is not a valid approach it is noted that in our experiments with yield stress fluids the total pressure drop through the porous media was successfully predicted from the value of k obtained from water injection without any significant deviation therefore similarly to the case of previous flow experiments with shear thinning fluids without yield stress rodríguez de castro and radilla 2017a no appreciabe effect of elongational viscosity has been observed in the present work also wall effect issue during shear thinning creeping flow in packed beds was previously addressed in the literature on this subject rao and chhabra 1993 studied the effects of column walls and particle size distribution on the flow rate pressure drop relationship proposing a wall correction method and confirming the applicability of the mean hydraulic radius of the particles to characterize a bed of mixed size spheres the latter authors showed that wall effect is less significant in the case of shear thinning fluids than in the newtonian case in the present experiments the porosity of all packed beads is 0 35 0 01 and the experimentally measured permeability is very close to kozeny carman prediction for the largest beads 3 6 difference therefore there is no evidence of significant wall effect affecting pressure drop vs flow rate relationship 6 summary and conclusions a simple approach to extend darcy s law to the flow of yield stress fluids through packed beds has been presented in this work this method takes into account the non proportional relationship between the apparent shear rate in the porous medium γ p m and average pore velocity u only the porosity ε and the permeability k of the porous medium exclusively for high flow rates are used as inputs of the method together with the herschel bulkley parameters of the fluid τ0 a n the following procedure to predict p as a function of u is proposed 1 determine the shear rheology parameters of the fluid using a rheometer τ0 a n 2 measure the porosity ε of the packed beds e g from difference in mass before and after saturation with water note that the usual values are close to ε 35 3 measure k from newtonian flow experiments alternatively k can be estimated from kozeny carman equation eq 4 or determined by other techniques e g x ray tomography however the cited methods provide k estimates with very different accuracy which can be roughly estimated to 5 for experimental assessment 10 for kozeny carman and 20 for tomography 4 calculate the values of α u 4 1 when low and moderate values of u are involved solve the following differential equation eq 13 to obtain α u α α n r 2 2 k 3 ε 2 α τ 0 2 n 2 a α u k ε n 2 n 2 a n α α u k ε n 2 u τ 0 2 n 2 a n 1 α u k ε n α u a value of α n 0 68 is proposed based on the results of the present experiments and previous theoretical works when only high values of u are involved u u τ 0 1 n k 1 2 ε 1 2 2 1 2 a 1 n α use a constant value α α α n 3 2 1 n 4 2 use eq 8 to compute μ eq as a function of u ε k τ0 a and n μ e q τ 0 k ε α 2 u a α 2 u k ε n 1 5 use eq 14 to calculate p as a function of u p μ e q k u c 1 α c 2 α n 1 u n with c 1 τ 0 ε 2 k and c 2 a k n 1 2 2 ε n 1 2 flow experiments of yield stress fluids covering a wide range of u were performed in order to assess the accuracy of the predictions obtained using the proposed method showing good agreement between model and experiments and negligible inertial effects within the explored range of u consequently darcy s law provides accurate u p predictions in contrast to the case of less concentrated solutions with no yield stress in which inertial effects were significant rodríguez de castro and radilla 2017a as an important industrial application the extended darcy s law can be included in computational studies of large scale non newtonian flow in unconsolidated porous media the conclusions of this work have now to be assessed using real granular media also future numerical studies should be performed in order to provide deeper insight into the physical mechanisms governing the non proportional relationship between γ p m and u acknowledgments the author would like to acknowledge pr giovanni radilla for providing the equipment used in the experimental part of this work the author also wishes to thank frédéric bastien for his technical support throughout the experimental campaign supplementary materials supplementary material associated with this article can be found in the online version at doi 10 1016 j advwatres 2019 01 012 appendix supplementary materials supporting figures are included as three figures in the online resource 1 file image application 1 
686,stochastic modeling is a common practice for modeling uncertainty in hydrogeology in stochastic modeling aquifer properties are characterized by their probability density functions pdfs the bayesian approach for inverse modeling is often used to assimilate information from field measurements collected at a site into properties posterior pdfs this necessitates the definition of a prior pdf characterizing the knowledge of hydrological properties before undertaking any investigation at the site and usually coming from previous studies at similar sites in this paper we introduce a bayesian hierarchical algorithm capable of assimilating various information like point measurements bounds and moments into a single informative pdf that we call ex situ prior this informative pdf summarizes the ex situ information available about a hydrogeological parameter at a site of interest which can then be used as a prior pdf in future studies at the site we demonstrate the behavior of the algorithm on several synthetic case studies compare it to other methods described in the literature and illustrate the approach by applying it to a public open access hydrogeological dataset keywords data assimilation data fusion bayesian hierarchical model informative prior databases 1 introduction 1 1 context of study modeling hydrogeological processes like groundwater flow and transport is a challenging task due to the general lack of information about subsurface conditions this lack of information is caused by spatial variability and general data scarcity this situation where modelers have to work with largely incomplete information has led to the increasing adoption of statistical methods able to account for resulting uncertainties rubin 2003 bayesian inference provides a framework to deal with these intrinsic uncertainties in this paper we conceptualize the uncertain hydrogeological parameters y as random variables y which are fully defined by their probability density function pdf f y y the goal of the inference is thus to condition this pdf on the available data z that are related to y i e deriving f y y z this is done using bayes theorem hence the name of this framework one salient feature of this framework is that the data z represent only one part of the inference with the other part being the prior pdf f y y bayesian reasoning demands that this prior pdf represents the knowledge about the target parameter y before i e prior to accounting for the case specific data whereas the posterior pdf f y y z represents the knowledge after accounting for such data rubin 2003 within the context of hydrogeological site characterization this means that the prior pdf should be derived without accounting for data z coming from the site under investigation prior pdfs are consequently key for any bayesian inference yet deriving a prior is not a straightforward process and only a few guidelines exist to help modelers in this task in general scales and tenorio 2001 and in hydrogeological studies in particular in fact the bayesian point of view is sometimes criticized precisely because modelers need to define a prior a step considered to be subjective and arbitrary as it depends on the modeler kass and wasserman 1996 ulrych et al 2001 however it is important to note that subjectivity arises necessarily in any modeling e g in the choice of the likelihood function or the objective function when performing deterministic calibrations e g over et al 2015 bayesian modelers actually see the definition of priors as an opportunity to incorporate previous experience working with variables y into the inverse estimation process in ways that are both transparent and theoretically sound such experience can come in many different forms next to direct point measurements of the target variable experience is shaped by estimates of covariates summary statistics or soft expert knowledge in the form of bounds or percentiles in this study we focus on informative priors which by definition provide a non trivial amount of information about the target variables before the assimilation of measurements taken at the target site the derivation of informative priors becomes possible by the fact that databases that gather parameter values are becoming increasingly common and accessible paraphrasing vereecken et al 2014 p 91 we are now entering an age of big data in environmental modeling with datasets containing an unprecedented quantity of information to the authors knowledge no algorithm has been presented to date that can assimilate such detailed information in the form of prior distributions for hydrogeological properties here we propose an algorithm that can take advantage of the constantly increasing amount of data available for the purpose of better informed environmental modeling and decision making the use of informative priors has several advantages first the state of uncertainty represented in an informative prior is a measure of the amount of background knowledge available for a particular field it can be seen as a benchmark for a field sampling campaign permitting the comparison of the expected information content in the measurements as expressed in the likelihood function to the information content of the background knowledge as expressed in the prior pdf geiges et al 2015 hou and rubin 2005 nowak et al 2010 2012 tang et al 2016 furthermore the information contained in informative priors may be crucial in situations where the information content in the measurements is insufficient to derive posterior pdfs with the desired level of certainty reichert 1997 this is often the case in hydrological studies with high levels of data scarcity to help identify those situations where the use of an informative prior might assist with parameter identification tang et al 2016 used the kullback leibler divergence kld to quantify the impact of prior and likelihood distributions on the posterior they concluded that their approach could be used to inform the choice of a meaningful prior where parameters that are insensitive to measurements should be given a well defined prior while sensitive parameters could have a prior defined more vaguely other situations where informative priors are helpful include ill posed problems and cases of non identifiability where different sets of parameters lead to similar goodness of fit results this issue is prevalent with inverse problems in hydrogeology carrera and neuman 1986 zhou et al 2014 in bayesian inverse modeling the problem of identifiability does not exist as equally representative parameter sets simply result in equal likelihood values reichert 1997 however there is clearly an obstacle when the goal is to estimate parameter values with a desired level of certainty by reducing the range over which parameters can vary the use of narrow prior distributions can better inform the inverse modeling process and help identify a unique parameter set from the derived posterior scharnagl et al 2011 zhou et al 2014 in sum informative priors allow modelers to incorporate previous experience gained from working with variables of interest and can improve the precision of derived posterior distributions to that end it is critical to be able to formulate an appropriate informative prior pdf for a site of interest using information from previous studies 1 2 site similarity and levels of variability the construction of an informative prior for hydrogeological studies relies on the careful selection of data used in the assimilation obtained from previous work at field sites similar to the target site rubin 2003 however the notion of site similarity is not straightforward and it depends on the processes and variables under investigation site similarity is established when the target site and a previously investigated field site share similar characteristics with respect to the target variable when comparing sites variability in target variable estimates can relate to two different aspects physical and epistemic li et al 2018 the physical aspect refers to the state of the physical system under consideration in hydrogeology physical aspects include elements such as the geological environment e g silt clay and sand for soils sedimentary metamorphic and igneous for rocks another example of a hydrogeological physical aspect is the depth below the ground surface as hydraulic conductivity generally decreases with depth e g li et al 2018 the epistemic aspect refers to how the observer interacts with the physical system to generate information this aspect covers the investigation strategy and in particular the method used to derive estimates a variable can be directly estimated using an in situ measurement technique or it can be indirectly estimated using inverse modeling in the first case variability can be introduced by the choice of a measurement technique e g core extraction or different types of in situ pump tests in the second case variability can be introduced by modeling choices e g the forward model can assume radial flow uniform in the average mean flow or 2d or 3d heterogeneity different choices for investigation strategies can lead to varying estimates for a unique physical medium another important epistemic aspect relates to scale dependence i e the dependence of the conductivity on the measurement scale indelman and dagan 1993 scale dependence should always be recognized this will be further discussed in section 4 site similarity should be established on the basis of the physical characteristics epistemic characteristics create additional variability in the derived values in this study we introduce an algorithm that accounts for epistemic variability by proposing a hierarchical approach to hydrogeological data assimilation 1 3 objective of study given these considerations we formulate a list of criteria c summarizing the elements to be considered for establishing ex situ priors c 1 ex situ the prior pdf should result from the assimilation of ex situ data i e coming from field sites different than the target site c 2 inter site versus intra site variability the assimilation algorithm should distinguish between measurements coming from the same site and those coming from different sites by accounting for additional variability when the measurements come from different sites c 3 data variability the assimilation algorithm should be able to handle different types of information e g direct measurements covariates moments bounds etc in this study we introduce an algorithm that permits the assimilation of data gathered from similar sites into an ex situ prior pdf honoring c therefore our objective pertains to summarizing and integrating the findings of previous studies in a particular area of research e g carlin 1992 here our analysis involves summarizing data from several independent field sites and integrating that data into an informed pdf for a target variable using the bayesian hierarchical modeling framework the algorithm assimilates the data from previous studies with a unified approach into a pdf honoring the criteria c which we call ex situ prior 2 methods 2 1 bayesian inference and the role of the prior distribution when performing a hydrogeological investigation at a particular site in situ measurements z are used to estimate the subsurface properties y in a stochastic framework the parameters of the model y are considered to be the realizations of the random variable y characterized by a probability density function pdf bayesian inference uses the in situ information contained in z to improve the statistical characterization of y at the site under investigation the basis of all bayesian inference is bayes theorem which can be stated as follows 1 f y y z f z z y f z z f y y here z represents the in situ data used for the inference additionally y represents the vector of physical parameters at the site under investigation and is described by the random variable y in this context f y y is the prior describing our belief in y before accounting for the in situ data z and f y y z is the posterior describing our belief in y after accounting for the in situ data z the term connecting both is the normalized likelihood consisting of the likelihood fz z y and the normalizing constant fz z for the sake of clarity we distinguish between the parameters in the physical model y and the parameters in the statistical data assimilation model henceforth referring to y as the target variable a prior distribution can be informative or non informative depending on its influence on the posterior distribution a prior is considered non informative when its influence on the posterior distribution is minimal such that the inference is unaffected by information external to z gelman et al 2014 using non informative priors implies effectively starting in a state of ignorance i e no prior knowledge on the question exists or can reasonably be used conversely using informative priors means that we are continuing the inference i e building upon prior results considered to be relevant to the current question since our study focuses on informative priors the prior in eq 1 contains relevant background information derived from available ex situ data d to reflect this notion we recast eq 1 as 2 f y y d z f z z y f z z f y y d f z z y f z z f d d y f d d f y y with f y y being the non informative prior and f y y d being the informative prior in eq 2 z refers to in situ data and d refers to ex situ data fig 1 it is important to clarify that even though f y y d results from data assimilation we still refer to it as a prior distribution since it represents what is known about the target variable y before accounting for the in situ data z the goal of the data assimilation algorithm introduced in this study is therefore the derivation of this ex situ prior distribution f y y d 2 2 existing methods for formulating prior distributions in hydrogeology previous studies have adopted different strategies when defining prior pdfs for hydrogeological properties in this section we summarize these strategies and show that none can account for elements listed in c demonstrating the need for a new data assimilation framework non informative priors represent the highest level of parameter uncertainty and are chosen when minimal prior knowledge is present they are the most common choice for researchers performing hydrogeological studies e g arnold et al 2013 de barros et al 2012 engeland and gottschalk 2002 houska et al 2014 here we review how previous studies defined informative priors and explore the extent to which they followed criteria c one common approach to the definition of informative priors in hydrogeology is the principle of maximum relative entropy mre mre states that when several distributions are acceptable the one that should be selected is the one with minimal information content i e the one with maximum entropy jaynes 1982 woodbury and ulrych 1993 pioneered the application of mre in hydrogeology they derived the statistical distributions of parameters in the transport equations using expected values lower and upper bounds that were based on three previous studies another example of an mre application was demonstrated by woodbury and rubin 2000 who defined a prior pdf for transport model parameters based on expected value upper and lower bounds obtained from a previous study hou and rubin 2005 later defined prior pdfs for hydraulic parameters based on lower bound upper bound mean and variance of the values pooled from the rosetta database although these studies showed how mre concepts permit the assimilation of different types of information e g moments bounds in a systematic and objective framework a frequently encountered limitation was the fact that mre does not distinguish between inter site and intra site variability c 2 rather it pools all information from previous studies at one single level an alternative approach for the formulation of informative prior distributions is the use of statistical distributions fitted to values in large databases a commonly cited example here is carsel et al 1988 who defined statistical distributions for bulk density field capacity wilting point and organic matter by gathering data from 2942 material series provided by the u s soil conservation service they sorted the data into four material classes and then used the johnson transformation to transform the empirical distribution of their parameters into normally distributed distributions for each material class carsel and parrish 1988 adopted a similar approach for unsaturated zone hydraulic parameters their work has received considerable attention and the statistical distributions they derived have served as informative priors in subsequent studies including wang et al 2003 and over et al 2015 the limitations of this approach however are that it does not distinguish between inter site and intra site variability c 2 and it does not allow for the assimilation of multiple types of information e g bounds moments c 3 in conclusion to the authors knowledge no data assimilation method has been introduced in hydrogeological studies that can account for the multiple levels of data variability c 2 and allows for the assimilation of different types of information c 3 2 3 types of data used in the assimilation ex situ information about a hydrogeological property can be classified into different categories when investigating the spatial distribution of a variable at one site journel 1986 distinguished between three broad categories for classifying the available data hard data i e numerical values hard inequality type data where the value is known to lie between given bounds and soft qualitative information e g smoothness of spatially distributed data here we modify these categories as follows 1 hard inequality type data this is data where the variable is known to physically lie within a bounded interval e g hydraulic conductivity is strictly positive and porosity is bounded between 0 and 1 this condition holds for all investigated sites 2 hard site specific data this is site specific data available in the form of a list of measurements of the variable a histogram of measured values a pdf or a cumulative distribution function cdf that is derived from measured values at a similar site different from the site under investigation 3 soft site specific data this refers to data that is not available in terms of measurements for example it can refer to a site specific range of values site specific statistical moments or site specific qualitative information obtained from expert judgment we detail how the data assimilation algorithm accounts for physical and site specific information in sections 2 4 2 and 2 4 3 respectively 2 4 data assimilation framework in this section we introduce the data assimilation framework for constructing the ex situ prior first using a simple example section 2 4 1 illustrates the application of bayesian hierarchical modeling to derive the ex situ prior then sections 2 4 2 and 2 4 3 generalize bayesian hierarchical modeling as a statistical framework for assimilating different types of data lastly section 2 4 4 presents the numerical implementation of the algorithm 2 4 1 illustration on a simple synthetic example let us start by illustrating the construction of the ex situ prior using a small synthetic data set the goal is to derive the ex situ prior for a target site s 0 using data related to porosity from three sites s 1 s 2 and s 3 fig 2 here porosity is the target variable y at site s 1 a single data point of y 1 is available with value y 1 1 0 2 first column from the left in fig 2 at site s 2 three data points of y 2 are available with values y 2 1 0 3 y 2 2 0 4 and y 2 3 0 2 second column from the left in fig 2 at site s 3 again three data points of y 3 are available with values y 3 1 0 3 y 3 2 0 2 and y 3 3 0 2 third column from the left in fig 2 together these seven data points are the ex situ data d in our example as can be seen from the values given above all data are numerical in nature e g direct point measurements of y furthermore it should be clear that the statistical distributions of y are bounded by 0 and 1 since y relates to porosity the ex situ prior f y y d can now be defined using an instance from the class of hierarchical models namely the basic normal hierarchical model gelman 2006b this model is illustrated in fig 2 and is described by the following equations 3a y i n 0 1 μ i σ 2 3b μ i n 0 1 α τ 2 3c f α τ σ f α f τ f σ the first level in this hierarchy models the intra site variability eq 3a it describes the statistical distribution of the target variable within each site at each site si data are viewed as realizations from a site specific distribution describing the random variable yi in this example we assume that at each site si measurements are independently drawn from the truncated normal distribution characterized by the site specific parameters μi and σ denoting the intra site mean and standard deviation respectively an underlying assumption is that the numerical values y i provided for site si are independent conditionally on μi and σ we discuss the implication of this assumption in more detail in section 2 4 2 and explain how it can be modified to account for spatial autocorrelation the second level in the hierarchy models the inter site variability eq 3b this example follows the basic normal hierarchical model for which site specific means μi are drawn from a normal distribution with parameters α and τ 2 and site specific variance σ 2 is defined by a prior pdf common for all sites polson and scott 2012 these parameters can be combined into a set η α τ σ termed hyperparameters to infer their distribution we need initial prior distributions for which we use non informative distributions a flat prior is used for α p α 1 for the level 1 variance parameter σ 2 we choose jeffrey s prior p σ 2 σ 2 since it obeys the invariance principle jeffreys 1946 although it is an improper prior it leads to posteriors with acceptable properties polson and scott 2012 however jeffrey s prior is not acceptable for the level 2 variance parameter τ 2 because it leads to an improper posterior with the normal hierarchical model gelman 2002 for τ 2 polson and scott 2012 have recommended using the half cauchy prior as further detailed in sections 2 4 2 and 2 4 3 the two main steps of the ex situ prior derivation f y y d are as follows 1 data assimilation i e deriving the updated distribution of hyperparameters η f η η d fig 3 2 prediction i e deriving the ex situ prior f y y d fig 4 we next examine how these steps are performed in our simple synthetic example in step 1 we estimate the parameters in the statistical model η from the data d i e deriving the updated distribution of hyperparameters η f η η d fig 3 shows the marginal posterior pdfs of the hyperparameters η for this example the derivation and numerical implementation are presented in subsequent sections the initial priors are non informative and the posterior statistical distributions of these hyperparameters are determined by the information contained in the measurements this example shows how measurements d contributes to the better characterization of hyperparameters η in this first step the bayesian hierarchical model assimilates the measurements d by updating the statistical distribution of parameters η fig 3 in step 2 we derive the ex situ prior f y y d based on the updated distribution of the hyperparameters η fig 4 the predicted values for variable y at the target site peaks at approximately 0 25 with little probability outside the 0 0 5 range in line with the measurements d assimilated by the algorithm 2 4 2 bayesian hierarchical modeling having illustrated the prior derivation by virtue of a simple synthetic example section 2 4 1 we now look more generally at the data assimilation framework used here and in particular at bayesian hierarchical modeling in statistics a hierarchical model is used when a model with multiple parameters can be formulated such that certain parameters are wholly dependent on others finucane et al 2014 gelman et al 2014 since dependencies between the different parameters are expressed through statistical conditioning bayesian theory is a natural fit for these hierarchical models thus we can see that hydrogeology is a good candidate for bayesian hierarchical modeling data from disparate sites can be jointly used for the inference while accounting for epistemic differences between different sites hierarchical models account for such inter site variability by recognizing systematic unexplained variation among the sites while also recognizing the underlying similarity between them gelman 2006a the general outline of a hierarchical model as used here is illustrated in fig 5 notations are analogous to those in the previous section i e we consider the random variable y to be associated with a physical property y and we derive the ex situ prior distribution for y at target site s 0 the ex situ prior assimilates the data coming from sites si i 1 i we can extend the algorithm to cases where sites provide multiple types of data including not only measurements but also upper or lower bounds statistical moments cumulative distribution functions and histograms d represents all available data the ex situ prior is then defined as the pdf of y given the ex situ data d which is the posterior f y y d eq 2 the hierarchical model can be summarized by virtue of the following equations 4a y i f y y ϕ i 4b ϕ i f φ ϕ η 4c η f η η the random variables yi representing hydrogeological properties at sites s 1 s i are modeled using a bayesian hierarchical model with two levels of variability the lower level represents variability within a given site i e intra site variability eq 4a whereas the upper level represents variability between sites i e inter site variability eq 4b eq 4c is the prior distribution of the statistical parameters in the model the first level in the hierarchy summarizes the data within each site eq 4a at each site si the site specific pdf is considered as a realization from a common population of pdfs where each pdf describes the statistical distribution of variables y at one site this site specific pdf is modeled as parametric and it is characterized by site specific parameters ϕi parameters ϕi summarize the data available at site si in a form homogenized over all sites thus allowing for inter site comparison by bringing all information to the same level in a consistent manner this first level is key in the assimilation of data d i available at sites si which can be of different types among the various sites the formulation of the statistical model fy assumed in eq 4a depends on the target variable under investigation in the previous section we defined it as the gaussian distribution truncated between 0 and 1 eq 3a which is a common assumption for porosity e g freeze 1975 kitanidis and vomvoris 1983 when investigating hydraulic conductivity a common assumption is that hydraulic conductivity at a site follows a log normal distribution eggleston et al 1996 freeze 1975 in this case it is recommended that one uses the log normal distribution for yi in eq 4a with φ i μ i σ i as the mean and standard deviation of the logarithmically transformed yi for the case of a bimodal variable a mixture of gaussian distributions can be used where the individual gaussian distribution represents the statistical distribution of one attribute and the probability of each attribute is modeled by an indicator function for example in a sand shale formation the log conductivity of attributes sand and shale would be modeled by two distinct gaussian distributions and the indicator function would describe the overall proportion of sand versus shale rubin 1995 systems of transforming samples into normally distributed data can also be applied in eq 4a such as the johnson system of transformations e g carsel and parrish 1988 or the box cox transformations box and cox 1964 such transformations can also be used to ensure that the ex situ prior lies between physical bounds e g the log transform guarantees that the ex situ prior restricts the probabilities to strictly positive hydraulic conductivity values thus honoring the hard inequality type constraints defined in section 2 3 it is important to note that the statistical model defined in eq 4a is the same for all sites allowing for inter site comparison and assimilation of site specific parameters in general measurements at a site are collected in a clustered way such that numerical estimates are statistically correlated due to the spatial autocorrelation of hydrogeological properties e g pyrcz and deutsch 2003 rubin 2003 if one does not account for spatial correlations this can lead to biased results the data assimilation model can account for possible correlations by using multivariate statistical distributions as the site specific model fy the most common paradigm for modeling a spatial random field srf is a multivariate normal mvn distribution e g rubin 2003 such a spatial model can be summarized by its mean μ as well as its semivariogram function defined by the parameters of spatial variability θi in its most basic form θ i σ y i 2 λ i τ i 2 where σ y i 2 is the variance of yi λi is the integral scale and τ i 2 is the nugget parameter rubin 2003 as can be seen from eq 4 each site must use the same model with only its parameters being site specific as a result a single unique variogram model has to be used to account for inter site variability in the variogram model function a flexible variogram function such as the matérn variogram can be used in actual practice spatial coordinates of measurements may not be provided in hydrogeological data sets in this case no information about the statistical correlation between numerical values can be derived from the available data a common practice here is to make the working assumption of statistical independence gelman et al 2014 if such correlations are present however the unacknowledged data redundancy might lead to an underestimation of intra site variability the second level in the hierarchy describes inter site variability eq 4b site specific parameters ϕi i 1 i are realizations from a random variable φ where the statistical distribution of φ describes how site specific parameters ϕi i 1 i vary between the different sites si i 1 i site specific parameters ϕi are realizations from the generating distribution f φ parameterized by hyperparameters η in general little is known about hyperparameters η thus f η is defined using weakly informative prior distributions so that data d can dominate the shape of the posterior distributions p η d weakly informative prior distributions are prior distributions that ensure proper posterior distributions while minimizing the information they contain the use of weakly informative priors for hyperparameters η is reasonable in this case because our goal is to derive information about the target variable y from the data d so we can be vague about the prior information gelman et al 2014 p 115 their formulation is critical particularly when the number of sites is small fewer than five gelman 2006b having now described the hierarchy the final goal of the algorithm is to derive the ex situ prior f y y d to derive this distribution the site specific parameters ϕi as well as the hyperparameters η are removed by marginalization in as shown here 5a f y y d f y y η f η η d d η with 5b f y y η f y y ϕ f φ ϕ η d ϕ 2 4 3 assimilation of available site specific data following the bayesian paradigm available data d are assimilated by updating the probability distribution of parameters η through bayes theorem i e 6 f η η d f y d η f η η the likelihood in this equation can be derived using the assumption of statistical independence for the different investigated sites this assumption leads to 7 f y d η i 1 i f y i d i ϕ i f φ ϕ i η d ϕ i i e the combined likelihood can be factorized into site specific likelihoods since the data from each site is assimilated in separate functions eq 7 provides the flexibility to assimilate the multiple types of data thus allowing sites with different numbers of measurements to be integrated the derivation of f y i d i ϕ i depends on the kind of information available at site si and is detailed in the following sections of this paper case a d i are numerical values of the target variable y to describe how one estimates f y i d i ϕ i we start with sites having information in the form of numerical values e g measurements point estimates of the investigated target variable y d i y i 1 y i j i to better explicate the different types of numerical data we distinguish between spatial and non spatial data 1 y refers to a non spatial variable for example y can model site specific summary statistics of a spatial variable e g integral scale multiple values for yi can correspond to estimates associated with different assumptions for example multiple variogram formulations can lead to multiple integral scale estimates in this case site specific estimates can be assumed to be independent realizations of the site specific pdf eq 3a in line with the assumption of exchangeability where no information other than numerical values is available for distinguishing the estimates gelman et al 2014 chap 5 2 y refers to a spatial variable for example y can model hydraulic conductivity here d i consists in ji collected numerical values d i j i n d i y i 1 y i j i which may or may not be associated with spatial coordinates x i 1 x i j i and may or may not be associated with parameters describing spatial variability at site si a measurements are associated with spatial coordinates for example y models log conductivity and each realization of y is associated with the corresponding spatial coordinates x in this case we recommend to use a multivariate distribution that accounts for spatial correlation for example with an mvn yi mvn μi θi when parameter values in the multivariate distribution μi θi are known their values can be set when declaring the hierarchical model section 2 4 4 when they are unknown their distribution will be estimated with the algorithm b measurements are not associated with spatial coordinates in this scenario there is insufficient information to quantify the correlation between measurements this is the worst case scenario the working assumption is usually to assume statistical independence case b generalization to other types of data let us now look at sites where information about the investigated variable y comes in a form other than numerical values of the target variable y such as statistical distributions bounds and moments 1 statistical distributions pdfs cdfs and histograms information about the investigated variable y at one site can be presented in the form of statistical distributions instead i e a pdf a cdf or a histogram this is the second case of having site specific hard data as presented in section 2 3 in this situation a sample of numerical values can be drawn from the statistical distribution and can be incorporated into the algorithm following the steps developed in section 2 4 3 here we use latin hypercube sampling lhs to ensure that the generated ensemble is a good representation of the variability that generated the pdf cdf or histogram hou and rubin 2005 mckay et al 1979 the data assimilation algorithm is sensitive to the number of measurements available at each site section 3 1 3 therefore the number of values drawn from the pdf cdf or histogram should correspond to the number of measurements used to generate it when the number of measurements used to derive the statistical distribution is not provided this can lead to biased results 2 bounds when data from site si are available in the form of numerical bounds d i b m i n i b m a x i we follow maximum entropy principles where the corresponding statistical distribution maximizing entropy is the uniform distribution between the lower and upper bounds hou and rubin 2005 woodbury and rubin 2000 in a case where only the lower upper bound is provided the upper lower bound is fixed to the maximum minimum of all values at the other sites alternatively if no values are provided at the other sites it is fixed to the maximum minimum bound of the physical range of the target variable we can then follow similar steps as those presented above in the paragraph on statistical distributions again the algorithm is sensitive to the number of numerical values provided therefore the number of values drawn from the uniform distribution should correspond to the number of measurements taken in the field 3 moments when information at a site is given in the form of moments e g mean variance it can be directly used as estimates of site specific parameters ϕi and can be declared as such when declaring the hierarchical model this is further detailed in section 2 4 4 discussing numerical implementation 2 4 4 numerical implementation we now turn to the numerical implementation of the bayesian hierarchical model section 2 4 2 as well as the data assimilation procedure section 2 4 3 the code is developed in the form of a library rprior developed within the r statistical environment r core team 2017 the library is available for download on github github com kcucchi rprior the library rprior is built around functions provided by the nimble r package which is designed for building analysis methods for bayesian hierarchical models in r nimble development team 2016 it allows for the expression of models and algorithms using a high level language while also maintaining good performance thanks to the use of low level languages like c when performing computationally intensive steps de valpine et al 2017 in this section we briefly describe how rprior applies functions in nimble to derive ex situ priors the bayesian hierarchical model eq 3 is declared using the function nimblecode the model specification is based on the bugs language gilks et al 1994 bugs provides the flexibility to declare a wide range of common parametric statistical distributions covering univariate and multivariate distributions both categorical and discrete and with or without transforms for example the bimodal model can be declared combining the categorical indicator distribution and the continuous gaussian distribution the prior distribution for hyperparameters η are also declared in the nimblecode function nimble provides a flexible way to declare the assimilated data d each declaration creates a node which can be either deterministic or stochastic for example when information at site si is provided in the form of moments the node mu i describing the site specific mean μi is assigned the corresponding numerical value in cases in which μi is unknown the node mu i is defined to be stochastic and is estimated from dependent nodes y 1 y ji according to the statistical distribution declared in the bayesian hierarchical model in the library rprior data d are provided in the form of an r dataframe object declarations of assimilated data are implemented accordingly after the bayesian hierarchical model and the data are declared rprior estimates the posterior distribution of the hyperparameters f η η d step 1 in section 2 4 1 eq 6 f η η d is estimated by generating a large random sample of values that follow this distribution using a markov chain monte carlo mcmc technique as implemented within nimble rprior then estimates f η η d by applying kernel density estimation on samples in the mcmc the next and final step is the derivation of the ex situ prior f y y d step 2 in section 2 4 1 eq 5 to do so each sample of η in the mcmc is used to draw realizations of site specific parameters ϕ in turn each sample ϕ is used to draw realizations of y this provides a sample of y values that follow the distribution f y y d which is then estimated by applying kernel density estimation to the obtained samples 3 results and discussion having described the methods used in this study we now demonstrate how the proposed data assimilation method behaved on a set of synthetic case studies using simulated spatial random fields and simulated field campaigns from these fields we investigated the performance of the data assimilation algorithm with a multiple number of sites a multiple number of measurements per site and with various spatial configurations of the measurements in addition we discuss results obtained when we applied our algorithm to an open access hydrogeological dataset 3 1 synthetic case studies to illustrate the algorithm and familiarize the reader with the assimilation of measurements and the process of prior derivation we start with a set of synthetic case studies to that end we used a varying number of sites and a varying number of measurements per site we then investigated the case of spatially correlated measurements and the role of the integral scale in the derivation of an ex situ prior 3 1 1 setup of the synthetic case studies since we used synthetic case studies to test and validate our algorithm we had to establish an algorithm to simulate field campaigns the overall procedure is illustrated in fig 6 and can be summarized as a four step process in the first step we generated spatially correlated random fields with specified stochastic properties the site specific means μi were assumed to follow the distribution n α 0 τ 0 2 and the spatial correlation structure was assumed to follow a gaussian variogram model with variance σ 0 2 and integral scale λ 0 the hyperparameter values were fixed to η 0 α 0 7 5 τ 0 2 0 25 σ 0 2 0 25 λ 0 1 we quantified the influence of the distance between measurements by sampling the measurements within circles of radii defined in relation to the integral scale λ 0 in the second step we simulated a field measurement campaign by sampling from these spatial random fields to explore the impact of different factors we varied the number of sites i the number of measurements per site j and the spacing between measurements r for combinations of i j and r values we randomly selected i sites and randomly sampled j measurement values from these sites within a circle of radius r λ in the following we call d i j r an example of data that corresponds to i sites j measurements per site and uniformly sampled within radius rλ for each unique combination of i j r we simulated 100 realizations of d i j r in the third step we derived the ex situ prior f y y d i j r for each realization of site specific data using the model presented in eq 3 in the fourth and final step we assessed the performance of the computed ex situ priors f y y d i j r the performance was evaluated by calculating the distance between the ex situ prior and the underlying desired distribution fy y η0 using the kullback leibler divergence kld as defined in eq 8 8 k l d f y d i j r f y η 0 f y y d i j r log f y y d i j r f y y η 0 d y the kld also known as relative entropy measures the divergence i e the non symmetrical distance between two pdfs it thus quantifies the distance between two pdfs and can be used to compare one reference state of knowledge to another state in general two different interpretations of this distance can be found in the literature depending on the direction fig 7 in information theory the kld is interpreted as information loss i e the amount of information that would be lost if instead of the true distribution another distribution is used in contrast in bayesian inference the kld is interpreted in the opposite direction as information gain i e the information gained by updating from the prior to the posterior distribution therefore in this latter interpretation the kld is a direct measure for the information content of the data assimilated during the inference in the following we use the kld in both of these ways depending on the context first in the synthetic examples we know the actual distribution of the underlying population in this case we compare these true distributions to the predictive distribution of our ex situ prior algorithm the values of the kld can thus be interpreted as information loss i e the amount of information that would still be needed for a full statistical characterization of the population however for cases where the true distribution is unknown we use the kld as a measure for information gain i e the information provided by our ex situ prior algorithm in order to avoid any misunderstandings we will clarify the interpretation of the kld in each case 3 1 2 inter site and intra site hierarchy first we demonstrate how the ex situ prior algorithm can handle a different number of measurements per site and how it considers measurements coming from different sites differently than measurements coming from a single site for this investigation we simulated field measurement campaigns and derived the ex situ prior using the framework described in section 3 1 1 here we consider the case where we sampled measurements from only two sites i 2 we sampled a varying number of measurements from site 1 j 1 2 5 10 20 30 and j 2 5 measurements from the second site for each combination of j 1 and j 2 100 realizations were performed for each field campaign realization we computed the ex situ prior obtained when using measurements from site s 1 only from site s 2 only from site s 1 and s 2 combined s 1 s 2 and when considering that measurements at site s 1 and s 2 were collected at a single site s 1 u s 2 fig 8 a shows the ex situ priors derived for a single realization of a field campaign the ex situ data came from two sites s 1 and s 2 with j 1 2 and j 2 5 number of measurements respectively to assess the uncertainty in the ex situ prior we used the variance of the pdf as a measure as seen fig 8a its value is similar for the cases s 1 s 2 and s 1 u s 2 but noticeably smaller for s 1 s 2 this can be explained by the fact that in the cases s 1 s 2 and s 1 u s 2 the algorithm was not able to estimate inter site parameters and thus the variability in the obtained ex situ prior was consequently very large when assimilating data from the two sites the algorithm was able to at least estimate this inter site variability to some degree which resulted in a narrower ex situ prior to investigate whether this behavior held up more generally we performed a series of field measurement campaigns with a varying number of measurements fig 8b to have a significant sample size for our analysis we used 100 realizations per configuration we again assessed the information gain by computing the kld between the non informed prior and each ex situ prior we found the same general behavior pertaining to all values of j 1 i e the kld was always significantly higher for s 1 s 2 compared to any of the other configurations fig 8b these results indicate the higher value of a dataset with high inter site variability i e many measurements from different sites compared to a dataset with high intra site variability i e many measurements from few sites or just one site this difference can be explained by the goal of the ex situ prior for predicting the target variable at a new site since this new site is a random sample from the population of all considered sites it is more important to get a grasp on the variability of the population of sites i e the inter site variability rather than the intra site variability of just a few sites or even a single site as was the case here 3 1 3 influence of the number sites and number of measurements per site now we turn to the influence of the number of sites and number of measurements per site on the performance of the ex situ prior it stands to reason that the performance of the ex situ prior will improve with the number of sites as well as with the number of measurements per site it is however not immediately clear how each factor will contribute overall to assess this relative impact we focus on interpreting the results in terms of predicting both the ex situ prior f y y d i j and the predictive distribution of the site specific mean f μ μ d i j to that purpose we performed a synthetic study described in section 3 1 1 we fixed the value r λ 10 corresponding to a case where the measurements were sampled over a large region with respect to the integral scale i e where little to no data redundancy was assumed measurements were sampled from a number of sites i between 2 and 30 with the number of measurements per site j varying between 2 and 30 for each combination of i and j we simulated 100 measurement campaigns d i j and calculated the corresponding 100 ex situ priors f y y d i j fig 9 a shows the variation of the kld as a function of the number of sites i and the number of measurements per site j here the kld measures the information loss in the target variable y in this figure the lines represent the smoothed variation of the kld with the number of sites i grouped by the number of measurements per site j as expected the kld decreased with the number of sites i and the performance of the ex situ priors improved as the number of sites used in the data assimilation increased moreover for a fixed number of sites i the kld decreased when the number of measurements per site j increased fig 9b shows the variation of the kld as a function of the number of sites i and the number of measurements per site j here again the kld measures the information loss for the distribution of the site specific means μ as in the scenario above the information loss decreased with the increasing number of sites the sensitivity of the results with respect to the number of measurements per site was low when compared to the number of sites where all lines were located within similar confidence intervals it is also interesting to note that the decrease in information loss as a function of the number of sites was more pronounced for a low number of sites than for a high number of sites these results illustrate how limitations due to a small number of measurements per site can be compensated for by increasing the number of similar sites used in the data assimilation for example an average kld of 0 25 can equivalently be obtained by assimilating 30 measurements per site from 18 sites or alternatively 2 measurements per site from 25 sites this shows that while both factors contribute positively the relative contribution is quite different and depending on the available dataset very different findings can result from the same amount of raw data 3 1 4 derivation of the parameters of a gaussian srf up to this point we have seen how the ex situ prior model can provide an estimate of the statistical distribution of conductivity values however due to the spatial correlation of conductivity fields higher order statistical moments need to be taken into account for a meaningful representation of real world aquifers by far the most used paradigm in spatial statistics is that of a gaussian spatial random field srf also known as a gaussian process or a multivariate gaussian field gelfand 2012 gelfand and schliep 2016 gaussian srfs are used widely due to being very parsimonious extremely flexible and hierarchical in nature they are parsimonious since in their most basic form they are fully parametrized by specifying their mean variance covariance model function and integral scale they are also flexible because additional features such as anisotropy or a non orthogonal axis can be introduced finally their hierarchical nature permits their conditioning to direct point measurements to demonstrate that our algorithm can be used in conjunction with a gaussian srf we extended the formulation of our hierarchical model with a spatial covariance matrix as the model function for the variogram we chose the exponential model this restriction on a single model function was seen as reasonable since several studies have indicated that the specific form of this function has limited influence compared to its parameters heße et al 2015 jafarpour and tarrahi 2011 riva and willmann 2009 after the revision of the ex situ prior model we used the same procedure described above to derive the predictive distributions of the parameter values to derive additional estimates for the distribution of the integral scale λ we had to amend our synthetic data set with spatial coordinates for our measurements the number of measurements per site was j 30 and the number of sites varied between i 3 13 30 in general again we saw good convergence of the estimated distributions for increased i see rows in fig 10 for each parameter of the gaussian srf in particular the behavior of the mean μ and variance σ was similar to the previous results although the latter showed some bias such that the true value was somewhat underestimated this bias was most likely caused by the nature of likelihood based estimation mardia et al 1999 it is known that the expectation of the likelihood estimate of a variance with a sample size of n is given as n 1 n times the true value thereby causing systematic underestimation frequentist inference usually employs a bias correction when using likelihood based estimation known as bessel s correction whereas those using bayesian reasoning are generally less concerned with this bias inherited from the likelihood estimator for a discussion on the topic see e g gelman 2006b 2008 gelman et al 2014 however the main new feature of our extended ex situ prior model was the spatial covariance function with the integral scale as its defining parameter infererring this parameter proved to be difficult especially compared to the other two parameters compare the third row in fig 10 to its first two rows although increasing the number of sites improved the estimates our results still showed bias as well as inaccuracies even for the scenario of i 30 our investigation into the inference of spatial correlations already allows us to make a number of relevant observations first achieving convergence for higher order statistics like the integral scale is data demanding compared to simple first order statistics since earlier studies have demonstrated a low sensitivity to the integral scale e g firmani et al 2006 this seems to be an inherent problem of spatial statistics second all our results were drawn from idealized virtual data with no structural errors present i e our conductivity fields were actually gaussian srfs with an exponential covariance function in reality both of these criteria may be violated to some extent in which case a different geostatistical paradigm may be needed all alternatives to gaussian srfs are however less parsimonious linde et al 2015 according to our results such an increase in the number of parameters will also lead to a strong increase in the amount of data needed for a full characterization 3 2 application to an established dataset having investigated the properties of our ex situ prior model with synthetic data we next applied the data assimilation framework to the world wide hydrogeological parameters database wwhypda comunian and renard 2009 with the objective of predicting the univariate log transformed hydraulic conductivity distribution y log 10 k at a target site in addition we also estimated the predictive distributions for site specific means μ e log 10 k and site specific standard deviations σ σ log 10 k at the target site the wwhypda is an open collaborative effort to gather dispersed and difficult to access hydrogeological information comunian and renard 2009 the data is accessible online through their website http wwhypda org and is available to users contributions amongst other meta information each measurement reported in the database is associated with a unique field site identifier where each field site is in turn associated with one or several material types and environment types the wwhypda provides the option of reporting the spatial location of each measurement however in the database no spatial coordinates are actually reported to better access the database we transformed the sql tables into an r dataframe object via the r package rprior following the usual assumption that hydraulic conductivity is log normally distributed within a site e g freeze 1975 we selected hydraulic conductivity measurements from the database and applied the logarithmic transform we defined four material types of interest sand silt gravel and clay we then grouped the measurements by the selected types the groups were defined depending on whether the material type was mentioned in the type description of the site for example measurements from a site labeled as silty clay were used for deriving both silt and clay ex situ priors this approach was selected to maximize the number of measurements assimilated within each material group the number of measurements reported for each material type and the corresponding number of field sites are reported in table 1 the histograms of the measurements are presented in fig 11a we found that predicted ex situ priors of the hydraulic conductivity values differed noticeably between types such that silt showed the lowest values followed by clay sand and finally gravel fig 12 a differences were also found in the prediction certainty as expressed by the width of the distribution for each type fig 12b certainty in the ex situ prior was highest for gravel followed by sand clay and silt the factors that affected these differences included the differences in assimilated measurements themselves as well as the size of the data set used when deriving the ex situ priors for instance gravel which had the highest prediction certainty had measurements showing less variability overall when compared to the three other types on the other hand the measurements for sand and silt showed a similar range of variability in their measurements but had different prediction certainty in the ex situ prior in this case the difference was caused by the different number of measurements with sandy materials having 1122 measurements from 17 different field sites whereas the silty material had only 353 measurements from 5 distinct field sites given the results above we argue that the difference in the number of sites rather than in the total number of measurements caused the resulting differences in prediction certainty fig 11b extends the discussion by showing the derived ex situ priors for each material type alongside the distributions derived by using the alternative density estimates presented in section 2 2 in all cases the ex situ priors showed higher uncertainty i e wider densities than alternative methods such an increase in uncertainty for a supposedly improved method may seem counter intuitive however this behavior is not unusual for instance a well known and quite similar example is the notion of pseudoreplication hurlbert 1984 where hidden correlations in the data cause the effective number of independent samples to be much lower than the raw number of samples methods that do not account for such hidden correlations consequently unduly inflate the certainty in the results in our research one notable exception to the general trend was the case of the sandy material for which the ex situ prior was close to the distributions obtained by applying the work of carsel and parrish 1988 and the maximum entropy approaches this may relate to the fact that the measurements came from 17 sites and therefore the overall distribution of measurements was representative of inter site variability for all other material types the measurements provided came from just five or six sites the corresponding ex situ priors recognized the underlying uncertainty by deriving a distribution with larger variance compared to the distribution obtained by other methods which by design cannot account for the inter site variability in all cases the kernel density estimate closely followed the underlying distribution of the measurements and it overfitted the predicted distribution to the measurements that were used to derive predictive distributions fig 12b and c show the derived pdfs for the site specific means and standard deviations respectively these are the pdfs for the level 1 parameters φ fig 5 pdfs for means of log transformed hydraulic conductivity fig 12b were almost as diffuse as the univariate distributions fig 12a by definition of the hierarchical model eq 3a the univariate log transformed hydraulic conductivity is normally distributed with mean μ fig 12b and standard deviation σ fig 12c therefore in this case and for all material types the parameters describing inter variability had higher variance than the parameters describing the intra site variability and the shapes of univariate distributions were dominated by the shapes of the inter site means this suggests the uncertainty represented by the ex situ priors i e the width of the ex situ priors could be decreased by a more careful selection of similar groups of hydrogeological sites in summary in this work we demonstrated how sites with similar material types can be selected as similar to the target site and how measurements from these similar sites can be used to derive ex situ priors for univariate log transformed hydraulic conductivity as well as the mean and variance of the variogram model at the target site 4 conclusion the goal of this study was to introduce a statistical framework for assimilating different types of ex situ data coming from multiple sites into a predictive prior for a hydrogeological variable at a target site we called this predictive prior ex situ prior the properties of the proposed framework were c 1 that assimilated data are ex situ only c 2 that an additional level of variability is associated when data come from different sites and c 3 that multiple types of site specific data can be assimilated to the best of our knowledge this is the first time that an algorithm with such properties has been introduced for predicting statistical distributions of hydrogeological variables while the examples and applications presented in the study are relatively simple they illustrate the opportunities offered by the algorithm in particular we proposed a complete framework for predicting parameters of univariate and bivariate statistics for log transformed hydraulic conductivity at a target site we then demonstrated the derivation of predictive univariate and bivariate statistics on a synthetic case study and illustrated their derivation using measurements from the wwhypda other predictions relevant to models of spatial variability are not covered in this paper such as predicting the value for the nugget parameter in the variogram or predicting the form of the variogram e g exponential gaussian nonetheless the proposed approach could be applied to these problems by modifying the formulation of the hierarchical model predicting the form of the variogram at a target site could be accomplished by either modifying the data assimilation algorithm to allow for the assimilation and the prediction of categorical variables related to the variogram forms or by using the matérn formulation of the variogram and predicting the value of the smoothness parameter these investigations however are outside the scope of the current study although they are likely to be the focus of subsequent work we note that a big hindrance to the development and applicability of the method is the availability and quality of assimilated data in particular this would be true for anything related to scale dependency since wwhypda lacks information pertaining to this topic scale dependency is relevant for quantities like mean and variance of the hydraulic conductivity as well as for the integral scale itself whose very definition is often highly contingent on the defined support scale depth of measurement is another piece of information that is missing from estimates in wwhypda we therefore view the application examples as a proof of concept to show the applicability of the proposed approach to address this topic we hope that more work will be performed to developing more comprehensive readily and easily accessible databases of hydrogeological parameters here the wwhypda introduced by comunian and renard 2009 is a significant step in the right direction these databases would detail the measurement scale of different hydrogeological quantities such that scale dependency could be accounted for in the data assimilation by rescaling the estimates on a common support e g dagan et al 2013 fiori et al 2011 such hydrogeological datasets would increase the predictive capabilities of data assimilation algorithms such as the one presented in this study which in turn could foster wider collaborations among researchers and practitioners toward the development of such databases another limitation of the presented applications is the dependence on the formulation of statistical distributions in the hierarchical model eq 3 the choice of a gaussian model was motivated by its popularity and simplicity bearing in mind that the data assimilation algorithm would be applied to datasets of small to moderate size we mentioned that the algorithm could be adapted to other more complex statistical distributions depending on the properties of the target variable and on the quality of the data the current formulation that uses a gaussian model dictates the unimodal behavior of the ex situ priors derived in the study more complex behavior could be predicted by assuming more complex statistical models in the hierarchical model although more data would be needed for this inference in our work ex situ prior results depended on values assimilated by the algorithm and in particular on the choice of similar sites in the presented application section 3 2 we decided to define similarity based on material type however other hydrological factors of interest such as the support volume or the depth of measurement also influence the value and variability of hydraulic conductivity estimates we can thus expect that accounting for such factors would lead to more precise predictions of hydrogeological variables again recognizing that the possibility of doing so would rely on the existence of datasets containing such information data assimilation algorithms like the one presented in this paper provide frameworks for making predictions based on multiple subgroups and test feature relevances with the goal of assimilating relevant data to guide the selection of similar sites as obtaining hydrogeological data is time consuming and expensive there has been limited incentive for researchers and practitioners to share their data in a structured and easily accessible form comunian and renard 2009 it is our hope that the introduced ex situ prior algorithm and its implementation in the r package rprior will trigger the development of open databases so that better use of information gained in previous investigations can be made in the future acknowledgments for this study karina cucchi was financially supported by the jane lewis fellowship from the university of california berkeley falk heße was financially supported by the deutsche forschungsgemeinschaft via grant no he 7028 1 2 nura kawa was financially supported by the deutsche forschungsgemeinschaft via the sonderforschungsbereich crc 1076 aquadiva the authors thank dr renard and dr comunian for their guidance in using the wwhypda the authors also acknowledge the helpful comments and suggestions from two anonymous reviewers supplementary material supplementary material associated with this article can be found in the online version at doi 10 1016 j advwatres 2019 02 003 appendix a supplementary materials supplementary data s1 supplementary raw research data this is open data under the cc by license http creativecommons org licenses by 4 0 supplementary data s1 
686,stochastic modeling is a common practice for modeling uncertainty in hydrogeology in stochastic modeling aquifer properties are characterized by their probability density functions pdfs the bayesian approach for inverse modeling is often used to assimilate information from field measurements collected at a site into properties posterior pdfs this necessitates the definition of a prior pdf characterizing the knowledge of hydrological properties before undertaking any investigation at the site and usually coming from previous studies at similar sites in this paper we introduce a bayesian hierarchical algorithm capable of assimilating various information like point measurements bounds and moments into a single informative pdf that we call ex situ prior this informative pdf summarizes the ex situ information available about a hydrogeological parameter at a site of interest which can then be used as a prior pdf in future studies at the site we demonstrate the behavior of the algorithm on several synthetic case studies compare it to other methods described in the literature and illustrate the approach by applying it to a public open access hydrogeological dataset keywords data assimilation data fusion bayesian hierarchical model informative prior databases 1 introduction 1 1 context of study modeling hydrogeological processes like groundwater flow and transport is a challenging task due to the general lack of information about subsurface conditions this lack of information is caused by spatial variability and general data scarcity this situation where modelers have to work with largely incomplete information has led to the increasing adoption of statistical methods able to account for resulting uncertainties rubin 2003 bayesian inference provides a framework to deal with these intrinsic uncertainties in this paper we conceptualize the uncertain hydrogeological parameters y as random variables y which are fully defined by their probability density function pdf f y y the goal of the inference is thus to condition this pdf on the available data z that are related to y i e deriving f y y z this is done using bayes theorem hence the name of this framework one salient feature of this framework is that the data z represent only one part of the inference with the other part being the prior pdf f y y bayesian reasoning demands that this prior pdf represents the knowledge about the target parameter y before i e prior to accounting for the case specific data whereas the posterior pdf f y y z represents the knowledge after accounting for such data rubin 2003 within the context of hydrogeological site characterization this means that the prior pdf should be derived without accounting for data z coming from the site under investigation prior pdfs are consequently key for any bayesian inference yet deriving a prior is not a straightforward process and only a few guidelines exist to help modelers in this task in general scales and tenorio 2001 and in hydrogeological studies in particular in fact the bayesian point of view is sometimes criticized precisely because modelers need to define a prior a step considered to be subjective and arbitrary as it depends on the modeler kass and wasserman 1996 ulrych et al 2001 however it is important to note that subjectivity arises necessarily in any modeling e g in the choice of the likelihood function or the objective function when performing deterministic calibrations e g over et al 2015 bayesian modelers actually see the definition of priors as an opportunity to incorporate previous experience working with variables y into the inverse estimation process in ways that are both transparent and theoretically sound such experience can come in many different forms next to direct point measurements of the target variable experience is shaped by estimates of covariates summary statistics or soft expert knowledge in the form of bounds or percentiles in this study we focus on informative priors which by definition provide a non trivial amount of information about the target variables before the assimilation of measurements taken at the target site the derivation of informative priors becomes possible by the fact that databases that gather parameter values are becoming increasingly common and accessible paraphrasing vereecken et al 2014 p 91 we are now entering an age of big data in environmental modeling with datasets containing an unprecedented quantity of information to the authors knowledge no algorithm has been presented to date that can assimilate such detailed information in the form of prior distributions for hydrogeological properties here we propose an algorithm that can take advantage of the constantly increasing amount of data available for the purpose of better informed environmental modeling and decision making the use of informative priors has several advantages first the state of uncertainty represented in an informative prior is a measure of the amount of background knowledge available for a particular field it can be seen as a benchmark for a field sampling campaign permitting the comparison of the expected information content in the measurements as expressed in the likelihood function to the information content of the background knowledge as expressed in the prior pdf geiges et al 2015 hou and rubin 2005 nowak et al 2010 2012 tang et al 2016 furthermore the information contained in informative priors may be crucial in situations where the information content in the measurements is insufficient to derive posterior pdfs with the desired level of certainty reichert 1997 this is often the case in hydrological studies with high levels of data scarcity to help identify those situations where the use of an informative prior might assist with parameter identification tang et al 2016 used the kullback leibler divergence kld to quantify the impact of prior and likelihood distributions on the posterior they concluded that their approach could be used to inform the choice of a meaningful prior where parameters that are insensitive to measurements should be given a well defined prior while sensitive parameters could have a prior defined more vaguely other situations where informative priors are helpful include ill posed problems and cases of non identifiability where different sets of parameters lead to similar goodness of fit results this issue is prevalent with inverse problems in hydrogeology carrera and neuman 1986 zhou et al 2014 in bayesian inverse modeling the problem of identifiability does not exist as equally representative parameter sets simply result in equal likelihood values reichert 1997 however there is clearly an obstacle when the goal is to estimate parameter values with a desired level of certainty by reducing the range over which parameters can vary the use of narrow prior distributions can better inform the inverse modeling process and help identify a unique parameter set from the derived posterior scharnagl et al 2011 zhou et al 2014 in sum informative priors allow modelers to incorporate previous experience gained from working with variables of interest and can improve the precision of derived posterior distributions to that end it is critical to be able to formulate an appropriate informative prior pdf for a site of interest using information from previous studies 1 2 site similarity and levels of variability the construction of an informative prior for hydrogeological studies relies on the careful selection of data used in the assimilation obtained from previous work at field sites similar to the target site rubin 2003 however the notion of site similarity is not straightforward and it depends on the processes and variables under investigation site similarity is established when the target site and a previously investigated field site share similar characteristics with respect to the target variable when comparing sites variability in target variable estimates can relate to two different aspects physical and epistemic li et al 2018 the physical aspect refers to the state of the physical system under consideration in hydrogeology physical aspects include elements such as the geological environment e g silt clay and sand for soils sedimentary metamorphic and igneous for rocks another example of a hydrogeological physical aspect is the depth below the ground surface as hydraulic conductivity generally decreases with depth e g li et al 2018 the epistemic aspect refers to how the observer interacts with the physical system to generate information this aspect covers the investigation strategy and in particular the method used to derive estimates a variable can be directly estimated using an in situ measurement technique or it can be indirectly estimated using inverse modeling in the first case variability can be introduced by the choice of a measurement technique e g core extraction or different types of in situ pump tests in the second case variability can be introduced by modeling choices e g the forward model can assume radial flow uniform in the average mean flow or 2d or 3d heterogeneity different choices for investigation strategies can lead to varying estimates for a unique physical medium another important epistemic aspect relates to scale dependence i e the dependence of the conductivity on the measurement scale indelman and dagan 1993 scale dependence should always be recognized this will be further discussed in section 4 site similarity should be established on the basis of the physical characteristics epistemic characteristics create additional variability in the derived values in this study we introduce an algorithm that accounts for epistemic variability by proposing a hierarchical approach to hydrogeological data assimilation 1 3 objective of study given these considerations we formulate a list of criteria c summarizing the elements to be considered for establishing ex situ priors c 1 ex situ the prior pdf should result from the assimilation of ex situ data i e coming from field sites different than the target site c 2 inter site versus intra site variability the assimilation algorithm should distinguish between measurements coming from the same site and those coming from different sites by accounting for additional variability when the measurements come from different sites c 3 data variability the assimilation algorithm should be able to handle different types of information e g direct measurements covariates moments bounds etc in this study we introduce an algorithm that permits the assimilation of data gathered from similar sites into an ex situ prior pdf honoring c therefore our objective pertains to summarizing and integrating the findings of previous studies in a particular area of research e g carlin 1992 here our analysis involves summarizing data from several independent field sites and integrating that data into an informed pdf for a target variable using the bayesian hierarchical modeling framework the algorithm assimilates the data from previous studies with a unified approach into a pdf honoring the criteria c which we call ex situ prior 2 methods 2 1 bayesian inference and the role of the prior distribution when performing a hydrogeological investigation at a particular site in situ measurements z are used to estimate the subsurface properties y in a stochastic framework the parameters of the model y are considered to be the realizations of the random variable y characterized by a probability density function pdf bayesian inference uses the in situ information contained in z to improve the statistical characterization of y at the site under investigation the basis of all bayesian inference is bayes theorem which can be stated as follows 1 f y y z f z z y f z z f y y here z represents the in situ data used for the inference additionally y represents the vector of physical parameters at the site under investigation and is described by the random variable y in this context f y y is the prior describing our belief in y before accounting for the in situ data z and f y y z is the posterior describing our belief in y after accounting for the in situ data z the term connecting both is the normalized likelihood consisting of the likelihood fz z y and the normalizing constant fz z for the sake of clarity we distinguish between the parameters in the physical model y and the parameters in the statistical data assimilation model henceforth referring to y as the target variable a prior distribution can be informative or non informative depending on its influence on the posterior distribution a prior is considered non informative when its influence on the posterior distribution is minimal such that the inference is unaffected by information external to z gelman et al 2014 using non informative priors implies effectively starting in a state of ignorance i e no prior knowledge on the question exists or can reasonably be used conversely using informative priors means that we are continuing the inference i e building upon prior results considered to be relevant to the current question since our study focuses on informative priors the prior in eq 1 contains relevant background information derived from available ex situ data d to reflect this notion we recast eq 1 as 2 f y y d z f z z y f z z f y y d f z z y f z z f d d y f d d f y y with f y y being the non informative prior and f y y d being the informative prior in eq 2 z refers to in situ data and d refers to ex situ data fig 1 it is important to clarify that even though f y y d results from data assimilation we still refer to it as a prior distribution since it represents what is known about the target variable y before accounting for the in situ data z the goal of the data assimilation algorithm introduced in this study is therefore the derivation of this ex situ prior distribution f y y d 2 2 existing methods for formulating prior distributions in hydrogeology previous studies have adopted different strategies when defining prior pdfs for hydrogeological properties in this section we summarize these strategies and show that none can account for elements listed in c demonstrating the need for a new data assimilation framework non informative priors represent the highest level of parameter uncertainty and are chosen when minimal prior knowledge is present they are the most common choice for researchers performing hydrogeological studies e g arnold et al 2013 de barros et al 2012 engeland and gottschalk 2002 houska et al 2014 here we review how previous studies defined informative priors and explore the extent to which they followed criteria c one common approach to the definition of informative priors in hydrogeology is the principle of maximum relative entropy mre mre states that when several distributions are acceptable the one that should be selected is the one with minimal information content i e the one with maximum entropy jaynes 1982 woodbury and ulrych 1993 pioneered the application of mre in hydrogeology they derived the statistical distributions of parameters in the transport equations using expected values lower and upper bounds that were based on three previous studies another example of an mre application was demonstrated by woodbury and rubin 2000 who defined a prior pdf for transport model parameters based on expected value upper and lower bounds obtained from a previous study hou and rubin 2005 later defined prior pdfs for hydraulic parameters based on lower bound upper bound mean and variance of the values pooled from the rosetta database although these studies showed how mre concepts permit the assimilation of different types of information e g moments bounds in a systematic and objective framework a frequently encountered limitation was the fact that mre does not distinguish between inter site and intra site variability c 2 rather it pools all information from previous studies at one single level an alternative approach for the formulation of informative prior distributions is the use of statistical distributions fitted to values in large databases a commonly cited example here is carsel et al 1988 who defined statistical distributions for bulk density field capacity wilting point and organic matter by gathering data from 2942 material series provided by the u s soil conservation service they sorted the data into four material classes and then used the johnson transformation to transform the empirical distribution of their parameters into normally distributed distributions for each material class carsel and parrish 1988 adopted a similar approach for unsaturated zone hydraulic parameters their work has received considerable attention and the statistical distributions they derived have served as informative priors in subsequent studies including wang et al 2003 and over et al 2015 the limitations of this approach however are that it does not distinguish between inter site and intra site variability c 2 and it does not allow for the assimilation of multiple types of information e g bounds moments c 3 in conclusion to the authors knowledge no data assimilation method has been introduced in hydrogeological studies that can account for the multiple levels of data variability c 2 and allows for the assimilation of different types of information c 3 2 3 types of data used in the assimilation ex situ information about a hydrogeological property can be classified into different categories when investigating the spatial distribution of a variable at one site journel 1986 distinguished between three broad categories for classifying the available data hard data i e numerical values hard inequality type data where the value is known to lie between given bounds and soft qualitative information e g smoothness of spatially distributed data here we modify these categories as follows 1 hard inequality type data this is data where the variable is known to physically lie within a bounded interval e g hydraulic conductivity is strictly positive and porosity is bounded between 0 and 1 this condition holds for all investigated sites 2 hard site specific data this is site specific data available in the form of a list of measurements of the variable a histogram of measured values a pdf or a cumulative distribution function cdf that is derived from measured values at a similar site different from the site under investigation 3 soft site specific data this refers to data that is not available in terms of measurements for example it can refer to a site specific range of values site specific statistical moments or site specific qualitative information obtained from expert judgment we detail how the data assimilation algorithm accounts for physical and site specific information in sections 2 4 2 and 2 4 3 respectively 2 4 data assimilation framework in this section we introduce the data assimilation framework for constructing the ex situ prior first using a simple example section 2 4 1 illustrates the application of bayesian hierarchical modeling to derive the ex situ prior then sections 2 4 2 and 2 4 3 generalize bayesian hierarchical modeling as a statistical framework for assimilating different types of data lastly section 2 4 4 presents the numerical implementation of the algorithm 2 4 1 illustration on a simple synthetic example let us start by illustrating the construction of the ex situ prior using a small synthetic data set the goal is to derive the ex situ prior for a target site s 0 using data related to porosity from three sites s 1 s 2 and s 3 fig 2 here porosity is the target variable y at site s 1 a single data point of y 1 is available with value y 1 1 0 2 first column from the left in fig 2 at site s 2 three data points of y 2 are available with values y 2 1 0 3 y 2 2 0 4 and y 2 3 0 2 second column from the left in fig 2 at site s 3 again three data points of y 3 are available with values y 3 1 0 3 y 3 2 0 2 and y 3 3 0 2 third column from the left in fig 2 together these seven data points are the ex situ data d in our example as can be seen from the values given above all data are numerical in nature e g direct point measurements of y furthermore it should be clear that the statistical distributions of y are bounded by 0 and 1 since y relates to porosity the ex situ prior f y y d can now be defined using an instance from the class of hierarchical models namely the basic normal hierarchical model gelman 2006b this model is illustrated in fig 2 and is described by the following equations 3a y i n 0 1 μ i σ 2 3b μ i n 0 1 α τ 2 3c f α τ σ f α f τ f σ the first level in this hierarchy models the intra site variability eq 3a it describes the statistical distribution of the target variable within each site at each site si data are viewed as realizations from a site specific distribution describing the random variable yi in this example we assume that at each site si measurements are independently drawn from the truncated normal distribution characterized by the site specific parameters μi and σ denoting the intra site mean and standard deviation respectively an underlying assumption is that the numerical values y i provided for site si are independent conditionally on μi and σ we discuss the implication of this assumption in more detail in section 2 4 2 and explain how it can be modified to account for spatial autocorrelation the second level in the hierarchy models the inter site variability eq 3b this example follows the basic normal hierarchical model for which site specific means μi are drawn from a normal distribution with parameters α and τ 2 and site specific variance σ 2 is defined by a prior pdf common for all sites polson and scott 2012 these parameters can be combined into a set η α τ σ termed hyperparameters to infer their distribution we need initial prior distributions for which we use non informative distributions a flat prior is used for α p α 1 for the level 1 variance parameter σ 2 we choose jeffrey s prior p σ 2 σ 2 since it obeys the invariance principle jeffreys 1946 although it is an improper prior it leads to posteriors with acceptable properties polson and scott 2012 however jeffrey s prior is not acceptable for the level 2 variance parameter τ 2 because it leads to an improper posterior with the normal hierarchical model gelman 2002 for τ 2 polson and scott 2012 have recommended using the half cauchy prior as further detailed in sections 2 4 2 and 2 4 3 the two main steps of the ex situ prior derivation f y y d are as follows 1 data assimilation i e deriving the updated distribution of hyperparameters η f η η d fig 3 2 prediction i e deriving the ex situ prior f y y d fig 4 we next examine how these steps are performed in our simple synthetic example in step 1 we estimate the parameters in the statistical model η from the data d i e deriving the updated distribution of hyperparameters η f η η d fig 3 shows the marginal posterior pdfs of the hyperparameters η for this example the derivation and numerical implementation are presented in subsequent sections the initial priors are non informative and the posterior statistical distributions of these hyperparameters are determined by the information contained in the measurements this example shows how measurements d contributes to the better characterization of hyperparameters η in this first step the bayesian hierarchical model assimilates the measurements d by updating the statistical distribution of parameters η fig 3 in step 2 we derive the ex situ prior f y y d based on the updated distribution of the hyperparameters η fig 4 the predicted values for variable y at the target site peaks at approximately 0 25 with little probability outside the 0 0 5 range in line with the measurements d assimilated by the algorithm 2 4 2 bayesian hierarchical modeling having illustrated the prior derivation by virtue of a simple synthetic example section 2 4 1 we now look more generally at the data assimilation framework used here and in particular at bayesian hierarchical modeling in statistics a hierarchical model is used when a model with multiple parameters can be formulated such that certain parameters are wholly dependent on others finucane et al 2014 gelman et al 2014 since dependencies between the different parameters are expressed through statistical conditioning bayesian theory is a natural fit for these hierarchical models thus we can see that hydrogeology is a good candidate for bayesian hierarchical modeling data from disparate sites can be jointly used for the inference while accounting for epistemic differences between different sites hierarchical models account for such inter site variability by recognizing systematic unexplained variation among the sites while also recognizing the underlying similarity between them gelman 2006a the general outline of a hierarchical model as used here is illustrated in fig 5 notations are analogous to those in the previous section i e we consider the random variable y to be associated with a physical property y and we derive the ex situ prior distribution for y at target site s 0 the ex situ prior assimilates the data coming from sites si i 1 i we can extend the algorithm to cases where sites provide multiple types of data including not only measurements but also upper or lower bounds statistical moments cumulative distribution functions and histograms d represents all available data the ex situ prior is then defined as the pdf of y given the ex situ data d which is the posterior f y y d eq 2 the hierarchical model can be summarized by virtue of the following equations 4a y i f y y ϕ i 4b ϕ i f φ ϕ η 4c η f η η the random variables yi representing hydrogeological properties at sites s 1 s i are modeled using a bayesian hierarchical model with two levels of variability the lower level represents variability within a given site i e intra site variability eq 4a whereas the upper level represents variability between sites i e inter site variability eq 4b eq 4c is the prior distribution of the statistical parameters in the model the first level in the hierarchy summarizes the data within each site eq 4a at each site si the site specific pdf is considered as a realization from a common population of pdfs where each pdf describes the statistical distribution of variables y at one site this site specific pdf is modeled as parametric and it is characterized by site specific parameters ϕi parameters ϕi summarize the data available at site si in a form homogenized over all sites thus allowing for inter site comparison by bringing all information to the same level in a consistent manner this first level is key in the assimilation of data d i available at sites si which can be of different types among the various sites the formulation of the statistical model fy assumed in eq 4a depends on the target variable under investigation in the previous section we defined it as the gaussian distribution truncated between 0 and 1 eq 3a which is a common assumption for porosity e g freeze 1975 kitanidis and vomvoris 1983 when investigating hydraulic conductivity a common assumption is that hydraulic conductivity at a site follows a log normal distribution eggleston et al 1996 freeze 1975 in this case it is recommended that one uses the log normal distribution for yi in eq 4a with φ i μ i σ i as the mean and standard deviation of the logarithmically transformed yi for the case of a bimodal variable a mixture of gaussian distributions can be used where the individual gaussian distribution represents the statistical distribution of one attribute and the probability of each attribute is modeled by an indicator function for example in a sand shale formation the log conductivity of attributes sand and shale would be modeled by two distinct gaussian distributions and the indicator function would describe the overall proportion of sand versus shale rubin 1995 systems of transforming samples into normally distributed data can also be applied in eq 4a such as the johnson system of transformations e g carsel and parrish 1988 or the box cox transformations box and cox 1964 such transformations can also be used to ensure that the ex situ prior lies between physical bounds e g the log transform guarantees that the ex situ prior restricts the probabilities to strictly positive hydraulic conductivity values thus honoring the hard inequality type constraints defined in section 2 3 it is important to note that the statistical model defined in eq 4a is the same for all sites allowing for inter site comparison and assimilation of site specific parameters in general measurements at a site are collected in a clustered way such that numerical estimates are statistically correlated due to the spatial autocorrelation of hydrogeological properties e g pyrcz and deutsch 2003 rubin 2003 if one does not account for spatial correlations this can lead to biased results the data assimilation model can account for possible correlations by using multivariate statistical distributions as the site specific model fy the most common paradigm for modeling a spatial random field srf is a multivariate normal mvn distribution e g rubin 2003 such a spatial model can be summarized by its mean μ as well as its semivariogram function defined by the parameters of spatial variability θi in its most basic form θ i σ y i 2 λ i τ i 2 where σ y i 2 is the variance of yi λi is the integral scale and τ i 2 is the nugget parameter rubin 2003 as can be seen from eq 4 each site must use the same model with only its parameters being site specific as a result a single unique variogram model has to be used to account for inter site variability in the variogram model function a flexible variogram function such as the matérn variogram can be used in actual practice spatial coordinates of measurements may not be provided in hydrogeological data sets in this case no information about the statistical correlation between numerical values can be derived from the available data a common practice here is to make the working assumption of statistical independence gelman et al 2014 if such correlations are present however the unacknowledged data redundancy might lead to an underestimation of intra site variability the second level in the hierarchy describes inter site variability eq 4b site specific parameters ϕi i 1 i are realizations from a random variable φ where the statistical distribution of φ describes how site specific parameters ϕi i 1 i vary between the different sites si i 1 i site specific parameters ϕi are realizations from the generating distribution f φ parameterized by hyperparameters η in general little is known about hyperparameters η thus f η is defined using weakly informative prior distributions so that data d can dominate the shape of the posterior distributions p η d weakly informative prior distributions are prior distributions that ensure proper posterior distributions while minimizing the information they contain the use of weakly informative priors for hyperparameters η is reasonable in this case because our goal is to derive information about the target variable y from the data d so we can be vague about the prior information gelman et al 2014 p 115 their formulation is critical particularly when the number of sites is small fewer than five gelman 2006b having now described the hierarchy the final goal of the algorithm is to derive the ex situ prior f y y d to derive this distribution the site specific parameters ϕi as well as the hyperparameters η are removed by marginalization in as shown here 5a f y y d f y y η f η η d d η with 5b f y y η f y y ϕ f φ ϕ η d ϕ 2 4 3 assimilation of available site specific data following the bayesian paradigm available data d are assimilated by updating the probability distribution of parameters η through bayes theorem i e 6 f η η d f y d η f η η the likelihood in this equation can be derived using the assumption of statistical independence for the different investigated sites this assumption leads to 7 f y d η i 1 i f y i d i ϕ i f φ ϕ i η d ϕ i i e the combined likelihood can be factorized into site specific likelihoods since the data from each site is assimilated in separate functions eq 7 provides the flexibility to assimilate the multiple types of data thus allowing sites with different numbers of measurements to be integrated the derivation of f y i d i ϕ i depends on the kind of information available at site si and is detailed in the following sections of this paper case a d i are numerical values of the target variable y to describe how one estimates f y i d i ϕ i we start with sites having information in the form of numerical values e g measurements point estimates of the investigated target variable y d i y i 1 y i j i to better explicate the different types of numerical data we distinguish between spatial and non spatial data 1 y refers to a non spatial variable for example y can model site specific summary statistics of a spatial variable e g integral scale multiple values for yi can correspond to estimates associated with different assumptions for example multiple variogram formulations can lead to multiple integral scale estimates in this case site specific estimates can be assumed to be independent realizations of the site specific pdf eq 3a in line with the assumption of exchangeability where no information other than numerical values is available for distinguishing the estimates gelman et al 2014 chap 5 2 y refers to a spatial variable for example y can model hydraulic conductivity here d i consists in ji collected numerical values d i j i n d i y i 1 y i j i which may or may not be associated with spatial coordinates x i 1 x i j i and may or may not be associated with parameters describing spatial variability at site si a measurements are associated with spatial coordinates for example y models log conductivity and each realization of y is associated with the corresponding spatial coordinates x in this case we recommend to use a multivariate distribution that accounts for spatial correlation for example with an mvn yi mvn μi θi when parameter values in the multivariate distribution μi θi are known their values can be set when declaring the hierarchical model section 2 4 4 when they are unknown their distribution will be estimated with the algorithm b measurements are not associated with spatial coordinates in this scenario there is insufficient information to quantify the correlation between measurements this is the worst case scenario the working assumption is usually to assume statistical independence case b generalization to other types of data let us now look at sites where information about the investigated variable y comes in a form other than numerical values of the target variable y such as statistical distributions bounds and moments 1 statistical distributions pdfs cdfs and histograms information about the investigated variable y at one site can be presented in the form of statistical distributions instead i e a pdf a cdf or a histogram this is the second case of having site specific hard data as presented in section 2 3 in this situation a sample of numerical values can be drawn from the statistical distribution and can be incorporated into the algorithm following the steps developed in section 2 4 3 here we use latin hypercube sampling lhs to ensure that the generated ensemble is a good representation of the variability that generated the pdf cdf or histogram hou and rubin 2005 mckay et al 1979 the data assimilation algorithm is sensitive to the number of measurements available at each site section 3 1 3 therefore the number of values drawn from the pdf cdf or histogram should correspond to the number of measurements used to generate it when the number of measurements used to derive the statistical distribution is not provided this can lead to biased results 2 bounds when data from site si are available in the form of numerical bounds d i b m i n i b m a x i we follow maximum entropy principles where the corresponding statistical distribution maximizing entropy is the uniform distribution between the lower and upper bounds hou and rubin 2005 woodbury and rubin 2000 in a case where only the lower upper bound is provided the upper lower bound is fixed to the maximum minimum of all values at the other sites alternatively if no values are provided at the other sites it is fixed to the maximum minimum bound of the physical range of the target variable we can then follow similar steps as those presented above in the paragraph on statistical distributions again the algorithm is sensitive to the number of numerical values provided therefore the number of values drawn from the uniform distribution should correspond to the number of measurements taken in the field 3 moments when information at a site is given in the form of moments e g mean variance it can be directly used as estimates of site specific parameters ϕi and can be declared as such when declaring the hierarchical model this is further detailed in section 2 4 4 discussing numerical implementation 2 4 4 numerical implementation we now turn to the numerical implementation of the bayesian hierarchical model section 2 4 2 as well as the data assimilation procedure section 2 4 3 the code is developed in the form of a library rprior developed within the r statistical environment r core team 2017 the library is available for download on github github com kcucchi rprior the library rprior is built around functions provided by the nimble r package which is designed for building analysis methods for bayesian hierarchical models in r nimble development team 2016 it allows for the expression of models and algorithms using a high level language while also maintaining good performance thanks to the use of low level languages like c when performing computationally intensive steps de valpine et al 2017 in this section we briefly describe how rprior applies functions in nimble to derive ex situ priors the bayesian hierarchical model eq 3 is declared using the function nimblecode the model specification is based on the bugs language gilks et al 1994 bugs provides the flexibility to declare a wide range of common parametric statistical distributions covering univariate and multivariate distributions both categorical and discrete and with or without transforms for example the bimodal model can be declared combining the categorical indicator distribution and the continuous gaussian distribution the prior distribution for hyperparameters η are also declared in the nimblecode function nimble provides a flexible way to declare the assimilated data d each declaration creates a node which can be either deterministic or stochastic for example when information at site si is provided in the form of moments the node mu i describing the site specific mean μi is assigned the corresponding numerical value in cases in which μi is unknown the node mu i is defined to be stochastic and is estimated from dependent nodes y 1 y ji according to the statistical distribution declared in the bayesian hierarchical model in the library rprior data d are provided in the form of an r dataframe object declarations of assimilated data are implemented accordingly after the bayesian hierarchical model and the data are declared rprior estimates the posterior distribution of the hyperparameters f η η d step 1 in section 2 4 1 eq 6 f η η d is estimated by generating a large random sample of values that follow this distribution using a markov chain monte carlo mcmc technique as implemented within nimble rprior then estimates f η η d by applying kernel density estimation on samples in the mcmc the next and final step is the derivation of the ex situ prior f y y d step 2 in section 2 4 1 eq 5 to do so each sample of η in the mcmc is used to draw realizations of site specific parameters ϕ in turn each sample ϕ is used to draw realizations of y this provides a sample of y values that follow the distribution f y y d which is then estimated by applying kernel density estimation to the obtained samples 3 results and discussion having described the methods used in this study we now demonstrate how the proposed data assimilation method behaved on a set of synthetic case studies using simulated spatial random fields and simulated field campaigns from these fields we investigated the performance of the data assimilation algorithm with a multiple number of sites a multiple number of measurements per site and with various spatial configurations of the measurements in addition we discuss results obtained when we applied our algorithm to an open access hydrogeological dataset 3 1 synthetic case studies to illustrate the algorithm and familiarize the reader with the assimilation of measurements and the process of prior derivation we start with a set of synthetic case studies to that end we used a varying number of sites and a varying number of measurements per site we then investigated the case of spatially correlated measurements and the role of the integral scale in the derivation of an ex situ prior 3 1 1 setup of the synthetic case studies since we used synthetic case studies to test and validate our algorithm we had to establish an algorithm to simulate field campaigns the overall procedure is illustrated in fig 6 and can be summarized as a four step process in the first step we generated spatially correlated random fields with specified stochastic properties the site specific means μi were assumed to follow the distribution n α 0 τ 0 2 and the spatial correlation structure was assumed to follow a gaussian variogram model with variance σ 0 2 and integral scale λ 0 the hyperparameter values were fixed to η 0 α 0 7 5 τ 0 2 0 25 σ 0 2 0 25 λ 0 1 we quantified the influence of the distance between measurements by sampling the measurements within circles of radii defined in relation to the integral scale λ 0 in the second step we simulated a field measurement campaign by sampling from these spatial random fields to explore the impact of different factors we varied the number of sites i the number of measurements per site j and the spacing between measurements r for combinations of i j and r values we randomly selected i sites and randomly sampled j measurement values from these sites within a circle of radius r λ in the following we call d i j r an example of data that corresponds to i sites j measurements per site and uniformly sampled within radius rλ for each unique combination of i j r we simulated 100 realizations of d i j r in the third step we derived the ex situ prior f y y d i j r for each realization of site specific data using the model presented in eq 3 in the fourth and final step we assessed the performance of the computed ex situ priors f y y d i j r the performance was evaluated by calculating the distance between the ex situ prior and the underlying desired distribution fy y η0 using the kullback leibler divergence kld as defined in eq 8 8 k l d f y d i j r f y η 0 f y y d i j r log f y y d i j r f y y η 0 d y the kld also known as relative entropy measures the divergence i e the non symmetrical distance between two pdfs it thus quantifies the distance between two pdfs and can be used to compare one reference state of knowledge to another state in general two different interpretations of this distance can be found in the literature depending on the direction fig 7 in information theory the kld is interpreted as information loss i e the amount of information that would be lost if instead of the true distribution another distribution is used in contrast in bayesian inference the kld is interpreted in the opposite direction as information gain i e the information gained by updating from the prior to the posterior distribution therefore in this latter interpretation the kld is a direct measure for the information content of the data assimilated during the inference in the following we use the kld in both of these ways depending on the context first in the synthetic examples we know the actual distribution of the underlying population in this case we compare these true distributions to the predictive distribution of our ex situ prior algorithm the values of the kld can thus be interpreted as information loss i e the amount of information that would still be needed for a full statistical characterization of the population however for cases where the true distribution is unknown we use the kld as a measure for information gain i e the information provided by our ex situ prior algorithm in order to avoid any misunderstandings we will clarify the interpretation of the kld in each case 3 1 2 inter site and intra site hierarchy first we demonstrate how the ex situ prior algorithm can handle a different number of measurements per site and how it considers measurements coming from different sites differently than measurements coming from a single site for this investigation we simulated field measurement campaigns and derived the ex situ prior using the framework described in section 3 1 1 here we consider the case where we sampled measurements from only two sites i 2 we sampled a varying number of measurements from site 1 j 1 2 5 10 20 30 and j 2 5 measurements from the second site for each combination of j 1 and j 2 100 realizations were performed for each field campaign realization we computed the ex situ prior obtained when using measurements from site s 1 only from site s 2 only from site s 1 and s 2 combined s 1 s 2 and when considering that measurements at site s 1 and s 2 were collected at a single site s 1 u s 2 fig 8 a shows the ex situ priors derived for a single realization of a field campaign the ex situ data came from two sites s 1 and s 2 with j 1 2 and j 2 5 number of measurements respectively to assess the uncertainty in the ex situ prior we used the variance of the pdf as a measure as seen fig 8a its value is similar for the cases s 1 s 2 and s 1 u s 2 but noticeably smaller for s 1 s 2 this can be explained by the fact that in the cases s 1 s 2 and s 1 u s 2 the algorithm was not able to estimate inter site parameters and thus the variability in the obtained ex situ prior was consequently very large when assimilating data from the two sites the algorithm was able to at least estimate this inter site variability to some degree which resulted in a narrower ex situ prior to investigate whether this behavior held up more generally we performed a series of field measurement campaigns with a varying number of measurements fig 8b to have a significant sample size for our analysis we used 100 realizations per configuration we again assessed the information gain by computing the kld between the non informed prior and each ex situ prior we found the same general behavior pertaining to all values of j 1 i e the kld was always significantly higher for s 1 s 2 compared to any of the other configurations fig 8b these results indicate the higher value of a dataset with high inter site variability i e many measurements from different sites compared to a dataset with high intra site variability i e many measurements from few sites or just one site this difference can be explained by the goal of the ex situ prior for predicting the target variable at a new site since this new site is a random sample from the population of all considered sites it is more important to get a grasp on the variability of the population of sites i e the inter site variability rather than the intra site variability of just a few sites or even a single site as was the case here 3 1 3 influence of the number sites and number of measurements per site now we turn to the influence of the number of sites and number of measurements per site on the performance of the ex situ prior it stands to reason that the performance of the ex situ prior will improve with the number of sites as well as with the number of measurements per site it is however not immediately clear how each factor will contribute overall to assess this relative impact we focus on interpreting the results in terms of predicting both the ex situ prior f y y d i j and the predictive distribution of the site specific mean f μ μ d i j to that purpose we performed a synthetic study described in section 3 1 1 we fixed the value r λ 10 corresponding to a case where the measurements were sampled over a large region with respect to the integral scale i e where little to no data redundancy was assumed measurements were sampled from a number of sites i between 2 and 30 with the number of measurements per site j varying between 2 and 30 for each combination of i and j we simulated 100 measurement campaigns d i j and calculated the corresponding 100 ex situ priors f y y d i j fig 9 a shows the variation of the kld as a function of the number of sites i and the number of measurements per site j here the kld measures the information loss in the target variable y in this figure the lines represent the smoothed variation of the kld with the number of sites i grouped by the number of measurements per site j as expected the kld decreased with the number of sites i and the performance of the ex situ priors improved as the number of sites used in the data assimilation increased moreover for a fixed number of sites i the kld decreased when the number of measurements per site j increased fig 9b shows the variation of the kld as a function of the number of sites i and the number of measurements per site j here again the kld measures the information loss for the distribution of the site specific means μ as in the scenario above the information loss decreased with the increasing number of sites the sensitivity of the results with respect to the number of measurements per site was low when compared to the number of sites where all lines were located within similar confidence intervals it is also interesting to note that the decrease in information loss as a function of the number of sites was more pronounced for a low number of sites than for a high number of sites these results illustrate how limitations due to a small number of measurements per site can be compensated for by increasing the number of similar sites used in the data assimilation for example an average kld of 0 25 can equivalently be obtained by assimilating 30 measurements per site from 18 sites or alternatively 2 measurements per site from 25 sites this shows that while both factors contribute positively the relative contribution is quite different and depending on the available dataset very different findings can result from the same amount of raw data 3 1 4 derivation of the parameters of a gaussian srf up to this point we have seen how the ex situ prior model can provide an estimate of the statistical distribution of conductivity values however due to the spatial correlation of conductivity fields higher order statistical moments need to be taken into account for a meaningful representation of real world aquifers by far the most used paradigm in spatial statistics is that of a gaussian spatial random field srf also known as a gaussian process or a multivariate gaussian field gelfand 2012 gelfand and schliep 2016 gaussian srfs are used widely due to being very parsimonious extremely flexible and hierarchical in nature they are parsimonious since in their most basic form they are fully parametrized by specifying their mean variance covariance model function and integral scale they are also flexible because additional features such as anisotropy or a non orthogonal axis can be introduced finally their hierarchical nature permits their conditioning to direct point measurements to demonstrate that our algorithm can be used in conjunction with a gaussian srf we extended the formulation of our hierarchical model with a spatial covariance matrix as the model function for the variogram we chose the exponential model this restriction on a single model function was seen as reasonable since several studies have indicated that the specific form of this function has limited influence compared to its parameters heße et al 2015 jafarpour and tarrahi 2011 riva and willmann 2009 after the revision of the ex situ prior model we used the same procedure described above to derive the predictive distributions of the parameter values to derive additional estimates for the distribution of the integral scale λ we had to amend our synthetic data set with spatial coordinates for our measurements the number of measurements per site was j 30 and the number of sites varied between i 3 13 30 in general again we saw good convergence of the estimated distributions for increased i see rows in fig 10 for each parameter of the gaussian srf in particular the behavior of the mean μ and variance σ was similar to the previous results although the latter showed some bias such that the true value was somewhat underestimated this bias was most likely caused by the nature of likelihood based estimation mardia et al 1999 it is known that the expectation of the likelihood estimate of a variance with a sample size of n is given as n 1 n times the true value thereby causing systematic underestimation frequentist inference usually employs a bias correction when using likelihood based estimation known as bessel s correction whereas those using bayesian reasoning are generally less concerned with this bias inherited from the likelihood estimator for a discussion on the topic see e g gelman 2006b 2008 gelman et al 2014 however the main new feature of our extended ex situ prior model was the spatial covariance function with the integral scale as its defining parameter infererring this parameter proved to be difficult especially compared to the other two parameters compare the third row in fig 10 to its first two rows although increasing the number of sites improved the estimates our results still showed bias as well as inaccuracies even for the scenario of i 30 our investigation into the inference of spatial correlations already allows us to make a number of relevant observations first achieving convergence for higher order statistics like the integral scale is data demanding compared to simple first order statistics since earlier studies have demonstrated a low sensitivity to the integral scale e g firmani et al 2006 this seems to be an inherent problem of spatial statistics second all our results were drawn from idealized virtual data with no structural errors present i e our conductivity fields were actually gaussian srfs with an exponential covariance function in reality both of these criteria may be violated to some extent in which case a different geostatistical paradigm may be needed all alternatives to gaussian srfs are however less parsimonious linde et al 2015 according to our results such an increase in the number of parameters will also lead to a strong increase in the amount of data needed for a full characterization 3 2 application to an established dataset having investigated the properties of our ex situ prior model with synthetic data we next applied the data assimilation framework to the world wide hydrogeological parameters database wwhypda comunian and renard 2009 with the objective of predicting the univariate log transformed hydraulic conductivity distribution y log 10 k at a target site in addition we also estimated the predictive distributions for site specific means μ e log 10 k and site specific standard deviations σ σ log 10 k at the target site the wwhypda is an open collaborative effort to gather dispersed and difficult to access hydrogeological information comunian and renard 2009 the data is accessible online through their website http wwhypda org and is available to users contributions amongst other meta information each measurement reported in the database is associated with a unique field site identifier where each field site is in turn associated with one or several material types and environment types the wwhypda provides the option of reporting the spatial location of each measurement however in the database no spatial coordinates are actually reported to better access the database we transformed the sql tables into an r dataframe object via the r package rprior following the usual assumption that hydraulic conductivity is log normally distributed within a site e g freeze 1975 we selected hydraulic conductivity measurements from the database and applied the logarithmic transform we defined four material types of interest sand silt gravel and clay we then grouped the measurements by the selected types the groups were defined depending on whether the material type was mentioned in the type description of the site for example measurements from a site labeled as silty clay were used for deriving both silt and clay ex situ priors this approach was selected to maximize the number of measurements assimilated within each material group the number of measurements reported for each material type and the corresponding number of field sites are reported in table 1 the histograms of the measurements are presented in fig 11a we found that predicted ex situ priors of the hydraulic conductivity values differed noticeably between types such that silt showed the lowest values followed by clay sand and finally gravel fig 12 a differences were also found in the prediction certainty as expressed by the width of the distribution for each type fig 12b certainty in the ex situ prior was highest for gravel followed by sand clay and silt the factors that affected these differences included the differences in assimilated measurements themselves as well as the size of the data set used when deriving the ex situ priors for instance gravel which had the highest prediction certainty had measurements showing less variability overall when compared to the three other types on the other hand the measurements for sand and silt showed a similar range of variability in their measurements but had different prediction certainty in the ex situ prior in this case the difference was caused by the different number of measurements with sandy materials having 1122 measurements from 17 different field sites whereas the silty material had only 353 measurements from 5 distinct field sites given the results above we argue that the difference in the number of sites rather than in the total number of measurements caused the resulting differences in prediction certainty fig 11b extends the discussion by showing the derived ex situ priors for each material type alongside the distributions derived by using the alternative density estimates presented in section 2 2 in all cases the ex situ priors showed higher uncertainty i e wider densities than alternative methods such an increase in uncertainty for a supposedly improved method may seem counter intuitive however this behavior is not unusual for instance a well known and quite similar example is the notion of pseudoreplication hurlbert 1984 where hidden correlations in the data cause the effective number of independent samples to be much lower than the raw number of samples methods that do not account for such hidden correlations consequently unduly inflate the certainty in the results in our research one notable exception to the general trend was the case of the sandy material for which the ex situ prior was close to the distributions obtained by applying the work of carsel and parrish 1988 and the maximum entropy approaches this may relate to the fact that the measurements came from 17 sites and therefore the overall distribution of measurements was representative of inter site variability for all other material types the measurements provided came from just five or six sites the corresponding ex situ priors recognized the underlying uncertainty by deriving a distribution with larger variance compared to the distribution obtained by other methods which by design cannot account for the inter site variability in all cases the kernel density estimate closely followed the underlying distribution of the measurements and it overfitted the predicted distribution to the measurements that were used to derive predictive distributions fig 12b and c show the derived pdfs for the site specific means and standard deviations respectively these are the pdfs for the level 1 parameters φ fig 5 pdfs for means of log transformed hydraulic conductivity fig 12b were almost as diffuse as the univariate distributions fig 12a by definition of the hierarchical model eq 3a the univariate log transformed hydraulic conductivity is normally distributed with mean μ fig 12b and standard deviation σ fig 12c therefore in this case and for all material types the parameters describing inter variability had higher variance than the parameters describing the intra site variability and the shapes of univariate distributions were dominated by the shapes of the inter site means this suggests the uncertainty represented by the ex situ priors i e the width of the ex situ priors could be decreased by a more careful selection of similar groups of hydrogeological sites in summary in this work we demonstrated how sites with similar material types can be selected as similar to the target site and how measurements from these similar sites can be used to derive ex situ priors for univariate log transformed hydraulic conductivity as well as the mean and variance of the variogram model at the target site 4 conclusion the goal of this study was to introduce a statistical framework for assimilating different types of ex situ data coming from multiple sites into a predictive prior for a hydrogeological variable at a target site we called this predictive prior ex situ prior the properties of the proposed framework were c 1 that assimilated data are ex situ only c 2 that an additional level of variability is associated when data come from different sites and c 3 that multiple types of site specific data can be assimilated to the best of our knowledge this is the first time that an algorithm with such properties has been introduced for predicting statistical distributions of hydrogeological variables while the examples and applications presented in the study are relatively simple they illustrate the opportunities offered by the algorithm in particular we proposed a complete framework for predicting parameters of univariate and bivariate statistics for log transformed hydraulic conductivity at a target site we then demonstrated the derivation of predictive univariate and bivariate statistics on a synthetic case study and illustrated their derivation using measurements from the wwhypda other predictions relevant to models of spatial variability are not covered in this paper such as predicting the value for the nugget parameter in the variogram or predicting the form of the variogram e g exponential gaussian nonetheless the proposed approach could be applied to these problems by modifying the formulation of the hierarchical model predicting the form of the variogram at a target site could be accomplished by either modifying the data assimilation algorithm to allow for the assimilation and the prediction of categorical variables related to the variogram forms or by using the matérn formulation of the variogram and predicting the value of the smoothness parameter these investigations however are outside the scope of the current study although they are likely to be the focus of subsequent work we note that a big hindrance to the development and applicability of the method is the availability and quality of assimilated data in particular this would be true for anything related to scale dependency since wwhypda lacks information pertaining to this topic scale dependency is relevant for quantities like mean and variance of the hydraulic conductivity as well as for the integral scale itself whose very definition is often highly contingent on the defined support scale depth of measurement is another piece of information that is missing from estimates in wwhypda we therefore view the application examples as a proof of concept to show the applicability of the proposed approach to address this topic we hope that more work will be performed to developing more comprehensive readily and easily accessible databases of hydrogeological parameters here the wwhypda introduced by comunian and renard 2009 is a significant step in the right direction these databases would detail the measurement scale of different hydrogeological quantities such that scale dependency could be accounted for in the data assimilation by rescaling the estimates on a common support e g dagan et al 2013 fiori et al 2011 such hydrogeological datasets would increase the predictive capabilities of data assimilation algorithms such as the one presented in this study which in turn could foster wider collaborations among researchers and practitioners toward the development of such databases another limitation of the presented applications is the dependence on the formulation of statistical distributions in the hierarchical model eq 3 the choice of a gaussian model was motivated by its popularity and simplicity bearing in mind that the data assimilation algorithm would be applied to datasets of small to moderate size we mentioned that the algorithm could be adapted to other more complex statistical distributions depending on the properties of the target variable and on the quality of the data the current formulation that uses a gaussian model dictates the unimodal behavior of the ex situ priors derived in the study more complex behavior could be predicted by assuming more complex statistical models in the hierarchical model although more data would be needed for this inference in our work ex situ prior results depended on values assimilated by the algorithm and in particular on the choice of similar sites in the presented application section 3 2 we decided to define similarity based on material type however other hydrological factors of interest such as the support volume or the depth of measurement also influence the value and variability of hydraulic conductivity estimates we can thus expect that accounting for such factors would lead to more precise predictions of hydrogeological variables again recognizing that the possibility of doing so would rely on the existence of datasets containing such information data assimilation algorithms like the one presented in this paper provide frameworks for making predictions based on multiple subgroups and test feature relevances with the goal of assimilating relevant data to guide the selection of similar sites as obtaining hydrogeological data is time consuming and expensive there has been limited incentive for researchers and practitioners to share their data in a structured and easily accessible form comunian and renard 2009 it is our hope that the introduced ex situ prior algorithm and its implementation in the r package rprior will trigger the development of open databases so that better use of information gained in previous investigations can be made in the future acknowledgments for this study karina cucchi was financially supported by the jane lewis fellowship from the university of california berkeley falk heße was financially supported by the deutsche forschungsgemeinschaft via grant no he 7028 1 2 nura kawa was financially supported by the deutsche forschungsgemeinschaft via the sonderforschungsbereich crc 1076 aquadiva the authors thank dr renard and dr comunian for their guidance in using the wwhypda the authors also acknowledge the helpful comments and suggestions from two anonymous reviewers supplementary material supplementary material associated with this article can be found in the online version at doi 10 1016 j advwatres 2019 02 003 appendix a supplementary materials supplementary data s1 supplementary raw research data this is open data under the cc by license http creativecommons org licenses by 4 0 supplementary data s1 
687,simulation of flood inundation with primo systematically captures many complex flooding processes including compound flooding such as a storage behind sepulveda dam overtopping of its spillway and the combine effects of fluvial and pluvial flooding downstream b flow over a series of 17 drop structures along the very steep san gabriel river and storage behind santa fe dam c spreading of los angeles river floodwater and pluvial flooding in downtown los angeles which is built on alluvial fan and d the combined effect of fluvial pluvial and coastal flooding around long beach and the ports of los angeles and long beach fig 13 table 1 baldwin hills test case wall clock times and flood extent accuracy metrics table 1 α t wall min fa fo fu 2 1090 8 71 8 7 7 20 5 5 82 2 70 8 7 9 21 3 10 19 2 68 9 9 7 21 5 20 5 1 60 4 15 4 24 2 table 2 los angeles test case parameters and wall clock times table 2 i ηb t t wall nproc in hr ft hrs α n s flux n s vol n s fric min 1 608 1 0 8 0 12 5 3 6 6 380 2 608 1 0 8 0 12 10 5 11 11 66 3 608 1 0 8 0 12 20 5 11 11 4 8 4 608 1 0 8 0 12 50 5 11 11 0 25 primo parallel raster inundation model brett f sanders a b jochen e schubert a a department of civil and environmental engineering uc irvine irvine ca 92697 usa department of civil and environmental engineering uc irvine irvine ca 92697 usa b department of urban planning and public policy uc irvine irvine ca 92697 usa department of urban planning and public policy uc irvine irvine ca 92697 usa corresponding author at department of civil and environmental engineering uc irvine irvine ca 92697 usa dept of civil and environmental engineering uc irvine irvine ca 92697 usa simulation of flood inundation at metric resolution is important for making hazard information useful to a wide range of end users involved in flood risk management and addressing the alarming increase in flood losses that have been observed over recent decades however high data volumes and computational demands make this challenging over large spatial extents comparable to the metropolitan areas of major cities where flood impacts are concentrated especially for time sensitive applications such as forecasting and repetitive simulation for uncertainty assessment additionally several factors present difficulties for numerical solvers including combinations of steep and flat topography that promote transcritical flows the need to resolve flow in relatively narrow features such as drainage channels and roadways in urban areas which channel flood water during extreme events and the need to depict compound hazards resulting from the interaction of pluvial fluvial and coastal flooding a new flood inundation model is presented here to address these challenges the parallel raster inundation model primo solves the shallow water equations on an upscaled grid that is far coarser than the underlying raster digital topographic model dtm and uses a subgrid modeling approach so that the solution benefits from dtm scale topographic data additionally an approximate riemann solver is applied in an innovative way to integrate fluxes between cells as needed to update the solution by the finite volume method which makes the method applicable to subcritical supercritical and transcritical flows primo is implemented using a two dimensional domain decomposition approach to single process multiple data spmd parallel computing and overlapping communications and computations are implemented to yield ideal parallel scaling for well balanced test cases with both a subgrid model and ideal parallel scaling the model can scale to meet the demands of any application several benchmarks are presented to demonstrate predictive skill and the potential for timely whole city metric resolution flooding simulations limitations of the methods and opportunities for improvements are also presented keywords flood inundation modeling godunov type scheme finite volume method parallel computing digital elevation model upscaling flood hazards flood risk 1 introduction flooding has emerged as the most significant natural hazard facing society jongman et al 2012 over the last decade flooding accounted for half of all weather related disasters and affected 2 3 billion people cred 2015 the u s set a record in 2017 with 300 billion in disaster losses from hurricanes and flooding noaa 2018 and over the summer of that year a humanitarian crisis emerged in nepal bangladesh and india where more than 1000 people died and at least 41 million were affected by monsoon flooding and landslides gettleman 2017 additionally the human health risks of flooding are significant and unacceptable cred 2015 di baldassarre et al 2010 few et al 2004 watts et al 2015 several trends including rising sea levels urbanization deforestation and rural to urban population shifts are expected to increase the impacts of flooding in the future hallegatte et al 2013 jongman et al 2012 flood inundation modeling is taking on an increasingly important role in flood risk management and disaster risk reduction initiatives djordjević et al 2011 hénonin et al 2015 sanders 2017 while the use of hydraulic or hydrodynamic flood models has traditionally been limited to engineering design e g sizing of culverts placement and height of levees implementation of flood insurance programs and support of dam safety programs many more applications are emerging as communities a shift focus from controlling flooding hazards to minimizing flooding consequences southgate et al 2013 and b embrace participatory methods for planning preparedness and design whereby experts and stakeholders co develop interventions heimhuber et al 2015 maskrey et al 2016 smith et al 2017 flood simulations are powerful tools for anyone to immediately grasp the consequences of flooding based on familiar reference points and to identify steps that can be taken to minimize impacts additionally flood simulations can help to orient diverse groups of stakeholders around flood risks and facilitate dialogue about how to manage it djordjević et al 2011 lane et al 2011 indeed flood simulations offer support for all phases of the disaster management cycle including mitigation planning preparedness early warning emergency response and recovery mackay et al 2015 sanders 2017 wilkinson et al 2015 what is critical when using flood simulations for flood risk management is the iterative engagement of stakeholders early stakeholders involvement is important for building trust and configuring simulations around priority issues facing communities evers et al 2012 luke et al 2018 repeated engagement is important for ensuring that model output is tailored to local decision making needs djordjević et al 2011 evers et al 2012 luke et al 2018 meyer et al 2012 sanders 2017 for simulations to depict flooding in an intuitive way that anyone can understand an accurate geometrical model of flood zones provided by a digital elevation model is essential and high quality data are increasingly available from laser scanning lidar and photogrammetry bates 2012 sampson et al 2012 schumann et al 2009 moreover there have been recent calls to improve the availability of high quality data globally based on its importance for flood management schumann et al 2014 however using metric resolution data becomes increasingly challenging as the spatial extent of the modeling domain increases the memory of computing systems is strained by the vast number of points that need to be resolved and the workload of processors is intense based on the number of points and flood model requirements for a short time steps seconds or less for accuracy and stability this has motivated several lines of recent flood inundation modeling research including more efficient solution algorithms bates et al 2010 cea and bladé 2015 parallel computing methodologies brodtkorb et al 2012 castro et al 2011 neal et al 2010 sanders et al 2010 vacondio et al 2014 and subgrid models sometimes called porosity models for aggregating the effects of fine scale topographic data at a relatively coarse resolution where flow calculations are made casulli and stelling 2011 defina 2000 guinot et al 2017 hénonin et al 2015 neal et al 2012a özgen et al 2016 sanders et al 2008 soares frazão et al 2008 stelling 2012 volp et al 2013 version 5 0 of the popular hec ras model u s army corps of engineers hydraulic engineering center davis ca now supports two dimensional flood inundation modeling and uses an implicit finite volume method with a subgrid model for topography similar to the scheme reported by casulli and stelling 2011 models based on implicit finite volume or difference schemes are especially advantageous when there are slowly varying subcritical flows because the time step is restricted based on the fluid velocity not the speed of gravity waves as with explicit models and thus solutions can be attained using fewer time steps and fewer computations however the advantage is diminished or reversed compared to explicit methods when flow is supercritical changing quickly or when there are flood fronts moving across the land surface under these circumstances an implicit solver may need to use several iterations per time step to converge and the allowable time step may become smaller moreover explicit schemes tend to support better parallel scaling because all calculations are made locally compared to implicit schemes which rely on the solution of a large matrix problem to simultaneously update the solution over the entire flow domain and thus parallel efficiency is closely tied to the parallel efficiency of the matrix solver used the lisflood fp model uses an explicit scheme for solving a simplified form of the shallow water equations and has received widespread use for large scale flood inundation modeling with moderate resolution 10 90 m digital terrain models dtms bates et al 2010 neal et al 2012a wing et al 2018 and it has been configured for parallel execution with shared memory hardware distributed memory hardward and graphical processing units neal et al 2010 several variants of lisflood fp have been reported with different features including schemes that use an approximate riemann solvers to account for transcritical flow neal et al 2012b and the version of lisflood fp used for large scale flood hazard model is designed to resolve subcritical flows based on its simplified formulation neal et al 2012a wing et al 2018 in summary there are many different approaches for simulating flood inundation using different shallow water equations different subgrid models and different numerical solvers but thus far there has been limited success developing models that apply to all classes of flow subcritical supercritical and transcritical and all sources of flooding pluvial fluvial and coastal and that efficiently scale on parallel computing systems with hundreds or even thousands of processors models that respond to this challenge could have tremendous impact as a flood risk management tool communicating flood hazards to a wide range of end users in support of all phases of the disaster management cycle and especially serving the presently unmet needs for real time decision support during major floods luke et al 2018 sanders 2017 this paper presents a flood simulation model that solves the shallow water equations by an innovative godunov type finite volume algorithm the model is grounded in several conventional features of godunov type finite volume schemes including use of the hllc solver to estimate fluxes at cell edges toro 2001 fluxes and bed slope source terms that are well balanced to prevent spurious acceleration valiani and begnudelli 2006 and the implicit treatment of friction terms for accuracy and stability xia et al 2017 hence this paper emphasizes aspects of the model that enable relatively fast execution in large scale applications including a a single process multiple data spmd parallel implementation based on domain decomposition which limits both memory and workload on a per processor basis b a subgrid model that supports a relatively coarse grid solution update and c innovative optimizations of the subgrid model that further reduce computations without overly sacrificing accuracy optimization involves the sampling of topographic data in the subgrid model which reduces the required number of calls to the computationally demanding approximate riemann solver and enables faster execution of look up tables the model is designed to load elevation and resistance data from commonly used raster formatted digital topographic models dtms making it relatively easy to set up compared with unstructured grid models that are constrained by important land features like levees and building walls e g bilskie et al 2015 hagen et al 2001 schubert and sanders 2012 schubert et al 2008 this inspires the model name parallel raster inundation model primo primo is capable of resolving subcritical supercritical and transcritical flows based on its use of an approximate riemann solver to estimate fluxes and while it is formally first order accurate it considers slopes in the free surface elevation when reconstructing fluxes at cell edges which improves accuracy and also enables more accurate downscaling for improved visualization of results applications of the model are presented to verify the model including its mass conservation and well balanced properties document strengths and weaknesses and report benchmark performance standards useful for informing the readiness of the method for large scale and or real time modeling applications where computational efficiency is paramount 2 model formulation primo solves the nonlinear shallow water equations which predict water level and horizontal velocities under the assumption of a hydrostatic pressure distribution and turbulent momentum dissipation scaled by a manning resistance coefficient the integral form of the governing equations can be written for a two dimensional 2d domain ω with boundary γ and unit outward normal vector n n x n y t as follows valiani and begnudelli 2006 1 t ω u d ω γ f z n d γ ω r d ω ω p d ω where u d u d v d t d depth u x velocity v y velocity f f x f y where 2 f x u d u 2 d 1 2 g d 2 u v d and f y v d u v d v 2 d 1 2 g d 2 and finally z r and p are related to traditional source terms of the shallow water equations bottom slope resistance and the effective precipitation rainfall converted to runoff respectively and are given as follows 3 z z x z y 0 0 1 2 g d η o 2 0 0 1 2 g d η o 2 r g d 0 s f x s f y p p 1 0 0 where g gravity s f x and s f y represent x and y direction friction slopes respectively d η o η o z and represents the depth based on a cell wise constant free surface elevation ηo and p is the effective rainfall rate the form of the shallow water equations appearing in eq 1 is chosen because it simplifies balancing of fluxes and source terms when needed to preserve stationary solutions valiani and begnudelli 2006 in essence the computation of f z along each edge of a flow model cell represents a difference between the fluxes computed by the riemann solver and hydrostatic fluxes associated with the ground slope source term in the limit of a stationary solution these two terms are identical and thus f z 0 on an edge by edge basis additional detail is provided in section 2 4 2 1 data structure similar to previously reported models involving upscaling of topographic data hénonin et al 2015 volp et al 2013 primo is designed to load a raster grid of elevation data z with cell size δ z in both the x and y directions and to solve eq 1 on a relatively course grid with a cell size δ u α δ z where α is an integer termed the upscale factor to succinctly differentiate between grids in the narrative that follows we will refer to the fine resolution grid as the z grid and the coarse resolution grid as the u grid primo also accommodates a raster grid of manning resistance parameter values nm with cell size δ z fig 1 shows the u grid red superimposed on the z grid gray based on α 10 as well as the indices for the z grid i j and the indices for the u grid im jm the origin for the indices is the lower left hand corner of the grid whereas raster formatted data commonly uses an origin in the upper left hand corner of the grid with row indices that increase in the downwards direction hence careful attention to the ordering of data points is needed when reading and writing gridded data based on this number system topographic data within model cell im jm are indexed by i i m 1 α 1 i m α and j j m 1 α 1 j m α however to simplify the presentation in the material that follows i and j will always be presented as local indices to reference the α 2 z values located within an individual u grid cell because edges of the flow model cells are not aligned with the center of topographic data cells there is ambiguity in the definition of topography data used to compute fluxes and different approaches can be taken for example volp et al 2013 used all topographic data from a δ u δ u domain centered on the edge in the context of a semi implicit finite volume scheme for a godunov type scheme it is important to define a specific cross sectional shape connecting two neighboring u grid cells across which an approximate riemann solver can be applied therefore a set of α edge elevation points z k k 1 α are computed as follows 4 z k max z k min l z min r z where 5 z k 1 2 z k l z k r here z k l and z k r represent elevation in the z grid cell to the left and right respectively of the kth segment of the u grid edge and min l z and min r z represent the minimum elevation among the α 2 elevation data points in the u grid cells on the left and right respectively of the u grid edge hence z values along u grid edges are taken as a simple average of neighboring z grid data points unless the average is lower than the minimum elevation of z data within neighboring u grid cells this condition prevents the reconstruction of positive depth values d 0 at u grid edges when adjacent cells are dry no storage which will lead to spurious fluxes as will be described next the free surface elevation in u grid cells is set equal to the height of the lowest of its α 2 z values when the cell is dry the discrete solution represents a cell average over the u grid cell ω i m j m with area δ u 2 and includes information about storage per unit area h and cell average discharge per unit width in the x direction p and y direction q as follows 6 u i m j m h p q i m j m where 7 h i m j m 1 δ u 2 ω i m j m d d ω 8 p i m j m 1 δ u 2 ω i m j m u d d ω q i m j m 1 δ u 2 ω i m j m v d d ω the notation h is introduced to distinguish the total storage of water in each u grid cell an integral property of the solution from the pointwise depth d which varies spatially within the cell and is downscaled to the z grid resolution similarly the notation p and q is also used to designate integral cell average properties of the solution in contrast to the pointwise variables ud and vd respectively to link water level η and storage h in each u grid cell storage curves are computed by summing over z grid cells as follows 9 h η 1 α 2 i 1 α j 1 α max 0 η z i j eq 9 assumes a horizontal water level within each u grid cell which is a good approximation of most floods but for improved accuracy primo considers slopes in the water level δxη and δyη when updating the solution in time as reported by begnudelli 2016 slopes are computed using the minmod limiter e g sanders and bradford 2006 for example to compute slopes in the x direction first the left and right slope are computed separately as follows 10 δ x η l a η i m j m η i m 1 j m δ x η r b η i m 1 j m η i m j m and in a second step the minmod limiter is applied to give the slope 11 δ x η i m j m sgn a min a b if a b 0 0 otherwise this procedure is repeated in the y direction to compute δ y η i m j m we note that primo does not compute slopes in discharge values as is common in second order accurate godunov type finite volume schemes we summarize the main data structures in primo as follows topographic data z and optionally manning resistance parameters nm are raster fields with a cell size δ z and topographic data along edges of the u grid z are also saved with a cell size of δ z the solution to the shallow water equations is given by h p q η δxη and δyη which is saved on a the u grid with resolution δ u α δ z where α is the integer upscale factor to downscale the solution to the z grid resolution at location xi yj the free surface elevation η i j is computed as follows 12 η i j η δ x η x i x c δ y η y j y c where xc yc represents the cell center coordinates of the u grid cell the downscaled depth is subsequently computed as d i j max η i j z i j 0 2 2 solution update scheme the solution is updated using a godunov type finite volume scheme which involves the application of approximate riemann solvers to compute mass and momentum fluxes however a distinguishing feature of primo is that the approximate riemann solver is applied α times along each u grid edge once for each z grid edge and summed to give the integral along the boundary as indicated by eq 1 hence primo is designed to leverage the fine resolution topographic data in godunov type finite volume schemes the computation of fluxes by approximate riemann solvers is one of the most computationally demanding steps in a traditional model with n n cells for simplicity there are 2 n 2 2 n edges where the riemann solver is applied and n 2 cells where the solution is updated however by limiting flux calculations to the u grid edges the required number of calls to the riemann solver is reduced to 2 n 2 α 2 n and the number of solution updates is reduced to n α 2 hence calls to the riemann solver and solution updates are reduced by a factor of α and α 2 per time step we will show later that the number of riemann calls can be reduced further with a sampling technique whereby fewer than α calls to the riemann solver are made to integrate the flux along the u grid edge as an aside the number of riemann calls can also be reduced by using a cross sectionally averaged riemann solver that is called only once per cell edge but this requires the introduction of look up tables for interpolating the cross sectional geometry as a function of water level which adds to the computational effort further investigation is left for a future study the solution is updated by δt from time level n to n 1 using a fractional step method such that friction terms are treated implicitly and therefore do restrict the time step beyond what is required for stability in the absence of friction in the first step the solution is advanced to an intermediate step by based on the flux terms as follows 13 u i m j m u i m j m n δ t α δ z k 1 α δ f x l k i m 1 2 j m k 1 α δ f x r k i m 1 2 j m k 1 α δ f y l k i m j m 1 2 k 1 α δ f y r k i m j m 1 2 n where 14 δ f x l f x z x l δ f x r f x z x r 15 δ f y l f y z y l δ f y r f y z y r and the superscripts l and r indicate that z is evaluated on the left and right hand side respectively relative to an observer positioned with the cardinal directions i e either x or y positive to the right here the summation notation reflects the integration of fluxes over each u grid cell edge based its α values of z in the second step the solution is advanced to the next full time level by integrating the friction terms implicitly through a two point iteration as follows begnudelli 2016 16 u i m j m u i m j m m i m j m n 17 u i m j m n 1 u i m j m m i m j m where 18 m 1 0 0 0 1 r 1 0 0 0 1 r 1 here r is a dimensionless flow resistance parameter that is described in the next section iteration becomes increasingly important when the depth is very small use of two iterations was found to be adequate in the previously reported upscale type models begnudelli 2016 while others developing godunov type flood inundation models have recommended use of the newton raphson method to ensure that iterations are adequate xia et al 2017 2 3 resistance discretization the resistance parameter r follows from an implicit discretization of the source term eq 3 that is unconditionally stable when solving the classical shallow water equations the resistance parameter is commonly evaluated as follows 19 r n δ t g n m 2 v n h n 4 3 where v n u n 2 v n 2 1 2 however when there is a distribution of topographic heights within a u grid cell and a distribution of resistance parameters eq 19 can lead to significant errors in the estimate of the friction slope stelling 2012 several researchers have formulated expressions designed to aggregate the effects of a spatial distribution of depth velocity and empirical resistance parameters such as manning strickler or chézy defina 2000 stelling 2012 volp et al 2013 here we assume that the friction slope and water level is spatially uniform within each u grid cell and that the manning equation can be applied individually to each z grid cell based on d i j and nm i j stelling 2012 volp et al 2013 hence starting with the manning equation for the magnitude of the discharge per unit width p 2 q 2 1 2 s f 1 2 d 5 3 n m the average discharge with each u grid cell follows as 20 p 2 q 2 1 2 s f f 1 2 σ f where 21 σ f 1 α 2 i 1 α j 1 α d i j 5 3 n m i j expressions for the friction source terms in the x and y direction appearing in eq 1 follow as 22 g d s f x g h p p 2 q 2 1 2 σ f 2 g d s f y g h q p 2 q 2 1 2 σ f 2 where the overline reflects a cell average and we note that h p and q appearing on the right side of these equations represent u grid cell average quantities finally the dimensionless flow resistance parameter r required by eq 16 is given by 23 r n δ t g h n 1 p n 2 q n 2 1 2 σ f n 1 2 and the expression used for eq 17 is given by 24 r δ t g h n 1 p 2 q 2 1 2 σ f n 1 2 note that checks are needed to avoid division by zero application of eq 23 is computationally demanding because of the need to sweep over α 2 topographic data points z grid cells for each u grid cell run time computational costs are minimized by creating look up tables for σf as a function of η in a pre processing step in the construction of lookup tables inclusion of basis points comparable to the finest resolved depths is important for accuracy and stability xia et al 2017 2 4 computation of fluxes the first step of the solution update eq 13 involves the summation of α separate fluxes for each u grid cell edge calculation of fluxes is nearly identical in the x and y directions so the description here is limited to the x direction as follows 25 δ f x l k f k h z x l k δ f x r k f k h z x r k where f h denotes application of the harten lax and van leer hllc riemann solver as described in the next section and the superscripts l and r imply reconstruction of the solution from data contained in u grid cells to the left and right of the cell edge respectively to apply the approximate riemann solver for each value of z along the u grid edge the depth on the left and right side is reconstructed as follows 26 d k l max η l 1 2 δ x η l z k 0 d k r max η r 1 2 δ x η r z k 0 and the discharge per unit width is reconstructed based on a tolerance for depth δ w 1 10 6 m as follows 27 p k l r p l r d k l r δ w 0 otherwise 28 q k l r q l r d k l r δ w 0 otherwise on the other hand to compute the bottom slope terms zx l and zx r cf eq 3 the depth is reconstructed assuming a horizontal free surface 29 d η o k l max η l z k 0 d η o k r max η r z k 0 an important consequence of the above discretization of fluxes and bottom slope source terms is that δfx l δfx r δfy l and δfy r are identically zero at every z grid edge for stationary solutions defined by δ x η 0 δ y η 0 p 0 and q 0 2 4 1 hllc solver the harten lax and van leer hll solver is a well known approximate riemann solver that has been applied extensively in godunov type shallow water models and the hllc solver is an extension for two dimensional flows that contain a shear wave toro 2001 implementations of the hllc solver differ depending on how wave speeds are computed and treatment of dry cells and the approach used here is what toro 2001 p 182 183 describes as a simpler version as follows when both the left and right side cell is dry f hll 0 otherwise fluxes in the x direction f h f 1 h f 2 h f 3 h t are computed as follows 30 f 1 2 h f 1 2 l s l 0 f 1 2 r s r 0 s r u 1 2 l s l u 1 2 r s l s r u 1 2 r u 1 2 l s r s l otherwise and 31 f 3 h f 1 h v l f 1 h 0 f 1 h v r f 1 h 0 when dl δw and d r δw both the left and right sides of the edge are wet and the wave speeds are given by 32 s l min u l a l u a s r max u r a r u a where a l g d l 1 2 a r g d r 1 2 and 33 u 1 2 u l u r a l a r a 1 2 a l a r 1 4 u l u r in the case where the left side of the edge is dry dl δw the wave speeds are based on the exact solution to the dry bed problem as follows 34 s l u r 2 a r s r u r a r and in the case where the right side of the edge is dry d r δw the exact solution is used again 35 s l u l a l s r u l 2 a l for fluxes in the y direction the hllc solver differs only slightly 36 f 1 3 h f 1 3 l s l 0 f 1 3 r s r 0 s r u 1 3 l s l u 1 3 r s l s r u 1 3 r u 1 3 l s r s l otherwise and 37 f 2 h f 1 h u l f 1 h 0 f 1 h u r f 1 h 0 and the wave speeds are computed the same as in the x direction only v is used in place of u we note that roe s approximate riemann solver for the shallow water equations e g bradford and sanders 2002 was also tested and found to perform similarly to the hllc solver at slightly greater 10 computational expense 2 5 stability the time step δt is constrained by a courant friedrichs lewy cfl condition which on a 2d cartesian grid appears as follows 38 λ x max δ t δ u λ y max δ t δ u β where λ x y max represent the maximum wave speeds in the x and y directions respectively and β 1 2 when η slopes δxη and δyη are computed to update the solution if the η slopes are set to zero this corresponds to a purely first order accuracy mode and β 1 the stability condition can be simplified assuming λ x max λ y max as follows 39 λ max δ t δ u 1 2 β note that the maximum allowable time step for stability is increased by a factor of α compared to a model run at a resolution of δ z and this implies that fluxes and solution updates are reduced by a factor of α 2 and α 3 overall respectively 2 6 data sorting sampling and look up tables primo sorts the α 2 values of z in each u grid cell and the α values of z on each u grid edge from lowest to highest in a pre processing step using the sorted data two look up tables are computed for each u grid cell storage as a function of water level h η and a friction parameter as a function of water level σ f 2 η the sorted edge data also set the sequence by which fluxes are integrated along cell edges as indicated by eq 13 i e the fluxes for the deepest edge are computed first and the fluxes for the shallowest edge are computed last computational costs are further reduced in primo by limiting the number of entries in look up tables which reduces the basis points used for interpolation and by limiting the number of z values where fluxes are computed which reduces the number of calls to the approximate riemann solver if we consider the number of z data points used for either interpolation or flux calculation to be np then n p α for u grid edges and n p α 2 for u grid areas to implement a sampling strategy involving a reduced number of points ns np the indices for sampling s m m 1 n s are computed as follows 40 s m 1 floor m 1 n p 1 n s 1 m 1 n s which ensures that the lowest and highest values of z are always used as basis points in addition to intermediate points furthermore when fluxes are computed using a reduced number of z data points each flux is weighted by α ns to give the total flux correctly along the u grid edge as required by eq 13 the process of sorting sampling and evenly weighting the chosen topographic data points is presented graphically in fig 2 when using a sampling strategy for z the total number of edges required for flux calculations is reduced in proportion to fraction of cells that are sampled for example if α 10 and n s 5 as shown in fig 2 then the number of calls to the riemann solver is reduced 50 the notation used later to indicate sample size for flux calculations interpolation of volume versus water level and interpolation of the resistance term versus water level is given by n s flux n s vol and n s fric respectively 2 7 parallel implementation primo is configured for parallel execution using message passing interface mpi directives so it is applicable to either shared memory distributed memory or hybrid high performance computing systems spmd is adopted whereby domain decomposition is used to create separate input data for each process the spatial data defined by a nx ny raster at resolution δ z is subdivided into a grid of mx my subdomains or tiles each containing nx mx ny my raster cells at resolution δz care is taken in grid preparation so nx mx and ny my are multiples of the upscale factor α to enable the subgrid modeling technique in many applications there will be one or more tiles within the mx my grid where no flooding occurs and there is no need to perform flood computations under these conditions primo can be configured so processes only run on tiles of data where flooding is expected the solution variables needed to compute fluxes on edges that divide subdomains are exchanged between processes ever time step along each subdomain boundary edge the variables passed between processes are on u v and η from the first layer of boundary data and η from the second layer of boundary data since these variables are all defined on the u grid there are nx αmx values of each variable exchanged in the along horizontal edges and ny αmy values of each variable shared along vertical edges these variables are packed into a single buffer for each process subdomain so at most eight messages are exchanged between each process every time step two way messaging with four neighbors additional messages containing topographic data are exchanged in a pre processing step so each process has the necessary data to compute the elevation data required for fluxes and source terms z two layers of η values are exchanged so η slopes δxη and δyη can be computed within the first layer of u grid cells inside and outside of the subdomain boundary computing and communications are overlapped to maximize parallel efficiency specifically flux and slope calculations that are not dependent on exchanged boundary data edges and cells within the interior core of each subdomain are performed in parallel with non blocking mpi directives that exchange data between processes this leads to fluxes for edges along subdomain boundaries and slopes in cells along boundary edges that are computed once by two different processes and thus greater computational effort overall than for sequential execution duplication of computations could be avoided by sharing a combination of solution data and flux data across between subdomains but this comes at the cost of greater complexity to the message passing strategy and greater latency and the added cost appears to have negligible impact on performance based on results presented in the next section lastly mpi allreduce is called once per time step to compute a global time step for all processes that satisfies the cfl condition on all subdomains the main time loop of the algorithm proceeds as follows time loop begins boundary data packed into buffer for transfer one layer of u and v values and two layers of η values non blocking mpi irecv called to receive boundary data non blocking mpi isend called to send boundary data slopes δxη and δyη computed for the interior core fluxes computed for the interior core mpi wait called to ensure boundary data transfer complete transferred buffers unpacked into solution variables slopes computed for layer of cells i inside and ii outside of model subdomain boundary fluxes computed on i boundary edges and ii first interior edges of flow model subdomain grid mpi allreduce called to set global time step δt solution advanced δt from time level n to n 1 time loop ends the parallel implementation of primo leads to balanced compute loads and message sizes when all cells across the domain domain are wetted h δw and unbalanced loads when there are partially wetted cells and or cells that are masked and saved as nodata when cells are not wetted dry primo will sweep over adjacent edges to compute fluxes and update the solution with presumably zero valued fluxes hence dry cells also demand computations but the number of computations is reduced because the approximate riemann solver includes logical checks that return zero valued fluxes when two neighboring cells are dry additionally we note that after p and q are computed following eq 16 u and v are computed as p h and q h respectively in all cells with h δh where δh is typically set to 0 001 m although smaller values can be used hence primo will exchange fluid between cells whenever h δw but u and v are computed only when h δh otherwise u and v are set to zero 2 8 summary of model features as described above primo is developed with several features to balance accuracy and computational effort 1 the shallow water equations are solved on an upscaled grid u grid with cell size δ u α δ z where α represents the upscale factor 2 fluxes are integrated along each u grid cell edge by summing over a sample of n s flux α elevation data points 3 storage is tabulated versus water level based on the α 2 elevation data points within each u grid cell and interpolation is performed using a limited number of basis points n s vol α 2 4 a friction parameter σ f is tabulated versus water level based on the α 2 elevation data points within each u grid cell and interpolation is performed using a limited number of basis points n s fric α 2 5 slopes in the water surface δxη and δyη are computed for each u grid cell for more accurate estimation of fluxes and more accurate downscaling of depth to the z grid 6 parallel execution is enabled for either shared memory or distributed memory compute clusters with mpi directives the remainder of the paper presents test cases to verify the model with analytical solutions and an observed dam break flood test its parallel scaling and benchmark its performance in a hypothetical whole city scale application based on these results strengths and weaknesses of the method are presented in the discussion section along with promising directions for future research 3 applications 3 1 dry bed dam break problem we consider a 1 km long and 100 m wide channel that is horizontal and frictionless and create a z grid at 1 m resolution for input into primo initially water is a depth of 3 m for x 500 m and motionless and dry elsewhere the solution is integrated for 40 s and saved every 10 s with a velocity calculation tolerance δ h 1 10 6 m for improved accuracy at the wetting front fig 3 compares predictions red with the exact solutions gray under four different model configurations a an upscale factor α 1 no upscaling without η slopes i e δ x η 0 b an upscale factor α 1 with η slopes i e δxη 0 c an upscale factor α 5 without η slopes and d an upscale factor α 5 with η slopes these results show that primo performs similar to other first order accurate godunov type schemes when α 1 and δ x η 0 fig 3a that is the solution remains monotone there is a small diffusive error in depth at the leading edge of the rarefaction wave there is a phase error in the leading edge of the wetting front and there is diffusion of the velocity profile at the leading edge of the wetting front when α 1 and η slopes are computed fig 3b there is less numerical diffusion as expected and the phase error of the wetting front increases slightly fig 3c and d show that the effect of upscaling is similar to the effect of grid coarsening in this case the effective grid resolution is 5 m instead of 1 m and there is a noticeable increase in numerical diffusion in the depth profile at the leading edge of the rarefaction wave and in the velocity profile at the leading edge of the wetting front however increasing the upscale factor does not substantially change the phase error of the wetting front 3 2 parabolic floodplain with meandering channel we now consider an idealized parabolic floodplain with a meandering channel to test the model for compound channel cross sections as shown in fig 4 the spatial domain is rectangular with a length of l x 6 km and a width of l w 1 km and we assume δ z 1 m the meandering channel has a rectangular cross section with a width of w 20 m measured perpendicular to the channel centerline additionally the meander takes on a sinusoidal form with a wavelength of 1 km and an amplitude of 100 m based on a coordinate system with the origin placed in the lower left corner of the domain dtm elevations in meters are computed as follows 41 z floodplain x y 1 10 4 x 1 10 5 y 3 000 2 42 z channel x y 1 10 4 x 2 except for the first and last 500 m of the domain where z 2 m as shown in fig 4 note that the channel is 2 m deep at y 3000 m and slightly deeper at the top and bottom of the meanders where the floodplain elevation is slightly higher due to the parabolic cross sectional profile the channel and floodplain are dry at t 0 all boundaries are treated as solid walls and flooding is initiated by a constant inflow of q 1000 m3 s specified as a point source at x 5800 m and y 500 m this leads to a flood front moving right to left over a dry bed first into the channel and then over the floodplain the solution is integrated for 2 hr and flow resistance is modeled with n m 0 03 m 1 3 s in each case the sampling for fluxes volume and friction was given by n s flux α 2 n s vol 2 α and n s friction 2 α fig 5 shows contours of flood depth at t 100 min without η slopes left and with η slopes right for α 200 100 50 and 10 top to bottom first the model is shown to route flows through channels for all values of α including cases where δ u much larger than the channel width i e α 200 corresponds to δ u 200 m fig 5 also reveals sensitivities in the propagation of the flood front to upscaling and the use of η slopes that are similar to dam break test case considered previously that is simulations without η slopes labeled δ η 0 in fig 5 exhibit slightly faster flood front movement than with η slopes labeled δη 0 in fig 5 differences in the position of the flood front and the depth of flooding due to upscaling appear minimal when η slopes are computed δη 0 fig 5 shows that the leading edge of the flood front extends close to x 1000 m along the flood plain for the four different values of α and the leading edge of the flood front in the channel extends very close to the channel outlet or slightly past it on the other hand when η slopes are not computed δ η 0 there is a substantial differences in the progression of the flood front between α 10 flood front extending roughly to x 1000 m and α 200 flood front extending past x 500 m hence flood simulations using η slopes appear to reduce sensitivity of the flood wave progression to the upscale factor a weakness of upscaling methods that has been reported in previous studies is the potential for granularity in downscaled flood depths hénonin et al 2015 and the appearance of granularity is prominent in fig 5 when η slopes are not computed and α 100 and 200 however granularity does not appear in fig 5 when η slopes are computed and used to compute the downscaled flood depth hence use of the η slopes offers an advantage with respect to more accurate downscaling of the flood depth in the remaining tests all simulations involve use he use of η slopes for solution updates and downscaling 3 3 baldwin hills dam break test case model accuracy in a practical test case is now considered by applying primo to simulate urban flooding from the 1963 failure of the baldwin hills dam previous studies have shown that flood extent can be predicted with over 70 accuracy in this test case gallegos et al 2009 schubert and sanders 2012 hence primo is applied here using several sources of data previously reported by gallegos et al 2009 1 a 1 5 m resolution dtm 8 5 cm vertical rmse 2 the location height and length of curb inlets to storm drains 3 a spatially distributed resistance parameter distribution based on land cover including vegetated open space n m 0 05 concrete surfaces n m 0 013 reservoir n m 0 013 roads n m 0 014 ballona creek n m 0 016 and buildings n m 0 3 4 breach geometry data 5 flood extent data for validation 6 as built reservoir geometry data and 7 an estimate of the water level at the time of failure η 141 m navd88 needed for the initial condition the effect of structures on flooding dynamics is treated with the building resistance method schubert and sanders 2012 and breaching which took place over 20 min was assumed to occur instantaneously fig 6 shows the site topography flood extent data and storm drain locations modeling of storm drain interception of overland flow by curb inlets is based on either an orifice or a weir equation gallegos et al 2009 given a curb inlet with a length l and height ho and a local depth prediction d the interception rate is computed as 43 q c d l min d h o κ g d where cd is a discharge coefficient κ 1 corresponds to a weir approximation for d ho and κ 2 corresponds to an orifice approximation that is used otherwise here c d 0 5 was specified to match a previous calibration by gallegos et al 2009 primo was implemented in parallel using 16 processors on the uci hpc cluster which consists of 64 core compute nodes with 2 33 ghz amd processors and 512 gb of ram fig 7 shows the accuracy of flood extent predictions using α 2 5 10 and 20 which corresponds to δ u 3 7 5 15 and 30 m here correct predictions of flooding green areas of overprediction blue and areas of under prediction red are shown additionally table 1 presents wall clock execution times and metrics of flood extent accuracy including an agreement metric fa and overprediction metric fo and an underprediction metric fu schubert and sanders 2012 perfect accuracy corresponds to f a 100 f o 0 and f u 0 these results show that primo simulates flood extent with a high level of accuracy roughly 70 for α 2 5 and 10 without model specific calibration that is parameter values and taken from previous studies these results also show that flood extent accuracy differs by only 1 between α 2 and 5 and by 3 between α 2 and 10 which is likely within the accuracy of the measured flood extent data hence the potential exists for significant upscaling without significant loss of accuracy however there are limits α 20 leads to substantially increased overprediction and a reduction of flood extent accuracy by about 11 compared to the case with α 2 finally table 1 shows that increasing α significantly reduces wall clock execution times as expected in the remaining test cases attention turns to the parallel scaling of primo and the potential for fast simulation of flooding over large spatial domains 3 4 load balanced parallel performance testing domain decomposition into tiles with equal size leads to perfect load balancing across processes if the domain is fully wetted and thus the number of operations on each processor per time step is equal fig 8 shows a square domain of 1 m resolution topographic data spanning 4 km2 of the platte river in central nebraska data previously used for hydrodynamic river modeling schubert et al 2015 hence the raster is 2000 2000 cells which is amenable to subdivision several times into grid sizes that are multiples of 10 and 20 and supportive of upscaling with relatively large values of α the test problem involves an initial condition corresponding to a constant water level η 712 m navd 88 which inundates the highest topography in the domain by only several cm and a fluid velocity of zero wall boundary conditions are specified on all four sides and a point source q 300 m3 s is specified in the main channel near the left boundary of the domain which represents the upstream end of this river reach flow resistance is not important but is modeled using a spatially uniform manning coefficient n m 0 03 m 1 3 s the simulation period for this test problem is 6000 s to test the parallel performance of primo the domain was divided twice four times and eight times in each cardinal direction leading to grids designed for nproc 1 4 16 and 64 processes and upscaling using α 10 as an example fig 8 shows the domain decomposition for nproc 16 and we note that the numbering of processes begins with zero consistent with the standard mpi convention primo sampling parameters used in this test case were n s flux 5 n s vol 11 n s fric 11 testing was performed on the uci hpc cluster and the cheyenne cluster at the ncar wyoming supercomputing center which consisted of 36 core compute nodes with 2 3 ghz intel xeon processors and 64 gb of ram both systems use infiniband high speed interconnect for communication between nodes fig 9 shows that parallel performance closely tracks the ideal speedup profile on the cheyenne cluster and on the uci cluster when primo is run in a shared execution mode whereby other jobs run concurrently on the node however primo does not demonstrate ideal scaling on the uci cluster when jobs have exclusive access to the node this is attributed relatively faster execution of the test involving nproc 1 which suggests that the hardware is performing internal optimizations to take advantage of idle computing power similar behavior has been observed in previous studies of parallel performance e g sanders et al 2010 and run times using nproc 64 were equivalent using the uci cluster in shared and exclusive modes we note that the case involving nproc 64 required use of two nodes on the cheyenne cluster and communication between nodes yet the parallel performance exhibits no adverse consequences both clusters could support larger jobs requiring significantly more cores but this 2000 2000 cell test problem doesn t justify use of additional cores using 64 cores the 6000 s simulation period requires only about 10 s of wall clock time on the cheyenne cluster which is significantly faster than the uci cluster due to a compiler that is optimized for the hardware a more demanding test problem requiring numerous compute nodes is created by randomly generating topographic heights at 1 m resolution over a spatial extent of 1 024 km2 assuming a uniform distribution between 0 and 1 m this corresponds to a topographic grid of 32 000 32 000 1 024 109 cells which conveniently supports domain decomposition for 16 cores with a 4 4 matrix of 8 000 8 000 cell grids 64 cores with a 8 8 matrix of 4 000 4 000 cell grids 256 cores with a 16 16 matrix of 2 000 2 000 cell grids and 1024 cores with a 32 32 matrix of 1 000 1 000 cell grids the test problem is defined by an initial water elevation of 3 m wall boundary conditions on all four sides a rainfall rate of 1 cm hr a spatially uniform n m 0 03 m 1 3 s and a simulation period of 1 hr primo is run on the cheyenne cluster using nproc 16 64 256 and 1024 processes and α 10 primo sampling parameters used in this test case were n s flux 5 n s vol 11 n s fric 11 fig 9 shows that speedup is better than ideal when normalized by the execution time for 16 cores the reason for better than ideal performance is not immediately clear but it could be due to low memory demands per node or to more efficient communication realized by smaller sized arrays being passed between processes as the number of processes increase the 1 hr simulation completed in 98 s using 1024 cores how does the choice of the upscale factor affect run times fig 10 shows that wall clock times using α 4 10 and 20 closely follow a power law function of the upscale factor of the form t t o α k ideal performance hence these results demonstrate that primo achieves ideal power law run time reductions from two separate mechanisms parallel processing and upscaling and the combination of these mechanisms allows for an hourly simulation of flooding at 1 m resolution over 103 km2 in a matter of seconds a level of performance well suited to nowcasting and forecasting applications 3 5 los angeles region extreme flooding test as a final test we consider a simulation of flooding across los angeles the second largest city in the usa to benchmark wall clock times and test the computational feasibility of primo for time sensitive applications such as real time forecasting and uncertainty quantification with monte carlo simulation to achieve a reasonable calibration of a flood hazard model for an urban area many factors must be carefully considered including the representation of flow pathways in the dtm subsurface drainage through pipes storage in reservoirs flow resistance the variability in precipitation and surface subsurface interactions such as infiltration e g gallegos et al 2009 gallien et al 2011 since the objective here is only to benchmark wall clock times and validate the shallow water routing of pluvial flooding and calibration requires observations of observed flooding of extreme events that are not easily obtained for many reasons no attempt is made to calibrate the model the los angeles model domain spans the portions of los angeles county that drain to the south and west from the san gabriel mountains the model was developed from a 10 ft resolution dtm for the county of los angeles usgs 2006 which was prepared for public access by downsampling a 5 ft resolution dtm with 0 91 ft vertical accuracy at the 95 confidence level the spatial domain was trimmed along the north and east based on the limits of the los angeles river watershed spanning the areas where the vast majority of the county s 10 million people reside u s census bureau 2010 the trimmed domain was subsequently partitioned into 608 square tiles consisting of 103 103 raster grids as shown in fig 11 a number that allowed for parallel execution with 32 processes per node across 19 compute nodes on cheyenne hence a total of 608 million z grid points are included in the model and the domain spans an area of 2181 mi2 or 5 652 km2 the hazard scenario that forms the basis of this test corresponds to the coincidence of a record rainfall with record high tide and likely has a return period of at least 1000 years goodridge 1994 a constant water level boundary of η b 8 ft navd 88 is assigned along the coastal boundary representative of the record high tide level which roughly corresponds to the maximum recorded water level in los angeles harbor noaa gage 9410660 and a constant effective rainfall rate of p 1 in hr is specified uniformly for a period of t 12 h over which flooding is simulated this rainfall rate roughly corresponds to historical maximum conditions within los angeles county where 26 in of rainfall was once recorded over a 24 h period at a gage in the san gabriel mountains and on several occasions more than 10 in of rainfall have been recorded over a 12 h period goodridge 1994 the assumption of spatially uniform rainfall at this rate is not consistent with observations as rainfall is magnified in the san gabriel mountains by orographic effects nevertheless it is a useful simplification for measuring the run time attributes of primo and creating a scenario that can be easily communicated i e 1 in hr rainfall for 12 h four simulations were completed using α 5 10 20 and 50 and other model parameters shown in table 2 primo was executed on cheyenne and wall clock times are also reported in table 2 this shows that wall clock times are reduced from about 6 h using α 5 to about 15 s using α 50 flood depth at t 12 hrs using α 5 is shown in fig 12 a and points to extensive inundation across the region as flood flows move south from the san gabriel mountains towards long beach cf fig 11 indeed the region between los angeles and long beach is an amalgam of alluvial fans where the simulation shows flood water spreading out and pooling behind a major east west highway ca route 91 as α is increased predicted drainage patterns remain similar but the depth behind obstructions such as freeways and dams is reduced and the depth near the southern coastline of the study area is locally increased the mean absolute value of δd was computed to be 0 17 ft 5 2 cm 0 28 ft 8 5 cm and 0 47 ft 14 cm for α 10 20 and 50 compared to the α 5 case reduced pooling behind obstructions is attributed to flow cell edges that no longer capture important blockage features such as elevated roadways or embankments this weakness of upscaling methods has previously been reported by hodges 2015 nevertheless the cases involving α 10 and 20 show potential for forecasting applications based on run times of 66 and 4 8 min respectively and the sensitivity to the upscale factor more detail is found in fig 13 where flood depth in the regions marked a d in fig 11 is shown based on α 10 fig 13a shows a reach of the los angeles river that flows east through a flood detention basin the sepulveda basin and then through the community of sherman oaks primo simulates overtopping of the sepuveda basin spillway which is associated with transcritical flow regimes as well as compound fluvial pluvial flooding in sherman oaks due the combined effects of los angeles river flows from the west and pluvial runoff from the north fig 13b shows where the san gabriel river descends south out of the san gabriel mountains over 17 drop structures that dissipate energy and into the santa fe dam recreational area which is a major flood control facility in the area modeling of flow over the drop structures leads to transcritical flows with hydraulic jumps that are easily resolved by the model fig 13c shows the combined effect of fluvial flooding and pluvial flooding in downtown los angeles the area west of the los angeles river is an alluvial fan and the simulation shows several preferential pathways for floodwater within this highly urbanized area finally fig 13d shows the combined effects of pluvial fluvial and coastal flooding in the vicinity of long beach where the los angeles river east side of the harbor complex and dominguez channel west side of harbor complex empty into the pacific ocean through the ports of long beach and los angeles the busiest port in the usa based on container shipments 4 discussion upscaling in shallow water modeling of flood inundation is relatively new presented here is a technique for retaining the precision of fine scale topography and the skill of approximate riemann solvers to estimate fluxes in a relatively course solution update with a godunov type finite volume scheme the goal is to improve computational efficiency and resolve a wide range of flows without overly sacrificing accuracy how fluxes are evaluated and in particular how topographic data are used to inform fluxes is arguably the most important consideration in the design of an upscaling method and the best strategy will depend on many factors including the type of numerical solver used for this study a distribution of topographic data from z grid cells along each u grid edge were computed to support flux estimation as shown in eq 4 to enable implementation of a godunov type finite volume scheme another approach used for an implicit finite volume scheme is to integrate all of the z data within a staggered u grid cell volp et al 2013 and hodges 2015 proposed processing z values to identify topographic objects that control the spreading of flood water and subsequently assigning z values to edges to preserve the functionality of the object others have pursued adaptive meshing approaches that increase resolution around key topographic features hou et al 2018 indeed many options are possible and more research is needed to deepen understanding of strengths and weaknesses and to develop improved methods results presented here point to several strengths including the ability to resolve flooding over a wide range of topographic and flow conditions the ability to resolve flow in channels resolved on the z grid but finer that the resolution of the u grid the ability to improve accuracy by making η piecewise linear instead of piecewise constant and the ability of the scheme to support ideal parallel scaling which allows the method to be efficiently applied at any scale another major advantage of this upscaling method is that a single parameter α can be changed to best meet accuracy and run time requirements that is there is no need for time consuming re gridding to make a model run faster the main weakness of the method is that blockage features may not be correctly resolved which leads to artificial spreading of flood water additionally the method does not overcome a well known limitation of finite volume schemes whereby flow down steep slopes is modeled as a series of waterfalls kim et al 2012 xia et al 2017 in fact use of piecewise linear η changes the representation of this waterfall dynamic and additional research is needed to develop improved representation of flow down steep slopes in conjunction with upscaling methods moreover while the scheme conserves mass to numerical precision based on the direct discretization of conservative equations and the consistent updating of the discrete solution based on globally computed flux values the scheme does not guarantee positivity the prediction of negative depth is possible if the volume of water predicted to leave a cell over one time step exceeds volume stored in the cell begnudelli and sanders 2006 bradford and sanders 2002 brufau et al 2004 in the test cases considered negative depth predictions were minimal and presented no noticeable impacts on accuracy or stability but strategies to address this limitation in conjunction with upscaling methods could be considered the future whereby tradeoffs between accuracy stability and computational effort in parallel execution are examined it is unclear whether primo would have scaled better or run faster had it been implemented on a gpu or a cluster of gpus instead of a cluster of multi core compute nodes several studies point to fast flood inundation model performance on gpus brodtkorb et al 2012 castro et al 2011 neal et al 2010 vacondio et al 2014 and as the size of applications increases memory and communication demands increase and ultimately this controls the parallel scaling of the model additional research is needed to address this possibility and improve understanding of the strengths and weaknesses of gpus and multi core compute nodes to guide development of future models a broader issue raised by this research is the potential to deliver timely flood hazard information at fine resolution over whole city scales world population has more than doubled since 1960 from around 3 billion people to around 7 billion today and the exposure and vulnerability of communities to flooding has been magnified with over half of the global population concentrated in cities that are located along coastlines or major rivers hinkel et al 2014 wolsko and marino 2016 this motivates a pressing need for flood risk information that is responsive to the wide web of decision making causing the increase in exposure and vulnerability or actionable information morss et al 2005 spiekermann et al 2015 fine scale visualizations of flooding have tremendous power for communicating flood hazards in intuitive ways that anyone can understand lane et al 2011 luke et al 2018 and collaboration with stakeholders on visualizations can meet site specific decision making needs in a context sensitive way for all phases of the disaster management cycle including planning preparedness early warning emergency response and recovery dawson et al 2011 evers et al 2012 luke et al 2018 pasche et al 2009 steinführer 2009 what is demonstrated here is the computational feasibility of a fast response fine resolution hydrodynamic flood hazard simulation over a large metropolitan area transitioning from a feasibility to a reality introduces several more research challenges deserving attention including a the ability to accurately force flood inundation models with rainfall streamflow and ocean height data b capturing all important hydrological processes that contribute to the distribution of flood water e g urban drainage infrastructure and c managing the enormous volumes of data associated with time dependent fine resolution spatial fields over large spatial extents lessons learned from fine resolution flood hazard modeling over smaller spatial extents are relevant here for example dtms can be conditioned to improve the representation of drainage pathways and representation of flow barriers such a levees but models have generally relied on manual processing or at best semi automated methods e g gallegos et al 2009 gallien et al 2011 schubert and sanders 2012 which is tedious over the scale of entire metropolitan areas like los angeles nevertheless dtm processing is clearly needed in this case based on the amount of ponding predicted to occur behind highways where flood extent is surely overpredicted given that highway underpasses are not resolved by the model poor channel bathymetry accuracy in the dtm is also likely to be a contributor to the overprediction of flood depths and flood extent in coastal areas and motivates the need for coastal dtms that merge the best available topographic and bathymetric data such as those developed by noaa noaa national center for environmental information 2018 5 conclusions a godunov type finite volume scheme for solving the shallow water equations primo is implemented with an innovative sub grid model that enables flow computations at scale δ u that is a multiple α of the dtm resolution δ z the method reduces the number of flux computations by at least a factor of α per time step and the number of solution updates by a factor of α 2 per time step compared with a flow model that performs flow computations at a resolution of δ z additionally the maximum allowable time step for stability is increased by a factor of α compared to a model run at a resolution of δ z and thus fluxes and solution updates are reduced by a factor of α 2 and α 3 overall respectively execution time is further reduced by a sampling technique that limits the number of times the approximate riemann solver is used each time step to compute fluxes additionally piecewise linear treatment of the free surface elevation η is combined with upscaling to reduce numerical dissipation and improve the accuracy and visualization of downscaled flood depth primo is shown to simulate subcritical supercritical and transcritical flows which is expected of a godunov type flow solver that uses an approximate riemann solver suited to all classes of shallow water flow primo is also shown to propagate flow through channels that are resolved on the z grid but finer than the u grid additionally piecewise linear treatment of the water surface η is shown to improve accuracy of downscaled flood depths over piecewise linear models δ η 0 application of primo to the baldwin hills dam break flood showed the potential to upscale up to α 10 in an urban area using δ z 1 5 m with negligible impact on flood extent accuracy while further increases in α lead to overprediction of flood extent the los angeles test case showed practical benefits such as the ability to simulate flow over spillways and weirs flow down steep slopes and backwater effects such as those that occur near the coastline additionally the los angeles test case showed the potential to simulate compound flooding involving pluvial fluvial and coastal flood hazards primo is shown to achieve ideal parallel scaling with up to 1024 processes the largest problem tested the combined effects of upscaled flow computations and parallel execution enable metric resolution simulations over thousands of km2 and hourly time scales in a matter of seconds to minutes depending on flow conditions depth and velocity which control the maximum allowable time step this capacity is well suited to supporting time sensitive applications such as nowcasting and forecasting and implies that hydrodynamic simulations can be run in real time to assist with several phases of disaster management including early warning emergency response and recovery fast execution times also increase the feasibility of monte carlo simulation to characterize uncertainties for planning preparedness and mitigation aspects of disaster management acknowledgements the upscaling method adopted by primo was conceived by b sanders and on several occasions was discussed with l begnudelli who was involved in the development of a separate model that uses a similar upscaling technique the authors acknowledge l begnudelli for valuable dialogue and correspondence which contributed to this work the authors also acknowledge b m ginting for valuable dialogue and correspondence regarding parallelization strategies a luke for valuable dialogue regarding the practical value of fast simulations and j vrugt for suggestions to improve the paper this research was made possible by a grant from the national science foundation dms 1331611 we also like acknowledge the use of the uci high performance computing system http hpc oit uci edu and computational resources doi 10 5065 d6rx99hx at the ncar wyoming supercomputing center provided by the national science foundation and the state of wyoming and supported by ncar s computational and information systems laboratory finally we thank the anonymous reviewers for comments and questions that helped us to improve the paper 
687,simulation of flood inundation with primo systematically captures many complex flooding processes including compound flooding such as a storage behind sepulveda dam overtopping of its spillway and the combine effects of fluvial and pluvial flooding downstream b flow over a series of 17 drop structures along the very steep san gabriel river and storage behind santa fe dam c spreading of los angeles river floodwater and pluvial flooding in downtown los angeles which is built on alluvial fan and d the combined effect of fluvial pluvial and coastal flooding around long beach and the ports of los angeles and long beach fig 13 table 1 baldwin hills test case wall clock times and flood extent accuracy metrics table 1 α t wall min fa fo fu 2 1090 8 71 8 7 7 20 5 5 82 2 70 8 7 9 21 3 10 19 2 68 9 9 7 21 5 20 5 1 60 4 15 4 24 2 table 2 los angeles test case parameters and wall clock times table 2 i ηb t t wall nproc in hr ft hrs α n s flux n s vol n s fric min 1 608 1 0 8 0 12 5 3 6 6 380 2 608 1 0 8 0 12 10 5 11 11 66 3 608 1 0 8 0 12 20 5 11 11 4 8 4 608 1 0 8 0 12 50 5 11 11 0 25 primo parallel raster inundation model brett f sanders a b jochen e schubert a a department of civil and environmental engineering uc irvine irvine ca 92697 usa department of civil and environmental engineering uc irvine irvine ca 92697 usa b department of urban planning and public policy uc irvine irvine ca 92697 usa department of urban planning and public policy uc irvine irvine ca 92697 usa corresponding author at department of civil and environmental engineering uc irvine irvine ca 92697 usa dept of civil and environmental engineering uc irvine irvine ca 92697 usa simulation of flood inundation at metric resolution is important for making hazard information useful to a wide range of end users involved in flood risk management and addressing the alarming increase in flood losses that have been observed over recent decades however high data volumes and computational demands make this challenging over large spatial extents comparable to the metropolitan areas of major cities where flood impacts are concentrated especially for time sensitive applications such as forecasting and repetitive simulation for uncertainty assessment additionally several factors present difficulties for numerical solvers including combinations of steep and flat topography that promote transcritical flows the need to resolve flow in relatively narrow features such as drainage channels and roadways in urban areas which channel flood water during extreme events and the need to depict compound hazards resulting from the interaction of pluvial fluvial and coastal flooding a new flood inundation model is presented here to address these challenges the parallel raster inundation model primo solves the shallow water equations on an upscaled grid that is far coarser than the underlying raster digital topographic model dtm and uses a subgrid modeling approach so that the solution benefits from dtm scale topographic data additionally an approximate riemann solver is applied in an innovative way to integrate fluxes between cells as needed to update the solution by the finite volume method which makes the method applicable to subcritical supercritical and transcritical flows primo is implemented using a two dimensional domain decomposition approach to single process multiple data spmd parallel computing and overlapping communications and computations are implemented to yield ideal parallel scaling for well balanced test cases with both a subgrid model and ideal parallel scaling the model can scale to meet the demands of any application several benchmarks are presented to demonstrate predictive skill and the potential for timely whole city metric resolution flooding simulations limitations of the methods and opportunities for improvements are also presented keywords flood inundation modeling godunov type scheme finite volume method parallel computing digital elevation model upscaling flood hazards flood risk 1 introduction flooding has emerged as the most significant natural hazard facing society jongman et al 2012 over the last decade flooding accounted for half of all weather related disasters and affected 2 3 billion people cred 2015 the u s set a record in 2017 with 300 billion in disaster losses from hurricanes and flooding noaa 2018 and over the summer of that year a humanitarian crisis emerged in nepal bangladesh and india where more than 1000 people died and at least 41 million were affected by monsoon flooding and landslides gettleman 2017 additionally the human health risks of flooding are significant and unacceptable cred 2015 di baldassarre et al 2010 few et al 2004 watts et al 2015 several trends including rising sea levels urbanization deforestation and rural to urban population shifts are expected to increase the impacts of flooding in the future hallegatte et al 2013 jongman et al 2012 flood inundation modeling is taking on an increasingly important role in flood risk management and disaster risk reduction initiatives djordjević et al 2011 hénonin et al 2015 sanders 2017 while the use of hydraulic or hydrodynamic flood models has traditionally been limited to engineering design e g sizing of culverts placement and height of levees implementation of flood insurance programs and support of dam safety programs many more applications are emerging as communities a shift focus from controlling flooding hazards to minimizing flooding consequences southgate et al 2013 and b embrace participatory methods for planning preparedness and design whereby experts and stakeholders co develop interventions heimhuber et al 2015 maskrey et al 2016 smith et al 2017 flood simulations are powerful tools for anyone to immediately grasp the consequences of flooding based on familiar reference points and to identify steps that can be taken to minimize impacts additionally flood simulations can help to orient diverse groups of stakeholders around flood risks and facilitate dialogue about how to manage it djordjević et al 2011 lane et al 2011 indeed flood simulations offer support for all phases of the disaster management cycle including mitigation planning preparedness early warning emergency response and recovery mackay et al 2015 sanders 2017 wilkinson et al 2015 what is critical when using flood simulations for flood risk management is the iterative engagement of stakeholders early stakeholders involvement is important for building trust and configuring simulations around priority issues facing communities evers et al 2012 luke et al 2018 repeated engagement is important for ensuring that model output is tailored to local decision making needs djordjević et al 2011 evers et al 2012 luke et al 2018 meyer et al 2012 sanders 2017 for simulations to depict flooding in an intuitive way that anyone can understand an accurate geometrical model of flood zones provided by a digital elevation model is essential and high quality data are increasingly available from laser scanning lidar and photogrammetry bates 2012 sampson et al 2012 schumann et al 2009 moreover there have been recent calls to improve the availability of high quality data globally based on its importance for flood management schumann et al 2014 however using metric resolution data becomes increasingly challenging as the spatial extent of the modeling domain increases the memory of computing systems is strained by the vast number of points that need to be resolved and the workload of processors is intense based on the number of points and flood model requirements for a short time steps seconds or less for accuracy and stability this has motivated several lines of recent flood inundation modeling research including more efficient solution algorithms bates et al 2010 cea and bladé 2015 parallel computing methodologies brodtkorb et al 2012 castro et al 2011 neal et al 2010 sanders et al 2010 vacondio et al 2014 and subgrid models sometimes called porosity models for aggregating the effects of fine scale topographic data at a relatively coarse resolution where flow calculations are made casulli and stelling 2011 defina 2000 guinot et al 2017 hénonin et al 2015 neal et al 2012a özgen et al 2016 sanders et al 2008 soares frazão et al 2008 stelling 2012 volp et al 2013 version 5 0 of the popular hec ras model u s army corps of engineers hydraulic engineering center davis ca now supports two dimensional flood inundation modeling and uses an implicit finite volume method with a subgrid model for topography similar to the scheme reported by casulli and stelling 2011 models based on implicit finite volume or difference schemes are especially advantageous when there are slowly varying subcritical flows because the time step is restricted based on the fluid velocity not the speed of gravity waves as with explicit models and thus solutions can be attained using fewer time steps and fewer computations however the advantage is diminished or reversed compared to explicit methods when flow is supercritical changing quickly or when there are flood fronts moving across the land surface under these circumstances an implicit solver may need to use several iterations per time step to converge and the allowable time step may become smaller moreover explicit schemes tend to support better parallel scaling because all calculations are made locally compared to implicit schemes which rely on the solution of a large matrix problem to simultaneously update the solution over the entire flow domain and thus parallel efficiency is closely tied to the parallel efficiency of the matrix solver used the lisflood fp model uses an explicit scheme for solving a simplified form of the shallow water equations and has received widespread use for large scale flood inundation modeling with moderate resolution 10 90 m digital terrain models dtms bates et al 2010 neal et al 2012a wing et al 2018 and it has been configured for parallel execution with shared memory hardware distributed memory hardward and graphical processing units neal et al 2010 several variants of lisflood fp have been reported with different features including schemes that use an approximate riemann solvers to account for transcritical flow neal et al 2012b and the version of lisflood fp used for large scale flood hazard model is designed to resolve subcritical flows based on its simplified formulation neal et al 2012a wing et al 2018 in summary there are many different approaches for simulating flood inundation using different shallow water equations different subgrid models and different numerical solvers but thus far there has been limited success developing models that apply to all classes of flow subcritical supercritical and transcritical and all sources of flooding pluvial fluvial and coastal and that efficiently scale on parallel computing systems with hundreds or even thousands of processors models that respond to this challenge could have tremendous impact as a flood risk management tool communicating flood hazards to a wide range of end users in support of all phases of the disaster management cycle and especially serving the presently unmet needs for real time decision support during major floods luke et al 2018 sanders 2017 this paper presents a flood simulation model that solves the shallow water equations by an innovative godunov type finite volume algorithm the model is grounded in several conventional features of godunov type finite volume schemes including use of the hllc solver to estimate fluxes at cell edges toro 2001 fluxes and bed slope source terms that are well balanced to prevent spurious acceleration valiani and begnudelli 2006 and the implicit treatment of friction terms for accuracy and stability xia et al 2017 hence this paper emphasizes aspects of the model that enable relatively fast execution in large scale applications including a a single process multiple data spmd parallel implementation based on domain decomposition which limits both memory and workload on a per processor basis b a subgrid model that supports a relatively coarse grid solution update and c innovative optimizations of the subgrid model that further reduce computations without overly sacrificing accuracy optimization involves the sampling of topographic data in the subgrid model which reduces the required number of calls to the computationally demanding approximate riemann solver and enables faster execution of look up tables the model is designed to load elevation and resistance data from commonly used raster formatted digital topographic models dtms making it relatively easy to set up compared with unstructured grid models that are constrained by important land features like levees and building walls e g bilskie et al 2015 hagen et al 2001 schubert and sanders 2012 schubert et al 2008 this inspires the model name parallel raster inundation model primo primo is capable of resolving subcritical supercritical and transcritical flows based on its use of an approximate riemann solver to estimate fluxes and while it is formally first order accurate it considers slopes in the free surface elevation when reconstructing fluxes at cell edges which improves accuracy and also enables more accurate downscaling for improved visualization of results applications of the model are presented to verify the model including its mass conservation and well balanced properties document strengths and weaknesses and report benchmark performance standards useful for informing the readiness of the method for large scale and or real time modeling applications where computational efficiency is paramount 2 model formulation primo solves the nonlinear shallow water equations which predict water level and horizontal velocities under the assumption of a hydrostatic pressure distribution and turbulent momentum dissipation scaled by a manning resistance coefficient the integral form of the governing equations can be written for a two dimensional 2d domain ω with boundary γ and unit outward normal vector n n x n y t as follows valiani and begnudelli 2006 1 t ω u d ω γ f z n d γ ω r d ω ω p d ω where u d u d v d t d depth u x velocity v y velocity f f x f y where 2 f x u d u 2 d 1 2 g d 2 u v d and f y v d u v d v 2 d 1 2 g d 2 and finally z r and p are related to traditional source terms of the shallow water equations bottom slope resistance and the effective precipitation rainfall converted to runoff respectively and are given as follows 3 z z x z y 0 0 1 2 g d η o 2 0 0 1 2 g d η o 2 r g d 0 s f x s f y p p 1 0 0 where g gravity s f x and s f y represent x and y direction friction slopes respectively d η o η o z and represents the depth based on a cell wise constant free surface elevation ηo and p is the effective rainfall rate the form of the shallow water equations appearing in eq 1 is chosen because it simplifies balancing of fluxes and source terms when needed to preserve stationary solutions valiani and begnudelli 2006 in essence the computation of f z along each edge of a flow model cell represents a difference between the fluxes computed by the riemann solver and hydrostatic fluxes associated with the ground slope source term in the limit of a stationary solution these two terms are identical and thus f z 0 on an edge by edge basis additional detail is provided in section 2 4 2 1 data structure similar to previously reported models involving upscaling of topographic data hénonin et al 2015 volp et al 2013 primo is designed to load a raster grid of elevation data z with cell size δ z in both the x and y directions and to solve eq 1 on a relatively course grid with a cell size δ u α δ z where α is an integer termed the upscale factor to succinctly differentiate between grids in the narrative that follows we will refer to the fine resolution grid as the z grid and the coarse resolution grid as the u grid primo also accommodates a raster grid of manning resistance parameter values nm with cell size δ z fig 1 shows the u grid red superimposed on the z grid gray based on α 10 as well as the indices for the z grid i j and the indices for the u grid im jm the origin for the indices is the lower left hand corner of the grid whereas raster formatted data commonly uses an origin in the upper left hand corner of the grid with row indices that increase in the downwards direction hence careful attention to the ordering of data points is needed when reading and writing gridded data based on this number system topographic data within model cell im jm are indexed by i i m 1 α 1 i m α and j j m 1 α 1 j m α however to simplify the presentation in the material that follows i and j will always be presented as local indices to reference the α 2 z values located within an individual u grid cell because edges of the flow model cells are not aligned with the center of topographic data cells there is ambiguity in the definition of topography data used to compute fluxes and different approaches can be taken for example volp et al 2013 used all topographic data from a δ u δ u domain centered on the edge in the context of a semi implicit finite volume scheme for a godunov type scheme it is important to define a specific cross sectional shape connecting two neighboring u grid cells across which an approximate riemann solver can be applied therefore a set of α edge elevation points z k k 1 α are computed as follows 4 z k max z k min l z min r z where 5 z k 1 2 z k l z k r here z k l and z k r represent elevation in the z grid cell to the left and right respectively of the kth segment of the u grid edge and min l z and min r z represent the minimum elevation among the α 2 elevation data points in the u grid cells on the left and right respectively of the u grid edge hence z values along u grid edges are taken as a simple average of neighboring z grid data points unless the average is lower than the minimum elevation of z data within neighboring u grid cells this condition prevents the reconstruction of positive depth values d 0 at u grid edges when adjacent cells are dry no storage which will lead to spurious fluxes as will be described next the free surface elevation in u grid cells is set equal to the height of the lowest of its α 2 z values when the cell is dry the discrete solution represents a cell average over the u grid cell ω i m j m with area δ u 2 and includes information about storage per unit area h and cell average discharge per unit width in the x direction p and y direction q as follows 6 u i m j m h p q i m j m where 7 h i m j m 1 δ u 2 ω i m j m d d ω 8 p i m j m 1 δ u 2 ω i m j m u d d ω q i m j m 1 δ u 2 ω i m j m v d d ω the notation h is introduced to distinguish the total storage of water in each u grid cell an integral property of the solution from the pointwise depth d which varies spatially within the cell and is downscaled to the z grid resolution similarly the notation p and q is also used to designate integral cell average properties of the solution in contrast to the pointwise variables ud and vd respectively to link water level η and storage h in each u grid cell storage curves are computed by summing over z grid cells as follows 9 h η 1 α 2 i 1 α j 1 α max 0 η z i j eq 9 assumes a horizontal water level within each u grid cell which is a good approximation of most floods but for improved accuracy primo considers slopes in the water level δxη and δyη when updating the solution in time as reported by begnudelli 2016 slopes are computed using the minmod limiter e g sanders and bradford 2006 for example to compute slopes in the x direction first the left and right slope are computed separately as follows 10 δ x η l a η i m j m η i m 1 j m δ x η r b η i m 1 j m η i m j m and in a second step the minmod limiter is applied to give the slope 11 δ x η i m j m sgn a min a b if a b 0 0 otherwise this procedure is repeated in the y direction to compute δ y η i m j m we note that primo does not compute slopes in discharge values as is common in second order accurate godunov type finite volume schemes we summarize the main data structures in primo as follows topographic data z and optionally manning resistance parameters nm are raster fields with a cell size δ z and topographic data along edges of the u grid z are also saved with a cell size of δ z the solution to the shallow water equations is given by h p q η δxη and δyη which is saved on a the u grid with resolution δ u α δ z where α is the integer upscale factor to downscale the solution to the z grid resolution at location xi yj the free surface elevation η i j is computed as follows 12 η i j η δ x η x i x c δ y η y j y c where xc yc represents the cell center coordinates of the u grid cell the downscaled depth is subsequently computed as d i j max η i j z i j 0 2 2 solution update scheme the solution is updated using a godunov type finite volume scheme which involves the application of approximate riemann solvers to compute mass and momentum fluxes however a distinguishing feature of primo is that the approximate riemann solver is applied α times along each u grid edge once for each z grid edge and summed to give the integral along the boundary as indicated by eq 1 hence primo is designed to leverage the fine resolution topographic data in godunov type finite volume schemes the computation of fluxes by approximate riemann solvers is one of the most computationally demanding steps in a traditional model with n n cells for simplicity there are 2 n 2 2 n edges where the riemann solver is applied and n 2 cells where the solution is updated however by limiting flux calculations to the u grid edges the required number of calls to the riemann solver is reduced to 2 n 2 α 2 n and the number of solution updates is reduced to n α 2 hence calls to the riemann solver and solution updates are reduced by a factor of α and α 2 per time step we will show later that the number of riemann calls can be reduced further with a sampling technique whereby fewer than α calls to the riemann solver are made to integrate the flux along the u grid edge as an aside the number of riemann calls can also be reduced by using a cross sectionally averaged riemann solver that is called only once per cell edge but this requires the introduction of look up tables for interpolating the cross sectional geometry as a function of water level which adds to the computational effort further investigation is left for a future study the solution is updated by δt from time level n to n 1 using a fractional step method such that friction terms are treated implicitly and therefore do restrict the time step beyond what is required for stability in the absence of friction in the first step the solution is advanced to an intermediate step by based on the flux terms as follows 13 u i m j m u i m j m n δ t α δ z k 1 α δ f x l k i m 1 2 j m k 1 α δ f x r k i m 1 2 j m k 1 α δ f y l k i m j m 1 2 k 1 α δ f y r k i m j m 1 2 n where 14 δ f x l f x z x l δ f x r f x z x r 15 δ f y l f y z y l δ f y r f y z y r and the superscripts l and r indicate that z is evaluated on the left and right hand side respectively relative to an observer positioned with the cardinal directions i e either x or y positive to the right here the summation notation reflects the integration of fluxes over each u grid cell edge based its α values of z in the second step the solution is advanced to the next full time level by integrating the friction terms implicitly through a two point iteration as follows begnudelli 2016 16 u i m j m u i m j m m i m j m n 17 u i m j m n 1 u i m j m m i m j m where 18 m 1 0 0 0 1 r 1 0 0 0 1 r 1 here r is a dimensionless flow resistance parameter that is described in the next section iteration becomes increasingly important when the depth is very small use of two iterations was found to be adequate in the previously reported upscale type models begnudelli 2016 while others developing godunov type flood inundation models have recommended use of the newton raphson method to ensure that iterations are adequate xia et al 2017 2 3 resistance discretization the resistance parameter r follows from an implicit discretization of the source term eq 3 that is unconditionally stable when solving the classical shallow water equations the resistance parameter is commonly evaluated as follows 19 r n δ t g n m 2 v n h n 4 3 where v n u n 2 v n 2 1 2 however when there is a distribution of topographic heights within a u grid cell and a distribution of resistance parameters eq 19 can lead to significant errors in the estimate of the friction slope stelling 2012 several researchers have formulated expressions designed to aggregate the effects of a spatial distribution of depth velocity and empirical resistance parameters such as manning strickler or chézy defina 2000 stelling 2012 volp et al 2013 here we assume that the friction slope and water level is spatially uniform within each u grid cell and that the manning equation can be applied individually to each z grid cell based on d i j and nm i j stelling 2012 volp et al 2013 hence starting with the manning equation for the magnitude of the discharge per unit width p 2 q 2 1 2 s f 1 2 d 5 3 n m the average discharge with each u grid cell follows as 20 p 2 q 2 1 2 s f f 1 2 σ f where 21 σ f 1 α 2 i 1 α j 1 α d i j 5 3 n m i j expressions for the friction source terms in the x and y direction appearing in eq 1 follow as 22 g d s f x g h p p 2 q 2 1 2 σ f 2 g d s f y g h q p 2 q 2 1 2 σ f 2 where the overline reflects a cell average and we note that h p and q appearing on the right side of these equations represent u grid cell average quantities finally the dimensionless flow resistance parameter r required by eq 16 is given by 23 r n δ t g h n 1 p n 2 q n 2 1 2 σ f n 1 2 and the expression used for eq 17 is given by 24 r δ t g h n 1 p 2 q 2 1 2 σ f n 1 2 note that checks are needed to avoid division by zero application of eq 23 is computationally demanding because of the need to sweep over α 2 topographic data points z grid cells for each u grid cell run time computational costs are minimized by creating look up tables for σf as a function of η in a pre processing step in the construction of lookup tables inclusion of basis points comparable to the finest resolved depths is important for accuracy and stability xia et al 2017 2 4 computation of fluxes the first step of the solution update eq 13 involves the summation of α separate fluxes for each u grid cell edge calculation of fluxes is nearly identical in the x and y directions so the description here is limited to the x direction as follows 25 δ f x l k f k h z x l k δ f x r k f k h z x r k where f h denotes application of the harten lax and van leer hllc riemann solver as described in the next section and the superscripts l and r imply reconstruction of the solution from data contained in u grid cells to the left and right of the cell edge respectively to apply the approximate riemann solver for each value of z along the u grid edge the depth on the left and right side is reconstructed as follows 26 d k l max η l 1 2 δ x η l z k 0 d k r max η r 1 2 δ x η r z k 0 and the discharge per unit width is reconstructed based on a tolerance for depth δ w 1 10 6 m as follows 27 p k l r p l r d k l r δ w 0 otherwise 28 q k l r q l r d k l r δ w 0 otherwise on the other hand to compute the bottom slope terms zx l and zx r cf eq 3 the depth is reconstructed assuming a horizontal free surface 29 d η o k l max η l z k 0 d η o k r max η r z k 0 an important consequence of the above discretization of fluxes and bottom slope source terms is that δfx l δfx r δfy l and δfy r are identically zero at every z grid edge for stationary solutions defined by δ x η 0 δ y η 0 p 0 and q 0 2 4 1 hllc solver the harten lax and van leer hll solver is a well known approximate riemann solver that has been applied extensively in godunov type shallow water models and the hllc solver is an extension for two dimensional flows that contain a shear wave toro 2001 implementations of the hllc solver differ depending on how wave speeds are computed and treatment of dry cells and the approach used here is what toro 2001 p 182 183 describes as a simpler version as follows when both the left and right side cell is dry f hll 0 otherwise fluxes in the x direction f h f 1 h f 2 h f 3 h t are computed as follows 30 f 1 2 h f 1 2 l s l 0 f 1 2 r s r 0 s r u 1 2 l s l u 1 2 r s l s r u 1 2 r u 1 2 l s r s l otherwise and 31 f 3 h f 1 h v l f 1 h 0 f 1 h v r f 1 h 0 when dl δw and d r δw both the left and right sides of the edge are wet and the wave speeds are given by 32 s l min u l a l u a s r max u r a r u a where a l g d l 1 2 a r g d r 1 2 and 33 u 1 2 u l u r a l a r a 1 2 a l a r 1 4 u l u r in the case where the left side of the edge is dry dl δw the wave speeds are based on the exact solution to the dry bed problem as follows 34 s l u r 2 a r s r u r a r and in the case where the right side of the edge is dry d r δw the exact solution is used again 35 s l u l a l s r u l 2 a l for fluxes in the y direction the hllc solver differs only slightly 36 f 1 3 h f 1 3 l s l 0 f 1 3 r s r 0 s r u 1 3 l s l u 1 3 r s l s r u 1 3 r u 1 3 l s r s l otherwise and 37 f 2 h f 1 h u l f 1 h 0 f 1 h u r f 1 h 0 and the wave speeds are computed the same as in the x direction only v is used in place of u we note that roe s approximate riemann solver for the shallow water equations e g bradford and sanders 2002 was also tested and found to perform similarly to the hllc solver at slightly greater 10 computational expense 2 5 stability the time step δt is constrained by a courant friedrichs lewy cfl condition which on a 2d cartesian grid appears as follows 38 λ x max δ t δ u λ y max δ t δ u β where λ x y max represent the maximum wave speeds in the x and y directions respectively and β 1 2 when η slopes δxη and δyη are computed to update the solution if the η slopes are set to zero this corresponds to a purely first order accuracy mode and β 1 the stability condition can be simplified assuming λ x max λ y max as follows 39 λ max δ t δ u 1 2 β note that the maximum allowable time step for stability is increased by a factor of α compared to a model run at a resolution of δ z and this implies that fluxes and solution updates are reduced by a factor of α 2 and α 3 overall respectively 2 6 data sorting sampling and look up tables primo sorts the α 2 values of z in each u grid cell and the α values of z on each u grid edge from lowest to highest in a pre processing step using the sorted data two look up tables are computed for each u grid cell storage as a function of water level h η and a friction parameter as a function of water level σ f 2 η the sorted edge data also set the sequence by which fluxes are integrated along cell edges as indicated by eq 13 i e the fluxes for the deepest edge are computed first and the fluxes for the shallowest edge are computed last computational costs are further reduced in primo by limiting the number of entries in look up tables which reduces the basis points used for interpolation and by limiting the number of z values where fluxes are computed which reduces the number of calls to the approximate riemann solver if we consider the number of z data points used for either interpolation or flux calculation to be np then n p α for u grid edges and n p α 2 for u grid areas to implement a sampling strategy involving a reduced number of points ns np the indices for sampling s m m 1 n s are computed as follows 40 s m 1 floor m 1 n p 1 n s 1 m 1 n s which ensures that the lowest and highest values of z are always used as basis points in addition to intermediate points furthermore when fluxes are computed using a reduced number of z data points each flux is weighted by α ns to give the total flux correctly along the u grid edge as required by eq 13 the process of sorting sampling and evenly weighting the chosen topographic data points is presented graphically in fig 2 when using a sampling strategy for z the total number of edges required for flux calculations is reduced in proportion to fraction of cells that are sampled for example if α 10 and n s 5 as shown in fig 2 then the number of calls to the riemann solver is reduced 50 the notation used later to indicate sample size for flux calculations interpolation of volume versus water level and interpolation of the resistance term versus water level is given by n s flux n s vol and n s fric respectively 2 7 parallel implementation primo is configured for parallel execution using message passing interface mpi directives so it is applicable to either shared memory distributed memory or hybrid high performance computing systems spmd is adopted whereby domain decomposition is used to create separate input data for each process the spatial data defined by a nx ny raster at resolution δ z is subdivided into a grid of mx my subdomains or tiles each containing nx mx ny my raster cells at resolution δz care is taken in grid preparation so nx mx and ny my are multiples of the upscale factor α to enable the subgrid modeling technique in many applications there will be one or more tiles within the mx my grid where no flooding occurs and there is no need to perform flood computations under these conditions primo can be configured so processes only run on tiles of data where flooding is expected the solution variables needed to compute fluxes on edges that divide subdomains are exchanged between processes ever time step along each subdomain boundary edge the variables passed between processes are on u v and η from the first layer of boundary data and η from the second layer of boundary data since these variables are all defined on the u grid there are nx αmx values of each variable exchanged in the along horizontal edges and ny αmy values of each variable shared along vertical edges these variables are packed into a single buffer for each process subdomain so at most eight messages are exchanged between each process every time step two way messaging with four neighbors additional messages containing topographic data are exchanged in a pre processing step so each process has the necessary data to compute the elevation data required for fluxes and source terms z two layers of η values are exchanged so η slopes δxη and δyη can be computed within the first layer of u grid cells inside and outside of the subdomain boundary computing and communications are overlapped to maximize parallel efficiency specifically flux and slope calculations that are not dependent on exchanged boundary data edges and cells within the interior core of each subdomain are performed in parallel with non blocking mpi directives that exchange data between processes this leads to fluxes for edges along subdomain boundaries and slopes in cells along boundary edges that are computed once by two different processes and thus greater computational effort overall than for sequential execution duplication of computations could be avoided by sharing a combination of solution data and flux data across between subdomains but this comes at the cost of greater complexity to the message passing strategy and greater latency and the added cost appears to have negligible impact on performance based on results presented in the next section lastly mpi allreduce is called once per time step to compute a global time step for all processes that satisfies the cfl condition on all subdomains the main time loop of the algorithm proceeds as follows time loop begins boundary data packed into buffer for transfer one layer of u and v values and two layers of η values non blocking mpi irecv called to receive boundary data non blocking mpi isend called to send boundary data slopes δxη and δyη computed for the interior core fluxes computed for the interior core mpi wait called to ensure boundary data transfer complete transferred buffers unpacked into solution variables slopes computed for layer of cells i inside and ii outside of model subdomain boundary fluxes computed on i boundary edges and ii first interior edges of flow model subdomain grid mpi allreduce called to set global time step δt solution advanced δt from time level n to n 1 time loop ends the parallel implementation of primo leads to balanced compute loads and message sizes when all cells across the domain domain are wetted h δw and unbalanced loads when there are partially wetted cells and or cells that are masked and saved as nodata when cells are not wetted dry primo will sweep over adjacent edges to compute fluxes and update the solution with presumably zero valued fluxes hence dry cells also demand computations but the number of computations is reduced because the approximate riemann solver includes logical checks that return zero valued fluxes when two neighboring cells are dry additionally we note that after p and q are computed following eq 16 u and v are computed as p h and q h respectively in all cells with h δh where δh is typically set to 0 001 m although smaller values can be used hence primo will exchange fluid between cells whenever h δw but u and v are computed only when h δh otherwise u and v are set to zero 2 8 summary of model features as described above primo is developed with several features to balance accuracy and computational effort 1 the shallow water equations are solved on an upscaled grid u grid with cell size δ u α δ z where α represents the upscale factor 2 fluxes are integrated along each u grid cell edge by summing over a sample of n s flux α elevation data points 3 storage is tabulated versus water level based on the α 2 elevation data points within each u grid cell and interpolation is performed using a limited number of basis points n s vol α 2 4 a friction parameter σ f is tabulated versus water level based on the α 2 elevation data points within each u grid cell and interpolation is performed using a limited number of basis points n s fric α 2 5 slopes in the water surface δxη and δyη are computed for each u grid cell for more accurate estimation of fluxes and more accurate downscaling of depth to the z grid 6 parallel execution is enabled for either shared memory or distributed memory compute clusters with mpi directives the remainder of the paper presents test cases to verify the model with analytical solutions and an observed dam break flood test its parallel scaling and benchmark its performance in a hypothetical whole city scale application based on these results strengths and weaknesses of the method are presented in the discussion section along with promising directions for future research 3 applications 3 1 dry bed dam break problem we consider a 1 km long and 100 m wide channel that is horizontal and frictionless and create a z grid at 1 m resolution for input into primo initially water is a depth of 3 m for x 500 m and motionless and dry elsewhere the solution is integrated for 40 s and saved every 10 s with a velocity calculation tolerance δ h 1 10 6 m for improved accuracy at the wetting front fig 3 compares predictions red with the exact solutions gray under four different model configurations a an upscale factor α 1 no upscaling without η slopes i e δ x η 0 b an upscale factor α 1 with η slopes i e δxη 0 c an upscale factor α 5 without η slopes and d an upscale factor α 5 with η slopes these results show that primo performs similar to other first order accurate godunov type schemes when α 1 and δ x η 0 fig 3a that is the solution remains monotone there is a small diffusive error in depth at the leading edge of the rarefaction wave there is a phase error in the leading edge of the wetting front and there is diffusion of the velocity profile at the leading edge of the wetting front when α 1 and η slopes are computed fig 3b there is less numerical diffusion as expected and the phase error of the wetting front increases slightly fig 3c and d show that the effect of upscaling is similar to the effect of grid coarsening in this case the effective grid resolution is 5 m instead of 1 m and there is a noticeable increase in numerical diffusion in the depth profile at the leading edge of the rarefaction wave and in the velocity profile at the leading edge of the wetting front however increasing the upscale factor does not substantially change the phase error of the wetting front 3 2 parabolic floodplain with meandering channel we now consider an idealized parabolic floodplain with a meandering channel to test the model for compound channel cross sections as shown in fig 4 the spatial domain is rectangular with a length of l x 6 km and a width of l w 1 km and we assume δ z 1 m the meandering channel has a rectangular cross section with a width of w 20 m measured perpendicular to the channel centerline additionally the meander takes on a sinusoidal form with a wavelength of 1 km and an amplitude of 100 m based on a coordinate system with the origin placed in the lower left corner of the domain dtm elevations in meters are computed as follows 41 z floodplain x y 1 10 4 x 1 10 5 y 3 000 2 42 z channel x y 1 10 4 x 2 except for the first and last 500 m of the domain where z 2 m as shown in fig 4 note that the channel is 2 m deep at y 3000 m and slightly deeper at the top and bottom of the meanders where the floodplain elevation is slightly higher due to the parabolic cross sectional profile the channel and floodplain are dry at t 0 all boundaries are treated as solid walls and flooding is initiated by a constant inflow of q 1000 m3 s specified as a point source at x 5800 m and y 500 m this leads to a flood front moving right to left over a dry bed first into the channel and then over the floodplain the solution is integrated for 2 hr and flow resistance is modeled with n m 0 03 m 1 3 s in each case the sampling for fluxes volume and friction was given by n s flux α 2 n s vol 2 α and n s friction 2 α fig 5 shows contours of flood depth at t 100 min without η slopes left and with η slopes right for α 200 100 50 and 10 top to bottom first the model is shown to route flows through channels for all values of α including cases where δ u much larger than the channel width i e α 200 corresponds to δ u 200 m fig 5 also reveals sensitivities in the propagation of the flood front to upscaling and the use of η slopes that are similar to dam break test case considered previously that is simulations without η slopes labeled δ η 0 in fig 5 exhibit slightly faster flood front movement than with η slopes labeled δη 0 in fig 5 differences in the position of the flood front and the depth of flooding due to upscaling appear minimal when η slopes are computed δη 0 fig 5 shows that the leading edge of the flood front extends close to x 1000 m along the flood plain for the four different values of α and the leading edge of the flood front in the channel extends very close to the channel outlet or slightly past it on the other hand when η slopes are not computed δ η 0 there is a substantial differences in the progression of the flood front between α 10 flood front extending roughly to x 1000 m and α 200 flood front extending past x 500 m hence flood simulations using η slopes appear to reduce sensitivity of the flood wave progression to the upscale factor a weakness of upscaling methods that has been reported in previous studies is the potential for granularity in downscaled flood depths hénonin et al 2015 and the appearance of granularity is prominent in fig 5 when η slopes are not computed and α 100 and 200 however granularity does not appear in fig 5 when η slopes are computed and used to compute the downscaled flood depth hence use of the η slopes offers an advantage with respect to more accurate downscaling of the flood depth in the remaining tests all simulations involve use he use of η slopes for solution updates and downscaling 3 3 baldwin hills dam break test case model accuracy in a practical test case is now considered by applying primo to simulate urban flooding from the 1963 failure of the baldwin hills dam previous studies have shown that flood extent can be predicted with over 70 accuracy in this test case gallegos et al 2009 schubert and sanders 2012 hence primo is applied here using several sources of data previously reported by gallegos et al 2009 1 a 1 5 m resolution dtm 8 5 cm vertical rmse 2 the location height and length of curb inlets to storm drains 3 a spatially distributed resistance parameter distribution based on land cover including vegetated open space n m 0 05 concrete surfaces n m 0 013 reservoir n m 0 013 roads n m 0 014 ballona creek n m 0 016 and buildings n m 0 3 4 breach geometry data 5 flood extent data for validation 6 as built reservoir geometry data and 7 an estimate of the water level at the time of failure η 141 m navd88 needed for the initial condition the effect of structures on flooding dynamics is treated with the building resistance method schubert and sanders 2012 and breaching which took place over 20 min was assumed to occur instantaneously fig 6 shows the site topography flood extent data and storm drain locations modeling of storm drain interception of overland flow by curb inlets is based on either an orifice or a weir equation gallegos et al 2009 given a curb inlet with a length l and height ho and a local depth prediction d the interception rate is computed as 43 q c d l min d h o κ g d where cd is a discharge coefficient κ 1 corresponds to a weir approximation for d ho and κ 2 corresponds to an orifice approximation that is used otherwise here c d 0 5 was specified to match a previous calibration by gallegos et al 2009 primo was implemented in parallel using 16 processors on the uci hpc cluster which consists of 64 core compute nodes with 2 33 ghz amd processors and 512 gb of ram fig 7 shows the accuracy of flood extent predictions using α 2 5 10 and 20 which corresponds to δ u 3 7 5 15 and 30 m here correct predictions of flooding green areas of overprediction blue and areas of under prediction red are shown additionally table 1 presents wall clock execution times and metrics of flood extent accuracy including an agreement metric fa and overprediction metric fo and an underprediction metric fu schubert and sanders 2012 perfect accuracy corresponds to f a 100 f o 0 and f u 0 these results show that primo simulates flood extent with a high level of accuracy roughly 70 for α 2 5 and 10 without model specific calibration that is parameter values and taken from previous studies these results also show that flood extent accuracy differs by only 1 between α 2 and 5 and by 3 between α 2 and 10 which is likely within the accuracy of the measured flood extent data hence the potential exists for significant upscaling without significant loss of accuracy however there are limits α 20 leads to substantially increased overprediction and a reduction of flood extent accuracy by about 11 compared to the case with α 2 finally table 1 shows that increasing α significantly reduces wall clock execution times as expected in the remaining test cases attention turns to the parallel scaling of primo and the potential for fast simulation of flooding over large spatial domains 3 4 load balanced parallel performance testing domain decomposition into tiles with equal size leads to perfect load balancing across processes if the domain is fully wetted and thus the number of operations on each processor per time step is equal fig 8 shows a square domain of 1 m resolution topographic data spanning 4 km2 of the platte river in central nebraska data previously used for hydrodynamic river modeling schubert et al 2015 hence the raster is 2000 2000 cells which is amenable to subdivision several times into grid sizes that are multiples of 10 and 20 and supportive of upscaling with relatively large values of α the test problem involves an initial condition corresponding to a constant water level η 712 m navd 88 which inundates the highest topography in the domain by only several cm and a fluid velocity of zero wall boundary conditions are specified on all four sides and a point source q 300 m3 s is specified in the main channel near the left boundary of the domain which represents the upstream end of this river reach flow resistance is not important but is modeled using a spatially uniform manning coefficient n m 0 03 m 1 3 s the simulation period for this test problem is 6000 s to test the parallel performance of primo the domain was divided twice four times and eight times in each cardinal direction leading to grids designed for nproc 1 4 16 and 64 processes and upscaling using α 10 as an example fig 8 shows the domain decomposition for nproc 16 and we note that the numbering of processes begins with zero consistent with the standard mpi convention primo sampling parameters used in this test case were n s flux 5 n s vol 11 n s fric 11 testing was performed on the uci hpc cluster and the cheyenne cluster at the ncar wyoming supercomputing center which consisted of 36 core compute nodes with 2 3 ghz intel xeon processors and 64 gb of ram both systems use infiniband high speed interconnect for communication between nodes fig 9 shows that parallel performance closely tracks the ideal speedup profile on the cheyenne cluster and on the uci cluster when primo is run in a shared execution mode whereby other jobs run concurrently on the node however primo does not demonstrate ideal scaling on the uci cluster when jobs have exclusive access to the node this is attributed relatively faster execution of the test involving nproc 1 which suggests that the hardware is performing internal optimizations to take advantage of idle computing power similar behavior has been observed in previous studies of parallel performance e g sanders et al 2010 and run times using nproc 64 were equivalent using the uci cluster in shared and exclusive modes we note that the case involving nproc 64 required use of two nodes on the cheyenne cluster and communication between nodes yet the parallel performance exhibits no adverse consequences both clusters could support larger jobs requiring significantly more cores but this 2000 2000 cell test problem doesn t justify use of additional cores using 64 cores the 6000 s simulation period requires only about 10 s of wall clock time on the cheyenne cluster which is significantly faster than the uci cluster due to a compiler that is optimized for the hardware a more demanding test problem requiring numerous compute nodes is created by randomly generating topographic heights at 1 m resolution over a spatial extent of 1 024 km2 assuming a uniform distribution between 0 and 1 m this corresponds to a topographic grid of 32 000 32 000 1 024 109 cells which conveniently supports domain decomposition for 16 cores with a 4 4 matrix of 8 000 8 000 cell grids 64 cores with a 8 8 matrix of 4 000 4 000 cell grids 256 cores with a 16 16 matrix of 2 000 2 000 cell grids and 1024 cores with a 32 32 matrix of 1 000 1 000 cell grids the test problem is defined by an initial water elevation of 3 m wall boundary conditions on all four sides a rainfall rate of 1 cm hr a spatially uniform n m 0 03 m 1 3 s and a simulation period of 1 hr primo is run on the cheyenne cluster using nproc 16 64 256 and 1024 processes and α 10 primo sampling parameters used in this test case were n s flux 5 n s vol 11 n s fric 11 fig 9 shows that speedup is better than ideal when normalized by the execution time for 16 cores the reason for better than ideal performance is not immediately clear but it could be due to low memory demands per node or to more efficient communication realized by smaller sized arrays being passed between processes as the number of processes increase the 1 hr simulation completed in 98 s using 1024 cores how does the choice of the upscale factor affect run times fig 10 shows that wall clock times using α 4 10 and 20 closely follow a power law function of the upscale factor of the form t t o α k ideal performance hence these results demonstrate that primo achieves ideal power law run time reductions from two separate mechanisms parallel processing and upscaling and the combination of these mechanisms allows for an hourly simulation of flooding at 1 m resolution over 103 km2 in a matter of seconds a level of performance well suited to nowcasting and forecasting applications 3 5 los angeles region extreme flooding test as a final test we consider a simulation of flooding across los angeles the second largest city in the usa to benchmark wall clock times and test the computational feasibility of primo for time sensitive applications such as real time forecasting and uncertainty quantification with monte carlo simulation to achieve a reasonable calibration of a flood hazard model for an urban area many factors must be carefully considered including the representation of flow pathways in the dtm subsurface drainage through pipes storage in reservoirs flow resistance the variability in precipitation and surface subsurface interactions such as infiltration e g gallegos et al 2009 gallien et al 2011 since the objective here is only to benchmark wall clock times and validate the shallow water routing of pluvial flooding and calibration requires observations of observed flooding of extreme events that are not easily obtained for many reasons no attempt is made to calibrate the model the los angeles model domain spans the portions of los angeles county that drain to the south and west from the san gabriel mountains the model was developed from a 10 ft resolution dtm for the county of los angeles usgs 2006 which was prepared for public access by downsampling a 5 ft resolution dtm with 0 91 ft vertical accuracy at the 95 confidence level the spatial domain was trimmed along the north and east based on the limits of the los angeles river watershed spanning the areas where the vast majority of the county s 10 million people reside u s census bureau 2010 the trimmed domain was subsequently partitioned into 608 square tiles consisting of 103 103 raster grids as shown in fig 11 a number that allowed for parallel execution with 32 processes per node across 19 compute nodes on cheyenne hence a total of 608 million z grid points are included in the model and the domain spans an area of 2181 mi2 or 5 652 km2 the hazard scenario that forms the basis of this test corresponds to the coincidence of a record rainfall with record high tide and likely has a return period of at least 1000 years goodridge 1994 a constant water level boundary of η b 8 ft navd 88 is assigned along the coastal boundary representative of the record high tide level which roughly corresponds to the maximum recorded water level in los angeles harbor noaa gage 9410660 and a constant effective rainfall rate of p 1 in hr is specified uniformly for a period of t 12 h over which flooding is simulated this rainfall rate roughly corresponds to historical maximum conditions within los angeles county where 26 in of rainfall was once recorded over a 24 h period at a gage in the san gabriel mountains and on several occasions more than 10 in of rainfall have been recorded over a 12 h period goodridge 1994 the assumption of spatially uniform rainfall at this rate is not consistent with observations as rainfall is magnified in the san gabriel mountains by orographic effects nevertheless it is a useful simplification for measuring the run time attributes of primo and creating a scenario that can be easily communicated i e 1 in hr rainfall for 12 h four simulations were completed using α 5 10 20 and 50 and other model parameters shown in table 2 primo was executed on cheyenne and wall clock times are also reported in table 2 this shows that wall clock times are reduced from about 6 h using α 5 to about 15 s using α 50 flood depth at t 12 hrs using α 5 is shown in fig 12 a and points to extensive inundation across the region as flood flows move south from the san gabriel mountains towards long beach cf fig 11 indeed the region between los angeles and long beach is an amalgam of alluvial fans where the simulation shows flood water spreading out and pooling behind a major east west highway ca route 91 as α is increased predicted drainage patterns remain similar but the depth behind obstructions such as freeways and dams is reduced and the depth near the southern coastline of the study area is locally increased the mean absolute value of δd was computed to be 0 17 ft 5 2 cm 0 28 ft 8 5 cm and 0 47 ft 14 cm for α 10 20 and 50 compared to the α 5 case reduced pooling behind obstructions is attributed to flow cell edges that no longer capture important blockage features such as elevated roadways or embankments this weakness of upscaling methods has previously been reported by hodges 2015 nevertheless the cases involving α 10 and 20 show potential for forecasting applications based on run times of 66 and 4 8 min respectively and the sensitivity to the upscale factor more detail is found in fig 13 where flood depth in the regions marked a d in fig 11 is shown based on α 10 fig 13a shows a reach of the los angeles river that flows east through a flood detention basin the sepulveda basin and then through the community of sherman oaks primo simulates overtopping of the sepuveda basin spillway which is associated with transcritical flow regimes as well as compound fluvial pluvial flooding in sherman oaks due the combined effects of los angeles river flows from the west and pluvial runoff from the north fig 13b shows where the san gabriel river descends south out of the san gabriel mountains over 17 drop structures that dissipate energy and into the santa fe dam recreational area which is a major flood control facility in the area modeling of flow over the drop structures leads to transcritical flows with hydraulic jumps that are easily resolved by the model fig 13c shows the combined effect of fluvial flooding and pluvial flooding in downtown los angeles the area west of the los angeles river is an alluvial fan and the simulation shows several preferential pathways for floodwater within this highly urbanized area finally fig 13d shows the combined effects of pluvial fluvial and coastal flooding in the vicinity of long beach where the los angeles river east side of the harbor complex and dominguez channel west side of harbor complex empty into the pacific ocean through the ports of long beach and los angeles the busiest port in the usa based on container shipments 4 discussion upscaling in shallow water modeling of flood inundation is relatively new presented here is a technique for retaining the precision of fine scale topography and the skill of approximate riemann solvers to estimate fluxes in a relatively course solution update with a godunov type finite volume scheme the goal is to improve computational efficiency and resolve a wide range of flows without overly sacrificing accuracy how fluxes are evaluated and in particular how topographic data are used to inform fluxes is arguably the most important consideration in the design of an upscaling method and the best strategy will depend on many factors including the type of numerical solver used for this study a distribution of topographic data from z grid cells along each u grid edge were computed to support flux estimation as shown in eq 4 to enable implementation of a godunov type finite volume scheme another approach used for an implicit finite volume scheme is to integrate all of the z data within a staggered u grid cell volp et al 2013 and hodges 2015 proposed processing z values to identify topographic objects that control the spreading of flood water and subsequently assigning z values to edges to preserve the functionality of the object others have pursued adaptive meshing approaches that increase resolution around key topographic features hou et al 2018 indeed many options are possible and more research is needed to deepen understanding of strengths and weaknesses and to develop improved methods results presented here point to several strengths including the ability to resolve flooding over a wide range of topographic and flow conditions the ability to resolve flow in channels resolved on the z grid but finer that the resolution of the u grid the ability to improve accuracy by making η piecewise linear instead of piecewise constant and the ability of the scheme to support ideal parallel scaling which allows the method to be efficiently applied at any scale another major advantage of this upscaling method is that a single parameter α can be changed to best meet accuracy and run time requirements that is there is no need for time consuming re gridding to make a model run faster the main weakness of the method is that blockage features may not be correctly resolved which leads to artificial spreading of flood water additionally the method does not overcome a well known limitation of finite volume schemes whereby flow down steep slopes is modeled as a series of waterfalls kim et al 2012 xia et al 2017 in fact use of piecewise linear η changes the representation of this waterfall dynamic and additional research is needed to develop improved representation of flow down steep slopes in conjunction with upscaling methods moreover while the scheme conserves mass to numerical precision based on the direct discretization of conservative equations and the consistent updating of the discrete solution based on globally computed flux values the scheme does not guarantee positivity the prediction of negative depth is possible if the volume of water predicted to leave a cell over one time step exceeds volume stored in the cell begnudelli and sanders 2006 bradford and sanders 2002 brufau et al 2004 in the test cases considered negative depth predictions were minimal and presented no noticeable impacts on accuracy or stability but strategies to address this limitation in conjunction with upscaling methods could be considered the future whereby tradeoffs between accuracy stability and computational effort in parallel execution are examined it is unclear whether primo would have scaled better or run faster had it been implemented on a gpu or a cluster of gpus instead of a cluster of multi core compute nodes several studies point to fast flood inundation model performance on gpus brodtkorb et al 2012 castro et al 2011 neal et al 2010 vacondio et al 2014 and as the size of applications increases memory and communication demands increase and ultimately this controls the parallel scaling of the model additional research is needed to address this possibility and improve understanding of the strengths and weaknesses of gpus and multi core compute nodes to guide development of future models a broader issue raised by this research is the potential to deliver timely flood hazard information at fine resolution over whole city scales world population has more than doubled since 1960 from around 3 billion people to around 7 billion today and the exposure and vulnerability of communities to flooding has been magnified with over half of the global population concentrated in cities that are located along coastlines or major rivers hinkel et al 2014 wolsko and marino 2016 this motivates a pressing need for flood risk information that is responsive to the wide web of decision making causing the increase in exposure and vulnerability or actionable information morss et al 2005 spiekermann et al 2015 fine scale visualizations of flooding have tremendous power for communicating flood hazards in intuitive ways that anyone can understand lane et al 2011 luke et al 2018 and collaboration with stakeholders on visualizations can meet site specific decision making needs in a context sensitive way for all phases of the disaster management cycle including planning preparedness early warning emergency response and recovery dawson et al 2011 evers et al 2012 luke et al 2018 pasche et al 2009 steinführer 2009 what is demonstrated here is the computational feasibility of a fast response fine resolution hydrodynamic flood hazard simulation over a large metropolitan area transitioning from a feasibility to a reality introduces several more research challenges deserving attention including a the ability to accurately force flood inundation models with rainfall streamflow and ocean height data b capturing all important hydrological processes that contribute to the distribution of flood water e g urban drainage infrastructure and c managing the enormous volumes of data associated with time dependent fine resolution spatial fields over large spatial extents lessons learned from fine resolution flood hazard modeling over smaller spatial extents are relevant here for example dtms can be conditioned to improve the representation of drainage pathways and representation of flow barriers such a levees but models have generally relied on manual processing or at best semi automated methods e g gallegos et al 2009 gallien et al 2011 schubert and sanders 2012 which is tedious over the scale of entire metropolitan areas like los angeles nevertheless dtm processing is clearly needed in this case based on the amount of ponding predicted to occur behind highways where flood extent is surely overpredicted given that highway underpasses are not resolved by the model poor channel bathymetry accuracy in the dtm is also likely to be a contributor to the overprediction of flood depths and flood extent in coastal areas and motivates the need for coastal dtms that merge the best available topographic and bathymetric data such as those developed by noaa noaa national center for environmental information 2018 5 conclusions a godunov type finite volume scheme for solving the shallow water equations primo is implemented with an innovative sub grid model that enables flow computations at scale δ u that is a multiple α of the dtm resolution δ z the method reduces the number of flux computations by at least a factor of α per time step and the number of solution updates by a factor of α 2 per time step compared with a flow model that performs flow computations at a resolution of δ z additionally the maximum allowable time step for stability is increased by a factor of α compared to a model run at a resolution of δ z and thus fluxes and solution updates are reduced by a factor of α 2 and α 3 overall respectively execution time is further reduced by a sampling technique that limits the number of times the approximate riemann solver is used each time step to compute fluxes additionally piecewise linear treatment of the free surface elevation η is combined with upscaling to reduce numerical dissipation and improve the accuracy and visualization of downscaled flood depth primo is shown to simulate subcritical supercritical and transcritical flows which is expected of a godunov type flow solver that uses an approximate riemann solver suited to all classes of shallow water flow primo is also shown to propagate flow through channels that are resolved on the z grid but finer than the u grid additionally piecewise linear treatment of the water surface η is shown to improve accuracy of downscaled flood depths over piecewise linear models δ η 0 application of primo to the baldwin hills dam break flood showed the potential to upscale up to α 10 in an urban area using δ z 1 5 m with negligible impact on flood extent accuracy while further increases in α lead to overprediction of flood extent the los angeles test case showed practical benefits such as the ability to simulate flow over spillways and weirs flow down steep slopes and backwater effects such as those that occur near the coastline additionally the los angeles test case showed the potential to simulate compound flooding involving pluvial fluvial and coastal flood hazards primo is shown to achieve ideal parallel scaling with up to 1024 processes the largest problem tested the combined effects of upscaled flow computations and parallel execution enable metric resolution simulations over thousands of km2 and hourly time scales in a matter of seconds to minutes depending on flow conditions depth and velocity which control the maximum allowable time step this capacity is well suited to supporting time sensitive applications such as nowcasting and forecasting and implies that hydrodynamic simulations can be run in real time to assist with several phases of disaster management including early warning emergency response and recovery fast execution times also increase the feasibility of monte carlo simulation to characterize uncertainties for planning preparedness and mitigation aspects of disaster management acknowledgements the upscaling method adopted by primo was conceived by b sanders and on several occasions was discussed with l begnudelli who was involved in the development of a separate model that uses a similar upscaling technique the authors acknowledge l begnudelli for valuable dialogue and correspondence which contributed to this work the authors also acknowledge b m ginting for valuable dialogue and correspondence regarding parallelization strategies a luke for valuable dialogue regarding the practical value of fast simulations and j vrugt for suggestions to improve the paper this research was made possible by a grant from the national science foundation dms 1331611 we also like acknowledge the use of the uci high performance computing system http hpc oit uci edu and computational resources doi 10 5065 d6rx99hx at the ncar wyoming supercomputing center provided by the national science foundation and the state of wyoming and supported by ncar s computational and information systems laboratory finally we thank the anonymous reviewers for comments and questions that helped us to improve the paper 
688,the objective of this study was to investigate the effect of wettability heterogeneity on pore scale characteristics of supercritical sc co2 displacement dynamics and its capillary trapping mechanism during a scco2 brine drainage and imbibition cycle a multiphase lattice boltzmann lb model was employed to simulate scco2 brine flow in rock samples of tuscaloosa sandstone taken from the cranfield co2 injection site using a spectral method we adopted various wettability fields to generate rock samples containing distributed co2 wet regions to gain a better insight into the effect of fractional wettability on scco2 displacement patterns during drainage we quantified the evolution of scco2 interface with brine and rock surface for samples with various wettability heterogeneities in addition the effect of heterogeneous wettability on the drainage relative permeability and capillary pressure curves has been investigated in this study according to our results heterogeneous distribution of co2 wet regions in the rock leads to more dispersed fluid distribution and hence more tortuous flow paths resulting in higher interfacial area between fluid phases and rock surface at any given scco2 saturation furthermore the spatial distribution of wettability controls the scco2 entrapment pattern and spatial distribution of residual scco2 clusters during brine flooding in fractional wet samples residence of scco2 phase in co2 wet regions creates more trapped scco2 clusters suppressing the connectivity of the co2 phase thus enhancing more residual trapping our results imply that the total number of scco2 clusters and as a result their residual trapping increases as the fraction of co2 wet regions becomes larger leading to a larger surface area of scco2 with brine and rock surface potentially facilitating the likelihood of long term dissolution and mineral trapping keywords porous medium heterogeneity multiphase flow wettability lattice boltzmann modeling co2 geo sequestration 1 introduction sequestration of carbon dioxide co2 in geological formations is considered to be one of the most promising solutions for mitigating carbon emissions and hence global warming dashtian et al 2019 shaffer 2010 injection of co2 into deep saline aquifers is known to be the most feasible option for carbon storage because these aquifers have the largest accessibility and highest storage capacity bachu 2003 bakhshian and sahimi 2017 bakhshian et al 2018 dashtian et al 2018 hosseini and nicot 2012 kharaka et al 2009 mathias et al 2008 michael et al 2010 o carroll et al 2005 soltanian et al 2016 an understanding of the behavior of co2 in saline reservoirs is thus required for a better assessment of the efficiency of geo sequestration projects and optimization of subsurface flow management owing to the density contrast between co2 and brine the co2 plume is likely to flow upward and potentially leak through natural fractures faults well bores or caprocks macminn and juanes 2009 physical and chemical trapping mechanisms such as structural trapping capillary or residual trapping dissolution trapping and mineral trapping can reduce or prevent the possibility of upward co2 migration and lead to co2 immobilization in geological formations hesse et al 2008 iglauer 2011 juanes et al 2006 krevor et al 2015 after completion of the co2 injection process brine imbibes back into the pore space to displace drained co2 resulting in co2 being trapped as isolated blobs or ganglia krevor et al 2015 target storage formations therefore need to be selected carefully to ensure effective trapping mechanisms co2 migration and the trapping capacity of formations are highly influenced by rock type and wettabiliy preferences of constituent minerals wettability is generally quantified by contact angle θ which is the angle between the fluid fluid interface and the solid surface iglauer et al 2014 although most multiphase flow analyses are based on the simple assumption of uniform or homogeneous wettability in the reservoir rocks heterogeneity non uniformity of wettability is common in natural porous media al khdheeawi et al 2018 iglauer 2017 wettability heterogeneity which represents spatial variation of the wettability state in the rock formation is typically recognized as being one of two types mixed wettability and fractional wettability in a mixed wet medium wettability varies according to pore size distribution meaning that most of the wetting fluid is inclined to occupy the smallest pore channels however fractional wettability refers to a state in which different regions of the rock have various wettability preferences kovscek et al 1993 salathiel 1973 skauge et al 2007 fractional wettability in natural rocks might vary spatially owing to surface roughness wenzel 1949 variation in mineralogy borysenko et al 2009 aqueous chemistry demond et al 1994 or organic matter distributions dekker and ritsema 1994 wettability is a major factor that controls the flow and spatial distribution of fluids in porous media and macroscopic multiphase properties such as capillary pressure and relative permeability murison et al 2014 cieplak and robbins 1990 studied the effect of wettability on the fluid invasion pattern in two dimensional porous media and characterized three different meniscus motion types including overlap touch and burst they found that the finger width of the invasion pattern increases as the contact angle decreases and the medium becomes more wet to the invading fluid the divergence of invading fingers was observed at a critical contact angle θc holtzman and segre 2015 studied fluid invasion patterns in a pore scale model to investigate the effect of capillarity viscosity and wettability on immiscible displacement dynamics their results indicate that the increase of invading fluid wettability yields a cooperative pore filling mechanism and a stable invasion pattern whereas the effect of wettability becomes insignificant in a viscous fingering regime it has also been found that the wettability of reservoirs strongly affects the capillary trapping of co2 chalbaud et al 2009 chaudhary et al 2013 many studies suggest that residual trapping is suppressed as the medium becomes less water wet herring et al 2016b or more co2 oil wet rahman et al 2016 nevertheless several studies have shown that wettability alteration of pore surfaces to being less water wet improves residual trapping efficiency wang et al 2016 further studies on this subject are therefore essential for better prediction of reservoir performance during carbon storage additionally for simplicity most previous studies seem to have generally focused on multiphase flow in a uniformly wetted system and too little attention has been paid to the impact of wettability heterogeneity even though it significantly affects multiphase flow properties including recovery efficiency capillary pressure and relative permeability bradford and leij 1995 masalmeh 2003 most actual reservoirs represent heterogeneous wettability however in which different portions of rock have varying wettability preferences this study therefore demonstrates multiphase flow in rock samples having fractional wettability which strongly controls the pore scale fate of co2 during injection and post injection periods an understanding of the effect of wettability heterogeneity on pore scale fluid displacement is crucial for the prediction of large scale flow models the traditional grid based computational fluid dynamics cfd methods for multiphase flow simulations include volume of fluid vof rabbani et al 2018 and level set prodanović and bryant 2006 and phase field badalassi et al 2003 in these methods the fluid fluid interface which is captured using different volume functions is difficult to track in multiphase flows moreover a relatively fine grid is required in the vicinity of the interface to obtain a reasonable resolution on the other hand the lattice boltzamann lb method has emerged as a numerically robust technique for simulation of multiphase flows with sharp changes of the interface it is not required to employ an interface tracking or interface capturing step in the lb method whereas the phase separation is maintained automatically tölke et al 2013 the most widely used lb models are the shan and chen s potential model shan and chen 1993 the colour fluid model gunstensen et al 1991 the free energy model swift et al 1996 and the phase field based model he et al 1999 in this study we apply a multiphase lb model which has been introduced by gunstensen et al 1991 as an extension of the colour gradient model and can handle flow simulations with high viscosity ratios and low capillary numbers the objective of this work is to study the impact of wettability heterogeneity on scco2 brine two phase flow at the pore scale using the lattice boltzmann method we show how fractional wettability affects scco2 migration patterns during its drainage to the rock samples of tuscaloosa sandstone as well as scco2 residual trapping during brine imbibition first the lb model is introduced and its accuracy validated subsequently then we implement different wettability distribution fields in the rock sample using a stochastic method afterward displacement patterns and microscale distribution of scco2 are visualized and quantified in rock samples having various spatial wettability distributions finally residual trapping mechanisms of the samples with various fractional wettability are compared and a quantitative study is done on the trapped scco2 clusters 2 methodologies 2 1 multiphase lattice boltzmann method in this study an optimized colour gradient lb model tölke et al 2006 is adopted to simulate two phase flow in a rock sample and a d3q19 multiple relaxation time mrt lb scheme has been applied in the lb model the fluid is represented by a distribution function f i x t which undergoes the propagation and collision operators described by the following equation bakhshian and sahimi 2016 bao and schaefer 2013 1 f i x e i t t t f i x t ω i i 0 18 where δt represents the time step in lattice units ω i is the collision operator and e i is the velocity basis vectors the velocity set ei is given by 2 e i 0 0 0 i 0 c 1 0 0 c 0 1 0 c 0 0 1 i 1 2 6 c 1 1 0 c 1 0 1 c 0 1 1 i 7 8 18 where c δ x δ t is the lattice velocity and δx is the grid spacing the collision operator ω i is described by a multiple relation time mrt scheme which can be expressed as 3 ω m 1 s m f m e q where m is the transformation matrix which transforms the distribution function f to the moment space m as 4 m m f m eq m f e q where feq and meq are equilibrium functions at distribution and moment space respectively the detail of moment m and transformation matrix m can be found in previous studies ahrenholz et al 2008 d humières et al 2002 and the effect of interfacial tension σ is incorporated into the equilibrium moment vector meq as ahrenholz et al 2008 jiang et al 2014 5a m 0 e q ρ 5b m 1 e q e e q σ c 5c m 3 e q j x 5d m 5 e q j y 5e m 7 e q j z 5f m 9 e q 3 p x x e q 1 2 σ c 2 n x 2 n y 2 n z 2 5g m 11 e q p z z e q 1 2 σ c 2 n y 2 n z 2 5h m 13 e q p x y e q 1 2 σ c n x n y 5i m 14 e q p y z e q 1 2 σ c n y n z 5j m 15 e q p x z e q 1 2 σ c n x n z 5k m 2 e q m 4 e q m 6 e q m 8 e q m 16 e q m 17 e q m 18 e q 0 the definitions of c nx ny and nz are presented in eqs 11 and 13 the collision matrix s is a diagonal matrix with 19 relaxation parameters s i i also known as eigenvalues of the collision matrix m 1 sm 6 s s e s ξ 0 s q 0 s q 0 s q s ν s π s ν s π s ν s ν s ν s m s m s m 0 the relaxation parameter sν is defined as 7 s ν 2 6 ν c 2 δ t 1 where ν is kinematic viscosity the remaining parameters of the matrix s must lie between 0 and 2 to improve stability the optimal values of these parameters can be found in previous studies saito et al 2017 suga et al 2015 the fluid macroscopic properties including density ρ and velocity u are calculated using the following relations 8 ρ i f i 9 ρ u i e i ρ i to model two phase flow we adopt an optimized colour gradient approach ahrenholz et al 2008 tölke et al 2006 that can handle low capillary number and high viscosity ratio cases order parameter ϕ is defined as 10 ϕ ρ w ρ n w ρ w ρ n w where ρw and ρnw are dimensionless density fields of wetting and non wetting phases respectively order parameter ϕ represents fluid phase distribution and its value is 1 and 1 for non wetting and wetting phases respectively its value varies between 1 and 1 at the diffusive interface of wetting and non wetting phases the colour gradient of the phase field is calculated as 11 c t x 3 c 2 δ t n 1 w i e i ϕ t x e i δ t the weight coefficients wi are given as 12 w i 1 3 i 0 1 18 i 1 2 6 1 36 i 7 8 18 the normalized gradient which represents the orientation of the interface between the phases is defined as 13 n k c k c where k represents either the wetting phase or the non wetting phase we applied the lb model to compute the advection of density fields ψ ρ w ρ n w as 14 g i x e i t t t g i e q ψ t x u t x i 0 18 where g i e q is the equilibrium distribution function which is given by 15 g i e q ψ u w i ψ 1 3 c 2 e i u i 0 18 velocity profile u is calculated by mrt lbm eq 9 which was described earlier then the densities of the wetting and non wetting fluids are calculated by ψ i g i finally we apply a recoloring scheme to redistribute the distribution function gi so as to minimize the diffusion near the interface and attain separation of the two fluids details of the recoloring approach can be found in previous studies ahrenholz et al 2008 tölke et al 2002 the accuracy of our lb model can be validated through a series of simulations for static contact angle evaluation and capillary filling dynamics which are described in the next section 2 2 model validation 2 2 1 static contact angle evaluation accuracy of the present two phase flow lb model can be validated through the simulation of wetting phenomenon for a wide range of surface wettability in a test case we simply set up several simulations to demonstrate formation of a wetting phase droplet in contact with a non wetting phase on a solid surface to assign different contact angles between fluid and solid surface an order parameter ϕsolid is set to the solid nodes that controls surface wettability the equilibrium contact angle θ can be expressed by latva kokko and rothman 2005 16 c o s θ ϕ s o l i d the order parameter ϕsolid varies between 1 and 1 where 1 depicts a hydrophobic surface and 1 represents a hydrophilic one assigning an order parameter to the solid nodes leads to a colour gradient with the fluid nodes and an interaction between the fluid and solid nodes is applied a computational domain with a size of 200 200 200 lu3 lu lattice unit is selected in which two immiscible fluids are placed above a solid surface and different wettabilities are assigned to the system the viscosity ratio m μnw μw where μnw and μw are the dynamic viscosity of the non wetting and wetting fluids respectively and interfacial tension are assumed to be 0 25 and 70 mn m a periodic boundary condition is imposed for both left and right boundaries whereas the bottom and top boundaries are considered to be solid walls the simulation starts with the wetting phase which has a cubic configuration being placed on the solid surface the remaining space is occupied with the non wetting phase through lb simulation the wetting phase finally reaches an equilibrium state resulting in a final contact angle and a stable configuration form the final configurations of wetting phase droplets with different contact angles are presented in fig 1 the parameter ϕsolid has been chosen as 0 86 0 34 and 0 5 for equilibrium contact angles of 30 70 and 120 respectively these simulated contact angles are measured using the method proposed by huang et al 2007 the results are compared with contact angles analytically calculated using eq 16 in fig 1 as results indicate the contact angles obtained from the present lb model agree well with the theoretical values and are consistent with the applied parameter ϕsolid 2 2 2 capillary filling dynamics to verify the lb model for capillary filling and displacement simulations the invasion of a non wetting fluid into a single capillary tube with a rectangular cross section is simulated we considered a three dimensional tube of size 150 20 20 lu3 that initially contains a wetting fluid see the inset of fig 2 two buffer layers with a size of 10 lu have been added at the inlet and outlet of the tube a non wetting fluid which is initially filled the inlet buffer layer is driven to the tube by applying a pressure gradient δp between the inlet and the outlet the invaded length of the tube as a function of time t can be calculated analytically by ahrenholz et al 2008 17 x t μ w l 1 μ w 2 l 1 2 μ n w μ w δ p r e q 2 4 t μ w μ n w where l 1 is the length of the tube req is the tube radius or equivalent curvature of the tube with a rectangular cross section that is defined as 18 r e q 1 2 l 2 2 l 3 where l 2 and l 3 are the width and length of the rectangular cross section in the simulation the viscosity ratio was set to 0 25 the surface tension and contact angle are considered to be 70 mn m and 30 respectively in fig 2 the red curve represents the simulation result for the evolution of the length of the non wetting fluid which invaded the capillary tube the results obtained from the lb simulation is consistent with the analytical solution which is calculated using eq 17 2 3 co2 brine two phase flow simulations in natural rock samples after model verification we conduct two phase flow simulation of scco2 brine in a three dimensional 3d heterogeneous rock sample the 3d rock model is extracted from micro ct images of a cylindrical core sample of tuscaloosa sandstone taken from the cranfield site in mississippi hosseini et al 2013 lu et al 2012 the image stack created using x ray microtomography xmt with a resolution of 6 17 µm is digitized using a segmentation process the threshold for the segmentation process is selected as the porosity of the obtained binary image stack is the same as that of the original core samples 26 lb simulations are performed directly on the resulting digitized image stacks fig 3 represents the 3d volume rendering of the rock sample and its pore space the wettability of the rock surface is considered to be heterogeneous to implement the wettability heterogeneity to the rock model we map extracted contact angle distributions using the stochastic method see section 2 4 onto the solid phase of the 3d binarized image stack and use it as the input to the lb model for immiscible two phase fluid flow simulations here the computational domain has dimensions of 200 200 200 lu3 two buffer layers with the size of 10 lu are placed on the left and right boundaries and the porous medium between the buffer layers is initially saturated with the wetting fluid brine the medium is connected to the non wetting phase source through the left buffer layer which is saturated with scco2 to begin the simulation for drainage scco2 is injected continuously with a constant flow rate from the left buffer layer while a constant pressure is applied at the outlet on the right side the other boundaries are assumed to be impermeable and we impose a no slip boundary condition through the bounce back rule we then monitor the scco2 saturation and average fluid velocity during the drainage we stop the drainage simulation when scco2 saturation and velocity profiles reach steady state we subsequently commence the imbibition process by injecting brine into the sample in the following figures the solid skeleton is shown in gray the wetting phase brine is shown in red and the non wetting phase scco2 is shown in blue in the present study the densities of both wetting and non wetting fluids are considered to be equal 1000 kg m3 since the density contrast between scco2 and brine is small and the gravity effect has not been considered in our simulations the assumption of equal density does not affect the results in addition the choice of equal densities and implementation of multiple relaxation time into the lb model significantly suppress the effect of spurious velocity around curved interfaces the viscosity ratio and interfacial tension are assumed to be 0 15 and 70 mn m respectively even though numerical simulation using high resolution micro ct images is an accurate and reliable option for multiphase flow characterization in porous media the geometrical complexity of the pore space makes lbm computation challenging and expensive we have therefore applied the model to a parallel scheme written in c using the message passing interface mpi in order to improve the computational efficiency of lb implementation 2 4 generation of heterogeneous wettability field to handle wettability heterogeneity we consider the porous medium to be fractionally wet and the rock sample to be composed of different co2 wet solid fractions f c o 2 which are distributed through the solid phase the spatial distribution of the solid sites composed of co2 wet regions is generated using a standard spectral method kainourgiakis et al 2005 yiotis et al 2013 and is mapped on the solid skeleton the contact angles of the water and co2 wet portions of the rock are assigned to be 30 and 170 respectively and we use a stochastic algorithm to generate the solid regions containing co2 wet regions initially a 3d white random noise matrix r r about the same size as the rock sample 200 200 200 lu3 is generated then its fourier transform f r is computed and multiplied by a gaussian function 19 g s α f s e s 2 s 0 2 subsequently we take the inverse fourier transform of g s which gives us a correlated gaussian distribution field r r resulting in an auto correlation function 20 r r x e s 0 2 8 x 2 e 1 2 π x 2 λ 2 where represents the convolution product and λ π s 0 denotes the correlation length for the spatial distribution of the co2 wet portion of the solid phase to generate the distribution patterns of the co2 wet fractions of the solid phase we compare the matrix r r with the binary matrix obtained from the 3d micro ct image of the rock sample and the corresponding void pixels arrays in r r are set to a high value therefore we only consider sites that are occupied by the solid for a given co2 wet fraction f c o 2 co2 wet regions in the solid phase are defined as r r c r r s o l i d r r f c o 2 resulting wettability distributions for various co2 wet solid fractions are shown in fig 4 where correlation length λ 10 δ x black white and brown respectively represent pore space solid skeleton and co2 wet regions mapped to the solid skeleton 3 results and discussion 3 1 characterization of microscale displacement patterns during scco 2 drainage to start we perform a simulation of scco2 drainage into an initially brine filled rock sample until the fluid saturations reaches a value that does not change significantly over time drainage simulation results at a specific scco2 saturation 30 are selected as initial conditions for the subsequent imbibition process in which brine is injected into the sample the reason for selecting the value of 30 is that at this point the drainage simulations in all samples reach steady states and saturation profiles do not change significantly over time the flooding processes are simulated in the rock samples with different spatial distribution of wettability f c o 2 and various wettability fields employed in the rock samples are constructed using the method explained in section 2 4 the capillary number is defined as c a u μ c o 2 σ where u is a characteristic velocity and μ c o 2 is the dynamic viscosity of scco2 the simulation results are obtained with ca 1 5e 5 by considering u as the input flow velocity the viscosity ratio is assumed to be 0 15 in the following we show how differing local wettabilities affect scco2 and brine distributions and their displacement patterns in this section we focus on the effect of wettability heterogeneity on scco2 and brine displacement patterns interfacial area between fluid phases and rock surface macroscopic capillary pressure and relative permeability curves during scco2 flooding simulations fig 5 displays the distribution of scco2 and brine phases at two different time steps lattice unit time during scco2 drainage in a plane along the flow direction for samples with various heterogeneous wettability distributions in samples having larger fractions of co2 wet regions the scco2 phase is more connected inside the pore space compare phase distributions in fig 5 moreover in a fractionally wet medium scco2 contacts the rock surface on co2 wet portions of large pores and brine preferentially resides in water wet regions as the percentage of co2 wet regions in the rock sample and consequently in large pores increases the scco2 phase tends to be more connected thus smaller portions of pores are occupied with the brine phase which is trapped mostly as clusters surrounded by the scco2 phase in other words the spatial distribution of wettability controls the entrapment pattern of defending fluid brine during the drainage process as the distribution patterns show the likelihood of brine trapping in the scco2 phase increases as the fraction of co2 wet regions decreases and this effect leads to a lower sweep efficiency of scco2 overall scco2 saturation is therefore larger as the portion of co2 wet regions in the rock sample increases furthermore these distributions confirm the presence of both convex and concave interfaces under heterogeneous wettability conditions rabbani et al 2017 also observed the coexistance of concave and convex interfaces in their two phase flow simulations under intermediate wet conditions fig 6 shows the slice averaged scco2 saturation profile along the drainage direction x direction at time step 800 000 for samples with differing f c o 2 the profiles indicate that wettability distribution affects the spatial distribution of the scco2 phase increasing the portion of co2 wet regions in the rock sample leads to an increase in the average saturation of scco2 fig 7 a compares the time evolution of overall scco2 saturation during primary drainage in the samples having varying wettability heterogeneity as this figure shows at any given time the saturation of scco2 is higher as f c o 2 increases from 0 1 to 0 5 furthermore it can be observed that at initial time steps scco2 saturation in the water wet sample increases with a lower rate than that in other samples whereas at some point the saturation profile of the water wet medium crosses over the saturation distribution of that of other samples because the scco2 phase in the complete water wet sample occupies the central part of the large pores and trapping of residual brine is less favored the overall saturation of scco2 during drainage in the water wet sample is finally higher than that in the fractional wet samples in other words in the complete water wet sample the scco2 phase forms a continuous flowing paths and thus after a certain time step its saturation increases faster than that in other cases and its sweep efficiency is higher fig 8 represents scco2 phase distribution at a specific time step 200 000 for samples with varying wettability distributions note that in the complete water wet medium to maintain local capillary equilibrium the scco2 phase breaks up in some regions and snap off occurs herring et al 2018 lenormand et al 1988 roof 1970 as the portion of co2 wet regions in the rock increases snap off of the scco2 phase during drainage is less favored and the displacement regime resembles piston like patterns the movies provided in the supplementary materials show the displacement of scco2 phase during drainage in samples with f c o 2 0 and 0 5 to better quantify the development of interfacial morphology of fluids during drainage we calculate the specific interfacial area ratio of interfacial area over volume of the rock sample between fluid phases scco2 and brine and rock surface the interfacial area is an important factor affecting interphase mass and energy transfer rate jain et al 2003 jiang et al 2019 and has a huge impact on scco2 dissolution trapping in brine and triggered geochemical reactions with brine and rock formation in co2 sequestration schemes in the following results interfacial area refers to the interfacial area between fluid phases and rock surface time evolution of the interfacial area during primary drainage of rock samples with various wettability heterogeneities is presented in fig 7b the results indicate how morphology of the phases and thus their interfacial areas depend on wettability distribution the patterns of interfacial area increases during drainage in rocks with co2 wet fractions ranging from 0 1 to 0 5 have the same trend however at any given time the interfacial area becomes higher as the portion of co2 wet regions increases in the rock sample in the sample with a higher percentage of co2 wet portions the scco2 phase is better connected and has a larger contact with the rock surface hence the resulting interfacial area is larger the interfacial area calculated during primary drainage in the water wet sample shows a trend different from that of the other cases in the early stages the behavior of water wet sample is similar to that of samples having differing co2 wet heterogeneities however as the drainage continues the rate of increase in the interfacial area is higher than that in the other cases fig 7c represents the variation of specific interfacial areas with scco2 saturation during the drainage process results show that increasing co2 wet regions in the rock sample leads to a higher interfacial area at any given saturation heterogeneous wettability distribution in a fractional wet medium causes more dispersed fluid distribution and hence more tortuous flow paths resulting in a higher interfacial area at any given scco2 saturation we have also calculated drainage relative permeability using the extended darcy s law dullien 1992 and studied its dependence on fractional wettability scco2 brine relative permeability curves for various wettability conditions are shown in fig 9 a according to the results at a very low brine saturation irreducible brine saturation its relative permeability k r brine approaches zero indicating that brine is trapped in the pore space for saturations higher than irreducible brine saturation relative permeability of brine in purely water wet sample is lower than that in samples containing co2 wet regions for the case of a water wet sample wetting phase brine tends to occupy the small pores or resides on the solid surface hence forms a greater resistance to flow compared with the samples with a heterogeneous wettability distribution thus the relative permeability of brine in a water wet sample is lower than that in samples with heterogeneous wettabilities at a given saturation for samples with fractional wettabilities relative permeability of brine increases with f c o 2 as the fraction of co2 wet regions increases scco2 phase gets more connected see fig 5 and the likelihood of brine entrapment and thus the flow resistance to brine decreases a large fraction of brine sandwiched by scco2 in the sample with the lowest fraction of co2 wet regions f c o 2 0 1 confirms this argument shown in fig 5 moreover in a sample containing both water and co2 wet regions a larger fraction of brine can move in larger pores which have higher conductivity therefore brine relative permeability increases as the fraction of co2 wet surfaces increases scco2 relative permeability k r c o 2 tends to decrease at a given brine saturation as the fraction of co2 wet regions increases according to fig 7b at a larger f c o 2 interfacial area and interaction of scco2 with the solid surface and brine is larger leading to a greater flow resistance and a smaller value of scco2 relative permeability at any given saturation even though scco2 phase is more connected as the fraction of co2 wet regions increases but the resistance effect of higher interfacial area of scco2 with the solid surface and surrounding brine outweighs the effect of scco2 connectivity our finding is consistent with the result reported by hwang et al 2006 macro scale capillary pressure is one of the main characteristics of two phase flow and highly depends on wettability pore structure and saturation history in this study we define the macroscopic capillary pressure by applying a certain pressure gradient δp to both phases and performing the scco2 injection simulation until the system reaches a steady state condition the pressure gradient δp and steady state saturation are considered as one point of the capillary pressure curve applying different δp yields more points of capillary pressure curve this method is often used in experimental studies to obtain capillary pressure curves armstrong et al 2012 hingerl et al 2016 fig 9b represents this quantity as a function of brine saturation sb for samples with f c o 2 0 0 0 2 and 0 5 results show that the capillary pressure at a particular brine saturation decreases as the fraction of co2 wet regions increases in a water wet system scco2 moves into larger pores whereas in a system with fractional wettability scco2 can also move into smaller pores containing co2 wet surfaces resulting higher scco2 saturation lower brine saturation at a given capillary pressure our result is consistent with previous studies by bradford and leij 1995 fatt and klikoff 1959 and hwang et al 2006 3 2 effect of wettability heterogeneity on scco2 residual trapping we quantify the effect of fractional wettability on scco2 capillary trapping mechanisms after the brine imbibition process the simulation results are obtained with ca 1 5e 5 our results depict that spatial variation of wettability in the rock sample significantly affects the pore scale characteristics of the residual scco2 trapped in the medium fig 10 shows the residual scco2 cluster distributions in samples with various fractional wettability note how the spatial distribution of wettability affects the size and distribution of clusters in the pore space to gain a better understanding of the impact of fractional wettability on scco2 residual trapping we characterize the co2 clusters trapped in the samples after the imbibition cycle the number of disconnected scco2 clusters n s with size s in voxels are identified and the cumulative cluster size distribution is calculated using iglauer et al 2012 jiang and tsuji 2015 21 s s s s n s s τ 2 22 n s n s n t where nt is the total number of voxels at the pore space and s s shows the contribution to the scco2 saturation of clusters larger than size s the distribution follows a power law function fig 11 represents s s as a function of s in a log log plot for trapped scco2 clusters in rock samples with various wettability distributions results indicate that cumulative cluster size distribution s s is almost constant for sizes of s less than 1000 voxels and decreases significantly in large sizes of s greater than that implying that large clusters make only a small contribution to the total number of residual clusters a large portion of total residual saturation however is due to these large clusters on the other hand small clusters s 1000 voxels contribute significantly to the total number of residual clusters but make a negligible contribution to residual saturation note also that the completely water wet medium represents a distribution containing fewer numbers of clusters leading to lower residual saturation fig 12 depicts the distribution of scco2 and brine phases during imbibition in the same plane along the flow direction phase distributions show that the presence of co2 wet regions in the rock causes clusters of trapped scco2 to be smaller when the fraction of co2 wet regions is increased the average volume of clusters decreases from 1242 voxels to 144 voxels for completely water wet and 50 co2 wet mediums respectively in other words the number of small blobs increases as the fraction of co2 wet regions in the rock sample increases comparing cluster size distributions of different cases reflects this fact moreover the total number of scco2 clusters increases as f c o 2 becomes larger fig 13 the interfacial area of scco2 with brine and rock surface in the fractional wet case is significantly larger than that in the water wet cases fig 13 in other words in rocks containing larger portions of co2 wet regions larger numbers of clusters as well as the higher possibility for scco2 to attach to the rock surface lead to an increase in scco2 interfacial area with the surrounding invading phase brine and rock surface results indicate that the number of scco2 clusters is monotonically increasing with f c o 2 whereas the rate of interfacial area increase is slow when f c o 2 is greater than 0 3 we conclude that not only cluster numbers but also their size distribution determine the total interfacial area between fluid phases and rock surface results also show that the residual saturation of trapped scco2 s c o 2 r in the completely water wet medium is smaller than those in cases containing co2 wet regions the value of s c o 2 r for cases with f c o 2 0 0 0 1 0 2 0 3 0 4 and 0 5 are respectively 4 10 9 20 10 19 11 90 11 49 and 11 71 in other words residual saturation of trapped scco2 is higher in the fractional wet sample the movies provided in the supplementary materials show capillary trapping of co2 during imbibition in samples with f c o 2 0 and 0 5 in support of our conclusions a previous study has represented remarkable increase in residual trapping as wettability alters to scco2 wet wang and tokunaga 2015 herring et al 2016a also found that prolonged exposure to scco2 causes heterogeneous wettability alteration and consequently higher stability and saturation of the trapped scco2 phase 4 summary this study presents a pore scale simulation of scco2 brine drainage and imbibition in a realistic rock model of tuscaloosa sandstone having heterogeneous wettability distributions we performed primary drainage simulation through scco2 injection in a brine filled sample followed by a secondary imbibition in which brine displaced the scco2 invaded during the preceding drainage our results indicate how wettability heterogeneity affects scco2 immiscible displacement dynamics during the injection period and its capillary trapping during the post injection period because the wettability of rocks might vary spatially owing to different mineralogy sorption of organic matter or long term exposure to scco2 considering the impact of fractional wettability on co2 storage is of great importance we found that in the water wet medium during drainage the invading fluid scco2 drains preferentially through the largest pore throats and its front has a ramified shape also as the portion of co2 wet regions in the fractional wet condition increases the scco2 front has a more compact shape in other words co2 wet regions of the rock cause displacement patterns during drainage to be more stabilized and snap off of scco2 phase is less favored samples with an increasing percentage of co2 wet regions exhibit a higher brine relative permeability and lower scco2 relative permeability at a given saturation for the drainage cycle the capillary pressure at a particular brine saturation decreases as the fraction of co2 wet regions increases the presence of co2 wet regions in the rock sample is conductive to more scco2 trapping during brine flooding because the residual saturation of scco2 for the sample containing the highest portion of co2 wet regions 50 is 2 9 times greater than that for the water wet sample results indicate that the presence of less water wet regions in the rock results in the formation of more immobile scco2 islands which have resulted from the bypassing of brine phase moreover note that the maximum size of scco2 clusters trapped in the water wet medium is significantly smaller than that of the ones for the fractional wet samples residual cluster distribution can significantly contribute to assessment of the storage capacity of carbon capture schemes and having knowledge about the scco2 interfacial area can provide insight into the scco2 amount that could be trapped by dissolution our results indicate that presence of co2 wet regions in the rock sample facilitates scco2 trapping for co2 sequestration projects thus further investigations of the various rock formations with different wettability preferences are required for an increase of storage capacity in co2 sequestration projects because experimental investigations into this subject have high cost and time demands our simulation approach is a promising one that is capable of providing information on the behavior of scco2 brine systems in contact with rocks having various properties and can thus benefit both co2 sequestration and enhanced oil recovery eor projects acknowledgements the authors are grateful to anonymous reviewers and beg internal reviewers this work was supported by the secarb project managed by the southern states energy board and funded by the u s department of energy netl under contract number de fc26 05nt42590 publication authorized by the director bureau of economic geology jackson school of geosciences the university of texas at austin computation for the work described in this paper was supported by texas advanced computing center supplementary material supplementary material associated with this article can be found in the online version at doi 10 1016 j advwatres 2019 02 008 appendix a supplementary materials supplementary data s1 supplementary raw research data this is open data under the cc by license http creativecommons org licenses by 4 0 supplementary data s1 supplementary data s2 supplementary raw research data this is open data under the cc by license http creativecommons org licenses by 4 0 supplementary data s2 
688,the objective of this study was to investigate the effect of wettability heterogeneity on pore scale characteristics of supercritical sc co2 displacement dynamics and its capillary trapping mechanism during a scco2 brine drainage and imbibition cycle a multiphase lattice boltzmann lb model was employed to simulate scco2 brine flow in rock samples of tuscaloosa sandstone taken from the cranfield co2 injection site using a spectral method we adopted various wettability fields to generate rock samples containing distributed co2 wet regions to gain a better insight into the effect of fractional wettability on scco2 displacement patterns during drainage we quantified the evolution of scco2 interface with brine and rock surface for samples with various wettability heterogeneities in addition the effect of heterogeneous wettability on the drainage relative permeability and capillary pressure curves has been investigated in this study according to our results heterogeneous distribution of co2 wet regions in the rock leads to more dispersed fluid distribution and hence more tortuous flow paths resulting in higher interfacial area between fluid phases and rock surface at any given scco2 saturation furthermore the spatial distribution of wettability controls the scco2 entrapment pattern and spatial distribution of residual scco2 clusters during brine flooding in fractional wet samples residence of scco2 phase in co2 wet regions creates more trapped scco2 clusters suppressing the connectivity of the co2 phase thus enhancing more residual trapping our results imply that the total number of scco2 clusters and as a result their residual trapping increases as the fraction of co2 wet regions becomes larger leading to a larger surface area of scco2 with brine and rock surface potentially facilitating the likelihood of long term dissolution and mineral trapping keywords porous medium heterogeneity multiphase flow wettability lattice boltzmann modeling co2 geo sequestration 1 introduction sequestration of carbon dioxide co2 in geological formations is considered to be one of the most promising solutions for mitigating carbon emissions and hence global warming dashtian et al 2019 shaffer 2010 injection of co2 into deep saline aquifers is known to be the most feasible option for carbon storage because these aquifers have the largest accessibility and highest storage capacity bachu 2003 bakhshian and sahimi 2017 bakhshian et al 2018 dashtian et al 2018 hosseini and nicot 2012 kharaka et al 2009 mathias et al 2008 michael et al 2010 o carroll et al 2005 soltanian et al 2016 an understanding of the behavior of co2 in saline reservoirs is thus required for a better assessment of the efficiency of geo sequestration projects and optimization of subsurface flow management owing to the density contrast between co2 and brine the co2 plume is likely to flow upward and potentially leak through natural fractures faults well bores or caprocks macminn and juanes 2009 physical and chemical trapping mechanisms such as structural trapping capillary or residual trapping dissolution trapping and mineral trapping can reduce or prevent the possibility of upward co2 migration and lead to co2 immobilization in geological formations hesse et al 2008 iglauer 2011 juanes et al 2006 krevor et al 2015 after completion of the co2 injection process brine imbibes back into the pore space to displace drained co2 resulting in co2 being trapped as isolated blobs or ganglia krevor et al 2015 target storage formations therefore need to be selected carefully to ensure effective trapping mechanisms co2 migration and the trapping capacity of formations are highly influenced by rock type and wettabiliy preferences of constituent minerals wettability is generally quantified by contact angle θ which is the angle between the fluid fluid interface and the solid surface iglauer et al 2014 although most multiphase flow analyses are based on the simple assumption of uniform or homogeneous wettability in the reservoir rocks heterogeneity non uniformity of wettability is common in natural porous media al khdheeawi et al 2018 iglauer 2017 wettability heterogeneity which represents spatial variation of the wettability state in the rock formation is typically recognized as being one of two types mixed wettability and fractional wettability in a mixed wet medium wettability varies according to pore size distribution meaning that most of the wetting fluid is inclined to occupy the smallest pore channels however fractional wettability refers to a state in which different regions of the rock have various wettability preferences kovscek et al 1993 salathiel 1973 skauge et al 2007 fractional wettability in natural rocks might vary spatially owing to surface roughness wenzel 1949 variation in mineralogy borysenko et al 2009 aqueous chemistry demond et al 1994 or organic matter distributions dekker and ritsema 1994 wettability is a major factor that controls the flow and spatial distribution of fluids in porous media and macroscopic multiphase properties such as capillary pressure and relative permeability murison et al 2014 cieplak and robbins 1990 studied the effect of wettability on the fluid invasion pattern in two dimensional porous media and characterized three different meniscus motion types including overlap touch and burst they found that the finger width of the invasion pattern increases as the contact angle decreases and the medium becomes more wet to the invading fluid the divergence of invading fingers was observed at a critical contact angle θc holtzman and segre 2015 studied fluid invasion patterns in a pore scale model to investigate the effect of capillarity viscosity and wettability on immiscible displacement dynamics their results indicate that the increase of invading fluid wettability yields a cooperative pore filling mechanism and a stable invasion pattern whereas the effect of wettability becomes insignificant in a viscous fingering regime it has also been found that the wettability of reservoirs strongly affects the capillary trapping of co2 chalbaud et al 2009 chaudhary et al 2013 many studies suggest that residual trapping is suppressed as the medium becomes less water wet herring et al 2016b or more co2 oil wet rahman et al 2016 nevertheless several studies have shown that wettability alteration of pore surfaces to being less water wet improves residual trapping efficiency wang et al 2016 further studies on this subject are therefore essential for better prediction of reservoir performance during carbon storage additionally for simplicity most previous studies seem to have generally focused on multiphase flow in a uniformly wetted system and too little attention has been paid to the impact of wettability heterogeneity even though it significantly affects multiphase flow properties including recovery efficiency capillary pressure and relative permeability bradford and leij 1995 masalmeh 2003 most actual reservoirs represent heterogeneous wettability however in which different portions of rock have varying wettability preferences this study therefore demonstrates multiphase flow in rock samples having fractional wettability which strongly controls the pore scale fate of co2 during injection and post injection periods an understanding of the effect of wettability heterogeneity on pore scale fluid displacement is crucial for the prediction of large scale flow models the traditional grid based computational fluid dynamics cfd methods for multiphase flow simulations include volume of fluid vof rabbani et al 2018 and level set prodanović and bryant 2006 and phase field badalassi et al 2003 in these methods the fluid fluid interface which is captured using different volume functions is difficult to track in multiphase flows moreover a relatively fine grid is required in the vicinity of the interface to obtain a reasonable resolution on the other hand the lattice boltzamann lb method has emerged as a numerically robust technique for simulation of multiphase flows with sharp changes of the interface it is not required to employ an interface tracking or interface capturing step in the lb method whereas the phase separation is maintained automatically tölke et al 2013 the most widely used lb models are the shan and chen s potential model shan and chen 1993 the colour fluid model gunstensen et al 1991 the free energy model swift et al 1996 and the phase field based model he et al 1999 in this study we apply a multiphase lb model which has been introduced by gunstensen et al 1991 as an extension of the colour gradient model and can handle flow simulations with high viscosity ratios and low capillary numbers the objective of this work is to study the impact of wettability heterogeneity on scco2 brine two phase flow at the pore scale using the lattice boltzmann method we show how fractional wettability affects scco2 migration patterns during its drainage to the rock samples of tuscaloosa sandstone as well as scco2 residual trapping during brine imbibition first the lb model is introduced and its accuracy validated subsequently then we implement different wettability distribution fields in the rock sample using a stochastic method afterward displacement patterns and microscale distribution of scco2 are visualized and quantified in rock samples having various spatial wettability distributions finally residual trapping mechanisms of the samples with various fractional wettability are compared and a quantitative study is done on the trapped scco2 clusters 2 methodologies 2 1 multiphase lattice boltzmann method in this study an optimized colour gradient lb model tölke et al 2006 is adopted to simulate two phase flow in a rock sample and a d3q19 multiple relaxation time mrt lb scheme has been applied in the lb model the fluid is represented by a distribution function f i x t which undergoes the propagation and collision operators described by the following equation bakhshian and sahimi 2016 bao and schaefer 2013 1 f i x e i t t t f i x t ω i i 0 18 where δt represents the time step in lattice units ω i is the collision operator and e i is the velocity basis vectors the velocity set ei is given by 2 e i 0 0 0 i 0 c 1 0 0 c 0 1 0 c 0 0 1 i 1 2 6 c 1 1 0 c 1 0 1 c 0 1 1 i 7 8 18 where c δ x δ t is the lattice velocity and δx is the grid spacing the collision operator ω i is described by a multiple relation time mrt scheme which can be expressed as 3 ω m 1 s m f m e q where m is the transformation matrix which transforms the distribution function f to the moment space m as 4 m m f m eq m f e q where feq and meq are equilibrium functions at distribution and moment space respectively the detail of moment m and transformation matrix m can be found in previous studies ahrenholz et al 2008 d humières et al 2002 and the effect of interfacial tension σ is incorporated into the equilibrium moment vector meq as ahrenholz et al 2008 jiang et al 2014 5a m 0 e q ρ 5b m 1 e q e e q σ c 5c m 3 e q j x 5d m 5 e q j y 5e m 7 e q j z 5f m 9 e q 3 p x x e q 1 2 σ c 2 n x 2 n y 2 n z 2 5g m 11 e q p z z e q 1 2 σ c 2 n y 2 n z 2 5h m 13 e q p x y e q 1 2 σ c n x n y 5i m 14 e q p y z e q 1 2 σ c n y n z 5j m 15 e q p x z e q 1 2 σ c n x n z 5k m 2 e q m 4 e q m 6 e q m 8 e q m 16 e q m 17 e q m 18 e q 0 the definitions of c nx ny and nz are presented in eqs 11 and 13 the collision matrix s is a diagonal matrix with 19 relaxation parameters s i i also known as eigenvalues of the collision matrix m 1 sm 6 s s e s ξ 0 s q 0 s q 0 s q s ν s π s ν s π s ν s ν s ν s m s m s m 0 the relaxation parameter sν is defined as 7 s ν 2 6 ν c 2 δ t 1 where ν is kinematic viscosity the remaining parameters of the matrix s must lie between 0 and 2 to improve stability the optimal values of these parameters can be found in previous studies saito et al 2017 suga et al 2015 the fluid macroscopic properties including density ρ and velocity u are calculated using the following relations 8 ρ i f i 9 ρ u i e i ρ i to model two phase flow we adopt an optimized colour gradient approach ahrenholz et al 2008 tölke et al 2006 that can handle low capillary number and high viscosity ratio cases order parameter ϕ is defined as 10 ϕ ρ w ρ n w ρ w ρ n w where ρw and ρnw are dimensionless density fields of wetting and non wetting phases respectively order parameter ϕ represents fluid phase distribution and its value is 1 and 1 for non wetting and wetting phases respectively its value varies between 1 and 1 at the diffusive interface of wetting and non wetting phases the colour gradient of the phase field is calculated as 11 c t x 3 c 2 δ t n 1 w i e i ϕ t x e i δ t the weight coefficients wi are given as 12 w i 1 3 i 0 1 18 i 1 2 6 1 36 i 7 8 18 the normalized gradient which represents the orientation of the interface between the phases is defined as 13 n k c k c where k represents either the wetting phase or the non wetting phase we applied the lb model to compute the advection of density fields ψ ρ w ρ n w as 14 g i x e i t t t g i e q ψ t x u t x i 0 18 where g i e q is the equilibrium distribution function which is given by 15 g i e q ψ u w i ψ 1 3 c 2 e i u i 0 18 velocity profile u is calculated by mrt lbm eq 9 which was described earlier then the densities of the wetting and non wetting fluids are calculated by ψ i g i finally we apply a recoloring scheme to redistribute the distribution function gi so as to minimize the diffusion near the interface and attain separation of the two fluids details of the recoloring approach can be found in previous studies ahrenholz et al 2008 tölke et al 2002 the accuracy of our lb model can be validated through a series of simulations for static contact angle evaluation and capillary filling dynamics which are described in the next section 2 2 model validation 2 2 1 static contact angle evaluation accuracy of the present two phase flow lb model can be validated through the simulation of wetting phenomenon for a wide range of surface wettability in a test case we simply set up several simulations to demonstrate formation of a wetting phase droplet in contact with a non wetting phase on a solid surface to assign different contact angles between fluid and solid surface an order parameter ϕsolid is set to the solid nodes that controls surface wettability the equilibrium contact angle θ can be expressed by latva kokko and rothman 2005 16 c o s θ ϕ s o l i d the order parameter ϕsolid varies between 1 and 1 where 1 depicts a hydrophobic surface and 1 represents a hydrophilic one assigning an order parameter to the solid nodes leads to a colour gradient with the fluid nodes and an interaction between the fluid and solid nodes is applied a computational domain with a size of 200 200 200 lu3 lu lattice unit is selected in which two immiscible fluids are placed above a solid surface and different wettabilities are assigned to the system the viscosity ratio m μnw μw where μnw and μw are the dynamic viscosity of the non wetting and wetting fluids respectively and interfacial tension are assumed to be 0 25 and 70 mn m a periodic boundary condition is imposed for both left and right boundaries whereas the bottom and top boundaries are considered to be solid walls the simulation starts with the wetting phase which has a cubic configuration being placed on the solid surface the remaining space is occupied with the non wetting phase through lb simulation the wetting phase finally reaches an equilibrium state resulting in a final contact angle and a stable configuration form the final configurations of wetting phase droplets with different contact angles are presented in fig 1 the parameter ϕsolid has been chosen as 0 86 0 34 and 0 5 for equilibrium contact angles of 30 70 and 120 respectively these simulated contact angles are measured using the method proposed by huang et al 2007 the results are compared with contact angles analytically calculated using eq 16 in fig 1 as results indicate the contact angles obtained from the present lb model agree well with the theoretical values and are consistent with the applied parameter ϕsolid 2 2 2 capillary filling dynamics to verify the lb model for capillary filling and displacement simulations the invasion of a non wetting fluid into a single capillary tube with a rectangular cross section is simulated we considered a three dimensional tube of size 150 20 20 lu3 that initially contains a wetting fluid see the inset of fig 2 two buffer layers with a size of 10 lu have been added at the inlet and outlet of the tube a non wetting fluid which is initially filled the inlet buffer layer is driven to the tube by applying a pressure gradient δp between the inlet and the outlet the invaded length of the tube as a function of time t can be calculated analytically by ahrenholz et al 2008 17 x t μ w l 1 μ w 2 l 1 2 μ n w μ w δ p r e q 2 4 t μ w μ n w where l 1 is the length of the tube req is the tube radius or equivalent curvature of the tube with a rectangular cross section that is defined as 18 r e q 1 2 l 2 2 l 3 where l 2 and l 3 are the width and length of the rectangular cross section in the simulation the viscosity ratio was set to 0 25 the surface tension and contact angle are considered to be 70 mn m and 30 respectively in fig 2 the red curve represents the simulation result for the evolution of the length of the non wetting fluid which invaded the capillary tube the results obtained from the lb simulation is consistent with the analytical solution which is calculated using eq 17 2 3 co2 brine two phase flow simulations in natural rock samples after model verification we conduct two phase flow simulation of scco2 brine in a three dimensional 3d heterogeneous rock sample the 3d rock model is extracted from micro ct images of a cylindrical core sample of tuscaloosa sandstone taken from the cranfield site in mississippi hosseini et al 2013 lu et al 2012 the image stack created using x ray microtomography xmt with a resolution of 6 17 µm is digitized using a segmentation process the threshold for the segmentation process is selected as the porosity of the obtained binary image stack is the same as that of the original core samples 26 lb simulations are performed directly on the resulting digitized image stacks fig 3 represents the 3d volume rendering of the rock sample and its pore space the wettability of the rock surface is considered to be heterogeneous to implement the wettability heterogeneity to the rock model we map extracted contact angle distributions using the stochastic method see section 2 4 onto the solid phase of the 3d binarized image stack and use it as the input to the lb model for immiscible two phase fluid flow simulations here the computational domain has dimensions of 200 200 200 lu3 two buffer layers with the size of 10 lu are placed on the left and right boundaries and the porous medium between the buffer layers is initially saturated with the wetting fluid brine the medium is connected to the non wetting phase source through the left buffer layer which is saturated with scco2 to begin the simulation for drainage scco2 is injected continuously with a constant flow rate from the left buffer layer while a constant pressure is applied at the outlet on the right side the other boundaries are assumed to be impermeable and we impose a no slip boundary condition through the bounce back rule we then monitor the scco2 saturation and average fluid velocity during the drainage we stop the drainage simulation when scco2 saturation and velocity profiles reach steady state we subsequently commence the imbibition process by injecting brine into the sample in the following figures the solid skeleton is shown in gray the wetting phase brine is shown in red and the non wetting phase scco2 is shown in blue in the present study the densities of both wetting and non wetting fluids are considered to be equal 1000 kg m3 since the density contrast between scco2 and brine is small and the gravity effect has not been considered in our simulations the assumption of equal density does not affect the results in addition the choice of equal densities and implementation of multiple relaxation time into the lb model significantly suppress the effect of spurious velocity around curved interfaces the viscosity ratio and interfacial tension are assumed to be 0 15 and 70 mn m respectively even though numerical simulation using high resolution micro ct images is an accurate and reliable option for multiphase flow characterization in porous media the geometrical complexity of the pore space makes lbm computation challenging and expensive we have therefore applied the model to a parallel scheme written in c using the message passing interface mpi in order to improve the computational efficiency of lb implementation 2 4 generation of heterogeneous wettability field to handle wettability heterogeneity we consider the porous medium to be fractionally wet and the rock sample to be composed of different co2 wet solid fractions f c o 2 which are distributed through the solid phase the spatial distribution of the solid sites composed of co2 wet regions is generated using a standard spectral method kainourgiakis et al 2005 yiotis et al 2013 and is mapped on the solid skeleton the contact angles of the water and co2 wet portions of the rock are assigned to be 30 and 170 respectively and we use a stochastic algorithm to generate the solid regions containing co2 wet regions initially a 3d white random noise matrix r r about the same size as the rock sample 200 200 200 lu3 is generated then its fourier transform f r is computed and multiplied by a gaussian function 19 g s α f s e s 2 s 0 2 subsequently we take the inverse fourier transform of g s which gives us a correlated gaussian distribution field r r resulting in an auto correlation function 20 r r x e s 0 2 8 x 2 e 1 2 π x 2 λ 2 where represents the convolution product and λ π s 0 denotes the correlation length for the spatial distribution of the co2 wet portion of the solid phase to generate the distribution patterns of the co2 wet fractions of the solid phase we compare the matrix r r with the binary matrix obtained from the 3d micro ct image of the rock sample and the corresponding void pixels arrays in r r are set to a high value therefore we only consider sites that are occupied by the solid for a given co2 wet fraction f c o 2 co2 wet regions in the solid phase are defined as r r c r r s o l i d r r f c o 2 resulting wettability distributions for various co2 wet solid fractions are shown in fig 4 where correlation length λ 10 δ x black white and brown respectively represent pore space solid skeleton and co2 wet regions mapped to the solid skeleton 3 results and discussion 3 1 characterization of microscale displacement patterns during scco 2 drainage to start we perform a simulation of scco2 drainage into an initially brine filled rock sample until the fluid saturations reaches a value that does not change significantly over time drainage simulation results at a specific scco2 saturation 30 are selected as initial conditions for the subsequent imbibition process in which brine is injected into the sample the reason for selecting the value of 30 is that at this point the drainage simulations in all samples reach steady states and saturation profiles do not change significantly over time the flooding processes are simulated in the rock samples with different spatial distribution of wettability f c o 2 and various wettability fields employed in the rock samples are constructed using the method explained in section 2 4 the capillary number is defined as c a u μ c o 2 σ where u is a characteristic velocity and μ c o 2 is the dynamic viscosity of scco2 the simulation results are obtained with ca 1 5e 5 by considering u as the input flow velocity the viscosity ratio is assumed to be 0 15 in the following we show how differing local wettabilities affect scco2 and brine distributions and their displacement patterns in this section we focus on the effect of wettability heterogeneity on scco2 and brine displacement patterns interfacial area between fluid phases and rock surface macroscopic capillary pressure and relative permeability curves during scco2 flooding simulations fig 5 displays the distribution of scco2 and brine phases at two different time steps lattice unit time during scco2 drainage in a plane along the flow direction for samples with various heterogeneous wettability distributions in samples having larger fractions of co2 wet regions the scco2 phase is more connected inside the pore space compare phase distributions in fig 5 moreover in a fractionally wet medium scco2 contacts the rock surface on co2 wet portions of large pores and brine preferentially resides in water wet regions as the percentage of co2 wet regions in the rock sample and consequently in large pores increases the scco2 phase tends to be more connected thus smaller portions of pores are occupied with the brine phase which is trapped mostly as clusters surrounded by the scco2 phase in other words the spatial distribution of wettability controls the entrapment pattern of defending fluid brine during the drainage process as the distribution patterns show the likelihood of brine trapping in the scco2 phase increases as the fraction of co2 wet regions decreases and this effect leads to a lower sweep efficiency of scco2 overall scco2 saturation is therefore larger as the portion of co2 wet regions in the rock sample increases furthermore these distributions confirm the presence of both convex and concave interfaces under heterogeneous wettability conditions rabbani et al 2017 also observed the coexistance of concave and convex interfaces in their two phase flow simulations under intermediate wet conditions fig 6 shows the slice averaged scco2 saturation profile along the drainage direction x direction at time step 800 000 for samples with differing f c o 2 the profiles indicate that wettability distribution affects the spatial distribution of the scco2 phase increasing the portion of co2 wet regions in the rock sample leads to an increase in the average saturation of scco2 fig 7 a compares the time evolution of overall scco2 saturation during primary drainage in the samples having varying wettability heterogeneity as this figure shows at any given time the saturation of scco2 is higher as f c o 2 increases from 0 1 to 0 5 furthermore it can be observed that at initial time steps scco2 saturation in the water wet sample increases with a lower rate than that in other samples whereas at some point the saturation profile of the water wet medium crosses over the saturation distribution of that of other samples because the scco2 phase in the complete water wet sample occupies the central part of the large pores and trapping of residual brine is less favored the overall saturation of scco2 during drainage in the water wet sample is finally higher than that in the fractional wet samples in other words in the complete water wet sample the scco2 phase forms a continuous flowing paths and thus after a certain time step its saturation increases faster than that in other cases and its sweep efficiency is higher fig 8 represents scco2 phase distribution at a specific time step 200 000 for samples with varying wettability distributions note that in the complete water wet medium to maintain local capillary equilibrium the scco2 phase breaks up in some regions and snap off occurs herring et al 2018 lenormand et al 1988 roof 1970 as the portion of co2 wet regions in the rock increases snap off of the scco2 phase during drainage is less favored and the displacement regime resembles piston like patterns the movies provided in the supplementary materials show the displacement of scco2 phase during drainage in samples with f c o 2 0 and 0 5 to better quantify the development of interfacial morphology of fluids during drainage we calculate the specific interfacial area ratio of interfacial area over volume of the rock sample between fluid phases scco2 and brine and rock surface the interfacial area is an important factor affecting interphase mass and energy transfer rate jain et al 2003 jiang et al 2019 and has a huge impact on scco2 dissolution trapping in brine and triggered geochemical reactions with brine and rock formation in co2 sequestration schemes in the following results interfacial area refers to the interfacial area between fluid phases and rock surface time evolution of the interfacial area during primary drainage of rock samples with various wettability heterogeneities is presented in fig 7b the results indicate how morphology of the phases and thus their interfacial areas depend on wettability distribution the patterns of interfacial area increases during drainage in rocks with co2 wet fractions ranging from 0 1 to 0 5 have the same trend however at any given time the interfacial area becomes higher as the portion of co2 wet regions increases in the rock sample in the sample with a higher percentage of co2 wet portions the scco2 phase is better connected and has a larger contact with the rock surface hence the resulting interfacial area is larger the interfacial area calculated during primary drainage in the water wet sample shows a trend different from that of the other cases in the early stages the behavior of water wet sample is similar to that of samples having differing co2 wet heterogeneities however as the drainage continues the rate of increase in the interfacial area is higher than that in the other cases fig 7c represents the variation of specific interfacial areas with scco2 saturation during the drainage process results show that increasing co2 wet regions in the rock sample leads to a higher interfacial area at any given saturation heterogeneous wettability distribution in a fractional wet medium causes more dispersed fluid distribution and hence more tortuous flow paths resulting in a higher interfacial area at any given scco2 saturation we have also calculated drainage relative permeability using the extended darcy s law dullien 1992 and studied its dependence on fractional wettability scco2 brine relative permeability curves for various wettability conditions are shown in fig 9 a according to the results at a very low brine saturation irreducible brine saturation its relative permeability k r brine approaches zero indicating that brine is trapped in the pore space for saturations higher than irreducible brine saturation relative permeability of brine in purely water wet sample is lower than that in samples containing co2 wet regions for the case of a water wet sample wetting phase brine tends to occupy the small pores or resides on the solid surface hence forms a greater resistance to flow compared with the samples with a heterogeneous wettability distribution thus the relative permeability of brine in a water wet sample is lower than that in samples with heterogeneous wettabilities at a given saturation for samples with fractional wettabilities relative permeability of brine increases with f c o 2 as the fraction of co2 wet regions increases scco2 phase gets more connected see fig 5 and the likelihood of brine entrapment and thus the flow resistance to brine decreases a large fraction of brine sandwiched by scco2 in the sample with the lowest fraction of co2 wet regions f c o 2 0 1 confirms this argument shown in fig 5 moreover in a sample containing both water and co2 wet regions a larger fraction of brine can move in larger pores which have higher conductivity therefore brine relative permeability increases as the fraction of co2 wet surfaces increases scco2 relative permeability k r c o 2 tends to decrease at a given brine saturation as the fraction of co2 wet regions increases according to fig 7b at a larger f c o 2 interfacial area and interaction of scco2 with the solid surface and brine is larger leading to a greater flow resistance and a smaller value of scco2 relative permeability at any given saturation even though scco2 phase is more connected as the fraction of co2 wet regions increases but the resistance effect of higher interfacial area of scco2 with the solid surface and surrounding brine outweighs the effect of scco2 connectivity our finding is consistent with the result reported by hwang et al 2006 macro scale capillary pressure is one of the main characteristics of two phase flow and highly depends on wettability pore structure and saturation history in this study we define the macroscopic capillary pressure by applying a certain pressure gradient δp to both phases and performing the scco2 injection simulation until the system reaches a steady state condition the pressure gradient δp and steady state saturation are considered as one point of the capillary pressure curve applying different δp yields more points of capillary pressure curve this method is often used in experimental studies to obtain capillary pressure curves armstrong et al 2012 hingerl et al 2016 fig 9b represents this quantity as a function of brine saturation sb for samples with f c o 2 0 0 0 2 and 0 5 results show that the capillary pressure at a particular brine saturation decreases as the fraction of co2 wet regions increases in a water wet system scco2 moves into larger pores whereas in a system with fractional wettability scco2 can also move into smaller pores containing co2 wet surfaces resulting higher scco2 saturation lower brine saturation at a given capillary pressure our result is consistent with previous studies by bradford and leij 1995 fatt and klikoff 1959 and hwang et al 2006 3 2 effect of wettability heterogeneity on scco2 residual trapping we quantify the effect of fractional wettability on scco2 capillary trapping mechanisms after the brine imbibition process the simulation results are obtained with ca 1 5e 5 our results depict that spatial variation of wettability in the rock sample significantly affects the pore scale characteristics of the residual scco2 trapped in the medium fig 10 shows the residual scco2 cluster distributions in samples with various fractional wettability note how the spatial distribution of wettability affects the size and distribution of clusters in the pore space to gain a better understanding of the impact of fractional wettability on scco2 residual trapping we characterize the co2 clusters trapped in the samples after the imbibition cycle the number of disconnected scco2 clusters n s with size s in voxels are identified and the cumulative cluster size distribution is calculated using iglauer et al 2012 jiang and tsuji 2015 21 s s s s n s s τ 2 22 n s n s n t where nt is the total number of voxels at the pore space and s s shows the contribution to the scco2 saturation of clusters larger than size s the distribution follows a power law function fig 11 represents s s as a function of s in a log log plot for trapped scco2 clusters in rock samples with various wettability distributions results indicate that cumulative cluster size distribution s s is almost constant for sizes of s less than 1000 voxels and decreases significantly in large sizes of s greater than that implying that large clusters make only a small contribution to the total number of residual clusters a large portion of total residual saturation however is due to these large clusters on the other hand small clusters s 1000 voxels contribute significantly to the total number of residual clusters but make a negligible contribution to residual saturation note also that the completely water wet medium represents a distribution containing fewer numbers of clusters leading to lower residual saturation fig 12 depicts the distribution of scco2 and brine phases during imbibition in the same plane along the flow direction phase distributions show that the presence of co2 wet regions in the rock causes clusters of trapped scco2 to be smaller when the fraction of co2 wet regions is increased the average volume of clusters decreases from 1242 voxels to 144 voxels for completely water wet and 50 co2 wet mediums respectively in other words the number of small blobs increases as the fraction of co2 wet regions in the rock sample increases comparing cluster size distributions of different cases reflects this fact moreover the total number of scco2 clusters increases as f c o 2 becomes larger fig 13 the interfacial area of scco2 with brine and rock surface in the fractional wet case is significantly larger than that in the water wet cases fig 13 in other words in rocks containing larger portions of co2 wet regions larger numbers of clusters as well as the higher possibility for scco2 to attach to the rock surface lead to an increase in scco2 interfacial area with the surrounding invading phase brine and rock surface results indicate that the number of scco2 clusters is monotonically increasing with f c o 2 whereas the rate of interfacial area increase is slow when f c o 2 is greater than 0 3 we conclude that not only cluster numbers but also their size distribution determine the total interfacial area between fluid phases and rock surface results also show that the residual saturation of trapped scco2 s c o 2 r in the completely water wet medium is smaller than those in cases containing co2 wet regions the value of s c o 2 r for cases with f c o 2 0 0 0 1 0 2 0 3 0 4 and 0 5 are respectively 4 10 9 20 10 19 11 90 11 49 and 11 71 in other words residual saturation of trapped scco2 is higher in the fractional wet sample the movies provided in the supplementary materials show capillary trapping of co2 during imbibition in samples with f c o 2 0 and 0 5 in support of our conclusions a previous study has represented remarkable increase in residual trapping as wettability alters to scco2 wet wang and tokunaga 2015 herring et al 2016a also found that prolonged exposure to scco2 causes heterogeneous wettability alteration and consequently higher stability and saturation of the trapped scco2 phase 4 summary this study presents a pore scale simulation of scco2 brine drainage and imbibition in a realistic rock model of tuscaloosa sandstone having heterogeneous wettability distributions we performed primary drainage simulation through scco2 injection in a brine filled sample followed by a secondary imbibition in which brine displaced the scco2 invaded during the preceding drainage our results indicate how wettability heterogeneity affects scco2 immiscible displacement dynamics during the injection period and its capillary trapping during the post injection period because the wettability of rocks might vary spatially owing to different mineralogy sorption of organic matter or long term exposure to scco2 considering the impact of fractional wettability on co2 storage is of great importance we found that in the water wet medium during drainage the invading fluid scco2 drains preferentially through the largest pore throats and its front has a ramified shape also as the portion of co2 wet regions in the fractional wet condition increases the scco2 front has a more compact shape in other words co2 wet regions of the rock cause displacement patterns during drainage to be more stabilized and snap off of scco2 phase is less favored samples with an increasing percentage of co2 wet regions exhibit a higher brine relative permeability and lower scco2 relative permeability at a given saturation for the drainage cycle the capillary pressure at a particular brine saturation decreases as the fraction of co2 wet regions increases the presence of co2 wet regions in the rock sample is conductive to more scco2 trapping during brine flooding because the residual saturation of scco2 for the sample containing the highest portion of co2 wet regions 50 is 2 9 times greater than that for the water wet sample results indicate that the presence of less water wet regions in the rock results in the formation of more immobile scco2 islands which have resulted from the bypassing of brine phase moreover note that the maximum size of scco2 clusters trapped in the water wet medium is significantly smaller than that of the ones for the fractional wet samples residual cluster distribution can significantly contribute to assessment of the storage capacity of carbon capture schemes and having knowledge about the scco2 interfacial area can provide insight into the scco2 amount that could be trapped by dissolution our results indicate that presence of co2 wet regions in the rock sample facilitates scco2 trapping for co2 sequestration projects thus further investigations of the various rock formations with different wettability preferences are required for an increase of storage capacity in co2 sequestration projects because experimental investigations into this subject have high cost and time demands our simulation approach is a promising one that is capable of providing information on the behavior of scco2 brine systems in contact with rocks having various properties and can thus benefit both co2 sequestration and enhanced oil recovery eor projects acknowledgements the authors are grateful to anonymous reviewers and beg internal reviewers this work was supported by the secarb project managed by the southern states energy board and funded by the u s department of energy netl under contract number de fc26 05nt42590 publication authorized by the director bureau of economic geology jackson school of geosciences the university of texas at austin computation for the work described in this paper was supported by texas advanced computing center supplementary material supplementary material associated with this article can be found in the online version at doi 10 1016 j advwatres 2019 02 008 appendix a supplementary materials supplementary data s1 supplementary raw research data this is open data under the cc by license http creativecommons org licenses by 4 0 supplementary data s1 supplementary data s2 supplementary raw research data this is open data under the cc by license http creativecommons org licenses by 4 0 supplementary data s2 
689,several lagrangian methodologies have been proposed in recent years to simulate advection dispersion of solutes in fluids as a mass exchange between numerical particles carrying the fluid in this paper we unify these methodologies showing that mass transfer particle tracking mtpt algorithms can be framed within the context of smoothed particle hydrodynamics sph provided the choice of a gaussian smoothing kernel whose bandwidth depends on the dispersion and the time discretization numerical simulations are performed for a simple dispersion problem and they are compared to an analytical solution based on the results we advocate for the use of a kernel bandwidth of the size of the characteristic dispersion length ℓ 2 d δ t at least given a dense enough distribution of particles for in this case the mass transfer operation is not just an approximation but in fact the exact solution of the solute s displacement by dispersion in a time step keywords lagrangian modeling dispersion smoothed particle hydrodynamics mass transfer particle tracking kernel bandwidth 1 introduction in recent years a number of lagrangian numerical schemes have been proposed to simulate advection dispersion processes in fluids some of these approaches rely exclusively on traditional random walks to simulate dispersion benson and meerschaert 20008 benson et al 2017 bolster et al 2016a bolster et al 2016b ding et al 2012 ding and benson 2015 ding et al 2017 paster et al 2013 paster et al 2014 schmidt et al 2017 sole mari et al 2017 sole mari and fernàndez garcia 2018 whereas a second class represents dispersion through mass transfer between particles that carry a given amount of fluid herrera et al 2009 herrera and beckie benson and bolster 2016 schmidt et al 2018a other authors have hybridized random walks with mass transfer engdahl et al 2017 herrera et al in an approach that allows partitioning of total dispersion between mixing simulated by mass transfer and non mixed spreading simulated via random walks mass transfer algorithms can be further subdivided into two groups the first group herrera et al 2009 herrera and beckie derives the mass exchange rates from the well established smoothed particle hydrodynamics sph method gingold and monaghan 1977 which besides solute transport has been used in a variety of applications monaghan 2012 such as astrophysics fluid dynamics and solid mechanics a second group of approaches often referred to as mass transfer particle tracking mtpt algorithms benson and bolster 2016 schmidt et al 2018a derive the mass exchange rate from stochastic rules governing the co location probability of particles moving via dispersion to date a relationship between these two methodologies for mass transfer has not been established in this paper we analytically derive the connection between the sph and mtpt conventions and show that for specific kernel choices and provided that equivalent normalization and averaging conventions are used the sph and mtpt approaches are numerically equivalent additionally for the fixed choice of a gaussian kernel we investigate the effect of differing bandwidth choices on deviations from the analytical well mixed solution 2 the link between sph and mtpt the sph approach to approximating dispersion can be summarized by following herrera et al 2009 herrera and beckie therein the following equation describes the time evolution of the concentration ci t carried by a numerical particle labeled i 1 n assuming that all particles contain the same amount of fluid 1 d c i d t 2 j 1 n d i j ρ i j c i c j f x i x j h here n is the number of particles x i is the position of particle i and f r h is a radial function satisfying 2 r f r h w r h with w representing a radially symmetric translation invariant kernel with bandwidth h additionally d i j is the effective dispersion coefficient that in the isotropic but spatially variable case reduces to 3 d i j g d x i d x j where g is an averaging function e g arithmetic or harmonic average the quantity ρ i j defined by 4 ρ i j g ρ i ρ j 5 ρ x h k 1 n w x x k h ρ q ρ x q h q i j is an average of the particle densities estimated at x i and x j a popular choice for g in this case is the arithmetic average note that we make explicit the previously suppressed dependence of ρ i j on the locations of the particles x i i 1 n and the parameter h which represents the bandwidth of the kernel function w in the specific case that w r h is a gaussian kernel with the form 6 w r h 2 π h 2 d 2 exp r 2 2 h 2 where d is the number of spatial dimensions we have 7 f r h 1 h 2 w r h substituting 7 into 1 integrating the expression first order explicit and then rearranging we arrive at 8 c i t δ t c i t j 1 n β i j w i j c j t c i t in which we define 9 β i j h ℓ i j 2 h 2 ℓ i j 2 d i j δ t 10 w i j h w x i x j h ρ i j h here we once again denote the dependence of β and w on h because for a different kernel bandwidth choice these quantities will be altered correspondingly note that ℓ ij in 9 is equal to the characteristic distance of the average dispersion of particles i and j in a time step δt we now consider for the sake of comparison the mtpt algorithm originally formulated by benson and bolster 2016 further discussed in schmidt et al 2018a and given by 11 c i t δ t c i t 1 2 j 1 n p i j c j t c i t where p i j is the probabilistic weighting function for a mass transfer from particle j to particle i with the form 12 p i j p x i x j ρ i j here the function p is the probability density for the co location of particles i and j moving via dispersion 13 p x i x j d δ t 4 π d i d j δ t d 2 exp x i x j 2 4 d i d j δ t w x i x j 2 d i d j δ t w x i x j 4 d i j δ t w x i x j 2 ℓ i j where dk d x k and ρ i j is a normalizing factor that has classically been chosen to be ρj as in 5 with h 2 ℓ i j in order to make the matrix p with i jth entry p i j a left stochastic matrix i e a matrix where all columns sum to 1 however this does not guarantee that p i j p j i hence the concentration increase or decrease at particle i due to its interaction with particle j by 11 may not match the decrease or increase at particle j due to interaction with particle i as a consequence of this asymmetry normalization by ρ i j ρ j may not impose exact mass conservation also in the original paper benson and bolster 2016 eqs 11 is formulated in terms of solute masses instead of concentrations which are in this case interchangeable since all particles carry an equal amount of fluid comparing eqs 8 and 11 it is evident that the co location probability based mass exchange algorithm of benson and bolster 2016 is numerically equivalent to the sph formalism for β i j 1 2 for all i j 1 n with the standard deviation associated with particle co location by dispersion used for the bandwidth of w in 6 note that according to 9 and 10 imposing a constant value for βij implies that the kernel bandwidth h will change with the positions of particles i and j for spatially variable dispersion and will depend on δt as can be seen from 13 expressions 8 and 11 can be written in a general matrix vector form as 14 c t δ t a t c t where c i ci and 15 a i β w diag β w 1 above i is the n n identity matrix 1 is an n 1 vector of ones denotes the entrywise or hadamard product diag x is a square matrix with the entries of x on its main diagonal and the i jth entries of the matrices β and w are βij and w i j respectively note that as mentioned above and elsewhere see schmidt et al 2018a choosing ρ i j to be ρj in 12 ensures that p denoted as w in 15 is a left stochastic matrix but not necessarily symmetric on the other hand we note from 15 that if w is symmetric then a is also symmetric with rows and columns that sum to 1 guaranteeing conservation of mass thus a better normalization approach is to choose ρ i j to be ρ i j as in 4 resulting in symmetric w and mass conserving a schmidt et al 2018a b also present a discretized green s function approach to simulating dispersion by mass transfer for a time step δt this algorithm is described as 16 c t δ t g t c t with 17 g i j w x i x j ℓ i j ρ i j where once again ρ i j is traditionally defined to be ρj as in 5 we see that the matrix g is nearly identical to w for h ℓ i j and to p with twice the square bandwidth the only difference being the choice of non symmetric normalization using ρ i j ρ j we note that for a sufficiently large n j 1 n g i j ρ x w x i x ρ x d x 1 which implies diag g 1 i hence knowing that for h ℓ i j w g we see that the discretized green s function algorithm 16 is also nearly identical to the sph and particle co location expression given in 14 and 15 under the constraint that β i j 1 for all i j hereafter for simplicity we refer to any matrix β with all equal entries as a scalar β thus we have unified the previously divergent approaches to simulating dispersion that are employed by the sph and mtpt algorithms namely to frame things in the sph context the mtpt algorithms hold the mass transfer scaling parameter β constant 1 2 or 1 and adapt the kernel itself to the magnitude of dispersion over a time step this is in contrast to the traditional sph approach where the kernel bandwidth is independent from the dispersion magnitude and often set to contain a prescribed number of neighbors either locally or on average tartakovsky et al 2016 the kernel is then scaled in amplitude by the parameter βij to capture the magnitude of the dispersion action having established the link through the parameter β in 15 alternatively viewed as the choice of kernel bandwidth h between the sph and mtpt formalisms for simulating dispersion in a lagrangian context we next consider the implications of varying this parameter in the following section we conduct some numerical experiments to consider these effects 3 numerical investigations to analyze the effect of the kernel bandwidth h on sph transport simulations we study a simple case of 1d constant dispersion where the initial condition is a dirac delta pulse located at the center of the domain x 0 5 l for simplicity the model has no units the dispersion coefficient is fixed as d 10 3 l2t 1 and the total simulation time is t 4 t the analytical solution is then a gaussian with variance σ 2 2 d t see fig 1 or to be more precise the analytical solution is a normalized n bin histogram populated with evaluations of the density of a normal distribution n 0 5 σ 2 at the positions of the particles we compare this analytical solution to the numerical results for a range of values of h n and δt using root mean squared error rmse as the error metric which is defined to be 18 rmse c si 1 n i 1 n c i si t c i an t 2 where c an t is the analytical solution vector at time t c si t is the corresponding result from a given simulation for our numerical experiments the n particles are initially distributed over a fixed interval 0 l with l 1 l the dirac delta initial condition is represented in the numerical model by placing a particle with concentration n l at the center of the domain we compare three different types of simulations equally spaced stationary particles section 3 1 randomly spaced stationary particles section 3 2 and particles moving by brownian motion random walks section 3 3 for the latter two cases initial particle positions are assigned according to draws from a uniform u 0 1 distribution and ensembles of 9 520 and 1 660 realizations of each configuration respectively are performed in order to obtain a smooth estimation of the expected error by averaging over the ensemble for fixed values of n and δt we define h as the bandwidth for which the lowest average rmse is obtained i e 19 h argmin h 0 rmse c si h where rmse c si h is the average rmse over all realizations 3 1 equally spaced stationary particles fig 2 shows rmse 18 as a function of h for different values of n and δt for simulations with evenly spaced stationary particles in this case we observe a high degree of overlap between the curves since marginal changes in n and or δt do not always have a significant effect on the simulation results the simple explanation for this is that for a fixed δt that implies a given dispersion distance ℓ 2 d δ t increasing n beyond a certain point does nothing to improve the resolution of the simulation and the reverse also holds we see that given a high enough density of particles n sufficiently large the closest possible representation of the dispersion equation lowest rmse occurs for β 1 in other words for evenly spaced particles the smoothing kernel associated with β 1 is virtually free of numerical error when used in the sph algorithm as it in fact matches the analytical solution of the solute s dispersion over a time step it is worth noting here that this value of β 1 does not correspond to the particle co location algorithm given in 11 see benson and bolster 2016 but to the generalization of the green s function algorithm instead schmidt et al 2018a which is discussed in section 2 from a physical point of view using a kernel bandwidth larger than ℓ β 1 could be seen as equivalent to assuming that the solute mass carried by each particle is gaussian distributed in space over some support rather than a dirac delta prior to the start of the time step schmidt et al 2017 this is consistent with the fact that for low n the rmse can be reduced up to a certain point by using a larger kernel i e the assumption that each particle is distributed over some support can mitigate the need for more particles conversely choosing a kernel bandwidth significantly smaller than ℓ β 1 in addition to not having a clear physical meaning generates numerical instabilities because the mass transfer between two particles in one time step may be larger than the difference between their masses see 8 as a result these cases are excluded from the results shown in fig 2 some of the aforementioned relations can be better observed in fig 3 given a coarse time discretization fig 3 a green curves and markers h does not depend on s and h ℓ given a finer time discretization and a low particle density we have the relation h s see the linear trend for large s in the yellow curves of fig 3 a this proportionality is consistent with the known theoretical behavior for the truncation error of the sph interpolation given evenly spaced particles quinlan et al 2006 in examining the relation of h to the dispersion distance ℓ 2 d δ t in fig 3 b we observe that for sufficiently high values of n and δt we have h ℓ corresponding to β 1 see the clearly distinguished minima in fig 2 and otherwise h s 2 the curves with less pronounced minima in fig 2 all these relations are summarized by the two distinguishable regimes that can be seen in fig 3 c wherein h and s are non dimensionalized via scaling by the dispersion distance ℓ 3 2 randomly spaced stationary particles the numerical results for randomly distributed particles show less distinct trends in terms of matching the analytical solution than those seen for the evenly distributed particles of section 3 1 and this can be seen in fig 4 in this case the rmse does not always have such a clearly identifiable minimum in the vicinity of h nor does h reliably correspond to β 1 as we saw in section 3 1 rather its behavior appears to roughly agree with the theoretical sph truncation error for randomly spaced particles quinlan et al 2006 tartakovsky et al 2016 which can be expressed as the summation of two terms the smoothing error which scales with h and the quadrature error which scales with s h where s is the expected particle separation here s l n balancing these two terms results in h s and hence for that choice of bandwidth the truncation error scales with s this is consistent with the results shown in fig 4 where given h h i e considering only each curve s minimum the rmse scales with the particle number as rmse n 1 2 it is only when δt adopts large values that it appears to have a noticeable influence on the rmse this behavior is also evident in the relative insensitivity of h to ℓ as can be seen in fig 5 b in fig 5 a we see that the relation of h to the average particle spacing s is not linear not even for small δt unlike in the evenly spaced particle case instead we observe a range of slopes in the log log space about 1 2 and lower which can be related to the aforementioned truncation error quinlan et al 2006 which is minimized when h s unlike the equally spaced case fig 3 c we do not observe a single linear trend in fig 5 c for the relationship between h ℓ and s ℓ rather we observe the general tendency that s 0 implies h ℓ for the range of tested values a relatively high particle density of s 0 01ℓ is required to observe the relation h ℓ 3 3 random walking particles the same set of simulations are also conducted for a hybrid model in which the dispersion coefficient is partitioned as 20 d d rw d mt where d mt is the dispersion coefficient used in the sph mtpt algorithm described in the previous section and particles move by brownian motion according to the langevin equation for a time discretization t 1 t 2 t n with t k 1 t k δ t 21 x i k 1 x i k ξ i k 2 d rw δ t where x i k x i t k and ξ i k is a random number drawn from a standard normal n 0 1 distribution with an appropriate choice of d rw and d mt this type of approach can be used to give a separate treatment to the non mixed spreading rw and the actual mixing mt several authors gelhar et al 1979 gelhar and axness 1983 cirpka et al 1999 and werth et al 2006 have suggested that these correspond to the anisotropic spreading longitudinal minus transverse hydrodynamic dispersion and the isotropic mixing molecular diffusion plus transverse hydrodynamic dispersion parts of the dispersion tensor respectively here we simply set d rw d mt d 2 note that for this partitioning random walks do not significantly perturb spatial concentrations about their expected value that is the concentration difference between two spatially coincident particles is negligible meaning that the concentrations at a given time vary smoothly with the particle positions xi see fig 1 yellow markers this is because particles exchange mass at the same rate at which they diffuse by brownian motion for this reason we can study the influence of h on the numerical results when particles are random walking and compare to the case where particles are stationary as in sections 3 1 and 3 2 without introducing the concentration variance that would be otherwise purposefully induced by setting d rw d mt since at t 0 there is only one particle with nonzero concentration a strong variability in the results is introduced by the random motion of that particle in the initial stages of the simulation when it is carrying nearly all the solute mass in the system for this reason in order to favor faster convergence of the rmse with the number of simulations we set that singular particle to be motionless and to use the full dispersion coefficient in its mass transfer calculations i e for that particle d mt d and d rw 0 an alternative approach to overcome the same issue could be to use more particles to represent the initial dirac delta condition the behavior of the rmse in this case fig 6 can be seen as occupying a middle ground between the equally spaced fig 2 and the randomly spaced fig 4 stationary cases the distribution of particle spacings in the random walking case at any given time is identical to the stationary randomly distributed case but in the former the expected or time averaged particle spacing distribution is much narrower approximating the stationary evenly spaced case in that sense for that reason we do expect the value of h for a random walking model in the context of this specific example to be bounded between the two extreme stationary cases which may be thought of as the most ordered and disordered systems respectively note however that the actual values of the rmse in fig 6 are on the same order of magnitude as for the randomly distributed stationary particles fig 4 and they can be even higher this may be attributed to the added natural variability of brownian random walks used to represent half of the dispersion as opposed to the deterministic nature of mass transfers for high enough n and δt we can see that rmse minima occur at h ℓ and are strongly pronounced otherwise we see milder minima and h ℓ similarly to what is observed for equally spaced particles fig 2 in these regions of milder minima we see the approximate scaling rmse n 1 2 given h h which in this behavior is similar to the randomly spaced stationary case fig 4 we see that for a fine time discretization blue line in fig 7 a we have h s which as mentioned in section 3 2 indicates that h in these regimes is mainly controlled by the truncation error of the spatial interpolation on the other hand we see a clear trend that h ℓ for large enough n and δt as evidenced by the triangle symbols and green markers in fig 7 b as in the previous cases h departs from ℓ at some threshold as the relative spacing s ℓ increases like in the stationary randomly spaced case and unlike the equally spaced case this threshold value for s ℓ appears to depend on ℓ i e no single linear trend is observed in fig 7 c unlike in fig 3 c nevertheless for the range of tested values h ℓ for s 0 1ℓ 4 summary and discussion in this paper we demonstrate an equivalence between the lagrangian sph smoothed particle hydrodynamics and mtpt mass transfer particle tracking methods for simulating dispersion provided that the spatial kernel being employed is gaussian these two methods originate from completely different interpretations the sph community views their methods classically speaking as recent work has included random walks in sph simulations herrera et al as solving the dispersion equation by projecting the particles onto the continuum using radial basis functions kernels and approximating the solution on that kernel space the random walk particle tracking community views the mtpt methods considered in this paper in two ways i a first principles approach wherein mass transfers between moving particles are scaled by the probability that these particles co locate via dispersion ii a discretization of the green s function for the dispersion equation in which a particle s solute mass is spread in space via mass transfers to its nearest neighbors previously these two mtpt methods were considered to be distinct approaches and neither had rigorous proofs associated with it as a result of this work however both of these mtpt methods now inherit a rigorous theoretical underpinning from the sph literature the numerical investigations we conduct yield compelling results regarding the proper gaussian kernel bandwidth for particle tracking simulations we see strong evidence that a kernel with bandwidth h ℓ 2 d δ t i e imposing β 1 is the ideal choice provided there is a dense enough spatial distribution of particles this makes intuitive physical sense because with bandwidth ℓ this gaussian function is the fundamental solution of the dispersion equation in other words aside from the error introduced in the normalization step using this kernel for mass transfer is not an approximation but rather a semi analytical solution of the dispersion in a time step of length δt we also observe that counter intuitively a coarser time discretization may be a better choice than a finer one if that allows one to use bandwidth ℓ however there may be cases in which the intent is to reproduce the dispersion equation without the distortion associated with a low particle density a subject that we discuss below but a high particle density cannot be afforded computationally as may be likely to occur in multi dimensional systems if in these cases the use of a long time step would generate other forms of error for instance in the chemical reactions then a wider kernel bandwidth than ℓ following the traditional sph bandwidth selection rules of thumb may be a better choice when seeking a compromise between accuracy and efficiency one way to think of this is to consider the wider bandwidth particle to be a macro particle or cluster of smaller particles that is distributed in space over some support volume additional conclusions can be drawn from each of the individual cases tested in section 3 in the equally spaced stationary particle case h ℓ is clearly the optimal bandwidth choice provided that n is sufficiently large as to capture the magnitude of dispersion described by ℓ 2 d δ t i e particles must be close enough to see one another considering the randomly distributed stationary particle case we see a different story in that rmse tends to be more related to average inter particle spacing s l n than it is to the dispersion distance ℓ this is most likely because for the range of n and δt values tested the rmse is dominated by the truncation error of the sph interpolation nevertheless according to some authors in particle methods e g ding et al 2017 paster et al 2014 the distortion of the numerical solution caused by heterogeneity in the inter particle spacing and low particle densities can represent incomplete mixing conditions rather than being just a numerical error if we subscribe to this view then the randomly spaced case represents areas in which particles are poorly mixed and remain poorly mixed for the duration of the simulation from that perspective using the ℓ bandwidth would only be capturing the average mixedness of such a simulation fully simulating diffusive mixing in well mixed areas and under simulating mixing in poorly mixed areas in light of this the increase in rmse could be thought of not as an error but as desirable deviations from the well mixed solution due to physically meaningful areas of poor mixing for the case of random walking particles we find that the qualitative behavior of the rmse with respect to the bandwidth h can be placed in a middle ground between the other two scenarios in fact the minima h are found to be bounded in this case between the two former cases it is clear from the results that despite the particle disorder the dependence of the rmse on h should not be understood as a function of the particle density alone instead the error originated in deviating from the dispersion kernel bandwidth h ℓ should also be considered again if the effects of particle disorder on the numerical solution are considered to be physically meaningful it makes sense that random walking particles are closer to representing a well mixed system distinguishable by h ℓ than stationary randomly distributed particles since in this case the poorly mixed areas are not persistent in time we believe the results of our numerical experiments are relevant in a general sense despite representing the specific simple case of a dirac delta initial condition in a one dimensional setting this particular dispersion problem where one initial concentration pulse spreads by dispersion is no doubt the simplest one however any more complex problem can be thought of as unions of dirac delta initial conditions at least from a computational discrete standpoint as long as the physics are being captured on a local particle level as is demonstrated here more complicated conditions will also be properly simulated additionally we expect the scaling with s and ℓ to be analogous for isotropic dispersion in higher dimensions because mass transfers are merely a function of euclidean distance between particles and hence not substantively different in higher spatial dimensions however the scaling relations will likely need to be reformulated in terms of fill distance rather than the simple inter particle spacing we see here in 1d besides the analysis performed in section 3 would undoubtedly become more complex in the case of anisotropic and spatially variable dispersion the traditional sph extension to anisotropic dispersion entails a more complicated expression for d i j in 1 while maintaining the isotropy of the kernel w and this approach may result in negative concentrations herrera and beckie 2013 this is in contrast to the more straightforward extension of traditional mtpt to anisotropic dispersion which would involve redefining w as an anisotropic multi gaussian with variance 2δt g d x i d x j β where g is some averaging function the subject of anisotropy is out of the scope of this paper and should be addressed in future work nevertheless as mentioned in section 3 another suitable approach to reproducing anisotropic dispersion would be to split the dispersion tensor between an isotropic and an anisotropic part using the isotropic sph mtpt method addressed here to simulate the former and reproducing the latter with random walks open questions do remain in this area for instance we only consider the gaussian kernel in our analysis and results other kernels are commonly used in the sph literature and compactly supported kernels are known to result in computational speedup a standard choice is the compactly supported wendland kernel that has been shown to approach a gaussian in the infinitely smooth limiting case chernih et al 2014 how much error is introduced by this approximation and how does this compare to the common practice or imposing a cutoff distance of 3h for mass transfers as is commonly done in the particle tracking literature the hybridization of sph mtpt with random walks is a very recent technique that to date has not been studied in depth in this work we compare the numerical results from one such model with an analytical solution in the particular case wherein the simulation of the full dispersion tensor is partitioned equally between random walks and mass transfers if the purpose of this hybridization is to simulate a two scale system as in herrera et al 2017 in which the random walk accounts for spreading and the mass transfer accounts for mixing it would be proper for the magnitude of mixing to be much smaller than that of spreading in order to generate states of local disequilibrium as for instance to simulate the effect of local heterogeneities in porous media hence further investigation is needed in this area in order to i analyze the effect of using different spreading mixing ratios and ii evaluate the capability of this kind of model to correctly reproduce the generation propagation and decay of sub scale concentration variance 
689,several lagrangian methodologies have been proposed in recent years to simulate advection dispersion of solutes in fluids as a mass exchange between numerical particles carrying the fluid in this paper we unify these methodologies showing that mass transfer particle tracking mtpt algorithms can be framed within the context of smoothed particle hydrodynamics sph provided the choice of a gaussian smoothing kernel whose bandwidth depends on the dispersion and the time discretization numerical simulations are performed for a simple dispersion problem and they are compared to an analytical solution based on the results we advocate for the use of a kernel bandwidth of the size of the characteristic dispersion length ℓ 2 d δ t at least given a dense enough distribution of particles for in this case the mass transfer operation is not just an approximation but in fact the exact solution of the solute s displacement by dispersion in a time step keywords lagrangian modeling dispersion smoothed particle hydrodynamics mass transfer particle tracking kernel bandwidth 1 introduction in recent years a number of lagrangian numerical schemes have been proposed to simulate advection dispersion processes in fluids some of these approaches rely exclusively on traditional random walks to simulate dispersion benson and meerschaert 20008 benson et al 2017 bolster et al 2016a bolster et al 2016b ding et al 2012 ding and benson 2015 ding et al 2017 paster et al 2013 paster et al 2014 schmidt et al 2017 sole mari et al 2017 sole mari and fernàndez garcia 2018 whereas a second class represents dispersion through mass transfer between particles that carry a given amount of fluid herrera et al 2009 herrera and beckie benson and bolster 2016 schmidt et al 2018a other authors have hybridized random walks with mass transfer engdahl et al 2017 herrera et al in an approach that allows partitioning of total dispersion between mixing simulated by mass transfer and non mixed spreading simulated via random walks mass transfer algorithms can be further subdivided into two groups the first group herrera et al 2009 herrera and beckie derives the mass exchange rates from the well established smoothed particle hydrodynamics sph method gingold and monaghan 1977 which besides solute transport has been used in a variety of applications monaghan 2012 such as astrophysics fluid dynamics and solid mechanics a second group of approaches often referred to as mass transfer particle tracking mtpt algorithms benson and bolster 2016 schmidt et al 2018a derive the mass exchange rate from stochastic rules governing the co location probability of particles moving via dispersion to date a relationship between these two methodologies for mass transfer has not been established in this paper we analytically derive the connection between the sph and mtpt conventions and show that for specific kernel choices and provided that equivalent normalization and averaging conventions are used the sph and mtpt approaches are numerically equivalent additionally for the fixed choice of a gaussian kernel we investigate the effect of differing bandwidth choices on deviations from the analytical well mixed solution 2 the link between sph and mtpt the sph approach to approximating dispersion can be summarized by following herrera et al 2009 herrera and beckie therein the following equation describes the time evolution of the concentration ci t carried by a numerical particle labeled i 1 n assuming that all particles contain the same amount of fluid 1 d c i d t 2 j 1 n d i j ρ i j c i c j f x i x j h here n is the number of particles x i is the position of particle i and f r h is a radial function satisfying 2 r f r h w r h with w representing a radially symmetric translation invariant kernel with bandwidth h additionally d i j is the effective dispersion coefficient that in the isotropic but spatially variable case reduces to 3 d i j g d x i d x j where g is an averaging function e g arithmetic or harmonic average the quantity ρ i j defined by 4 ρ i j g ρ i ρ j 5 ρ x h k 1 n w x x k h ρ q ρ x q h q i j is an average of the particle densities estimated at x i and x j a popular choice for g in this case is the arithmetic average note that we make explicit the previously suppressed dependence of ρ i j on the locations of the particles x i i 1 n and the parameter h which represents the bandwidth of the kernel function w in the specific case that w r h is a gaussian kernel with the form 6 w r h 2 π h 2 d 2 exp r 2 2 h 2 where d is the number of spatial dimensions we have 7 f r h 1 h 2 w r h substituting 7 into 1 integrating the expression first order explicit and then rearranging we arrive at 8 c i t δ t c i t j 1 n β i j w i j c j t c i t in which we define 9 β i j h ℓ i j 2 h 2 ℓ i j 2 d i j δ t 10 w i j h w x i x j h ρ i j h here we once again denote the dependence of β and w on h because for a different kernel bandwidth choice these quantities will be altered correspondingly note that ℓ ij in 9 is equal to the characteristic distance of the average dispersion of particles i and j in a time step δt we now consider for the sake of comparison the mtpt algorithm originally formulated by benson and bolster 2016 further discussed in schmidt et al 2018a and given by 11 c i t δ t c i t 1 2 j 1 n p i j c j t c i t where p i j is the probabilistic weighting function for a mass transfer from particle j to particle i with the form 12 p i j p x i x j ρ i j here the function p is the probability density for the co location of particles i and j moving via dispersion 13 p x i x j d δ t 4 π d i d j δ t d 2 exp x i x j 2 4 d i d j δ t w x i x j 2 d i d j δ t w x i x j 4 d i j δ t w x i x j 2 ℓ i j where dk d x k and ρ i j is a normalizing factor that has classically been chosen to be ρj as in 5 with h 2 ℓ i j in order to make the matrix p with i jth entry p i j a left stochastic matrix i e a matrix where all columns sum to 1 however this does not guarantee that p i j p j i hence the concentration increase or decrease at particle i due to its interaction with particle j by 11 may not match the decrease or increase at particle j due to interaction with particle i as a consequence of this asymmetry normalization by ρ i j ρ j may not impose exact mass conservation also in the original paper benson and bolster 2016 eqs 11 is formulated in terms of solute masses instead of concentrations which are in this case interchangeable since all particles carry an equal amount of fluid comparing eqs 8 and 11 it is evident that the co location probability based mass exchange algorithm of benson and bolster 2016 is numerically equivalent to the sph formalism for β i j 1 2 for all i j 1 n with the standard deviation associated with particle co location by dispersion used for the bandwidth of w in 6 note that according to 9 and 10 imposing a constant value for βij implies that the kernel bandwidth h will change with the positions of particles i and j for spatially variable dispersion and will depend on δt as can be seen from 13 expressions 8 and 11 can be written in a general matrix vector form as 14 c t δ t a t c t where c i ci and 15 a i β w diag β w 1 above i is the n n identity matrix 1 is an n 1 vector of ones denotes the entrywise or hadamard product diag x is a square matrix with the entries of x on its main diagonal and the i jth entries of the matrices β and w are βij and w i j respectively note that as mentioned above and elsewhere see schmidt et al 2018a choosing ρ i j to be ρj in 12 ensures that p denoted as w in 15 is a left stochastic matrix but not necessarily symmetric on the other hand we note from 15 that if w is symmetric then a is also symmetric with rows and columns that sum to 1 guaranteeing conservation of mass thus a better normalization approach is to choose ρ i j to be ρ i j as in 4 resulting in symmetric w and mass conserving a schmidt et al 2018a b also present a discretized green s function approach to simulating dispersion by mass transfer for a time step δt this algorithm is described as 16 c t δ t g t c t with 17 g i j w x i x j ℓ i j ρ i j where once again ρ i j is traditionally defined to be ρj as in 5 we see that the matrix g is nearly identical to w for h ℓ i j and to p with twice the square bandwidth the only difference being the choice of non symmetric normalization using ρ i j ρ j we note that for a sufficiently large n j 1 n g i j ρ x w x i x ρ x d x 1 which implies diag g 1 i hence knowing that for h ℓ i j w g we see that the discretized green s function algorithm 16 is also nearly identical to the sph and particle co location expression given in 14 and 15 under the constraint that β i j 1 for all i j hereafter for simplicity we refer to any matrix β with all equal entries as a scalar β thus we have unified the previously divergent approaches to simulating dispersion that are employed by the sph and mtpt algorithms namely to frame things in the sph context the mtpt algorithms hold the mass transfer scaling parameter β constant 1 2 or 1 and adapt the kernel itself to the magnitude of dispersion over a time step this is in contrast to the traditional sph approach where the kernel bandwidth is independent from the dispersion magnitude and often set to contain a prescribed number of neighbors either locally or on average tartakovsky et al 2016 the kernel is then scaled in amplitude by the parameter βij to capture the magnitude of the dispersion action having established the link through the parameter β in 15 alternatively viewed as the choice of kernel bandwidth h between the sph and mtpt formalisms for simulating dispersion in a lagrangian context we next consider the implications of varying this parameter in the following section we conduct some numerical experiments to consider these effects 3 numerical investigations to analyze the effect of the kernel bandwidth h on sph transport simulations we study a simple case of 1d constant dispersion where the initial condition is a dirac delta pulse located at the center of the domain x 0 5 l for simplicity the model has no units the dispersion coefficient is fixed as d 10 3 l2t 1 and the total simulation time is t 4 t the analytical solution is then a gaussian with variance σ 2 2 d t see fig 1 or to be more precise the analytical solution is a normalized n bin histogram populated with evaluations of the density of a normal distribution n 0 5 σ 2 at the positions of the particles we compare this analytical solution to the numerical results for a range of values of h n and δt using root mean squared error rmse as the error metric which is defined to be 18 rmse c si 1 n i 1 n c i si t c i an t 2 where c an t is the analytical solution vector at time t c si t is the corresponding result from a given simulation for our numerical experiments the n particles are initially distributed over a fixed interval 0 l with l 1 l the dirac delta initial condition is represented in the numerical model by placing a particle with concentration n l at the center of the domain we compare three different types of simulations equally spaced stationary particles section 3 1 randomly spaced stationary particles section 3 2 and particles moving by brownian motion random walks section 3 3 for the latter two cases initial particle positions are assigned according to draws from a uniform u 0 1 distribution and ensembles of 9 520 and 1 660 realizations of each configuration respectively are performed in order to obtain a smooth estimation of the expected error by averaging over the ensemble for fixed values of n and δt we define h as the bandwidth for which the lowest average rmse is obtained i e 19 h argmin h 0 rmse c si h where rmse c si h is the average rmse over all realizations 3 1 equally spaced stationary particles fig 2 shows rmse 18 as a function of h for different values of n and δt for simulations with evenly spaced stationary particles in this case we observe a high degree of overlap between the curves since marginal changes in n and or δt do not always have a significant effect on the simulation results the simple explanation for this is that for a fixed δt that implies a given dispersion distance ℓ 2 d δ t increasing n beyond a certain point does nothing to improve the resolution of the simulation and the reverse also holds we see that given a high enough density of particles n sufficiently large the closest possible representation of the dispersion equation lowest rmse occurs for β 1 in other words for evenly spaced particles the smoothing kernel associated with β 1 is virtually free of numerical error when used in the sph algorithm as it in fact matches the analytical solution of the solute s dispersion over a time step it is worth noting here that this value of β 1 does not correspond to the particle co location algorithm given in 11 see benson and bolster 2016 but to the generalization of the green s function algorithm instead schmidt et al 2018a which is discussed in section 2 from a physical point of view using a kernel bandwidth larger than ℓ β 1 could be seen as equivalent to assuming that the solute mass carried by each particle is gaussian distributed in space over some support rather than a dirac delta prior to the start of the time step schmidt et al 2017 this is consistent with the fact that for low n the rmse can be reduced up to a certain point by using a larger kernel i e the assumption that each particle is distributed over some support can mitigate the need for more particles conversely choosing a kernel bandwidth significantly smaller than ℓ β 1 in addition to not having a clear physical meaning generates numerical instabilities because the mass transfer between two particles in one time step may be larger than the difference between their masses see 8 as a result these cases are excluded from the results shown in fig 2 some of the aforementioned relations can be better observed in fig 3 given a coarse time discretization fig 3 a green curves and markers h does not depend on s and h ℓ given a finer time discretization and a low particle density we have the relation h s see the linear trend for large s in the yellow curves of fig 3 a this proportionality is consistent with the known theoretical behavior for the truncation error of the sph interpolation given evenly spaced particles quinlan et al 2006 in examining the relation of h to the dispersion distance ℓ 2 d δ t in fig 3 b we observe that for sufficiently high values of n and δt we have h ℓ corresponding to β 1 see the clearly distinguished minima in fig 2 and otherwise h s 2 the curves with less pronounced minima in fig 2 all these relations are summarized by the two distinguishable regimes that can be seen in fig 3 c wherein h and s are non dimensionalized via scaling by the dispersion distance ℓ 3 2 randomly spaced stationary particles the numerical results for randomly distributed particles show less distinct trends in terms of matching the analytical solution than those seen for the evenly distributed particles of section 3 1 and this can be seen in fig 4 in this case the rmse does not always have such a clearly identifiable minimum in the vicinity of h nor does h reliably correspond to β 1 as we saw in section 3 1 rather its behavior appears to roughly agree with the theoretical sph truncation error for randomly spaced particles quinlan et al 2006 tartakovsky et al 2016 which can be expressed as the summation of two terms the smoothing error which scales with h and the quadrature error which scales with s h where s is the expected particle separation here s l n balancing these two terms results in h s and hence for that choice of bandwidth the truncation error scales with s this is consistent with the results shown in fig 4 where given h h i e considering only each curve s minimum the rmse scales with the particle number as rmse n 1 2 it is only when δt adopts large values that it appears to have a noticeable influence on the rmse this behavior is also evident in the relative insensitivity of h to ℓ as can be seen in fig 5 b in fig 5 a we see that the relation of h to the average particle spacing s is not linear not even for small δt unlike in the evenly spaced particle case instead we observe a range of slopes in the log log space about 1 2 and lower which can be related to the aforementioned truncation error quinlan et al 2006 which is minimized when h s unlike the equally spaced case fig 3 c we do not observe a single linear trend in fig 5 c for the relationship between h ℓ and s ℓ rather we observe the general tendency that s 0 implies h ℓ for the range of tested values a relatively high particle density of s 0 01ℓ is required to observe the relation h ℓ 3 3 random walking particles the same set of simulations are also conducted for a hybrid model in which the dispersion coefficient is partitioned as 20 d d rw d mt where d mt is the dispersion coefficient used in the sph mtpt algorithm described in the previous section and particles move by brownian motion according to the langevin equation for a time discretization t 1 t 2 t n with t k 1 t k δ t 21 x i k 1 x i k ξ i k 2 d rw δ t where x i k x i t k and ξ i k is a random number drawn from a standard normal n 0 1 distribution with an appropriate choice of d rw and d mt this type of approach can be used to give a separate treatment to the non mixed spreading rw and the actual mixing mt several authors gelhar et al 1979 gelhar and axness 1983 cirpka et al 1999 and werth et al 2006 have suggested that these correspond to the anisotropic spreading longitudinal minus transverse hydrodynamic dispersion and the isotropic mixing molecular diffusion plus transverse hydrodynamic dispersion parts of the dispersion tensor respectively here we simply set d rw d mt d 2 note that for this partitioning random walks do not significantly perturb spatial concentrations about their expected value that is the concentration difference between two spatially coincident particles is negligible meaning that the concentrations at a given time vary smoothly with the particle positions xi see fig 1 yellow markers this is because particles exchange mass at the same rate at which they diffuse by brownian motion for this reason we can study the influence of h on the numerical results when particles are random walking and compare to the case where particles are stationary as in sections 3 1 and 3 2 without introducing the concentration variance that would be otherwise purposefully induced by setting d rw d mt since at t 0 there is only one particle with nonzero concentration a strong variability in the results is introduced by the random motion of that particle in the initial stages of the simulation when it is carrying nearly all the solute mass in the system for this reason in order to favor faster convergence of the rmse with the number of simulations we set that singular particle to be motionless and to use the full dispersion coefficient in its mass transfer calculations i e for that particle d mt d and d rw 0 an alternative approach to overcome the same issue could be to use more particles to represent the initial dirac delta condition the behavior of the rmse in this case fig 6 can be seen as occupying a middle ground between the equally spaced fig 2 and the randomly spaced fig 4 stationary cases the distribution of particle spacings in the random walking case at any given time is identical to the stationary randomly distributed case but in the former the expected or time averaged particle spacing distribution is much narrower approximating the stationary evenly spaced case in that sense for that reason we do expect the value of h for a random walking model in the context of this specific example to be bounded between the two extreme stationary cases which may be thought of as the most ordered and disordered systems respectively note however that the actual values of the rmse in fig 6 are on the same order of magnitude as for the randomly distributed stationary particles fig 4 and they can be even higher this may be attributed to the added natural variability of brownian random walks used to represent half of the dispersion as opposed to the deterministic nature of mass transfers for high enough n and δt we can see that rmse minima occur at h ℓ and are strongly pronounced otherwise we see milder minima and h ℓ similarly to what is observed for equally spaced particles fig 2 in these regions of milder minima we see the approximate scaling rmse n 1 2 given h h which in this behavior is similar to the randomly spaced stationary case fig 4 we see that for a fine time discretization blue line in fig 7 a we have h s which as mentioned in section 3 2 indicates that h in these regimes is mainly controlled by the truncation error of the spatial interpolation on the other hand we see a clear trend that h ℓ for large enough n and δt as evidenced by the triangle symbols and green markers in fig 7 b as in the previous cases h departs from ℓ at some threshold as the relative spacing s ℓ increases like in the stationary randomly spaced case and unlike the equally spaced case this threshold value for s ℓ appears to depend on ℓ i e no single linear trend is observed in fig 7 c unlike in fig 3 c nevertheless for the range of tested values h ℓ for s 0 1ℓ 4 summary and discussion in this paper we demonstrate an equivalence between the lagrangian sph smoothed particle hydrodynamics and mtpt mass transfer particle tracking methods for simulating dispersion provided that the spatial kernel being employed is gaussian these two methods originate from completely different interpretations the sph community views their methods classically speaking as recent work has included random walks in sph simulations herrera et al as solving the dispersion equation by projecting the particles onto the continuum using radial basis functions kernels and approximating the solution on that kernel space the random walk particle tracking community views the mtpt methods considered in this paper in two ways i a first principles approach wherein mass transfers between moving particles are scaled by the probability that these particles co locate via dispersion ii a discretization of the green s function for the dispersion equation in which a particle s solute mass is spread in space via mass transfers to its nearest neighbors previously these two mtpt methods were considered to be distinct approaches and neither had rigorous proofs associated with it as a result of this work however both of these mtpt methods now inherit a rigorous theoretical underpinning from the sph literature the numerical investigations we conduct yield compelling results regarding the proper gaussian kernel bandwidth for particle tracking simulations we see strong evidence that a kernel with bandwidth h ℓ 2 d δ t i e imposing β 1 is the ideal choice provided there is a dense enough spatial distribution of particles this makes intuitive physical sense because with bandwidth ℓ this gaussian function is the fundamental solution of the dispersion equation in other words aside from the error introduced in the normalization step using this kernel for mass transfer is not an approximation but rather a semi analytical solution of the dispersion in a time step of length δt we also observe that counter intuitively a coarser time discretization may be a better choice than a finer one if that allows one to use bandwidth ℓ however there may be cases in which the intent is to reproduce the dispersion equation without the distortion associated with a low particle density a subject that we discuss below but a high particle density cannot be afforded computationally as may be likely to occur in multi dimensional systems if in these cases the use of a long time step would generate other forms of error for instance in the chemical reactions then a wider kernel bandwidth than ℓ following the traditional sph bandwidth selection rules of thumb may be a better choice when seeking a compromise between accuracy and efficiency one way to think of this is to consider the wider bandwidth particle to be a macro particle or cluster of smaller particles that is distributed in space over some support volume additional conclusions can be drawn from each of the individual cases tested in section 3 in the equally spaced stationary particle case h ℓ is clearly the optimal bandwidth choice provided that n is sufficiently large as to capture the magnitude of dispersion described by ℓ 2 d δ t i e particles must be close enough to see one another considering the randomly distributed stationary particle case we see a different story in that rmse tends to be more related to average inter particle spacing s l n than it is to the dispersion distance ℓ this is most likely because for the range of n and δt values tested the rmse is dominated by the truncation error of the sph interpolation nevertheless according to some authors in particle methods e g ding et al 2017 paster et al 2014 the distortion of the numerical solution caused by heterogeneity in the inter particle spacing and low particle densities can represent incomplete mixing conditions rather than being just a numerical error if we subscribe to this view then the randomly spaced case represents areas in which particles are poorly mixed and remain poorly mixed for the duration of the simulation from that perspective using the ℓ bandwidth would only be capturing the average mixedness of such a simulation fully simulating diffusive mixing in well mixed areas and under simulating mixing in poorly mixed areas in light of this the increase in rmse could be thought of not as an error but as desirable deviations from the well mixed solution due to physically meaningful areas of poor mixing for the case of random walking particles we find that the qualitative behavior of the rmse with respect to the bandwidth h can be placed in a middle ground between the other two scenarios in fact the minima h are found to be bounded in this case between the two former cases it is clear from the results that despite the particle disorder the dependence of the rmse on h should not be understood as a function of the particle density alone instead the error originated in deviating from the dispersion kernel bandwidth h ℓ should also be considered again if the effects of particle disorder on the numerical solution are considered to be physically meaningful it makes sense that random walking particles are closer to representing a well mixed system distinguishable by h ℓ than stationary randomly distributed particles since in this case the poorly mixed areas are not persistent in time we believe the results of our numerical experiments are relevant in a general sense despite representing the specific simple case of a dirac delta initial condition in a one dimensional setting this particular dispersion problem where one initial concentration pulse spreads by dispersion is no doubt the simplest one however any more complex problem can be thought of as unions of dirac delta initial conditions at least from a computational discrete standpoint as long as the physics are being captured on a local particle level as is demonstrated here more complicated conditions will also be properly simulated additionally we expect the scaling with s and ℓ to be analogous for isotropic dispersion in higher dimensions because mass transfers are merely a function of euclidean distance between particles and hence not substantively different in higher spatial dimensions however the scaling relations will likely need to be reformulated in terms of fill distance rather than the simple inter particle spacing we see here in 1d besides the analysis performed in section 3 would undoubtedly become more complex in the case of anisotropic and spatially variable dispersion the traditional sph extension to anisotropic dispersion entails a more complicated expression for d i j in 1 while maintaining the isotropy of the kernel w and this approach may result in negative concentrations herrera and beckie 2013 this is in contrast to the more straightforward extension of traditional mtpt to anisotropic dispersion which would involve redefining w as an anisotropic multi gaussian with variance 2δt g d x i d x j β where g is some averaging function the subject of anisotropy is out of the scope of this paper and should be addressed in future work nevertheless as mentioned in section 3 another suitable approach to reproducing anisotropic dispersion would be to split the dispersion tensor between an isotropic and an anisotropic part using the isotropic sph mtpt method addressed here to simulate the former and reproducing the latter with random walks open questions do remain in this area for instance we only consider the gaussian kernel in our analysis and results other kernels are commonly used in the sph literature and compactly supported kernels are known to result in computational speedup a standard choice is the compactly supported wendland kernel that has been shown to approach a gaussian in the infinitely smooth limiting case chernih et al 2014 how much error is introduced by this approximation and how does this compare to the common practice or imposing a cutoff distance of 3h for mass transfers as is commonly done in the particle tracking literature the hybridization of sph mtpt with random walks is a very recent technique that to date has not been studied in depth in this work we compare the numerical results from one such model with an analytical solution in the particular case wherein the simulation of the full dispersion tensor is partitioned equally between random walks and mass transfers if the purpose of this hybridization is to simulate a two scale system as in herrera et al 2017 in which the random walk accounts for spreading and the mass transfer accounts for mixing it would be proper for the magnitude of mixing to be much smaller than that of spreading in order to generate states of local disequilibrium as for instance to simulate the effect of local heterogeneities in porous media hence further investigation is needed in this area in order to i analyze the effect of using different spreading mixing ratios and ii evaluate the capability of this kind of model to correctly reproduce the generation propagation and decay of sub scale concentration variance 
