index,text
470,carbon dioxide sequestration in deep saline aquifers requires accurate and precise methods to monitor carbon capture and storage and detect leakage risks the assessment of the co2 plume location during injection and storage depends on the accuracy of the spatial distribution model of petrophysical properties such as porosity and permeability this work focuses on stochastic methods for petrophysical characterization and presents a method for the prediction of porosity and permeability using borehole observations and surface geophysical data this study utilizes injection and monitoring measurements at the borehole locations and time lapse seismic surveys the proposed method is based on a stochastic approach to inverse problems with data assimilation namely the ensemble smoother with multi data assimilation ensemble based methods are generally unfeasible when applied to large geophysical datasets such as time lapse seismic surveys in the proposed approach a machine learning method namely the deep convolutional autoencoder is applied to reduce the dimension of the seismic data the ensemble smoother is then applied in a lower dimensional data space to predict the aquifer petrophysical properties this method updated predictions of porosity and permeability every time new data either seismic surveys or borehole data are available to reduce the uncertainty in the co2 plume prediction the method is tested and validated on a synthetic geophysical dataset generated for the johansen formation a potential large scale offshore site for co2 storage keywords co2 storage co2 plume prediction data assimilation deep learning deep saline aquifers 1 introduction co2 sequestration and storage in deep saline aquifers has been widely studied kopp et al 2010 goodman et al 2013 castelletto et al 2013 pool et al 2013 jeong et al 2013 ghorbanidehno et al 2015 li and benson 2015 levine et al 2016 ma et al 2019 zhu et al 2019 one of the main challenges for a successful implementation of carbon storage in deep saline formations is the ability to reliably monitor the co2 plume migration and the dynamic conditions during and after injection goodman et al 2011 and 2017 bachu 2008 and 2015 miller et al 2014 due to the complexity and heterogeneity of geological structures in the subsurface and the lack of spatially exhaustive datasets with direct measurements of rock and fluid properties the assessment of the potential storage and forecasting of the dynamic conditions such as co2 plume location are uncertain one of the main sources of uncertainty is the lack of knowledge of the spatial distributions of rock and fluid properties in the aquifer li et al 2011 if the aquifer spatial distributions of rock and fluid properties are known then numerical simulations of fluid flow through porous media allow predicting the fluid and pressure conditions during and after injection however the distribution of rock and fluid properties in the reservoirs cannot be exactly determined due to the lack of direct measurements noise in the indirect measurements such as well logs and surface geophysical data the approximations in the physical models and the natural heterogeneity of the reservoir rocks geostatistical realizations and fluid flow simulations have been previously combined to study the uncertainty in reservoir models and predictions in petroleum geoscience oliver et al 2008 mohamed et al 2010 caeiro et al 2015 and hydrogeology capilla et al 1997 gómez hernánez et al 1997 kitanidis 1997 zimmerman et al 1998 the application of these methods to carbon storage studies is not completely new szulczewski and juanes 2009 bellenfant et al 2009 bergmo et al 2011 gasda et al 2012 szulczewski et al 2012 deng et al 2012 bhowmik et al 2013 castelletto et al 2013 jeong et al 2013 li and benson 2015 levine et al 2016 risk assessment in storage capacity has been investigated in several works wilson et al 2003 zhou et al 2008 sifuentes et al 2009 burruss et al 2009 brennan et al 2010 nordbotten et al 2012 sun et al 2013 ellett et al 2013 peck et al 2014 gorecki et al 2015 jung et al 2018 however accurate analysis of all the sources of uncertainty in the pre injection static model and complete studies on the propagation of the uncertainty in the model predictions are still missing in literature several geophysical measurements have been utilized for static reservoir characterization and reservoir monitoring including 3d prestack seismic data electromagnetic data gravity data and time lapse seismic data chadwick et al 2005 chadwick et al 2006 chadwick et al 2009 alnes et al 2011 grude et al 2013 ivandic et al 2015 grana et al 2017 roach and white 2018 wang et al 2018 glubokovskikh et al 2019 the above mentioned studies focus on the location of the co2 plume several works focusing on the pressure front extent are also available nordbotten and celia 2010 oruganti et al 2011 senel and chugunov 2013 mathias et al 2013 finally some of the published works also include the integration of studies of enhanced oil recovery liu et al 2013 dai et al 2014 water production to optimize the aquifer storage capacity bergmo et al 2011 and history matching for reservoir updating chadwick and noy 2010 most of the presented methods have been tested in depleted reservoirs and some of the most successful and documented applications are located in the north sea in particular the sleipner field and the johansen field arts et al 2008 eigestad et al 2009 wei and saaf 2009 bergmo et al 2009a bergmo et al 2009b chadwick and noy 2010 gasda et al 2012 sundal et al 2013 different methods including stochastic optimization and sampling algorithms can be adopted to update the static reservoir model of rock and fluid properties and make the dynamic model more predictive for future forecasting bellenfant et al 2009 tavakoli et al 2013 sun et al 2013 grana et al 2017 in particular data assimilation algorithms are commonly used for history matching in petroleum engineering two of the main common algorithms for history matching of reservoir data are the ensemble kalman filter and the ensemble smoother evensen 2009 emerick and reynolds 2013 these methods have been applied to production and geophysical data chen and oliver 2012 and 2017 luo et al 2016 2018 liu and grana 2018 lorentzen et al 2019 canchumuni et al 2019a 2019b ma et al 2019 machine learning methods have also been used to solve the data assimilation problem tahmasebi et al 2018 laloy et al 2018 chen et al 2018 liu and grana 2019 liu et al 2019 sun and durlofsky 2019 luo 2019 puzyrev 2019 etienam 2019 the goal of this work is to present a mathematical method to improve the aquifer characterization in terms of petrophysical properties and their spatial distribution we propose to integrate injection data and well observations with time lapse seismic surveys to update the static model of porosity and permeability every time new data seismic or borehole data are measured and improve the accuracy and precision of the predictive fluid flow model the advantage of the proposed approach is that it provides more reliable estimates of the co2 plume migration thanks to an improved understating and characterization of the spatial petrophysical model the mathematical method is based on a stochastic approach to data assimilation namely the ensemble smoother and it combines geostatistical realizations of spatial property models fluid flow simulations geophysical modeling of measured surface seismic data and machine learning algorithms in particular we generate multiple static pre injection models of reservoir properties to obtain a set of potential reservoir models that account for all the possible geological scenarios i e all the possible combinations of properties that cannot be excluded by geological prior knowledge this ensemble of models represents the uncertainty in the reservoir model before data acquisition we then update the reservoir model using time lapse seismic data and monitoring wells the result is a set of equiprobable petrophysical models that allows the prediction and uncertainty quantification of aquifer conditions at every time of the co2 storage process before during and after injection in particular this assessment allows predicting the uncertainty in the storage capacity as well as in the co2 plume location the methodology accounts for several sources of information the accurate quantification of these properties for carbon storage studies allows successful risk analysis and decision making for co2 storage studies the dataset in this study is the johansen formation located offshore norway bergmo et al 2009a 2009b and 2011 the static and dynamic fluid flow simulation models have been presented in bergmo et al 2011 in this work a synthetic time lapse seismic dataset was created to demonstrate the methodology and prove the validity of stochastic data assimilation methods for improving the petrophysical characterization of deep saline aquifers and the value of information of time lapse seismic data for reservoir monitoring of the co2 plume migration 2 methodology we aim to predict porosity and permeability from borehole injection and monitoring data and time lapse seismic data the modeling can be formulated as an inverse problem 1 d h m e where d includes injection and monitoring borehole data and time lapse seismic surveys m is the spatial petrophysical model of porosity and permeability and h is the forward operator including rock physics seismic wave propagation and fluid flow simulation tarantola 2005 oliver et al 2008 the goal is to study the most likely value and the uncertainty in the predictions of model variables m based on the measured data d therefore we aim to compute the posterior distribution of the model properties m given the observed data d the probability p m d indicates the posterior density function of the model properties m given the data d and it is computed as 2 p m d p d m p m p d where p m is the prior distribution of the model properties m p d m is the likelihood function that predicts the probability of the data d of being observed as a response of the model properties m and p d is a normalizing constant equal to the integral p d m p m d m 2 1 forward operator in this section we describe the physical models linking the model parameters porosity and permeability to the measured borehole and geophysics data the geophysics operator includes a rock physics model to predict elastic properties such as p and s wave velocity and density from petrophysical properties porosity mineralogy and saturations and the seismic wave propagation model to predict travel time and seismic wave amplitudes from the elastic properties aquifer rocks might have different mineral fractions porosity and fluid saturation and consequently generate different elastic responses the elastic variables of interest v are generally expressed as a function of rock and fluid properties p 3 v g p where g is the rock physics model there is an extensive literature on rock physics modeling for different rock types such as shale poorly and highly consolidated sandstone and carbonate given that the target formation of this co2 storage study consists of sandstones the adopted rock physics model in this study is the soft sandstone model mavko et al 2009 in this model at each location in the model the matrix bulk and shear moduli are computed using the voigt reuss hill average whereas the bulk and shear moduli of the dry unsaturated rock at the critical porosity φ0 are obtained using the hertz mindlin equations appendix a the moduli of the dry rock with variable porosity φ in the range 0 φ0 are estimated by interpolating the two end members matrix and dry rock at critical porosity using the modified hashin shtrikman lower bounds appendix a finally by applying gassmann s fluid model gassmann 1951 we can obtain the bulk and shear moduli of the saturated rock ksr and μ sr and compute the p and s wave velocity by definition 4 v p k s r 4 3 μ s r ρ s r 5 v s μ s r ρ s r where ρ sr is the density of the saturated rock and it is computed as the weighted linear average of the matrix density ρ m and fluid density ρ f 6 ρ s r 1 ϕ ρ m f m ϕ ρ f s f where φ is the porosity of the rock and the matrix density ρ m and fluid density ρ f depend on the mineral fractions fm and fluid saturations sf respectively we can then predict the seismic response through a simplified seismic wave propagation model in this work we assume that the geophysical data are time lapse post stack seismic s t measured in time domain generated by the convolutional model 7 s t w t r t v t e where w t is the seismic wavelet assumed to be known r represents the reflection coefficients calculated from the elastic properties v t and e represents the measurement errors therefore assuming that the mineral fractions are homogeneous in the aquifer the seismic data are modeled as the response of the spatial rock and fluid property model 8 s x y t g ϕ x y z s f x y z e where x y z are the spatial locations of the model and g is the geophysical operator the fluid flow operator includes equations for fluid flow in porous media to predict the saturation and pressure at each time step given the initial petrophysical model and injection parameters which is commonly refer to as reservoir simulation in particular the co2 injection and migration in a deep saline aquifer can be modeled as a two phase miscible flow of the brine co2 system that can be numerically solved using the black oil framework eigestad et al 2009 lie 2016 to investigate the long term co2 migration after injection for instance hundreds of years the standard three dimensional simulation is computationally prohibitive for this reason alternative simplified approaches have been developed with the assumption that the flow system is in vertical equilibrium ve nilsen et al 2011 so that the analytical expressions of the vertical distribution of fluid phases are available in such simplified models the three dimensional fluid flow simulation is reduced to two dimensional problems while correctly approximating the three dimensional behavior consequently the so called ve models largely speeds up the computational time nilsen et al 2011 in this work we adopt the mrst co2lab lie 2016 in which a wide class of vertical equilibrium models have been implemented to simulate the process of co2 injection and migration in general given a spatial model of rock properties porosity φ x y z and permeability k x y z and initial fluid parameters at time t 0 in the aquifer and assuming that rock properties do not change over time t the simulation returns the spatial model of saturation and pressure conditions at any given time t 9 s f x y z t p f x y z t f ϕ x y z k x y z s f x y z t 0 p f x y z t 0 the fluid flow model f in eq 9 is used to predict the saturation conditions at the monitoring well locations during and after injection in our approach we assume that the initial fluid parameters at time t 0 in the aquifer are constant and known fully brine saturated aquifer with constant fluid pressure whereas rock properties porosity φ x y z and permeability k x y z are unknown 2 2 inverse problem in this section we propose a mathematical approach for the solution of the inverse problem in eq 1 where the unknown model variables m are porosity and permeability the observed data d are the surface seismic data and the injection and monitoring data at the well locations and the forward operator h includes the rock physics seismic and fluid flow models in eqs 4 9 the method we adopt is the ensemble smoother with multi data assimilation es mda emerick and reynolds 2013 in particular we generate an initial set of petrophysical models m porosity and permeability and we update the ensemble using the es mda to match the measured data d borehole and time lapse seismic data the ensemble of updated models provides an approximation of the posterior distribution of the model variables the large dimension of the geophysical dataset would make the application of the es mda unfeasible because it would require a large amount of petrophysical models in the initial ensemble to avoid the ensemble collapse for this reason we propose to apply the deep convolutional autoencoder dcae goodfellow et al 2016 to reduce the dimension of the data space in es mda the model variables are assumed to be gaussian defined in the domain when model variables are defined in a bounded domain a logit transformation can be applied to map them from the domain mmin mmax to the domain apply the es mda and back transform the results in the original model space after updating liu and grana 2018 because the inverse problem is not linear the mean and covariance matrices cannot be analytically computed in the es mda the covariance matrices of the model and of the data and their cross covariance are approximated using the sample covariance matrix of the model and prediction ensemble the algorithm includes the following steps 1 we define the number ne of models in the ensemble and a sequence of inflation coefficients α i i 1 n with i 1 n 1 α i 1 where n is the number of data assimilations emerick and reynolds 2013 we generate an ensemble of ne prior realizations m j i j 1 n e of the model properties using geostatistical algorithms where the superscript i indicates the iteration number that is equal to 1 for the initial iteration fig 1 step 1 2 we then run the fluid flow simulation eq 9 to obtain the saturation distributions for each ensemble model at each time fig 1 step 2a and we compute the corresponding the geophysical response eq 8 to predict the seismic data d j i j 1 n e fig 1 step 2b and apply a perturbation d p j i d j i α i σ d 1 2 z p j i to each model prediction where σ d is the covariance matrix of the error measurements and z p j i n 0 i n for j 1 ne i e z p j i is distributed according to a gaussian distribution with zero mean covariance matrix i n with i n being identity matrix of size n n and n being the number of data measurements 3 we update the model ensemble fig 1 step 3 according to the following expression 10 m j i 1 m j i k i d p j d j i for j 1 ne where the superscript i indicates the iteration in eq 10 k i is computed as 11 k i σ m d i σ d d i α i σ d 1 where σ m d i is the m n cross covariance matrix of models m i and predicted data d i where m is the number of model variables and σ d d i is the n n covariance matrix of the predicted data d i 12 σ m d i 1 n e 1 j 1 n e m j i m i d j i d i 13 σ d d i 1 n e 1 j 1 n e d j i d i d j i d i with m i and d i being the means of the models and data predictions of the ensemble 4 we repeat steps 2 and 3 for n iterations fig 1 step 4 for the geostatistical simulations in step 1 we use the fast fourier transform moving average method fft ma however other algorithms such as sequential gaussian simulations could be used for bounded properties logit transformation or normal score transformations could be applied to map the properties in the set of real numbers for highly bimodal properties sequential gaussian mixture simulations or multi point statistics algorithms mariethoz and caers 2014 can be adopted the forward model in step 2 combines the fluid flow simulations step 2a and the seismic forward model step 2b the saturation model predictions of the initial porosity and permeability models are obtained using fluid flow simulations based on the vertical equilibrium as in nilsen et al 2011 however other approaches such as percolation based simulators could be used the elastic and seismic response of the saturation models are computed using a rock physics model and seismic convolutional operators poroelastic and wave equations could also be applied in our application the number of model variables m is the number of unknown porosity and permeability values and the number of data measurements n is the number of borehole observations plus the number of seismic data measures for example for an aquifer model with 100 000 cells the number of model variables m is 200 000 100 000 unknown values of porosity and 100 000 unknown values of permeability for a dataset with borehole measurements every month in 1 injection and 1 monitoring well and seismic data every 5 years for 20 years of injection the number of data measurements n is n 480 500 000 500 480 assuming that the seismic acquisition grid coincide with the aquifer model grid the dimension of the dataset makes the application of the es mda algorithm unfeasible indeed in the es mda the ensemble of models must be large enough to avoid the collapse of the ensemble i e model variances converging to 0 because of the large dimension of the data the required number of models in the ensemble is extremely large and would require a long computational time for the calculation of the data predictions using the fluid flow and geophysical forward models instead of increasing the size of the ensemble we propose to reduce the dimension of the data and apply the es mda in the reduced data space fig 2 in particular we apply the deep convolutional autoencoder dcae to reduce the data as in liu and grana 2019 autoencoders are an unsupervised learning technique for the sparse representation learning using neural networks goodfellow et al 2016 unlike the multilayer perceptron mlp the architecture of the autoencoder is generally designed by imposing a bottleneck in the network to compress the original input data fig 3 such bottleneck divides the autoencoder network into two parts the encoder component in which the network learns how to compress the input data x into a sparse latent feature space z and the decoder component in which the network learns how to recover the data x from the sparse features z with minimum reconstruction error l x x 2 2 an autoencoder consisting of fully connected layers with linear activation function is theoretically equivalent to the principle component analysis goodfellow et al 2016 therefore it can only capture the linear patterns and correlations in the data space to avoid the limitation we combine the autoencoder with convolutional neural network cnn namely the deep convolutional autoencoder dcae the building blocks of dcae are convolutional layers with non linear activation function and max pooling layers rather than fully connected layers which provides the network the ability of exploiting spatial and non linear relationships in the data space fig 3 depicts the network architecture of the dcae that we design to reduce the seismic data in this study the detailed parameters of each layer are summarized in table 1 the network is almost symmetric the encoder consists of three convolutional layers with decreasing number of filters and three max pooling layers to extract the most relevant features in the input space by contrast the decoder consists of three convolutional layers with increasing number of filters and up pooling to recover the input data from the sparse features in this study the model is implemented using keras with tensorflow as backend engine and the nvidia geforce gtx 1080 gpu is used to speed up the training 3 application in this section we apply the es mda with dcae to the petrophysical characterization of deep saline aquifers in terms of porosity and permeability for co2 storage and plume prediction the dataset in this study is built based on the johansen formation a potential candidate for large scale co2 storage offshore the south western coast of norway bergmo et al 2011 the burial depth of the johansen formation varies between 2200 m and 3100 m below sea level and the thickness ranges from 80 m to 120 m such depth and thickness allow storing considerable amount of co2 in the form of a dense phase 3 1 aquifer model and observations the simulation grid in the johansen data set is 100 100 11 grid cells each of size 500 m 500 m laterally and variable heights from 16 to 24 m fig 4 the model consists of three geological zones and is divided into two sectors by a major fault the uppermost five layers correspond to the dunlin shale above the johansen formation and the lowermost layer corresponds to the amundsen shale the permeabilities of the dunlin and amundsen shale are assumed to be constant 0 01 and 0 1 milli darcy md respectively both shale formations are set to be inactive grid cells in the simulation the permeability and porosity of the johansen formation are presented in figs 4a and 4b respectively the injection well well i1 in fig 4a is positioned near the main fault with a constant co2 injection rate of 60 000 m3 day the injection period is 20 years when injection stops the induced pressure gradient gradually dissipates as a result the fluid phases are driven by the gravity and capillary forces to rearrange themselves in the aquifer as part of the migration process to monitor the co2 sequestration two monitoring wells wells p1 and p2 in fig 4a are configured ten grid cells east and north of the injector well respectively corresponding to a distance of approximately 5 km figs 4c and d show two sections of porosity crossing the injector and monitoring wells to reduce the computational cost we reduce the study area of interest to a reduced 32 32 5 with the injector at the center the fluid flow simulation shows that the co2 plume remains in this region in the period under study the permeability and porosity of each of the 5 layers of the johansen formation are presented in fig 5 in this work the bottom hole pressure bhp of the injection well and co2 saturation at the monitoring wells are simulated using mrst co2lab and available every month during the entire injection period i e 20 years time lapse post stack seismic data are computed using the convolution seismic and rock physics models the generated geophysical dataset includes a base survey at the beginning of injection and a monitoring survey after 10 years the time lapse seismic data are shown in fig 6 3 2 prior models and predicted data in this study we generate 1000 prior petrophysical models through the fft ma and probability field simulation algorithm srivastava 1992 froidevaux 1993 such realizations constitute the initial ensemble for the application of es mda in the geostatistical simulation the local mean is set to be the low frequency trend of the true petrophysical model figs 7 a and 7b show the mean permeability and porosity of the prior models respectively two prior realizations realization 100 and 500 are also shown in fig 8 the corresponding injection and monitoring data are simulated using mrst co2lab with the same well configurations and simulation parameters as the true model fig 9 3 3 data reparameterization with dcae to avoid the ensemble collapse due to the large dimension of seismic data we apply a data reduction to reduce the time lapse seismic data to a much lower dimensional data space by applying the dcae with the dcae depicted in fig 3 the 32 32 10 input data that are extracted from time lapse data at the interface of model layers are sparsely represented by a 1024 dimensional vector in this application we used all the predicted seismic data from prior realizations and the true observation to train the dcae with one nvidia geforce gtx 1080 gpu the training takes approximately 5 minutes to converge fig 10 shows the original and reconstructed seismic of one realization randomly selected from the training set we can see that the reconstruction error is relatively small and the main space structures are well captured which indicates that the extracted sparse features contain most of the information of the input data therefore it is applicable and efficient to use the trained dcae to reduce the order of seismic data 3 4 data assimilation results in this study the iteration number of es mda is set to be 4 with the inflation coefficients of 9 333 7 0 4 0 and 2 0 respectively as suggested for history matching studies in emerick and reynolds 2013 we assume that the covariance matrix σ d is a diagonal matrix where the elements on the diagonal represent the noise level in the data since the deep neural network cannot be expressed explicitly we use the noise level in the original seismic data to estimate the noise standard deviation in the latent space the standard deviations of noise of co2 saturation bhp and the original seismic amplitudes are 0 01 1 and 0 02 respectively after encoding using the dcae the seismic data are scaled from 0 3 0 2 to 0 1 and the standard deviation of the noise of the re parameterized seismic data is 0 08 figs 11 a and 11b show the posterior mean of the petrophysical models and illustrate a good match between the predicted models and the true models fig 12 shows that the uncertainty in the posterior models is largely reduced compared to the prior models after the assimilation of both the borehole data and re parameterized seismic furthermore fig 13 shows that the predicted injection and monitoring data from the obtained posterior models match the true observations not only the mean but also the individual realizations converge to the true solution fig 14 shows the posterior realizations obtained from the prior models in fig 8 and illustrates that both realizations converge to the true model after updating to determine the value of seismic data in the prediction of the petrophysical properties we also perform the inversion without time lapse seismic data i e with borehole data only the results are shown in figs 11c and 11d in general the results with seismic data show a much better agreement with the true models compares with the results with borehole data only especially in the regions away well locations from the root mean squared errors rmse in fig 15 we can also conclude that seismic data provide important information to reduce the model mismatch 3 5 prediction of co2 plume in the co2 storage studies it is of great significance to accurately predict the co2 plume location in the aquifer in the long term figs 16 and 17 compare the true and predicted co2 plume after 100 years of migration from the prior and posterior models the predictions are conditioned to borehole data and re parameterized seismic data and are illustrated layer by layer fig 16 and along the vertical sections crossing the injection and monitoring wells fig 17 figs 18 and 19 show the same results after 400 years of migration in both cases the posterior predicted co2 plume has a much better agreement with the co2 plume from the true model in each layer compared to the prior models 4 conclusion traditionally geophysical methods have been used for static modeling of pre injection aquifer conditions however due to several sources of uncertainty in the data and in the physical models pre injection models of rock and fluid properties are often inaccurate in this work we proposed the integration of geophysical data in history matching in particular the simultaneous assimilation of borehole data and time lapse seismic survey to overcome the limitations due to the large dimension of geophysical data we combined the stochastic inversion method with the deep convolutional autoencoder to reduce the dimension of the observed data and perform the data assimilation in a lower dimensional space this approach allows improving the aquifer characterization in terms of petrophysical properties such as porosity and permeability and obtaining more precise predictions of the dynamic behavior of the co2 plume during and after injection the method was illustrated and validated on a synthetic deep saline aquifer model and shows the ability of the proposed methodology to improve the petrophysical characterization as well as the model forecasting during migration credit authorship contribution statement mingliang liu conceptualization methodology software writing original draft dario grana conceptualization supervision writing review editing acknowledgements authors acknowledge the school of energy resources and the department of geology and geophysics of university of wyoming for the support the authors also would like to thank sintef for providing the matlab reservoir simulation toolbox appendix a soft sand model the soft sand model mavko et al 2009 allows computing the dry rock elastic moduli of a rock with known porosity based hertz mindlin contact theory and hashin shtrimkman elastic bounds first the bulk and shear moduli of the dry rock at the critical porosity value khm and ghm are computed using hertz mindlin equations a1 k h m p e n 1 ϕ 0 g m a t 2 18 π 1 ν 2 3 a2 g h m 5 4 ν 5 2 ν 3 p e n 1 ϕ 0 g m a t 2 2 π 1 ν 2 3 where φ0 is the critical porosity ν is the grain poisson s ratio pe is the effective pressure and n is average number of contacts per grain mindlin 1949 then for porosity values within the range 0 φ0 the bulk and shear moduli of the dry rock kdry and gdry are estimated by interpolating the matrix elastic moduli at zero porosity and the hertz mindlin elastic moduli at the critical porosity using the modified hashin shtrikman lower bounds hashin and shtrikman 1963 a3 k d r y ϕ ϕ 0 k h m 4 3 μ h m 1 ϕ ϕ 0 k m a t 4 3 μ h m 1 4 3 μ h m a4 μ d r y ϕ ϕ 0 μ h m 1 6 ξ μ h m 1 ϕ ϕ 0 μ m a t 1 6 ξ μ h m 1 1 6 ξ μ h m where a5 ξ 9 k h m 8 μ h m k h m 2 μ h m 
470,carbon dioxide sequestration in deep saline aquifers requires accurate and precise methods to monitor carbon capture and storage and detect leakage risks the assessment of the co2 plume location during injection and storage depends on the accuracy of the spatial distribution model of petrophysical properties such as porosity and permeability this work focuses on stochastic methods for petrophysical characterization and presents a method for the prediction of porosity and permeability using borehole observations and surface geophysical data this study utilizes injection and monitoring measurements at the borehole locations and time lapse seismic surveys the proposed method is based on a stochastic approach to inverse problems with data assimilation namely the ensemble smoother with multi data assimilation ensemble based methods are generally unfeasible when applied to large geophysical datasets such as time lapse seismic surveys in the proposed approach a machine learning method namely the deep convolutional autoencoder is applied to reduce the dimension of the seismic data the ensemble smoother is then applied in a lower dimensional data space to predict the aquifer petrophysical properties this method updated predictions of porosity and permeability every time new data either seismic surveys or borehole data are available to reduce the uncertainty in the co2 plume prediction the method is tested and validated on a synthetic geophysical dataset generated for the johansen formation a potential large scale offshore site for co2 storage keywords co2 storage co2 plume prediction data assimilation deep learning deep saline aquifers 1 introduction co2 sequestration and storage in deep saline aquifers has been widely studied kopp et al 2010 goodman et al 2013 castelletto et al 2013 pool et al 2013 jeong et al 2013 ghorbanidehno et al 2015 li and benson 2015 levine et al 2016 ma et al 2019 zhu et al 2019 one of the main challenges for a successful implementation of carbon storage in deep saline formations is the ability to reliably monitor the co2 plume migration and the dynamic conditions during and after injection goodman et al 2011 and 2017 bachu 2008 and 2015 miller et al 2014 due to the complexity and heterogeneity of geological structures in the subsurface and the lack of spatially exhaustive datasets with direct measurements of rock and fluid properties the assessment of the potential storage and forecasting of the dynamic conditions such as co2 plume location are uncertain one of the main sources of uncertainty is the lack of knowledge of the spatial distributions of rock and fluid properties in the aquifer li et al 2011 if the aquifer spatial distributions of rock and fluid properties are known then numerical simulations of fluid flow through porous media allow predicting the fluid and pressure conditions during and after injection however the distribution of rock and fluid properties in the reservoirs cannot be exactly determined due to the lack of direct measurements noise in the indirect measurements such as well logs and surface geophysical data the approximations in the physical models and the natural heterogeneity of the reservoir rocks geostatistical realizations and fluid flow simulations have been previously combined to study the uncertainty in reservoir models and predictions in petroleum geoscience oliver et al 2008 mohamed et al 2010 caeiro et al 2015 and hydrogeology capilla et al 1997 gómez hernánez et al 1997 kitanidis 1997 zimmerman et al 1998 the application of these methods to carbon storage studies is not completely new szulczewski and juanes 2009 bellenfant et al 2009 bergmo et al 2011 gasda et al 2012 szulczewski et al 2012 deng et al 2012 bhowmik et al 2013 castelletto et al 2013 jeong et al 2013 li and benson 2015 levine et al 2016 risk assessment in storage capacity has been investigated in several works wilson et al 2003 zhou et al 2008 sifuentes et al 2009 burruss et al 2009 brennan et al 2010 nordbotten et al 2012 sun et al 2013 ellett et al 2013 peck et al 2014 gorecki et al 2015 jung et al 2018 however accurate analysis of all the sources of uncertainty in the pre injection static model and complete studies on the propagation of the uncertainty in the model predictions are still missing in literature several geophysical measurements have been utilized for static reservoir characterization and reservoir monitoring including 3d prestack seismic data electromagnetic data gravity data and time lapse seismic data chadwick et al 2005 chadwick et al 2006 chadwick et al 2009 alnes et al 2011 grude et al 2013 ivandic et al 2015 grana et al 2017 roach and white 2018 wang et al 2018 glubokovskikh et al 2019 the above mentioned studies focus on the location of the co2 plume several works focusing on the pressure front extent are also available nordbotten and celia 2010 oruganti et al 2011 senel and chugunov 2013 mathias et al 2013 finally some of the published works also include the integration of studies of enhanced oil recovery liu et al 2013 dai et al 2014 water production to optimize the aquifer storage capacity bergmo et al 2011 and history matching for reservoir updating chadwick and noy 2010 most of the presented methods have been tested in depleted reservoirs and some of the most successful and documented applications are located in the north sea in particular the sleipner field and the johansen field arts et al 2008 eigestad et al 2009 wei and saaf 2009 bergmo et al 2009a bergmo et al 2009b chadwick and noy 2010 gasda et al 2012 sundal et al 2013 different methods including stochastic optimization and sampling algorithms can be adopted to update the static reservoir model of rock and fluid properties and make the dynamic model more predictive for future forecasting bellenfant et al 2009 tavakoli et al 2013 sun et al 2013 grana et al 2017 in particular data assimilation algorithms are commonly used for history matching in petroleum engineering two of the main common algorithms for history matching of reservoir data are the ensemble kalman filter and the ensemble smoother evensen 2009 emerick and reynolds 2013 these methods have been applied to production and geophysical data chen and oliver 2012 and 2017 luo et al 2016 2018 liu and grana 2018 lorentzen et al 2019 canchumuni et al 2019a 2019b ma et al 2019 machine learning methods have also been used to solve the data assimilation problem tahmasebi et al 2018 laloy et al 2018 chen et al 2018 liu and grana 2019 liu et al 2019 sun and durlofsky 2019 luo 2019 puzyrev 2019 etienam 2019 the goal of this work is to present a mathematical method to improve the aquifer characterization in terms of petrophysical properties and their spatial distribution we propose to integrate injection data and well observations with time lapse seismic surveys to update the static model of porosity and permeability every time new data seismic or borehole data are measured and improve the accuracy and precision of the predictive fluid flow model the advantage of the proposed approach is that it provides more reliable estimates of the co2 plume migration thanks to an improved understating and characterization of the spatial petrophysical model the mathematical method is based on a stochastic approach to data assimilation namely the ensemble smoother and it combines geostatistical realizations of spatial property models fluid flow simulations geophysical modeling of measured surface seismic data and machine learning algorithms in particular we generate multiple static pre injection models of reservoir properties to obtain a set of potential reservoir models that account for all the possible geological scenarios i e all the possible combinations of properties that cannot be excluded by geological prior knowledge this ensemble of models represents the uncertainty in the reservoir model before data acquisition we then update the reservoir model using time lapse seismic data and monitoring wells the result is a set of equiprobable petrophysical models that allows the prediction and uncertainty quantification of aquifer conditions at every time of the co2 storage process before during and after injection in particular this assessment allows predicting the uncertainty in the storage capacity as well as in the co2 plume location the methodology accounts for several sources of information the accurate quantification of these properties for carbon storage studies allows successful risk analysis and decision making for co2 storage studies the dataset in this study is the johansen formation located offshore norway bergmo et al 2009a 2009b and 2011 the static and dynamic fluid flow simulation models have been presented in bergmo et al 2011 in this work a synthetic time lapse seismic dataset was created to demonstrate the methodology and prove the validity of stochastic data assimilation methods for improving the petrophysical characterization of deep saline aquifers and the value of information of time lapse seismic data for reservoir monitoring of the co2 plume migration 2 methodology we aim to predict porosity and permeability from borehole injection and monitoring data and time lapse seismic data the modeling can be formulated as an inverse problem 1 d h m e where d includes injection and monitoring borehole data and time lapse seismic surveys m is the spatial petrophysical model of porosity and permeability and h is the forward operator including rock physics seismic wave propagation and fluid flow simulation tarantola 2005 oliver et al 2008 the goal is to study the most likely value and the uncertainty in the predictions of model variables m based on the measured data d therefore we aim to compute the posterior distribution of the model properties m given the observed data d the probability p m d indicates the posterior density function of the model properties m given the data d and it is computed as 2 p m d p d m p m p d where p m is the prior distribution of the model properties m p d m is the likelihood function that predicts the probability of the data d of being observed as a response of the model properties m and p d is a normalizing constant equal to the integral p d m p m d m 2 1 forward operator in this section we describe the physical models linking the model parameters porosity and permeability to the measured borehole and geophysics data the geophysics operator includes a rock physics model to predict elastic properties such as p and s wave velocity and density from petrophysical properties porosity mineralogy and saturations and the seismic wave propagation model to predict travel time and seismic wave amplitudes from the elastic properties aquifer rocks might have different mineral fractions porosity and fluid saturation and consequently generate different elastic responses the elastic variables of interest v are generally expressed as a function of rock and fluid properties p 3 v g p where g is the rock physics model there is an extensive literature on rock physics modeling for different rock types such as shale poorly and highly consolidated sandstone and carbonate given that the target formation of this co2 storage study consists of sandstones the adopted rock physics model in this study is the soft sandstone model mavko et al 2009 in this model at each location in the model the matrix bulk and shear moduli are computed using the voigt reuss hill average whereas the bulk and shear moduli of the dry unsaturated rock at the critical porosity φ0 are obtained using the hertz mindlin equations appendix a the moduli of the dry rock with variable porosity φ in the range 0 φ0 are estimated by interpolating the two end members matrix and dry rock at critical porosity using the modified hashin shtrikman lower bounds appendix a finally by applying gassmann s fluid model gassmann 1951 we can obtain the bulk and shear moduli of the saturated rock ksr and μ sr and compute the p and s wave velocity by definition 4 v p k s r 4 3 μ s r ρ s r 5 v s μ s r ρ s r where ρ sr is the density of the saturated rock and it is computed as the weighted linear average of the matrix density ρ m and fluid density ρ f 6 ρ s r 1 ϕ ρ m f m ϕ ρ f s f where φ is the porosity of the rock and the matrix density ρ m and fluid density ρ f depend on the mineral fractions fm and fluid saturations sf respectively we can then predict the seismic response through a simplified seismic wave propagation model in this work we assume that the geophysical data are time lapse post stack seismic s t measured in time domain generated by the convolutional model 7 s t w t r t v t e where w t is the seismic wavelet assumed to be known r represents the reflection coefficients calculated from the elastic properties v t and e represents the measurement errors therefore assuming that the mineral fractions are homogeneous in the aquifer the seismic data are modeled as the response of the spatial rock and fluid property model 8 s x y t g ϕ x y z s f x y z e where x y z are the spatial locations of the model and g is the geophysical operator the fluid flow operator includes equations for fluid flow in porous media to predict the saturation and pressure at each time step given the initial petrophysical model and injection parameters which is commonly refer to as reservoir simulation in particular the co2 injection and migration in a deep saline aquifer can be modeled as a two phase miscible flow of the brine co2 system that can be numerically solved using the black oil framework eigestad et al 2009 lie 2016 to investigate the long term co2 migration after injection for instance hundreds of years the standard three dimensional simulation is computationally prohibitive for this reason alternative simplified approaches have been developed with the assumption that the flow system is in vertical equilibrium ve nilsen et al 2011 so that the analytical expressions of the vertical distribution of fluid phases are available in such simplified models the three dimensional fluid flow simulation is reduced to two dimensional problems while correctly approximating the three dimensional behavior consequently the so called ve models largely speeds up the computational time nilsen et al 2011 in this work we adopt the mrst co2lab lie 2016 in which a wide class of vertical equilibrium models have been implemented to simulate the process of co2 injection and migration in general given a spatial model of rock properties porosity φ x y z and permeability k x y z and initial fluid parameters at time t 0 in the aquifer and assuming that rock properties do not change over time t the simulation returns the spatial model of saturation and pressure conditions at any given time t 9 s f x y z t p f x y z t f ϕ x y z k x y z s f x y z t 0 p f x y z t 0 the fluid flow model f in eq 9 is used to predict the saturation conditions at the monitoring well locations during and after injection in our approach we assume that the initial fluid parameters at time t 0 in the aquifer are constant and known fully brine saturated aquifer with constant fluid pressure whereas rock properties porosity φ x y z and permeability k x y z are unknown 2 2 inverse problem in this section we propose a mathematical approach for the solution of the inverse problem in eq 1 where the unknown model variables m are porosity and permeability the observed data d are the surface seismic data and the injection and monitoring data at the well locations and the forward operator h includes the rock physics seismic and fluid flow models in eqs 4 9 the method we adopt is the ensemble smoother with multi data assimilation es mda emerick and reynolds 2013 in particular we generate an initial set of petrophysical models m porosity and permeability and we update the ensemble using the es mda to match the measured data d borehole and time lapse seismic data the ensemble of updated models provides an approximation of the posterior distribution of the model variables the large dimension of the geophysical dataset would make the application of the es mda unfeasible because it would require a large amount of petrophysical models in the initial ensemble to avoid the ensemble collapse for this reason we propose to apply the deep convolutional autoencoder dcae goodfellow et al 2016 to reduce the dimension of the data space in es mda the model variables are assumed to be gaussian defined in the domain when model variables are defined in a bounded domain a logit transformation can be applied to map them from the domain mmin mmax to the domain apply the es mda and back transform the results in the original model space after updating liu and grana 2018 because the inverse problem is not linear the mean and covariance matrices cannot be analytically computed in the es mda the covariance matrices of the model and of the data and their cross covariance are approximated using the sample covariance matrix of the model and prediction ensemble the algorithm includes the following steps 1 we define the number ne of models in the ensemble and a sequence of inflation coefficients α i i 1 n with i 1 n 1 α i 1 where n is the number of data assimilations emerick and reynolds 2013 we generate an ensemble of ne prior realizations m j i j 1 n e of the model properties using geostatistical algorithms where the superscript i indicates the iteration number that is equal to 1 for the initial iteration fig 1 step 1 2 we then run the fluid flow simulation eq 9 to obtain the saturation distributions for each ensemble model at each time fig 1 step 2a and we compute the corresponding the geophysical response eq 8 to predict the seismic data d j i j 1 n e fig 1 step 2b and apply a perturbation d p j i d j i α i σ d 1 2 z p j i to each model prediction where σ d is the covariance matrix of the error measurements and z p j i n 0 i n for j 1 ne i e z p j i is distributed according to a gaussian distribution with zero mean covariance matrix i n with i n being identity matrix of size n n and n being the number of data measurements 3 we update the model ensemble fig 1 step 3 according to the following expression 10 m j i 1 m j i k i d p j d j i for j 1 ne where the superscript i indicates the iteration in eq 10 k i is computed as 11 k i σ m d i σ d d i α i σ d 1 where σ m d i is the m n cross covariance matrix of models m i and predicted data d i where m is the number of model variables and σ d d i is the n n covariance matrix of the predicted data d i 12 σ m d i 1 n e 1 j 1 n e m j i m i d j i d i 13 σ d d i 1 n e 1 j 1 n e d j i d i d j i d i with m i and d i being the means of the models and data predictions of the ensemble 4 we repeat steps 2 and 3 for n iterations fig 1 step 4 for the geostatistical simulations in step 1 we use the fast fourier transform moving average method fft ma however other algorithms such as sequential gaussian simulations could be used for bounded properties logit transformation or normal score transformations could be applied to map the properties in the set of real numbers for highly bimodal properties sequential gaussian mixture simulations or multi point statistics algorithms mariethoz and caers 2014 can be adopted the forward model in step 2 combines the fluid flow simulations step 2a and the seismic forward model step 2b the saturation model predictions of the initial porosity and permeability models are obtained using fluid flow simulations based on the vertical equilibrium as in nilsen et al 2011 however other approaches such as percolation based simulators could be used the elastic and seismic response of the saturation models are computed using a rock physics model and seismic convolutional operators poroelastic and wave equations could also be applied in our application the number of model variables m is the number of unknown porosity and permeability values and the number of data measurements n is the number of borehole observations plus the number of seismic data measures for example for an aquifer model with 100 000 cells the number of model variables m is 200 000 100 000 unknown values of porosity and 100 000 unknown values of permeability for a dataset with borehole measurements every month in 1 injection and 1 monitoring well and seismic data every 5 years for 20 years of injection the number of data measurements n is n 480 500 000 500 480 assuming that the seismic acquisition grid coincide with the aquifer model grid the dimension of the dataset makes the application of the es mda algorithm unfeasible indeed in the es mda the ensemble of models must be large enough to avoid the collapse of the ensemble i e model variances converging to 0 because of the large dimension of the data the required number of models in the ensemble is extremely large and would require a long computational time for the calculation of the data predictions using the fluid flow and geophysical forward models instead of increasing the size of the ensemble we propose to reduce the dimension of the data and apply the es mda in the reduced data space fig 2 in particular we apply the deep convolutional autoencoder dcae to reduce the data as in liu and grana 2019 autoencoders are an unsupervised learning technique for the sparse representation learning using neural networks goodfellow et al 2016 unlike the multilayer perceptron mlp the architecture of the autoencoder is generally designed by imposing a bottleneck in the network to compress the original input data fig 3 such bottleneck divides the autoencoder network into two parts the encoder component in which the network learns how to compress the input data x into a sparse latent feature space z and the decoder component in which the network learns how to recover the data x from the sparse features z with minimum reconstruction error l x x 2 2 an autoencoder consisting of fully connected layers with linear activation function is theoretically equivalent to the principle component analysis goodfellow et al 2016 therefore it can only capture the linear patterns and correlations in the data space to avoid the limitation we combine the autoencoder with convolutional neural network cnn namely the deep convolutional autoencoder dcae the building blocks of dcae are convolutional layers with non linear activation function and max pooling layers rather than fully connected layers which provides the network the ability of exploiting spatial and non linear relationships in the data space fig 3 depicts the network architecture of the dcae that we design to reduce the seismic data in this study the detailed parameters of each layer are summarized in table 1 the network is almost symmetric the encoder consists of three convolutional layers with decreasing number of filters and three max pooling layers to extract the most relevant features in the input space by contrast the decoder consists of three convolutional layers with increasing number of filters and up pooling to recover the input data from the sparse features in this study the model is implemented using keras with tensorflow as backend engine and the nvidia geforce gtx 1080 gpu is used to speed up the training 3 application in this section we apply the es mda with dcae to the petrophysical characterization of deep saline aquifers in terms of porosity and permeability for co2 storage and plume prediction the dataset in this study is built based on the johansen formation a potential candidate for large scale co2 storage offshore the south western coast of norway bergmo et al 2011 the burial depth of the johansen formation varies between 2200 m and 3100 m below sea level and the thickness ranges from 80 m to 120 m such depth and thickness allow storing considerable amount of co2 in the form of a dense phase 3 1 aquifer model and observations the simulation grid in the johansen data set is 100 100 11 grid cells each of size 500 m 500 m laterally and variable heights from 16 to 24 m fig 4 the model consists of three geological zones and is divided into two sectors by a major fault the uppermost five layers correspond to the dunlin shale above the johansen formation and the lowermost layer corresponds to the amundsen shale the permeabilities of the dunlin and amundsen shale are assumed to be constant 0 01 and 0 1 milli darcy md respectively both shale formations are set to be inactive grid cells in the simulation the permeability and porosity of the johansen formation are presented in figs 4a and 4b respectively the injection well well i1 in fig 4a is positioned near the main fault with a constant co2 injection rate of 60 000 m3 day the injection period is 20 years when injection stops the induced pressure gradient gradually dissipates as a result the fluid phases are driven by the gravity and capillary forces to rearrange themselves in the aquifer as part of the migration process to monitor the co2 sequestration two monitoring wells wells p1 and p2 in fig 4a are configured ten grid cells east and north of the injector well respectively corresponding to a distance of approximately 5 km figs 4c and d show two sections of porosity crossing the injector and monitoring wells to reduce the computational cost we reduce the study area of interest to a reduced 32 32 5 with the injector at the center the fluid flow simulation shows that the co2 plume remains in this region in the period under study the permeability and porosity of each of the 5 layers of the johansen formation are presented in fig 5 in this work the bottom hole pressure bhp of the injection well and co2 saturation at the monitoring wells are simulated using mrst co2lab and available every month during the entire injection period i e 20 years time lapse post stack seismic data are computed using the convolution seismic and rock physics models the generated geophysical dataset includes a base survey at the beginning of injection and a monitoring survey after 10 years the time lapse seismic data are shown in fig 6 3 2 prior models and predicted data in this study we generate 1000 prior petrophysical models through the fft ma and probability field simulation algorithm srivastava 1992 froidevaux 1993 such realizations constitute the initial ensemble for the application of es mda in the geostatistical simulation the local mean is set to be the low frequency trend of the true petrophysical model figs 7 a and 7b show the mean permeability and porosity of the prior models respectively two prior realizations realization 100 and 500 are also shown in fig 8 the corresponding injection and monitoring data are simulated using mrst co2lab with the same well configurations and simulation parameters as the true model fig 9 3 3 data reparameterization with dcae to avoid the ensemble collapse due to the large dimension of seismic data we apply a data reduction to reduce the time lapse seismic data to a much lower dimensional data space by applying the dcae with the dcae depicted in fig 3 the 32 32 10 input data that are extracted from time lapse data at the interface of model layers are sparsely represented by a 1024 dimensional vector in this application we used all the predicted seismic data from prior realizations and the true observation to train the dcae with one nvidia geforce gtx 1080 gpu the training takes approximately 5 minutes to converge fig 10 shows the original and reconstructed seismic of one realization randomly selected from the training set we can see that the reconstruction error is relatively small and the main space structures are well captured which indicates that the extracted sparse features contain most of the information of the input data therefore it is applicable and efficient to use the trained dcae to reduce the order of seismic data 3 4 data assimilation results in this study the iteration number of es mda is set to be 4 with the inflation coefficients of 9 333 7 0 4 0 and 2 0 respectively as suggested for history matching studies in emerick and reynolds 2013 we assume that the covariance matrix σ d is a diagonal matrix where the elements on the diagonal represent the noise level in the data since the deep neural network cannot be expressed explicitly we use the noise level in the original seismic data to estimate the noise standard deviation in the latent space the standard deviations of noise of co2 saturation bhp and the original seismic amplitudes are 0 01 1 and 0 02 respectively after encoding using the dcae the seismic data are scaled from 0 3 0 2 to 0 1 and the standard deviation of the noise of the re parameterized seismic data is 0 08 figs 11 a and 11b show the posterior mean of the petrophysical models and illustrate a good match between the predicted models and the true models fig 12 shows that the uncertainty in the posterior models is largely reduced compared to the prior models after the assimilation of both the borehole data and re parameterized seismic furthermore fig 13 shows that the predicted injection and monitoring data from the obtained posterior models match the true observations not only the mean but also the individual realizations converge to the true solution fig 14 shows the posterior realizations obtained from the prior models in fig 8 and illustrates that both realizations converge to the true model after updating to determine the value of seismic data in the prediction of the petrophysical properties we also perform the inversion without time lapse seismic data i e with borehole data only the results are shown in figs 11c and 11d in general the results with seismic data show a much better agreement with the true models compares with the results with borehole data only especially in the regions away well locations from the root mean squared errors rmse in fig 15 we can also conclude that seismic data provide important information to reduce the model mismatch 3 5 prediction of co2 plume in the co2 storage studies it is of great significance to accurately predict the co2 plume location in the aquifer in the long term figs 16 and 17 compare the true and predicted co2 plume after 100 years of migration from the prior and posterior models the predictions are conditioned to borehole data and re parameterized seismic data and are illustrated layer by layer fig 16 and along the vertical sections crossing the injection and monitoring wells fig 17 figs 18 and 19 show the same results after 400 years of migration in both cases the posterior predicted co2 plume has a much better agreement with the co2 plume from the true model in each layer compared to the prior models 4 conclusion traditionally geophysical methods have been used for static modeling of pre injection aquifer conditions however due to several sources of uncertainty in the data and in the physical models pre injection models of rock and fluid properties are often inaccurate in this work we proposed the integration of geophysical data in history matching in particular the simultaneous assimilation of borehole data and time lapse seismic survey to overcome the limitations due to the large dimension of geophysical data we combined the stochastic inversion method with the deep convolutional autoencoder to reduce the dimension of the observed data and perform the data assimilation in a lower dimensional space this approach allows improving the aquifer characterization in terms of petrophysical properties such as porosity and permeability and obtaining more precise predictions of the dynamic behavior of the co2 plume during and after injection the method was illustrated and validated on a synthetic deep saline aquifer model and shows the ability of the proposed methodology to improve the petrophysical characterization as well as the model forecasting during migration credit authorship contribution statement mingliang liu conceptualization methodology software writing original draft dario grana conceptualization supervision writing review editing acknowledgements authors acknowledge the school of energy resources and the department of geology and geophysics of university of wyoming for the support the authors also would like to thank sintef for providing the matlab reservoir simulation toolbox appendix a soft sand model the soft sand model mavko et al 2009 allows computing the dry rock elastic moduli of a rock with known porosity based hertz mindlin contact theory and hashin shtrimkman elastic bounds first the bulk and shear moduli of the dry rock at the critical porosity value khm and ghm are computed using hertz mindlin equations a1 k h m p e n 1 ϕ 0 g m a t 2 18 π 1 ν 2 3 a2 g h m 5 4 ν 5 2 ν 3 p e n 1 ϕ 0 g m a t 2 2 π 1 ν 2 3 where φ0 is the critical porosity ν is the grain poisson s ratio pe is the effective pressure and n is average number of contacts per grain mindlin 1949 then for porosity values within the range 0 φ0 the bulk and shear moduli of the dry rock kdry and gdry are estimated by interpolating the matrix elastic moduli at zero porosity and the hertz mindlin elastic moduli at the critical porosity using the modified hashin shtrikman lower bounds hashin and shtrikman 1963 a3 k d r y ϕ ϕ 0 k h m 4 3 μ h m 1 ϕ ϕ 0 k m a t 4 3 μ h m 1 4 3 μ h m a4 μ d r y ϕ ϕ 0 μ h m 1 6 ξ μ h m 1 ϕ ϕ 0 μ m a t 1 6 ξ μ h m 1 1 6 ξ μ h m where a5 ξ 9 k h m 8 μ h m k h m 2 μ h m 
471,forecasting water level is an extremely important task as it allows to mitigate the effects of floods reduce and prevent disasters physically based models often give good results but they require expensive computational time and various types of hydro geomorphological data to develop the forecasting system alternatively data driven forecasting models are usually faster and easier to build during the past decades statistical machine learning ml methods have greatly contributed to the advancement of data driven forecasting systems that provide cost effective solutions and better performance meanwhile autoregressive integrated moving average arima is one of the famous linear statistical models for time series forecasting in this paper we propose a hybrid approach that takes advantages of linear and nonlinear models the proposed method combines statistical machine learning algorithms and arima for forecasting water level observed water level of the red river at the vu quang hanoi 3 hourly sampled from 2008 to 2017 and hung yen hydrological stations hourly collected data from 2008 to 4 2015 are used to evaluate the performance of different methods experimental results on these 3 real big datasets show the effectiveness of our proposed hybrid models keywords water level forecasting hybrid model statistical machine learning arima long term univariate time series 1 introduction accurate forecasting of water level in rivers and lakes is essential for warning floods and water resource management water level data obtained from hydrological stations usually have a time series structure hence researchers often use time series hydrological prediction models to forecast future data using past data to predict future water level future behavior hidden information can be disclosed which is of great significance for mitigating the effects of floods reducing or preventing disasters and managing water resource in general there are two lines of approaches for building the water level forecasting models that are process based numerical prediction and data driven the process based models hec ras dang and de smedt 2017 mike 11 patro et al 2009 kumar et al 2019 mike 21 zhu et al 2013 tran anh et al 2018 etc usually provide precise results and fully describe the nature of physical phenomena however they are computationally expensive requiring various types of hydro geomorphological data such as topography geology conduction roughness cross section fatichi et al 2016 to name but a few to develop forecasting systems they also demand sufficient expertise to interpret the results and require huge volumes of data which might be hard to satisfy in the case of water level and flow di et al 2014 fatichi et al 2016 zhong et al 2017a an alternative way for building forecasting models is to use data driven approaches riad et al 2004 wu et al 2009 ghumman et al 2011 peng et al 2017 gjika et al 2019 data driven forecasting models aim to reveal relationship between features or hidden information in the data using mostly only information from available data they are often faster and easier to build and therefore useful for real time rivers lakes flow forecasting with accurate water level prediction data driven models are mainly built from data using statistical and machine learning techniques hydrological data are usually affected by numerous external factors which make their time series contain both linear and nonlinear behaviors components autoregressive integrated moving average arima is one of the most popular and successful linear statistical models for time series forecasting ghimire 2017 birylo et al 2018 meanwhile statistical machine learning ml methods have been widely applied in building forecasting models for nonlinear time series wu et al 2009 peng et al 2017 yaseen et al 2018 gjika et al 2019 combining these two techniques for the sake of better time series forecasting is sensible and in fact there have been numerous studies in the literature on such hybrid approach giving an affirmative answer chen and wang 2007 khashei and bijari 2010 yan and zou 2013 wongsathan and jaroenwiriyapap 2016 pannakkong et al 2017 zhong et al 2017a peng et al 2017 mohan and reddy 2018 yaseen et al 2018 mousavi mirkalaei and banihabib 2019 temr et al 2019 xie and lou 2019 the success of these hybrid models in different fields of application has inspired us to utilize this approach for building water level forecasting models and test it on the case of the red river in particular we propose a data driven hybrid model building process with two components for modeling linear and non linear parts of water level time series using arima and non parametric statistical learning we hypothesize that by using two types of models for capturing linear and non linear parts in water level time series hidden patterns in the time series data could be better revealed compared to the single type of model approach we shall test and empirically prove our hypothesis by solving the water level forecasting problem for the red river with three real big datasets obtained from three hydrological stations the rest of this paper is organized as follows the next section overviews the related works in the literature sections 3 describes in details our proposed method data representation pre processing evaluation metrics and experiment results are presented and discussed in section 4 the last section section 5 concludes the paper and highlights some possible future works 2 related works arima is one of the most well known and effective linear statistical models for time series forecasting in birylo et al 2018 the authors utilized arima model to predict ground water level requiring three parameters precipitation surface runoff and evapotranspiration prediction results for twelve months showed that arima models obtained a good performance similarly ghimire 2017 developed arima model to forecast the river discharges in the us with significant success work in mirzavand and ghazavi 2015 showed the successful application of five time series models moving average ma auto regressive moving average arma arima and seasonal arima sarima and combination of several time series models to forecast groundwater level notably the experiment results indicated that combining time series models really improved the accuracy of ground water level forecasts valipour et al 2013 made a comparison of the predictive performance of arma arima and the autoregressive artificial neural networks ann in forecasting the monthly inflow of dez dam reservoir the results clearly showed that arima model is better than arma in the 12 past months but is not as good as autogressive ann for the past 60 months yu et al 2017 investigated arima model to predict daily water level of three stations in the middle reaches of the yangtze river it is found that the accuracy of the arima model decreases as the forecasting horizon increases ie it gives good results for the short term predictions but not for long term water level forecasts the authors also pointed out that the nonlinear and non stationary nature of the time series may lead to the uncertainty of using directly and only arima model therefore it is necessary to combine different types of models to improve the predictive performance in the recent years non parametric statistical machine learning ml methods have greatly contributed to the advancement of forecasting systems that provide cost effective solutions with satisfying performance using historical time series of water level data nguyen et al 2015 compared the performance of three statistical machine learning models lasso random forest rf and support vector regression svr for forecasting water level of the mekong river svr produced good results with mean absolute error as 0 486 m for 5 lead day acceptable error for a flood forecasting model is in 0 5m 0 75m garcia et al 2016 investigated rf algorithm to predict the water level on two different stations of cagayan river basin in philippines the correlations between predicted water level and ground truth data indicated the favorable predictive performance of the approach and suggested that it can be implemented for other stations on the major river basins across the philippines pasupa and jungjareantrat 2016 made prediction of water level on chao phyra river in thailand using a number of different statistical machine learning methods namely linear regression lr kernel regression kl svr k nearest neighbor knn and random forest rf the svr model with radial basis function kernel and 72 hour past time series data generated the best prediction results with the least error and better than the previous approach used by the royal thai navy in other works svr also demonstrated its ability to predict river flows garsole and rajurkar 2015 adnan et al 2018 bafitlhile and li 2019 yang et al 2017 conducted forecasts for water level time series on taiwans shimen reservoir using five statistical machine learning methods rbf network kstar knn rf and random tree the experimental results showed that rf improved forecasting performance over the other methods rf also achieved better results than other statistical machine learning methods such as svm artificial neural networks anns decision tree dt when predicting daily water levels wang et al 2018 choi et al 2019 hipni et al 2013 applied svm to forecast the daily dam water level of the klang gate in malaysia in comparison with adaptive neuro fuzzy inference system anfis the results demonstrated clearly that svm is better than anfis in the other study khan and coulibaly 2006 predicted long term lake water level using svm multilayer perceptron mlp and seasonal autoregressive model sar the best of these three approaches is svm anns have been widely applied as a statistical machine learning method in the field of hydrology such as in modeling water flow assessing water quality and forecasting water level toro et al 2013 kim and seo 2015 kasiviswanathan et al 2016 hamid et al 2019 recently deep and recurrent neural networks deep learning have made great success in numerous fields of applications catching attention from both academia and industry lecun et al 2015 one of such networks called long short term memory lstm has been successfully applied in solving problems in hydrology such as flood forecasting le et al 2019 and lake water level prediction hrnjica and bonacci 2019 xu et al 2019 hybrid models are capable of taking advantages of each component model in order to achieve improved modeling accuracy and flexibility zhang 2003 recently numerous hybrid models have been proposed to enhance the forecast performance of hydrological time series di et al 2014 seo et al 2015 zhong et al 2017a yaseen et al 2018 chen et al 2018 yaseen et al 2018 rezaie balf et al 2019 mousavi mirkalaei and banihabib 2019 nazir et al 2019 seo et al 2015 investigated two hybrid models namely wavelet based artificial neural network wann and wavelet based adaptive neuro fuzzy inference system wanfis wann was proven to be an effective tool for predicting water levels and achieving better results than other conventional forecasting models in zhong et al 2017b a hybrid ann kalman filtering was proposed to predict daily water level for maanshan station the forecasting results of the hybrid model were overall favorable comparing with ann in mousavi mirkalaei and banihabib 2019 the authors coupled arima and nonlinear auto regressive exogenous to forecast daily urban water consumption uwc for tehran metropolis the experiments clearly showed that the hybrid model which combined both linear and nonlinear models had higher accuracy in forecasting uwc xu et al 2019 developed an hybrid model combining arima and rnn recurrent neural network for water level prediction the experimental results indicated that the combined model can better capture the overall trend and amplitude fluctuation an other conjunction of arima and svr was proposed to anticipate daily average water level data of liuhe station the proposed algorithm showed a good anti jamming performance to data and good accuracy for water level forecasting motivated by the success of hybrid forecasting models we propose a hybrid approach that combines arima 1 1 in the section 3 6 1 we shall explain why arima is chosen ahead of other statistical models to capture the linear component of the time series in this paper with different statistical machine learning methods to capture the linear and non linear components of the time series separately the novel method is then tested on the real datasets of the red river for hourly water level forecasting 3 methods 3 1 autoregressive integrated moving average model arima arima introduced in box and jenkins 1976 is one of the most popular statistical linear models for forecasting univariate time series the main idea is that time series can be decomposed into present past values and random errors hence arima is a combination of auto regression ar p an additive linear function of p past observations moving average ma q q random errors and d which is an integer making a series to be stationary the arima p q d can be represented as 1 δ d y t c j 1 p α j y t j ϵ t j 1 q β j ϵ t j where δ 1 b b is the backward operator and b y t y t 1 y t is the observation data at time t c is the constant α 1 α p are the auto regressive parameters ϵ t is the white noise at time t and β 1 β q are the moving average coefficients fitting an arima model involves 4 steps as follows identification of the arima p d q structure estimation of parameters diagnostic checking on the estimated residuals forecasting future values based on the known data the autocorrelation function acf and the partial autocorrelation function pacf of the data are used to find out the order q and p of the arima model box and jenkins 1976 besides a number of other methods for selecting arima order have been proposed based on aic akaikes information criterion shibata 1976 mdl minimum description length ljung 1986 hurvich and tsai 1989 aic and bic aho et al 2014 or using fuzzy systems haseyama and kitajima 2001 the strength of arima is its ability in making a non stationary time series to stationary time series by differencing it d times stationary is essential as it makes the prediction practical and useful therefore before fitting an arima model we often need data transformation if the data contains trend component hetero scedasticity a time series is stationary when its mean variance and covariance at different lags are constant over time the α and β coefficients are estimated so that the overall measure of errors is minimal in the diagnostic checking step statistical assumptions about the errors of the model are examined by some diagnostic statistics and plots of the residuals 3 2 random forest rf since the 1990s ensemble learning models have been studied to improve the performance of classification and regression algorithms the purpose of the aggregation models is to reduce the variance and or bias error components of the learning algorithms bias is the conceptual error of learning model and variance is an error due to the variability of the model compared to the randomness of the data samples random forests rf proposed in breiman 2001 is one of the most successful ensemble learning methods it is an extension of bagging bootstrap aggregation to build dissimilar trees that develops the idea of random subspace sampling random forest includes multiple single trees each of which is built on a random sample of the training data the algorithm creates fully grown trees without pruning to keep the bias low each random forest tree is learned on a bootstrap sample set and at each node a random subset of attributes are considered for splitting the randomness makes diversity among the trees and allows to control the low correlation between trees in the forest so naturally they are more accurate and stable as more trees are added the random forrest algorithm fig 1 can be briefly described as follows from a training dataset learning set ls with m samples and n variables features construct independently t decision trees the tth decision tree model is built on the tth bootstrap sample set from ls at each inner node randomly choose n variables n n and calculate the best partition based on these n variables un pruned trees are built with the maximum depth then each tree provides a prediction value y and the final prediction value is obtained by aggregating the results given by t individual trees from the forest the rf prediction is 2 y 1 t i 1 t f i x where x rn is a new input t is the number of trees in the forest f i x is the prediction of unknown value y of input x generated from the i tree i 1 t when building a tree an additional tuning parameter is the number of candidate variables selected for node splitting at each iteration called mtry and 1 mtry n an objective of selecting mtry n is to reduce the computational time normally mtry is chosen as m t r y n for classification problems and m t r y n 3 for regression ones another parameter to consider is ntree trees usually tend to be unstable with high variance in the random forest algorithm number of trees ntree is used to reduce the variance 3 3 support vector regression svr support vector regression svr is the regression version of support vector machines svm a statistical machine learning algorithm based on statistical learning theory developed in vapnik 1995 the basic idea of svm is to find an optimal hyper plane for linearly separable patterns in a high dimensional space where features are mapped onto there are more than one hyper plane satisfying this criterion the task is to discover the one that maximizes the margin around the separating hyper plane fig 2 this is done with the helps of the support vectors which are the data points that lie closest to the decision surface and have direct bearing on the optimum location of the decision surface the linear regression estimating function can be illustrated as follows 3 f x w t x b where w is the weight vector b is the bias and x is the input vector svms can be extended for classification or regression problems that are not linearly separable by transforming original data into a new space using kernel functions the new space called feature space is usually high dimensional so that the classes become linearly separable 3 4 k nearest neighbors knn k nearest neighbors altman 1992 a non parametric statistical learning method has been widely used for both classification and regression problems this algorithm predicts values of any new data points using a similarity measure i e distance function this means that a new data sample is assigned a value based on how close it is to the points in the training set in detail for each sample of a test set we compute the distance e g euclidean manhattan or minkowski between that sample and all samples in the training set and then we choose the k closest training data samples the prediction value is the average of the target output values of these k nearest neighbors 3 5 long short term memory lstm long short term memory lstm networks introduced in hochreiter and schmidhuber 1997 are a special type of recurrent neural networks rnn that are capable of learning long distance dependencies they have been successfully applied to numerous sequence learning problem and have become the popular building block of numerous well known deep learning systems lstm can remember information from a long previous time and transmit it to the next cell they can determine which information is important to learn that is their intrinsic ability can be memorized without any explicit intervention the lstm network consists of many linked lstm memory cells and the architecture of each cell is shown in fig 3 the idea of lstm is that in addition to the hidden state h the cell internal state c and the three gates including forget gate ft input gate it and output gate ot are taken into account at each time step t a gate always gets the same input values xt and h t 1 obtained from the output of the previous memory cell at time t 1 each gate filters information for different purposes forget gate removing unnecessary information from the internal state cell input gate selecting what necessary information is added to the internal cell state output gate determining what information from the internal state cell is used as the output 3 6 the proposed method time series data usually consists of linear part as well as nonlinear part zhang 2003 in fact there is no universal model which can successfully achieve both linear and nonlinear relationships linear statistical models such as arima might not be good at modeling the nonlinear relationships in the time series but it is sufficient for modeling linear component ömer faruk 2010 valenzuela et al 2008 meanwhile non parametric statistical machine learning models such as svm rf knn or lstm can model any nonlinear components universal approximators therefore in the hope of obtaining better forecasting results we propose the hybrid models based on the idea of separate modeling of linear and nonlinear components of time series before presenting our approach in detail we give our arguments and justifications for the individual algorithms chosen to develop the novel hybrid approach 3 6 1 selection of component models in the literature there are a number of linear statistical models for time series modelling notably arima family arima arma seasonal arma sarma ar etc gegenbauer arma garma woodward et al 2017 arima family and garma models work under the assumption of stationary processes valipour et al 2013 zhang 2003 woodward et al 2017 which means that the mean of the series and the covariance among its observations do not change over time however arima model can generally fit the non stationary time series based on the arma model with a differencing process which effectively transforms the non stationary data into stationary one furthermore valipour et al 2013 pointed out that arima has a better performance than arma because of making time series stationary in both training and forecasting phases in addition in the training step of arima we always take into account the seasonal factor as the property of our data parma periodic arma models are devoted to model the time series whose mean variance and covariance function vary with seasons anderson et al 2013 they are usually used for data having cyclostationary property chaari et al 2015 thus parma could handle to some extent the non stationary of time series if and only if it coincides with seasons all of the datasets used in this paper have a seasonality component see table 2 and fig 5 that makes data are non stationary therefore arima parma and gamrma are the most relevant candidates for modeling the linear component of our time series data in which arima has been credited in the literature to be the most suitable of the three for handling non stationary time series to confirm the suitability of arima we have conducted the experiments to compare parma garma and arima on our time series of water level described in section 4 the results not shown in full here to save the space consistently confirmed the superiority of arima based on the performance indicators described in section 4 2 for instance table 1 depicts the average forecast errors of arima parma gamrma for 12h ahead on hanoi time series it is clear that arima was significantly better than the others on all performance indicators therefore in this paper arima is chosen to develop our proposed hybrid approach to capture the non linear component of time series rf svm knn and lstm methods are utilized we have selected these methods for a number of reasons first they are all non parametric statistical learning methods that have relatively firm theoretical foundations eg with theoretical guarantee on learnability learning consistency and universality second they are representatives of large and popular classes of statistical machine learning techniques lazy learning knn recurrent neural networks lstm kernel based methods svm and ensemble learning methods rf third they have been successfully applied in modeling learning hydrological time series nguyen et al 2015 yang et al 2017 wang et al 2018 choi et al 2019 hipni et al 2013 khan and coulibaly 2006 fernandez delgado et al 2014 3 6 2 details of the proposed method fig 4 describes all steps of our proposed method for forecasting univariate time series the objective of the proposed method is to take into account the advantages of the different forecasting models in terms of type i e linear and non linear and complexity i e single model and hybrid model a time series can be considered as a combination of two components linear and nonlinear ones zhang 2003 that is 4 y t l t n t where l t and n t are the linear and nonlinear components of the time series both components i e l t n t must be estimated from time series data the proposed method involves three stages i linear modeling ii linear nonlinear modeling and iii forecasting future values in the first stage we apply the arima model to extract the linear part of time series then in the second stage the residuals obtained from the arima and the lags values of the time series are used as the input for training statistical machine learning models finally in the third stage future values are estimated from different hybrid models the details of these steps are described in the following linear modelling arima is conducted on the whole time series to get the predicted values l t and their residuals the residuals of this step are used as a part of the input in the second stage hybrid models specifically in this study we utilize the order p obtained from the arima model to identify the optimal lag time for the inputs of the hybrid models this allows us to transform one dimensional time series into p dimension data so that statistical machine learning methods could be applied for forecasting univariate time series 2 2 these p dimensional data are also used for training single statistical machine learning methods to compute y in fig 4 for comparison with the hybrid approaches in section 4 it is noted that rather than heuristically chosen the order p is obtained in a data driven manner from the process of building arima model acf auto correlation function and pacf partial auto correlation function are performed to determine different time lags the calculation of p with arima is fast therefore we do not need extra work which could be unnecessarily expensive to tune p for each statistical machine learning model linear nonlinear modelling the residuals are important in selecting appropriate linear models because a model is not completely linear if it still contains nonlinear component in its residuals normally we cannot detect any nonlinear component when analyzing the residuals and in fact there is no statistical method that allows to determine nonlinear autogressive relationships mousavi mirkalaei and banihabib 2019 therefore we take into account the residuals by using statistical machine learning models in order to discover the nonlinear relations in the residuals the residual from arima model is calculated by 5 ϵ t y t l t where ϵ t is the residual and l t is the forecast value of arima model at time t in order to discover the nonlinear relationships of the time series ϵ t is could be fitted by statistical machine learning models such as knn svm rf and lstm in this study we build and test two types of hybrid models as follows model 1 6 y h y b r i d 1 t f y t 1 y t p ϵ t 1 ϵ t 2 where f a nonlinear function learnt from data by the machine learning models here we perform some pre experiments on our data to find an appropriate stop of ϵ with different values 1 2 3 p the results showed that when the past window is greater than 2 the predicted values are not better and it requires longer computational time therefore the past window of ϵ is empirically chosen as 2 in this paper model 2 at this stage we forecast the nonlinear data as 7 y ϵ t f ϵ t 1 ϵ t p where f a nonlinear function learnt from data by the statistical machine learning methods and ϵ is the residuals obtained from stage 1 therefore the final combined forecast will be 8 y h y b r i d 2 t y ϵ t y a r i m a t forecasting the developed models in the previous step are used to forecast water level in our study is for the red river 4 experimental results and discussions in this section we first describe the datasets and their pre processing stage in our experiments then define the performance evaluation metrics and finally discuss the experimental results with key findings and observations 4 1 data representation and pre processing in this study water level data from three hydrology stations of the red river the main river in the northern delta of vietnam namely vu quang hanoi and hung yen are used to test the proposed hybrid approach and compare with well known machine learning methods fig 5 presents the data of the three selected stations vu quang and hanoi datasets the water level of these two datasets were collected at the vu quang and hanoi hydrology stations in vietnam the samples were taken from 01 january 2008 to 31 december 2017 with different frequencies the reason is that on normal days without rainfall or flood the frequency of sampling is low for example with 4 6 7 or 8 times per day or no data available however on rainy days the frequency of sampling is much more such as with 10 11 12 18 19 20 21 22 or even 23 times a day therefore before applying time series forecasting algorithms to predict the water level it is necessary to re sample data at equal time intervals and pre process them for these time series we normalized the sampling frequency to 8 times per day at 1h 4h 10h 13h 16h 19h 22h consequently there are two cases the data sampling a day is less than 8 times or no data in this case we consider the periods of non sampling or days without data as missing data then we use imputation methods such as linear interpolation and or moving average to fill in the missing data for the large consecutive missing data i e a full day missing or more the result will be a straight line when applying linear interpolation so in order to capture the dynamism of the data we utilize dtwbi method phan et al 2017 that allows to complete consecutive missing data while taking into account the dynamism of data the data sampling a day is more than 8 times we base on the sampling times to recalculate the water level with the time aforementioned this means we perform re sampling 8 times a day at 1h 4h 7h 10h 13h 16h 19h and 22h hung yen dataset this dataset was collected at the hung yen hydrology station every hour from 01 01 2008 to 23 04 2015 fig 5 it also contains some missing data points and we use the interpolation method to complete those missing the characteristics of the three datasets are summarized in table 2 4 2 performance evaluation indicators after the prediction phase six evaluation metrics were used to assess different models they are sim mae rmse r score fsd and nse these metrics were selected as they possess different properties that are important to efficiently understand the performance of forecasting models from different angles they are defined as follows 1 similarity defines the percentage of similar values between the predicted values y and the actual values x it is calculated as 9 s i m y x 1 t i 1 t 1 1 y i x i max x min x where t is the number of forecasting values a higher similarity sim value 0 1 highlights a better ability of the method for the forecasting task 2 mae the mean absolute error between the predicted values y and the actual ones x is computed as 10 m a e y x 1 t i 1 t y i x i a lower mae value means better performance method for the prediction task 3 rmse the root mean square error is defined as the average squared difference between the forecast values y and the respective true values x this indicator is very useful for measuring overall precision or accuracy in general the most effective method would have the lowest rmse 11 r m s e y x 1 t i 1 t y i x i 2 4 r score is determined as the correlation coefficient between two variables y and x this indicator makes it possible to assess the quality of a forecasting model a method presents better performance when its r score is higher r 0 1 5 fsd the fraction of standard deviation is defined as 12 f s d y x 2 s d y s d x s d y s d x this fraction points out whether a method is acceptable or not for a forecasting method when fsd value approaches 0 it is impeccable 6 nse the nash sutcliffe efficiency is used to evaluate the predictive ability of hydrological models the nse values range from to 1 with higher values mean better fit between observed and forecast water level nash and sutcliffe 1970 13 n s e 1 i 1 t x i y i 2 i 1 t x i x i 2 4 3 results and discussions to evaluate and compare all tested methods the whole collected data were divided into two parts for training and testing with vu quang and hanoi time series the training datasets were those from 2008 to 2015 accounted for 80 of the data and the remaining observed samples 20 from 2016 to 2017 were used to assess the forecasting models with hung yen dataset the data from 2008 to 2013 were used for training models and the remaining data 2014 2015 were utilized to test the learnt models the problem of water level forecasting for the red river was tested with arima aforementioned statistical machine learning methods namely knn svr rf lstm and the proposed hybrid models in this paper we develop multi step ahead prediction models based on one step ahead prediction this means that to forecast a value y t at time t p previous values in the past y t 1 y t 2 y t p are used in the next prediction step y t is used as one of the input values to predict the next value y t 1 and so on 3 3 it is noted that by doing that way the prediction error could get accumulated overtime all single ml and hybrid models were trained one time on the training datasets and then the learnt models were applied to forecast at different time t on the testing sets however for arima in order to get better results all the whole past history values until time t 1 were used to build the model then this model was used to forecast t values from time t so for each forecast at a different time the retrain of arima model was needed the process for arima is shown in fig 6 it depicts how the training samples are utilized to make the model to implement the hybrid methods the first step is to capture the linear nonlinear components of the time series data and the order p in the next phase for the model 1 the original data and nonlinear parts are combined as in eq 6 to build new data used for developing hybrid forecasting methods for the model 2 nonlinear data are used directly eq 7 by the ml algorithms with this model the final predictive result is a combination of linear prediction generated by arima and non linear prediction generated by ml methods determining the parameters of these models plays an important role these parameters influence the forecasting accuracy significantly here when building a machine learning method we used 5 fold cross validation to find the optimal parameters of each model for arima model we apply auto arima from the forecast r package hyndman and khandakar 2008 to find the parameters p d and q we also take into account the seasonality component for all datasets when training arima models table 3 shows our parameter settings tables 4 5 and 6 give the details on the average forecast results of all tested methods at 20 different random times for the vu quang hanoi and hungyen datasets different forecast horizons were used to assess the forecasting performance of the proposed hybrid approaches and other methods for vu quang and hanoi time series the forecasting models were established for 12h 24h 48h 72h and 5 days ahead time intervals for hung yen data these models were applied to predict 6h 12h 24h 48h 72h and 5 days ahead the best results for each forecasting horizon are highlighted in bold it can be seen that the hybrid methods using model 1 eq 6 such as arima rf and arima knn most often produced better results than individual models i e arima rf knn svm or even lstm and other combined models arima svm arima lstm as well as hybrid methods using model 2 eq 7 table 6 from table 4 it can be seen that arima rf achieved the lowest mae rmse fsd and nse and highest sim measures on most predicted periods the second rank is arima knn for these indicators it also produced the best results on vu quang dataset for 48h forecasting of water level this indicates arima rf and arima knn based on eq 6 are more accurate than other methods significantly they could fully exploit separation of linear and nonlinear components looking at results of arima svm svm arima lstm and lstm methods we see that arima svm and arima lstm do not improve the performance over svm and lstm respectively that is when combining the nonlinear part with the original data neither svm nor lstm could exploit and capture well the nonlinear component of the time series table 5 presents the results of the different forecasting methods on the hung yen dataset in this table arima rf and arima knn methods do not yield good results at all forecast horizons as in the table 4 but the both methods still have performance improvements over single machine learning models rf knn once again arima svm and arima lstm methods failed to capture the non linear part of the time series data these methods yield results even worse than single machine learning models svm and lstm table 6 gives the average forecasting results using the proposed algorithms model 2 based on eq 7 on the vu quang dataset the results indicate that the hybrid approaches based on eq 7 model 2 are no more effective than arima overall results of the hybrid methods model 1 on this dataset table 4 clearly shows that the methods of the first type of hybrid models model 1 using eq 6 outperforms significantly the methods of the second type of the hybrid approach model 2 figs 7 8 and 9 plot the results of time series forecasts from different algorithms on hung yen and hanoi datasets the observed water level values and predicted ones generated from each model are showed to compare their performances fig 7 compares the predicted water level of all models for 3 days ahead in this figure we could see that arima rf yields the best results for the first day its forecast values are nearly identical to the actual ones however when predicting for the second and third days the results are no longer as good as for the first day it still captures rather well the trend of real data but the forecast errors are higher on the contrary arima knn forecasts poorly for the first day but relatively well for the second and third days rf knn lstm arima lstm produce good predicted values initially but then quickly fail to capture the trend of the real data it can be seen from figs 7 and 8 that in general arima is not suitable for forecasting water level on red river although initially it produces good results fig 7 fig 8 demonstrates again the superiority of arima rf and arima knn for forecasting water level on hanoi dataset for the first 5 days prediction values of these two methods are closer to observed water levels than arima rf and knn methods for the last 5 days only arima rf is able to catch approximately the trend of real data but the forecast error is large by observing the experimental results table 4 fig 9 we have found that lstm and arima lstm produced worse prediction performance for the next 10 days apart from normal forecasts it is interesting and important to see if the hybrid approach is effective in critical situations such as forecasting at a few days before the water level goes significantly high suggesting a peak in a flood fig 10 11 and 12 present the visual comparisons of all methods for forecasting several periods of water level at hanoi vu quang and hung yen stations fig 10 shows the results of forecasting the water level before the peak july 24 2017 at 4h at hanoi station with 1 day fig 10a 2 days fig 10b 3 days fig 10c and 5 days fig 10d ahead again it is evidenced from the figures that arima rf yields the best results 1 day ahead and 2 days ahead forecast values are close to the real ones when forecasting 3 days and 5 days in advance the arima rf could capture quite well the trend of the actual data but the forecasting errors are rather high other methods do not work well for all these cases fig 11 demonstrates the results of forecasting the water level before the peak august 21 2016 at vu quang station with 1 day fig 11a 2 days fig 11b 3 days fig 11c and 5 days fig 11d ahead the figure shows a similar results to the case of hanoi station in that arima rf could forecast quite accurately the peak with 1 and 2 days ahead it still captures rather well the trend of the data in the 3 and 5 day ahead forecasts but the errors are high meanwhile all of other models failed to forecast the peak and could not capture the data trend for hung yen data with hourly sampling the highest water level was on july 23 2014 at 22h 368 cm we forecast the water levels before the peak 6h fig 12a 12h fig 12b 24h days fig 12c and 3 days fig 12d fig 12 demonstrates once again that the hybrid approach is superior to the single component methods when predicting water levels 6h ahead arima knn produces the closest values to the real ones fig 12a however when forecasting water levels with 12h and 24h in advance arima knn no longer gives good results as in the case of 6h it is always that arima rf produces predicted values which are more similar to the true ones than other methods fig 12b 12 c although arima knn is second behind arima rf the gaps between the forecast errors of the two methods are rather wide for hungyen dataset when predicting 3 day ahead of the peak only arima rf could capture the trend of data but the forecast error is relatively large 5 conclusion forecasting accurately time series especially water level for flood warning systems is a very important but challenging task arima knn rf svm and lstm are widely popular and effective forecasting models proposed and tested on hydrological time series in the literature arima can model well linear whereas the other statistical machine learning models are most suitable for nonlinear time series however in reality a hydrological time series often includes both linear and nonlinear correlation structures therefore in this paper we have proposed two types of hybrid approaches in order to improve the forecasting performance they have taken the advantages of each individual model type i e linear and non linear and complexity levels i e single or hybrid in time series forecasting the first type of hybrid approach model 1 is to combine the original data and the residuals obtained from arima to build the predictive models namely arima knn arima rf arima svr and arima lstm the second hybrid type is to perform predictions on the original data and arima s residuals then to aggregate the forecast results model 2 these models have been tested on three real big datasets on the water level time series of the red river and compared with each single component model the experimental results showed that the combined methods arima rf and arima knn of model 1 yield superior and more reliable results than other hybrid models e g arima svm and arima lstm as well as better than the traditional single component methods arima knn rf svm and lstm in future we are planning to apply our novel methods arima rf and arima knn for forecasting water level on other horological stations of the red river as well as for other rivers credit authorship contribution statement thi thu hong phan conceptualization methodology formal analysis writing original draft writing review editing software xuan hoai nguyen conceptualization methodology formal analysis writing original draft writing review editing declaration of competing interest all authors have participated in a conception and design or analysis and interpretation of the data b drafting the article or revising it critically for important intellectual content and c approval of the final version this manuscript has not been submitted to nor is under review at another journal or other publishing venue the authors have no affiliation with any organization with a direct or indirect financial interest in the subject matter discussed in the manuscript acknowledgments the research presented in this paper was funded by national science and technology program to respond to climate change manage natural resources and the environment in the period 2016 2020 in the project titled applications of artificial intelligence for forecasting hydro meteorological anomalies in vietnam under the context of climate change grant number bđkh 34 16 20 supplementary material supplementary material associated with this article can be found in the online version at doi 10 1016 j advwatres 2020 103656 appendix a supplementary materials supplementary data s1 supplementary raw research data this is open data under the cc by license http creativecommons org licenses by 4 0 supplementary data s1 
471,forecasting water level is an extremely important task as it allows to mitigate the effects of floods reduce and prevent disasters physically based models often give good results but they require expensive computational time and various types of hydro geomorphological data to develop the forecasting system alternatively data driven forecasting models are usually faster and easier to build during the past decades statistical machine learning ml methods have greatly contributed to the advancement of data driven forecasting systems that provide cost effective solutions and better performance meanwhile autoregressive integrated moving average arima is one of the famous linear statistical models for time series forecasting in this paper we propose a hybrid approach that takes advantages of linear and nonlinear models the proposed method combines statistical machine learning algorithms and arima for forecasting water level observed water level of the red river at the vu quang hanoi 3 hourly sampled from 2008 to 2017 and hung yen hydrological stations hourly collected data from 2008 to 4 2015 are used to evaluate the performance of different methods experimental results on these 3 real big datasets show the effectiveness of our proposed hybrid models keywords water level forecasting hybrid model statistical machine learning arima long term univariate time series 1 introduction accurate forecasting of water level in rivers and lakes is essential for warning floods and water resource management water level data obtained from hydrological stations usually have a time series structure hence researchers often use time series hydrological prediction models to forecast future data using past data to predict future water level future behavior hidden information can be disclosed which is of great significance for mitigating the effects of floods reducing or preventing disasters and managing water resource in general there are two lines of approaches for building the water level forecasting models that are process based numerical prediction and data driven the process based models hec ras dang and de smedt 2017 mike 11 patro et al 2009 kumar et al 2019 mike 21 zhu et al 2013 tran anh et al 2018 etc usually provide precise results and fully describe the nature of physical phenomena however they are computationally expensive requiring various types of hydro geomorphological data such as topography geology conduction roughness cross section fatichi et al 2016 to name but a few to develop forecasting systems they also demand sufficient expertise to interpret the results and require huge volumes of data which might be hard to satisfy in the case of water level and flow di et al 2014 fatichi et al 2016 zhong et al 2017a an alternative way for building forecasting models is to use data driven approaches riad et al 2004 wu et al 2009 ghumman et al 2011 peng et al 2017 gjika et al 2019 data driven forecasting models aim to reveal relationship between features or hidden information in the data using mostly only information from available data they are often faster and easier to build and therefore useful for real time rivers lakes flow forecasting with accurate water level prediction data driven models are mainly built from data using statistical and machine learning techniques hydrological data are usually affected by numerous external factors which make their time series contain both linear and nonlinear behaviors components autoregressive integrated moving average arima is one of the most popular and successful linear statistical models for time series forecasting ghimire 2017 birylo et al 2018 meanwhile statistical machine learning ml methods have been widely applied in building forecasting models for nonlinear time series wu et al 2009 peng et al 2017 yaseen et al 2018 gjika et al 2019 combining these two techniques for the sake of better time series forecasting is sensible and in fact there have been numerous studies in the literature on such hybrid approach giving an affirmative answer chen and wang 2007 khashei and bijari 2010 yan and zou 2013 wongsathan and jaroenwiriyapap 2016 pannakkong et al 2017 zhong et al 2017a peng et al 2017 mohan and reddy 2018 yaseen et al 2018 mousavi mirkalaei and banihabib 2019 temr et al 2019 xie and lou 2019 the success of these hybrid models in different fields of application has inspired us to utilize this approach for building water level forecasting models and test it on the case of the red river in particular we propose a data driven hybrid model building process with two components for modeling linear and non linear parts of water level time series using arima and non parametric statistical learning we hypothesize that by using two types of models for capturing linear and non linear parts in water level time series hidden patterns in the time series data could be better revealed compared to the single type of model approach we shall test and empirically prove our hypothesis by solving the water level forecasting problem for the red river with three real big datasets obtained from three hydrological stations the rest of this paper is organized as follows the next section overviews the related works in the literature sections 3 describes in details our proposed method data representation pre processing evaluation metrics and experiment results are presented and discussed in section 4 the last section section 5 concludes the paper and highlights some possible future works 2 related works arima is one of the most well known and effective linear statistical models for time series forecasting in birylo et al 2018 the authors utilized arima model to predict ground water level requiring three parameters precipitation surface runoff and evapotranspiration prediction results for twelve months showed that arima models obtained a good performance similarly ghimire 2017 developed arima model to forecast the river discharges in the us with significant success work in mirzavand and ghazavi 2015 showed the successful application of five time series models moving average ma auto regressive moving average arma arima and seasonal arima sarima and combination of several time series models to forecast groundwater level notably the experiment results indicated that combining time series models really improved the accuracy of ground water level forecasts valipour et al 2013 made a comparison of the predictive performance of arma arima and the autoregressive artificial neural networks ann in forecasting the monthly inflow of dez dam reservoir the results clearly showed that arima model is better than arma in the 12 past months but is not as good as autogressive ann for the past 60 months yu et al 2017 investigated arima model to predict daily water level of three stations in the middle reaches of the yangtze river it is found that the accuracy of the arima model decreases as the forecasting horizon increases ie it gives good results for the short term predictions but not for long term water level forecasts the authors also pointed out that the nonlinear and non stationary nature of the time series may lead to the uncertainty of using directly and only arima model therefore it is necessary to combine different types of models to improve the predictive performance in the recent years non parametric statistical machine learning ml methods have greatly contributed to the advancement of forecasting systems that provide cost effective solutions with satisfying performance using historical time series of water level data nguyen et al 2015 compared the performance of three statistical machine learning models lasso random forest rf and support vector regression svr for forecasting water level of the mekong river svr produced good results with mean absolute error as 0 486 m for 5 lead day acceptable error for a flood forecasting model is in 0 5m 0 75m garcia et al 2016 investigated rf algorithm to predict the water level on two different stations of cagayan river basin in philippines the correlations between predicted water level and ground truth data indicated the favorable predictive performance of the approach and suggested that it can be implemented for other stations on the major river basins across the philippines pasupa and jungjareantrat 2016 made prediction of water level on chao phyra river in thailand using a number of different statistical machine learning methods namely linear regression lr kernel regression kl svr k nearest neighbor knn and random forest rf the svr model with radial basis function kernel and 72 hour past time series data generated the best prediction results with the least error and better than the previous approach used by the royal thai navy in other works svr also demonstrated its ability to predict river flows garsole and rajurkar 2015 adnan et al 2018 bafitlhile and li 2019 yang et al 2017 conducted forecasts for water level time series on taiwans shimen reservoir using five statistical machine learning methods rbf network kstar knn rf and random tree the experimental results showed that rf improved forecasting performance over the other methods rf also achieved better results than other statistical machine learning methods such as svm artificial neural networks anns decision tree dt when predicting daily water levels wang et al 2018 choi et al 2019 hipni et al 2013 applied svm to forecast the daily dam water level of the klang gate in malaysia in comparison with adaptive neuro fuzzy inference system anfis the results demonstrated clearly that svm is better than anfis in the other study khan and coulibaly 2006 predicted long term lake water level using svm multilayer perceptron mlp and seasonal autoregressive model sar the best of these three approaches is svm anns have been widely applied as a statistical machine learning method in the field of hydrology such as in modeling water flow assessing water quality and forecasting water level toro et al 2013 kim and seo 2015 kasiviswanathan et al 2016 hamid et al 2019 recently deep and recurrent neural networks deep learning have made great success in numerous fields of applications catching attention from both academia and industry lecun et al 2015 one of such networks called long short term memory lstm has been successfully applied in solving problems in hydrology such as flood forecasting le et al 2019 and lake water level prediction hrnjica and bonacci 2019 xu et al 2019 hybrid models are capable of taking advantages of each component model in order to achieve improved modeling accuracy and flexibility zhang 2003 recently numerous hybrid models have been proposed to enhance the forecast performance of hydrological time series di et al 2014 seo et al 2015 zhong et al 2017a yaseen et al 2018 chen et al 2018 yaseen et al 2018 rezaie balf et al 2019 mousavi mirkalaei and banihabib 2019 nazir et al 2019 seo et al 2015 investigated two hybrid models namely wavelet based artificial neural network wann and wavelet based adaptive neuro fuzzy inference system wanfis wann was proven to be an effective tool for predicting water levels and achieving better results than other conventional forecasting models in zhong et al 2017b a hybrid ann kalman filtering was proposed to predict daily water level for maanshan station the forecasting results of the hybrid model were overall favorable comparing with ann in mousavi mirkalaei and banihabib 2019 the authors coupled arima and nonlinear auto regressive exogenous to forecast daily urban water consumption uwc for tehran metropolis the experiments clearly showed that the hybrid model which combined both linear and nonlinear models had higher accuracy in forecasting uwc xu et al 2019 developed an hybrid model combining arima and rnn recurrent neural network for water level prediction the experimental results indicated that the combined model can better capture the overall trend and amplitude fluctuation an other conjunction of arima and svr was proposed to anticipate daily average water level data of liuhe station the proposed algorithm showed a good anti jamming performance to data and good accuracy for water level forecasting motivated by the success of hybrid forecasting models we propose a hybrid approach that combines arima 1 1 in the section 3 6 1 we shall explain why arima is chosen ahead of other statistical models to capture the linear component of the time series in this paper with different statistical machine learning methods to capture the linear and non linear components of the time series separately the novel method is then tested on the real datasets of the red river for hourly water level forecasting 3 methods 3 1 autoregressive integrated moving average model arima arima introduced in box and jenkins 1976 is one of the most popular statistical linear models for forecasting univariate time series the main idea is that time series can be decomposed into present past values and random errors hence arima is a combination of auto regression ar p an additive linear function of p past observations moving average ma q q random errors and d which is an integer making a series to be stationary the arima p q d can be represented as 1 δ d y t c j 1 p α j y t j ϵ t j 1 q β j ϵ t j where δ 1 b b is the backward operator and b y t y t 1 y t is the observation data at time t c is the constant α 1 α p are the auto regressive parameters ϵ t is the white noise at time t and β 1 β q are the moving average coefficients fitting an arima model involves 4 steps as follows identification of the arima p d q structure estimation of parameters diagnostic checking on the estimated residuals forecasting future values based on the known data the autocorrelation function acf and the partial autocorrelation function pacf of the data are used to find out the order q and p of the arima model box and jenkins 1976 besides a number of other methods for selecting arima order have been proposed based on aic akaikes information criterion shibata 1976 mdl minimum description length ljung 1986 hurvich and tsai 1989 aic and bic aho et al 2014 or using fuzzy systems haseyama and kitajima 2001 the strength of arima is its ability in making a non stationary time series to stationary time series by differencing it d times stationary is essential as it makes the prediction practical and useful therefore before fitting an arima model we often need data transformation if the data contains trend component hetero scedasticity a time series is stationary when its mean variance and covariance at different lags are constant over time the α and β coefficients are estimated so that the overall measure of errors is minimal in the diagnostic checking step statistical assumptions about the errors of the model are examined by some diagnostic statistics and plots of the residuals 3 2 random forest rf since the 1990s ensemble learning models have been studied to improve the performance of classification and regression algorithms the purpose of the aggregation models is to reduce the variance and or bias error components of the learning algorithms bias is the conceptual error of learning model and variance is an error due to the variability of the model compared to the randomness of the data samples random forests rf proposed in breiman 2001 is one of the most successful ensemble learning methods it is an extension of bagging bootstrap aggregation to build dissimilar trees that develops the idea of random subspace sampling random forest includes multiple single trees each of which is built on a random sample of the training data the algorithm creates fully grown trees without pruning to keep the bias low each random forest tree is learned on a bootstrap sample set and at each node a random subset of attributes are considered for splitting the randomness makes diversity among the trees and allows to control the low correlation between trees in the forest so naturally they are more accurate and stable as more trees are added the random forrest algorithm fig 1 can be briefly described as follows from a training dataset learning set ls with m samples and n variables features construct independently t decision trees the tth decision tree model is built on the tth bootstrap sample set from ls at each inner node randomly choose n variables n n and calculate the best partition based on these n variables un pruned trees are built with the maximum depth then each tree provides a prediction value y and the final prediction value is obtained by aggregating the results given by t individual trees from the forest the rf prediction is 2 y 1 t i 1 t f i x where x rn is a new input t is the number of trees in the forest f i x is the prediction of unknown value y of input x generated from the i tree i 1 t when building a tree an additional tuning parameter is the number of candidate variables selected for node splitting at each iteration called mtry and 1 mtry n an objective of selecting mtry n is to reduce the computational time normally mtry is chosen as m t r y n for classification problems and m t r y n 3 for regression ones another parameter to consider is ntree trees usually tend to be unstable with high variance in the random forest algorithm number of trees ntree is used to reduce the variance 3 3 support vector regression svr support vector regression svr is the regression version of support vector machines svm a statistical machine learning algorithm based on statistical learning theory developed in vapnik 1995 the basic idea of svm is to find an optimal hyper plane for linearly separable patterns in a high dimensional space where features are mapped onto there are more than one hyper plane satisfying this criterion the task is to discover the one that maximizes the margin around the separating hyper plane fig 2 this is done with the helps of the support vectors which are the data points that lie closest to the decision surface and have direct bearing on the optimum location of the decision surface the linear regression estimating function can be illustrated as follows 3 f x w t x b where w is the weight vector b is the bias and x is the input vector svms can be extended for classification or regression problems that are not linearly separable by transforming original data into a new space using kernel functions the new space called feature space is usually high dimensional so that the classes become linearly separable 3 4 k nearest neighbors knn k nearest neighbors altman 1992 a non parametric statistical learning method has been widely used for both classification and regression problems this algorithm predicts values of any new data points using a similarity measure i e distance function this means that a new data sample is assigned a value based on how close it is to the points in the training set in detail for each sample of a test set we compute the distance e g euclidean manhattan or minkowski between that sample and all samples in the training set and then we choose the k closest training data samples the prediction value is the average of the target output values of these k nearest neighbors 3 5 long short term memory lstm long short term memory lstm networks introduced in hochreiter and schmidhuber 1997 are a special type of recurrent neural networks rnn that are capable of learning long distance dependencies they have been successfully applied to numerous sequence learning problem and have become the popular building block of numerous well known deep learning systems lstm can remember information from a long previous time and transmit it to the next cell they can determine which information is important to learn that is their intrinsic ability can be memorized without any explicit intervention the lstm network consists of many linked lstm memory cells and the architecture of each cell is shown in fig 3 the idea of lstm is that in addition to the hidden state h the cell internal state c and the three gates including forget gate ft input gate it and output gate ot are taken into account at each time step t a gate always gets the same input values xt and h t 1 obtained from the output of the previous memory cell at time t 1 each gate filters information for different purposes forget gate removing unnecessary information from the internal state cell input gate selecting what necessary information is added to the internal cell state output gate determining what information from the internal state cell is used as the output 3 6 the proposed method time series data usually consists of linear part as well as nonlinear part zhang 2003 in fact there is no universal model which can successfully achieve both linear and nonlinear relationships linear statistical models such as arima might not be good at modeling the nonlinear relationships in the time series but it is sufficient for modeling linear component ömer faruk 2010 valenzuela et al 2008 meanwhile non parametric statistical machine learning models such as svm rf knn or lstm can model any nonlinear components universal approximators therefore in the hope of obtaining better forecasting results we propose the hybrid models based on the idea of separate modeling of linear and nonlinear components of time series before presenting our approach in detail we give our arguments and justifications for the individual algorithms chosen to develop the novel hybrid approach 3 6 1 selection of component models in the literature there are a number of linear statistical models for time series modelling notably arima family arima arma seasonal arma sarma ar etc gegenbauer arma garma woodward et al 2017 arima family and garma models work under the assumption of stationary processes valipour et al 2013 zhang 2003 woodward et al 2017 which means that the mean of the series and the covariance among its observations do not change over time however arima model can generally fit the non stationary time series based on the arma model with a differencing process which effectively transforms the non stationary data into stationary one furthermore valipour et al 2013 pointed out that arima has a better performance than arma because of making time series stationary in both training and forecasting phases in addition in the training step of arima we always take into account the seasonal factor as the property of our data parma periodic arma models are devoted to model the time series whose mean variance and covariance function vary with seasons anderson et al 2013 they are usually used for data having cyclostationary property chaari et al 2015 thus parma could handle to some extent the non stationary of time series if and only if it coincides with seasons all of the datasets used in this paper have a seasonality component see table 2 and fig 5 that makes data are non stationary therefore arima parma and gamrma are the most relevant candidates for modeling the linear component of our time series data in which arima has been credited in the literature to be the most suitable of the three for handling non stationary time series to confirm the suitability of arima we have conducted the experiments to compare parma garma and arima on our time series of water level described in section 4 the results not shown in full here to save the space consistently confirmed the superiority of arima based on the performance indicators described in section 4 2 for instance table 1 depicts the average forecast errors of arima parma gamrma for 12h ahead on hanoi time series it is clear that arima was significantly better than the others on all performance indicators therefore in this paper arima is chosen to develop our proposed hybrid approach to capture the non linear component of time series rf svm knn and lstm methods are utilized we have selected these methods for a number of reasons first they are all non parametric statistical learning methods that have relatively firm theoretical foundations eg with theoretical guarantee on learnability learning consistency and universality second they are representatives of large and popular classes of statistical machine learning techniques lazy learning knn recurrent neural networks lstm kernel based methods svm and ensemble learning methods rf third they have been successfully applied in modeling learning hydrological time series nguyen et al 2015 yang et al 2017 wang et al 2018 choi et al 2019 hipni et al 2013 khan and coulibaly 2006 fernandez delgado et al 2014 3 6 2 details of the proposed method fig 4 describes all steps of our proposed method for forecasting univariate time series the objective of the proposed method is to take into account the advantages of the different forecasting models in terms of type i e linear and non linear and complexity i e single model and hybrid model a time series can be considered as a combination of two components linear and nonlinear ones zhang 2003 that is 4 y t l t n t where l t and n t are the linear and nonlinear components of the time series both components i e l t n t must be estimated from time series data the proposed method involves three stages i linear modeling ii linear nonlinear modeling and iii forecasting future values in the first stage we apply the arima model to extract the linear part of time series then in the second stage the residuals obtained from the arima and the lags values of the time series are used as the input for training statistical machine learning models finally in the third stage future values are estimated from different hybrid models the details of these steps are described in the following linear modelling arima is conducted on the whole time series to get the predicted values l t and their residuals the residuals of this step are used as a part of the input in the second stage hybrid models specifically in this study we utilize the order p obtained from the arima model to identify the optimal lag time for the inputs of the hybrid models this allows us to transform one dimensional time series into p dimension data so that statistical machine learning methods could be applied for forecasting univariate time series 2 2 these p dimensional data are also used for training single statistical machine learning methods to compute y in fig 4 for comparison with the hybrid approaches in section 4 it is noted that rather than heuristically chosen the order p is obtained in a data driven manner from the process of building arima model acf auto correlation function and pacf partial auto correlation function are performed to determine different time lags the calculation of p with arima is fast therefore we do not need extra work which could be unnecessarily expensive to tune p for each statistical machine learning model linear nonlinear modelling the residuals are important in selecting appropriate linear models because a model is not completely linear if it still contains nonlinear component in its residuals normally we cannot detect any nonlinear component when analyzing the residuals and in fact there is no statistical method that allows to determine nonlinear autogressive relationships mousavi mirkalaei and banihabib 2019 therefore we take into account the residuals by using statistical machine learning models in order to discover the nonlinear relations in the residuals the residual from arima model is calculated by 5 ϵ t y t l t where ϵ t is the residual and l t is the forecast value of arima model at time t in order to discover the nonlinear relationships of the time series ϵ t is could be fitted by statistical machine learning models such as knn svm rf and lstm in this study we build and test two types of hybrid models as follows model 1 6 y h y b r i d 1 t f y t 1 y t p ϵ t 1 ϵ t 2 where f a nonlinear function learnt from data by the machine learning models here we perform some pre experiments on our data to find an appropriate stop of ϵ with different values 1 2 3 p the results showed that when the past window is greater than 2 the predicted values are not better and it requires longer computational time therefore the past window of ϵ is empirically chosen as 2 in this paper model 2 at this stage we forecast the nonlinear data as 7 y ϵ t f ϵ t 1 ϵ t p where f a nonlinear function learnt from data by the statistical machine learning methods and ϵ is the residuals obtained from stage 1 therefore the final combined forecast will be 8 y h y b r i d 2 t y ϵ t y a r i m a t forecasting the developed models in the previous step are used to forecast water level in our study is for the red river 4 experimental results and discussions in this section we first describe the datasets and their pre processing stage in our experiments then define the performance evaluation metrics and finally discuss the experimental results with key findings and observations 4 1 data representation and pre processing in this study water level data from three hydrology stations of the red river the main river in the northern delta of vietnam namely vu quang hanoi and hung yen are used to test the proposed hybrid approach and compare with well known machine learning methods fig 5 presents the data of the three selected stations vu quang and hanoi datasets the water level of these two datasets were collected at the vu quang and hanoi hydrology stations in vietnam the samples were taken from 01 january 2008 to 31 december 2017 with different frequencies the reason is that on normal days without rainfall or flood the frequency of sampling is low for example with 4 6 7 or 8 times per day or no data available however on rainy days the frequency of sampling is much more such as with 10 11 12 18 19 20 21 22 or even 23 times a day therefore before applying time series forecasting algorithms to predict the water level it is necessary to re sample data at equal time intervals and pre process them for these time series we normalized the sampling frequency to 8 times per day at 1h 4h 10h 13h 16h 19h 22h consequently there are two cases the data sampling a day is less than 8 times or no data in this case we consider the periods of non sampling or days without data as missing data then we use imputation methods such as linear interpolation and or moving average to fill in the missing data for the large consecutive missing data i e a full day missing or more the result will be a straight line when applying linear interpolation so in order to capture the dynamism of the data we utilize dtwbi method phan et al 2017 that allows to complete consecutive missing data while taking into account the dynamism of data the data sampling a day is more than 8 times we base on the sampling times to recalculate the water level with the time aforementioned this means we perform re sampling 8 times a day at 1h 4h 7h 10h 13h 16h 19h and 22h hung yen dataset this dataset was collected at the hung yen hydrology station every hour from 01 01 2008 to 23 04 2015 fig 5 it also contains some missing data points and we use the interpolation method to complete those missing the characteristics of the three datasets are summarized in table 2 4 2 performance evaluation indicators after the prediction phase six evaluation metrics were used to assess different models they are sim mae rmse r score fsd and nse these metrics were selected as they possess different properties that are important to efficiently understand the performance of forecasting models from different angles they are defined as follows 1 similarity defines the percentage of similar values between the predicted values y and the actual values x it is calculated as 9 s i m y x 1 t i 1 t 1 1 y i x i max x min x where t is the number of forecasting values a higher similarity sim value 0 1 highlights a better ability of the method for the forecasting task 2 mae the mean absolute error between the predicted values y and the actual ones x is computed as 10 m a e y x 1 t i 1 t y i x i a lower mae value means better performance method for the prediction task 3 rmse the root mean square error is defined as the average squared difference between the forecast values y and the respective true values x this indicator is very useful for measuring overall precision or accuracy in general the most effective method would have the lowest rmse 11 r m s e y x 1 t i 1 t y i x i 2 4 r score is determined as the correlation coefficient between two variables y and x this indicator makes it possible to assess the quality of a forecasting model a method presents better performance when its r score is higher r 0 1 5 fsd the fraction of standard deviation is defined as 12 f s d y x 2 s d y s d x s d y s d x this fraction points out whether a method is acceptable or not for a forecasting method when fsd value approaches 0 it is impeccable 6 nse the nash sutcliffe efficiency is used to evaluate the predictive ability of hydrological models the nse values range from to 1 with higher values mean better fit between observed and forecast water level nash and sutcliffe 1970 13 n s e 1 i 1 t x i y i 2 i 1 t x i x i 2 4 3 results and discussions to evaluate and compare all tested methods the whole collected data were divided into two parts for training and testing with vu quang and hanoi time series the training datasets were those from 2008 to 2015 accounted for 80 of the data and the remaining observed samples 20 from 2016 to 2017 were used to assess the forecasting models with hung yen dataset the data from 2008 to 2013 were used for training models and the remaining data 2014 2015 were utilized to test the learnt models the problem of water level forecasting for the red river was tested with arima aforementioned statistical machine learning methods namely knn svr rf lstm and the proposed hybrid models in this paper we develop multi step ahead prediction models based on one step ahead prediction this means that to forecast a value y t at time t p previous values in the past y t 1 y t 2 y t p are used in the next prediction step y t is used as one of the input values to predict the next value y t 1 and so on 3 3 it is noted that by doing that way the prediction error could get accumulated overtime all single ml and hybrid models were trained one time on the training datasets and then the learnt models were applied to forecast at different time t on the testing sets however for arima in order to get better results all the whole past history values until time t 1 were used to build the model then this model was used to forecast t values from time t so for each forecast at a different time the retrain of arima model was needed the process for arima is shown in fig 6 it depicts how the training samples are utilized to make the model to implement the hybrid methods the first step is to capture the linear nonlinear components of the time series data and the order p in the next phase for the model 1 the original data and nonlinear parts are combined as in eq 6 to build new data used for developing hybrid forecasting methods for the model 2 nonlinear data are used directly eq 7 by the ml algorithms with this model the final predictive result is a combination of linear prediction generated by arima and non linear prediction generated by ml methods determining the parameters of these models plays an important role these parameters influence the forecasting accuracy significantly here when building a machine learning method we used 5 fold cross validation to find the optimal parameters of each model for arima model we apply auto arima from the forecast r package hyndman and khandakar 2008 to find the parameters p d and q we also take into account the seasonality component for all datasets when training arima models table 3 shows our parameter settings tables 4 5 and 6 give the details on the average forecast results of all tested methods at 20 different random times for the vu quang hanoi and hungyen datasets different forecast horizons were used to assess the forecasting performance of the proposed hybrid approaches and other methods for vu quang and hanoi time series the forecasting models were established for 12h 24h 48h 72h and 5 days ahead time intervals for hung yen data these models were applied to predict 6h 12h 24h 48h 72h and 5 days ahead the best results for each forecasting horizon are highlighted in bold it can be seen that the hybrid methods using model 1 eq 6 such as arima rf and arima knn most often produced better results than individual models i e arima rf knn svm or even lstm and other combined models arima svm arima lstm as well as hybrid methods using model 2 eq 7 table 6 from table 4 it can be seen that arima rf achieved the lowest mae rmse fsd and nse and highest sim measures on most predicted periods the second rank is arima knn for these indicators it also produced the best results on vu quang dataset for 48h forecasting of water level this indicates arima rf and arima knn based on eq 6 are more accurate than other methods significantly they could fully exploit separation of linear and nonlinear components looking at results of arima svm svm arima lstm and lstm methods we see that arima svm and arima lstm do not improve the performance over svm and lstm respectively that is when combining the nonlinear part with the original data neither svm nor lstm could exploit and capture well the nonlinear component of the time series table 5 presents the results of the different forecasting methods on the hung yen dataset in this table arima rf and arima knn methods do not yield good results at all forecast horizons as in the table 4 but the both methods still have performance improvements over single machine learning models rf knn once again arima svm and arima lstm methods failed to capture the non linear part of the time series data these methods yield results even worse than single machine learning models svm and lstm table 6 gives the average forecasting results using the proposed algorithms model 2 based on eq 7 on the vu quang dataset the results indicate that the hybrid approaches based on eq 7 model 2 are no more effective than arima overall results of the hybrid methods model 1 on this dataset table 4 clearly shows that the methods of the first type of hybrid models model 1 using eq 6 outperforms significantly the methods of the second type of the hybrid approach model 2 figs 7 8 and 9 plot the results of time series forecasts from different algorithms on hung yen and hanoi datasets the observed water level values and predicted ones generated from each model are showed to compare their performances fig 7 compares the predicted water level of all models for 3 days ahead in this figure we could see that arima rf yields the best results for the first day its forecast values are nearly identical to the actual ones however when predicting for the second and third days the results are no longer as good as for the first day it still captures rather well the trend of real data but the forecast errors are higher on the contrary arima knn forecasts poorly for the first day but relatively well for the second and third days rf knn lstm arima lstm produce good predicted values initially but then quickly fail to capture the trend of the real data it can be seen from figs 7 and 8 that in general arima is not suitable for forecasting water level on red river although initially it produces good results fig 7 fig 8 demonstrates again the superiority of arima rf and arima knn for forecasting water level on hanoi dataset for the first 5 days prediction values of these two methods are closer to observed water levels than arima rf and knn methods for the last 5 days only arima rf is able to catch approximately the trend of real data but the forecast error is large by observing the experimental results table 4 fig 9 we have found that lstm and arima lstm produced worse prediction performance for the next 10 days apart from normal forecasts it is interesting and important to see if the hybrid approach is effective in critical situations such as forecasting at a few days before the water level goes significantly high suggesting a peak in a flood fig 10 11 and 12 present the visual comparisons of all methods for forecasting several periods of water level at hanoi vu quang and hung yen stations fig 10 shows the results of forecasting the water level before the peak july 24 2017 at 4h at hanoi station with 1 day fig 10a 2 days fig 10b 3 days fig 10c and 5 days fig 10d ahead again it is evidenced from the figures that arima rf yields the best results 1 day ahead and 2 days ahead forecast values are close to the real ones when forecasting 3 days and 5 days in advance the arima rf could capture quite well the trend of the actual data but the forecasting errors are rather high other methods do not work well for all these cases fig 11 demonstrates the results of forecasting the water level before the peak august 21 2016 at vu quang station with 1 day fig 11a 2 days fig 11b 3 days fig 11c and 5 days fig 11d ahead the figure shows a similar results to the case of hanoi station in that arima rf could forecast quite accurately the peak with 1 and 2 days ahead it still captures rather well the trend of the data in the 3 and 5 day ahead forecasts but the errors are high meanwhile all of other models failed to forecast the peak and could not capture the data trend for hung yen data with hourly sampling the highest water level was on july 23 2014 at 22h 368 cm we forecast the water levels before the peak 6h fig 12a 12h fig 12b 24h days fig 12c and 3 days fig 12d fig 12 demonstrates once again that the hybrid approach is superior to the single component methods when predicting water levels 6h ahead arima knn produces the closest values to the real ones fig 12a however when forecasting water levels with 12h and 24h in advance arima knn no longer gives good results as in the case of 6h it is always that arima rf produces predicted values which are more similar to the true ones than other methods fig 12b 12 c although arima knn is second behind arima rf the gaps between the forecast errors of the two methods are rather wide for hungyen dataset when predicting 3 day ahead of the peak only arima rf could capture the trend of data but the forecast error is relatively large 5 conclusion forecasting accurately time series especially water level for flood warning systems is a very important but challenging task arima knn rf svm and lstm are widely popular and effective forecasting models proposed and tested on hydrological time series in the literature arima can model well linear whereas the other statistical machine learning models are most suitable for nonlinear time series however in reality a hydrological time series often includes both linear and nonlinear correlation structures therefore in this paper we have proposed two types of hybrid approaches in order to improve the forecasting performance they have taken the advantages of each individual model type i e linear and non linear and complexity levels i e single or hybrid in time series forecasting the first type of hybrid approach model 1 is to combine the original data and the residuals obtained from arima to build the predictive models namely arima knn arima rf arima svr and arima lstm the second hybrid type is to perform predictions on the original data and arima s residuals then to aggregate the forecast results model 2 these models have been tested on three real big datasets on the water level time series of the red river and compared with each single component model the experimental results showed that the combined methods arima rf and arima knn of model 1 yield superior and more reliable results than other hybrid models e g arima svm and arima lstm as well as better than the traditional single component methods arima knn rf svm and lstm in future we are planning to apply our novel methods arima rf and arima knn for forecasting water level on other horological stations of the red river as well as for other rivers credit authorship contribution statement thi thu hong phan conceptualization methodology formal analysis writing original draft writing review editing software xuan hoai nguyen conceptualization methodology formal analysis writing original draft writing review editing declaration of competing interest all authors have participated in a conception and design or analysis and interpretation of the data b drafting the article or revising it critically for important intellectual content and c approval of the final version this manuscript has not been submitted to nor is under review at another journal or other publishing venue the authors have no affiliation with any organization with a direct or indirect financial interest in the subject matter discussed in the manuscript acknowledgments the research presented in this paper was funded by national science and technology program to respond to climate change manage natural resources and the environment in the period 2016 2020 in the project titled applications of artificial intelligence for forecasting hydro meteorological anomalies in vietnam under the context of climate change grant number bđkh 34 16 20 supplementary material supplementary material associated with this article can be found in the online version at doi 10 1016 j advwatres 2020 103656 appendix a supplementary materials supplementary data s1 supplementary raw research data this is open data under the cc by license http creativecommons org licenses by 4 0 supplementary data s1 
472,introduction of fine sand into gravel bed rivers has surged tremendously as a result of human activities the ecological concerns for river habitats have called for suitable management strategies for cleaning these particles from gravel bed rivers this paper is concerned with determining movement threshold of fine sand initially entrapped in the spaces between large immobile elements resembling a coarse riverine substrate with flow depths comparable to non erodible element sizes static equilibrium condition was reached during an experimental run by maintaining a zero supply condition for sand it was assumed that the flow condition corresponding to this sand level was representative of critical conditions for onset of sand motion a nonlinear regression curve was fitted between the level of the sand and associated critical shear stress the flow field measurements using an ultrasonic doppler rofiler delineate main features of the shallow flow over large roughness elements such as sharp velocity gradients near the crest of the immobile elements and also a deviation of instantaneous velocity probability density function from normal distribution very close to the sand level keywords fine sand winnowing sediment transport gravel bed rivers rough bed low submergence ratio flows river habitats salmon embryos 1 introduction an increase in frequency of natural catastrophes such as wild fires and floods and direct human activities such as road construction urbanization and agriculture have introduced large amounts of sand into rivers this has led ecological consequences for river habitats to an alarming level among the impacted habitats are spawning salmonids which usually choose coarse bed rivers as their nesting sites leonardson 2010 the local flow conditions clean gravel and shape of the redd depression created by female salmon using tail and body drive water through the nest supplying dissolved oxygen and removing metabolic wastes the sealing of pores of the coarse gravel elements due to sand intrusion reduces the permeability of the redd and subsequent filling with finer particles lead to oxygen depletion for embryos wood and armitage 1997 reported that a decline in solved oxygen due to coal mining resulted in one hundred percent death of salmon embryos in south wales also fine sediment was considered a major cause of the severe reduction of trout salmons in uk chalk streams accorny and sear 1999 releasing an artificial flood from dam reservoirs is a strategy for removal of the fine sand usually there are two methods of flushing one is surface based in which the sand is only removed without entraining the gravel underneath the other type is subsurface flushing that is intended to pull out sand from deeper bed and this is usually accompanied with destroying the structure of the bed the process of entrainment of sand from interstices of an immobile substrate into the flow requires consideration of several contrasting processes grams and wilcock 2007 first the surface area of the bed available for entrainment is reduced for the sand fraction second when the sand particles lie in the interstices of gravels they are shielded by the protruded elements against flow impingement on bed and third the turbulent wakes around the immobile elements give rise to local erosion of the sand limiting the discussion to surface flushing and bedload transport only we need to find the minimum discharge which would be necessary to mobilize fine sand at a certain level below the crest of the gravel elements usually there are two approaches for formulating a bedload transport equation the first is a deterministic approach in which the rate of transport of the particles is expressed as a function of excess shear stress in this approach particles are assumed to get into motion when the time averaged bed shear stress exceeds a particular value called critical bed shear stress this is usually derived from the shields diagram the second group of formulations of bedload transport rate is through a stochastic expression of the instantaneous forces on individual or ensemble of particles einstein was the first to pioneer a probabilistic method for prediction of bedload transport rate dey 2014 deterministic bedload transport formulas such as meyer peter and mueller 1948 are still widely used for their practicality and ease of parameterization since they have been only devised for particles of uniform size on the bed they need to be modified to be applicable to a non uniform mixture of particles therefore deterministic sediment mixture transport formulas have been formulated based on modification of shield s critical shear stress to compensate for the more energy needed by the flow to move particles hidden in the lee side of larger particles a modification of mayer peter formula using a hiding function for bedload transport rate of fraction ds in a mixture of particles on bed surface reads as kleinhans and van rijn 2002 1 ϕ b s p s α θ s θ c r s c o r r e c t e d 1 5 2 θ c r s c o r r e c t e d ξ s θ c r d s s h i e l d s where φ b s is a dimensionless bedload transport rate q b s ρ s φ b s s 1 g d s 1 5 ps surface fraction of sand ds θ s grain shear stress for ds θ cr s corrected corrected dimensionless critical bed shear stress for grain size ds ξ s hiding factor for size fraction ds and θ s τ s ρ s ρ gds dimensionless grain shear stress for the sand several hiding factors have been introduced in the literature among them the theoretical formula of egiazaroff 1965 and empirical formulas by parker et al 1982 and wilcock and crowe 2003 parker 2008 brings a full account of various hiding factors and their calculation procedure the hiding factor provided by these researchers usually belongs to cases where all surface particles are moving together are in the same size order and their sizes are small fraction of total flow depth there might be specific situations that do not meet these assumptions for example downstream of dams the river bed comprises of elements that may remain immobile and their interstices are frequently filled and depleted by fine sand partial transport formulas are needed for these sediment mixtures also in mountainous regions the flow depth is in the order of size of the underlying elements in this condition the logarithmic velocity profile may not be valid yager et al 2007 and different resistance laws have been used by researchers like bathurst 1985 kuhnle et al 2013 examined the viability of a two fraction mixture sediment transport rate formula proposed by wilcock and kenworthy 2002 to quantify the fine sand transport rate through and over immobile elements with large size discrepancy comparing to experimental results they concluded that wilcock and kenworthy s formula overpredicts and underpredicts experimental observation depending on the level of the sand below the crest of immobile elements they attributed this discrepancy to larger values of reference shear stress in formula proposed by wilcock and kenworthy 2003 with respect to their observations the reference shear stress is used in mixture transport formula as a replicate for critical shear stress and is the bed shear stress for a very small discharge of the sediment that can be measured in a mixture transport experiment the study of sand erosion affected by non erodible elements have been pursued both by aeolian and fluvial hydraulics researchers lyles et al 1974 examined the effect of standing crops on protecting underlying soli from erosion using cylindrical and spherical non erodible elements with various planting densities they performed wind tunnel experiments to observe the sand erosion initially filling spaces between elements up to their height a free stream velocity higher than the threshold velocity of the sand particles started the erosion and it proceeded until a very low rate of sand could be measured exiting the tunnel the stabilization of the sand surface was the result of appearance of the elements and their absorption of flow momentum which resulted in less momentum flux towards the sand surface and ultimately stoppage of the erosion process under the imposed flow magnitude using a logarithmic velocity law they calculated bed shear stress of the stabilized surface namely the critical shear stress they obtained a non dimensional shear velocity that could be expressed as a linear or power law function of geometrical characteristics of the roughness elements i e the ratio of the height of the element to distance between them and also number of elements per unit area the same approach in obtaining the critical shear stress for the sand movement under effect of cubic non erodible elements was done by iversen et al 1990 they performed their experiment in a wind tunnel under different concentration of roughness elements and concluded that power law can describe the variation of critical shear stress with the concentration of the elements ruapach et al 1993 proposed a theoretical criterion for onset of motion of sand particles under effect of non erodible elements based on his theoretical derivation for partitioning of total drag into form and skin components raupach 1992 in fluvial hydraulic yager et al 2007 examined the bedload transport of natural sediment affected by presence of boulders in a steep channel with shallow flow depth resembling one in mountainous regions they developed a theory for partitioning of the drag based on geometrical characteristics of spherical immobile elements implementing the part of the drag on the intervening surface into a conventional bedload transport formula such as fernandez luque and van beek 1976 they computed the equilibrium bedload transport rate for the mobile phase they validated their experiments in a flume with immobile phase consisting of spheres of 30 mm and mobile particles of fine gravel with 3 7 mm diameters their theoretical formula could predict the experimental observations with good accuracy with a maximum of order of magnitude discrepancy with experimental data in some experiments grams and wilcock 2007 performed experiments to propose a formula for entrainment into suspension of fine sediment initially trapped in the voids of an immobile substrate composed of diagonal arrangement of hemispherical elements their dimensional and experimental observations showed that the elevation of the sand below the immobile element crest should be directly considered in an equilibrium entrainment function of such flows and implemented that as an adjustment factor in the entrainment function of garcia and parker 1991 kuhnle et al 2013 conducted flume experiments for the quantification of equilibrium bedload transport of fine sand moving through and above immobile natural gravel elements they surmised that the shear stress on the surface of mean sand level can be calculated as a linear fraction of the total be shear stress on the crest of the immobile phase 3 τ s a z s τ b the proportionality factor namely a zs was named as roughness geometry function from nikora et al 2001 2007 it can be calculated as 4 a z s a s z z s a s z z s a g z z s where as z zs and ag z zs are the planar areas of the bed covered by sand and gravel respectively in fact the roughness geometry function is equal to the cumulative probability distribution of bed fluctuations whose importance in the quantification of sediment mixture transport has been brought by parker 2008 and the references therein kuhnle et al 2013 assumed that the roughness geometry function can be used effectively as a scale factor for the distribution of total shear stress below gravel crest they proposed a formula to estimate the fine sand transport rate from their experimental observations 5 ϕ b s q b s s 1 g d s 0 5 ρ s d s 2 29 10 5 a z s 2 14 τ b τ c s 3 49 τ b is the total bed shear stress τ cs is the critical shields stress for the sand when covering the crest of the gravels ds is the diameter of fine sand ρ s is the mass density of the fine sand and a zs is the roughness geometry function at the mean sand level calculated from eq 4 wren et al 2014 extended their experiments to equilibrium fine sand bedload transport over larger cobble bed and concluded that the general format of the formula 5 could be preserved but the exponents need to be changed to fit their new observations in another investigation kuhnle et al 2016 sought the maximum erosion depth of sand from interstices of immobile gravel elements the sand in each experiment initially filled the voids of larger elements up to their crest and no sediment was supplied into the channel the erosion of the sand continued until no sand was running out the flume anymore they estimated the shear velocity of the new sand level with the formula 3 and assumed that the erosion of the sand proceeds until the shear velocity in the gravel was equal to the fall velocity of the sand times a constant by calibrating this constant from experimental data they could calculate the maximum clean out depth from the combination of formula 3 and inverse of cumulative probability distribution of the bed fluctuations the common feature of all the above reported investigations is inclusion of roughness characteristics of the immobile phase into the transport rate of the mobile phase the effect of the arrangement and proximity of the obstacles is a controlling factor in trailing vortices hence mass and momentum transfer in the bed region iversen et al 1990 one of the factors that need more attention in sediment transport in fluvial environments is the relative submergence or the depth of fluid divided by size of the immobile element all the mentioned investigations were dedicated to flows with high relative submergence whereas for mountainous rivers the depth of the flow may be in the order of magnitude of grain size the significance of relative submergence and its effect on flow hydrodynamics has been elucidated by nikora et al 2001 2007 according to nikora et al 2001 the flow over a rough substrate can be divided into several subregions along the vertical and depending on the degree of perturbation of flow by roughness elements these subregions may or may not exist spatially averaging of the reynolds averaged navier stokes rans equations they derived a new set of equations which include new terms of form induced stresses and pressure drag in the momentum equations spatial averaging is performed in two steps first the navier stokes equations that fully describe the motion of flow over rough bed are averaged in time which is conventionally called reynolds average navier stokes equations rans and in the next step the time averaged variables are averaged spatially on a very thin plane parallel to the bed this plane will be continuous above the crest elements and disconnected below the gravel crest according to nikora et al 2001 four subregions can be recognized an interfacial subregion between roughness crests and troughs a form induced sublayer in which form induced stresses have appreciable significance a logarithmic region where form induced stresses start to vanish and a typical overlap region similar to that in hydraulically smooth flows develops and finally an outer flow region that is affected by free stream velocity rather than near bed velocity scales for high relative submergence flows over a rough bed the four sublayers coexist however for very shallow flows only the first two sublayers near the bed may form bayazit 1982 examined the threshold behavior of a sphere in a very shallow open channel flow over very rough bed and observed that the non dimensional shields stress parameter increased from 0 04 up to 0 12 when the relative submergence was reduced to 3 3 the effect of relative submergence on threshold shear stress of uniform and mixture particles was also investigated and proved by shvidchenko 2000 bayazit and denker 1980 reported that bed load discharge for a particle on a very rough channel may reduce up to thirty times when the relative submergence fell below five this paper is aimed at presenting laboratory experiments for estimation of bed shear stress for threshold of movement of sand at various levels below immobile roughness elements deterministic bedload transport formula should be corrected for when sand is protected by the larger elements little data exist in the literature for low submergence ratio flows the experiments of yager et al 2007 addresses the problem but the size ratio between the mobile and immobile phases is limited to around 0 1 and also all the experiments were performed for supercritical conditions presenting flow field measurements using ultrasonic velocity profiler uvp at the critical condition of the bed the unique property of the uvp is its non intrusive nature the measurements in very shallow flow depths is almost impossible with intrusive instruments such as adv acoustic doppler velocimeter we were able to measure the instantaneous velocity profiles whose time and spatially averaged quantities are reported here testing various drag partitioning approaches and compare them to direct numerical simulation dns results from the literature 2 methodology and materials our experiments were designed based on the same ideas of lyles et al 1974 iversen et al 1990 and kuhnle et al 2016 the immobile elements are covered to the crest by fine sand and a flow is set in a channel then erosion of the sand layer begins and in time the level of the sand drops below of the immobile elements crest by maintaining a uniform flow during the erosion process the movement of sediment proceeds to a level for which threshold condition can be assumed the laboratory experiments were conducted in a 600 cm meter long 40 cm wide water recirculating open channel flume with plexiglas side walls fig 1 water was pumped from an underground storage tank a magnetic flow meter was used to control the discharge magnitude into the channel at the entrance of the channel a distilling tank and flow straighteners were used to guide the flow smoothly into the channel the immobile substrate was prepared using two layers of spherical plastic balls with diameter of 50 mm an aluminum rod was used to pierce a hole in a wooden plate attached to the flume bed on one side the steel rod was perforated through the plastic balls on its other side to keep them firm in the main channel the pores of the first 120 cm of the flume were filled with crushed gravel particles so that it is ensured that fully developed flow condition prevailed in the working section because of the length of the channel usually a tailgate was needed to establish uniform flow conditions uniform sand of 0 5 mm diameter was used as the transported material three sets of experiments were performed for three different bed slopes several longitudinal sections can be identified in a typical non equilibrium sediment transport experiment just downstream of the rigid apron and at the beginning of the movable bed region a pit usually formed whose length is called a saturation length a fluid vortex governs the flow field in this region after that an equilibrium region is formed in which the sediment transport rate reaches an equilibrium with the transport capacity of the imposed flow all measurements and observations during the experiments are reported for the equilibrium region and no attempt was made to record bed geometry or flow features in the saturation length region another feature of non equilibrium sediment transport in the experiments was evidence of formation of barchan dunes this happened at the beginning of the experiments when the sand completely filled the voids up to the crest of the spheres table 1 shows the total combination of parameters for the laboratory experiments in this table the flow depth h is indicating the height of the water above the crest of the elements average velocity is based on uave q bh in which q is the total discharge measured by the flow meter h is the water depth on the crest and b is the width of the channel the bed shear velocity is computed as u g h s 0 in which g is acceleration due to gravity and s 0 is the slope of the channel at the crest of the spheres the flow depth ranged between 13 and 78 mm giving a minimum aspect ratio the width of the channel divided by the flow depth over crest equal to 5 13 at the beginning of each experimental run the substrate was filled up to the crest of the immobile elements no sediment was supplied from upstream so only sediment was transported due to bed degradation the range of the discharges and sediment sizes were chosen to ensure that sediment particles were in frequent contact with the bed each series of experiments was started by setting a low steady discharge and uniform flow was established during initial stages of the experiment by adjusting the tailgate in a typical experiment the sediment discharge was declining exponentially in time until it would reach to almost 1 of that at the beginning of the experiment in this condition it was assumed that the bed had reached a static equilibrium for which a threshold bulk shear stress can be assigned for that specific flow and geometrical conditions after reaching the static equilibrium the flow field was measured using an ultrasonic velocity profiler uvp the water was allowed to drain and the bed topography was captured using a laser scanner these steps completed the data recording for a run another run of the experiment was established by the next higher discharge this brought the sand level lower and to a new equilibrium condition fig 2 shows planar photos of the bed during the degradation stage and a typical transport rate variation during one experiment 3 results and discussion 3 1 bed level changes a laser scanning of the bed was performed as the bed reached the final stage of erosion at the end of each experimental run the area of 1000 mm in 350 mm was selected for scanning the recording interval of the scanner was 1 mm in longitudinal direction and 1 mm in traverse direction fig 3 shows configuration of the bed from the laser scanner the probability distribution function of the bed elevation and cumulative probability distribution function or roughness geometry function as shown in the picture the pdf of the bed level elevation is changing from normal distribution at the beginning of the experiments when the substrate is filled up to the crest to a bimodal distribution as the sand is eroded from the interstices the bimodality increases as the sand level goes down for higher discharges the roughness geometry function experiences an inflection when crossing from one modality to the other the slope of this transition line reduces as the bimodality increases when sand level reaches deeper depths bellow the crest of non erodible elements 3 2 flow field measurements one of the main challenges in the quantification of sediment transport in high gradient gravel bed rivers is dealing with shallow flow depths of the order of the size of the gravel elements these ranges of depth happen in the roughness sublayer and velocity distribution and friction factor are completely different from that of the same stream with much more submergence depths especially in the configuration of our experiments even the minimum depth of 20 mm caused an erosion depth of comparable magnitude below the crest therefore besides perturbation of roughness elements into the flow depth above the crest there is also an interaction of the surface and subsurface flow regions giving rise into a mass and momentum exchange an ultrasonic velocity profiler uvp was used for recording the velocity field uvp is able to measure the instantaneous one dimensional velocity along a line an ultrasound pulse is emitted from a transducer into the liquid and the same transducer receives the echoes which originate from tracer particles suspended in the fluid more details of the working principle of a uvp can be found in takeda 1995 uvp is very useful for its almost non intrusive advantages and also the fact that we had very shallow flow which makes applicability of other instruments such as acoustic doppler velocimeter adv impossible velocity profiles were recorded on a grid comprising of nine points as shown in fig 4 the minimum elevation on the sand layer that could be captured varied between points due to the microtopography of sand layer usually the emitted pulse from the uvp had intersections with these points on the bed with elevations above the average elevation the recording time of the velocity data was almost four minutes to effectively produce stationary time series of velocity data an approximate criterion ηu ν 50 bandyopadhyay 1987 ensured that all the measurements are in fully rough turbulent flow conditions η u and ν are the level of the sand below crest the bed shear velocity and water kinematic viscosity respectively fig 4 shows the time averaged velocity profile for three different sets of experiments i e q 4l s q 6l s and q 8 l s for a constant bed slope of s 0 005 the vertical axis shows the elevation from the crest of the spheres mm and the horizontal axis shows the time averaged longitudinal velocity mm s in all three discharges and above the crest there is a sharp gradient in velocity profile on the crest of the sphere following a segment with lower gradient below the crest the velocity distribution can be both linear q 4l s and exponential q 6l s q 8l s this substantiates the role of fluid flow below the crest which must be considered for quantification of mass and momentum transport in integrated shallow water equations and sediment conservation equation fig 5 shows the spatially averaged flow velocity above crest for the three discharges whose local velocity distributions were reported in fig 5 recently spatially averaged representations of the flow field equations have been proven useful for flows with heterogeneous beds rooted in porous media research they have found their path toward flow over rough surfaces in hydraulics nikora et al 2001 flow field statistics near the bed have important implications for sediment transport formulations usually the probability density function of near bed flow variables such as instantaneous velocity distribution and instantaneous bed shear stress distribution are used in formulating the pickup probability of bedload formulas and threshold criterion analysis fig 6 shows the pdf of instantaneous streamwise velocity component for different regions of the flow along the beam of the uvp as fig 6 shows in most of the flow depth the normal pdf prevails however very close to the bed the pdf is deviating from normal distribution and a stable distribution could be fit to the pdf curve this must be considered in stochastic formulation of above threshold bedload transport formulas 3 3 critical bed shear stress the pattern of sand erosion around and between the immobile elements was not uniform so measurements of the erosion depth bellow the crest was averaged spatially in rows between elements the spatially averaged erosion depth below the crest is plotted against the dimensionless bed shear stress θ critical τ critical ρ s ρ gds in fig 7 on the vertical axis the level of the sand below the crest η is non dimensionalized with the non erodible phase diameter dg the variations could be fitted with a michaelis menten equation 6 η d g a θ e x c e s s b θ e x c e s s where θ excess θ critical θ critical surface θ critical the dimensionless critical shear stress at sand level θ critical surface dimensionless critical shear stress of the sand at the level of the crest of the non erodible phase which was estimated using shields diagram a 0 989 and b 0 06207 3 4 drag partitioning approaches there is still much debate about the partitioning of the drag in complex rough wall flows various researchers have approached the problem of drag partitioning on rough substrates ranging from direct analysis of the governing equations nikora et al 2019 and simple theoretical expressions validated by experiments raupach 1992 to direct measurements of forces in the laboratory experiments wooding et al 1973 the total drag τ b can be divided into pressure drag term τ r on roughness elements plus a skin drag term τ s on the intervening surface between elements the geometry arrangement proximity and planar concentration of obstacles contribute to this distribution there are several drag partitioning approaches in the literature formulated for regular arrangements of cylinders cubes and spheres these approaches for drag partitioning have been mostly formulated based on roughness density of the surface elements i e the total surface area of the elements projected on a vertical plane divided by the total horizontal surface area fig 8 shows the ratio of pressure drag τ r to total drag as a function of roughness density frontal area per unit ground area for drag partitioning methods developed by wooding et al 1973 arya 1975 raupach 1992 and shao and yang 2005 all these approaches have been developed in meteorological community and all except that of shao and yang 2005 consider a sparse distribution of the elements for comparison we have also included the relevant parameter from direct numerical simulations of leonardi and castro 2010 and wu et al 2020 as it is seen in the figure the methods of raupach 1992 and shao and yang 2005 are closer to the dns data of leonardi and castro 2010 but departs from that of wu et al 2020 probably due to different obstacle shapes cubes against hemispheres reynolds numbers and flow regime fully rough and transitionally rough we could not assess the applicability of these drag partitioning approaches to our experiments the drag partitioning of kuhnle et al 2013 2015 considers a multiplication of roughness geometry function a z s a s z z s a s z z s a g z z s and total wall stress to compute the shear at the sand level this might be effective in cases where a zs at the mean sand level is a decreasing function of elevation but for example if the immobile roughness elements are cubes this formula gives a fixed shear stress for all levels of the sand or it only works for spherical elements to some certain depth the range of reynolds number r e u a v e h ν in our experiments lies between 3500 and 34 000 uave is the cross sectional average velocity h is the flow depth and ν is the kinematic viscosity the lower margin belongs to sand levels near the crest and higher reynolds numbers to deeper sand levels this might result in a friction dominated flow for all our experiments or at least for lower reynolds numbers this can only be clarified by conducting high resolution numerical modeling using dns or les large eddy simulation with considering all the bed undulations because they contribute to form and skin drag the recorded bathymetry can be input to the model and velocity profile data captured by the doppler profiler can be used for verification of the models 4 conclusions the paper aims to understand the hydrodynamics and sediment transport phenomenon at the interface of immobile gravel substrate and the overlying shallow fluid flow several shortcomings of the previous approaches were discussed here most sediment transport formulas neglect the effect of relative submergence and concentration of the immobile phase of the bed materials there are several deterministic and stochastic approaches for sediment transport calculations which neglect these aspects regarding stochastic approaches most of them are based on normal pdf instantaneous velocity distributions while our preliminary observations shows that there is deviation from gaussian pdf for near bed velocities fig 6 we performed experiments to contribute to data for threshold behavior of the fine sand for implementing into deterministic bedload transport formulas a nonlinear regression curve was fit to the threshold shear stress data fig 7 most of the drag partitioning approaches in the literature are devoted to sparse roughness densities and they were shown to contradict with results from direct numerical simulation dns a drag partitioning methodology based on physics of depth limited water flow on rough substrates is needed a high resolution numerical simulation which can adequately consider the bed undulations and compatible with the recorded velocities is envisaged for our future research ds sand diameter dg gravel diameter ρ g gravel elements mass density qs sand transport rate ρ s sand mass density μ water viscosity ρ water mass density u characteristic flow velocity j water surface slope φ b s dimensioless sediment transport rate θ s non dimensional skin shear stress θ cr s corrected corrected non dimensional shear stress ps surface fraction of sand ξ s hiding factor for sand zt t level of the sand as z zs plan area of the portion of the bed covered by sand ag z zs plan area of the portion of the bed covered by gravel τ b bed shear stress τ cs critical shields stress for the sand h height of the water above the crest of the elements q total water discharge b width of the channel g acceleration due to gravity s 0 slope of the channel u c critical shear velocity for the sand level u cs critical bed shear velocity for the fine sand covering the crest τ r pressure drag term τ s skin drag term af frontal area λ e effective density r radius of spheres equal to 25mm η distance between surface of the sand and crest of the gravel θexcess bed shear stress credit authorship contribution statement mahdi khademishamami conceptualization methodology software formal analysis validation investigation resources data curation writing original draft william nardin writing review editing declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgement the first author is grateful to the research faculty and technical staff of hydraulics laboratory of department of civil engineering at the university of trento for their help and support for providing and preparation of the experimental facilities supplementary materials supplementary material associated with this article can be found in the online version at doi 10 1016 j advwatres 2020 103659 appendix supplementary materials image application 1 
472,introduction of fine sand into gravel bed rivers has surged tremendously as a result of human activities the ecological concerns for river habitats have called for suitable management strategies for cleaning these particles from gravel bed rivers this paper is concerned with determining movement threshold of fine sand initially entrapped in the spaces between large immobile elements resembling a coarse riverine substrate with flow depths comparable to non erodible element sizes static equilibrium condition was reached during an experimental run by maintaining a zero supply condition for sand it was assumed that the flow condition corresponding to this sand level was representative of critical conditions for onset of sand motion a nonlinear regression curve was fitted between the level of the sand and associated critical shear stress the flow field measurements using an ultrasonic doppler rofiler delineate main features of the shallow flow over large roughness elements such as sharp velocity gradients near the crest of the immobile elements and also a deviation of instantaneous velocity probability density function from normal distribution very close to the sand level keywords fine sand winnowing sediment transport gravel bed rivers rough bed low submergence ratio flows river habitats salmon embryos 1 introduction an increase in frequency of natural catastrophes such as wild fires and floods and direct human activities such as road construction urbanization and agriculture have introduced large amounts of sand into rivers this has led ecological consequences for river habitats to an alarming level among the impacted habitats are spawning salmonids which usually choose coarse bed rivers as their nesting sites leonardson 2010 the local flow conditions clean gravel and shape of the redd depression created by female salmon using tail and body drive water through the nest supplying dissolved oxygen and removing metabolic wastes the sealing of pores of the coarse gravel elements due to sand intrusion reduces the permeability of the redd and subsequent filling with finer particles lead to oxygen depletion for embryos wood and armitage 1997 reported that a decline in solved oxygen due to coal mining resulted in one hundred percent death of salmon embryos in south wales also fine sediment was considered a major cause of the severe reduction of trout salmons in uk chalk streams accorny and sear 1999 releasing an artificial flood from dam reservoirs is a strategy for removal of the fine sand usually there are two methods of flushing one is surface based in which the sand is only removed without entraining the gravel underneath the other type is subsurface flushing that is intended to pull out sand from deeper bed and this is usually accompanied with destroying the structure of the bed the process of entrainment of sand from interstices of an immobile substrate into the flow requires consideration of several contrasting processes grams and wilcock 2007 first the surface area of the bed available for entrainment is reduced for the sand fraction second when the sand particles lie in the interstices of gravels they are shielded by the protruded elements against flow impingement on bed and third the turbulent wakes around the immobile elements give rise to local erosion of the sand limiting the discussion to surface flushing and bedload transport only we need to find the minimum discharge which would be necessary to mobilize fine sand at a certain level below the crest of the gravel elements usually there are two approaches for formulating a bedload transport equation the first is a deterministic approach in which the rate of transport of the particles is expressed as a function of excess shear stress in this approach particles are assumed to get into motion when the time averaged bed shear stress exceeds a particular value called critical bed shear stress this is usually derived from the shields diagram the second group of formulations of bedload transport rate is through a stochastic expression of the instantaneous forces on individual or ensemble of particles einstein was the first to pioneer a probabilistic method for prediction of bedload transport rate dey 2014 deterministic bedload transport formulas such as meyer peter and mueller 1948 are still widely used for their practicality and ease of parameterization since they have been only devised for particles of uniform size on the bed they need to be modified to be applicable to a non uniform mixture of particles therefore deterministic sediment mixture transport formulas have been formulated based on modification of shield s critical shear stress to compensate for the more energy needed by the flow to move particles hidden in the lee side of larger particles a modification of mayer peter formula using a hiding function for bedload transport rate of fraction ds in a mixture of particles on bed surface reads as kleinhans and van rijn 2002 1 ϕ b s p s α θ s θ c r s c o r r e c t e d 1 5 2 θ c r s c o r r e c t e d ξ s θ c r d s s h i e l d s where φ b s is a dimensionless bedload transport rate q b s ρ s φ b s s 1 g d s 1 5 ps surface fraction of sand ds θ s grain shear stress for ds θ cr s corrected corrected dimensionless critical bed shear stress for grain size ds ξ s hiding factor for size fraction ds and θ s τ s ρ s ρ gds dimensionless grain shear stress for the sand several hiding factors have been introduced in the literature among them the theoretical formula of egiazaroff 1965 and empirical formulas by parker et al 1982 and wilcock and crowe 2003 parker 2008 brings a full account of various hiding factors and their calculation procedure the hiding factor provided by these researchers usually belongs to cases where all surface particles are moving together are in the same size order and their sizes are small fraction of total flow depth there might be specific situations that do not meet these assumptions for example downstream of dams the river bed comprises of elements that may remain immobile and their interstices are frequently filled and depleted by fine sand partial transport formulas are needed for these sediment mixtures also in mountainous regions the flow depth is in the order of size of the underlying elements in this condition the logarithmic velocity profile may not be valid yager et al 2007 and different resistance laws have been used by researchers like bathurst 1985 kuhnle et al 2013 examined the viability of a two fraction mixture sediment transport rate formula proposed by wilcock and kenworthy 2002 to quantify the fine sand transport rate through and over immobile elements with large size discrepancy comparing to experimental results they concluded that wilcock and kenworthy s formula overpredicts and underpredicts experimental observation depending on the level of the sand below the crest of immobile elements they attributed this discrepancy to larger values of reference shear stress in formula proposed by wilcock and kenworthy 2003 with respect to their observations the reference shear stress is used in mixture transport formula as a replicate for critical shear stress and is the bed shear stress for a very small discharge of the sediment that can be measured in a mixture transport experiment the study of sand erosion affected by non erodible elements have been pursued both by aeolian and fluvial hydraulics researchers lyles et al 1974 examined the effect of standing crops on protecting underlying soli from erosion using cylindrical and spherical non erodible elements with various planting densities they performed wind tunnel experiments to observe the sand erosion initially filling spaces between elements up to their height a free stream velocity higher than the threshold velocity of the sand particles started the erosion and it proceeded until a very low rate of sand could be measured exiting the tunnel the stabilization of the sand surface was the result of appearance of the elements and their absorption of flow momentum which resulted in less momentum flux towards the sand surface and ultimately stoppage of the erosion process under the imposed flow magnitude using a logarithmic velocity law they calculated bed shear stress of the stabilized surface namely the critical shear stress they obtained a non dimensional shear velocity that could be expressed as a linear or power law function of geometrical characteristics of the roughness elements i e the ratio of the height of the element to distance between them and also number of elements per unit area the same approach in obtaining the critical shear stress for the sand movement under effect of cubic non erodible elements was done by iversen et al 1990 they performed their experiment in a wind tunnel under different concentration of roughness elements and concluded that power law can describe the variation of critical shear stress with the concentration of the elements ruapach et al 1993 proposed a theoretical criterion for onset of motion of sand particles under effect of non erodible elements based on his theoretical derivation for partitioning of total drag into form and skin components raupach 1992 in fluvial hydraulic yager et al 2007 examined the bedload transport of natural sediment affected by presence of boulders in a steep channel with shallow flow depth resembling one in mountainous regions they developed a theory for partitioning of the drag based on geometrical characteristics of spherical immobile elements implementing the part of the drag on the intervening surface into a conventional bedload transport formula such as fernandez luque and van beek 1976 they computed the equilibrium bedload transport rate for the mobile phase they validated their experiments in a flume with immobile phase consisting of spheres of 30 mm and mobile particles of fine gravel with 3 7 mm diameters their theoretical formula could predict the experimental observations with good accuracy with a maximum of order of magnitude discrepancy with experimental data in some experiments grams and wilcock 2007 performed experiments to propose a formula for entrainment into suspension of fine sediment initially trapped in the voids of an immobile substrate composed of diagonal arrangement of hemispherical elements their dimensional and experimental observations showed that the elevation of the sand below the immobile element crest should be directly considered in an equilibrium entrainment function of such flows and implemented that as an adjustment factor in the entrainment function of garcia and parker 1991 kuhnle et al 2013 conducted flume experiments for the quantification of equilibrium bedload transport of fine sand moving through and above immobile natural gravel elements they surmised that the shear stress on the surface of mean sand level can be calculated as a linear fraction of the total be shear stress on the crest of the immobile phase 3 τ s a z s τ b the proportionality factor namely a zs was named as roughness geometry function from nikora et al 2001 2007 it can be calculated as 4 a z s a s z z s a s z z s a g z z s where as z zs and ag z zs are the planar areas of the bed covered by sand and gravel respectively in fact the roughness geometry function is equal to the cumulative probability distribution of bed fluctuations whose importance in the quantification of sediment mixture transport has been brought by parker 2008 and the references therein kuhnle et al 2013 assumed that the roughness geometry function can be used effectively as a scale factor for the distribution of total shear stress below gravel crest they proposed a formula to estimate the fine sand transport rate from their experimental observations 5 ϕ b s q b s s 1 g d s 0 5 ρ s d s 2 29 10 5 a z s 2 14 τ b τ c s 3 49 τ b is the total bed shear stress τ cs is the critical shields stress for the sand when covering the crest of the gravels ds is the diameter of fine sand ρ s is the mass density of the fine sand and a zs is the roughness geometry function at the mean sand level calculated from eq 4 wren et al 2014 extended their experiments to equilibrium fine sand bedload transport over larger cobble bed and concluded that the general format of the formula 5 could be preserved but the exponents need to be changed to fit their new observations in another investigation kuhnle et al 2016 sought the maximum erosion depth of sand from interstices of immobile gravel elements the sand in each experiment initially filled the voids of larger elements up to their crest and no sediment was supplied into the channel the erosion of the sand continued until no sand was running out the flume anymore they estimated the shear velocity of the new sand level with the formula 3 and assumed that the erosion of the sand proceeds until the shear velocity in the gravel was equal to the fall velocity of the sand times a constant by calibrating this constant from experimental data they could calculate the maximum clean out depth from the combination of formula 3 and inverse of cumulative probability distribution of the bed fluctuations the common feature of all the above reported investigations is inclusion of roughness characteristics of the immobile phase into the transport rate of the mobile phase the effect of the arrangement and proximity of the obstacles is a controlling factor in trailing vortices hence mass and momentum transfer in the bed region iversen et al 1990 one of the factors that need more attention in sediment transport in fluvial environments is the relative submergence or the depth of fluid divided by size of the immobile element all the mentioned investigations were dedicated to flows with high relative submergence whereas for mountainous rivers the depth of the flow may be in the order of magnitude of grain size the significance of relative submergence and its effect on flow hydrodynamics has been elucidated by nikora et al 2001 2007 according to nikora et al 2001 the flow over a rough substrate can be divided into several subregions along the vertical and depending on the degree of perturbation of flow by roughness elements these subregions may or may not exist spatially averaging of the reynolds averaged navier stokes rans equations they derived a new set of equations which include new terms of form induced stresses and pressure drag in the momentum equations spatial averaging is performed in two steps first the navier stokes equations that fully describe the motion of flow over rough bed are averaged in time which is conventionally called reynolds average navier stokes equations rans and in the next step the time averaged variables are averaged spatially on a very thin plane parallel to the bed this plane will be continuous above the crest elements and disconnected below the gravel crest according to nikora et al 2001 four subregions can be recognized an interfacial subregion between roughness crests and troughs a form induced sublayer in which form induced stresses have appreciable significance a logarithmic region where form induced stresses start to vanish and a typical overlap region similar to that in hydraulically smooth flows develops and finally an outer flow region that is affected by free stream velocity rather than near bed velocity scales for high relative submergence flows over a rough bed the four sublayers coexist however for very shallow flows only the first two sublayers near the bed may form bayazit 1982 examined the threshold behavior of a sphere in a very shallow open channel flow over very rough bed and observed that the non dimensional shields stress parameter increased from 0 04 up to 0 12 when the relative submergence was reduced to 3 3 the effect of relative submergence on threshold shear stress of uniform and mixture particles was also investigated and proved by shvidchenko 2000 bayazit and denker 1980 reported that bed load discharge for a particle on a very rough channel may reduce up to thirty times when the relative submergence fell below five this paper is aimed at presenting laboratory experiments for estimation of bed shear stress for threshold of movement of sand at various levels below immobile roughness elements deterministic bedload transport formula should be corrected for when sand is protected by the larger elements little data exist in the literature for low submergence ratio flows the experiments of yager et al 2007 addresses the problem but the size ratio between the mobile and immobile phases is limited to around 0 1 and also all the experiments were performed for supercritical conditions presenting flow field measurements using ultrasonic velocity profiler uvp at the critical condition of the bed the unique property of the uvp is its non intrusive nature the measurements in very shallow flow depths is almost impossible with intrusive instruments such as adv acoustic doppler velocimeter we were able to measure the instantaneous velocity profiles whose time and spatially averaged quantities are reported here testing various drag partitioning approaches and compare them to direct numerical simulation dns results from the literature 2 methodology and materials our experiments were designed based on the same ideas of lyles et al 1974 iversen et al 1990 and kuhnle et al 2016 the immobile elements are covered to the crest by fine sand and a flow is set in a channel then erosion of the sand layer begins and in time the level of the sand drops below of the immobile elements crest by maintaining a uniform flow during the erosion process the movement of sediment proceeds to a level for which threshold condition can be assumed the laboratory experiments were conducted in a 600 cm meter long 40 cm wide water recirculating open channel flume with plexiglas side walls fig 1 water was pumped from an underground storage tank a magnetic flow meter was used to control the discharge magnitude into the channel at the entrance of the channel a distilling tank and flow straighteners were used to guide the flow smoothly into the channel the immobile substrate was prepared using two layers of spherical plastic balls with diameter of 50 mm an aluminum rod was used to pierce a hole in a wooden plate attached to the flume bed on one side the steel rod was perforated through the plastic balls on its other side to keep them firm in the main channel the pores of the first 120 cm of the flume were filled with crushed gravel particles so that it is ensured that fully developed flow condition prevailed in the working section because of the length of the channel usually a tailgate was needed to establish uniform flow conditions uniform sand of 0 5 mm diameter was used as the transported material three sets of experiments were performed for three different bed slopes several longitudinal sections can be identified in a typical non equilibrium sediment transport experiment just downstream of the rigid apron and at the beginning of the movable bed region a pit usually formed whose length is called a saturation length a fluid vortex governs the flow field in this region after that an equilibrium region is formed in which the sediment transport rate reaches an equilibrium with the transport capacity of the imposed flow all measurements and observations during the experiments are reported for the equilibrium region and no attempt was made to record bed geometry or flow features in the saturation length region another feature of non equilibrium sediment transport in the experiments was evidence of formation of barchan dunes this happened at the beginning of the experiments when the sand completely filled the voids up to the crest of the spheres table 1 shows the total combination of parameters for the laboratory experiments in this table the flow depth h is indicating the height of the water above the crest of the elements average velocity is based on uave q bh in which q is the total discharge measured by the flow meter h is the water depth on the crest and b is the width of the channel the bed shear velocity is computed as u g h s 0 in which g is acceleration due to gravity and s 0 is the slope of the channel at the crest of the spheres the flow depth ranged between 13 and 78 mm giving a minimum aspect ratio the width of the channel divided by the flow depth over crest equal to 5 13 at the beginning of each experimental run the substrate was filled up to the crest of the immobile elements no sediment was supplied from upstream so only sediment was transported due to bed degradation the range of the discharges and sediment sizes were chosen to ensure that sediment particles were in frequent contact with the bed each series of experiments was started by setting a low steady discharge and uniform flow was established during initial stages of the experiment by adjusting the tailgate in a typical experiment the sediment discharge was declining exponentially in time until it would reach to almost 1 of that at the beginning of the experiment in this condition it was assumed that the bed had reached a static equilibrium for which a threshold bulk shear stress can be assigned for that specific flow and geometrical conditions after reaching the static equilibrium the flow field was measured using an ultrasonic velocity profiler uvp the water was allowed to drain and the bed topography was captured using a laser scanner these steps completed the data recording for a run another run of the experiment was established by the next higher discharge this brought the sand level lower and to a new equilibrium condition fig 2 shows planar photos of the bed during the degradation stage and a typical transport rate variation during one experiment 3 results and discussion 3 1 bed level changes a laser scanning of the bed was performed as the bed reached the final stage of erosion at the end of each experimental run the area of 1000 mm in 350 mm was selected for scanning the recording interval of the scanner was 1 mm in longitudinal direction and 1 mm in traverse direction fig 3 shows configuration of the bed from the laser scanner the probability distribution function of the bed elevation and cumulative probability distribution function or roughness geometry function as shown in the picture the pdf of the bed level elevation is changing from normal distribution at the beginning of the experiments when the substrate is filled up to the crest to a bimodal distribution as the sand is eroded from the interstices the bimodality increases as the sand level goes down for higher discharges the roughness geometry function experiences an inflection when crossing from one modality to the other the slope of this transition line reduces as the bimodality increases when sand level reaches deeper depths bellow the crest of non erodible elements 3 2 flow field measurements one of the main challenges in the quantification of sediment transport in high gradient gravel bed rivers is dealing with shallow flow depths of the order of the size of the gravel elements these ranges of depth happen in the roughness sublayer and velocity distribution and friction factor are completely different from that of the same stream with much more submergence depths especially in the configuration of our experiments even the minimum depth of 20 mm caused an erosion depth of comparable magnitude below the crest therefore besides perturbation of roughness elements into the flow depth above the crest there is also an interaction of the surface and subsurface flow regions giving rise into a mass and momentum exchange an ultrasonic velocity profiler uvp was used for recording the velocity field uvp is able to measure the instantaneous one dimensional velocity along a line an ultrasound pulse is emitted from a transducer into the liquid and the same transducer receives the echoes which originate from tracer particles suspended in the fluid more details of the working principle of a uvp can be found in takeda 1995 uvp is very useful for its almost non intrusive advantages and also the fact that we had very shallow flow which makes applicability of other instruments such as acoustic doppler velocimeter adv impossible velocity profiles were recorded on a grid comprising of nine points as shown in fig 4 the minimum elevation on the sand layer that could be captured varied between points due to the microtopography of sand layer usually the emitted pulse from the uvp had intersections with these points on the bed with elevations above the average elevation the recording time of the velocity data was almost four minutes to effectively produce stationary time series of velocity data an approximate criterion ηu ν 50 bandyopadhyay 1987 ensured that all the measurements are in fully rough turbulent flow conditions η u and ν are the level of the sand below crest the bed shear velocity and water kinematic viscosity respectively fig 4 shows the time averaged velocity profile for three different sets of experiments i e q 4l s q 6l s and q 8 l s for a constant bed slope of s 0 005 the vertical axis shows the elevation from the crest of the spheres mm and the horizontal axis shows the time averaged longitudinal velocity mm s in all three discharges and above the crest there is a sharp gradient in velocity profile on the crest of the sphere following a segment with lower gradient below the crest the velocity distribution can be both linear q 4l s and exponential q 6l s q 8l s this substantiates the role of fluid flow below the crest which must be considered for quantification of mass and momentum transport in integrated shallow water equations and sediment conservation equation fig 5 shows the spatially averaged flow velocity above crest for the three discharges whose local velocity distributions were reported in fig 5 recently spatially averaged representations of the flow field equations have been proven useful for flows with heterogeneous beds rooted in porous media research they have found their path toward flow over rough surfaces in hydraulics nikora et al 2001 flow field statistics near the bed have important implications for sediment transport formulations usually the probability density function of near bed flow variables such as instantaneous velocity distribution and instantaneous bed shear stress distribution are used in formulating the pickup probability of bedload formulas and threshold criterion analysis fig 6 shows the pdf of instantaneous streamwise velocity component for different regions of the flow along the beam of the uvp as fig 6 shows in most of the flow depth the normal pdf prevails however very close to the bed the pdf is deviating from normal distribution and a stable distribution could be fit to the pdf curve this must be considered in stochastic formulation of above threshold bedload transport formulas 3 3 critical bed shear stress the pattern of sand erosion around and between the immobile elements was not uniform so measurements of the erosion depth bellow the crest was averaged spatially in rows between elements the spatially averaged erosion depth below the crest is plotted against the dimensionless bed shear stress θ critical τ critical ρ s ρ gds in fig 7 on the vertical axis the level of the sand below the crest η is non dimensionalized with the non erodible phase diameter dg the variations could be fitted with a michaelis menten equation 6 η d g a θ e x c e s s b θ e x c e s s where θ excess θ critical θ critical surface θ critical the dimensionless critical shear stress at sand level θ critical surface dimensionless critical shear stress of the sand at the level of the crest of the non erodible phase which was estimated using shields diagram a 0 989 and b 0 06207 3 4 drag partitioning approaches there is still much debate about the partitioning of the drag in complex rough wall flows various researchers have approached the problem of drag partitioning on rough substrates ranging from direct analysis of the governing equations nikora et al 2019 and simple theoretical expressions validated by experiments raupach 1992 to direct measurements of forces in the laboratory experiments wooding et al 1973 the total drag τ b can be divided into pressure drag term τ r on roughness elements plus a skin drag term τ s on the intervening surface between elements the geometry arrangement proximity and planar concentration of obstacles contribute to this distribution there are several drag partitioning approaches in the literature formulated for regular arrangements of cylinders cubes and spheres these approaches for drag partitioning have been mostly formulated based on roughness density of the surface elements i e the total surface area of the elements projected on a vertical plane divided by the total horizontal surface area fig 8 shows the ratio of pressure drag τ r to total drag as a function of roughness density frontal area per unit ground area for drag partitioning methods developed by wooding et al 1973 arya 1975 raupach 1992 and shao and yang 2005 all these approaches have been developed in meteorological community and all except that of shao and yang 2005 consider a sparse distribution of the elements for comparison we have also included the relevant parameter from direct numerical simulations of leonardi and castro 2010 and wu et al 2020 as it is seen in the figure the methods of raupach 1992 and shao and yang 2005 are closer to the dns data of leonardi and castro 2010 but departs from that of wu et al 2020 probably due to different obstacle shapes cubes against hemispheres reynolds numbers and flow regime fully rough and transitionally rough we could not assess the applicability of these drag partitioning approaches to our experiments the drag partitioning of kuhnle et al 2013 2015 considers a multiplication of roughness geometry function a z s a s z z s a s z z s a g z z s and total wall stress to compute the shear at the sand level this might be effective in cases where a zs at the mean sand level is a decreasing function of elevation but for example if the immobile roughness elements are cubes this formula gives a fixed shear stress for all levels of the sand or it only works for spherical elements to some certain depth the range of reynolds number r e u a v e h ν in our experiments lies between 3500 and 34 000 uave is the cross sectional average velocity h is the flow depth and ν is the kinematic viscosity the lower margin belongs to sand levels near the crest and higher reynolds numbers to deeper sand levels this might result in a friction dominated flow for all our experiments or at least for lower reynolds numbers this can only be clarified by conducting high resolution numerical modeling using dns or les large eddy simulation with considering all the bed undulations because they contribute to form and skin drag the recorded bathymetry can be input to the model and velocity profile data captured by the doppler profiler can be used for verification of the models 4 conclusions the paper aims to understand the hydrodynamics and sediment transport phenomenon at the interface of immobile gravel substrate and the overlying shallow fluid flow several shortcomings of the previous approaches were discussed here most sediment transport formulas neglect the effect of relative submergence and concentration of the immobile phase of the bed materials there are several deterministic and stochastic approaches for sediment transport calculations which neglect these aspects regarding stochastic approaches most of them are based on normal pdf instantaneous velocity distributions while our preliminary observations shows that there is deviation from gaussian pdf for near bed velocities fig 6 we performed experiments to contribute to data for threshold behavior of the fine sand for implementing into deterministic bedload transport formulas a nonlinear regression curve was fit to the threshold shear stress data fig 7 most of the drag partitioning approaches in the literature are devoted to sparse roughness densities and they were shown to contradict with results from direct numerical simulation dns a drag partitioning methodology based on physics of depth limited water flow on rough substrates is needed a high resolution numerical simulation which can adequately consider the bed undulations and compatible with the recorded velocities is envisaged for our future research ds sand diameter dg gravel diameter ρ g gravel elements mass density qs sand transport rate ρ s sand mass density μ water viscosity ρ water mass density u characteristic flow velocity j water surface slope φ b s dimensioless sediment transport rate θ s non dimensional skin shear stress θ cr s corrected corrected non dimensional shear stress ps surface fraction of sand ξ s hiding factor for sand zt t level of the sand as z zs plan area of the portion of the bed covered by sand ag z zs plan area of the portion of the bed covered by gravel τ b bed shear stress τ cs critical shields stress for the sand h height of the water above the crest of the elements q total water discharge b width of the channel g acceleration due to gravity s 0 slope of the channel u c critical shear velocity for the sand level u cs critical bed shear velocity for the fine sand covering the crest τ r pressure drag term τ s skin drag term af frontal area λ e effective density r radius of spheres equal to 25mm η distance between surface of the sand and crest of the gravel θexcess bed shear stress credit authorship contribution statement mahdi khademishamami conceptualization methodology software formal analysis validation investigation resources data curation writing original draft william nardin writing review editing declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgement the first author is grateful to the research faculty and technical staff of hydraulics laboratory of department of civil engineering at the university of trento for their help and support for providing and preparation of the experimental facilities supplementary materials supplementary material associated with this article can be found in the online version at doi 10 1016 j advwatres 2020 103659 appendix supplementary materials image application 1 
473,a three scale model for flow in karst conduit networks in fractured carbonates is rigorously constructed based on a reiterated homogenization procedure the first upscaling performed from the high fidelity flow model is based on sequential partially and fully topological model reduction procedures considering two discrete networks of solution enlarged fractures and conduits the subsequent macroscopization procedure projects the reduced model into the cells of a coarse computational grid where homogenized equivalent properties are numerically constructed such a two level upscaling gives rise to a macroscopic flow model characterized by mass transfer functions between the geological structures a notable consequence of the approach proposed herein is the appearance of a new karst index concept whose underlying physics relies on the generalization of the traditional peaceman s theory of well index such a concept rules the mass exchange between conduits and matrix and can be extended to the general scenario of coupled flow in multi branch karst conduit systems displaying general non circular cross sections and surrounding damage zones the downscaling representation for the karst index can be further explored to improve accuracy of the exchange coefficient between the geological objects numerical experiments are carried out showing the magnitude of the index for certain conduit and fracture arrangements along with illustrating the impact upon flow patterns keywords karst conduits carbonate rocks multiscale coupled flow model reduction well index karst hydrogeology 1 introduction fractured and karstified carbonate reservoirs are extremely heterogeneous and exhibit pore spaces with different sizes geometries and connections extremely difficult to characterize and predict in the subsurface see e g baomin and jingjiang 2009 the pore space is formed during primary sediment deposition followed by coupled deposition and burial which give rise to secondary porosity associated with these sediments moore 1989 karstification occurs during infiltration of aggressive fluids that dissolve the host rock palmer 1991 which is mostly developed along fractures and sedimentary bedding in carbonate rocks and is caused by fluids from the surface epigenic karst or ascending fluids from the subsurface hypogenic karst klimchouk et al 2016 such a complex scenario leads to the appearance of highly irregular dissolution structures in carbonate rocks commonly referred to as karst systems whose associated network dominated by interconnected cave conduits may provide pathways of high conductivity for localized flow such systems may also extend towards the surface through sinkholes and springs palmer 1991 throughout the manuscript we adopt the karst terminology to designate a geologic environment characterized by the abundance of aqueous dissolution processes in underground conditions that propagate progressively through the host rock mostly along a highly fractured dissolutionally modified carbonate rock annable 2003 the macroscopic geobodies originated from karstification consist of coalesced cave passages referred to as karst conduits intertwined by a network of enlarged fractures and bedding planes loucks 2007 the conduit network forms macropores and commonly exhibits self organization features displaying long correlations in the permeability field which give rise to localized flow pathways saller et al 2013 in this setting pore systems may differ from each other by several orders of magnitude giving rise to triple porosity permeability systems worthington et al 2000 wu et al 2011 the classification of cave patterns was pioneered by palmer 1991 who analyzed several thousands of cave passages and proposed different self organized structures to describe epigenic dissolution caused by meteoric fluids from the surface and hypogenic karst dissolution caused by ascending fluids from the subsurface to represent the entire cave network system see klimchouk et al 2012 and references therein for an exhaustive review in addition annable 2003 provided an exhaustive overview of the evolution of the conceptual models to describe the formation of karst systems among other effects fracture density in the host rock is one of the most salient features that controls their evolution in this context fracture aperture and bedding partings are enlarged preferentially in the vicinity of prominent faults leading to the appearance of collapsed cave complex distributions loucks 1999 the geometric and hydraulic properties of the conduit network play a determining role in flow regimes for instance cave diameter values usually lie in the 1 to 10 m range and consequently exhibit enormous potential for localized fluid flow saller et al 2013 which entail explicit treatment in the hydrodynamic model moreover abrupt variations in the cross sectional area of the conduit system along with small scale irregularity wall roughness may also have a profound impact on flow and transport regimes and therefore their effects upon flow patterns must be carefully assessed peterson and wicks 2006 a wide variety of formulations has been proposed to describe coupled flow in matrix conduit enlarged fracture systems see e g cao et al 2011 and nordbotten and boon 2017 for survey on mixed dimensional pdes within this framework conduit has been treated as a discrete pipe network with 1d high permeability cao et al 2011 in addition continuum formulations have been postulated where under the pseudo steady regime mass transfer component is constitutively given in terms of an exchange coefficient whose inverse plays the rule of a conduit resistance multiplied by the pressure difference between the two sub structures d angelo and quarteroni 2008 gjerde et al 2019 in a similar fashion hybrid hierarchical models have been developed in vidotto et al 2019 with emphasis on flow in blood networks containing 1d larger vessels coupled with homogenized capillaries and tissue in spite of well established the accuracy and reliability of the aforementioned models can be somewhat improved by bridging the effective medium behavior with the local description at the finer scales such a bridging consists in one of the main objectives of this work and leads to the birth of a new class of three scale models for flow in coalesced collapsed paleocaves lopes et al 2019 the related hierarchy of length scales naturally appears and is depicted in fig 1 at the finest resolution o 10 2 m the high fidelity hydrodynamics is posed wherein the geological structures are envisioned as 3d objects percolated by coupled free porous media flows popov et al 2009 which may also include inertial effects bailly et al 2009 at the intermediate description o 10 0 m a model reduction is performed resulting in hydrodynamics governed by coupled 3d 2d 1d flows in matrix fracture conduit respectively such a reduced model exhibits a lower computational cost and aims at inheriting the main patterns of the high fidelity flow characterized by highly localized mass interchange functions described by line source dirac measures along the conduit lines furthermore in the meso macro upscaling an additional length scale is introduced with characteristic length associated with a typical grid block of a coarse computational mesh o 10 1 m in this macroscopic context the conduit network is embedded in a coarse grid and we proceed in a similar fashion to the traditional peaceman s well index concept extended in wolfsteiner et al 2003 to unconventional wells and devise a new macroscopic conduit fracture matrix exchange coefficient which gives rise to an emerging concept referred herein to as karst index within this framework our aim consists in taking preliminary steps toward addressing the three scale description underlying the new karst index concept to describe mutual mass interchange between conduit enlarged fracture matrix systems our striking feature consists of the rigorous development of the 3d 2d 1d flow models at different scales based on combination of topological model reduction and source based upscaling methods numerical examples illustrate the potential of the proposed methodology in describing flows in karst systems in particular in the simulations we consider the late cretaceous carbonate rocks of the jandaíra formation in the potiguar basin semiarid brazil as an analogue of karstified and fractured reservoir from which we extract real input data 2 fine scale high fidelity model we begin by presenting the microscopic high fidelity model where all geobodies are treated as 3d objects in order to focus on our main target for the sake of simplicity throughout the manuscript exterior sources sinks are neglected and neither gravitational nor inertial effects are taken into account in the momentum equations we then consider fully saturated darcy regime with flow solely driven by a pressure gradient the model can easily be extended to allow for higher reynolds numbers and to incorporate brinkman and capillary effects in shallow non saturated flows governed by richard s equation our subsequent developments can be applied to both aquifers and carbonate reservoirs with the proper selection of the input hydraulic parameters let ω r 3 be the microscopic domain with boundary ω occupied by a karst system saturated by a slightly compressible liquid the domain can be reconstructed by the union of subdomains ω ω m ω f ω c occupied by the intact rock matrix the network of enlarged fractures and the conduit system respectively the latter representing the coalesced cave complex fig 1 denoting k β the pair of permeability and total compressibility parameters the restriction to each subdomain designated by the subscript i reads as 1 β x β i x k x k i x x ω i i m f c further denoting p v μ the pore pressure darcy s velocity and the fluid viscosity the local hydrodynamics is governed by the system 2 β x p t v 0 in ω v k x μ p supplemented by boundary conditions on ω and initial conditions 3 model reduction we now proceed within the framework of the micro meso upscaling by developing two distinct levels of model reduction in the sequel we begin by constructing the partially reduced model where only the elements of the enlarged fractures network are treated as n 1 dimensional objects n 2 3 subsequently we develop the fully reduced model where on addition to the fracture network the conduit system is reduced to a 1d curvilinear geobody 3 1 partially reduced model the partially reduced model can be accomplished by extending the discrete fracture model dfm presented in frih et al 2008 to incorporate the hydraulic interaction between fractures and conduits in addition to the fracture matrix mass transfer following the usual framework of dfm type models each fracture divides the domain ω m into two subdomains ω m1 and ω m2 in addition to establish the proper mathematical setting fractures are virtually extended toward the outer boundary ω m see ganis et al 2014 for details the matrix fractures interfaces are denoted by γ f m j ω f ω m j j 1 2 in a similar fashion to incorporate fracture conduit microscopic interactions each fracture divides the conduit subdomain ω c into ω c1 and ω c2 giving rise to the interfaces γ f c j ω f ω c j j 1 2 see fig 2 denote n f and n c the unitary normal outward to ω f and ω c respectively by postulating continuity of pressures and normal velocities at γ fi i m c the flow model 2 can be recast in the form 3 β i p i t v i 0 in ω i i m 1 m 2 f c 1 c 2 v i k i μ p i in ω i i m 1 m 2 f c 1 c 2 p i j p f on γ f i j i m c j 1 2 v i j n f v f n f on γ f i j i m c j 1 2 p c p m on γ c m v c n c v m n c on γ c m hence following the usual dfm framework denote n and τ the spatial operators in the normal direction and tangential plane respectively and proceed by decomposing the velocity field in the form v f n v f n τ v f τ where the additional subscripts designate normal and tangential components respectively mass conservation for the fractures reads as β f p f t n v f n τ v f τ 0 in ω f the domain ω f r 3 is then collapsed into γ f r 2 with hydrodynamics represented in terms of averaged fields over the fracture aperture d 4 v f τ 1 d d 2 d 2 v f τ d n p f 1 d d 2 d 2 p f d n thus considering γ f γ f m γ f c given by the union of the suddomains intercepted by the other geobodies integrating the mass conservation equation in the normal direction gives d β f p f t v i x n f γ f τ d v f τ 0 in γ f where v i n f γ f i m c designate the jump across γf 5 v i x n f γ f v m 1 n f 1 γ f m v m 2 n f 2 γ f m x γ f m v c 1 n f 1 γ f c v c 2 n f 2 γ f c x γ f c with n f 1 n f 2 n f and n c 1 n c 2 n c in addition normal and tangential components of darcy s law are given by 6 v f τ k f τ μ τ p f 7 v f n k f n μ n p f with k f τ and k f n the permeabilities in the tangential and normal directions respectively hence integrating 6 and assuming homogeneous permeability in the normal direction leads to the reduced form 8 v f d v f τ d 2 d 2 v f τ d n d k f τ μ τ p f in γ f it remains now to build up a constitutive law for the pressures jump on γf to this end we average 7 in the direction normal to the fracture and use pressure continuity on γf this yields d 2 d 2 v f n n f d n k f n μ p f γ f i 2 p f γ f i 1 k f n μ p i 2 γ f i 2 p i 1 γ f i 1 i m c hereafter for the sake of simplicity we shall focus on the case of highly permeable fractures where pressure continuity on γ fi can be enforced frih et al 2008 the general case including seal fractures can be further accomplished using the analysis developed in martin et al 2005 9 p f p i 1 γ f i 1 p i 2 γ f i 2 i m c hence the partially reduced flow model is governed by the system 10 β i p i t v i 0 in ω i i m c v i k i μ p i in ω i i m c d β f p f t τ v f v i n f γ f in γ f i v i x n f γ f v i 1 n f 1 γ f i v i 2 n f 2 γ f i i m c v f d k f τ μ τ p f in γ f p f p i on γ f i i m c p c p m on γ c m v c n c v m n c on γ c m 3 2 fully reduced model we now proceed with the construction of the fully reduced model where in addition to fractures the elements of the conduit network are also collapsed to 1d objects fig 3 in this r 3 r model reduction procedure denote l the length of the conduit and begin by attaching a local system of curvilinear coordinates r θ s with r and θ the radial and angular components within each cross section and s 0 l the arclength in this context in order to localize the elements of the surface of the conduit we introduce the radius function r 0 2 π 0 l r with θ s r θ s the microscopic domain occupied by the conduit ω c along with the center line γc where the reduced model shall be posed fig 4 are characterized by ω c x r θ s r 3 r r γ c x r θ s ω c r 0 s 0 l in addition for a fixed arc length s the intersection curve with the surface of the conduit c s together with the cross section ac s such that ω c a c 0 l can be represented as c s x r θ r 0 2 π r r a c s x r θ r 0 2 π r r in order to constrain flow to the 3d conduit network set i c in 10 to obtain 11 β c p c t v c 0 in ω c v c k c μ p c v c n c v m n c on γ c m p c p m on γ c m p c p f on γ c f in a similar fashion to the dfm procedure decompose the spatial operators and velocity into axial curvilinear s and transverse components t r θ in each cross section this yields 12 s t s t v c v c s v c t k c k c s k c t thus from 11 and 12 we have 13 β c p c t t v c t s v c s 0 in ω c v c t k c t μ t p c v c s k c s μ s p c the reduced equations can be obtained by averaging the flow model across the cross section of the conduit ac to this end in the developments that follow we assume the necessary regularity related to continuity of pressure and its time derivative in order to interchange integration and differentiation in the storage component thus denoting p c and v c the averaged pressure and velocity and ac the area we have 14 p c 1 a c a c p c d a v c 1 a c a c v c s d a hence denoting n c the unit normal on the intersection curve c outward to ω c and assuming βc constant in ac by averaging 13 using the continuity conditions furnishes for each arc length coordinate s 0 l 15 β c p c t s v c 1 a c a c t v c t d a 1 a c c v c t n c d c in the analysis that follows we decompose the center line of the conduit in the form γ c γ c m γ c f to distinguish the conduit subsets intercepted by matrix and fractures in this setting the source term in the rhs admits the decomposition 16 1 a c c v c t n c d c a c v m n c x γ c m a c d v f n c x γ c f where a c s c a c denotes the surface area density of the conduit c the length of c s on the conduit surface and f c the length average of a quantity v n c along c 17 v m n c s 1 c 0 2 π v n c r θ d θ with c s 0 2 π r θ d θ it should be noted that in the source component stemming from the mass exchange with the fracture network 16 the high fidelity velocity was replaced by the averaged counterpart previously computed with the dfm model thus assuming constant axial permeability in each cross section k c s k c s s we have from darcy s law 18 v c 1 a c a c v c s d a 1 a c a c k c s μ s p c d a k c s μ s p c thus introducing the partial sources 19 q c m x γ c m c v m n c d c q c f x γ c f 1 d c v f n c d c with q c i x γ c j 0 for i j along with the overall conduit source per unit of area q c q c m a c for x γ c m q c f a c for x γ c f the reduced hydrodynamics in the 1d domain occupied by the conduit is given by 20 β c p c t s v c q c in γ c v c k c s μ s p c it remains to analyse the influence of the topological reduction of the conduit system on the source term in the partially reduced model in 10 to this end we proceed by decomposing the integral form of mass balance in the composed domain ω c ω m in the form 21 β m ω m p m t d ω ω m v m d ω β c ω c p c t d ω ω c v c d ω now denote ω m and the pair pm v m the homogenized matrix domain with associated pressure and velocity fields under the topological reduction ω c γc hence adopting analogous decomposition for the conduit length l l m l f and using the 1d mass balance in 20 we have after taking the limit within the reduction procedure lim ω m ω m β m ω m p m t d ω ω m v m d ω lim ω c γ c γ c m a c β c p c t v c d a d s 0 l m β c p c t s v c a c s d s 0 l m q c m s d s thus mass conservation in the matrix with embedded conduit system can be recast in the form 22 β m ω m p m t d ω ω m v m d ω 0 l m q c m s d s in an analogous manner we have for the conduit line embedded in the planar fractures γ f d β f p f t d γ γ f τ v f d γ γ f v m n f γ f d γ 0 l f q c f s d s with asymptotic properties designated by the subscript f for conciseness of notation the tilde symbol was suppressed from the mean variables thus introducing the line delta function δ λ l 2 associated with the curve δc embedded in ω m as the distribution satisfying 23 ω m δ λ f d ω δ c m f s d s 0 l m f s d s and the lower dimensional counterpart δλ l 1 associated with the planar domain γf 24 γ f δ λ f d γ γ c f f s d s 0 l f f s d s the reduced flow equations are given by 25 β m p m t v m q c m δ λ in ω m d β f p f t τ v f q c f δ λ q m f in γ f v f d k f τ μ τ p f in γ f v m k m μ p m in ω m β c p c t s v c q c in γ c v c k c s μ s p c in γ c with q c m c v m n c d c q c f 1 d c v f n c d c and 26 q c s 1 a c c v c n c d c q m f v m n f γ f m it should be noted that the above two scale system entails a dynamic coupling between the mesoscopic reduced model and the micro high fidelity counterpart as it requires computation of the local normal velocities on the surface of the conduit in order to calculate the sources qcm and qcf an important observation regarding the coupled 3d 2d 1d system relies on the different treatment of the sources in the flow equation in the matrix in contrast to the matrix conduit transfer function which appears as a line source within the matrix cells within the dfm framework the influence of fractures on matrix hydrodynamics appears manifested through pressure continuity on γf rather than a source component it should be noted that alternative embedded treatment of fractures in the matrix can be adopted in the context of the edfm method lee et al 2000 in what follows we show that under a time scale assumption associated with the local pseudo steady regime the reduced model can be represented in a somewhat simplified manner 3 3 lumped parameter approximation by restricting our analysis to the long term pseudo steady regime local pressure and velocity profiles exhibit a flat plateau in the center of the conduit together with sharp layers in the vicinity of the shell with averaged values decaying exponentially in time in this setting the local mass exchange can be approximated by a constitutive law dictated by a lumped parameter gap resistance enhanced by a pressure difference gjerde et al 2019 27 q c m s 1 ω c μ p c p m q c f s 1 ω c μ p c p f where p c designates the aforementioned mean over the cross section and p i p i c i m f the line averaged over the curve c defined in 17 in the above pseudo steady representation ωc represents the conduit resistance for radial flow the usual asymptotic cases ωc of nearly impermeable and ωc 0 of high conductivity can easily be reproduced 4 macroscopic model hereafter we proceed in the context of the subsequent upscaling towards the construction of the homogenized description of the conduit mass transfer functions now embedded in a coarse mesh therefore consider the macroscopic scenario wherein a discretization procedure has been adopted and the reduced 1d conduit network now appears embedded in the partition of the entire domain associated with this discretization the meso macro upscaling is based on a point to segment wise discrete treatment of the exchange parameter within the intercepted coarser cells by the conduit network wolfsteiner et al 2003 4 1 karst index concept in order to quantify the mass transfer functions in light of the characteristic length of a coarse grid in what follows we propose a generalization of the well index concept to the present context which shall be restricted to the coarse cells intercepted by the conduit network thus consider the partition of the domain into n subdomains d k k 1 n where each element may or may not intercept the conduit network γc see fig 5 for each subdomain d k define the local curve γ c k γ c d k given by the union between the intercepted subsomains γ c k γ c m k γ c f k with the total length l given by the sum of segments l k l d k each one subdivided into l m k and l f k designating the partial lengths of the matrix and fracture intersections within each cell further introduce the volume of each cell d k along with the areas of the fracture network and conduit cross section restricted to each cell a f k and a c k respectively in addition define the line average macroscopic counterparts of the sources within each cell 28 q c m k 1 γ c m k γ c k q c m d γ 1 l m k 0 l k q c m s d s q c f k 1 γ c f k γ c k q c f d γ 1 l k f 0 l k q c f s d s recalling the property q c i x γ c j 0 for i j further introduce the averaged pressures over each coarse cell p m k 1 d k d k p m d d p f k 1 a f k γ f k p f d γ under the lumped parameter pseudo steady regime the above definitions lead to a natural characterization of the karst indexes and k i m k and k i f k per unit of the corresponding lengths restricted to the cell l m k and l f k denoting p c k v c k the restriction of p c v c to each block we have 29 k i m k μ q c m k p c k p m k k i f k μ q c f k p c k p f k denoting β j k k j k j m f k 1 n the restriction of compressibility and permeability to each grid block the mass balances of the previous fully reduced model can be recast in the form 30 β m k p m k t v m k k i m k μ p c k p m k δ λ in d k k 1 n d β f k p f k t τ v f k k i f k μ p c k p f k δ λ q m f k 0 in γ f k β c k p c k t s v c k k i m k μ a c k p c k p m k k i f k μ a c k p c k p f k in γ c k with the velocities given by the macroscopic form of darcy s law the high fidelity problems for computing of the karst indexes k i i k i m f consist on solving the transient problem for each cell d k k 1 n i m f it is remarkable to see that the three scale setting also allows for a sharper representation of ki by computing the sources q c m k and q c f k directly from the high fidelity description using a straightforward integration over the surface area of the interfaces k i m k μ p c k p m k q c m k 1 l m k 0 l k q c m s d s 1 l m k 0 l k c v m n c d c d s 1 l m k γ c m k v m n c d γ where γ c m k denotes the restriction of the microscopic conduit matrix interface to each cell the above closure relation can be envisioned as a double down scaling procedure for assessing the karst index such a representation provides an accurate form for computing the exchange coefficient between discrete conduit networks and continuum matrix see saller et al 2013 for circular cross sections the downscaling representations 29 for k i m k and k i f k resemble in form the ones derived in karimi fard and durlofsky 2012 for the well index constructed within the framework of the near well upscaling method in the case of single phase flow in the darcyan regime the fully reduced formulation proposed herein allows for the construction of a family of hierarchical models capable of handling general non asymmetric cross sections of the karst conduit it should be noted that by invoking 26 the exchange functions qc and qcm can be computed without assuming pseudo steady regime moreover within the context proposed herein it is worthwhile highlighting that the collapsed breccia zone in the vicinity of the conduit described in loucks 1999 can be incorporated in a straightforward fashion in terms of a skin factor 5 numerical examples hereafter we illustrate the numerical performance of the three scale model initially preliminary experiments are conducted in order to study the sensitivity of the magnitude of the karst index with different scenarios involved in this setting we begin by considering flow in circular and elliptical sinkholes as depicted in menezes et al 2020 subsequently dynamic simulations are carried out aiming at illustrating the performance of the fully reduced model karst index computations the computations of the karst index are carried out at the finest scale where the local pressure and velocity profiles are computed by solving the high fidelity diffusion equations in the matrix and computing the flow rate at the conduit interfaces γ c i i m f we then have 31 β i p i t v i 0 v i k i μ p i in ω i supplemented by boundary and initial conditions 32 p x t p x on γ d p v d n q on γ n p p x 0 p 0 x in ω with γ d p and γ n p the portions of the outer boundary of the grid where dirichlet and neumann conditions are enforced after solving the high fidelity flow the index can be computed within a post processing averaging over the cell by invoking definitions 29 the numerical experiments that follow are carried out considering discretization performed by the galerkin method using piece wise linear interpolations in triangles combined with backward euler time stepping method 5 1 circular sinkhole our first goal is to illustrate that in the oversimplified scenario of a vertical sinkhole with constant circular cross section the transient karst index reduces to the traditional well index in this context w i 2 π k l z ln r e r w denotes the classical well index representation with lz the vertical length of the well in the subsequent 2d simulations we select h 500 m the cell size r w 3 1 10 2 m the well radius r e 0 20788 h the equivalent radius k 1 0 10 15 m2 the permeability of the intact rock and μ 1 0 10 3 pa s the fluid viscosity the vertical karst lies in the center of the 2d squared subdomain which is partitioned using an unstructured triangular mesh progressively more refined in the vicinity of the karst fig 6 dirichlet boundary conditions are prescribed on γ cm while the external boundaries are considered sealed fig 7 we perform transient simulations considering a planar cell of 500 m x 500 m for a time interval i 0 t 5 10 6 s in fig 8 we display the time evolution of the ki towards its stationary value in the same plot we compare with peaceman s results based on equivalent radius we may observe that as time evolves ki t tends to a value close to the corresponding analytical well index w i 7 74033 10 16 m 3 5 2 elliptical sinkhole in our second experiment we consider a similar numerical example of flow around a non circular sinkhole the scenario stems from the drone image shown in fig 9 which corresponds to the well known xavier sinkhole located in the north state of rio grande do norte brazil the right picture highlights three distinct fracture sets group of fractures with similar behavior or orientation the first and second sets are composed of pervasive fractures throughout the outcrop which were formed before the collapse of the sinkhole and are n s to nne ssw oriented and e w to ese wnw oriented respectively the third more pronounced set is composed of collapse induced fractures which form a concentric and semicircular curved set concentrated around the collapsed zone this type of curved fractures occurs in carbonate units in semiarid brazil menezes et al 2020 and also resemble the cylindrical faults found in several paleokarst oil reservoirs such as those in the lower ordovician ellenburger group in west texas loucks 1999 our methodology is based on a hierarchical approach where the smaller pre existing structures highlighted in green and blue are homogenized to a lattice of associated equivalent permeabilities using the simple oda s analytical upscaling oda 1985 in contrast the more prominent collapsed fractures highlighted in red are explicitly accounted for in the fine scale simulation within the dfm framework the model was implemented adopting the input values k f 8 333 10 10 m 2 and d 1 10 4 m for permeability and aperture respectively of both pre existing and collapsed induced fractures the computed equivalent permeabilities associated with the pre existing micro fractures using oda s method are given by k x 1 132 10 14 m 2 and k y 1 201 10 14 m 2 considering a 100 m 100 m cell in fig 10 we depict the distinct subdomains adopted in the subsequent flow based upscaling the red portion labeled with material 4 in table 1 represents the interior of the sinkhole in this scenario filled with sand whereas the orange part material 3 highlights the region filled by collapsed fractures whose reduced elements are treated explicitly within the framework of dfm in particular in our analogue reservoir in semiarid brazil the sand portion stems from small nearby streams captured by sinkholes silva et al 2017 the subregion highlighted in light blue material 2 is occupied by pre existing fractures previously homogenized in a straightforward fashion using oda s method finally the blue area material 1 stands for the intact rock in the last column of table 1 we show the permeability values corresponding to each subregion likewise the previous example we compute the index k i c m k with k 1 for the single simulation cell containing the sinkhole with γ ω and γ cm denoting the exterior boundary and the interface between the two structures respectively we apply dirichlet boundary conditions p x 56 mpa on γ and p x 55 mpa on γ c m and solve the transient problem till stationarity is reached which furnishes k i t 6 90479 10 13 m3 in addition following the usual framework of flow based upscaling combined with the dfm approach the diagonal components of the equivalent permeability are computed by imposing stationary pressure gradients in the two orthogonal directions which furnishes the values of 3 255 10 13 m 2 and 3 293 10 13 m 2 for horizontal and vertical permeabilities respectively in order to validate the homogenized model described by the upscaled values of karst index and permeabilities we perform transient simulations by solving both macroscopic and high fidelity models aiming at comparing accuracy considering an extended domain ω ω m ω c 500 m 500 m with the sinkhole lying in the center we solve the high fidelity model adopting a triangular mesh of 290 107 elements and 578 412 nodes on the other hand the resolution of the macroscopic problem requires a much coarser mesh of 25 quadrilateral bilinear elements and 36 nodes fig 11 the sub region highlighted in red corresponds to the inner domain 100 m 100 m occupied by the sinkhole an initial pressure p 0 x t 0 56 mpa along with the dirichlet value p x p x 55 mpa on the right outer boundary γ r are enforced while maintaining the remaining outer boundaries of the cell sealed for flow the high fidelity model requires enforcement of the additional dirichlet value p x p x 55 mpa on γ cm in contrast in the homogenized model influence of the pressure in the sinkhole is imposed through the source term in 30 in figs 12 and 13 we compare the time evolution of the pressure field computed from the high fidelity and macroscopic models whereas in fig 14 we display the cumulative production curves obtained with the two formulations the numerical results suggest that in spite of overlooking the local features of the flow the macroscopic formulation seated on the upscaled karst index and permeabilities is capable of providing production curves with excellent agreement with the high fidelity counterparts with considerable reduction of the computation burden 5 3 conduit enlarged fracture index we now present local simulations for computing the karst index k i f k k 1 between a conduit and an enlarged fracture in a single cell in this context we analyze the influence of the intersection angle θ between the two geobodies on the magnitude of the index see figs 15 and 16 we consider initial and conduit pressures p 0 55 mpa and p c 40 mpa along with homogeneous neumann conditions on the outer boundaries the dependence of kif on θ is displayed in fig 17 as expected the index grows with the angle since the increase in θ amplifies the contact area for mass interchange between the two objects 5 4 well karst interaction finally we present computational simulations to illustrate the performance of the fully reduced formulation the numerical example consists of depletion of a box shaped reservoir containing a single conduit induced by a production well fig 18 our aim is to analyze the hydrodynamical interactions between the well and the conduit we consider initial and well pressures p 0 55 mpa and p w 40 mpa and homogeneous neumann conditions on the outer boundaries the 3d mesh is composed of tetrahedron elements for the intact rock and 1d linear elements along the conduit the system is solved within the framework of a fully coupled formulation where the additional degrees of freedom associated with pressure in the karst conduit are solved simultaneously with the ones related to the matrix the input physical and mesh parameters are listed in table 2 we begin by considering scenario a displayed in fig 18 and compare the pressure profiles in the xy plane for a fixed depth z 5 5m intercepting the conduit in fig 19 we display the results computed using the high fidelity left and fully reduced right models for two different times we may observe that localized effects between well and conduit captured by the high fidelity model tend be to smeared out within the fully reduced approach in fig 20 we compare the cumulative production curves computed with the aforementioned micro and mesoscopic formulations in spite of the regularization effect in the vicinity of the well conduit interface we observe slight differences between production curves the fully reduced model shows ability to capture the coarse scale features of the problem with lower computational burden see mesh sizes in table 2 furthermore in fig 21 we compare the evolution of the pressure profiles along the conduit line owing to the high contrast in permeability the pressure behaves nearly constant next to the conduit showing a flat plateau with faster depletion in scenario a finally we analyze the 1d cuts in the matrix pressure profile along line l located at 50m 0 100m 2m in fig 22 we compare the aforementioned pressure profiles with the case of absence of the conduit represented by the black solid line the sharpest boundary layer case near the well corresponds to the single matrix problem in the absence of the cave system the presence of the conduit tends to split the pressure gradient in different magnitudes in the adjacent subregions in the vicinity and away from the well ywell 40m and 40m y 60m such a perturbation appears more pronounced in scenario a of smaller conduit well distance moreover in the scenario without conduit the pressure is almost constant through the element intercepted by the well y 50m 60m whereas the interaction between well and conduit in case a implies in linear profile within the elements intercepted by the well y 40m 60m it should be noted the delayed influence of the conduit on the pressure profile in scenario b which us due to the longer time in which the pressure dissipation front reaches the conduit location 6 conclusions we have considered fine scale high fidelity single phase flow models in carbonates from which we constructed partially and fully reduced mesoscale models for hydrodynamics in fractured karstified rocks in the reduced mesoscopic model solution enlarged fractures and karst conduit systems were treated as geobodies of 2d and 1d dimension respectively our framework was developed in the context of an innovative three scale flow model wherein the topological reduction procedure applied to the high fidelity equations gave rise to a coupled 3d 2d 1d model with interchange mass transfer between the three geobodies quantified by source terms in the flow equations such a formulation was explored to further characterize a macroscopic karst index in coarser cells which under the pseudo steady regime plays the role of a generalized well index valid for complex branches of non circular conduit networks computational simulations illustrate the potential of the formulation proposed herein the methodology proposed opens new directions for the construction of accurate flow models in fractured karstified carbonates displaying different levels of geological complexity in future work we shall consider more complex branched karst networks along with extensions to the nonlinear richards equation for unsaturated flow and include boarder region analysis in the sense of wen et al 2003 in order to improve accuracy of the numerical ki and consequently reduce the sensitivity of the value with imposed boundary conditions finally the multiple subregion methodology developed in karimi fard et al 2006 based on the preliminary upscaling to sub grid cells characterized by iso pressure contours consists of a very promising tool for easier implementation in the case of intensively karstified limestones credit authorship contribution statement marcio a murad conceptualization writing original draft writing review editing tuane v lopes software validation patricia a pereira software validation francisco h r bezerra data curation aline c rocha software declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments we thank the anonymous reviewers for their detailed and constructive comments which significantly improved our work this work was supported by the reservoir geology geophysic and geomechic management at cenpes petrobras under the contract number 4600556376 we also thank the brazilian agency of oil gas and biofuels agência nacional do petróleo gás e biocombustíveis anp 
473,a three scale model for flow in karst conduit networks in fractured carbonates is rigorously constructed based on a reiterated homogenization procedure the first upscaling performed from the high fidelity flow model is based on sequential partially and fully topological model reduction procedures considering two discrete networks of solution enlarged fractures and conduits the subsequent macroscopization procedure projects the reduced model into the cells of a coarse computational grid where homogenized equivalent properties are numerically constructed such a two level upscaling gives rise to a macroscopic flow model characterized by mass transfer functions between the geological structures a notable consequence of the approach proposed herein is the appearance of a new karst index concept whose underlying physics relies on the generalization of the traditional peaceman s theory of well index such a concept rules the mass exchange between conduits and matrix and can be extended to the general scenario of coupled flow in multi branch karst conduit systems displaying general non circular cross sections and surrounding damage zones the downscaling representation for the karst index can be further explored to improve accuracy of the exchange coefficient between the geological objects numerical experiments are carried out showing the magnitude of the index for certain conduit and fracture arrangements along with illustrating the impact upon flow patterns keywords karst conduits carbonate rocks multiscale coupled flow model reduction well index karst hydrogeology 1 introduction fractured and karstified carbonate reservoirs are extremely heterogeneous and exhibit pore spaces with different sizes geometries and connections extremely difficult to characterize and predict in the subsurface see e g baomin and jingjiang 2009 the pore space is formed during primary sediment deposition followed by coupled deposition and burial which give rise to secondary porosity associated with these sediments moore 1989 karstification occurs during infiltration of aggressive fluids that dissolve the host rock palmer 1991 which is mostly developed along fractures and sedimentary bedding in carbonate rocks and is caused by fluids from the surface epigenic karst or ascending fluids from the subsurface hypogenic karst klimchouk et al 2016 such a complex scenario leads to the appearance of highly irregular dissolution structures in carbonate rocks commonly referred to as karst systems whose associated network dominated by interconnected cave conduits may provide pathways of high conductivity for localized flow such systems may also extend towards the surface through sinkholes and springs palmer 1991 throughout the manuscript we adopt the karst terminology to designate a geologic environment characterized by the abundance of aqueous dissolution processes in underground conditions that propagate progressively through the host rock mostly along a highly fractured dissolutionally modified carbonate rock annable 2003 the macroscopic geobodies originated from karstification consist of coalesced cave passages referred to as karst conduits intertwined by a network of enlarged fractures and bedding planes loucks 2007 the conduit network forms macropores and commonly exhibits self organization features displaying long correlations in the permeability field which give rise to localized flow pathways saller et al 2013 in this setting pore systems may differ from each other by several orders of magnitude giving rise to triple porosity permeability systems worthington et al 2000 wu et al 2011 the classification of cave patterns was pioneered by palmer 1991 who analyzed several thousands of cave passages and proposed different self organized structures to describe epigenic dissolution caused by meteoric fluids from the surface and hypogenic karst dissolution caused by ascending fluids from the subsurface to represent the entire cave network system see klimchouk et al 2012 and references therein for an exhaustive review in addition annable 2003 provided an exhaustive overview of the evolution of the conceptual models to describe the formation of karst systems among other effects fracture density in the host rock is one of the most salient features that controls their evolution in this context fracture aperture and bedding partings are enlarged preferentially in the vicinity of prominent faults leading to the appearance of collapsed cave complex distributions loucks 1999 the geometric and hydraulic properties of the conduit network play a determining role in flow regimes for instance cave diameter values usually lie in the 1 to 10 m range and consequently exhibit enormous potential for localized fluid flow saller et al 2013 which entail explicit treatment in the hydrodynamic model moreover abrupt variations in the cross sectional area of the conduit system along with small scale irregularity wall roughness may also have a profound impact on flow and transport regimes and therefore their effects upon flow patterns must be carefully assessed peterson and wicks 2006 a wide variety of formulations has been proposed to describe coupled flow in matrix conduit enlarged fracture systems see e g cao et al 2011 and nordbotten and boon 2017 for survey on mixed dimensional pdes within this framework conduit has been treated as a discrete pipe network with 1d high permeability cao et al 2011 in addition continuum formulations have been postulated where under the pseudo steady regime mass transfer component is constitutively given in terms of an exchange coefficient whose inverse plays the rule of a conduit resistance multiplied by the pressure difference between the two sub structures d angelo and quarteroni 2008 gjerde et al 2019 in a similar fashion hybrid hierarchical models have been developed in vidotto et al 2019 with emphasis on flow in blood networks containing 1d larger vessels coupled with homogenized capillaries and tissue in spite of well established the accuracy and reliability of the aforementioned models can be somewhat improved by bridging the effective medium behavior with the local description at the finer scales such a bridging consists in one of the main objectives of this work and leads to the birth of a new class of three scale models for flow in coalesced collapsed paleocaves lopes et al 2019 the related hierarchy of length scales naturally appears and is depicted in fig 1 at the finest resolution o 10 2 m the high fidelity hydrodynamics is posed wherein the geological structures are envisioned as 3d objects percolated by coupled free porous media flows popov et al 2009 which may also include inertial effects bailly et al 2009 at the intermediate description o 10 0 m a model reduction is performed resulting in hydrodynamics governed by coupled 3d 2d 1d flows in matrix fracture conduit respectively such a reduced model exhibits a lower computational cost and aims at inheriting the main patterns of the high fidelity flow characterized by highly localized mass interchange functions described by line source dirac measures along the conduit lines furthermore in the meso macro upscaling an additional length scale is introduced with characteristic length associated with a typical grid block of a coarse computational mesh o 10 1 m in this macroscopic context the conduit network is embedded in a coarse grid and we proceed in a similar fashion to the traditional peaceman s well index concept extended in wolfsteiner et al 2003 to unconventional wells and devise a new macroscopic conduit fracture matrix exchange coefficient which gives rise to an emerging concept referred herein to as karst index within this framework our aim consists in taking preliminary steps toward addressing the three scale description underlying the new karst index concept to describe mutual mass interchange between conduit enlarged fracture matrix systems our striking feature consists of the rigorous development of the 3d 2d 1d flow models at different scales based on combination of topological model reduction and source based upscaling methods numerical examples illustrate the potential of the proposed methodology in describing flows in karst systems in particular in the simulations we consider the late cretaceous carbonate rocks of the jandaíra formation in the potiguar basin semiarid brazil as an analogue of karstified and fractured reservoir from which we extract real input data 2 fine scale high fidelity model we begin by presenting the microscopic high fidelity model where all geobodies are treated as 3d objects in order to focus on our main target for the sake of simplicity throughout the manuscript exterior sources sinks are neglected and neither gravitational nor inertial effects are taken into account in the momentum equations we then consider fully saturated darcy regime with flow solely driven by a pressure gradient the model can easily be extended to allow for higher reynolds numbers and to incorporate brinkman and capillary effects in shallow non saturated flows governed by richard s equation our subsequent developments can be applied to both aquifers and carbonate reservoirs with the proper selection of the input hydraulic parameters let ω r 3 be the microscopic domain with boundary ω occupied by a karst system saturated by a slightly compressible liquid the domain can be reconstructed by the union of subdomains ω ω m ω f ω c occupied by the intact rock matrix the network of enlarged fractures and the conduit system respectively the latter representing the coalesced cave complex fig 1 denoting k β the pair of permeability and total compressibility parameters the restriction to each subdomain designated by the subscript i reads as 1 β x β i x k x k i x x ω i i m f c further denoting p v μ the pore pressure darcy s velocity and the fluid viscosity the local hydrodynamics is governed by the system 2 β x p t v 0 in ω v k x μ p supplemented by boundary conditions on ω and initial conditions 3 model reduction we now proceed within the framework of the micro meso upscaling by developing two distinct levels of model reduction in the sequel we begin by constructing the partially reduced model where only the elements of the enlarged fractures network are treated as n 1 dimensional objects n 2 3 subsequently we develop the fully reduced model where on addition to the fracture network the conduit system is reduced to a 1d curvilinear geobody 3 1 partially reduced model the partially reduced model can be accomplished by extending the discrete fracture model dfm presented in frih et al 2008 to incorporate the hydraulic interaction between fractures and conduits in addition to the fracture matrix mass transfer following the usual framework of dfm type models each fracture divides the domain ω m into two subdomains ω m1 and ω m2 in addition to establish the proper mathematical setting fractures are virtually extended toward the outer boundary ω m see ganis et al 2014 for details the matrix fractures interfaces are denoted by γ f m j ω f ω m j j 1 2 in a similar fashion to incorporate fracture conduit microscopic interactions each fracture divides the conduit subdomain ω c into ω c1 and ω c2 giving rise to the interfaces γ f c j ω f ω c j j 1 2 see fig 2 denote n f and n c the unitary normal outward to ω f and ω c respectively by postulating continuity of pressures and normal velocities at γ fi i m c the flow model 2 can be recast in the form 3 β i p i t v i 0 in ω i i m 1 m 2 f c 1 c 2 v i k i μ p i in ω i i m 1 m 2 f c 1 c 2 p i j p f on γ f i j i m c j 1 2 v i j n f v f n f on γ f i j i m c j 1 2 p c p m on γ c m v c n c v m n c on γ c m hence following the usual dfm framework denote n and τ the spatial operators in the normal direction and tangential plane respectively and proceed by decomposing the velocity field in the form v f n v f n τ v f τ where the additional subscripts designate normal and tangential components respectively mass conservation for the fractures reads as β f p f t n v f n τ v f τ 0 in ω f the domain ω f r 3 is then collapsed into γ f r 2 with hydrodynamics represented in terms of averaged fields over the fracture aperture d 4 v f τ 1 d d 2 d 2 v f τ d n p f 1 d d 2 d 2 p f d n thus considering γ f γ f m γ f c given by the union of the suddomains intercepted by the other geobodies integrating the mass conservation equation in the normal direction gives d β f p f t v i x n f γ f τ d v f τ 0 in γ f where v i n f γ f i m c designate the jump across γf 5 v i x n f γ f v m 1 n f 1 γ f m v m 2 n f 2 γ f m x γ f m v c 1 n f 1 γ f c v c 2 n f 2 γ f c x γ f c with n f 1 n f 2 n f and n c 1 n c 2 n c in addition normal and tangential components of darcy s law are given by 6 v f τ k f τ μ τ p f 7 v f n k f n μ n p f with k f τ and k f n the permeabilities in the tangential and normal directions respectively hence integrating 6 and assuming homogeneous permeability in the normal direction leads to the reduced form 8 v f d v f τ d 2 d 2 v f τ d n d k f τ μ τ p f in γ f it remains now to build up a constitutive law for the pressures jump on γf to this end we average 7 in the direction normal to the fracture and use pressure continuity on γf this yields d 2 d 2 v f n n f d n k f n μ p f γ f i 2 p f γ f i 1 k f n μ p i 2 γ f i 2 p i 1 γ f i 1 i m c hereafter for the sake of simplicity we shall focus on the case of highly permeable fractures where pressure continuity on γ fi can be enforced frih et al 2008 the general case including seal fractures can be further accomplished using the analysis developed in martin et al 2005 9 p f p i 1 γ f i 1 p i 2 γ f i 2 i m c hence the partially reduced flow model is governed by the system 10 β i p i t v i 0 in ω i i m c v i k i μ p i in ω i i m c d β f p f t τ v f v i n f γ f in γ f i v i x n f γ f v i 1 n f 1 γ f i v i 2 n f 2 γ f i i m c v f d k f τ μ τ p f in γ f p f p i on γ f i i m c p c p m on γ c m v c n c v m n c on γ c m 3 2 fully reduced model we now proceed with the construction of the fully reduced model where in addition to fractures the elements of the conduit network are also collapsed to 1d objects fig 3 in this r 3 r model reduction procedure denote l the length of the conduit and begin by attaching a local system of curvilinear coordinates r θ s with r and θ the radial and angular components within each cross section and s 0 l the arclength in this context in order to localize the elements of the surface of the conduit we introduce the radius function r 0 2 π 0 l r with θ s r θ s the microscopic domain occupied by the conduit ω c along with the center line γc where the reduced model shall be posed fig 4 are characterized by ω c x r θ s r 3 r r γ c x r θ s ω c r 0 s 0 l in addition for a fixed arc length s the intersection curve with the surface of the conduit c s together with the cross section ac s such that ω c a c 0 l can be represented as c s x r θ r 0 2 π r r a c s x r θ r 0 2 π r r in order to constrain flow to the 3d conduit network set i c in 10 to obtain 11 β c p c t v c 0 in ω c v c k c μ p c v c n c v m n c on γ c m p c p m on γ c m p c p f on γ c f in a similar fashion to the dfm procedure decompose the spatial operators and velocity into axial curvilinear s and transverse components t r θ in each cross section this yields 12 s t s t v c v c s v c t k c k c s k c t thus from 11 and 12 we have 13 β c p c t t v c t s v c s 0 in ω c v c t k c t μ t p c v c s k c s μ s p c the reduced equations can be obtained by averaging the flow model across the cross section of the conduit ac to this end in the developments that follow we assume the necessary regularity related to continuity of pressure and its time derivative in order to interchange integration and differentiation in the storage component thus denoting p c and v c the averaged pressure and velocity and ac the area we have 14 p c 1 a c a c p c d a v c 1 a c a c v c s d a hence denoting n c the unit normal on the intersection curve c outward to ω c and assuming βc constant in ac by averaging 13 using the continuity conditions furnishes for each arc length coordinate s 0 l 15 β c p c t s v c 1 a c a c t v c t d a 1 a c c v c t n c d c in the analysis that follows we decompose the center line of the conduit in the form γ c γ c m γ c f to distinguish the conduit subsets intercepted by matrix and fractures in this setting the source term in the rhs admits the decomposition 16 1 a c c v c t n c d c a c v m n c x γ c m a c d v f n c x γ c f where a c s c a c denotes the surface area density of the conduit c the length of c s on the conduit surface and f c the length average of a quantity v n c along c 17 v m n c s 1 c 0 2 π v n c r θ d θ with c s 0 2 π r θ d θ it should be noted that in the source component stemming from the mass exchange with the fracture network 16 the high fidelity velocity was replaced by the averaged counterpart previously computed with the dfm model thus assuming constant axial permeability in each cross section k c s k c s s we have from darcy s law 18 v c 1 a c a c v c s d a 1 a c a c k c s μ s p c d a k c s μ s p c thus introducing the partial sources 19 q c m x γ c m c v m n c d c q c f x γ c f 1 d c v f n c d c with q c i x γ c j 0 for i j along with the overall conduit source per unit of area q c q c m a c for x γ c m q c f a c for x γ c f the reduced hydrodynamics in the 1d domain occupied by the conduit is given by 20 β c p c t s v c q c in γ c v c k c s μ s p c it remains to analyse the influence of the topological reduction of the conduit system on the source term in the partially reduced model in 10 to this end we proceed by decomposing the integral form of mass balance in the composed domain ω c ω m in the form 21 β m ω m p m t d ω ω m v m d ω β c ω c p c t d ω ω c v c d ω now denote ω m and the pair pm v m the homogenized matrix domain with associated pressure and velocity fields under the topological reduction ω c γc hence adopting analogous decomposition for the conduit length l l m l f and using the 1d mass balance in 20 we have after taking the limit within the reduction procedure lim ω m ω m β m ω m p m t d ω ω m v m d ω lim ω c γ c γ c m a c β c p c t v c d a d s 0 l m β c p c t s v c a c s d s 0 l m q c m s d s thus mass conservation in the matrix with embedded conduit system can be recast in the form 22 β m ω m p m t d ω ω m v m d ω 0 l m q c m s d s in an analogous manner we have for the conduit line embedded in the planar fractures γ f d β f p f t d γ γ f τ v f d γ γ f v m n f γ f d γ 0 l f q c f s d s with asymptotic properties designated by the subscript f for conciseness of notation the tilde symbol was suppressed from the mean variables thus introducing the line delta function δ λ l 2 associated with the curve δc embedded in ω m as the distribution satisfying 23 ω m δ λ f d ω δ c m f s d s 0 l m f s d s and the lower dimensional counterpart δλ l 1 associated with the planar domain γf 24 γ f δ λ f d γ γ c f f s d s 0 l f f s d s the reduced flow equations are given by 25 β m p m t v m q c m δ λ in ω m d β f p f t τ v f q c f δ λ q m f in γ f v f d k f τ μ τ p f in γ f v m k m μ p m in ω m β c p c t s v c q c in γ c v c k c s μ s p c in γ c with q c m c v m n c d c q c f 1 d c v f n c d c and 26 q c s 1 a c c v c n c d c q m f v m n f γ f m it should be noted that the above two scale system entails a dynamic coupling between the mesoscopic reduced model and the micro high fidelity counterpart as it requires computation of the local normal velocities on the surface of the conduit in order to calculate the sources qcm and qcf an important observation regarding the coupled 3d 2d 1d system relies on the different treatment of the sources in the flow equation in the matrix in contrast to the matrix conduit transfer function which appears as a line source within the matrix cells within the dfm framework the influence of fractures on matrix hydrodynamics appears manifested through pressure continuity on γf rather than a source component it should be noted that alternative embedded treatment of fractures in the matrix can be adopted in the context of the edfm method lee et al 2000 in what follows we show that under a time scale assumption associated with the local pseudo steady regime the reduced model can be represented in a somewhat simplified manner 3 3 lumped parameter approximation by restricting our analysis to the long term pseudo steady regime local pressure and velocity profiles exhibit a flat plateau in the center of the conduit together with sharp layers in the vicinity of the shell with averaged values decaying exponentially in time in this setting the local mass exchange can be approximated by a constitutive law dictated by a lumped parameter gap resistance enhanced by a pressure difference gjerde et al 2019 27 q c m s 1 ω c μ p c p m q c f s 1 ω c μ p c p f where p c designates the aforementioned mean over the cross section and p i p i c i m f the line averaged over the curve c defined in 17 in the above pseudo steady representation ωc represents the conduit resistance for radial flow the usual asymptotic cases ωc of nearly impermeable and ωc 0 of high conductivity can easily be reproduced 4 macroscopic model hereafter we proceed in the context of the subsequent upscaling towards the construction of the homogenized description of the conduit mass transfer functions now embedded in a coarse mesh therefore consider the macroscopic scenario wherein a discretization procedure has been adopted and the reduced 1d conduit network now appears embedded in the partition of the entire domain associated with this discretization the meso macro upscaling is based on a point to segment wise discrete treatment of the exchange parameter within the intercepted coarser cells by the conduit network wolfsteiner et al 2003 4 1 karst index concept in order to quantify the mass transfer functions in light of the characteristic length of a coarse grid in what follows we propose a generalization of the well index concept to the present context which shall be restricted to the coarse cells intercepted by the conduit network thus consider the partition of the domain into n subdomains d k k 1 n where each element may or may not intercept the conduit network γc see fig 5 for each subdomain d k define the local curve γ c k γ c d k given by the union between the intercepted subsomains γ c k γ c m k γ c f k with the total length l given by the sum of segments l k l d k each one subdivided into l m k and l f k designating the partial lengths of the matrix and fracture intersections within each cell further introduce the volume of each cell d k along with the areas of the fracture network and conduit cross section restricted to each cell a f k and a c k respectively in addition define the line average macroscopic counterparts of the sources within each cell 28 q c m k 1 γ c m k γ c k q c m d γ 1 l m k 0 l k q c m s d s q c f k 1 γ c f k γ c k q c f d γ 1 l k f 0 l k q c f s d s recalling the property q c i x γ c j 0 for i j further introduce the averaged pressures over each coarse cell p m k 1 d k d k p m d d p f k 1 a f k γ f k p f d γ under the lumped parameter pseudo steady regime the above definitions lead to a natural characterization of the karst indexes and k i m k and k i f k per unit of the corresponding lengths restricted to the cell l m k and l f k denoting p c k v c k the restriction of p c v c to each block we have 29 k i m k μ q c m k p c k p m k k i f k μ q c f k p c k p f k denoting β j k k j k j m f k 1 n the restriction of compressibility and permeability to each grid block the mass balances of the previous fully reduced model can be recast in the form 30 β m k p m k t v m k k i m k μ p c k p m k δ λ in d k k 1 n d β f k p f k t τ v f k k i f k μ p c k p f k δ λ q m f k 0 in γ f k β c k p c k t s v c k k i m k μ a c k p c k p m k k i f k μ a c k p c k p f k in γ c k with the velocities given by the macroscopic form of darcy s law the high fidelity problems for computing of the karst indexes k i i k i m f consist on solving the transient problem for each cell d k k 1 n i m f it is remarkable to see that the three scale setting also allows for a sharper representation of ki by computing the sources q c m k and q c f k directly from the high fidelity description using a straightforward integration over the surface area of the interfaces k i m k μ p c k p m k q c m k 1 l m k 0 l k q c m s d s 1 l m k 0 l k c v m n c d c d s 1 l m k γ c m k v m n c d γ where γ c m k denotes the restriction of the microscopic conduit matrix interface to each cell the above closure relation can be envisioned as a double down scaling procedure for assessing the karst index such a representation provides an accurate form for computing the exchange coefficient between discrete conduit networks and continuum matrix see saller et al 2013 for circular cross sections the downscaling representations 29 for k i m k and k i f k resemble in form the ones derived in karimi fard and durlofsky 2012 for the well index constructed within the framework of the near well upscaling method in the case of single phase flow in the darcyan regime the fully reduced formulation proposed herein allows for the construction of a family of hierarchical models capable of handling general non asymmetric cross sections of the karst conduit it should be noted that by invoking 26 the exchange functions qc and qcm can be computed without assuming pseudo steady regime moreover within the context proposed herein it is worthwhile highlighting that the collapsed breccia zone in the vicinity of the conduit described in loucks 1999 can be incorporated in a straightforward fashion in terms of a skin factor 5 numerical examples hereafter we illustrate the numerical performance of the three scale model initially preliminary experiments are conducted in order to study the sensitivity of the magnitude of the karst index with different scenarios involved in this setting we begin by considering flow in circular and elliptical sinkholes as depicted in menezes et al 2020 subsequently dynamic simulations are carried out aiming at illustrating the performance of the fully reduced model karst index computations the computations of the karst index are carried out at the finest scale where the local pressure and velocity profiles are computed by solving the high fidelity diffusion equations in the matrix and computing the flow rate at the conduit interfaces γ c i i m f we then have 31 β i p i t v i 0 v i k i μ p i in ω i supplemented by boundary and initial conditions 32 p x t p x on γ d p v d n q on γ n p p x 0 p 0 x in ω with γ d p and γ n p the portions of the outer boundary of the grid where dirichlet and neumann conditions are enforced after solving the high fidelity flow the index can be computed within a post processing averaging over the cell by invoking definitions 29 the numerical experiments that follow are carried out considering discretization performed by the galerkin method using piece wise linear interpolations in triangles combined with backward euler time stepping method 5 1 circular sinkhole our first goal is to illustrate that in the oversimplified scenario of a vertical sinkhole with constant circular cross section the transient karst index reduces to the traditional well index in this context w i 2 π k l z ln r e r w denotes the classical well index representation with lz the vertical length of the well in the subsequent 2d simulations we select h 500 m the cell size r w 3 1 10 2 m the well radius r e 0 20788 h the equivalent radius k 1 0 10 15 m2 the permeability of the intact rock and μ 1 0 10 3 pa s the fluid viscosity the vertical karst lies in the center of the 2d squared subdomain which is partitioned using an unstructured triangular mesh progressively more refined in the vicinity of the karst fig 6 dirichlet boundary conditions are prescribed on γ cm while the external boundaries are considered sealed fig 7 we perform transient simulations considering a planar cell of 500 m x 500 m for a time interval i 0 t 5 10 6 s in fig 8 we display the time evolution of the ki towards its stationary value in the same plot we compare with peaceman s results based on equivalent radius we may observe that as time evolves ki t tends to a value close to the corresponding analytical well index w i 7 74033 10 16 m 3 5 2 elliptical sinkhole in our second experiment we consider a similar numerical example of flow around a non circular sinkhole the scenario stems from the drone image shown in fig 9 which corresponds to the well known xavier sinkhole located in the north state of rio grande do norte brazil the right picture highlights three distinct fracture sets group of fractures with similar behavior or orientation the first and second sets are composed of pervasive fractures throughout the outcrop which were formed before the collapse of the sinkhole and are n s to nne ssw oriented and e w to ese wnw oriented respectively the third more pronounced set is composed of collapse induced fractures which form a concentric and semicircular curved set concentrated around the collapsed zone this type of curved fractures occurs in carbonate units in semiarid brazil menezes et al 2020 and also resemble the cylindrical faults found in several paleokarst oil reservoirs such as those in the lower ordovician ellenburger group in west texas loucks 1999 our methodology is based on a hierarchical approach where the smaller pre existing structures highlighted in green and blue are homogenized to a lattice of associated equivalent permeabilities using the simple oda s analytical upscaling oda 1985 in contrast the more prominent collapsed fractures highlighted in red are explicitly accounted for in the fine scale simulation within the dfm framework the model was implemented adopting the input values k f 8 333 10 10 m 2 and d 1 10 4 m for permeability and aperture respectively of both pre existing and collapsed induced fractures the computed equivalent permeabilities associated with the pre existing micro fractures using oda s method are given by k x 1 132 10 14 m 2 and k y 1 201 10 14 m 2 considering a 100 m 100 m cell in fig 10 we depict the distinct subdomains adopted in the subsequent flow based upscaling the red portion labeled with material 4 in table 1 represents the interior of the sinkhole in this scenario filled with sand whereas the orange part material 3 highlights the region filled by collapsed fractures whose reduced elements are treated explicitly within the framework of dfm in particular in our analogue reservoir in semiarid brazil the sand portion stems from small nearby streams captured by sinkholes silva et al 2017 the subregion highlighted in light blue material 2 is occupied by pre existing fractures previously homogenized in a straightforward fashion using oda s method finally the blue area material 1 stands for the intact rock in the last column of table 1 we show the permeability values corresponding to each subregion likewise the previous example we compute the index k i c m k with k 1 for the single simulation cell containing the sinkhole with γ ω and γ cm denoting the exterior boundary and the interface between the two structures respectively we apply dirichlet boundary conditions p x 56 mpa on γ and p x 55 mpa on γ c m and solve the transient problem till stationarity is reached which furnishes k i t 6 90479 10 13 m3 in addition following the usual framework of flow based upscaling combined with the dfm approach the diagonal components of the equivalent permeability are computed by imposing stationary pressure gradients in the two orthogonal directions which furnishes the values of 3 255 10 13 m 2 and 3 293 10 13 m 2 for horizontal and vertical permeabilities respectively in order to validate the homogenized model described by the upscaled values of karst index and permeabilities we perform transient simulations by solving both macroscopic and high fidelity models aiming at comparing accuracy considering an extended domain ω ω m ω c 500 m 500 m with the sinkhole lying in the center we solve the high fidelity model adopting a triangular mesh of 290 107 elements and 578 412 nodes on the other hand the resolution of the macroscopic problem requires a much coarser mesh of 25 quadrilateral bilinear elements and 36 nodes fig 11 the sub region highlighted in red corresponds to the inner domain 100 m 100 m occupied by the sinkhole an initial pressure p 0 x t 0 56 mpa along with the dirichlet value p x p x 55 mpa on the right outer boundary γ r are enforced while maintaining the remaining outer boundaries of the cell sealed for flow the high fidelity model requires enforcement of the additional dirichlet value p x p x 55 mpa on γ cm in contrast in the homogenized model influence of the pressure in the sinkhole is imposed through the source term in 30 in figs 12 and 13 we compare the time evolution of the pressure field computed from the high fidelity and macroscopic models whereas in fig 14 we display the cumulative production curves obtained with the two formulations the numerical results suggest that in spite of overlooking the local features of the flow the macroscopic formulation seated on the upscaled karst index and permeabilities is capable of providing production curves with excellent agreement with the high fidelity counterparts with considerable reduction of the computation burden 5 3 conduit enlarged fracture index we now present local simulations for computing the karst index k i f k k 1 between a conduit and an enlarged fracture in a single cell in this context we analyze the influence of the intersection angle θ between the two geobodies on the magnitude of the index see figs 15 and 16 we consider initial and conduit pressures p 0 55 mpa and p c 40 mpa along with homogeneous neumann conditions on the outer boundaries the dependence of kif on θ is displayed in fig 17 as expected the index grows with the angle since the increase in θ amplifies the contact area for mass interchange between the two objects 5 4 well karst interaction finally we present computational simulations to illustrate the performance of the fully reduced formulation the numerical example consists of depletion of a box shaped reservoir containing a single conduit induced by a production well fig 18 our aim is to analyze the hydrodynamical interactions between the well and the conduit we consider initial and well pressures p 0 55 mpa and p w 40 mpa and homogeneous neumann conditions on the outer boundaries the 3d mesh is composed of tetrahedron elements for the intact rock and 1d linear elements along the conduit the system is solved within the framework of a fully coupled formulation where the additional degrees of freedom associated with pressure in the karst conduit are solved simultaneously with the ones related to the matrix the input physical and mesh parameters are listed in table 2 we begin by considering scenario a displayed in fig 18 and compare the pressure profiles in the xy plane for a fixed depth z 5 5m intercepting the conduit in fig 19 we display the results computed using the high fidelity left and fully reduced right models for two different times we may observe that localized effects between well and conduit captured by the high fidelity model tend be to smeared out within the fully reduced approach in fig 20 we compare the cumulative production curves computed with the aforementioned micro and mesoscopic formulations in spite of the regularization effect in the vicinity of the well conduit interface we observe slight differences between production curves the fully reduced model shows ability to capture the coarse scale features of the problem with lower computational burden see mesh sizes in table 2 furthermore in fig 21 we compare the evolution of the pressure profiles along the conduit line owing to the high contrast in permeability the pressure behaves nearly constant next to the conduit showing a flat plateau with faster depletion in scenario a finally we analyze the 1d cuts in the matrix pressure profile along line l located at 50m 0 100m 2m in fig 22 we compare the aforementioned pressure profiles with the case of absence of the conduit represented by the black solid line the sharpest boundary layer case near the well corresponds to the single matrix problem in the absence of the cave system the presence of the conduit tends to split the pressure gradient in different magnitudes in the adjacent subregions in the vicinity and away from the well ywell 40m and 40m y 60m such a perturbation appears more pronounced in scenario a of smaller conduit well distance moreover in the scenario without conduit the pressure is almost constant through the element intercepted by the well y 50m 60m whereas the interaction between well and conduit in case a implies in linear profile within the elements intercepted by the well y 40m 60m it should be noted the delayed influence of the conduit on the pressure profile in scenario b which us due to the longer time in which the pressure dissipation front reaches the conduit location 6 conclusions we have considered fine scale high fidelity single phase flow models in carbonates from which we constructed partially and fully reduced mesoscale models for hydrodynamics in fractured karstified rocks in the reduced mesoscopic model solution enlarged fractures and karst conduit systems were treated as geobodies of 2d and 1d dimension respectively our framework was developed in the context of an innovative three scale flow model wherein the topological reduction procedure applied to the high fidelity equations gave rise to a coupled 3d 2d 1d model with interchange mass transfer between the three geobodies quantified by source terms in the flow equations such a formulation was explored to further characterize a macroscopic karst index in coarser cells which under the pseudo steady regime plays the role of a generalized well index valid for complex branches of non circular conduit networks computational simulations illustrate the potential of the formulation proposed herein the methodology proposed opens new directions for the construction of accurate flow models in fractured karstified carbonates displaying different levels of geological complexity in future work we shall consider more complex branched karst networks along with extensions to the nonlinear richards equation for unsaturated flow and include boarder region analysis in the sense of wen et al 2003 in order to improve accuracy of the numerical ki and consequently reduce the sensitivity of the value with imposed boundary conditions finally the multiple subregion methodology developed in karimi fard et al 2006 based on the preliminary upscaling to sub grid cells characterized by iso pressure contours consists of a very promising tool for easier implementation in the case of intensively karstified limestones credit authorship contribution statement marcio a murad conceptualization writing original draft writing review editing tuane v lopes software validation patricia a pereira software validation francisco h r bezerra data curation aline c rocha software declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments we thank the anonymous reviewers for their detailed and constructive comments which significantly improved our work this work was supported by the reservoir geology geophysic and geomechic management at cenpes petrobras under the contract number 4600556376 we also thank the brazilian agency of oil gas and biofuels agência nacional do petróleo gás e biocombustíveis anp 
474,groundwater level gwl forecasting is crucial for irrigation scheduling water supply and land development machine learning ml e g artificial neural networks has been increasingly adopted to forecast gwl due to its ability to model nonlinearities between gwl and its drivers e g rainfall although ml approaches have been successful at forecasting gwl they are often inaccurate when gwl exhibits multiscale changes e g due to urbanization to address this shortcoming wavelet transforms wt are routinely coupled with ml methods unfortunately researchers frequently neglect key issues associated with wt that render such forecasts useless for real world scenarios this study demonstrates how new ml methods such as extreme gradient boosting and random forests can be properly coupled with wt to generate accurate gwl forecasts 1 3 months ahead for 7 wells in kumamoto city in southern japan that can be used to help address current pressing issues such as groundwater quality and land subsidence keywords groundwater level forecasting maximal overlap discrete wavelet transform extreme gradient boosting machine random forests support vector machine 1 introduction groundwater level gwl forecasting is a crucial task that allows water resources managers to create effective irrigation schedules manage water supply and plan land development simulation of gwl is often performed by processes oriented models with modflow mcdonald and harbaugh 1988 being the most popular mohanty et al 2013 advanced fully coupled and distributed models e g hydrogeosphere therrien et al 2010 and getflows mori et al 2015 hosono et al 2019 are becoming more popular for surface water e g mori et al 2015 ala aho et al 2015 and groundwater simulation maxwell et al 2015 ala aho et al 2015 hosono et al 2019 these models require a range of spatio temporal data for approximating the complex hydrological processes heterogeneous subsurface systems and anthropogenic activities therrien et al 2010 barthel and banzhaf 2016 kollet et al 2017 and consequently involves high cost and long computational time maxwell et al 2015 in addition achieving accurate simulations through process based models is very challenging sun et al 2016 sahoo et al 2017 due to the over simplification of complex hydrological processes therrien et al 2010 discretization of model domains white et al 2019 and availability of data researchers are therefore exploring alternative approaches such as data driven i e statistical and machine learning ml approaches over the last two decades see review by rajaee et al 2019 ml approaches avoid having to specify the underlying complex physical hydrological processes and instead rely on only the statistical relationships between explanatory e g rainfall and response variables e g gwl despite this lack of physical knowledge ml has been successfully applied to gwl forecasting rajaee et al 2019 some studies e g adamowski and chan 2011 barzegar et al 2016 belayneh et al 2016 quilty et al 2018 have also shown that hybrid approaches that combine different ml approaches in various stages of the model development can be more accurate than standalone ml since particular patterns in the data e g trends periodicities level shifts can be better captured by different approaches ghaemi et al 2019 for instance the wavelet transform wt a time frequency localization method that is able to extract time varying behavior trends periodicities etc or multiscale change from time series has often been coupled with ml for gwl forecasting with many studies showing increased performance from this approach over standalone ml nourani et al 2014 rajaee et al 2019 however recent studies zhang et al 2015 du et al 2017 quilty and adamowski 2018 have shown that these hybrid wt based ml models have been incorrectly developed and cannot be used properly for real world applications this study aims to demonstrate how a recent wt based forecasting approach the wavelet data driven forecasting framework wddff quilty and adamowski 2018 can be coupled with emerging ml methods for real world gwl forecasting to date there has been a plethora of ml approaches applied to gwl forecasting see list in rajaee et al 2019 such as multiple linear regression mlr e g sahoo and jha 2013 sun et al 2016 ebrahimi and rajaee 2017 artificial neural networks ann e g daliakopoulose et al 2005 zhang et al 2018 guzman et al 2017 wunsch et al 2018 natarajan and sudheer 2019 mirarabi et al 2019 extreme learning machine alizamir et al 2018 malekzadeh et al 2019 natarajan and sudheer 2019 fuzzy logic fl e g alvisi et al 2006 shiri and kisi 2011 nadiri et al 2019 adaptive neuro fuzzy inference system anfis kholghi and hosseini 2009 moosavi et al 2013 gong et al 2018 support vector machine svm e g huang et al 2017 natarajan and sudheer 2019 tang et al 2019 mirarabi et al 2019 genetic programming gp e g shiri et al 2013 fallah mehdipour et al 2013 natarajan and sudheer 2019 gradient boosting machine gbm kenda et al 2018 along with statistical approaches such as arima model adamowski and chan 2011 however two very recent and promising approaches for gwl forecasting include random forests rf wang et al 2018 tang et al 2019 kenda et al 2018 koch et al 2019 and extreme gradient boosting xgb rf and xgb are both tree based ml algorithms the former is based on bagged regression trees while the latter is based on boosted regression trees a comparison between these two methods has yet to be carried out for gwl forecasting nor have these methods been explored within the context of the wddff more recently to address multiscale change in hydrology and water resources koutsoyiannis 2013 researchers have coupled wt with ml to generate hybrid models for improving forecast accuracy of gwl adamowski and chan 2011 streamflow adamowski and sun 2010 tiwari and chatterjee 2010 water quality barzegar et al 2016 urban water demand quilty et al 2019 and water balance components rahman et al 2018 wt is the most popular approach for developing the accurate hybrid models for hydrological and water resources forecasting nourani et al 2014 afan et al 2016 fahimi et al 2017 quilty and adamowski 2018 however a recent study by quilty and adamowski 2018 revealed that many studies incorrectly developed wavelet based hybrid models which are not applicable for real world forecasting problems the authors proposed a set of best practices to address this issue resulting in the wavelet data driven forecasting framework wddff which can be used to correctly forecast real world processes such as gwl to date wddff has only been used with simple least squares regression methods e g multiple linear regression volterra series models extreme learning machines and has only been explored for urban water demand forecasting irrigation flow forecasting mouatadid et al 2019 and monthly pan evaporation prediction ghaemi et al 2019 while it has high potential for gwl forecasting hence there is a need to develop correct wavelet based ml models based on wddff that can be properly applied for a variety of critical real world problems such as gwl forecasting moreover new ml methods such as xgb and rf are only beginning to be explored in the hydrology and water resources domains for example the tree based version of xgb xgbt has only been explored for evapotranspiration wu et al 2019 wu and fan 2019 tropical cyclone jin et al 2019 and streamflow hadi et al 2019 tyralis et al 2019 li et al 2019 zhang et al 2019 modelling while the linear xgb xgbl has yet to be investigated for hydrological or water resources modelling although rf is one of most the successful ml methods across a variety of domains it has not received enough attention for hydrological and water resources modelling yet tyralis et al 2019 recent studies wang et al 2018 tang et al 2019 kenda et al 2018 koch et al 2019 have explored the potential of standalone rf model for gwl forecasting and found that rf can generate accurate gwl forecasts however xgbt xgbl and wavelet based rf have yet to be explored in the context of gwl forecasting in our current study we explored for the first time in the literature the potential of xgbt xgbl and rf as well as xgbt xgbl and rf coupled with the wddff from quilty and adamowski 2018 for gwl forecasting we compared these new approaches against benchmark methods support vector regression svr and their wddff counterparts additionally we employ a bayesian optimization routine based on gaussian processes snoek et al 2012 for automatic hyper parameter selection in xgbt xgbl rf and svr including their wt counterparts which as far as we are aware has yet to be explored in earlier studies employing xgb and rf this bayesian optimization routine is an effective hyper parameter selection approach that has been used in many fields of science and engineering with considerable success falkner et al 2018 therefore the main goal of this study is to demonstrate the potential usefulness of xgbt xgbl wt xgbt wt xgbl and wt rf for gwl forecasting through a comparison with methods developed earlier in the literature i e rf svr and wt svr for our case study we forecast average monthly gwl at 1 2 and 3 months ahead for seven wells in kumamoto city japan using meteorological variables as input data along with previous gwl measurements our main contributions to the literature include the first exploration of 1 xgbt xgbl wt xgbt wt xgbl and wt rf for gwl forecasting 2 xgbt xgbl rf and svr coupled with wddff i e from quilty and adamowski 2018 3 bayesian optimization for the automatic selection of hyper parameters in xgb and rf based models the rest of this study is organized as follows section 2 provides information on the study area and data section 3 outlines our methodology section 4 presents results and discussion and section 5 provides conclusions of our findings 2 study area and data we conducted our experimental study using seven wells where gwl observations were available in kumamoto city which is in the center of kyushu island in the southern part of japan fig 1 kumamoto city is one of the regions with the highest groundwater use in japan about one million city dwellers depend completely on groundwater for their domestic purposes oshima 2010 shimada 2012 and approximately 8 107 m3 groundwater per year is withdrawn from 58 pump stations to meet their water demand hosono et al 2013 there are two types of aquifers in this region unconfined and semi confined to confined aquifers the former is composed of alluvium sedimentary deposits and unwelded pyroclastic deposits overlying an impermeable aquiclude of the lacustrine sedimentary layer the latter is semi confined to confined taniguchi et al 2003 hosono et al 2013 in nature and it consists of pyroclastic flow deposits and volcanic lavas this aquifer is locally referred to as a second aquifer and its depth below the ground surface generally varies between 60 m and 200 m hossain et al 2016 majority groundwater is withdrawn from the second aquifer for water supply in the city hence this is the most important aquifer in the area a detailed hydrogeological description of the study area can be found in hosono et al 2013 and kagabu et al 2017 the majority of groundwater in the second aquifer is recharged through infiltration of precipitation in the recharge areas such as kikuchi ueki and takayubaru highlands as well as by river and artificial ponding waters in paddy fields in and around the midsection of the shira river hosono et al 2013 taniguchi et al 2019 fig 1 groundwaters in the recharge areas are then transported through highly porous pyroclastic flow deposits aso 2 and aso 3 and discharged mostly to lake ezu the monthly gwl data spans 1980 2017 and was collected from the government office of kumamoto city kumamoto city waterworks and sewerage bureau we analyzed gwl data of 7 wells that are located along the regional groundwater flows from recharge to discharge areas fig 1 all wells are installed into the second deep aquifer with their well depths ranging from 31 m well no ss 09 to 124 m well no ss 108 withan average of 68 28 m the screen depths range from 21 to 31 m well no ss 108 and 81 5 to 114 50 m well no ss 10 from the surface respectively the observed gwl data contains few missing 2 records missing data for a particular month were filled by the average of the previous and following month i e neighbouring months furthermore meteorological data monthly mean temperature and monthly total rainfall of the kumamoto meteorological station were gathered from the japan meteorological agency jma website https www jma go jp jma indexe html as potential inputs for the ml models developed in this study 3 methodology methodology section arranges for brief descriptions of the ml approaches such as svm rf xgb and wt as well as descriptions of the steps followed for the model development 3 1 support vector machine support vector machine svm is a statistical learning theory the basic algorithm of svm is discussed in cortes and vapnik 1995 svm is also known as support vector regression svr when applied to regression problems the library of svm libsvm chang and lin 2011 is commonly used for developing svr models for regression problems libsvm implements the sequential minimal optimization algorithm and it supports epsilon ε svr and nu γ svr regressions in this study svr models were developed through the r programing language environment r core development team 2019 using the libsvm adopted r package e1071 meyer et al 2019 the ε svr approach was adopted since it has been shown to be an accurate approach for predicting hydrological and water resources variables tabari et al 2012 raghavendra and deka 2014 mehdizadeh et al 2017 huang et al 2017 ferreira et al 2019 in svr the selection of the hyper parameters is of key importance in developing accurate models hyper parameters include kernel type e g linear polynomial radial basis function which controls the similarity between a current input and each input in the training set epsilon ε the width of insensitive tube which determines the support vectors that are considered in the model gamma γ controls the shape of the hyperplane separation and cost c a regularization parameter that controls the sparsity of the model aside from the selection of the kernel hyper parameter selection is discussed in section 3 5 4 several studies have demonstrated that radial basis function rbf kernel provides strong performance for predicting hydrological and water resources variables mehdizadeh et al 2017 ferreira et al 2019 and it has been used in recent studies for gwl forecasting yoon et al 2016 huang et al 2017 tang et al 2019 guzman et al 2019 further details on theoretical background regarding the ε svr and rbf can be found in chang and lin 2011 3 2 random forests random forest breiman 2001 is a ml algorithm derived from the classification and regression trees cart paradigm breiman et al 1984 with some additional randomization such as bagging input data samples and variables tyralis et al 2019 rf is a popular ml approach used for prediction and classification in numerous domains due to its high precision and capability to handle a large number of input variables pal 2016 tyralis and papacharalampous 2017 tyralis et al 2019 there exist several varieties of rf based on the cart criterion biau and scornet 2016 tyralis et al 2019 all variations of rf are not applicable for solving regression prediction problems geurts et al 2006 wright and ziegler 2017 and many of them are tested mainly for classification geurts et al 2006 biau and scornet 2016 the regression variant of rf is a nonlinear model in which samples are partitioned at each node of a tree based on the value of a selected input variable breiman 1984 in this study extremely randomized trees details in geurts et al 2006 applicable for regression geurts et al 2006 biau and scornet 2016 wright and ziegler 2017 was applied for tree splitting this approach is the most computationally efficient variation of rf geurts et al 2006 wright and ziegler 2017 and has been shown to achieve similar performance with other similar rf based algorithms biau and scornet 2016 the performance of rf also depends on several hyper parameters biau and scornet 2016 wright and ziegler 2017 tyralis and papacharalampous 2017 probst et al 2019 tyralis et al 2019 such as number of trees n indicating the size of the forest mtry representing the number of input variables to split in each node minimal node size the minimum number of observations in a terminal node and sample size the number of observations randomly selected for growing each tree 3 3 extreme gradient boosting the extreme gradient boosting is a scalable tree boosting ml method chen and guestrin 2016 that can capture the nonlinear relationships between predictor and target variables chen et al 2019 hadi et al 2019 xgb is also based on cart and resembles gradient boosting machines with some variations in model structure chen and guestrin 2016 gbm and xgb differ from rf in how trees are grown for example gbm grows tree on the residuals of the former tree whereas rf trains each tree individually just et al 2018 xgb implements second order derivatives whereas ordinary gbm uses first order derivatives chen and guestrin 2016 xgb is more efficient compared to gbm and is less likely to result in model over fitting since it incorporates improved regularization features chen and guestrin 2016 chen et al 2019 the parallel processing of the computation makes it not only the fastest among the gradient boosting algorithms but also affords improved performance over ordinary gbm chen et al 2019 in the generic version of cart a tree ensemble model with k additive functions is used to predict the target variable y i r such as gwl using input variable s x i r d where i 1 2 n and n are the number of input target variable pairs via chen and guestrin 2016 1 y i ϕ x i k 1 k f k x i f k f here ϕ x i maps the model inputs to the model prediction y i r where f f x w q x q r m t w r t denotes the space for the regression tree q represents the structure of each tree fk corresponds to the individual tree structure t is the number of leaves in the tree fk and w is the leaf weights in xgb the objective function incorporates a loss function and a regularization term given as chen and guestrin 2016 2 l ϕ i l y i y i k ω f k where 3 ω f γ t 1 2 λ w 2 here l is the function for the differentiable convex loss used for measuring the difference between the target yi and prediction y i during training the second term ω denotes the regularization term which is used for model complexity penalization since model optimization cannot be achieved using traditional methods for the tree ensemble approach xgbt using eq 2 the additive approach is applied in model training for xgbt chen and guestrin 2016 moreover the function ft is added for improving the model eq 2 for predicting the y i t after the t th iteration for the i th instance as chen and guestrin 2016 4 l t i 1 n l y i y i t 1 f t x i ω f t for quick optimization of the objective function taylor second order expansion is applied by following friedman et al 2000 5 l t i 1 n l y i y i t 1 g i f t x i 1 2 h i f t 2 x i ω f t here the gradient statistics of the loss function for the first g i and second h i order are as follows 6 g i y t 1 l y i y i t 1 and 7 h i y t 1 2 l i y i t 1 the objective is simplified by removing the constant term for step t as 8 l t i 1 n g i f t x i 1 2 h i f t 2 x i ω f t the eq 8 can be rewritten by defining the ij i q x i j and expanding ω as 9 l t i 1 n g i f t x i 1 2 h i f t 2 x i γ t 1 2 λ j 1 t w j 2 j 1 t i i j g i w j 1 2 i i j h i λ w j 2 γ t now the weight w j and the subsequent optimal value l t q of a leaf j for a fixed structure q x can be estimated as 10 w j i i j g i i i j h i λ and 11 l t q 1 2 j 1 t i i j g i 2 i i j h i λ γ t a tree structure q quality can be measured using eq 11 however computing all the trees and simultaneously finding the best one from all possible options is computationally intractable and the problem is solved by using an algorithm that adds branches to the tree at each iteration from a single leaf if the tree splitting for right and left nodes are ir and il respectively the loss reduction by adding them as i il ir can be estimated as chen and guestrin 2016 12 l split 1 2 i i l g i 2 i i l h i λ i i r g i 2 i i r h i λ i i g i 2 i i h i λ γ in this study we applied two variants of xgb such as linear boosting of xgb i e xgbl and tree boosting i e xgbt the main difference between these two variations of xgb is the base learner the base leaner is the linear learner for the xgbl whereas the base learner is the tree learner for the xgbt chen 2014 the interested reader can find further details in chen 2014 a number of hyper parameters need to be set for developing xgb models the hyper parameters considered for xgb model are eta representing the learning rate gamma representing the minimum split loss max depth indicating the maximum depth of a tree min child weight the minimum sum of the weights subsample ratio of the samples used for model training column sample by tree parameter for subsampling of columns i e variables nround number of rounds and two regularization parameters lambda which controls l2 regularization and alpha which controls for l1 regularization 3 4 wavelet transformation wavelet transforms are a mathematical tool that is particularly useful for identifying important time frequency localized features in a time series e g periodicities transients level shifts wt can decompose a time series into different sub time series which provide coarse and fine grained details on the multiscale nature of the time series incorporating these sub time series into a ml model has often improved the accuracy of hydrological and water resources forecasts when compared to standalone ml models afan et al 2016 fahimi et al 2017 mouatadid et al 2019 there are two variations of wt continuous cwt and discrete dwt several studies have already discussed the advantages and disadvantages of the two variations of wt fugal 2009 nourani et al 2014 and recommended that dwt is better for decomposing hydrological and water resources time series data nourani et al 2014 rezaie balf et al 2017 samadianfard et al 2018 mouatadid et al 2019 recent studies have noted that dwt with multiresolution analysis dwt mra is the most common wt that is adopted in hydrological and water resources modelling nourani et al 2014 quilty and adamowski 2018 rajaee et al 2019 however the dwt mra has been demonstrated to have several drawbacks for when used in real world applications maheswaran and khosa 2012 du et al 2017 quilty and adamowski 2018 for example dwt is sensitive to the length of time series and adding new data points to the time series i e in an online system as new data is collected and performing wavelet decomposition causes the wavelet and scaling coefficients computed at a given time to change quilty and adamowski 2018 to overcome these drawbacks the maximal overlap dwt modwt proposed by percival and walden 2000 was implemented in this study as recommended by quilty and adamowski 2018 the modwt applies low and high pass filters to a given time series resulting in wavelet and scaling coefficients if h j l and g j l here l denotes the length of the filter j 1 2 j represents the j th scale and j is the level of decomposition are the wavelet and scaling filters the modwt can be given as percival and walden 2000 13 h j l h j l 2 j 2 14 g j l g j l 2 j 2 the modwt wavelet w j i and scaling v j i coefficients for a given time series x xi i 0 1 n 1 can be determined via percival and walden 2000 15 w j i l 0 l j 1 h j l x i lmodn 16 v j i l 0 l j 1 g j l x i lmodn here lj 2 j 1 l 1 1 further details on the modwt can be obtained from earlier studies e g percival and walden 2000 quilty and adamowski 2018 mouatadid et al 2019 3 5 model development this section gives details on the ml based models xgbt xgbl rf svr wt xgbt wt xgbl wt rf and wt svr developed in this study for gwl forecasting 3 5 1 dataset pre processing and boundary corrections for our case study the target response variable was average monthly gwl at lead times of 1 2 and 3 months ahead for the standalone ml models xgbt xgbl rf and svr candidate input explanatory variables included time lagged from lag 1 up to lag 12 depending on the target variable forecasting step more details in supplementary material sm table s1 data such as average monthly gwl average monthly air temperature tavg average monthly total rainfall r and cumulative monthly rainfall cr see also section 3 4 these are the common explanatory variables considered for gwl forecasting in several earlier studies e g adamowski and chan 2011 wunsch et al 2018 nadiri et al 2019 furthermore water fluxes into and out of the reservoir are considered by incorporating variables such as rainfall and air temperature related to evapotranspiration in the ml models for the wt based models wt xgbt wt xgbl wt rf and wt svr original and decomposed data were used as the candidate input variables modwt was applied for decomposing the explanatory variables see section 3 4 since the wavelet decomposed data is affected by boundary conditions quilty and adamowski 2018 percival and walden 2000 basta 2014 it was necessary to remove the first lj wavelet and scaling coefficients see section 3 4 affected by boundary conditions due to the selected wavelet filter length l and decomposition level j at the beginning of the decomposed data however since this study compares non wavelet based and wt models that vary based on decomposition level and wavelet filter it was necessary to remove the same number of boundary conditions affected coefficients from the beginning of both the target and all input variables for both non wavelet and wt models to ensure the same training and validation indices were used in the development and evaluation of the different models this is based on the best practices established in quilty and adamwoski 2018 the number of boundary conditions effected coefficients that were removed from the beginning of the target and input variables in all models lf was determined according to quilty and adamowski 2018 17 l f 2 j m a x 1 l 1 1 the careful selection of jmax and l is necessary to ensure the number of data available for model training and validation is sufficient in this study jmax and l were set to four hence the total number of the boundary conditions effected coefficient was 46 which was removed from the beginning of the target and input variables for all models e g fig 2 a decomposition level of four was selected to ensure that the prominent annual cycle present in all gwl time series was captured in the level four wavelet coefficients after adjusting the original dataset 456 records for the lagged target and input variables the historical record available ranged from march 1981 to december 2017 442 records the number of available records 396 for training and validating the different ml models after adjusting for time lags and boundary conditions were divided into the training 85 and validation 15 sets while there is no set rule for dataset partitioning during model training and validation deo et al 2017 it is typically accepted that the validation partition should be within 10 40 of the total dataset length barzegar et al 2019 3 5 2 input variable selection and importance of input variables input variable selection is one of the most important steps for developing ml models tiwari and chatterjee 2010 galelli et al 2014 quilty et al 2016 both linear e g sudheer et al 2002 yaseen et al 2015 and nonlinear approaches quilty et al 2016 have been applied for input variable selection in hydrological and water resources modelling galelli et al 2014 however linear approaches based on partial auto correlation function pacf and auto correlation function acf are typically not the most suitable approaches for hydrological and water resources modelling since such problems are generally nonlinear hadi et al 2019 nonlinear approaches based on mutual information mi shannon 1948 typically perform better than linear approaches for hydrological and water resources modelling see details in quilty et al 2016 gong et al 2018 taormina et al 2016 as well as other science and engineering applications peng et al 2005 galelli et al 2014 as will be discussed below since svr does not internally measure the importance of input variables it is important to perform input variable selection prior to developing an svr model in this study we applied one of the most widely used mi approaches for input variable selection minimal redundancy maximal relevance mrmr proposed by peng et al 2005 this approach selects input variables from a set of candidates by using mi to identify candidates that are relevant but not redundant this approach has been shown to select more appropriate input variables than other similar approaches berrendero et al 2016 peng et al 2005 defined an operator φ d r for simultaneously optimizing the maximum relevance d and minimal redundancy r to select an input subset s from the d input variables in x as 18 max φ d r φ d r the complete mathematical derivations for mrmr can be obtained from peng et al 2005 however the input variable selection is not mandatory for approaches like xgb and rf both approaches internally perform input variable selection as well as quantify the variables importance vi chen and guestrin 2016 tyralis et al 2019 hadi et al 2019 jin et al 2019 however svm does not produce any internal measures of vi boehmke and greenwell 2019 hadi et al 2019 pointed out that xgb is a very robust modelling approach even with a high number of explanatory variables the performance of rf is also not overly influenced by additional superfluous input variables i e granted the model is provided with some input variables that are useful in predicting the target variable biau and scornet 2016 pal 2016 probst et al 2019 tyralis et al 2019 the vi in rf was estimated by the advanced permutation method strobl et al 2009 and in xgbt vi was measured via the impurity approach chen and guestrin 2016 boehmke and greenwell 2019 while xgbl estimate vi by measuring the weight chen 2014 3 5 3 data scaling approaches like xgbl and svr are affected by the scale of the different input variables in other words if input variables are on widely different scales this can lead to the model favoring those variables that have a large range while ignoring those with a smaller range regardless of their importance in predicting the target variable hence input variable scaling was performed prior to the development of the xgbl and svr models since the svr models were developed using the e1071 r package it internally performs input data scaling by scaling all variables to have unit variance and zero mean meyer et al 2019 for the xgbl models the input variables were scaled such that they ranged from 0 to 1 using the min max normalization approach which has been successfully applied in many earlier hydrological and water resources ml models e g yaseen et al 2016 jeong and park 2019 min max normalization for a given input variable is determined using 19 x x x m i n x m a x x m i n here x is the original input variable and x is the normalized input variable xmin and xmax are the minimum and maximum values of the input variable respectively 3 5 4 hyper parameter optimization most ml algorithms require the careful selection of a set of hyper parameters probst et al 2019 examples of hyper parameters include the kernel function in svr the number of trees in rf the learning rate in xgb etc hyper parameters for each ml model used in this study are mentioned in the relevant sections 3 1 for svr 3 2 for rf and 3 3 for xgb the performance of ml models is highly influenced by hyper parameters settings eggensperger et al 2013 zhang et al 2016 and their improper selection can lead to poorly performing models many studies select hyper parameters through trial and error suryanarayana et al 2014 zhang et al 2016 jeong and park 2019 grid search and or random search bergstra and bengio 2012 and some use heuristic optimization algorithms such as particle swarm optimization and genetic algorithms snoek et al 2012 bergstra and bengio 2012 feurer and hutter 2019 ideally an accurate and efficient automated hyper parameter optimization method is desirable zhang et al 2016 and useful for permitting a fair comparison between ml alternative sculley et al 2018 furthermore when exploring different ml models they can be equally compared only if they all receive the same degree of optimization or level of attention for the problem at hand feurer and hutter 2019 bayesian hyper parameter optimization bho snoek et al 2012 is an emerging state of the art approach for global and local optimization of hyper parameters falkner et al 2018 feurer and hutter 2019 bho has been shown to outperform other approaches such as grid search and random search bergstra and bengio 2012 on several challenging optimization benchmarks jones 2001 bho has broad applicability to various problem settings falkner et al 2018 feurer and hutter 2019 can be used with integer and real valued hyper parameters and in many cases can outperform domain experts for finding optimal hyper parameters snoek et al 2012 eggensperger et al 2013 feurer and hutter 2019 in this study the bho algorithm snoek et al 2012 and software implementation in wilson 2019 was implemented for hyper parameter selection bho was used for hyper parameters optimization by using k fold cross validation on the training data k fold cross validation has been successfully applied for hyper parameter selection in several hydrological and water resources modelling studies e g barzegar et al 2019 wu and fan 2019 in this study we set k 5 for performing bho it is necessary to choose a prior and an acquisition function the former adopts a gaussian process prior and the latter uses expected improvement ei snoek et al 2012 the usefulness of gaussian processes is solely dependent on the covariance function which makes use of the matern52 kernel snoek et al 2012 3 5 5 coupling of ml and modwt to develop the wt models the ml approaches xgbt xgbl rf and svr were integrated within the wddff by following the direct approach and using the modwt for wavelet decomposition of the input variables approach 4 quilty and adamowski 2018 approach 4 includes both un decomposed and wavelet decomposed boundary condition corrected time series data as input variables furthermore there is no single wavelet filter and decomposition level j combination that has been demonstrated to provide the best performance across all hydrological and water resources modelling applications maheswaran and khosa 2012 mouatadid et al 2019 in this study three different wavelet families such as haar daubechies d2 and best localized bl 4 with j from 1 up to 4 were tested to identify the best wt models therefore the total number of hybrid models for the seven gwl monitoring wells and the three forecasting lead times 1 2 and 3 months ahead was 1008 along with 84 standalone models see table s2 3 5 6 model accuracy the accuracy of applied ml models was evaluated by a number goodness of fit statistics such as mean squared error mse mean absolute error mae root mean square error rmse rmse observations standard deviation ratio rsr moriasi et al 2007 coefficient of determination r2 nash sutcliffe efficiency nse nash and sutcliffe 1970 and kling gupta efficiency kge gupta et al 2009 rsr can be used to interpret a given model s forecasts as very good 0 00 rsr 0 50 good 0 50 rsr 0 60 satisfactory 0 60 rsr 0 70 and unsatisfactory rsr 070 moriasi et al 2007 and nse as very good 0 75 nse 1 good 0 65 nse 0 75 satisfactory 0 50 nse 0 65 and unsatisfactory nse 0 50 moriasi et al 2007 4 results and discussion the results of the ml approaches for 1 2 and 3 month s ahead lead time forecasting of gwl was evaluated using several goodness of fit statistics mse mae rmse rsr nse kge and r2 and graphical tools hydrographs scatterplots and boxplots the experiment results showed a good trade off between training and validation performance confirming the stable generalization capacity of the ml approaches for the standalone and wavelet based models the results obtained for the validation period are presented due to the large number of models and to focus on the generalization abilities of the various models 4 1 standalone models the results rmse nse and r2 of the standalone ml approaches for forecasting gwl at 1 2 and 3 month s ahead can be found in table 1 and results from additional statistics including mae mse rsr and kge are also provided in tables s3 s6 the average accuracy across the seven wells for the standalone ml approaches showed very good performance in terms of nse nse 0 75 and rsr rsr 0 50 moriasi et al 2007 with higher values of kge and lower values of mse and rmse for 1 and 2 except rf for 2 step month s ahead forecasts the nse values ranged between 0 84 and 0 96 with an average of 0 93 for svr 0 85 and 0 97 with an average of 0 92 for xgbl 0 75 and 0 95 with an average of 0 86 for xgbt and 0 55 and 0 93 with an average of 0 81 for rf respectively for 1 month ahead gwl forecasting table 1 the nse values for 2 months ahead gwl forecasts were slightly lower than 1 month ahead forecasts with average nse values across all seven wells of 0 86 for svr 0 81 for xgbl 0 75 for xgbt and 0 70 for rf respectively table 1 although the accuracy further decreased with increasing lead time svr avg nse 0 78 and xgbl avg nse 0 75 showed very good performance for 3 months ahead gwl forecasting while xgbt avg nse 0 67 and rf avg nse 0 66 showed satisfactory performance an example of comparison between observed and forecasted gwl for 1 2 and 3 months ahead is illustrated in fig 3 using hydrographs and scatterplots for well ss 001 it is noticeable that all the methods showed decaying accuracy as well as higher deviations from the 1 1 line with increasing time steps fig 3 and table 1 other gwl forecasting case studies also found similar results though considering different ml approaches yoon et al 2011 rezaie balf et al 2017 barzegar et al 2017 the best performing model was the svr for all lead time forecasting based on the average goodness of fit statistics a number of studies yoon et al 2011 suryanarayana et al 2014 huang et al 2017 mukherjee and ramachandran 2018 guzman et al 2019 mirarabi et al 2019 have reported that svr is the best performing model for gwl forecasting while comparisons were made against several ml approaches such as mlr ann and rf rajaee et al 2019 however most of the relevant studies only tested the capability of the different ml approaches for limited number 4 of observation wells however it can be argued from the obtained results of this study that different ml approaches showed the best performance for different observation wells table 1 for example svr was the best model for a well ss 07 whereas xgbl was the best model for another well ss 10 table 1 furthermore svr and xgbl showed similar performance for some wells ss 001 and ss 09 while rf exhibited the lowest accuracy for all lead times for almost all observation wells table 1 with an unsatisfactory performance for a well ss 07 the rf models show for certain cases lower accuracy than highly parameterized tree based models such as xgb due to the automation of the model efron and hastie 2016 tyralis et al 2019 it is worth mentioning again that it is necessary to select input variables for svr models using another algorithm while xgb and rf models do not require such extra efforts for modeling therefore it can be inferred that the standalone xgb model can effectively manage a high number of input variables without application of other algorithms which is the main advantage of xgb variants and they can perform equally with other high performing models like svr 4 2 wt coupled models the results of the best performing wavelet based hybrid model for each ml such as wt svr wt rf wt xgbl and wt xgbt approach can be found in table 2 and tables s3 s6 interestingly nearly half of the hybrid models 556 showed less accuracy than standalone models and many wavelet based models 198 showed similar results furthermore the same results were also found for the best models with different wt decomposition levels and wavelet filters table 2 the reason behind this is that wavelet based models are not guaranteed to outperform standalone models in this study only a small number of wavelet filters were explored due to the boundary condition at the beginning of the time series see eq 17 which needs to be carefully considered in light of the length of the dataset unfortunately many studies overlook this boundary condition and consider wavelet filters that are too wide and decomposition levels that are too high causing errors to be introduced in the wavelet and scaling coefficients sometimes leading to unrealistically high performance than what is achievable in real world scenarios quilty and adamowski 2018 which this study avoided by following the best practices in quilty and adamowski 2018 in this study it was confirmed that no particular wavelet filter and decomposition level universally led to improved performance of the hybrid models over the standalone models for gwl forecasting several studies also found similar results for other hydrological variables e g witten et al 2011 quilty and adamowski 2018 the most significant finding in table 2 is that the wt xgbl showed the best performance for many wells for 1 month ahead forecasting furthermore the performance of wt xgbl was also better than standalone xgbl for most cases tables s3 s6 the average nse and r2 values for the seven wells at the 2 month ahead were 0 86 and 0 87 for wt svr 0 84 and 0 87 for wt xgbl 0 76 and 0 80 for wt xgbt and 0 70 and 0 77 for wt rf respectively while for 3 months ahead gwl forecasts average nse and r2 values were 0 78 and 0 82 for wt svr 0 81 and 0 84 for wt xgbl 0 70 and 0 78 for wt xgbt and 0 66 and 0 74 for wt rf respectively when the comparisons between standalone models and wavelet based models were made fig 4 fig s1 and tables s3 s6 the improvement was more evident for 2 and 3 months ahead forecasts except few cases than that of 1 month ahead forecasting the improvement in accuracy measures based on average nse were 4 3 months ahead for wt svr 5 3 months ahead for wt xgbl 3 3 months ahead for wt xgbt and almost no improvement for rf 3 months ahead forecasts it is worthwhile to note that these percentages were calculated by considering the maximum nse value equal to 1 this finding is very crucial to improving water resources management since the mid term forecasting such as 2 and 3 months ahead is useful for irrigation planning land development and other environmental considerations 4 3 variables of importance rf and xgb variants naturally provide the importance of explanatory variables in predicting the target variable see section 3 5 2 it was found that some variables have significant relative contributions to the models and most of them showed very low contributions or no contribution as shown in fig 5 explanatory variables with at least 1 contribution for wavelet based model are only shown similar findings have also been reported by hadi et al 2019 while xgbt was applied for streamflow modelling it was generally observed that lagged gwl lag 1 as expected was the most important variables for 1 month ahead forecasting however cumulative rainfall cr was generally the most important variable for 2 and 3 months ahead forecasting figs s2 s3 it is noticeable that the number of variables for xgbt was much lower than that of xgbl and rf the variable importance plots fig 5b d f clearly demonstrate the importance of wt for example fig 5b indicates that lagged gwl lag 1 was the most important about 36 contribution variable followed by wavelet scaling coefficients v of cr tavg and r for wt xgbt model and wt rf also showed similar tendency fig 5f however the main contributing variables for xgbl were the wavelet coefficients w along with lagged gwl lag 1 fig 5d for 1 month ahead gwl forecasting another interesting finding of this study is that rf and xgb variants were less dependent on lagged gwl for longer lead time forecasts figs s2 s3 this is interesting since these approaches could be explored for long term gwl forecasting six months to several years mouatadid et al 2019 granted that adequate uncertainty assessment to be undertaken other studies have successfully applied ml approaches for long term forecasting of hydrological variables such as gwl by svr salem et al 2018 and stream flow by svr and elm zhu et al 2019 long term forecasting is important for sustainable water resources management though it is challenging due to high uncertainties mouatadid et al 2019 5 limitations and future trends despite significant progress in gwl modeling using ml approaches over the last two decades rajaee et al 2019 there are still some important issues that should be addressed to promote consistency between ml models and their conceptual and physical based counterparts koch et al 2019 so far several studies have drawn attention to the development of hybrid approaches for capturing different patterns e g trends periodicities transients or level shifts in time series data e g adamowski and chan 2011 suryanarayana et al 2014 rezaie balf et al 2017 rahman et al 2018 malekzadeh et al 2019 or to develop multi model ensemble approaches e g barzegar et al 2017 nadiri et al 2019 for improving model performance mainly for short term gwl forecasting for example this study developed extensive ml models for short term forecasting of gwl by coupling pre processing techniques such as wt we also coupled bho technique for optimizing model parameters for gwl forecasting the developed ml models demonstrated high performance for almost all cases and error generally increased with increasing forecast lead time however model performance for mid term and long term forecasting could likely be further enhanced by incorporating time series data of groundwater recharge groundwater pumping and evapotranspiration incorporating these relevant variables may improve the model performance for mid and long term forecasting for our study we only considered lagged gwl rainfall and average air temperature as model input variables which are generally considered as input variables for gwl forecasting using ml approaches rajaee et al 2019 in addition relevant studies generally ignore incorporating certain important variables such as well depths geological characteristics such as formation porosity and permeability and proximity to sink or source wells further exploration is necessary for systematically incorporating these important hydrological and geological variables in ml models in order to improve their performance for mid and long term forecasting as well as to ensure these models are consistent with conceptual and physical based models such considerations could provide further physical insights and would likely lead to a greater acceptance of ml models in the hydrological and water resources communities furthermore an alternative approach for generating gwl forecasts could be tested by considering an average gwl history based on all the available gwl historical records at all wells this average gwl can then be forecasted by ml approaches and the gwl forecasts at each well can be obtained from historical deviations of the wells gwl from the averaged value this approach only requires the development of a single ml model can be deployed at a regional scale and could provide competitive performance with the models developed in this study 6 conclusion multiscale gwl forecasting is a vital task for sustainable water resources management however generating accurate gwl forecasts is often challenging due to nonlinear relationships between gwl and explanatory variables as well as their multiscale behavior that changes with time one of the pre requisites for developing accurate ml models is to select only the most useful input variables from a set of candidate input variables along with optimizing model parameter for addressing these issues this study tested the ability of newer ml approaches such as random forests and variants of extreme gradient boosting which can perform input variable selection internally these models which are able to capture nonlinear dependencies between model inputs were fed with multiscale information extracted through wavelet transforms to enhance the forecasting models ability to address multiscale change furthermore these approaches were coupled for the first time with bayesian hyper parameter optimization for automatic model parameters estimation the developed models were explored for 1 2 and 3 months ahead gwl forecasting the performance of the wavelet based models wt xgbl wt xgbt and wt rf were compared against standalone non wavelet based models xgbl xgbt and rf including benchmark methods such as svr models and wt svr models which often show the best accuracy for gwl forecasting in different parts of the world however the svr model depends on additional external algorithms for selecting input variables the comparative study of standalone ml approaches revealed that new approaches especially the xgbl showed similar accuracy when compared to svr xgbt also showed acceptable accuracy although performance was slightly lower than that of svr rf generally showed the lowest performance for almost all cases the coupling of wt further improved the performance for all ml approaches and the improvement was more prominent for 3 months ahead 3 5 gwl forecasts than 1 month ahead lead forecasts which is promising for forecasting multiscale processes commonly found in hydrology and water resources furthermore xgb variants and rf provide an internal measure of variables importance which make the models more interpretative over other black box approaches such as svr the coupling of the ml approaches especially xgb variants with bho and wt therefore is a new promising framework for gwl forecasting and may deserve further study in the field of hydrology and water resources both for short term and long term forecasting credit authorship contribution statement a t m sakiur rahman conceptualization methodology formal analysis software validation visualization writing original draft takahiro hosono conceptualization supervision writing review editing john m quilty conceptualization software validation supervision writing review editing jayanta das formal analysis amiya basak formal analysis declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements we sincerely thank the editor prof g c sander guest editors profs pejman tahmasebi and muhammad sahimi and anonymous reviewers for their helpful comments that improved the quality of the final paper a t m s r is grateful to the ministry of culture sports science and technology japan for their financial support supplementary materials supplementary material associated with this article can be found in the online version at doi 10 1016 j advwatres 2020 103595 appendix supplementary materials image application 1 image application 2 
474,groundwater level gwl forecasting is crucial for irrigation scheduling water supply and land development machine learning ml e g artificial neural networks has been increasingly adopted to forecast gwl due to its ability to model nonlinearities between gwl and its drivers e g rainfall although ml approaches have been successful at forecasting gwl they are often inaccurate when gwl exhibits multiscale changes e g due to urbanization to address this shortcoming wavelet transforms wt are routinely coupled with ml methods unfortunately researchers frequently neglect key issues associated with wt that render such forecasts useless for real world scenarios this study demonstrates how new ml methods such as extreme gradient boosting and random forests can be properly coupled with wt to generate accurate gwl forecasts 1 3 months ahead for 7 wells in kumamoto city in southern japan that can be used to help address current pressing issues such as groundwater quality and land subsidence keywords groundwater level forecasting maximal overlap discrete wavelet transform extreme gradient boosting machine random forests support vector machine 1 introduction groundwater level gwl forecasting is a crucial task that allows water resources managers to create effective irrigation schedules manage water supply and plan land development simulation of gwl is often performed by processes oriented models with modflow mcdonald and harbaugh 1988 being the most popular mohanty et al 2013 advanced fully coupled and distributed models e g hydrogeosphere therrien et al 2010 and getflows mori et al 2015 hosono et al 2019 are becoming more popular for surface water e g mori et al 2015 ala aho et al 2015 and groundwater simulation maxwell et al 2015 ala aho et al 2015 hosono et al 2019 these models require a range of spatio temporal data for approximating the complex hydrological processes heterogeneous subsurface systems and anthropogenic activities therrien et al 2010 barthel and banzhaf 2016 kollet et al 2017 and consequently involves high cost and long computational time maxwell et al 2015 in addition achieving accurate simulations through process based models is very challenging sun et al 2016 sahoo et al 2017 due to the over simplification of complex hydrological processes therrien et al 2010 discretization of model domains white et al 2019 and availability of data researchers are therefore exploring alternative approaches such as data driven i e statistical and machine learning ml approaches over the last two decades see review by rajaee et al 2019 ml approaches avoid having to specify the underlying complex physical hydrological processes and instead rely on only the statistical relationships between explanatory e g rainfall and response variables e g gwl despite this lack of physical knowledge ml has been successfully applied to gwl forecasting rajaee et al 2019 some studies e g adamowski and chan 2011 barzegar et al 2016 belayneh et al 2016 quilty et al 2018 have also shown that hybrid approaches that combine different ml approaches in various stages of the model development can be more accurate than standalone ml since particular patterns in the data e g trends periodicities level shifts can be better captured by different approaches ghaemi et al 2019 for instance the wavelet transform wt a time frequency localization method that is able to extract time varying behavior trends periodicities etc or multiscale change from time series has often been coupled with ml for gwl forecasting with many studies showing increased performance from this approach over standalone ml nourani et al 2014 rajaee et al 2019 however recent studies zhang et al 2015 du et al 2017 quilty and adamowski 2018 have shown that these hybrid wt based ml models have been incorrectly developed and cannot be used properly for real world applications this study aims to demonstrate how a recent wt based forecasting approach the wavelet data driven forecasting framework wddff quilty and adamowski 2018 can be coupled with emerging ml methods for real world gwl forecasting to date there has been a plethora of ml approaches applied to gwl forecasting see list in rajaee et al 2019 such as multiple linear regression mlr e g sahoo and jha 2013 sun et al 2016 ebrahimi and rajaee 2017 artificial neural networks ann e g daliakopoulose et al 2005 zhang et al 2018 guzman et al 2017 wunsch et al 2018 natarajan and sudheer 2019 mirarabi et al 2019 extreme learning machine alizamir et al 2018 malekzadeh et al 2019 natarajan and sudheer 2019 fuzzy logic fl e g alvisi et al 2006 shiri and kisi 2011 nadiri et al 2019 adaptive neuro fuzzy inference system anfis kholghi and hosseini 2009 moosavi et al 2013 gong et al 2018 support vector machine svm e g huang et al 2017 natarajan and sudheer 2019 tang et al 2019 mirarabi et al 2019 genetic programming gp e g shiri et al 2013 fallah mehdipour et al 2013 natarajan and sudheer 2019 gradient boosting machine gbm kenda et al 2018 along with statistical approaches such as arima model adamowski and chan 2011 however two very recent and promising approaches for gwl forecasting include random forests rf wang et al 2018 tang et al 2019 kenda et al 2018 koch et al 2019 and extreme gradient boosting xgb rf and xgb are both tree based ml algorithms the former is based on bagged regression trees while the latter is based on boosted regression trees a comparison between these two methods has yet to be carried out for gwl forecasting nor have these methods been explored within the context of the wddff more recently to address multiscale change in hydrology and water resources koutsoyiannis 2013 researchers have coupled wt with ml to generate hybrid models for improving forecast accuracy of gwl adamowski and chan 2011 streamflow adamowski and sun 2010 tiwari and chatterjee 2010 water quality barzegar et al 2016 urban water demand quilty et al 2019 and water balance components rahman et al 2018 wt is the most popular approach for developing the accurate hybrid models for hydrological and water resources forecasting nourani et al 2014 afan et al 2016 fahimi et al 2017 quilty and adamowski 2018 however a recent study by quilty and adamowski 2018 revealed that many studies incorrectly developed wavelet based hybrid models which are not applicable for real world forecasting problems the authors proposed a set of best practices to address this issue resulting in the wavelet data driven forecasting framework wddff which can be used to correctly forecast real world processes such as gwl to date wddff has only been used with simple least squares regression methods e g multiple linear regression volterra series models extreme learning machines and has only been explored for urban water demand forecasting irrigation flow forecasting mouatadid et al 2019 and monthly pan evaporation prediction ghaemi et al 2019 while it has high potential for gwl forecasting hence there is a need to develop correct wavelet based ml models based on wddff that can be properly applied for a variety of critical real world problems such as gwl forecasting moreover new ml methods such as xgb and rf are only beginning to be explored in the hydrology and water resources domains for example the tree based version of xgb xgbt has only been explored for evapotranspiration wu et al 2019 wu and fan 2019 tropical cyclone jin et al 2019 and streamflow hadi et al 2019 tyralis et al 2019 li et al 2019 zhang et al 2019 modelling while the linear xgb xgbl has yet to be investigated for hydrological or water resources modelling although rf is one of most the successful ml methods across a variety of domains it has not received enough attention for hydrological and water resources modelling yet tyralis et al 2019 recent studies wang et al 2018 tang et al 2019 kenda et al 2018 koch et al 2019 have explored the potential of standalone rf model for gwl forecasting and found that rf can generate accurate gwl forecasts however xgbt xgbl and wavelet based rf have yet to be explored in the context of gwl forecasting in our current study we explored for the first time in the literature the potential of xgbt xgbl and rf as well as xgbt xgbl and rf coupled with the wddff from quilty and adamowski 2018 for gwl forecasting we compared these new approaches against benchmark methods support vector regression svr and their wddff counterparts additionally we employ a bayesian optimization routine based on gaussian processes snoek et al 2012 for automatic hyper parameter selection in xgbt xgbl rf and svr including their wt counterparts which as far as we are aware has yet to be explored in earlier studies employing xgb and rf this bayesian optimization routine is an effective hyper parameter selection approach that has been used in many fields of science and engineering with considerable success falkner et al 2018 therefore the main goal of this study is to demonstrate the potential usefulness of xgbt xgbl wt xgbt wt xgbl and wt rf for gwl forecasting through a comparison with methods developed earlier in the literature i e rf svr and wt svr for our case study we forecast average monthly gwl at 1 2 and 3 months ahead for seven wells in kumamoto city japan using meteorological variables as input data along with previous gwl measurements our main contributions to the literature include the first exploration of 1 xgbt xgbl wt xgbt wt xgbl and wt rf for gwl forecasting 2 xgbt xgbl rf and svr coupled with wddff i e from quilty and adamowski 2018 3 bayesian optimization for the automatic selection of hyper parameters in xgb and rf based models the rest of this study is organized as follows section 2 provides information on the study area and data section 3 outlines our methodology section 4 presents results and discussion and section 5 provides conclusions of our findings 2 study area and data we conducted our experimental study using seven wells where gwl observations were available in kumamoto city which is in the center of kyushu island in the southern part of japan fig 1 kumamoto city is one of the regions with the highest groundwater use in japan about one million city dwellers depend completely on groundwater for their domestic purposes oshima 2010 shimada 2012 and approximately 8 107 m3 groundwater per year is withdrawn from 58 pump stations to meet their water demand hosono et al 2013 there are two types of aquifers in this region unconfined and semi confined to confined aquifers the former is composed of alluvium sedimentary deposits and unwelded pyroclastic deposits overlying an impermeable aquiclude of the lacustrine sedimentary layer the latter is semi confined to confined taniguchi et al 2003 hosono et al 2013 in nature and it consists of pyroclastic flow deposits and volcanic lavas this aquifer is locally referred to as a second aquifer and its depth below the ground surface generally varies between 60 m and 200 m hossain et al 2016 majority groundwater is withdrawn from the second aquifer for water supply in the city hence this is the most important aquifer in the area a detailed hydrogeological description of the study area can be found in hosono et al 2013 and kagabu et al 2017 the majority of groundwater in the second aquifer is recharged through infiltration of precipitation in the recharge areas such as kikuchi ueki and takayubaru highlands as well as by river and artificial ponding waters in paddy fields in and around the midsection of the shira river hosono et al 2013 taniguchi et al 2019 fig 1 groundwaters in the recharge areas are then transported through highly porous pyroclastic flow deposits aso 2 and aso 3 and discharged mostly to lake ezu the monthly gwl data spans 1980 2017 and was collected from the government office of kumamoto city kumamoto city waterworks and sewerage bureau we analyzed gwl data of 7 wells that are located along the regional groundwater flows from recharge to discharge areas fig 1 all wells are installed into the second deep aquifer with their well depths ranging from 31 m well no ss 09 to 124 m well no ss 108 withan average of 68 28 m the screen depths range from 21 to 31 m well no ss 108 and 81 5 to 114 50 m well no ss 10 from the surface respectively the observed gwl data contains few missing 2 records missing data for a particular month were filled by the average of the previous and following month i e neighbouring months furthermore meteorological data monthly mean temperature and monthly total rainfall of the kumamoto meteorological station were gathered from the japan meteorological agency jma website https www jma go jp jma indexe html as potential inputs for the ml models developed in this study 3 methodology methodology section arranges for brief descriptions of the ml approaches such as svm rf xgb and wt as well as descriptions of the steps followed for the model development 3 1 support vector machine support vector machine svm is a statistical learning theory the basic algorithm of svm is discussed in cortes and vapnik 1995 svm is also known as support vector regression svr when applied to regression problems the library of svm libsvm chang and lin 2011 is commonly used for developing svr models for regression problems libsvm implements the sequential minimal optimization algorithm and it supports epsilon ε svr and nu γ svr regressions in this study svr models were developed through the r programing language environment r core development team 2019 using the libsvm adopted r package e1071 meyer et al 2019 the ε svr approach was adopted since it has been shown to be an accurate approach for predicting hydrological and water resources variables tabari et al 2012 raghavendra and deka 2014 mehdizadeh et al 2017 huang et al 2017 ferreira et al 2019 in svr the selection of the hyper parameters is of key importance in developing accurate models hyper parameters include kernel type e g linear polynomial radial basis function which controls the similarity between a current input and each input in the training set epsilon ε the width of insensitive tube which determines the support vectors that are considered in the model gamma γ controls the shape of the hyperplane separation and cost c a regularization parameter that controls the sparsity of the model aside from the selection of the kernel hyper parameter selection is discussed in section 3 5 4 several studies have demonstrated that radial basis function rbf kernel provides strong performance for predicting hydrological and water resources variables mehdizadeh et al 2017 ferreira et al 2019 and it has been used in recent studies for gwl forecasting yoon et al 2016 huang et al 2017 tang et al 2019 guzman et al 2019 further details on theoretical background regarding the ε svr and rbf can be found in chang and lin 2011 3 2 random forests random forest breiman 2001 is a ml algorithm derived from the classification and regression trees cart paradigm breiman et al 1984 with some additional randomization such as bagging input data samples and variables tyralis et al 2019 rf is a popular ml approach used for prediction and classification in numerous domains due to its high precision and capability to handle a large number of input variables pal 2016 tyralis and papacharalampous 2017 tyralis et al 2019 there exist several varieties of rf based on the cart criterion biau and scornet 2016 tyralis et al 2019 all variations of rf are not applicable for solving regression prediction problems geurts et al 2006 wright and ziegler 2017 and many of them are tested mainly for classification geurts et al 2006 biau and scornet 2016 the regression variant of rf is a nonlinear model in which samples are partitioned at each node of a tree based on the value of a selected input variable breiman 1984 in this study extremely randomized trees details in geurts et al 2006 applicable for regression geurts et al 2006 biau and scornet 2016 wright and ziegler 2017 was applied for tree splitting this approach is the most computationally efficient variation of rf geurts et al 2006 wright and ziegler 2017 and has been shown to achieve similar performance with other similar rf based algorithms biau and scornet 2016 the performance of rf also depends on several hyper parameters biau and scornet 2016 wright and ziegler 2017 tyralis and papacharalampous 2017 probst et al 2019 tyralis et al 2019 such as number of trees n indicating the size of the forest mtry representing the number of input variables to split in each node minimal node size the minimum number of observations in a terminal node and sample size the number of observations randomly selected for growing each tree 3 3 extreme gradient boosting the extreme gradient boosting is a scalable tree boosting ml method chen and guestrin 2016 that can capture the nonlinear relationships between predictor and target variables chen et al 2019 hadi et al 2019 xgb is also based on cart and resembles gradient boosting machines with some variations in model structure chen and guestrin 2016 gbm and xgb differ from rf in how trees are grown for example gbm grows tree on the residuals of the former tree whereas rf trains each tree individually just et al 2018 xgb implements second order derivatives whereas ordinary gbm uses first order derivatives chen and guestrin 2016 xgb is more efficient compared to gbm and is less likely to result in model over fitting since it incorporates improved regularization features chen and guestrin 2016 chen et al 2019 the parallel processing of the computation makes it not only the fastest among the gradient boosting algorithms but also affords improved performance over ordinary gbm chen et al 2019 in the generic version of cart a tree ensemble model with k additive functions is used to predict the target variable y i r such as gwl using input variable s x i r d where i 1 2 n and n are the number of input target variable pairs via chen and guestrin 2016 1 y i ϕ x i k 1 k f k x i f k f here ϕ x i maps the model inputs to the model prediction y i r where f f x w q x q r m t w r t denotes the space for the regression tree q represents the structure of each tree fk corresponds to the individual tree structure t is the number of leaves in the tree fk and w is the leaf weights in xgb the objective function incorporates a loss function and a regularization term given as chen and guestrin 2016 2 l ϕ i l y i y i k ω f k where 3 ω f γ t 1 2 λ w 2 here l is the function for the differentiable convex loss used for measuring the difference between the target yi and prediction y i during training the second term ω denotes the regularization term which is used for model complexity penalization since model optimization cannot be achieved using traditional methods for the tree ensemble approach xgbt using eq 2 the additive approach is applied in model training for xgbt chen and guestrin 2016 moreover the function ft is added for improving the model eq 2 for predicting the y i t after the t th iteration for the i th instance as chen and guestrin 2016 4 l t i 1 n l y i y i t 1 f t x i ω f t for quick optimization of the objective function taylor second order expansion is applied by following friedman et al 2000 5 l t i 1 n l y i y i t 1 g i f t x i 1 2 h i f t 2 x i ω f t here the gradient statistics of the loss function for the first g i and second h i order are as follows 6 g i y t 1 l y i y i t 1 and 7 h i y t 1 2 l i y i t 1 the objective is simplified by removing the constant term for step t as 8 l t i 1 n g i f t x i 1 2 h i f t 2 x i ω f t the eq 8 can be rewritten by defining the ij i q x i j and expanding ω as 9 l t i 1 n g i f t x i 1 2 h i f t 2 x i γ t 1 2 λ j 1 t w j 2 j 1 t i i j g i w j 1 2 i i j h i λ w j 2 γ t now the weight w j and the subsequent optimal value l t q of a leaf j for a fixed structure q x can be estimated as 10 w j i i j g i i i j h i λ and 11 l t q 1 2 j 1 t i i j g i 2 i i j h i λ γ t a tree structure q quality can be measured using eq 11 however computing all the trees and simultaneously finding the best one from all possible options is computationally intractable and the problem is solved by using an algorithm that adds branches to the tree at each iteration from a single leaf if the tree splitting for right and left nodes are ir and il respectively the loss reduction by adding them as i il ir can be estimated as chen and guestrin 2016 12 l split 1 2 i i l g i 2 i i l h i λ i i r g i 2 i i r h i λ i i g i 2 i i h i λ γ in this study we applied two variants of xgb such as linear boosting of xgb i e xgbl and tree boosting i e xgbt the main difference between these two variations of xgb is the base learner the base leaner is the linear learner for the xgbl whereas the base learner is the tree learner for the xgbt chen 2014 the interested reader can find further details in chen 2014 a number of hyper parameters need to be set for developing xgb models the hyper parameters considered for xgb model are eta representing the learning rate gamma representing the minimum split loss max depth indicating the maximum depth of a tree min child weight the minimum sum of the weights subsample ratio of the samples used for model training column sample by tree parameter for subsampling of columns i e variables nround number of rounds and two regularization parameters lambda which controls l2 regularization and alpha which controls for l1 regularization 3 4 wavelet transformation wavelet transforms are a mathematical tool that is particularly useful for identifying important time frequency localized features in a time series e g periodicities transients level shifts wt can decompose a time series into different sub time series which provide coarse and fine grained details on the multiscale nature of the time series incorporating these sub time series into a ml model has often improved the accuracy of hydrological and water resources forecasts when compared to standalone ml models afan et al 2016 fahimi et al 2017 mouatadid et al 2019 there are two variations of wt continuous cwt and discrete dwt several studies have already discussed the advantages and disadvantages of the two variations of wt fugal 2009 nourani et al 2014 and recommended that dwt is better for decomposing hydrological and water resources time series data nourani et al 2014 rezaie balf et al 2017 samadianfard et al 2018 mouatadid et al 2019 recent studies have noted that dwt with multiresolution analysis dwt mra is the most common wt that is adopted in hydrological and water resources modelling nourani et al 2014 quilty and adamowski 2018 rajaee et al 2019 however the dwt mra has been demonstrated to have several drawbacks for when used in real world applications maheswaran and khosa 2012 du et al 2017 quilty and adamowski 2018 for example dwt is sensitive to the length of time series and adding new data points to the time series i e in an online system as new data is collected and performing wavelet decomposition causes the wavelet and scaling coefficients computed at a given time to change quilty and adamowski 2018 to overcome these drawbacks the maximal overlap dwt modwt proposed by percival and walden 2000 was implemented in this study as recommended by quilty and adamowski 2018 the modwt applies low and high pass filters to a given time series resulting in wavelet and scaling coefficients if h j l and g j l here l denotes the length of the filter j 1 2 j represents the j th scale and j is the level of decomposition are the wavelet and scaling filters the modwt can be given as percival and walden 2000 13 h j l h j l 2 j 2 14 g j l g j l 2 j 2 the modwt wavelet w j i and scaling v j i coefficients for a given time series x xi i 0 1 n 1 can be determined via percival and walden 2000 15 w j i l 0 l j 1 h j l x i lmodn 16 v j i l 0 l j 1 g j l x i lmodn here lj 2 j 1 l 1 1 further details on the modwt can be obtained from earlier studies e g percival and walden 2000 quilty and adamowski 2018 mouatadid et al 2019 3 5 model development this section gives details on the ml based models xgbt xgbl rf svr wt xgbt wt xgbl wt rf and wt svr developed in this study for gwl forecasting 3 5 1 dataset pre processing and boundary corrections for our case study the target response variable was average monthly gwl at lead times of 1 2 and 3 months ahead for the standalone ml models xgbt xgbl rf and svr candidate input explanatory variables included time lagged from lag 1 up to lag 12 depending on the target variable forecasting step more details in supplementary material sm table s1 data such as average monthly gwl average monthly air temperature tavg average monthly total rainfall r and cumulative monthly rainfall cr see also section 3 4 these are the common explanatory variables considered for gwl forecasting in several earlier studies e g adamowski and chan 2011 wunsch et al 2018 nadiri et al 2019 furthermore water fluxes into and out of the reservoir are considered by incorporating variables such as rainfall and air temperature related to evapotranspiration in the ml models for the wt based models wt xgbt wt xgbl wt rf and wt svr original and decomposed data were used as the candidate input variables modwt was applied for decomposing the explanatory variables see section 3 4 since the wavelet decomposed data is affected by boundary conditions quilty and adamowski 2018 percival and walden 2000 basta 2014 it was necessary to remove the first lj wavelet and scaling coefficients see section 3 4 affected by boundary conditions due to the selected wavelet filter length l and decomposition level j at the beginning of the decomposed data however since this study compares non wavelet based and wt models that vary based on decomposition level and wavelet filter it was necessary to remove the same number of boundary conditions affected coefficients from the beginning of both the target and all input variables for both non wavelet and wt models to ensure the same training and validation indices were used in the development and evaluation of the different models this is based on the best practices established in quilty and adamwoski 2018 the number of boundary conditions effected coefficients that were removed from the beginning of the target and input variables in all models lf was determined according to quilty and adamowski 2018 17 l f 2 j m a x 1 l 1 1 the careful selection of jmax and l is necessary to ensure the number of data available for model training and validation is sufficient in this study jmax and l were set to four hence the total number of the boundary conditions effected coefficient was 46 which was removed from the beginning of the target and input variables for all models e g fig 2 a decomposition level of four was selected to ensure that the prominent annual cycle present in all gwl time series was captured in the level four wavelet coefficients after adjusting the original dataset 456 records for the lagged target and input variables the historical record available ranged from march 1981 to december 2017 442 records the number of available records 396 for training and validating the different ml models after adjusting for time lags and boundary conditions were divided into the training 85 and validation 15 sets while there is no set rule for dataset partitioning during model training and validation deo et al 2017 it is typically accepted that the validation partition should be within 10 40 of the total dataset length barzegar et al 2019 3 5 2 input variable selection and importance of input variables input variable selection is one of the most important steps for developing ml models tiwari and chatterjee 2010 galelli et al 2014 quilty et al 2016 both linear e g sudheer et al 2002 yaseen et al 2015 and nonlinear approaches quilty et al 2016 have been applied for input variable selection in hydrological and water resources modelling galelli et al 2014 however linear approaches based on partial auto correlation function pacf and auto correlation function acf are typically not the most suitable approaches for hydrological and water resources modelling since such problems are generally nonlinear hadi et al 2019 nonlinear approaches based on mutual information mi shannon 1948 typically perform better than linear approaches for hydrological and water resources modelling see details in quilty et al 2016 gong et al 2018 taormina et al 2016 as well as other science and engineering applications peng et al 2005 galelli et al 2014 as will be discussed below since svr does not internally measure the importance of input variables it is important to perform input variable selection prior to developing an svr model in this study we applied one of the most widely used mi approaches for input variable selection minimal redundancy maximal relevance mrmr proposed by peng et al 2005 this approach selects input variables from a set of candidates by using mi to identify candidates that are relevant but not redundant this approach has been shown to select more appropriate input variables than other similar approaches berrendero et al 2016 peng et al 2005 defined an operator φ d r for simultaneously optimizing the maximum relevance d and minimal redundancy r to select an input subset s from the d input variables in x as 18 max φ d r φ d r the complete mathematical derivations for mrmr can be obtained from peng et al 2005 however the input variable selection is not mandatory for approaches like xgb and rf both approaches internally perform input variable selection as well as quantify the variables importance vi chen and guestrin 2016 tyralis et al 2019 hadi et al 2019 jin et al 2019 however svm does not produce any internal measures of vi boehmke and greenwell 2019 hadi et al 2019 pointed out that xgb is a very robust modelling approach even with a high number of explanatory variables the performance of rf is also not overly influenced by additional superfluous input variables i e granted the model is provided with some input variables that are useful in predicting the target variable biau and scornet 2016 pal 2016 probst et al 2019 tyralis et al 2019 the vi in rf was estimated by the advanced permutation method strobl et al 2009 and in xgbt vi was measured via the impurity approach chen and guestrin 2016 boehmke and greenwell 2019 while xgbl estimate vi by measuring the weight chen 2014 3 5 3 data scaling approaches like xgbl and svr are affected by the scale of the different input variables in other words if input variables are on widely different scales this can lead to the model favoring those variables that have a large range while ignoring those with a smaller range regardless of their importance in predicting the target variable hence input variable scaling was performed prior to the development of the xgbl and svr models since the svr models were developed using the e1071 r package it internally performs input data scaling by scaling all variables to have unit variance and zero mean meyer et al 2019 for the xgbl models the input variables were scaled such that they ranged from 0 to 1 using the min max normalization approach which has been successfully applied in many earlier hydrological and water resources ml models e g yaseen et al 2016 jeong and park 2019 min max normalization for a given input variable is determined using 19 x x x m i n x m a x x m i n here x is the original input variable and x is the normalized input variable xmin and xmax are the minimum and maximum values of the input variable respectively 3 5 4 hyper parameter optimization most ml algorithms require the careful selection of a set of hyper parameters probst et al 2019 examples of hyper parameters include the kernel function in svr the number of trees in rf the learning rate in xgb etc hyper parameters for each ml model used in this study are mentioned in the relevant sections 3 1 for svr 3 2 for rf and 3 3 for xgb the performance of ml models is highly influenced by hyper parameters settings eggensperger et al 2013 zhang et al 2016 and their improper selection can lead to poorly performing models many studies select hyper parameters through trial and error suryanarayana et al 2014 zhang et al 2016 jeong and park 2019 grid search and or random search bergstra and bengio 2012 and some use heuristic optimization algorithms such as particle swarm optimization and genetic algorithms snoek et al 2012 bergstra and bengio 2012 feurer and hutter 2019 ideally an accurate and efficient automated hyper parameter optimization method is desirable zhang et al 2016 and useful for permitting a fair comparison between ml alternative sculley et al 2018 furthermore when exploring different ml models they can be equally compared only if they all receive the same degree of optimization or level of attention for the problem at hand feurer and hutter 2019 bayesian hyper parameter optimization bho snoek et al 2012 is an emerging state of the art approach for global and local optimization of hyper parameters falkner et al 2018 feurer and hutter 2019 bho has been shown to outperform other approaches such as grid search and random search bergstra and bengio 2012 on several challenging optimization benchmarks jones 2001 bho has broad applicability to various problem settings falkner et al 2018 feurer and hutter 2019 can be used with integer and real valued hyper parameters and in many cases can outperform domain experts for finding optimal hyper parameters snoek et al 2012 eggensperger et al 2013 feurer and hutter 2019 in this study the bho algorithm snoek et al 2012 and software implementation in wilson 2019 was implemented for hyper parameter selection bho was used for hyper parameters optimization by using k fold cross validation on the training data k fold cross validation has been successfully applied for hyper parameter selection in several hydrological and water resources modelling studies e g barzegar et al 2019 wu and fan 2019 in this study we set k 5 for performing bho it is necessary to choose a prior and an acquisition function the former adopts a gaussian process prior and the latter uses expected improvement ei snoek et al 2012 the usefulness of gaussian processes is solely dependent on the covariance function which makes use of the matern52 kernel snoek et al 2012 3 5 5 coupling of ml and modwt to develop the wt models the ml approaches xgbt xgbl rf and svr were integrated within the wddff by following the direct approach and using the modwt for wavelet decomposition of the input variables approach 4 quilty and adamowski 2018 approach 4 includes both un decomposed and wavelet decomposed boundary condition corrected time series data as input variables furthermore there is no single wavelet filter and decomposition level j combination that has been demonstrated to provide the best performance across all hydrological and water resources modelling applications maheswaran and khosa 2012 mouatadid et al 2019 in this study three different wavelet families such as haar daubechies d2 and best localized bl 4 with j from 1 up to 4 were tested to identify the best wt models therefore the total number of hybrid models for the seven gwl monitoring wells and the three forecasting lead times 1 2 and 3 months ahead was 1008 along with 84 standalone models see table s2 3 5 6 model accuracy the accuracy of applied ml models was evaluated by a number goodness of fit statistics such as mean squared error mse mean absolute error mae root mean square error rmse rmse observations standard deviation ratio rsr moriasi et al 2007 coefficient of determination r2 nash sutcliffe efficiency nse nash and sutcliffe 1970 and kling gupta efficiency kge gupta et al 2009 rsr can be used to interpret a given model s forecasts as very good 0 00 rsr 0 50 good 0 50 rsr 0 60 satisfactory 0 60 rsr 0 70 and unsatisfactory rsr 070 moriasi et al 2007 and nse as very good 0 75 nse 1 good 0 65 nse 0 75 satisfactory 0 50 nse 0 65 and unsatisfactory nse 0 50 moriasi et al 2007 4 results and discussion the results of the ml approaches for 1 2 and 3 month s ahead lead time forecasting of gwl was evaluated using several goodness of fit statistics mse mae rmse rsr nse kge and r2 and graphical tools hydrographs scatterplots and boxplots the experiment results showed a good trade off between training and validation performance confirming the stable generalization capacity of the ml approaches for the standalone and wavelet based models the results obtained for the validation period are presented due to the large number of models and to focus on the generalization abilities of the various models 4 1 standalone models the results rmse nse and r2 of the standalone ml approaches for forecasting gwl at 1 2 and 3 month s ahead can be found in table 1 and results from additional statistics including mae mse rsr and kge are also provided in tables s3 s6 the average accuracy across the seven wells for the standalone ml approaches showed very good performance in terms of nse nse 0 75 and rsr rsr 0 50 moriasi et al 2007 with higher values of kge and lower values of mse and rmse for 1 and 2 except rf for 2 step month s ahead forecasts the nse values ranged between 0 84 and 0 96 with an average of 0 93 for svr 0 85 and 0 97 with an average of 0 92 for xgbl 0 75 and 0 95 with an average of 0 86 for xgbt and 0 55 and 0 93 with an average of 0 81 for rf respectively for 1 month ahead gwl forecasting table 1 the nse values for 2 months ahead gwl forecasts were slightly lower than 1 month ahead forecasts with average nse values across all seven wells of 0 86 for svr 0 81 for xgbl 0 75 for xgbt and 0 70 for rf respectively table 1 although the accuracy further decreased with increasing lead time svr avg nse 0 78 and xgbl avg nse 0 75 showed very good performance for 3 months ahead gwl forecasting while xgbt avg nse 0 67 and rf avg nse 0 66 showed satisfactory performance an example of comparison between observed and forecasted gwl for 1 2 and 3 months ahead is illustrated in fig 3 using hydrographs and scatterplots for well ss 001 it is noticeable that all the methods showed decaying accuracy as well as higher deviations from the 1 1 line with increasing time steps fig 3 and table 1 other gwl forecasting case studies also found similar results though considering different ml approaches yoon et al 2011 rezaie balf et al 2017 barzegar et al 2017 the best performing model was the svr for all lead time forecasting based on the average goodness of fit statistics a number of studies yoon et al 2011 suryanarayana et al 2014 huang et al 2017 mukherjee and ramachandran 2018 guzman et al 2019 mirarabi et al 2019 have reported that svr is the best performing model for gwl forecasting while comparisons were made against several ml approaches such as mlr ann and rf rajaee et al 2019 however most of the relevant studies only tested the capability of the different ml approaches for limited number 4 of observation wells however it can be argued from the obtained results of this study that different ml approaches showed the best performance for different observation wells table 1 for example svr was the best model for a well ss 07 whereas xgbl was the best model for another well ss 10 table 1 furthermore svr and xgbl showed similar performance for some wells ss 001 and ss 09 while rf exhibited the lowest accuracy for all lead times for almost all observation wells table 1 with an unsatisfactory performance for a well ss 07 the rf models show for certain cases lower accuracy than highly parameterized tree based models such as xgb due to the automation of the model efron and hastie 2016 tyralis et al 2019 it is worth mentioning again that it is necessary to select input variables for svr models using another algorithm while xgb and rf models do not require such extra efforts for modeling therefore it can be inferred that the standalone xgb model can effectively manage a high number of input variables without application of other algorithms which is the main advantage of xgb variants and they can perform equally with other high performing models like svr 4 2 wt coupled models the results of the best performing wavelet based hybrid model for each ml such as wt svr wt rf wt xgbl and wt xgbt approach can be found in table 2 and tables s3 s6 interestingly nearly half of the hybrid models 556 showed less accuracy than standalone models and many wavelet based models 198 showed similar results furthermore the same results were also found for the best models with different wt decomposition levels and wavelet filters table 2 the reason behind this is that wavelet based models are not guaranteed to outperform standalone models in this study only a small number of wavelet filters were explored due to the boundary condition at the beginning of the time series see eq 17 which needs to be carefully considered in light of the length of the dataset unfortunately many studies overlook this boundary condition and consider wavelet filters that are too wide and decomposition levels that are too high causing errors to be introduced in the wavelet and scaling coefficients sometimes leading to unrealistically high performance than what is achievable in real world scenarios quilty and adamowski 2018 which this study avoided by following the best practices in quilty and adamowski 2018 in this study it was confirmed that no particular wavelet filter and decomposition level universally led to improved performance of the hybrid models over the standalone models for gwl forecasting several studies also found similar results for other hydrological variables e g witten et al 2011 quilty and adamowski 2018 the most significant finding in table 2 is that the wt xgbl showed the best performance for many wells for 1 month ahead forecasting furthermore the performance of wt xgbl was also better than standalone xgbl for most cases tables s3 s6 the average nse and r2 values for the seven wells at the 2 month ahead were 0 86 and 0 87 for wt svr 0 84 and 0 87 for wt xgbl 0 76 and 0 80 for wt xgbt and 0 70 and 0 77 for wt rf respectively while for 3 months ahead gwl forecasts average nse and r2 values were 0 78 and 0 82 for wt svr 0 81 and 0 84 for wt xgbl 0 70 and 0 78 for wt xgbt and 0 66 and 0 74 for wt rf respectively when the comparisons between standalone models and wavelet based models were made fig 4 fig s1 and tables s3 s6 the improvement was more evident for 2 and 3 months ahead forecasts except few cases than that of 1 month ahead forecasting the improvement in accuracy measures based on average nse were 4 3 months ahead for wt svr 5 3 months ahead for wt xgbl 3 3 months ahead for wt xgbt and almost no improvement for rf 3 months ahead forecasts it is worthwhile to note that these percentages were calculated by considering the maximum nse value equal to 1 this finding is very crucial to improving water resources management since the mid term forecasting such as 2 and 3 months ahead is useful for irrigation planning land development and other environmental considerations 4 3 variables of importance rf and xgb variants naturally provide the importance of explanatory variables in predicting the target variable see section 3 5 2 it was found that some variables have significant relative contributions to the models and most of them showed very low contributions or no contribution as shown in fig 5 explanatory variables with at least 1 contribution for wavelet based model are only shown similar findings have also been reported by hadi et al 2019 while xgbt was applied for streamflow modelling it was generally observed that lagged gwl lag 1 as expected was the most important variables for 1 month ahead forecasting however cumulative rainfall cr was generally the most important variable for 2 and 3 months ahead forecasting figs s2 s3 it is noticeable that the number of variables for xgbt was much lower than that of xgbl and rf the variable importance plots fig 5b d f clearly demonstrate the importance of wt for example fig 5b indicates that lagged gwl lag 1 was the most important about 36 contribution variable followed by wavelet scaling coefficients v of cr tavg and r for wt xgbt model and wt rf also showed similar tendency fig 5f however the main contributing variables for xgbl were the wavelet coefficients w along with lagged gwl lag 1 fig 5d for 1 month ahead gwl forecasting another interesting finding of this study is that rf and xgb variants were less dependent on lagged gwl for longer lead time forecasts figs s2 s3 this is interesting since these approaches could be explored for long term gwl forecasting six months to several years mouatadid et al 2019 granted that adequate uncertainty assessment to be undertaken other studies have successfully applied ml approaches for long term forecasting of hydrological variables such as gwl by svr salem et al 2018 and stream flow by svr and elm zhu et al 2019 long term forecasting is important for sustainable water resources management though it is challenging due to high uncertainties mouatadid et al 2019 5 limitations and future trends despite significant progress in gwl modeling using ml approaches over the last two decades rajaee et al 2019 there are still some important issues that should be addressed to promote consistency between ml models and their conceptual and physical based counterparts koch et al 2019 so far several studies have drawn attention to the development of hybrid approaches for capturing different patterns e g trends periodicities transients or level shifts in time series data e g adamowski and chan 2011 suryanarayana et al 2014 rezaie balf et al 2017 rahman et al 2018 malekzadeh et al 2019 or to develop multi model ensemble approaches e g barzegar et al 2017 nadiri et al 2019 for improving model performance mainly for short term gwl forecasting for example this study developed extensive ml models for short term forecasting of gwl by coupling pre processing techniques such as wt we also coupled bho technique for optimizing model parameters for gwl forecasting the developed ml models demonstrated high performance for almost all cases and error generally increased with increasing forecast lead time however model performance for mid term and long term forecasting could likely be further enhanced by incorporating time series data of groundwater recharge groundwater pumping and evapotranspiration incorporating these relevant variables may improve the model performance for mid and long term forecasting for our study we only considered lagged gwl rainfall and average air temperature as model input variables which are generally considered as input variables for gwl forecasting using ml approaches rajaee et al 2019 in addition relevant studies generally ignore incorporating certain important variables such as well depths geological characteristics such as formation porosity and permeability and proximity to sink or source wells further exploration is necessary for systematically incorporating these important hydrological and geological variables in ml models in order to improve their performance for mid and long term forecasting as well as to ensure these models are consistent with conceptual and physical based models such considerations could provide further physical insights and would likely lead to a greater acceptance of ml models in the hydrological and water resources communities furthermore an alternative approach for generating gwl forecasts could be tested by considering an average gwl history based on all the available gwl historical records at all wells this average gwl can then be forecasted by ml approaches and the gwl forecasts at each well can be obtained from historical deviations of the wells gwl from the averaged value this approach only requires the development of a single ml model can be deployed at a regional scale and could provide competitive performance with the models developed in this study 6 conclusion multiscale gwl forecasting is a vital task for sustainable water resources management however generating accurate gwl forecasts is often challenging due to nonlinear relationships between gwl and explanatory variables as well as their multiscale behavior that changes with time one of the pre requisites for developing accurate ml models is to select only the most useful input variables from a set of candidate input variables along with optimizing model parameter for addressing these issues this study tested the ability of newer ml approaches such as random forests and variants of extreme gradient boosting which can perform input variable selection internally these models which are able to capture nonlinear dependencies between model inputs were fed with multiscale information extracted through wavelet transforms to enhance the forecasting models ability to address multiscale change furthermore these approaches were coupled for the first time with bayesian hyper parameter optimization for automatic model parameters estimation the developed models were explored for 1 2 and 3 months ahead gwl forecasting the performance of the wavelet based models wt xgbl wt xgbt and wt rf were compared against standalone non wavelet based models xgbl xgbt and rf including benchmark methods such as svr models and wt svr models which often show the best accuracy for gwl forecasting in different parts of the world however the svr model depends on additional external algorithms for selecting input variables the comparative study of standalone ml approaches revealed that new approaches especially the xgbl showed similar accuracy when compared to svr xgbt also showed acceptable accuracy although performance was slightly lower than that of svr rf generally showed the lowest performance for almost all cases the coupling of wt further improved the performance for all ml approaches and the improvement was more prominent for 3 months ahead 3 5 gwl forecasts than 1 month ahead lead forecasts which is promising for forecasting multiscale processes commonly found in hydrology and water resources furthermore xgb variants and rf provide an internal measure of variables importance which make the models more interpretative over other black box approaches such as svr the coupling of the ml approaches especially xgb variants with bho and wt therefore is a new promising framework for gwl forecasting and may deserve further study in the field of hydrology and water resources both for short term and long term forecasting credit authorship contribution statement a t m sakiur rahman conceptualization methodology formal analysis software validation visualization writing original draft takahiro hosono conceptualization supervision writing review editing john m quilty conceptualization software validation supervision writing review editing jayanta das formal analysis amiya basak formal analysis declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements we sincerely thank the editor prof g c sander guest editors profs pejman tahmasebi and muhammad sahimi and anonymous reviewers for their helpful comments that improved the quality of the final paper a t m s r is grateful to the ministry of culture sports science and technology japan for their financial support supplementary materials supplementary material associated with this article can be found in the online version at doi 10 1016 j advwatres 2020 103595 appendix supplementary materials image application 1 image application 2 
