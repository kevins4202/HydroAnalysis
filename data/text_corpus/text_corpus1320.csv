index,text
6600,balancing the multiple purposes of reservoir operation including the social economic and ecological aspects has become one of the most complicated problems facing the managers and operators worldwide this paper formulates a detailed mathematical model to optimize the reservoir operation during the flood season considering the three major operational purposes i e flood control social purpose energy production economic purpose and management of sediment transport ecological purpose the entire formulations conform to a nonlinear programming nlp model which can be solved effectively and efficiently by the multi start solver from the lingo software through equivalent transformation this model is applied to the three gorges project tgp in china which is the world s largest and most complex hydraulic engineering in operation three typical flood hydrographs i e dry normal and wet scenarios are selected as model inputs under the three strategies i e design operating rules increasing the flood control water level fcwl and impounding in advance the trade offs among these three major operational purposes are quantitatively analyzed moreover phosphorus p one of the sediment associated environmental factors is introduced for exploring its responses to various scenarios and strategies results indicate that a higher fore bay water level leads to a greater energy production a smaller flood peak but less sediment and p transports to the downstream under the design operating rules the maximum energy production of the tgp can reach 560 620 and 699 108 kwh respectively in dry normal and wet scenarios corresponding to a flood peak of 25000 27500 m3 s a sediment load of 1517 1999 104 t and a p load of 3152 3740 104 kg to the downstream whereas the maximum sediment loads are 2796 4640 and 5377 104 t and the maximum p loads are 4021 5345 and 5784 104 kg respectively in dry normal and wet scenarios corresponding to an energy production of 500 588 108 kwh and a flood peak of 38600 52500 m3 s to the downstream in terms of achieving the comprehensive benefits of the tgp impounding in advance is a better choice than both the design operating rules and increasing the fcwl for it can increase the energy production and the probability of full refill with almost no increase in flood risk to both the dam and the middle and lower reaches of the yangtze river as well as smaller reduction in the sediment and p transports the formulated detailed mathematical model is a general one that is applicable to a variety of reservoirs owning multiple operational purposes moreover the achievements gained from this paper will provide important references for the managers and operators keywords reservoir operation multi objective optimization energy production flood control sediment and phosphorus transports three gorges project 1 introduction construction and operation of reservoirs can support a variety of social economic and ecological purposes including energy production flood control water supply navigation as well as management of sediment transport balancing these competing objectives has become one of the most complicated problems facing the managers and operators worldwide hydropower is the world s largest source of renewable energy and 16 4 of total energy demand was supplied by hydropower which is more than the total amount of all other renewable energies such as wind power 5 6 biopower 2 2 and solar photovoltaic 1 9 iha 2018 meanwhile hydropower has one of the lowest lifecycle greenhouse gas emissions per kilowatt hour among all energy sources and the use of which has reduced 4 109 t of greenhouse gas emissions ipcc 2014 to ensure the safety of flood control the fore bay water level of a reservoir should be kept at the flood control water level fcwl during the flood season and can be raised gradually to the normal pool level npl since the late flood season however this rule makes the reservoir be well prepared throughout the flood season for the very low probability flood event and huge amounts of flood water resources have to be spilled during the flood season affecting the comprehensive benefits of the reservoir huang et al 2015a thus a series of alternative operational strategies have been proposed such as impounding in advance and dynamic control of fore bay water level which aim at maximizing the comprehensive benefits increasing energy production decreasing spilled water and improving the full refill probability under the premise of not lowering the flood control standard li et al 2010 liu et al 2011 these strategies focus too much on the operational purposes of energy production and flood control while their impacts on the ecological purpose such as sediment and its associated environmental factors are still needed to be explored a reservoir blocks the river and destroys the continuity of riverine flows of ecological importance which is vulnerable to the effects of reservoir impoundment suen and eheart 2006 wang et al 2008 the river slope decreases and the flow slows in front of the dam after reservoir impoundment resulting in the sediment deposition in the reservoir and the consequent reduction in the effective storage capacity li et al 2011 syvitski et al 2005 estimated that a total of 1 1011 t sediment have been sequestered in reservoirs constructed largely within the past 50 years and the global estimates for the annual loss of reservoir storage due to sedimentation varied from 0 5 to 1 0 white 2010 accordingly there is a decline of sediment flux to the lower river reaches which is closely related to the number and storage of reservoirs present in the river basin gupta et al 2012 leading to the erosion of the riverbed behind the dam yuan et al 2012 wang et al 2017 and affecting the development and evolution of the estuarine delta as well as other ecological issues jiao et al 2007 thus the strategy that storing clear water and releasing muddy is generally applied for reservoirs in high sediment laden rivers wang and hu 2009 meanwhile the ecological impacts of reservoir operation have drawn wide attention in recent years mitsch et al 2008 yi et al 2010 xu et al 2011 for instance the impoundment increases the retention of nutrients in the reservoir giving rise to a frequent occurrence of eutrophication mep 2013 2016 ji et al 2017 while the reduction in nutrients transporting to the lower river reaches probably change the trophic structure and affect the plankton community and biodiversity of the lower river reaches stone 2008 chai et al 2009 phosphorus p one of the key nutrients affecting water quality plays a crucial role in the aquatic eco environment schindler 2006 elser et al 2007 schelske 2009 there are complicated physicochemical interactions between p and sediment particles especially the fine sediment horowitz 2008 huang et al 2016 and most p in water are adsorbed by sediment particles and transported in the particulate phase withers and jarvie 2008 that is to say a large amount of the upstream p would be intercepted together with the reservoir sedimentation iwhr 2012 huang et al 2015a 2017 and further the geochemical characteristics of river as well as the aquatic ecosystem are altered zhou et al 2013 however to our knowledge few operational strategies of reservoirs have focused on the transport of nutrients cunha et al 2016 yu et al 2018 and it is necessary to identify the response of p transport to the variation of sediment flux resulting from management and operation of a reservoir beusen et al 2005 thus optimal operation is required for reservoirs to balance the multiple operational purposes including the traditional operational purposes and ecological purposes wan et al 2010 wang et al 2015 this paper formulates a detailed mathematical model to optimize the reservoir operation during the flood season which considers the three major operational purposes of flood control social purpose energy production economic purpose and sediment transport ecological purpose the model is applied to the three gorges project tgp which is the world s largest and most complex hydraulic engineering in operation and the trade offs among these three operational purposes are presented quantitatively in dry normal and wet scenarios under various operational strategies meanwhile p one of the sediment associated environmental factors is introduced as a case for exploring its response to these scenarios and strategies it is worth noting that the reservoir development and operation would increase the water loss by increasing the evapotranspiration through impoundment and flow regulation jaramillo and destouni 2015 levi et al 2015 thereby altering the local water cycle and then the runoff and its temporal variability and introducing the uncertainties in studies destouni et al 2010 2013 khazaei et al 2019 meanwhile the cascade development of reservoirs e g in the upper yangtze river would further increase these uncertainties thus imposing considerable effects on the flow and sediment and nutrients transport bring et al 2015 t√∂rnqvist et al 2015 these changing circumstances are not taken into account in this study and will be addressed in the future moreover although p is only one case for identifying the trade off between traditional operational purposes and sediment associated nutrients it is a desirable attempt to simplify the complicated ecological purposes into simple expressions for quantitative analysis 2 problem formulation 2 1 objectives three major objectives of reservoir operation are considered during the flood season i e maximizing energy production minimizing flood peak and maximizing sediment transport specifically these purposes can be formulated as follows 2 1 1 sub model 1 maximizing energy production energy production is based on the conversion of potential energy into kinetic energy and then into electrical energy here the maximum total energy production is regarded as the objective function which can be expressed as 1 max t 1 t e t max t 1 t n t Œ¥ t max t 1 t 9 81 Œ∑ t r t h t Œ¥ t 2 r t r t r t 3 h t hf t ht t hl t 4 hf t a 0 a 1 v t a 2 v t 2 5 ht t b 0 b 1 r t b 2 r t 2 6 Œ∑ t c 0 c 1 h t c 2 h t 2 c 3 h t 3 where t is the operation horizon e t is the energy production of the reservoir during period t n t is the output production of the reservoir during period t Œ¥ t is the time interval r t r t and r t are the total release power release and non power release from the reservoir during period t respectively h t is the average water head at the reservoir during period t hf t is the average fore bay water level of the reservoir during period t which is a function of the average storage v t v t v t v t 1 2 v t 1 is the reservoir storage at the beginning of period t and v t is the reservoir storage at the end of period t ht t is the average tailrace water level of the reservoir during period t which is a function of the total release r t hl t is the average water head loss of the reservoir during period t Œ∑ t is the hydropower plant efficiency of the reservoir which is a function of the average water head h t and a 0 a 1 a 2 b 0 b 1 b 2 and c 0 c 1 c 2 c 3 are the fitting coefficients determined by the characteristics of the reservoir 2 1 2 sub model 2 minimizing flood peak flood control is the priority for most reservoirs on rivers in humid regions there are two aspects for flood control one is controlling the fore bay water level in the reservoir to guarantee the safety of the dam and the other one is controlling the total release from the reservoir to ensure the safety in the lower river reaches here minimizing the flood peak from the reservoir is chosen as the objective function which can be expressed as 7 m i n max t 1 t r t and its equivalent formulation is 8 r t r t min Œ≤ Œ¥ r t where r t min is the minimum total release from the reservoir during period t Œ¥ r is the increment of r t Œ≤ is an integer constant Œ≤ 0 1 2 2 1 3 sub model 3 maximizing sediment transport to maintain the effective storage capacity the sediment deposition in the reservoir should be reduced as much as possible here maximizing sediment transport is chosen as the objective function which can be expressed as 9 max t 1 t q t s Œ¥ t max t 1 t r t s t Œ¥ t where q t s is the total sediment release from the reservoir during period t q t s r t s t s t is the average sediment concentration released from the reservoir which is assumed similar to the sediment concentration in front of the dam and can be approximately represented by the sediment carrying capacity of flow as 10 s t k u t 3 g h t œâ m where u t is the average flow velocity during period t u t r t b h t b is the river width h t is the average water depth g is the gravity acceleration œâ is the settling velocity of sediment particle k and m are the empirical parameters in this case k is set as 0 245 and m is set as 0 92 qian and wan 1999 fang and wang 2000 it is assumed that the average storage v t v t 1 v t 2 k b h t 2 and eq 10 can be rewritten as 11 s t k r t 3 g b 3 h t 4 œâ m k k 2 g b œâ r t 3 v t 2 m Œ± r t 3 m v t 2 m 12 Œ± k k 2 g b œâ m here the value Œ± can be fitted by the observed data of reservoir operation including the sediment concentration s t the total release r t and the average storage v t 2 2 constraints 1 continuity equation 13 v t v t 1 i t r t Œ¥ t t where i t is the inflow into the reservoir during period t based on comparison between observed and simulated total release fore bay water level and power generation for the tgp in 2013 we can for this case study reasonably assume that evaporation loss is balanced by precipitation li et al 2014b si et al 2018 2 lower and upper bounds on fore bay water level 14 h f t min h f t h f t max t 15 h f t fcwl œÉ t t flood season where h f t min and h f t max are the minimum and maximum fore bay water levels of the reservoir at the end of period t respectively œÉ t is the partial fore bay water level of the reservoir that exceeds the fcwl at the end of period t eq 15 is included because the fore bay water level of the reservoir should be kept at the fcwl during the entire flood season unless huge flood events occur after a huge flood event the fore bay water level of the reservoir should be lowered to the fcwl again to reserve adequate flood control storage for future potential flood events 3 lower and upper bounds on total release 16 r t min r t r t max t where r t max is the maximum total release from the reservoir during period t 4 lower and upper bounds on power release 17 r t min r t r t max t where r t min and r t max are the minimum and maximum power releases from the hydropower plant of the reservoir during period t respectively 5 lower and upper bounds on output production 18 n t min n t n t max t where n t min and n t max are the minimum and maximum output productions of the reservoir during period t respectively 6 variation limitations on fore bay water level 19 Œ¥ h f t min h f t h f t 1 Œ¥ h f t max t where Œ¥ h f t min and Œ¥ h f t max are the minimum and maximum variation limitations on fore bay water level of the reservoir during period t respectively 7 initial and final fore bay water levels 20 h f 1 h f initial 21 h f t 1 h f final where h f initial and h f final are the initial and final fore bay water levels of the reservoir respectively 2 3 solution technique eqs 1 21 formulate the multi objective optimization model of reservoir operation during the flood season two steps are taken for the solution of the model 1 solve an equivalent comprehensive objective function as 22 max t 1 t e t Œ≥ t 1 t r t v t Œ∂ t 1 t œÉ t where Œ≥ and Œ∂ are the penalty factors the first term is to maximize the energy production of the reservoir the second term is to maximize the total release from the reservoir and minimize the average storage of the reservoir so as to maximize the sediment transport from the reservoir the third term is to minimize the accumulation of the partial fore bay water level of the reservoir that exceeds the fcwl making the fore bay water level of the reservoir lower to the fcwl as soon as a flood event ends the equivalent comprehensive objective function is established because it is hard to directly solve the entire formulations including sub models 3 which are highly nonlinear expressions and can be simplified with the second term 2 solve sub models 3 by inputting solutions r t and v t from the first step using these two steps the solutions to the multi objective problem can be gained and by adjusting Œ≤ in eq 8 the trade offs among the three operational purposes can be analyzed under certain strategies the entire formulation conforms to a nonlinear programming nlp model yeh and becker 1982 and can be solved with the multi start solver from the lingo software lindo systems inc 2015 3 case study 3 1 three gorges project the three gorges project tgp the largest hydraulic engineering worldwide is located on the yangtze river upstream from the city of yichang hubei province see fig 1 the tgp had begun construction since 1994 its impoundment started in june 2003 and subsequently the first hydropower generating unit was put into operation in october 2010 the fore bay water level of the tgp first reached its normal pool level npl of 175 m corresponding to a total capacity of 393 108 m3 the final 32th hydropower generating unit was brought online in june 2012 when the tgp reached its total installed capacity of 22 500 mw li et al 2014a the annual average runoff at the tgp is 4304 108 m3 1950 2015 accounting for 48 2 of runoff of the yangtze river cwrc 2016 and resulting in 905 108 kwh of annual average energy production of the tgp mep 2016 the tgp has a flood control capacity of 221 5 108 m3 ensuring the safety for both the dam and the middle and lower reaches of the yangtze river when the design flood 1000 year of return period or maximum flood 10 000 year 10 of return period occur li et al 2014b the sedimentation is one of the most controversial problem since the demonstration stage of the tgp the observation indicates that the sediment delivery ratio i e the fraction of input sediment load that is transported to the downstream of the tgp is estimated at only 24 1 during the post construction period 2003 2016 with an annual sediment deposition in the tgp of about 1 21 108 t cwrc 2016 moreover the median diameter d 50 of sediment in the tgp is finer than 0 02 mm silt which has a strong affinity to nutrients due to its high specific surface areas and surface active sites davis and kent 1990 wang et al 2009 fang et al 2013 several studies have been taken on the quantitative relation between sediment and nutrients such as p as well as the estimation of the total p trapped in the tgp huang et al 2015a 2017 3 2 data acquisition the data used in this study including the historical hydrological and operational data are collected from the hydrology bureau of the changjiang water resources commission and the china three gorges corporation specifically the daily runoff from 1956 to 2013 i e the daily runoff at the yichang station before 2003 and the daily inflow to the tgp after 2003 the daily sediment concentration at the huanglingmiao station which represents the average sediment concentration released from the tgp from 2003 to 2013 the monthly p concentration at the yichang hankou jiujiang and datong stations from 2008 to 2013 and the daily operational data of the tgp from 2003 to 2013 which include the power and non power releases fore bay and tailrace water levels and energy production are used the hydropower plant efficiency of the tgp Œ∑ t is derived by the 2013 observed data including the energy production the fore bay and tailrace water levels and the power release as shown in fig 2 fig 3 plots the relation between s t and r t 3 m v t 2 m which is presented based on the observed data of the tgp during the flood seasons of 2003 2013 including the sediment concentration the total release and the average storage in this case k is set as 0 245 and m is set as 0 92 qian and wan 1999 fang and wang 2000 Œ± is set as 513 351 with the coefficient of determination of r 2 0 6462 the average sediment concentration s t increases with the increase in total release r t and the decrease in average storage v t it can also be seen that the simulated values the red line in fig 3 are close to the observed values within a bound of 50 200 agreement moreover the maximum total release r t max is dependent on the flood control of the middle and lower reaches of the yangtze river i e the water level at the shashi station should not exceed 44 5 m and the discharge at the zhicheng station should be no more than 56 700 m3 s li et al 2010 ctg 2015 li et al 2013 the minimum total release r t min is set to be the ecological base flow of 6000 m3 s for the downstream i e r t min 6000 li et al 2014a 3 3 typical flood hydrographs during the flood season june 1st october 31st around 60 of the annual energy is produced and more than 90 of the annual sediment load is transported by around 2 3 of the annual runoff released from the tgp three typical flood hydrographs during the flood season are selected from 1956 to 2013 based on the hydrologic frequency analysis they are the year 1977 dry 2003 normal and 1989 wet which correspond to frequencies of 75 50 and 25 respectively the characteristics of the three typical flood hydrographs are shown in table 1 as can be seen the runoffs during the flood season are 2783 3045 and 3297 108 m3 accounting for 65 8 74 4 and 69 0 of the annual runoff respectively the maximum flood peaks are 38600 45000 and 60200 m3 s respectively which occur on july 11 september 4 and july 14 respectively the typical flood hydrographs are shown in fig 4 3 4 operating rules three strategies are used for comparison and analysis including the design operating rules increasing the fcwl and impounding in advance a summary of these strategies is presented in table 2 1 design operating rules the fore bay water level of the tgp should be kept at the fcwl of 145 m during the entire flood season unless huge flood events occur after a huge flood event the fore bay water level of the tgp should be lowered to the fcwl again to reserve adequate flood control storage for future potential flood events the fore bay water level of the tgp can be raised gradually to the npl of 175 m since october 1st 2 increasing the fcwl a relatively low fcwl of 145 m probably result in much non power release or spilled water on one hand it will affect the economic benefit of the energy production on the other hand it will affect the probability of full refill during the post flood period an alternative strategy is to appropriately increase the fcwl as shown in fig 5 a here two alternative fcwls i e 150 and 155 m are adopted for comparison of comprehensive benefits with the design operating rules i e fcwl 145 m 3 impounding in advance the analysis of historical data can tell that the floods in the yangtze river mainly occur in july and august and are less likely to appear in september to decrease spilled water and improve the probability of full refill it is appropriate to advance the impounding period as shown in fig 5 b here two alternative refill times i e september 1st and 16th are adopted for comparison of comprehensive benefits with the design operating rules i e refill starts from october 1st 4 results and discussion 4 1 simulation as mentioned earlier the final 32nd hydropower generating unit was brought online in 2012 and therefore the data of 2013 is selected for the model validation given the 2013 actual inflow to the tgp the total release from the tgp as well as the initial fore bay water level of the tgp the energy production of the tgp can be simulated with the formulated model and compared with the observed values in 2013 as shown in fig 6 where the solid and dashed lines represent the simulated and observed energy production respectively it can be found that the simulated energy production is very close to the observed one with the coefficient of determination of 0 9977 indicating that the model can well simulate the energy production of the tgp given the actual fore bay water level of the tgp and the total release from the tgp during the 2003 2013 flood seasons the sediment concentration released from the tgp can be simulated with the formulated model and compared with the observed values at the huanglingmiao station as shown in fig 7 generally the simulated sediment concentration is in good agreement with the observed one and the peak values are well reproduced with the coefficient of determination of 0 6408 4 2 optimization the optimizations are executed for the three typical flood hydrographs i e dry normal and wet scenarios under the three strategies i e design operating rules increasing the fcwl and impounding in advance for all the calculations the initial fore bay water level is set as 155 m on june 1st and the final fore bay water level is set as 175 m on october 31st i e h f initial 155 and h f final 175 with a total of 153 time steps the model is on a daily basis that is Œ¥ t 1 day the calculations were executed on a thinkpad x260 the number of starting points is set as 5 for the multi start solver the average runtime is around 10 30 s for each calculation note that the quality of the solution gained from the nlp solver of the lingo software were proved to be superior in the similar cases even with larger number of variables and constraints li et al 2015 si et al 2018 which will not be discussed any more here for analysis the optimal solutions for each flood hydrograph under each strategy are classified into three categories 1 category i the extreme solution that maximizes the energy production or minimizes the flood peak to the downstream 2 category ii the extreme solution that maximizes the sediment transport to the downstream or maximizes the flood peak to the downstream 3 category iii the medium solution that compromises the three operational purposes 1 design operating rules the solutions of three typical flood hydrographs under design operating rules are summarized in table 3 in the dry scenario the maximum energy production of the tgp is 560 108 kwh corresponding to a flood peak of 25 000 m3 s and a sediment load of 1517 104 t whereas the maximum sediment load is 2796 104 t corresponding to an energy production of 500 108 kwh and a flood peak of 38 600 m3 s in the normal scenario the maximum energy production is 620 108 kwh corresponding to a flood peak of 27 500 m3 s and a sediment load of 1999 104 t whereas the maximum sediment load is 4640 104 t corresponding to an energy production of 534 108 kwh and a flood peak of 45 000 m3 s in the wet scenario the maximum energy production is 699 108 kwh corresponding to a flood peak of 27 500 m3 s and a sediment load of 1734 104 t whereas the maximum sediment load is 5377 104 t corresponding to an energy production of 588 108 kwh and a flood peak of 52 500 m3 s fig 8 depicts the fore bay water level the energy production and sediment load of the three categories for the three typical flood hydrographs it can be seen that if a big flood event occurs the fore bay water level of the tgp will rise accordingly and then be lowered to the fcwl again until the excessive flood water can be released smoothly from the tgp which conforms to the practice on the operational control of the fore bay water level during the flood season in general for the three typical flood hydrographs a higher fore bay water level leads to a greater energy production a smaller flood peak and less sediment transport to the downstream 2 increasing the fcwl the solutions of three typical flood hydrographs under alternative fcwls are summarized in table 4 in comparison with the design operating rules fcwl 145 m the variations under alternative fcwls are identified quantitatively specifically if the fcwl 150 m the energy production is increased by 2 0 9 1 1 1 8 0 and 1 0 8 7 in the dry normal and wet scenarios respectively whereas the flood peak to the downstream is reduced by 0 12 5 0 4 7 and 0 and the sediment load is reduced by 4 2 26 0 1 7 20 5 0 2 23 6 respectively if the fcwl 155 m the energy production is increased by 4 2 16 5 1 8 14 2 and 1 4 15 4 in the dry normal and wet scenarios respectively whereas the flood peak to the downstream is reduced by 0 18 3 0 15 6 and 0 and the sediment load is reduced by 11 5 44 5 1 8 36 1 and 0 9 to 38 5 respectively the further increase in the fcwl should be careful especially for wet scenario as it might threaten the safety of flood control in this case for the wet scenario the fcwl is as high as 155 m while the flood peak is as much as 52500 m3 s to the downstream making it difficult to ensure both the safety of the dam as well as that of the middle and lower reaches of the yangtze river 3 impounding in advance the solutions of three typical flood hydrographs under alternative refill times are summarized in table 5 in comparison with the design operating rules the refill time starts from october 1st the variations under alternative refill times are also identified quantitatively specifically if the refill time starts from september 16th the energy production is increased by 4 0 4 5 2 8 5 8 and 2 6 4 1 in the dry normal and wet scenarios respectively whereas there is no variation in the flood peak to the downstream and the sediment load is reduced by 4 2 7 8 8 0 11 1 and 3 3 9 3 for the three scenarios respectively if the refill time starts from september 1st the energy production is increased by 7 9 8 3 5 0 13 4 and 4 6 9 8 in the dry normal and wet scenarios respectively whereas there is almost no variation in the flood peak to the downstream and the sediment load is reduced by 7 3 17 1 17 5 34 4 and 13 2 17 4 for the three scenarios respectively the pareto sets of solutions for the three typical flood hydrographs are shown in fig 9 the corresponding radar map of the three operational purposes under various scenarios and strategies are shown in fig 10 where the values the maximum energy production the maximum flood peak and the maximum sediment load are normalized by their individual maximum values i e 731 108 kwh 52500 m3 s and 5377 104 t for the energy production flood peak and sediment load respectively in general it can be found 1 both increasing the fcwl and impounding in advance will enhance the energy production and decrease the flood peak and the sediment transport to the downstream 2 the smaller increase in energy production resulting from increasing the fcwl will lead to more decrease in flood peak and sediment transport to the downstream whereas impounding in advance leads to relatively moderate decrease in flood peak and sediment transport to the downstream thus in terms of achieving the comprehensive benefits of the tgp impounding in advance is a better choice than both the design operating rules and increasing the fcwl for it can increase the energy production and the probability of full refill with almost no variation in flood control risk to both the dam and the middle and lower reaches of the yangtze river as well as smaller reduction in the sediment transport 4 3 response of phosphorus sediment is the major carrier of phosphorus the relation between the total p concentration c t tp and the sediment concentration s t can be derived through the observed data from the tgp see fig 11 including data of the reservoir may september 2004 and that of the cuntan station june september 2005 2010 huang et al 2015b which is expressed as 23 c t tp 0 4926 s t 0 4806 where c t tp is the total p concentration in mg l the coefficient of determination r 2 is 0 6839 indicating that c t tp can be effectively estimated from s t using eq 23 where s t is calculated with eq 11 then the total p load released from the reservoir can be estimated as l t tp c t tp r t the p loads released from the tgp are simulated and compared with the observed values at yichang station during the flood seasons of 2008 2013 as shown in fig 12 indicating that the model can well simulate the processes of p transport through the tgp the solutions of three typical flood hydrographs under various strategies are summarized in table 6 the variations of p load to the downstream under various strategies are identified quantitatively specifically under the design operating rules the p load to the downstream ranges from 3152 104 to 4021 104 kg from 3740 104 to 5345 104 kg and from 3682 104 to 5784 104 kg in the dry normal and wet scenarios respectively if increasing the fcwl the p load will be reduced by 2 9 23 4 1 0 18 7 and 0 1 to 20 1 in the dry normal and wet scenarios respectively if impounding in advance the p load will be reduced by 4 6 12 2 6 2 22 1 and 3 8 12 8 in the dry normal and wet scenarios respectively thus there is also a relatively moderate decrease in p transport to the downstream for impounding in advance the p load of three categories for three typical flood hydrographs the pareto sets of solutions and the radar map of three purposes under various scenarios and strategies are shown in figs s1 s3 where the sediment load is replaced with p load through eq 23 fig 13 shows the observed p loads at the yichang hankou jiujiang and datong stations see fig 1 during 2008 2013 where the dashed lines represent the average values it can be found that the p load in the flood season is much greater than that in the non flood season if the observed p load at the yichang station is regarded as a representative of the released value from the tgp the influence percentages on the p load at the hankou station will be 5 1 15 2 and 3 9 15 4 by increasing the fcwl and impounding in advance respectively there is additional more p load released into the river reach between the hankou and jiujiang stations and the mean p load almost doubles at the jiujiang station 3 103 kg s in comparison with that at the hankou station 1 556 kg s therefore the influences of alternative operating rules will be reduced by half at the jiujiang station which will be further reduced at the datong station that is to say the influence of alternative operating rules on the p load in the middle and lower reaches of the yangtze river gradually decreases with the increase in the distance from the dam due to the additional p discharge along the river 5 conclusions this paper formulates a detailed mathematical model to optimize the reservoir operation during the flood season three sub models of high simulation accuracies are formulated to describe the three major operational purposes i e flood control social purpose energy production economic purpose and management of sediment transports ecological purpose moreover phosphorus p one of the sediment associated environmental factors is introduced for exploring its responses to various scenarios and strategies the model is a general one that is applicable to a variety of reservoirs owning multiple operational purposes this model is applied to the three gorges project tgp in china which is the world s largest and most complex hydraulic engineering in operation three typical flood hydrographs i e dry normal and wet scenarios under the three strategies i e design operating rules increasing the fcwl and impounding in advance are selected as model inputs for analysis the main conclusions are drawn as follows 1 the formulations outweigh the previous ones in a the limitations for the total release from the reservoir and the fcwl are described by hard and soft constraints in the flood control sub model b a relation between hydropower plant efficiency and water head is introduced into the energy production sub model c a function expression of sediment concentration released from the reservoir is theoretically derived in the sediment transport sub model the entire formulation conforms to a nonlinear programming nlp model which can be solved effectively and efficiently with the multi start solver from the lingo software through equivalent transformations 2 the trade offs among these three major operational purposes are quantitatively analyzed in general a higher fore bay water level leads to a greater energy production a smaller flood peak and less sediment and phosphorus transport to the downstream under the design operating rules the maximum energy production of the tgp can reach 560 620 and 699 108 kwh respectively in dry normal and wet scenarios corresponding to a flood peak of 25000 27500 m3 s a sediment load of 1517 1999 104 t and a p load of 3152 3740 104 kg to the downstream whereas the maximum sediment loads are 2796 4640 and 5377 104 t and the maximum p loads are 4021 5345 and 5784 104 kg respectively in dry normal and wet scenarios corresponding to an energy production of 500 588 108 kwh and a flood peak of 38600 52500 m3 s to the downstream 3 both increasing the fcwl and impounding in advance will enhance the energy production and decrease the flood peak and the sediment and p transports to the downstream the smaller increase in energy production resulting from increasing the fcwl will lead to more decrease in flood peak and sediment and p transports to the downstream whereas impounding in advance leads to relatively moderate decrease in flood peak and sediment and p transports to the downstream thus in terms of achieving the comprehensive benefits of the tgp impounding in advance is a better choice than both the design operating rules and increasing the fcwl for it can increase the energy production and the probability of full refill with almost no variation in flood control risk to both the dam and the middle and lower reaches of the yangtze river as well as smaller reduction in the sediment and p transports the achievements gained from this paper will provide important references for the managers and operators future works could consider the change in the water cycle and the strong anthropogenic activities e g the development and operation of large scale reservoir system in the upper yangtze river and explore the coupling relationship among these conflicting purposes of the reservoir in consideration of the changing and uncertain circumstances asokan and destouni 2014 destouni and prieto 2018 declaration of interests none acknowledgments the authors are very grateful to the hydrology bureau of the changjiang water resources commission and the china three gorges corporation for supplying the valuable data this study is supported by the national key research and development program of china 2016yfc0401401 and 2016yfc0402506 the national natural science foundation of china 91647210 51609256 51609122 51522907 51739011 51479213 and 11802158 and the research foundations of the state key laboratory of simulation and regulation of water cycle in river basin 2016zy02 the state key laboratory of lake science and environment 2016skl012 the state key laboratory of hydro science and engineering 2018 ky 03 and the 111 project b18031 partial support is also supported by the young elite scientists sponsorship program by china association for science and technology 2017qnrc001 the authors wish to thank prof danny reible from the texas tech university and prof subhasish dey from the indian institute of technology kharagpur for improving the manuscript appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j jhydrol 2019 03 009 appendix a supplementary data the following are the supplementary data to this article supplementary data 1 
6600,balancing the multiple purposes of reservoir operation including the social economic and ecological aspects has become one of the most complicated problems facing the managers and operators worldwide this paper formulates a detailed mathematical model to optimize the reservoir operation during the flood season considering the three major operational purposes i e flood control social purpose energy production economic purpose and management of sediment transport ecological purpose the entire formulations conform to a nonlinear programming nlp model which can be solved effectively and efficiently by the multi start solver from the lingo software through equivalent transformation this model is applied to the three gorges project tgp in china which is the world s largest and most complex hydraulic engineering in operation three typical flood hydrographs i e dry normal and wet scenarios are selected as model inputs under the three strategies i e design operating rules increasing the flood control water level fcwl and impounding in advance the trade offs among these three major operational purposes are quantitatively analyzed moreover phosphorus p one of the sediment associated environmental factors is introduced for exploring its responses to various scenarios and strategies results indicate that a higher fore bay water level leads to a greater energy production a smaller flood peak but less sediment and p transports to the downstream under the design operating rules the maximum energy production of the tgp can reach 560 620 and 699 108 kwh respectively in dry normal and wet scenarios corresponding to a flood peak of 25000 27500 m3 s a sediment load of 1517 1999 104 t and a p load of 3152 3740 104 kg to the downstream whereas the maximum sediment loads are 2796 4640 and 5377 104 t and the maximum p loads are 4021 5345 and 5784 104 kg respectively in dry normal and wet scenarios corresponding to an energy production of 500 588 108 kwh and a flood peak of 38600 52500 m3 s to the downstream in terms of achieving the comprehensive benefits of the tgp impounding in advance is a better choice than both the design operating rules and increasing the fcwl for it can increase the energy production and the probability of full refill with almost no increase in flood risk to both the dam and the middle and lower reaches of the yangtze river as well as smaller reduction in the sediment and p transports the formulated detailed mathematical model is a general one that is applicable to a variety of reservoirs owning multiple operational purposes moreover the achievements gained from this paper will provide important references for the managers and operators keywords reservoir operation multi objective optimization energy production flood control sediment and phosphorus transports three gorges project 1 introduction construction and operation of reservoirs can support a variety of social economic and ecological purposes including energy production flood control water supply navigation as well as management of sediment transport balancing these competing objectives has become one of the most complicated problems facing the managers and operators worldwide hydropower is the world s largest source of renewable energy and 16 4 of total energy demand was supplied by hydropower which is more than the total amount of all other renewable energies such as wind power 5 6 biopower 2 2 and solar photovoltaic 1 9 iha 2018 meanwhile hydropower has one of the lowest lifecycle greenhouse gas emissions per kilowatt hour among all energy sources and the use of which has reduced 4 109 t of greenhouse gas emissions ipcc 2014 to ensure the safety of flood control the fore bay water level of a reservoir should be kept at the flood control water level fcwl during the flood season and can be raised gradually to the normal pool level npl since the late flood season however this rule makes the reservoir be well prepared throughout the flood season for the very low probability flood event and huge amounts of flood water resources have to be spilled during the flood season affecting the comprehensive benefits of the reservoir huang et al 2015a thus a series of alternative operational strategies have been proposed such as impounding in advance and dynamic control of fore bay water level which aim at maximizing the comprehensive benefits increasing energy production decreasing spilled water and improving the full refill probability under the premise of not lowering the flood control standard li et al 2010 liu et al 2011 these strategies focus too much on the operational purposes of energy production and flood control while their impacts on the ecological purpose such as sediment and its associated environmental factors are still needed to be explored a reservoir blocks the river and destroys the continuity of riverine flows of ecological importance which is vulnerable to the effects of reservoir impoundment suen and eheart 2006 wang et al 2008 the river slope decreases and the flow slows in front of the dam after reservoir impoundment resulting in the sediment deposition in the reservoir and the consequent reduction in the effective storage capacity li et al 2011 syvitski et al 2005 estimated that a total of 1 1011 t sediment have been sequestered in reservoirs constructed largely within the past 50 years and the global estimates for the annual loss of reservoir storage due to sedimentation varied from 0 5 to 1 0 white 2010 accordingly there is a decline of sediment flux to the lower river reaches which is closely related to the number and storage of reservoirs present in the river basin gupta et al 2012 leading to the erosion of the riverbed behind the dam yuan et al 2012 wang et al 2017 and affecting the development and evolution of the estuarine delta as well as other ecological issues jiao et al 2007 thus the strategy that storing clear water and releasing muddy is generally applied for reservoirs in high sediment laden rivers wang and hu 2009 meanwhile the ecological impacts of reservoir operation have drawn wide attention in recent years mitsch et al 2008 yi et al 2010 xu et al 2011 for instance the impoundment increases the retention of nutrients in the reservoir giving rise to a frequent occurrence of eutrophication mep 2013 2016 ji et al 2017 while the reduction in nutrients transporting to the lower river reaches probably change the trophic structure and affect the plankton community and biodiversity of the lower river reaches stone 2008 chai et al 2009 phosphorus p one of the key nutrients affecting water quality plays a crucial role in the aquatic eco environment schindler 2006 elser et al 2007 schelske 2009 there are complicated physicochemical interactions between p and sediment particles especially the fine sediment horowitz 2008 huang et al 2016 and most p in water are adsorbed by sediment particles and transported in the particulate phase withers and jarvie 2008 that is to say a large amount of the upstream p would be intercepted together with the reservoir sedimentation iwhr 2012 huang et al 2015a 2017 and further the geochemical characteristics of river as well as the aquatic ecosystem are altered zhou et al 2013 however to our knowledge few operational strategies of reservoirs have focused on the transport of nutrients cunha et al 2016 yu et al 2018 and it is necessary to identify the response of p transport to the variation of sediment flux resulting from management and operation of a reservoir beusen et al 2005 thus optimal operation is required for reservoirs to balance the multiple operational purposes including the traditional operational purposes and ecological purposes wan et al 2010 wang et al 2015 this paper formulates a detailed mathematical model to optimize the reservoir operation during the flood season which considers the three major operational purposes of flood control social purpose energy production economic purpose and sediment transport ecological purpose the model is applied to the three gorges project tgp which is the world s largest and most complex hydraulic engineering in operation and the trade offs among these three operational purposes are presented quantitatively in dry normal and wet scenarios under various operational strategies meanwhile p one of the sediment associated environmental factors is introduced as a case for exploring its response to these scenarios and strategies it is worth noting that the reservoir development and operation would increase the water loss by increasing the evapotranspiration through impoundment and flow regulation jaramillo and destouni 2015 levi et al 2015 thereby altering the local water cycle and then the runoff and its temporal variability and introducing the uncertainties in studies destouni et al 2010 2013 khazaei et al 2019 meanwhile the cascade development of reservoirs e g in the upper yangtze river would further increase these uncertainties thus imposing considerable effects on the flow and sediment and nutrients transport bring et al 2015 t√∂rnqvist et al 2015 these changing circumstances are not taken into account in this study and will be addressed in the future moreover although p is only one case for identifying the trade off between traditional operational purposes and sediment associated nutrients it is a desirable attempt to simplify the complicated ecological purposes into simple expressions for quantitative analysis 2 problem formulation 2 1 objectives three major objectives of reservoir operation are considered during the flood season i e maximizing energy production minimizing flood peak and maximizing sediment transport specifically these purposes can be formulated as follows 2 1 1 sub model 1 maximizing energy production energy production is based on the conversion of potential energy into kinetic energy and then into electrical energy here the maximum total energy production is regarded as the objective function which can be expressed as 1 max t 1 t e t max t 1 t n t Œ¥ t max t 1 t 9 81 Œ∑ t r t h t Œ¥ t 2 r t r t r t 3 h t hf t ht t hl t 4 hf t a 0 a 1 v t a 2 v t 2 5 ht t b 0 b 1 r t b 2 r t 2 6 Œ∑ t c 0 c 1 h t c 2 h t 2 c 3 h t 3 where t is the operation horizon e t is the energy production of the reservoir during period t n t is the output production of the reservoir during period t Œ¥ t is the time interval r t r t and r t are the total release power release and non power release from the reservoir during period t respectively h t is the average water head at the reservoir during period t hf t is the average fore bay water level of the reservoir during period t which is a function of the average storage v t v t v t v t 1 2 v t 1 is the reservoir storage at the beginning of period t and v t is the reservoir storage at the end of period t ht t is the average tailrace water level of the reservoir during period t which is a function of the total release r t hl t is the average water head loss of the reservoir during period t Œ∑ t is the hydropower plant efficiency of the reservoir which is a function of the average water head h t and a 0 a 1 a 2 b 0 b 1 b 2 and c 0 c 1 c 2 c 3 are the fitting coefficients determined by the characteristics of the reservoir 2 1 2 sub model 2 minimizing flood peak flood control is the priority for most reservoirs on rivers in humid regions there are two aspects for flood control one is controlling the fore bay water level in the reservoir to guarantee the safety of the dam and the other one is controlling the total release from the reservoir to ensure the safety in the lower river reaches here minimizing the flood peak from the reservoir is chosen as the objective function which can be expressed as 7 m i n max t 1 t r t and its equivalent formulation is 8 r t r t min Œ≤ Œ¥ r t where r t min is the minimum total release from the reservoir during period t Œ¥ r is the increment of r t Œ≤ is an integer constant Œ≤ 0 1 2 2 1 3 sub model 3 maximizing sediment transport to maintain the effective storage capacity the sediment deposition in the reservoir should be reduced as much as possible here maximizing sediment transport is chosen as the objective function which can be expressed as 9 max t 1 t q t s Œ¥ t max t 1 t r t s t Œ¥ t where q t s is the total sediment release from the reservoir during period t q t s r t s t s t is the average sediment concentration released from the reservoir which is assumed similar to the sediment concentration in front of the dam and can be approximately represented by the sediment carrying capacity of flow as 10 s t k u t 3 g h t œâ m where u t is the average flow velocity during period t u t r t b h t b is the river width h t is the average water depth g is the gravity acceleration œâ is the settling velocity of sediment particle k and m are the empirical parameters in this case k is set as 0 245 and m is set as 0 92 qian and wan 1999 fang and wang 2000 it is assumed that the average storage v t v t 1 v t 2 k b h t 2 and eq 10 can be rewritten as 11 s t k r t 3 g b 3 h t 4 œâ m k k 2 g b œâ r t 3 v t 2 m Œ± r t 3 m v t 2 m 12 Œ± k k 2 g b œâ m here the value Œ± can be fitted by the observed data of reservoir operation including the sediment concentration s t the total release r t and the average storage v t 2 2 constraints 1 continuity equation 13 v t v t 1 i t r t Œ¥ t t where i t is the inflow into the reservoir during period t based on comparison between observed and simulated total release fore bay water level and power generation for the tgp in 2013 we can for this case study reasonably assume that evaporation loss is balanced by precipitation li et al 2014b si et al 2018 2 lower and upper bounds on fore bay water level 14 h f t min h f t h f t max t 15 h f t fcwl œÉ t t flood season where h f t min and h f t max are the minimum and maximum fore bay water levels of the reservoir at the end of period t respectively œÉ t is the partial fore bay water level of the reservoir that exceeds the fcwl at the end of period t eq 15 is included because the fore bay water level of the reservoir should be kept at the fcwl during the entire flood season unless huge flood events occur after a huge flood event the fore bay water level of the reservoir should be lowered to the fcwl again to reserve adequate flood control storage for future potential flood events 3 lower and upper bounds on total release 16 r t min r t r t max t where r t max is the maximum total release from the reservoir during period t 4 lower and upper bounds on power release 17 r t min r t r t max t where r t min and r t max are the minimum and maximum power releases from the hydropower plant of the reservoir during period t respectively 5 lower and upper bounds on output production 18 n t min n t n t max t where n t min and n t max are the minimum and maximum output productions of the reservoir during period t respectively 6 variation limitations on fore bay water level 19 Œ¥ h f t min h f t h f t 1 Œ¥ h f t max t where Œ¥ h f t min and Œ¥ h f t max are the minimum and maximum variation limitations on fore bay water level of the reservoir during period t respectively 7 initial and final fore bay water levels 20 h f 1 h f initial 21 h f t 1 h f final where h f initial and h f final are the initial and final fore bay water levels of the reservoir respectively 2 3 solution technique eqs 1 21 formulate the multi objective optimization model of reservoir operation during the flood season two steps are taken for the solution of the model 1 solve an equivalent comprehensive objective function as 22 max t 1 t e t Œ≥ t 1 t r t v t Œ∂ t 1 t œÉ t where Œ≥ and Œ∂ are the penalty factors the first term is to maximize the energy production of the reservoir the second term is to maximize the total release from the reservoir and minimize the average storage of the reservoir so as to maximize the sediment transport from the reservoir the third term is to minimize the accumulation of the partial fore bay water level of the reservoir that exceeds the fcwl making the fore bay water level of the reservoir lower to the fcwl as soon as a flood event ends the equivalent comprehensive objective function is established because it is hard to directly solve the entire formulations including sub models 3 which are highly nonlinear expressions and can be simplified with the second term 2 solve sub models 3 by inputting solutions r t and v t from the first step using these two steps the solutions to the multi objective problem can be gained and by adjusting Œ≤ in eq 8 the trade offs among the three operational purposes can be analyzed under certain strategies the entire formulation conforms to a nonlinear programming nlp model yeh and becker 1982 and can be solved with the multi start solver from the lingo software lindo systems inc 2015 3 case study 3 1 three gorges project the three gorges project tgp the largest hydraulic engineering worldwide is located on the yangtze river upstream from the city of yichang hubei province see fig 1 the tgp had begun construction since 1994 its impoundment started in june 2003 and subsequently the first hydropower generating unit was put into operation in october 2010 the fore bay water level of the tgp first reached its normal pool level npl of 175 m corresponding to a total capacity of 393 108 m3 the final 32th hydropower generating unit was brought online in june 2012 when the tgp reached its total installed capacity of 22 500 mw li et al 2014a the annual average runoff at the tgp is 4304 108 m3 1950 2015 accounting for 48 2 of runoff of the yangtze river cwrc 2016 and resulting in 905 108 kwh of annual average energy production of the tgp mep 2016 the tgp has a flood control capacity of 221 5 108 m3 ensuring the safety for both the dam and the middle and lower reaches of the yangtze river when the design flood 1000 year of return period or maximum flood 10 000 year 10 of return period occur li et al 2014b the sedimentation is one of the most controversial problem since the demonstration stage of the tgp the observation indicates that the sediment delivery ratio i e the fraction of input sediment load that is transported to the downstream of the tgp is estimated at only 24 1 during the post construction period 2003 2016 with an annual sediment deposition in the tgp of about 1 21 108 t cwrc 2016 moreover the median diameter d 50 of sediment in the tgp is finer than 0 02 mm silt which has a strong affinity to nutrients due to its high specific surface areas and surface active sites davis and kent 1990 wang et al 2009 fang et al 2013 several studies have been taken on the quantitative relation between sediment and nutrients such as p as well as the estimation of the total p trapped in the tgp huang et al 2015a 2017 3 2 data acquisition the data used in this study including the historical hydrological and operational data are collected from the hydrology bureau of the changjiang water resources commission and the china three gorges corporation specifically the daily runoff from 1956 to 2013 i e the daily runoff at the yichang station before 2003 and the daily inflow to the tgp after 2003 the daily sediment concentration at the huanglingmiao station which represents the average sediment concentration released from the tgp from 2003 to 2013 the monthly p concentration at the yichang hankou jiujiang and datong stations from 2008 to 2013 and the daily operational data of the tgp from 2003 to 2013 which include the power and non power releases fore bay and tailrace water levels and energy production are used the hydropower plant efficiency of the tgp Œ∑ t is derived by the 2013 observed data including the energy production the fore bay and tailrace water levels and the power release as shown in fig 2 fig 3 plots the relation between s t and r t 3 m v t 2 m which is presented based on the observed data of the tgp during the flood seasons of 2003 2013 including the sediment concentration the total release and the average storage in this case k is set as 0 245 and m is set as 0 92 qian and wan 1999 fang and wang 2000 Œ± is set as 513 351 with the coefficient of determination of r 2 0 6462 the average sediment concentration s t increases with the increase in total release r t and the decrease in average storage v t it can also be seen that the simulated values the red line in fig 3 are close to the observed values within a bound of 50 200 agreement moreover the maximum total release r t max is dependent on the flood control of the middle and lower reaches of the yangtze river i e the water level at the shashi station should not exceed 44 5 m and the discharge at the zhicheng station should be no more than 56 700 m3 s li et al 2010 ctg 2015 li et al 2013 the minimum total release r t min is set to be the ecological base flow of 6000 m3 s for the downstream i e r t min 6000 li et al 2014a 3 3 typical flood hydrographs during the flood season june 1st october 31st around 60 of the annual energy is produced and more than 90 of the annual sediment load is transported by around 2 3 of the annual runoff released from the tgp three typical flood hydrographs during the flood season are selected from 1956 to 2013 based on the hydrologic frequency analysis they are the year 1977 dry 2003 normal and 1989 wet which correspond to frequencies of 75 50 and 25 respectively the characteristics of the three typical flood hydrographs are shown in table 1 as can be seen the runoffs during the flood season are 2783 3045 and 3297 108 m3 accounting for 65 8 74 4 and 69 0 of the annual runoff respectively the maximum flood peaks are 38600 45000 and 60200 m3 s respectively which occur on july 11 september 4 and july 14 respectively the typical flood hydrographs are shown in fig 4 3 4 operating rules three strategies are used for comparison and analysis including the design operating rules increasing the fcwl and impounding in advance a summary of these strategies is presented in table 2 1 design operating rules the fore bay water level of the tgp should be kept at the fcwl of 145 m during the entire flood season unless huge flood events occur after a huge flood event the fore bay water level of the tgp should be lowered to the fcwl again to reserve adequate flood control storage for future potential flood events the fore bay water level of the tgp can be raised gradually to the npl of 175 m since october 1st 2 increasing the fcwl a relatively low fcwl of 145 m probably result in much non power release or spilled water on one hand it will affect the economic benefit of the energy production on the other hand it will affect the probability of full refill during the post flood period an alternative strategy is to appropriately increase the fcwl as shown in fig 5 a here two alternative fcwls i e 150 and 155 m are adopted for comparison of comprehensive benefits with the design operating rules i e fcwl 145 m 3 impounding in advance the analysis of historical data can tell that the floods in the yangtze river mainly occur in july and august and are less likely to appear in september to decrease spilled water and improve the probability of full refill it is appropriate to advance the impounding period as shown in fig 5 b here two alternative refill times i e september 1st and 16th are adopted for comparison of comprehensive benefits with the design operating rules i e refill starts from october 1st 4 results and discussion 4 1 simulation as mentioned earlier the final 32nd hydropower generating unit was brought online in 2012 and therefore the data of 2013 is selected for the model validation given the 2013 actual inflow to the tgp the total release from the tgp as well as the initial fore bay water level of the tgp the energy production of the tgp can be simulated with the formulated model and compared with the observed values in 2013 as shown in fig 6 where the solid and dashed lines represent the simulated and observed energy production respectively it can be found that the simulated energy production is very close to the observed one with the coefficient of determination of 0 9977 indicating that the model can well simulate the energy production of the tgp given the actual fore bay water level of the tgp and the total release from the tgp during the 2003 2013 flood seasons the sediment concentration released from the tgp can be simulated with the formulated model and compared with the observed values at the huanglingmiao station as shown in fig 7 generally the simulated sediment concentration is in good agreement with the observed one and the peak values are well reproduced with the coefficient of determination of 0 6408 4 2 optimization the optimizations are executed for the three typical flood hydrographs i e dry normal and wet scenarios under the three strategies i e design operating rules increasing the fcwl and impounding in advance for all the calculations the initial fore bay water level is set as 155 m on june 1st and the final fore bay water level is set as 175 m on october 31st i e h f initial 155 and h f final 175 with a total of 153 time steps the model is on a daily basis that is Œ¥ t 1 day the calculations were executed on a thinkpad x260 the number of starting points is set as 5 for the multi start solver the average runtime is around 10 30 s for each calculation note that the quality of the solution gained from the nlp solver of the lingo software were proved to be superior in the similar cases even with larger number of variables and constraints li et al 2015 si et al 2018 which will not be discussed any more here for analysis the optimal solutions for each flood hydrograph under each strategy are classified into three categories 1 category i the extreme solution that maximizes the energy production or minimizes the flood peak to the downstream 2 category ii the extreme solution that maximizes the sediment transport to the downstream or maximizes the flood peak to the downstream 3 category iii the medium solution that compromises the three operational purposes 1 design operating rules the solutions of three typical flood hydrographs under design operating rules are summarized in table 3 in the dry scenario the maximum energy production of the tgp is 560 108 kwh corresponding to a flood peak of 25 000 m3 s and a sediment load of 1517 104 t whereas the maximum sediment load is 2796 104 t corresponding to an energy production of 500 108 kwh and a flood peak of 38 600 m3 s in the normal scenario the maximum energy production is 620 108 kwh corresponding to a flood peak of 27 500 m3 s and a sediment load of 1999 104 t whereas the maximum sediment load is 4640 104 t corresponding to an energy production of 534 108 kwh and a flood peak of 45 000 m3 s in the wet scenario the maximum energy production is 699 108 kwh corresponding to a flood peak of 27 500 m3 s and a sediment load of 1734 104 t whereas the maximum sediment load is 5377 104 t corresponding to an energy production of 588 108 kwh and a flood peak of 52 500 m3 s fig 8 depicts the fore bay water level the energy production and sediment load of the three categories for the three typical flood hydrographs it can be seen that if a big flood event occurs the fore bay water level of the tgp will rise accordingly and then be lowered to the fcwl again until the excessive flood water can be released smoothly from the tgp which conforms to the practice on the operational control of the fore bay water level during the flood season in general for the three typical flood hydrographs a higher fore bay water level leads to a greater energy production a smaller flood peak and less sediment transport to the downstream 2 increasing the fcwl the solutions of three typical flood hydrographs under alternative fcwls are summarized in table 4 in comparison with the design operating rules fcwl 145 m the variations under alternative fcwls are identified quantitatively specifically if the fcwl 150 m the energy production is increased by 2 0 9 1 1 1 8 0 and 1 0 8 7 in the dry normal and wet scenarios respectively whereas the flood peak to the downstream is reduced by 0 12 5 0 4 7 and 0 and the sediment load is reduced by 4 2 26 0 1 7 20 5 0 2 23 6 respectively if the fcwl 155 m the energy production is increased by 4 2 16 5 1 8 14 2 and 1 4 15 4 in the dry normal and wet scenarios respectively whereas the flood peak to the downstream is reduced by 0 18 3 0 15 6 and 0 and the sediment load is reduced by 11 5 44 5 1 8 36 1 and 0 9 to 38 5 respectively the further increase in the fcwl should be careful especially for wet scenario as it might threaten the safety of flood control in this case for the wet scenario the fcwl is as high as 155 m while the flood peak is as much as 52500 m3 s to the downstream making it difficult to ensure both the safety of the dam as well as that of the middle and lower reaches of the yangtze river 3 impounding in advance the solutions of three typical flood hydrographs under alternative refill times are summarized in table 5 in comparison with the design operating rules the refill time starts from october 1st the variations under alternative refill times are also identified quantitatively specifically if the refill time starts from september 16th the energy production is increased by 4 0 4 5 2 8 5 8 and 2 6 4 1 in the dry normal and wet scenarios respectively whereas there is no variation in the flood peak to the downstream and the sediment load is reduced by 4 2 7 8 8 0 11 1 and 3 3 9 3 for the three scenarios respectively if the refill time starts from september 1st the energy production is increased by 7 9 8 3 5 0 13 4 and 4 6 9 8 in the dry normal and wet scenarios respectively whereas there is almost no variation in the flood peak to the downstream and the sediment load is reduced by 7 3 17 1 17 5 34 4 and 13 2 17 4 for the three scenarios respectively the pareto sets of solutions for the three typical flood hydrographs are shown in fig 9 the corresponding radar map of the three operational purposes under various scenarios and strategies are shown in fig 10 where the values the maximum energy production the maximum flood peak and the maximum sediment load are normalized by their individual maximum values i e 731 108 kwh 52500 m3 s and 5377 104 t for the energy production flood peak and sediment load respectively in general it can be found 1 both increasing the fcwl and impounding in advance will enhance the energy production and decrease the flood peak and the sediment transport to the downstream 2 the smaller increase in energy production resulting from increasing the fcwl will lead to more decrease in flood peak and sediment transport to the downstream whereas impounding in advance leads to relatively moderate decrease in flood peak and sediment transport to the downstream thus in terms of achieving the comprehensive benefits of the tgp impounding in advance is a better choice than both the design operating rules and increasing the fcwl for it can increase the energy production and the probability of full refill with almost no variation in flood control risk to both the dam and the middle and lower reaches of the yangtze river as well as smaller reduction in the sediment transport 4 3 response of phosphorus sediment is the major carrier of phosphorus the relation between the total p concentration c t tp and the sediment concentration s t can be derived through the observed data from the tgp see fig 11 including data of the reservoir may september 2004 and that of the cuntan station june september 2005 2010 huang et al 2015b which is expressed as 23 c t tp 0 4926 s t 0 4806 where c t tp is the total p concentration in mg l the coefficient of determination r 2 is 0 6839 indicating that c t tp can be effectively estimated from s t using eq 23 where s t is calculated with eq 11 then the total p load released from the reservoir can be estimated as l t tp c t tp r t the p loads released from the tgp are simulated and compared with the observed values at yichang station during the flood seasons of 2008 2013 as shown in fig 12 indicating that the model can well simulate the processes of p transport through the tgp the solutions of three typical flood hydrographs under various strategies are summarized in table 6 the variations of p load to the downstream under various strategies are identified quantitatively specifically under the design operating rules the p load to the downstream ranges from 3152 104 to 4021 104 kg from 3740 104 to 5345 104 kg and from 3682 104 to 5784 104 kg in the dry normal and wet scenarios respectively if increasing the fcwl the p load will be reduced by 2 9 23 4 1 0 18 7 and 0 1 to 20 1 in the dry normal and wet scenarios respectively if impounding in advance the p load will be reduced by 4 6 12 2 6 2 22 1 and 3 8 12 8 in the dry normal and wet scenarios respectively thus there is also a relatively moderate decrease in p transport to the downstream for impounding in advance the p load of three categories for three typical flood hydrographs the pareto sets of solutions and the radar map of three purposes under various scenarios and strategies are shown in figs s1 s3 where the sediment load is replaced with p load through eq 23 fig 13 shows the observed p loads at the yichang hankou jiujiang and datong stations see fig 1 during 2008 2013 where the dashed lines represent the average values it can be found that the p load in the flood season is much greater than that in the non flood season if the observed p load at the yichang station is regarded as a representative of the released value from the tgp the influence percentages on the p load at the hankou station will be 5 1 15 2 and 3 9 15 4 by increasing the fcwl and impounding in advance respectively there is additional more p load released into the river reach between the hankou and jiujiang stations and the mean p load almost doubles at the jiujiang station 3 103 kg s in comparison with that at the hankou station 1 556 kg s therefore the influences of alternative operating rules will be reduced by half at the jiujiang station which will be further reduced at the datong station that is to say the influence of alternative operating rules on the p load in the middle and lower reaches of the yangtze river gradually decreases with the increase in the distance from the dam due to the additional p discharge along the river 5 conclusions this paper formulates a detailed mathematical model to optimize the reservoir operation during the flood season three sub models of high simulation accuracies are formulated to describe the three major operational purposes i e flood control social purpose energy production economic purpose and management of sediment transports ecological purpose moreover phosphorus p one of the sediment associated environmental factors is introduced for exploring its responses to various scenarios and strategies the model is a general one that is applicable to a variety of reservoirs owning multiple operational purposes this model is applied to the three gorges project tgp in china which is the world s largest and most complex hydraulic engineering in operation three typical flood hydrographs i e dry normal and wet scenarios under the three strategies i e design operating rules increasing the fcwl and impounding in advance are selected as model inputs for analysis the main conclusions are drawn as follows 1 the formulations outweigh the previous ones in a the limitations for the total release from the reservoir and the fcwl are described by hard and soft constraints in the flood control sub model b a relation between hydropower plant efficiency and water head is introduced into the energy production sub model c a function expression of sediment concentration released from the reservoir is theoretically derived in the sediment transport sub model the entire formulation conforms to a nonlinear programming nlp model which can be solved effectively and efficiently with the multi start solver from the lingo software through equivalent transformations 2 the trade offs among these three major operational purposes are quantitatively analyzed in general a higher fore bay water level leads to a greater energy production a smaller flood peak and less sediment and phosphorus transport to the downstream under the design operating rules the maximum energy production of the tgp can reach 560 620 and 699 108 kwh respectively in dry normal and wet scenarios corresponding to a flood peak of 25000 27500 m3 s a sediment load of 1517 1999 104 t and a p load of 3152 3740 104 kg to the downstream whereas the maximum sediment loads are 2796 4640 and 5377 104 t and the maximum p loads are 4021 5345 and 5784 104 kg respectively in dry normal and wet scenarios corresponding to an energy production of 500 588 108 kwh and a flood peak of 38600 52500 m3 s to the downstream 3 both increasing the fcwl and impounding in advance will enhance the energy production and decrease the flood peak and the sediment and p transports to the downstream the smaller increase in energy production resulting from increasing the fcwl will lead to more decrease in flood peak and sediment and p transports to the downstream whereas impounding in advance leads to relatively moderate decrease in flood peak and sediment and p transports to the downstream thus in terms of achieving the comprehensive benefits of the tgp impounding in advance is a better choice than both the design operating rules and increasing the fcwl for it can increase the energy production and the probability of full refill with almost no variation in flood control risk to both the dam and the middle and lower reaches of the yangtze river as well as smaller reduction in the sediment and p transports the achievements gained from this paper will provide important references for the managers and operators future works could consider the change in the water cycle and the strong anthropogenic activities e g the development and operation of large scale reservoir system in the upper yangtze river and explore the coupling relationship among these conflicting purposes of the reservoir in consideration of the changing and uncertain circumstances asokan and destouni 2014 destouni and prieto 2018 declaration of interests none acknowledgments the authors are very grateful to the hydrology bureau of the changjiang water resources commission and the china three gorges corporation for supplying the valuable data this study is supported by the national key research and development program of china 2016yfc0401401 and 2016yfc0402506 the national natural science foundation of china 91647210 51609256 51609122 51522907 51739011 51479213 and 11802158 and the research foundations of the state key laboratory of simulation and regulation of water cycle in river basin 2016zy02 the state key laboratory of lake science and environment 2016skl012 the state key laboratory of hydro science and engineering 2018 ky 03 and the 111 project b18031 partial support is also supported by the young elite scientists sponsorship program by china association for science and technology 2017qnrc001 the authors wish to thank prof danny reible from the texas tech university and prof subhasish dey from the indian institute of technology kharagpur for improving the manuscript appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j jhydrol 2019 03 009 appendix a supplementary data the following are the supplementary data to this article supplementary data 1 
6601,groundwater is one of the most beneficial natural resources worldwide the main aim of the present study is the mapping of groundwater potential of the zhangjiamao area in china using two novel hybrid data mining techniques that involve the adaptive neuro fuzzy inference system anfis ensembled with teaching learning based optimization tlbo and biogeography based optimization bbo first 93 spring locations were identified in the study area which were randomly divided in the ratio of 70 30 to be used for building models and validation respectively a total of 16 spring affecting factors viz slope aspect altitude slope angle plan curvature profile curvature curvature sediment transport index stream power index topographic wetness index distance to roads distance to rivers rainfall lithology soil ndvi and land use land cover were used as input variables the probability certainty factor pcf method was applied for the correlation analysis between spring occurrences and the affecting factors subsequently the anfis tlbo and anfis bbo models were generated to produce groundwater spring potential maps gspms finally the gspms were evaluated by using the area under the receiver operating characteristic auroc the results showed that the auroc values for the anfis tlbo model based on the training and validation datasets were 0 866 and 0 905 respectively in contrast the auroc values for the anfis bbo model based on the training and validation datasets were 0 861 and 0 887 respectively the results of anfis tlbo showed that 25 66 of the study area could be classified into the high and very high groundwater spring potential classes as compared to the anfis bbo model 30 52 the results of the present study can be useful for groundwater management keywords groundwater potentiality anfis tlbo bbo china 1 introduction water resources made up of surface water and groundwater are considered to be among the most crucial natural resources in arid and semi arid regions rahmati et al 2016 due to their characteristics of circulation fluidity and inequality in space time distribution groundwater is more precious than surface water in these regions because of its widespread distribution low vulnerability constant chemical composition fixed temperature and better quality rahmati et al 2018 it is defined as the water in saturated zone areas that fill the pore spaces nampak et al 2014 and groundwater potential probability is the probability of groundwater occurrence in a specific area khosravi et al 2018a currently the demand for groundwater for domestic use and industrial and agricultural water requirement mogaji et al 2015 is increasing therefore better management of groundwater is necessary hence it is advisable to use groundwater spring potential mapping for groundwater determination management and protection programs naghibi and pourghasemi 2015 in previous research works geographical information system gis and remote sensing rs were two popular and effective methods used for groundwater potential mapping al abadi et al 2016b machiwal et al 2011 oikonomidis et al 2015 rahmati et al 2014 many statistical methods such as weights of evidence woe al abadi 2015 frequency ratio fr moghaddam et al 2015 evidential belief function ebf nejad et al 2016 park et al 2014 and index of entropy ioe al abadi and shahid 2015 were also used in addition various machine learning approaches have also been applied in recent years such as the classification and regression tree cart naghibi et al 2016 support vector machine svm naghibi et al 2017a random forest rf rahmati et al 2016 and artificial neural network ann lee et al 2017 machine learning algorithms have some advantages such as the ability to handle huge amounts of non linear structured data from various sources at different scales khosravi et al 2018a the ann is one of the most popular and widely used methods and has been applied in a different aspect of hydrological sciences lee et al 2017 sokeng et al 2016 tsakiri et al 2018 anns have the advantage of computational efficiency but also the limitations of poor prediction and error in the modeling process tien bui et al 2016a thus the ann model ensemble with fuzzy logic viz the adaptive neuro fuzzy inference system anfis was proposed anfis is a hybrid system of a fuzzy and artificial neural network that can produce better results than the fuzzy inference system fis for solving non linear problems by developing a powerful structure jang and sun 1995 jang 1993 in fact this model is a multilayer feed forward network that uses fis for training dixon 2005 anfis can train the fis membership function mf parameters by using training input data and combining methods such as the back propagation gradient descent and least squares however it is still difficult to find the best internal weight values of anfis in a membership function khosravi et al 2018a an ensemble of meta heuristic algorithms with the anfis model can overcome the drawback of anfis and also enhance its prediction power khosravi et al 2018a more recently ensemble models particularly the ensembles of anfis with metaheuristic algorithms chen et al 2018a naghibi et al 2017b have been increasingly used for enhancing the result of modelling especially in groundwater potential mapping chen et al 2018a naghibi et al 2017b landslide susceptibility modelling pham et al 2018 umar et al 2014 and flood susceptibility mapping tehrany et al 2014 however the use of ensemble models of anfis with metaheuristic algorithms for groundwater potential mapping is still uncommon metaheuristic algorithms such as the teaching learning based optimization tlbo and biogeography based optimization bbo are widely used in many fields viz constrained mechanical design optimization problems rao et al 2011b dynamic group strategy for global optimization zou et al 2014 data clustering satapathy and naik 2011 building the matching mathematical model of organism distribution guo et al 2014 and blend migration ma and dan 2010 these approaches are simple yet efficient for solving non linear optimization problems hybrid methods are known to increase the prediction accuracy of single based classifiers due to their higher power of prediction and greater ability of recognition chen et al 2018b however the result of the modelling process depends on data accuracy as well as the prediction power of the model therefore by selecting a suitable dataset and model the probability of achieving accurate results will increase in the present study the new hybrid models of anfis with tlbo and bbo were designed and developed using matlab programming and verified for the case study of groundwater potential mapping at the zhangjiamao area in china in general a literature review has shown that no research work has been carried out as yet related to the use of a hybrid of anfis with tlbo and bbo algorithms it should also be noted that there are no global guidelines for the selection of the spring affecting factors which were determined in this study on the basis of literature reviews case study characteristics and data availability naghibi et al 2017a 2 materials and methods 2 1 study area the zhangjiamao area is located in the shaanxi province china which is within the latitudes of 38 57 38 n to 39 01 37 n and longitudes of 110 16 21 e to 110 23 31 e it covers an area of roughly 52 km2 fig 1 the altitudes of the study area vary from 1084 m to 1323 m and the slope angles from 0 to 47 63 according to the digital elevation model dem with a 20 m regular grid produced from the topographic map on a scale of 1 25000 it is located in the inland of western china with a semi arid continental climate according to the shenmu county meteorological station the mean annual rainfall in this region is 436 6 mm and most of it occurs from july to september most of the vegetation in this area consists of herbaceous and bush plants owing to the special geographical position the study area is covered with loose sand in the southwest whereas other areas are loess hilly with gullies fan et al 2018 the mainly outcropped lithological units are aeolian sand clay or sandy clay sandstone and siltstone fan et al 2018 2 2 spatial datasets at first according to documentary sources 93 springs were identified in the study area and checked using hand held gps and extensive field surveys and considered to be a spring inventory map the same number of non spring locations was produced randomly by using the arcgis software the spring and non spring locations were divided randomly into two parts in the ratio of 70 30 70 of the springs and non springs were merged for construction of a training dataset which was used for model building similarly the remaining 30 of the springs and non springs were merged for construction of a testing dataset which was applied for model validation marnerides et al 2015 the springs and non springs were assigned to 1 and 0 respectively for the modeling according to the literature review khosravi et al 2018a lee et al 2017 rahmati et al 2016 zeinivand and ghorbani nejad 2018 and characteristics of the study area 16 spring affecting factors viz slope aspect altitude slope angle plan curvature profile curvature curvature sediment transport index sti stream power index spi topographic wetness index twi distance to roads distance to rivers rainfall lithology soil ndvi and land use land cover lulc were selected for the preparation of the groundwater spring potential mapping fig 2 a digital elevation model dem with a spatial resolution of 20 20 m2 was created using the topographic map on a scale of 1 25000 different data viz the slope aspect altitude slope angle plan curvature profile curvature sti spi and twi were then directly extracted from the dem by using the arcgis and saga gis software the maps of the distances to roads and rivers were also produced using the topographic maps the rainfall map was prepared with a 20 year 1998 2017 mean annual rainfall dataset by using the inverse distance weighted method kriging due to the lowest amount of rmse and reclassified into seven classes at intervals of 20 mm the lithology map was obtained from the geological map on a scale of 1 10000 which was obtained from the china geology survey http www cgs gov cn ten geologic formations were divided into 10 classes table 1 the soil type was extracted from the soil maps of the study area on a scale of 1 1000000 the ndvi map was generated using landsat 8 operational land imager oli images path row 126 33 date november 20 2017 product id lc81260332017324lgn00 and envi software the lulc map was extracted from land use maps on a scale of 1 100000 the classification of the continuous thematic layers was carried out on the basis of different methods attributed to the function of each factor in the current study the continuous layers viz the slope angle plan curvature profile curvature curvature distance to roads distance to rivers and ndvi were classified into five classes based on the jenks natural break classification scheme the other continuous layers such as the altitude sti spi twi and rainfall were reclassified using certain intervals on the basis of the above mentioned steps all the spring affecting factors were eventually converted into a raster with the same spatial resolution of 20 20 m2 a detailed delineation and classification of all the factors is shown in table 2 2 3 data correlation analysis in this study the probability certainty factor pcf method was used to show the correlation between the springs and affecting factors the pcf method is beneficial for handling any uncertainty in the input data and has been used in many studies such as landslide susceptibility mapping chen et al 2017a dou et al 2014 and groundwater potential mapping razandi et al 2015 the expression for the pcf is as given below hou et al 2018 1 pcf p p m p p n p p m 1 p p n p p m p p n p p m p p n p p n 1 p p m p p m p p n where ppm is the probability of a spring occurring in category m and ppn is the prior probability n for the total springs in the study area the certainty factors range between 1 and 1 which signify definitely false and definitely true respectively chen et al 2017a 2 4 adaptive neuro fuzzy inference systems anfis fuzzy logic is increasingly used by scientists in many fields however it does not give good results in the case of the unforeseen chen et al 2017b dehnavi et al 2015 therefore artificial neural networks were used to optimize this method which led to the design of anfis in recent researches many different fuzzy inference styles have been used in anfis of which the most popular and usable are the sugeno and mamdani styles as shown in fig 3 a the fuzzy inference style used in this study consists of two inputs two mfs for each input and two rules the sugeno fuzzy style had been designed according to two if rules which are defined as follows jang 1993 2 rule 1 i f x i s a 1 a n d y i s b 1 t h e n f 1 p 1 x q 1 y r 1 3 rule 2 i f x i s a 2 a n d y i s b 2 t h e n f 2 p 2 x q 2 y r 2 where x y are inputs a b are the fuzzy sets f is the output and p q and r are parameters that are determined by artificial neural networks as shown in fig 3b the usual anfis structure consists of 5 layers as follows jang and sun 1995 jang 1993 the first layer is fuzzification this layer consists of a set of variable nodes shown as n in fig 3b and has the task of fuzzifying the values of the input data fuzzification is done by mfs of which the most popular are the triangular trapezoidal gaussian and bell functions 4 o i 1 Œº a i x i 1 2 5 o i 1 Œº b i 2 y i 3 4 where x and y are the inputs for the i th node a n d Œº a i x a n d Œº b i 2 y are the mfs in this study the bell function is used because it is a smooth and concise notation function commonly used in different fields as an mf √ºbeyli 2008 therefore in this study Œº a i x as given below is used 6 Œº a i x 1 1 x c i a i 2 b i where a i b i c i are the premise parameters of the bell function jang and sun 1995 jang 1993 the second layer is fuzzy and this layer consists of a set of fixed nodes shown as m in fig 3b and has a role as a fuzzy and operation with the task of calculating the value of the firing strengths of the rules 7 o i 2 w i Œº a i x Œº b i y i 1 2 where w i is the firing strength of a rule for the i th node the third layer is normalization this layer consists of a set of fixed nodes shown as n in fig 3b and has the role of normalizing the strength of all the rules this process can be represented on the basis of the following equation 8 o i 3 w i w i w 1 w 2 i 1 2 the fourth layer includes a set of avoided nodes which use a linear equation to calculate the required entry for the final layer and can be defined as 9 o i 4 w i f i w i p i x q i y r i i 1 2 where w i denotes the i th normalized firing strength and p i q i a n d r i are the set of parameters referred to as consequent parameters the fifth layer is established from constant nodes denoted by the symbol œÉ which sums up all the output values of the fourth layer and represents the final result of anfis 10 o i 5 i 1 2 w i f i i 1 2 w i f i w 1 w 2 recently many scientists in the field of geoscience have tried to optimize anfis and obtained perfect results chen et al 2019a hong et al 2018 tien bui et al 2018 in this study the teaching learning based optimization tlbo and biogeography based optimization bbo were used as the optimizing parameters of anfis 2 5 teaching learning based optimization tlbo one of the metaheuristic algorithms inspired by the process of teaching inside a classroom is called tlbo and was first introduced by rao et al 2011a b the tlbo is a population based method similar to all other evolutionary algorithms in which each constituent of the population is a learner in essence the learner acts as a solution to the problem this algorithm has become a simple yet efficient technique for solving non linear optimization problems rao et al 2012 after the creation of the initial population the algorithm splits into two main phases namely teacher and learner as follows 2 5 1 initialization the first phase deals with randomly developing the initial population n p d in which np is the population number or the learners and d stands for the dimensions available to the problem e g the total subjects affected the initial population is represented by the following matrix rao et al 2011b 11 i n i t i a l p o p u l a t i o n x x 1 1 x 1 d x np 1 x np d 2 5 1 1 teacher phase the second phase includes the efforts of the teacher to teach the learners and boost the mean score of the final results of the class related to the subject assigned to her the top learner i e the best solution is considered to be the teacher the mean value of each of the columns or the mean value of the marks obtained by different students in each of the subjects can be calculated as below 12 m d m 1 m 2 m d the difference between the mean results for a certain subject and those obtained by the teacher are calculated by using the expression given below rao et al 2011a 13 m diff r a n d 0 1 x best t f m d where t f is the factor of teaching r a n d 0 1 is a random number between 0 and 1 and the value of t f is either 1 or 2 this number is randomly chosen according to eq 14 given below rao et al 2011a 14 t f r o u n d 1 r a n d 0 1 the current population can therefore be updated by the following equation rao et al 2011a 15 x new x m diff the elements of x new are accepted and if f x new f x the other elements of x are acceptable 2 6 learner phase the last phase of the algorithm deals with the tlbo being utilized for the optimization of the local search capabilities in this phase a learner tries to improve his knowledge through interaction with his classmates each learner learns new things from other learners whom he chooses randomly on the condition that the chosen learner has higher knowledge than him the learning phase is expressed through the following formula rao et al 2011a two learners x i and x j are randomly selected such that j i 16 x new x i r a n d 0 1 x i x j i f f x i f x j else x new x i r a n d 0 1 x j x i a c c e p t x new i f i t p e r f o r m s b e t t e r 2 7 biogeography based optimization bbo the bbo is an evolutionary algorithm from biogeography which was first introduced by simon 2008 the study of species and ecosystems in geographical texture is achieved through geological time measurements and thus this algorithm has roots in idealizing the migration of species between islands trying to locate more satisfactory places the algorithm is analogous to general problem solutions and like all other evolutionary algorithms has a population of individuals each of which is regarded as a candidate solution for an optimization problem each particle is called a habitat and the specifications of each solution are represented by suitability index variables sivs the siv is usually utilized as a search variable therefore it constitutes a series of all possible sivs related to the space of search from which a solution is selected the habitat suitability index hsi is used for determining the degree of suitability of solutions i e a solution with a higher hsi is more suitable than that with a lower hsi in this algorithm the solutions or habitats are improved through two main functions called migration and mutation as described below 2 8 migration it is possible to improve the quality of a solution by modifying it in accordance with other solutions where the modification is performed by the operation of migration this may be of two forms immigration or emigration the immigration rate ks of the solutions is the criterion to decide whether modifying each siv is necessary or not bhattacharya and chattopadhyay 2010 similar to the siv which is selected for modification the rate of emigration is of the solutions is utilized in order to choose probabilistically which solutions from among the set of populations are most prone to migration bhattacharya and chattopadhyay 2010 it is expected that like any other evolutionary method some high quality and elite solutions in the populations of the bbo method are hindered from the operation of migration in order not to be probabilistically corrupted bhattacharya and chattopadhyay 2010 roy et al 2010 2 9 mutation in nature the his of a habitat may change suddenly and unexpectedly as a result of natural hazards and disasters or other incidents in such situations it is possible for the habitat to deviate from its hsi equilibrium value the siv mutation expresses the situation in the technique of the bbo and the probabilities of the species count evaluate the mutation rates bhattacharya and chattopadhyay 2010 hadidi and nazari 2013 roy et al 2010 the species count probabilities are calculated by the following formula simon 2008 17 p s h Œª s Œº s p s Œº s 1 p s 1 s 0 Œª s Œº s p s Œª s 1 p s 1 Œº s 1 p s 1 1 s s max 1 Œª s Œº s p s Œª s 1 p s 1 s s max where Œº s Œª s p s are the emigration immigration and probability rates of the habitats containing s species respectively in addition s max represents the maximum number of species the emigration and immigration rates can also be calculated by the following formula simon 2008 18 Œª s i 1 s s max 19 Œº s es s max in these equations e and i represent the maximum emigration and immigration rates respectively a probability is allocated to each member in the population recognizing the member as a potential solution to a certain problem this probability determines whether that member will or will not mutate a high probability of a member indicates that the solution is closer to the ultimate solution for that problem and therefore that member should not mutate however the members having low probability should mutate to other solutions the mutation rate for each solution which is a function of the count probability for the species is as follows simon 2008 20 m s m max 1 1 p s p max where m s m max and p max are the mutation rates for a habitat containing s species maximum mutation rate and maximum probability respectively in this process there is lower probability for low and high hsi valued habitats to mutate as compared to the average hsi valued habitats overall the mutation process improves the suitability index 2 10 application of anfis ensemble models in this study the performance of these approaches can be summarized as follows 1 weights are obtained by the pcf method for every class of the affecting factor 2 these weights are then considered as inputs to the anfis models 3 these numerical values are normalized in the range of 1 and 0 that represent the existence and non existence respectively of groundwater springs and 4 the modeling accuracy is calculated and checked by statistical methods and testing datasets the rmse parameter was used for determining the training and testing accuracy of each hybrid model this parameter indicates the closeness of the output of each hybrid model to the real value 2 11 model validation an evaluation of the models is the main step in modeling without which the result of a study does not have any scientific significance chen et al 2018e chung and fabbri 2003 the prediction ability of the developed models was investigated using the root mean square error rmse andreceiver operating characteristiccurve roc chen et al 2018d 2019c there are two types of the roc curve known as success and prediction rate curves which are constructed using training and testing datasets respectively as the success rate curve is built by the training dataset it shows only the suitability of the built model for spring modeling and cannot be used for model evaluation therefore the prediction rate curve which was not used in the modeling phase was applied for model evaluation and comparisons the area under these curves auc shows the model accuracy quantitatively chen et al 2017d khosravi et al 2018b the auc values range from 0 5 to 1 0 and a higher auc value indicates a better model prediction power chen et al 2017a c 3 results 3 1 correlation between springs and affecting factors using the pcf the results of the correlation between the springs and affecting factors are shown in table 2 the slope aspect in the north has the highest impact 0 424 on spring occurrences which is due to the lowest solar radiation and more land wetness as compared to other aspects in the case of the altitude the first class of 1084 1150 m has the highest effect on groundwater spring occurrences 0 601 as a lower altitude results in a higher probability of occurrence of groundwater springs due to hydraulic gradient and also because the water table reaches the ground surface at low altitude the more the slope angle the higher is the probability of occurrence of a spring in the study areas as slopes of more than 22 22 have the highest impact 0 589 on spring occurrences the result of the pcf method shows that the more the plan curvature the lower is the probability of occurrence of a spring whereas the more the profile curvature the higher are the pcf values 0 610 the results of curvature show that the springs occurred at lower values of curvature 0 547 which demonstrated that springs appear on a convex curvature at the foothills the results of the pcf methods indicate that the more the sti the higher is the probability of occurrence of springs as an sti of more than 20 has the highest value of pcf 0 734 the result shows that an spi of more than 20 and less than 5 has the highest 0 634 and lowest 0 453 impact respectively on spring occurrences in the zhangjiamao area the pcf method shows that the more the twi the higher is the probability of occurrence of a spring as springs are formed when the water table reaches the ground surface the wetness in these areas is higher than that in other regions and shows that there is a direct relationship between the twi and occurrence of springs there is no specific relationship between the distance to roads and occurrence of springs but springs occurred at distances of 445 748 m from the roads a distance of less than 104 1 m from the river has the highest impact on the occurrence of a spring whereas a distance of more than 104 1 m from the river does not have any effect the result of the relationship between rainfall and springs shows that most of the springs appear due to a rainfall of 380 47 381 5 mm and with increasing rainfall the pcf values are reduced this may be due to the fact that with an increase in the elevation the rainfall increases but springs occur at lower elevation the j2y 2 geological unit has the highest influence on the occurrence of springs 0 910 followed by j2y 3 0 896 q3s 0 798 and j2y 4 0 690 the result shows that light chestnut soil has more impact 0 361 than grassland sandy soil 0 466 on spring occurrences the result of the pcf method shows a direct relationship between the ndvi and spring occurrence as the more the ndvi the higher the probability of occurrence of a spring farmland areas have the highest effect 0 330 on spring occurrences but the correlation between forest areas and spring occurrences is negative indicating that there are other factors that have greater effect than the lulc factors on the occurrence of springs 3 2 selection of spring affecting factors in the present study the correlation attribute evaluation cae method based on the average merit am with 10 fold cross validation during the process tien bui et al 2016b was used for assessment of the predictive capabilities of 16 spring affecting factors this method evaluates the contribution of an attribute by measuring the pearson s correlation coefficient pcc between the attribute and the class chen et al 2018c witten et al 2011 and is shown in fig 4 it can be observed that the highest average merit is for ndvi am 0 562 followed by altitude am 0 557 distance to rivers am 0 520 lithology am 0 496 spi am 0 380 soil am 0 355 curvature am 0 293 profile curvature am 0 263 plan curvature am 0 262 sti am 0 255 twi am 0 224 slope angle am 0 142 distance to roads am 0 092 slope aspect am 0 067 lulc am 0 064 and rainfall am 0 019 as the predictive capabilities of all the 16 spring affecting factors were positive all of them were selected in the study 3 3 application of anfis ensemble models and their assessment as shown in figs 5 and 6 c the rmse values of anfis tblo and anfis bbo were calculated for the training phase as 0 179 and 0 173 respectively this indicates that these models have an almost equal performance in the training phase however the rmse of the training phase as well as that for the testing phase must be determined to ascertain the best model in optimization according to the results of the testing dataset the rmse value of both the anfis tblo and anfis bbo hybrid models was 0 166 and 0 172 respectively in which indicates that they exhibit equal performance as shown in figs 5 and 6 e in addition to accuracy the determination of the processing speed of applied models is also very important for rapid assessment therefore in this study the processing time of 1000 iterations for each model was calculated on the basis of programming and found to be 332 and 187 s for anfis tlbo and anfis bbo respectively fig 7 as the processing speed of anfis bbo is much higher than that of anfis tlbo the calculations take less time moreover as shown in fig 8 the learning convergence for the two hybrid models can be determined by plotting the calculated cost function value in each iteration the results showed that the cost function values of the anfis tlbo and anfis bbo models are constant in 600 and 200 iterations respectively which indicate the rapid convergence of the anfis bbo model as compared to the anfis tlbo model 3 4 generating groundwater potential maps at first two ensemble models of anfis tlbo and anfis bbo were constructed using the training dataset and pcf values the built models were then applied to the entire study area to calculate the groundwater spring potential indices gspi next according to the natural break classification scheme chen et al 2018a naghibi and pourghasemi 2015 the continuous gspi were divided into 5 classes viz very low low moderate high and very high to display groundwater potential mapping in the study areas figs 9 and 10 generally user defined classification of groundwater spring potential maps is more difficult for the reader to interpret and justify therefore current automatic classification systems should be used baeza et al 2016 but the selection of the method applied should be based on the histogram of indices in the present study the natural break method which is the most commonly used model was most suitable for the classification of groundwater spring potential indices according to the histogram of data distribution it can be seen that for the anfis tlbo model the low class has the largest area 27 23 followed by the moderate 27 11 high 19 99 very low 16 23 and very high 9 44 classes the corresponding values of the area for the anfis bbo model are 26 71 27 26 20 14 16 09 and 9 80 for the low moderate high very low and very high classes respectively fig 11 3 5 model validation and comparison the roc plots using the training and validation datasets for the two new hybrid models are shown in figs 12 and 13 the results show that the anfis tlbo model exhibits a higher auc value of 0 866 for the training dataset followed by the anfis bbo model with an auc value of 0 8861 although the results of these two models are close to each other for the validation dataset the anfis tlbo 0 905 model exhibits a slightly higher auc value than that of the anfis bbo model 0 887 therefore it is concluded that both the models show reasonable auc values for the study area 4 discussion generally there is no model and algorithm with a maximum prediction power that works perfectly and natural phenomena and natural resources modelling are non linear complex processes and cannot be based on simple models with a linear structure in the present study two completely novel metaheuristic algorithms namely the bbo and tlbo which were not used earlier were tested and coupled with anfis a largely used artificial intelligence model at first the factors that are effective for the groundwater occurrence were identified for recognition of the groundwater resources the precision of the results completely depends on the model prediction power as well as accuracy of the input data thus the effect of these factors should be investigated and the factor with null effects must be removed from the modeling as these cause a noise that reduces the prediction capability of the resulting models chen et al 2019b according to the results of the assessment of the predictive capability of the spring affecting factors fig 4 it can be concluded that all the 16 spring affecting factors have positive contributions to the modeling process and thus they were all selected in the study ndvi has the highest contribution due to some factors which are influenced by the vegetation index ndvi affects the water flow in the watershed and soil environment and its characteristics such as infiltration rate which results in higher infiltration and higher groundwater spring potential ability altitude is another important factor affecting groundwater spring occurrence which is in agreement with the results of naghibi et al 2017a and rahmati et al 2016 rainfall has the lowest contribution to the modeling because the study area is located in the semi arid areas of china where the annual rainfall is limited however the importance of the factors in groundwater spring potential mapping is greatly affected by the properties of the study area and methods used in the research naghibi and pourghasemi 2015 in the literature review it is seen that many bivariate multivariate and machine learning models have been used in groundwater spring potential modeling and some of the models produced good results al abadi et al 2016a corsini et al 2009 lee et al 2017 naghibi et al 2017b 2018 ozdemir 2011 however hybrid models are increasingly being used in recent years the efficacy of hybrid methods may assist researchers in future groundwater related studies therefore novel hybrid models of anfis with the tlbo and bbo were used in the present study according to the results the anfis tlbo model with the higher auc values of 0 859 and 0 894 for the training and validation datasets yielded better results in groundwater spring potential mapping in the present study in the modeling of anfis tlbo there was no need to tune any additional parameter and the best solution of the current iteration was used to increase the convergence rate das and padhy 2018 the results also showed that the processing speed of anfis bbo is much higher than that of anfis tlbo which indicates that the assignment of parameters is very important as regards the percentage of the high and very high groundwater spring potential classes the two hybrid models exhibited a similar spatial distribution however anfis tlbo targeted a smaller percentage 25 66 as compared to the anfis bbo model 30 52 this implies a more practical result for the anfis tlbo model which can reduce the time and cost of an effective groundwater exploration plan or land use plan in the study area to summarize there is no algorithm that works perfectly for all optimization problems and new algorithms must be applied and verified to determine the one that is most efficient khosravi et al 2018a besides it was established in the present study that hybrid models could achieve better results for spatial prediction of groundwater potential the ensembles of anfis with metaheuristic algorithms can also be applied for other spatial prediction modeling such as landslide susceptibility mapping flood susceptibility evaluation and other endeavors at a regional level 5 conclusions groundwater potential modeling using different affecting factors is an important aspect in groundwater studies in the present study two new hybrid models namely anfis tlbo and anfis bbo were applied for groundwater potential zonation the main results of the study are summarized as follows 1 the results showed that both the hybrid models exhibited good results the anfis tlbo model had the higher prediction power 0 866 and 0 905 followed by the anfis bbo model 0 861 and 0 887 2 according to the results of the pcf method most springs existed at altitudes of 1084 1150 m with the northern aspect a slope of 22 228 47 632 plan curvature of 5 427 to 1 135 profile curvature of 1 164 6 073 curvature of 11 5 to 2 059 sti 20 spi 20 twi 5 445 749 748 630 m distance to roads 0 104 181 m distance to rivers rainfall between 380 47 and 381 5 mm j2y 2 lithology unit light chestnut soil type and a farmland land cover category 3 based on the correlation attribute evaluation method the most important factors affecting groundwater occurrence were the ndvi altitude distance to rivers lithology spi soil curvature profile curvature plan curvature sti twi slope angle distance to roads slope aspect lulc and rainfall the novel approach that was developed successfully combined bivariate statistical analysis pcf anfis and two evolutionary algorithms in solving the non linear high dimensional problem such as groundwater potential modeling the proposed novel approach could be useful for groundwater management and exploration development declaration of interest none acknowledgments the authors wish to thank prof enke hou for useful information provided this study was supported by the national natural science foundation of china grant no 41807192 china postdoctoral science foundation grant nos 2018t111084 2017m613168 and project funded by shaanxi province postdoctoral science foundation grant no 2017bshydzz07 and college of agriculture shiraz university grant no 96grd1m271143 authors also would like to thank three anonymous reviewers and emmanouil anagnostou editor journal of hydrology for their helpful comments on the previous version of the manuscript appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j jhydrol 2019 03 013 appendix a supplementary data the following are the supplementary data to this article supplementary data 1 
6601,groundwater is one of the most beneficial natural resources worldwide the main aim of the present study is the mapping of groundwater potential of the zhangjiamao area in china using two novel hybrid data mining techniques that involve the adaptive neuro fuzzy inference system anfis ensembled with teaching learning based optimization tlbo and biogeography based optimization bbo first 93 spring locations were identified in the study area which were randomly divided in the ratio of 70 30 to be used for building models and validation respectively a total of 16 spring affecting factors viz slope aspect altitude slope angle plan curvature profile curvature curvature sediment transport index stream power index topographic wetness index distance to roads distance to rivers rainfall lithology soil ndvi and land use land cover were used as input variables the probability certainty factor pcf method was applied for the correlation analysis between spring occurrences and the affecting factors subsequently the anfis tlbo and anfis bbo models were generated to produce groundwater spring potential maps gspms finally the gspms were evaluated by using the area under the receiver operating characteristic auroc the results showed that the auroc values for the anfis tlbo model based on the training and validation datasets were 0 866 and 0 905 respectively in contrast the auroc values for the anfis bbo model based on the training and validation datasets were 0 861 and 0 887 respectively the results of anfis tlbo showed that 25 66 of the study area could be classified into the high and very high groundwater spring potential classes as compared to the anfis bbo model 30 52 the results of the present study can be useful for groundwater management keywords groundwater potentiality anfis tlbo bbo china 1 introduction water resources made up of surface water and groundwater are considered to be among the most crucial natural resources in arid and semi arid regions rahmati et al 2016 due to their characteristics of circulation fluidity and inequality in space time distribution groundwater is more precious than surface water in these regions because of its widespread distribution low vulnerability constant chemical composition fixed temperature and better quality rahmati et al 2018 it is defined as the water in saturated zone areas that fill the pore spaces nampak et al 2014 and groundwater potential probability is the probability of groundwater occurrence in a specific area khosravi et al 2018a currently the demand for groundwater for domestic use and industrial and agricultural water requirement mogaji et al 2015 is increasing therefore better management of groundwater is necessary hence it is advisable to use groundwater spring potential mapping for groundwater determination management and protection programs naghibi and pourghasemi 2015 in previous research works geographical information system gis and remote sensing rs were two popular and effective methods used for groundwater potential mapping al abadi et al 2016b machiwal et al 2011 oikonomidis et al 2015 rahmati et al 2014 many statistical methods such as weights of evidence woe al abadi 2015 frequency ratio fr moghaddam et al 2015 evidential belief function ebf nejad et al 2016 park et al 2014 and index of entropy ioe al abadi and shahid 2015 were also used in addition various machine learning approaches have also been applied in recent years such as the classification and regression tree cart naghibi et al 2016 support vector machine svm naghibi et al 2017a random forest rf rahmati et al 2016 and artificial neural network ann lee et al 2017 machine learning algorithms have some advantages such as the ability to handle huge amounts of non linear structured data from various sources at different scales khosravi et al 2018a the ann is one of the most popular and widely used methods and has been applied in a different aspect of hydrological sciences lee et al 2017 sokeng et al 2016 tsakiri et al 2018 anns have the advantage of computational efficiency but also the limitations of poor prediction and error in the modeling process tien bui et al 2016a thus the ann model ensemble with fuzzy logic viz the adaptive neuro fuzzy inference system anfis was proposed anfis is a hybrid system of a fuzzy and artificial neural network that can produce better results than the fuzzy inference system fis for solving non linear problems by developing a powerful structure jang and sun 1995 jang 1993 in fact this model is a multilayer feed forward network that uses fis for training dixon 2005 anfis can train the fis membership function mf parameters by using training input data and combining methods such as the back propagation gradient descent and least squares however it is still difficult to find the best internal weight values of anfis in a membership function khosravi et al 2018a an ensemble of meta heuristic algorithms with the anfis model can overcome the drawback of anfis and also enhance its prediction power khosravi et al 2018a more recently ensemble models particularly the ensembles of anfis with metaheuristic algorithms chen et al 2018a naghibi et al 2017b have been increasingly used for enhancing the result of modelling especially in groundwater potential mapping chen et al 2018a naghibi et al 2017b landslide susceptibility modelling pham et al 2018 umar et al 2014 and flood susceptibility mapping tehrany et al 2014 however the use of ensemble models of anfis with metaheuristic algorithms for groundwater potential mapping is still uncommon metaheuristic algorithms such as the teaching learning based optimization tlbo and biogeography based optimization bbo are widely used in many fields viz constrained mechanical design optimization problems rao et al 2011b dynamic group strategy for global optimization zou et al 2014 data clustering satapathy and naik 2011 building the matching mathematical model of organism distribution guo et al 2014 and blend migration ma and dan 2010 these approaches are simple yet efficient for solving non linear optimization problems hybrid methods are known to increase the prediction accuracy of single based classifiers due to their higher power of prediction and greater ability of recognition chen et al 2018b however the result of the modelling process depends on data accuracy as well as the prediction power of the model therefore by selecting a suitable dataset and model the probability of achieving accurate results will increase in the present study the new hybrid models of anfis with tlbo and bbo were designed and developed using matlab programming and verified for the case study of groundwater potential mapping at the zhangjiamao area in china in general a literature review has shown that no research work has been carried out as yet related to the use of a hybrid of anfis with tlbo and bbo algorithms it should also be noted that there are no global guidelines for the selection of the spring affecting factors which were determined in this study on the basis of literature reviews case study characteristics and data availability naghibi et al 2017a 2 materials and methods 2 1 study area the zhangjiamao area is located in the shaanxi province china which is within the latitudes of 38 57 38 n to 39 01 37 n and longitudes of 110 16 21 e to 110 23 31 e it covers an area of roughly 52 km2 fig 1 the altitudes of the study area vary from 1084 m to 1323 m and the slope angles from 0 to 47 63 according to the digital elevation model dem with a 20 m regular grid produced from the topographic map on a scale of 1 25000 it is located in the inland of western china with a semi arid continental climate according to the shenmu county meteorological station the mean annual rainfall in this region is 436 6 mm and most of it occurs from july to september most of the vegetation in this area consists of herbaceous and bush plants owing to the special geographical position the study area is covered with loose sand in the southwest whereas other areas are loess hilly with gullies fan et al 2018 the mainly outcropped lithological units are aeolian sand clay or sandy clay sandstone and siltstone fan et al 2018 2 2 spatial datasets at first according to documentary sources 93 springs were identified in the study area and checked using hand held gps and extensive field surveys and considered to be a spring inventory map the same number of non spring locations was produced randomly by using the arcgis software the spring and non spring locations were divided randomly into two parts in the ratio of 70 30 70 of the springs and non springs were merged for construction of a training dataset which was used for model building similarly the remaining 30 of the springs and non springs were merged for construction of a testing dataset which was applied for model validation marnerides et al 2015 the springs and non springs were assigned to 1 and 0 respectively for the modeling according to the literature review khosravi et al 2018a lee et al 2017 rahmati et al 2016 zeinivand and ghorbani nejad 2018 and characteristics of the study area 16 spring affecting factors viz slope aspect altitude slope angle plan curvature profile curvature curvature sediment transport index sti stream power index spi topographic wetness index twi distance to roads distance to rivers rainfall lithology soil ndvi and land use land cover lulc were selected for the preparation of the groundwater spring potential mapping fig 2 a digital elevation model dem with a spatial resolution of 20 20 m2 was created using the topographic map on a scale of 1 25000 different data viz the slope aspect altitude slope angle plan curvature profile curvature sti spi and twi were then directly extracted from the dem by using the arcgis and saga gis software the maps of the distances to roads and rivers were also produced using the topographic maps the rainfall map was prepared with a 20 year 1998 2017 mean annual rainfall dataset by using the inverse distance weighted method kriging due to the lowest amount of rmse and reclassified into seven classes at intervals of 20 mm the lithology map was obtained from the geological map on a scale of 1 10000 which was obtained from the china geology survey http www cgs gov cn ten geologic formations were divided into 10 classes table 1 the soil type was extracted from the soil maps of the study area on a scale of 1 1000000 the ndvi map was generated using landsat 8 operational land imager oli images path row 126 33 date november 20 2017 product id lc81260332017324lgn00 and envi software the lulc map was extracted from land use maps on a scale of 1 100000 the classification of the continuous thematic layers was carried out on the basis of different methods attributed to the function of each factor in the current study the continuous layers viz the slope angle plan curvature profile curvature curvature distance to roads distance to rivers and ndvi were classified into five classes based on the jenks natural break classification scheme the other continuous layers such as the altitude sti spi twi and rainfall were reclassified using certain intervals on the basis of the above mentioned steps all the spring affecting factors were eventually converted into a raster with the same spatial resolution of 20 20 m2 a detailed delineation and classification of all the factors is shown in table 2 2 3 data correlation analysis in this study the probability certainty factor pcf method was used to show the correlation between the springs and affecting factors the pcf method is beneficial for handling any uncertainty in the input data and has been used in many studies such as landslide susceptibility mapping chen et al 2017a dou et al 2014 and groundwater potential mapping razandi et al 2015 the expression for the pcf is as given below hou et al 2018 1 pcf p p m p p n p p m 1 p p n p p m p p n p p m p p n p p n 1 p p m p p m p p n where ppm is the probability of a spring occurring in category m and ppn is the prior probability n for the total springs in the study area the certainty factors range between 1 and 1 which signify definitely false and definitely true respectively chen et al 2017a 2 4 adaptive neuro fuzzy inference systems anfis fuzzy logic is increasingly used by scientists in many fields however it does not give good results in the case of the unforeseen chen et al 2017b dehnavi et al 2015 therefore artificial neural networks were used to optimize this method which led to the design of anfis in recent researches many different fuzzy inference styles have been used in anfis of which the most popular and usable are the sugeno and mamdani styles as shown in fig 3 a the fuzzy inference style used in this study consists of two inputs two mfs for each input and two rules the sugeno fuzzy style had been designed according to two if rules which are defined as follows jang 1993 2 rule 1 i f x i s a 1 a n d y i s b 1 t h e n f 1 p 1 x q 1 y r 1 3 rule 2 i f x i s a 2 a n d y i s b 2 t h e n f 2 p 2 x q 2 y r 2 where x y are inputs a b are the fuzzy sets f is the output and p q and r are parameters that are determined by artificial neural networks as shown in fig 3b the usual anfis structure consists of 5 layers as follows jang and sun 1995 jang 1993 the first layer is fuzzification this layer consists of a set of variable nodes shown as n in fig 3b and has the task of fuzzifying the values of the input data fuzzification is done by mfs of which the most popular are the triangular trapezoidal gaussian and bell functions 4 o i 1 Œº a i x i 1 2 5 o i 1 Œº b i 2 y i 3 4 where x and y are the inputs for the i th node a n d Œº a i x a n d Œº b i 2 y are the mfs in this study the bell function is used because it is a smooth and concise notation function commonly used in different fields as an mf √ºbeyli 2008 therefore in this study Œº a i x as given below is used 6 Œº a i x 1 1 x c i a i 2 b i where a i b i c i are the premise parameters of the bell function jang and sun 1995 jang 1993 the second layer is fuzzy and this layer consists of a set of fixed nodes shown as m in fig 3b and has a role as a fuzzy and operation with the task of calculating the value of the firing strengths of the rules 7 o i 2 w i Œº a i x Œº b i y i 1 2 where w i is the firing strength of a rule for the i th node the third layer is normalization this layer consists of a set of fixed nodes shown as n in fig 3b and has the role of normalizing the strength of all the rules this process can be represented on the basis of the following equation 8 o i 3 w i w i w 1 w 2 i 1 2 the fourth layer includes a set of avoided nodes which use a linear equation to calculate the required entry for the final layer and can be defined as 9 o i 4 w i f i w i p i x q i y r i i 1 2 where w i denotes the i th normalized firing strength and p i q i a n d r i are the set of parameters referred to as consequent parameters the fifth layer is established from constant nodes denoted by the symbol œÉ which sums up all the output values of the fourth layer and represents the final result of anfis 10 o i 5 i 1 2 w i f i i 1 2 w i f i w 1 w 2 recently many scientists in the field of geoscience have tried to optimize anfis and obtained perfect results chen et al 2019a hong et al 2018 tien bui et al 2018 in this study the teaching learning based optimization tlbo and biogeography based optimization bbo were used as the optimizing parameters of anfis 2 5 teaching learning based optimization tlbo one of the metaheuristic algorithms inspired by the process of teaching inside a classroom is called tlbo and was first introduced by rao et al 2011a b the tlbo is a population based method similar to all other evolutionary algorithms in which each constituent of the population is a learner in essence the learner acts as a solution to the problem this algorithm has become a simple yet efficient technique for solving non linear optimization problems rao et al 2012 after the creation of the initial population the algorithm splits into two main phases namely teacher and learner as follows 2 5 1 initialization the first phase deals with randomly developing the initial population n p d in which np is the population number or the learners and d stands for the dimensions available to the problem e g the total subjects affected the initial population is represented by the following matrix rao et al 2011b 11 i n i t i a l p o p u l a t i o n x x 1 1 x 1 d x np 1 x np d 2 5 1 1 teacher phase the second phase includes the efforts of the teacher to teach the learners and boost the mean score of the final results of the class related to the subject assigned to her the top learner i e the best solution is considered to be the teacher the mean value of each of the columns or the mean value of the marks obtained by different students in each of the subjects can be calculated as below 12 m d m 1 m 2 m d the difference between the mean results for a certain subject and those obtained by the teacher are calculated by using the expression given below rao et al 2011a 13 m diff r a n d 0 1 x best t f m d where t f is the factor of teaching r a n d 0 1 is a random number between 0 and 1 and the value of t f is either 1 or 2 this number is randomly chosen according to eq 14 given below rao et al 2011a 14 t f r o u n d 1 r a n d 0 1 the current population can therefore be updated by the following equation rao et al 2011a 15 x new x m diff the elements of x new are accepted and if f x new f x the other elements of x are acceptable 2 6 learner phase the last phase of the algorithm deals with the tlbo being utilized for the optimization of the local search capabilities in this phase a learner tries to improve his knowledge through interaction with his classmates each learner learns new things from other learners whom he chooses randomly on the condition that the chosen learner has higher knowledge than him the learning phase is expressed through the following formula rao et al 2011a two learners x i and x j are randomly selected such that j i 16 x new x i r a n d 0 1 x i x j i f f x i f x j else x new x i r a n d 0 1 x j x i a c c e p t x new i f i t p e r f o r m s b e t t e r 2 7 biogeography based optimization bbo the bbo is an evolutionary algorithm from biogeography which was first introduced by simon 2008 the study of species and ecosystems in geographical texture is achieved through geological time measurements and thus this algorithm has roots in idealizing the migration of species between islands trying to locate more satisfactory places the algorithm is analogous to general problem solutions and like all other evolutionary algorithms has a population of individuals each of which is regarded as a candidate solution for an optimization problem each particle is called a habitat and the specifications of each solution are represented by suitability index variables sivs the siv is usually utilized as a search variable therefore it constitutes a series of all possible sivs related to the space of search from which a solution is selected the habitat suitability index hsi is used for determining the degree of suitability of solutions i e a solution with a higher hsi is more suitable than that with a lower hsi in this algorithm the solutions or habitats are improved through two main functions called migration and mutation as described below 2 8 migration it is possible to improve the quality of a solution by modifying it in accordance with other solutions where the modification is performed by the operation of migration this may be of two forms immigration or emigration the immigration rate ks of the solutions is the criterion to decide whether modifying each siv is necessary or not bhattacharya and chattopadhyay 2010 similar to the siv which is selected for modification the rate of emigration is of the solutions is utilized in order to choose probabilistically which solutions from among the set of populations are most prone to migration bhattacharya and chattopadhyay 2010 it is expected that like any other evolutionary method some high quality and elite solutions in the populations of the bbo method are hindered from the operation of migration in order not to be probabilistically corrupted bhattacharya and chattopadhyay 2010 roy et al 2010 2 9 mutation in nature the his of a habitat may change suddenly and unexpectedly as a result of natural hazards and disasters or other incidents in such situations it is possible for the habitat to deviate from its hsi equilibrium value the siv mutation expresses the situation in the technique of the bbo and the probabilities of the species count evaluate the mutation rates bhattacharya and chattopadhyay 2010 hadidi and nazari 2013 roy et al 2010 the species count probabilities are calculated by the following formula simon 2008 17 p s h Œª s Œº s p s Œº s 1 p s 1 s 0 Œª s Œº s p s Œª s 1 p s 1 Œº s 1 p s 1 1 s s max 1 Œª s Œº s p s Œª s 1 p s 1 s s max where Œº s Œª s p s are the emigration immigration and probability rates of the habitats containing s species respectively in addition s max represents the maximum number of species the emigration and immigration rates can also be calculated by the following formula simon 2008 18 Œª s i 1 s s max 19 Œº s es s max in these equations e and i represent the maximum emigration and immigration rates respectively a probability is allocated to each member in the population recognizing the member as a potential solution to a certain problem this probability determines whether that member will or will not mutate a high probability of a member indicates that the solution is closer to the ultimate solution for that problem and therefore that member should not mutate however the members having low probability should mutate to other solutions the mutation rate for each solution which is a function of the count probability for the species is as follows simon 2008 20 m s m max 1 1 p s p max where m s m max and p max are the mutation rates for a habitat containing s species maximum mutation rate and maximum probability respectively in this process there is lower probability for low and high hsi valued habitats to mutate as compared to the average hsi valued habitats overall the mutation process improves the suitability index 2 10 application of anfis ensemble models in this study the performance of these approaches can be summarized as follows 1 weights are obtained by the pcf method for every class of the affecting factor 2 these weights are then considered as inputs to the anfis models 3 these numerical values are normalized in the range of 1 and 0 that represent the existence and non existence respectively of groundwater springs and 4 the modeling accuracy is calculated and checked by statistical methods and testing datasets the rmse parameter was used for determining the training and testing accuracy of each hybrid model this parameter indicates the closeness of the output of each hybrid model to the real value 2 11 model validation an evaluation of the models is the main step in modeling without which the result of a study does not have any scientific significance chen et al 2018e chung and fabbri 2003 the prediction ability of the developed models was investigated using the root mean square error rmse andreceiver operating characteristiccurve roc chen et al 2018d 2019c there are two types of the roc curve known as success and prediction rate curves which are constructed using training and testing datasets respectively as the success rate curve is built by the training dataset it shows only the suitability of the built model for spring modeling and cannot be used for model evaluation therefore the prediction rate curve which was not used in the modeling phase was applied for model evaluation and comparisons the area under these curves auc shows the model accuracy quantitatively chen et al 2017d khosravi et al 2018b the auc values range from 0 5 to 1 0 and a higher auc value indicates a better model prediction power chen et al 2017a c 3 results 3 1 correlation between springs and affecting factors using the pcf the results of the correlation between the springs and affecting factors are shown in table 2 the slope aspect in the north has the highest impact 0 424 on spring occurrences which is due to the lowest solar radiation and more land wetness as compared to other aspects in the case of the altitude the first class of 1084 1150 m has the highest effect on groundwater spring occurrences 0 601 as a lower altitude results in a higher probability of occurrence of groundwater springs due to hydraulic gradient and also because the water table reaches the ground surface at low altitude the more the slope angle the higher is the probability of occurrence of a spring in the study areas as slopes of more than 22 22 have the highest impact 0 589 on spring occurrences the result of the pcf method shows that the more the plan curvature the lower is the probability of occurrence of a spring whereas the more the profile curvature the higher are the pcf values 0 610 the results of curvature show that the springs occurred at lower values of curvature 0 547 which demonstrated that springs appear on a convex curvature at the foothills the results of the pcf methods indicate that the more the sti the higher is the probability of occurrence of springs as an sti of more than 20 has the highest value of pcf 0 734 the result shows that an spi of more than 20 and less than 5 has the highest 0 634 and lowest 0 453 impact respectively on spring occurrences in the zhangjiamao area the pcf method shows that the more the twi the higher is the probability of occurrence of a spring as springs are formed when the water table reaches the ground surface the wetness in these areas is higher than that in other regions and shows that there is a direct relationship between the twi and occurrence of springs there is no specific relationship between the distance to roads and occurrence of springs but springs occurred at distances of 445 748 m from the roads a distance of less than 104 1 m from the river has the highest impact on the occurrence of a spring whereas a distance of more than 104 1 m from the river does not have any effect the result of the relationship between rainfall and springs shows that most of the springs appear due to a rainfall of 380 47 381 5 mm and with increasing rainfall the pcf values are reduced this may be due to the fact that with an increase in the elevation the rainfall increases but springs occur at lower elevation the j2y 2 geological unit has the highest influence on the occurrence of springs 0 910 followed by j2y 3 0 896 q3s 0 798 and j2y 4 0 690 the result shows that light chestnut soil has more impact 0 361 than grassland sandy soil 0 466 on spring occurrences the result of the pcf method shows a direct relationship between the ndvi and spring occurrence as the more the ndvi the higher the probability of occurrence of a spring farmland areas have the highest effect 0 330 on spring occurrences but the correlation between forest areas and spring occurrences is negative indicating that there are other factors that have greater effect than the lulc factors on the occurrence of springs 3 2 selection of spring affecting factors in the present study the correlation attribute evaluation cae method based on the average merit am with 10 fold cross validation during the process tien bui et al 2016b was used for assessment of the predictive capabilities of 16 spring affecting factors this method evaluates the contribution of an attribute by measuring the pearson s correlation coefficient pcc between the attribute and the class chen et al 2018c witten et al 2011 and is shown in fig 4 it can be observed that the highest average merit is for ndvi am 0 562 followed by altitude am 0 557 distance to rivers am 0 520 lithology am 0 496 spi am 0 380 soil am 0 355 curvature am 0 293 profile curvature am 0 263 plan curvature am 0 262 sti am 0 255 twi am 0 224 slope angle am 0 142 distance to roads am 0 092 slope aspect am 0 067 lulc am 0 064 and rainfall am 0 019 as the predictive capabilities of all the 16 spring affecting factors were positive all of them were selected in the study 3 3 application of anfis ensemble models and their assessment as shown in figs 5 and 6 c the rmse values of anfis tblo and anfis bbo were calculated for the training phase as 0 179 and 0 173 respectively this indicates that these models have an almost equal performance in the training phase however the rmse of the training phase as well as that for the testing phase must be determined to ascertain the best model in optimization according to the results of the testing dataset the rmse value of both the anfis tblo and anfis bbo hybrid models was 0 166 and 0 172 respectively in which indicates that they exhibit equal performance as shown in figs 5 and 6 e in addition to accuracy the determination of the processing speed of applied models is also very important for rapid assessment therefore in this study the processing time of 1000 iterations for each model was calculated on the basis of programming and found to be 332 and 187 s for anfis tlbo and anfis bbo respectively fig 7 as the processing speed of anfis bbo is much higher than that of anfis tlbo the calculations take less time moreover as shown in fig 8 the learning convergence for the two hybrid models can be determined by plotting the calculated cost function value in each iteration the results showed that the cost function values of the anfis tlbo and anfis bbo models are constant in 600 and 200 iterations respectively which indicate the rapid convergence of the anfis bbo model as compared to the anfis tlbo model 3 4 generating groundwater potential maps at first two ensemble models of anfis tlbo and anfis bbo were constructed using the training dataset and pcf values the built models were then applied to the entire study area to calculate the groundwater spring potential indices gspi next according to the natural break classification scheme chen et al 2018a naghibi and pourghasemi 2015 the continuous gspi were divided into 5 classes viz very low low moderate high and very high to display groundwater potential mapping in the study areas figs 9 and 10 generally user defined classification of groundwater spring potential maps is more difficult for the reader to interpret and justify therefore current automatic classification systems should be used baeza et al 2016 but the selection of the method applied should be based on the histogram of indices in the present study the natural break method which is the most commonly used model was most suitable for the classification of groundwater spring potential indices according to the histogram of data distribution it can be seen that for the anfis tlbo model the low class has the largest area 27 23 followed by the moderate 27 11 high 19 99 very low 16 23 and very high 9 44 classes the corresponding values of the area for the anfis bbo model are 26 71 27 26 20 14 16 09 and 9 80 for the low moderate high very low and very high classes respectively fig 11 3 5 model validation and comparison the roc plots using the training and validation datasets for the two new hybrid models are shown in figs 12 and 13 the results show that the anfis tlbo model exhibits a higher auc value of 0 866 for the training dataset followed by the anfis bbo model with an auc value of 0 8861 although the results of these two models are close to each other for the validation dataset the anfis tlbo 0 905 model exhibits a slightly higher auc value than that of the anfis bbo model 0 887 therefore it is concluded that both the models show reasonable auc values for the study area 4 discussion generally there is no model and algorithm with a maximum prediction power that works perfectly and natural phenomena and natural resources modelling are non linear complex processes and cannot be based on simple models with a linear structure in the present study two completely novel metaheuristic algorithms namely the bbo and tlbo which were not used earlier were tested and coupled with anfis a largely used artificial intelligence model at first the factors that are effective for the groundwater occurrence were identified for recognition of the groundwater resources the precision of the results completely depends on the model prediction power as well as accuracy of the input data thus the effect of these factors should be investigated and the factor with null effects must be removed from the modeling as these cause a noise that reduces the prediction capability of the resulting models chen et al 2019b according to the results of the assessment of the predictive capability of the spring affecting factors fig 4 it can be concluded that all the 16 spring affecting factors have positive contributions to the modeling process and thus they were all selected in the study ndvi has the highest contribution due to some factors which are influenced by the vegetation index ndvi affects the water flow in the watershed and soil environment and its characteristics such as infiltration rate which results in higher infiltration and higher groundwater spring potential ability altitude is another important factor affecting groundwater spring occurrence which is in agreement with the results of naghibi et al 2017a and rahmati et al 2016 rainfall has the lowest contribution to the modeling because the study area is located in the semi arid areas of china where the annual rainfall is limited however the importance of the factors in groundwater spring potential mapping is greatly affected by the properties of the study area and methods used in the research naghibi and pourghasemi 2015 in the literature review it is seen that many bivariate multivariate and machine learning models have been used in groundwater spring potential modeling and some of the models produced good results al abadi et al 2016a corsini et al 2009 lee et al 2017 naghibi et al 2017b 2018 ozdemir 2011 however hybrid models are increasingly being used in recent years the efficacy of hybrid methods may assist researchers in future groundwater related studies therefore novel hybrid models of anfis with the tlbo and bbo were used in the present study according to the results the anfis tlbo model with the higher auc values of 0 859 and 0 894 for the training and validation datasets yielded better results in groundwater spring potential mapping in the present study in the modeling of anfis tlbo there was no need to tune any additional parameter and the best solution of the current iteration was used to increase the convergence rate das and padhy 2018 the results also showed that the processing speed of anfis bbo is much higher than that of anfis tlbo which indicates that the assignment of parameters is very important as regards the percentage of the high and very high groundwater spring potential classes the two hybrid models exhibited a similar spatial distribution however anfis tlbo targeted a smaller percentage 25 66 as compared to the anfis bbo model 30 52 this implies a more practical result for the anfis tlbo model which can reduce the time and cost of an effective groundwater exploration plan or land use plan in the study area to summarize there is no algorithm that works perfectly for all optimization problems and new algorithms must be applied and verified to determine the one that is most efficient khosravi et al 2018a besides it was established in the present study that hybrid models could achieve better results for spatial prediction of groundwater potential the ensembles of anfis with metaheuristic algorithms can also be applied for other spatial prediction modeling such as landslide susceptibility mapping flood susceptibility evaluation and other endeavors at a regional level 5 conclusions groundwater potential modeling using different affecting factors is an important aspect in groundwater studies in the present study two new hybrid models namely anfis tlbo and anfis bbo were applied for groundwater potential zonation the main results of the study are summarized as follows 1 the results showed that both the hybrid models exhibited good results the anfis tlbo model had the higher prediction power 0 866 and 0 905 followed by the anfis bbo model 0 861 and 0 887 2 according to the results of the pcf method most springs existed at altitudes of 1084 1150 m with the northern aspect a slope of 22 228 47 632 plan curvature of 5 427 to 1 135 profile curvature of 1 164 6 073 curvature of 11 5 to 2 059 sti 20 spi 20 twi 5 445 749 748 630 m distance to roads 0 104 181 m distance to rivers rainfall between 380 47 and 381 5 mm j2y 2 lithology unit light chestnut soil type and a farmland land cover category 3 based on the correlation attribute evaluation method the most important factors affecting groundwater occurrence were the ndvi altitude distance to rivers lithology spi soil curvature profile curvature plan curvature sti twi slope angle distance to roads slope aspect lulc and rainfall the novel approach that was developed successfully combined bivariate statistical analysis pcf anfis and two evolutionary algorithms in solving the non linear high dimensional problem such as groundwater potential modeling the proposed novel approach could be useful for groundwater management and exploration development declaration of interest none acknowledgments the authors wish to thank prof enke hou for useful information provided this study was supported by the national natural science foundation of china grant no 41807192 china postdoctoral science foundation grant nos 2018t111084 2017m613168 and project funded by shaanxi province postdoctoral science foundation grant no 2017bshydzz07 and college of agriculture shiraz university grant no 96grd1m271143 authors also would like to thank three anonymous reviewers and emmanouil anagnostou editor journal of hydrology for their helpful comments on the previous version of the manuscript appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j jhydrol 2019 03 013 appendix a supplementary data the following are the supplementary data to this article supplementary data 1 
6602,rainfall data scarcity has caused enormous problems in hydrologic and non point pollution h nps predictions as rainfall data represent the key input to watershed models in this study the effects of different imputation methods such as the data augmentation da and the expectation maximization with bootstrap emb algorithms on rainfall data scarcity were compared the effects of different data scarcity rates and periods on model performance and prediction uncertainty were then quantified finally the effects of different imputed data sets on h nps results were evaluated with the soil and water assessment tool swat a real case study in the daning river watershed three gorges reservoir region china was evaluated the results indicated that rainfall data scarcity during low flow periods would result in poorer model performance and larger prediction uncertainty especially to some minimum values and the time when the maximum values are more susceptible is the rainfall data scarcity during high flow periods the repair of rainfall data and the h nps model performance obtained by the emb algorithm are superior to the traditional da and weather generator performances this advantage of the emb algorithm would be more definitive if a specific threshold of data scarcity is reached it is noted that even if the best algorithm is used the imputed value is always lower than the peak observed value this paper reports important implications for the choice of imputation methods and the use of h nps models for solving data scarcity problems for watershed studies keywords emb algorithm multiple imputation methods non point source pollution scarce data swat 1 introduction one of the major concerns facing hydrological researchers can be characterized by variability in rainfall and frequent rainstorms worldwide castillo et al 2003 marzen et al 2017 ringham et al 2016 while continuous rainfall data input certainly represents the indispensable part of hydrology and water quality studies typically rainfall data are obtained from ground stations and remote sensing systems lakshmi 2004 at the same time rainfall stations often serve as the basic source of long term rainfall data kashani et al 2016 however even if the best equipment is used researchers often face data scarcity dilemmas due to electromagnetic lightning pulses and reduced performance of the instrument as well as the effects of human error this data scarcity dilemma can cause enormous problems in obtaining continuous rainfall data series and impact hydrological and water quality analyses which are indeed important for water resource management ram√≠rez et al 2005 junninen et al 2004 during the last few decades hydrologists and mathematicians have worked hard to explore the statistical characteristics of rainfall data sets and have tried to generate specific interpolating approaches to solve the scarcity problem several hypotheses including data scarcity mechanisms have been proposed by dividing the typical data scarcity into the three categories 1 missing completely at random mcar 2 missing at random mar and 3 missing not at random mnar little and rubin 1987 specifically recent model based methods such as maximum likelihood procedures and multiple imputations often assume that the data scarcity follows the mcar or the mar patterns so that specific data distribution could be assumed kalteh and hjorth 2009 according to the number of substitute values for each missing value the method of imputation can be divided into two categories single imputation and multiple imputation acock 2005 single imputation means that only one reasonable substitute value is constructed for each missing value in a certain way and interpolates it to the location of the original scarce data to construct a complete data set a general multiple imputation method for data scarcity was introduced by dempster et al 1977 in their unique research on the expectation maximization em algorithm the expectation maximization algorithm can take into account the pattern of scarce data to further estimate impute the statistical values that will be calculated in order to complete the data set the core of this imputation method is to interpolate missing values from their adjacent data points produce a couple of imputation values and then ultimately constructs complete sample data sets dempster et al 1977 the use of multiple imputations represents an effective method to address scarce data and has been used in various fields nieh et al 2014 evaluated a number of methods such as multiple imputations data deletion and mean interpolation for testing their interpolation effects of missing microbial data the results showed that the multiple imputation method resulted in the least bias compared to the other methods because it can obtain a better parameter estimate via several times imputations previous studies have also indicated that the em algorithm was more preferred than linear regression for imputation of monthly meteorological data sahi n and ci gi zoglu 2010 several studies have also been conducted to compare these imputation methods for example one study compared the effects of linear and non linear techniques on the interpolation of rainfall data and found that the hybrid approach showed the best performance mir√≥ et al 2017 however there are some problems with these methods for solving data scarcity such as the inability to calculate the standard error of parameters and the amount of calculation is too large to solve the problem of previous methods the expectation maximization with bootstrap emb algorithm was developed for data scarcity conditions by combining the advantages of both the multiple imputation and em algorithm approaches however few studies have applied this new method to the interpolation of rainfall data scarcity and as far as we know no study has focused on the comparison of the emb algorithm with other traditional imputation methods which is valuable and we believe is the main message of this paper non point source nps pollution has attracted global attention recently because of its threat to water quality liu et al 2015 chen et al 2016a and watershed models such as the topgraphy based hydrological model topmodel beven and kirkby 1979 the hydrological simulation program fortran hspf model which is developed by u s environmental protection agency in 1981 bicknell et al 2005 and the soil and water assessment tool swat developed by jeff arnold in 1994 arnold and srinivasan et al 1998 which has been used for hydrological and nps h nps analyses liu et al 2017 ouyang et al 2017 among these approaches the swat model has been widely used due to its good function and open source code and the swat has been used to predict h nps cycles under different climates and underlying surface characteristics as rainfall acts as the driving factor of runoff and nps processes rainfall data often serve as the core input and are crucial for these h nps models hrachowitz and clark 2017 for dealing with these scarce rainfall data weather generator tools have been incorporated into models and six meteorological variables including rainfall highest temperature lowest temperature daily average relative humidity daily radiation and daily average wind speed can be interpolated from the available high and low values by the weather generator kilsby et al 2007 several studies have focused on the impacts of rainfall data scarcity on h nps simulations and have reported that the weather generator produces a great deal of deviation because of the extreme values chen et al 2017 coulibaly and evora 2007 studies have also explored the transfer of the spatial distribution of rainfall data to h nps simulations via the quantification of changes in goodness of fit indicators during model calibration chaubey et al 1999 cho et al 2009 shen et al 2012 although the effect of rainfall input on h nps modelling is a well studied field few studies have focused on the effects of rainfall data scarcity on h nps modelling and have failed to compare the impacts of different imputation methods on h nps outputs furthermore the uncertainty is currently considered one of the core dilemmas in h nps studies due to human beings understanding limitations about the complex water environment it is necessary to study the impact of input data on the model uncertainty as it can not only improve the reliability of simulation results but also reduce human errors chen et al 2018a most researches focus on assessing the uncertainty of complex water quality models and its influence on the allowable pollutant load via improving bayesian analytical approach liang et al 2016 jia et al 2018 and few studies have quantified the impacts of rainfall data temporal scarcity nor have compared different imputation methods to reduce the model uncertainty caused by rainfall data scarcity this study focuses on the impacts of imputation methods on rainfall input data and then on h nps simulations the objectives of this study were to 1 compare the effects of different imputation methods on rainfall data scarcity 2 investigate the effects of data scarcity on model uncertainty and 3 evaluate and analyse the effects of the different imputed data sets on h nps results the case study was performed in the daning river watershed three gorges reservoir region china 2 methodology in this study the scarce rainfall datasets were interpolated by using two commonly used imputation methods and then the comparison of imputed values and observed values was used to identify the impacts of two imputation methods on rainfall data sets these interpolated rainfall datasets were then used as a watershed model input for quantifying the choice of different imputation methods on h nps simulations the overall process of this research is shown in fig 1 2 1 description of imputation methods 2 1 1 basic principle of the multiple imputation method the multiple imputation method often assumes that the data scarcity follows the pattern of mcar or mar the core of this method is to produce a number of imputation values and form complete sample data sets olinsky et al 2003 in addition via statistical analyses of the complete data set that was generated by the multiple imputation the expected value and the variance of the data sets are ultimately obtained donders et al 2006 this method generates several imputed values for each missing value and these imputed values are combined to create a number of complete data sets this process is then repeated several times for processing each data set which leads to several results that are used to estimate the target variable and estimate its precision the above process is divided into three steps imputation analysis and merging and the analysis and merger processes are described below in the next 4 equations the key variables of the analysis and merger processes q b u and t are defined 1 q 1 m i 1 m q i where m is the number of data sets generated by multiple imputation q i is the ith interpolated data set and q is the average value of the imputation data sets 2 b 1 m 1 i 1 m q i q 2 where b is the variance within the imputation set 3 u 1 m i 1 m u i where u i is the square of the standard error of the ith interpolated set and u is the sum of variances between imputation sets i e the group variance 4 t 1 1 m b u where t is the variance estimate of the total of the imputation sets 2 1 2 commonly used imputation methods the data augmentation da algorithm is an iterative optimization and sampling algorithm method that can introduces latent variables imtiaz and shah 2008 lanza and collins et al 2005 this method is a special kind of markov chain monte carlo mcmc method and is characterized by random alternate interpolate of each block of scarce data unlike the em algorithm ho et al 2001 firstly the da algorithm assumes two tentative initial values for missing data y miss and parameter Œ∏ and then a markov chain y miss 1 Œ∏ 1 y miss 2 Œ∏ 2 can be obtained from the subsequent iterations of imputed step i step and posterior step p step to be specific according to initial y miss and Œ∏ iteration operations are repeated until this markov chain satisfies a special convergence condition which means that in the final markov chain y miss and Œ∏ are obtained a final parameter Œ∏ is extracted randomly from the posteriori distribution of the parameter set the final imputed values are obtained from the final i step according to Œ∏ the two important steps of the da algorithm can be summarized as follows in the first step i e i step a sample is randomly selected from the posteriori distribution of the missing data for the next step if the observed sample data and the previous parameters are known as y miss t 1 f y miss y obs Œ∏ t where y miss t 1 is a sample data obtained by random extraction y miss is a missing value y obs is an observation Œ∏ is the parameter used for iteration in the second step i e p step a sample is randomly selected from the posterior distribution of the parameters for the next iteration in the case of the observation sample and the previous extraction of the missing sample data the emb algorithm is Œ∏ t 1 f Œ∏ y obs y miss t 1 a combination of the em algorithm and the bootstrap method the emb algorithm is used to estimate parameters of the representative bootstrapped data sets delleji et al 2008 among them the bootstrap step is mainly to obtain bootstrapped data sets of the original incomplete samples and the em step iteratively estimates parameters and scarce data makhuvha et al 1997a when this algorithm is used two basic assumptions should be satisfied 1 the interpolated data sets need to obey the multivariate normal distribution y n p Œº if necessary by transformation where y is the data set of the n p dimensional sample and is composed of scarce data and observation data n and p are the two dimensions that form the sample data sets that is y y obs y miss 2 the response mechanism of scarce data should be mar that is f m y œï f m y obs œï where m m ij n p is the matrix of the missing data pattern in m if y ij y miss then mij 1 and if y ij y obs then mij 0 where yij is the element in the sample data set in satisfying the above two basic assumptions such that Œ∏ Œº and œï is the parameter for determining the distributions of missing data patterns then the likelihood function l Œ∏ œï y obs m f y obs m Œ∏ œï of Œ∏ and œï may be broken down as follows 5 f y obs m Œ∏ œï f y obs Œ∏ f m y obs œï in this case the maximum likelihood estimate of parameter Œ∏ depends entirely on f y obs Œ∏ œï l Œ∏ y obs f y obs Œ∏ when the prior distribution of the parameter Œ∏ is an uninformative prior distribution eq 6 is used 6 f Œ∏ y obs f y obs Œ∏ f y Œ∏ d y miss the main difficulty in the analysis of an incomplete sample data set is the acquisition of the posterior distribution f Œ∏ y obs of the parameter Œ∏ the posterior estimate of the parameters can be obtained by the em algorithm the emb algorithm is based on the em algorithm and using the bootstrap method to sample the posterior distribution f Œ∏ y obs formally the whole calculating process of emb algorithm is shown in fig 2 the first step of the emb algorithm is to obtain m sample sets of bootstrapped rainfall data and the implementation of the return of the sample is subsequently combined with bootstrap algorithm for the missing rainfall data samples the em algorithm is then used to estimate the parameters of the m sample sets of bootstrapped rainfall data the third step is to calculate the expected value of the corresponding missing rainfall data and interpolate the missing period finally all samples are statistically analysed to obtain the expected total variance of each imputed rainfall data set the overall variance estimation was used to evaluate the effects of different imputation methods and was calculated as follows 7 x 1 n i 1 n x i 8 œÉ 2 x i x 2 n 1 where xi is rainfall data after interpolation x is the mean imputation and n is the number of simulations the standard deviation was then calculated as follows 9 sd i 1 n x i x 2 n 1 2 2 description of the swat model the swat model developed by the agriculture research service of the united states department of agriculture was chosen to quantify the effects of different imputation methods on h nps predictions the swat model as a physically based and semi distributed model has been widely used to predict nps pollution loads in large scale watersheds and at the catchment scale because of the heterogeneity of watershed processes chen et al 2014a by dividing the watershed into a number of sub watersheds the swat model considers the spatial variability of rainfall and underlying surface processes after which the model divides each sub watershed into several hydrological response units hrus with the same underlying surface factors that are the same soil type have the same vegetation cover and so on the h nps calculations are quantified for each hru and then aggregated in each sub watershed to obtain a final result for more information about the swat model please refer to the reports by douglas mankin et al 2010 and chen et al 2016b whether the rainfall data of at least one precipitation station is used in the swat model depends on the thiessen polygons which rely on the overall mean when data such as rainfall maximum temperature minimum temperature daily average relative humidity daily radiation and daily average wind speed are scarce the swat model will use its own weather generator to interpolate the missing values in addition the formula for calculating daily rainfall uses the markov chain skewed distribution function of the swat model which is as follows 10 r day Œº mon 2 œÉ mon snd day g mon 6 g mon 6 1 3 1 g mon where rday is one day rainfall Œº mon is average daily rainfall in one month œÉmon is a standard deviation of Œº mon sndday is the calculated standard normal deviation of one day and gmon is the skewness coefficient of Œº mon this study evaluated the impacts of different imputation methods on model accuracy and uncertainty in this study model precision is defined as the difference between a simulated value and its measured value the sequential uncertainty fitting version 2 sufi 2 method within the swat calibration and uncertainty programs swat cup was used for calibration and validation traditional evaluation methods including correlation coefficient r2 values legates and mccabe 1999 and the nash sutcliffe coefficient nse nash and sutcliffe 1970 were used for assessing model prediction the correlation coefficient r2 was calculated as follows 11 r 2 i 1 n o i o p i p i 1 n o i o i 1 n p i p 2 2 where oi is the ith observed value pi is the ith simulated value p and q are the average of the observed and simulated values and n is the number of observed values or simulated values similarly the nash sutcliffe coefficient nse was calculated as follows 12 ens 1 i 1 n o i p i 2 i 1 n o i o 2 different rainfall data scarcity will cause changes in both simulation results and parameter ranges to some extent therefore the generalized likelihood uncertainty estimation glue method beven and binley 1992 which is based on the regionalized sensitivity analysis rsa algorithm hornberger and spear 1981 was chosen to quantify the parameter uncertainty in hydrology simulations beven and freer 2001 and model evaluations chen et al 2014b the glue method eliminates the influence between parameters by sampling and simulating a set of parameters for more information about this process please refer to our previous study shen et al 2011 the 95 confidence interval 95ci was chosen to quantify prediction uncertainty the 95ci can characterize the uncertainty of the output via the identification of the 2 5 and 97 5 threshold values of the results the coefficient of variation cv was used to quantify the magnitude of uncertainty as a supplement to the 95ci to characterize the uncertainty of the simulation results the related calculation formulae are as follows 13 cv sd x 3 case study 3 1 description of the study area and available dataset the daning river watershed is a significant tributary of the three gorges reservoir area and is located in wushan and wuxi counties in the municipality of chongqing china the daning river watershed suffers from severe nps pollution and phosphorus is the limiting nutrient causing eutrophication in the three gorges reservoir region shen et al 2015 the wuxi section of the watershed covers an area of 2422 km2 the following gis data have been collected a digital elevation model dem on a scale of 1 250 000 a land use map at a resolution of 1 100 000 and a soil type map at a resolution of 1 50 000 several attribute databases have been constructed via the collection of daily rainfall data at 13 rainfall stations from 1998 to 2008 and of other meteorological data such as daily maximum and minimum air temperature relative humidity etc daily flow data and monthly water quality data were collected for model calibration physical and chemical properties of the soil such as soil density initial phosphorus content etc as well as crop management measures were obtained from wuxi county and from field investigations and the location of the daning river watershed as well as nine precipitation gages inside it are shown in fig 3 this study analysed the observed daily rainfall data from 1998 to 2008 at 13 rainfall stations to explore the actual rainfall data scarcity situation in the daning river watershed in summary the actual data scarcity situation comprises mainly continuous scarcity and partly short term series scarcity in this research in combination with the actual data scarcity conditions in the watershed the xining precipitation data a complete observed rainfall data set was used as an experimental design object the data scarcity scenarios were divided into two categories missing rates and missing periods to quantify the impact of missing data more accurately in addition the data scarcities were designed to be in accordance with the actual data scarcity in the daning river watershed i e every scenario contains continuous and short term scarcity on the one hand in the missing rate design we designed six missing rate scenarios of 10 20 30 40 50 and 60 among them the 10 missing rate scenario omits the daily data from a large part of 2007 and short parts of 2004 and 2005 in addition the 20 missing rates scenario omits daily data from the whole part of 2007 a large part of 2003 and short parts of 2004 and 2005 the 60 max missing rate scenario omits daily data from the whole parts of 2002 2003 2006 and 2007 a large part of 2004 and short parts of 2000 2001 2005 and 2008 moreover the design scenarios involving relatively high missing rates always encompass the design scenarios of the relatively lower missing rates on the other hand for the missing period designs we designed five missing periods of patterns 1 5 that cover high normal and low flow years rainfall from 1998 to 2008 ranged from 210 to 1582 mm and the precipitation in 2002 was 841 mm this rainfall value is the centre of the range so 2002 was defined as a normal flow year in this study similarly 2000 2005 and 2007 were defined as high flow years and 2004 was defined as a low flow year specifically patterns 1 4 and 5 contain data scarcity for 2000 2005 and 2007 respectively which are high flow years pattern 2 contains data scarcity for 2002 which is a normal flow year and pattern 3 contains data scarcity for 2004 which is a low flow year more detailed information regarding the data scarcity designs can be found in the work by chen et al 2018b 3 2 impacts of different imputation methods on rainfall data series the da algorithm and the emb algorithm were used to interpolate censored rainfall data with six missing rate scenarios and the overall variance estimation was used to evaluate the imputation results and in this paper variance represents the numerical interval of rainfall data series to reflect the degree of dispersion of the data sequence the imputation performances are shown in fig 4 and table 1 it could be observed that with the increase in scarcity rate from 0 to 60 the total variance estimation of the imputation sets gradually decreased for both the da and emb algorithms on the whole the variance in the da algorithm and emb algorithm decreased from 106 50 to 58 31 and from 106 50 to 90 67 respectively however the variance in the emb algorithm was not less than 86 00 and was fitted better to the observed value than the da algorithm when the missing rates increased from 0 to 10 the da algorithm decreased significantly and the subsequent performance of the da algorithm stayed relatively stable indicating that there exists a threshold effect of data scarcity associated with the da method in contrast the performance of emb algorithm exhibits a gentle reduction from the 0 to 60 missing rates even when the missing rate increased to the upper limit of 60 the emb algorithm could still maintain a relatively good imputation effect whereas the variance estimation of the da algorithm dropped to less than 59 00 therefore the data set that was interpolated by the emb algorithm was far better than that interpolated by the da algorithm and was closer to the observed value this advance is more obvious when the missing rate exceeded 30 and the imputation effect of the emb remained essentially unchanged to evaluate the credibility of the data set obtained by the emb algorithm specifically the probability density curve and the over imputation method were used the relative density curve and results of the over imputation analysis of the data set interpolated by the emb algorithm with different missing rates are shown in fig a 1 in addition the relative density curve and the results of the over imputation analysis with different missing periods are shown in fig a 2 the relative density curve on the left side of fig a 1 reflects the relationship between the distribution of the mean imputations and the distribution of the observed values it is clear that the distribution of imputations is consistent with the distribution of observations for the 10 60 missing rates the goodness of fit between the imputed vales and the observed values indicates that the actual distribution of the observed values is well reproduced via the imputation process however when the relative density of observations exceeded 1 5 the relative density of the imputations could be below 0 5 that is the imputed value is always lower than the peak observed value indicating that the rare peak value is difficult to capture with the emb algorithm this phenomenon might result in prediction errors in extreme rainfall simulations coincidentally it can be seen from fig a 2 that the distribution of imputation values is consistent with the distribution of observations in the case of different missing periods in addition this finding suggests that the emb imputation method can be applied to the imputation of missing data during high flow periods normal flow periods or low flow periods this fact should be considered an important advantage of the emb algorithm the right side of fig a 2 shows the results of the over imputation analysis of rainfall data for the study area the over imputation analysis means that each observed data of the data sets will be omitted and become a missing value and then this missing value will be imputed by multiple imputation approach afterward the 90 confidence level and the mean imputations could be obtained if the imputed values can fully follow the mar and normal distribution assumption all the scattered points in the figure will fall on the diagonal line that passes through the origin with a slope of 1 00 it is generally believed that the result is considered satisfactory as long as the 90 confidence interval level is able to cover this line is that at least 90 of the confidence interval the vertical lines intersects with the diagonal lines this method is an evaluation method for whether the imputed values can effectively replace the true values the right side of figs a 1 and a 2 shows that the 90 confidence interval of the imputed values can cover this theoretical line both at different missing rates and at different missing periods this result suggests that the emb algorithm can effectively solve the scarcity problem regardless of the different missing rates and different missing periods of rainfall data furthermore this result may be due to the simple calculation and stable convergence of the em algorithm as well as the combination of the bootstrap method which has high accuracy than the sample variance and assumption of normality chen et al 2016b 3 3 impacts of different data scarcity on model uncertainty the daily rainfall data with 10 60 missing rates were used for the uncertainty analysis and the results for different data scarcity are shown in fig 5 for the swat model output the 95 confidence interval 95ci was used for the uncertainty analysis the uncertainties of flow and total phosphorus tp were slightly different as shown in fig 5 a censored parts of the observed values all fall outside the 95ci of the simulated flow results in particular the scarce data during the high flow period caused large deviations in the simulation of the peak values of flow when the missing rate increased from 10 to 60 the 95ci of x increased from 0 79 151 47 to 0 19 164 03 and the sd and cv increased from 0 16 11 70 to 0 14 15 33 and from 0 03 1 06 to 0 05 3 35 respectively this result indicates that more missing rainfall data would result in greater flow prediction uncertainty as shown in fig 5 b the impact of rainfall data scarcity on tp prediction is similar to the flow prediction while the data scarcity would cause larger uncertainty in the tp output during the low flow period when the missing rate increased from 10 to 60 the interval of the cv increased from 0 04 0 64 to 0 34 2 32 presenting an increasingly larger change trend however the rainfall data scarcity had little effect on the peak values of tp simulation no matter what missing rates were considered with the increase in missing data the intervals of x and sd increased from 145 67 23692 06 to 24 36 19020 90 and from 36 44 9172 50 to 53 82 6551 84 respectively 3 4 impacts of different imputation data sets on nps pollution simulations to verify that the imputed data set could be applied to the nps pollution simulation we used the interpolated data set as input data and ran the swat model the simulation results are shown in figs 6 and 7 it can be seen from fig 6 that the estimated effect of the imputed data set of the flow and the tp loads with different missing rates improved moreover it can be seen from fig 6 that the simulated values obtained by the imputation are slightly lower than the measured values the gap was not significant if the missing rate was less than 30 that is the simulation results were satisfactory when the rainfall data were interpolated by the emb algorithm within a specific missing rate threshold it can also be seen from fig 7 that the simulated values in the normal flow years and the low flow years are closer to the baseline values than are those in the high flow years this finding is mainly because the overall mean of the rainfall data obtained by the emb algorithm is lower than the observed values this phenomenon resulted in a poor estimated effect of the peak and maximum values but a satisfactory overall effect could be reached the simulation results of flow and tp load before and after imputation in the cases of different missing rates and different missing periods are shown in tables 2 and 3 as shown in table 2 if the missing rates increased from 10 to 60 the nse values of the flow decreased from 0 69 to 0 56 after imputation and the nse values of tp decreased from 0 86 to 0 80 before imputation the nse values for the simulated flows and tp decreased from 0 61 to 0 55 and from 0 87 to 0 75 respectively indicating a better model performance if the imputed rainfall data were used after imputation the r2 values were approximately 0 70 and 0 90 for the simulated flows and tp respectively while the nse values were 0 60 and 0 80 respectively these small changes in nse and r2 values after imputation indicate that the imputation effect of rainfall data is less affected by the changes in missing rates coincidentally as shown in table 3 the model performance of the imputed values in different missing periods are also better than the simulation results before imputation specifically the nse values in normal flow years and low flow years patterns 2 and 3 both were 0 74 for the simulated flow and the nse values were 0 93 and 0 94 for the simulated tp in normal flow years and low flow years respectively however in normal flow years and low flow years the nse values were 0 73 and 0 92 for the simulated flow and tp before interpolation respectively in addition the simulation results of pattern 3 are very close to the baseline simulation results even if the maximum value of tp is missing in pattern 4 the model performance after the imputation is still satisfactory in addition this finding suggests that the emb imputation method is better than swat s weather generator this finding could be explained by the weather generator being susceptible to extreme values and by the adjacent values of the scarce data however the emb algorithm combines the advantages of the em algorithm and the multiple imputation method so this method could produce stable and relatively accurate interpolation data sets thus this paper suggests this new imputation method could be added to h nps models to obtain satisfactory results during data scarcity conditions 3 5 implications many studies have focused on the use of the emb algorithm in generating missing data and these studies have also provided evidence that this algorithm performs better than other traditional approaches for example honaker et al 2011 proposed that the emb algorithm is superior to various mcmc approaches for scarce data this proposal is consistent with our findings in which the emb algorithm is superior to the da algorithm and the weather generator incorporated into the swat in the study of missing value imputing the single imputation is often considered the gold standard takahashi 2017a instead the multiple imputation method includes all the advantages of the single imputation and solves the problem of the underestimated standard error caused by single imputation from this paper it is believed that the results of the emb algorithm are better than those of the other multiple imputation methods e g the da algorithm and other traditional imputation methods e g the weather generator for different missing rates and missing periods takahashi 2017b and ghapor et al 2017 also suggested that the emb algorithm produces satisfactory results regarding scarce data estimation because the emb algorithm has the advantages of the em algorithm as well as the multiple imputation method thus the emb algorithm can produce stable and highly accurate interpolation data sets due to the stable convergence of the em step lin 2010 and highly accurate results of the bootstrap step zhang et al 2017 in summary this paper suggests improving the ability of h nps models to estimate scarce data via the introduction of new imputation methods i e the use of emb algorithms instead of the weather generator to improve the scarce data processing capabilities of the swat on the other hand it should be noted that no single algorithm could solve all data scarcity problems for example it could be seen that the emb algorithm showed the worst imputation effect for peak rainfall values in addition the data scarcity during the low flow period would result in larger model prediction uncertainty and this effect would be transferred from the flow prediction to the tp simulation thus even with the best algorithm used more accurate rainfall monitoring data during extreme conditions such as high flow and low flow periods is also necessary it should be considered whether a modification of the em algorithm such as the smallest deviation the most accurate and by far the fastest pseudo em algorithm makhuvha et al 1997b can be used to impute peak rainfall values and according to pegram 1997 it can also be considered as a pre processing method for imputed data that uses the covariance biplot for preliminary repair of monthly scarce data additional studies are needed that consider the combination of data from multiple sources such as radar remote sensing precipitation monitoring and so on lakshmi 2004 li et al 2012 4 conclusion in this study the impacts of imputation methods on rainfall data were quantified under different data scarcity conditions and the effects of different rainfall scarcity scenarios on model uncertainty have been analysed after the investigation of rainfall data series and model performance the response of the output results of h nps simulation to the different imputed data sets was evaluated via traditional evaluation indicators e g nse and r2 the results indicated that the more scarce rainfall data would result in poorer model performance and larger prediction uncertainty and that this impact would be amplified from flow predictions to the h nps simulations in addition the rainfall data scarcity would cause larger h nps prediction uncertainty during the low flow periods compared to high and normal flow periods specifically the minimum values of total phosphorus and flow during low flow periods are more responsive to data scarcity and the maximum values of flow during high flow periods are more susceptible compared to the traditional da and weather generator methods the emb algorithm shows better performance in imputing rainfall data and h nps predictions regardless of missing rates and missing periods this advantage of the emb algorithm would be clearer if the specific threshold of data scarcity is reached which highlights the incorporation of the emb into the h nps models because emb can adapt well to the variability of rainfall data however this paper also noted that even if the best algorithm is used the imputed value is always lower than the large peak observations which highlights the importance of extreme rainfall monitoring during high flow periods this paper reports important implications for both the choice of imputation methods and the use of h nps models and provides an optimal scheme for solving data scarcity problems in watershed studies acknowledgements this research was funded by the national natural science foundation of china nos 51579011 and 51779010 the newton fund grant ref bb n013484 1 key laboratory of nonpoint source pollution control ministry of agriculture p r china 1610132016005 and the interdiscipline research funds of beijing normal university declaration of interest statement the authors declare that there are no conflicts of interest regarding this manuscript appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j jhydrol 2019 03 025 appendix a supplementary data the following are the supplementary data to this article supplementary data 1 
6602,rainfall data scarcity has caused enormous problems in hydrologic and non point pollution h nps predictions as rainfall data represent the key input to watershed models in this study the effects of different imputation methods such as the data augmentation da and the expectation maximization with bootstrap emb algorithms on rainfall data scarcity were compared the effects of different data scarcity rates and periods on model performance and prediction uncertainty were then quantified finally the effects of different imputed data sets on h nps results were evaluated with the soil and water assessment tool swat a real case study in the daning river watershed three gorges reservoir region china was evaluated the results indicated that rainfall data scarcity during low flow periods would result in poorer model performance and larger prediction uncertainty especially to some minimum values and the time when the maximum values are more susceptible is the rainfall data scarcity during high flow periods the repair of rainfall data and the h nps model performance obtained by the emb algorithm are superior to the traditional da and weather generator performances this advantage of the emb algorithm would be more definitive if a specific threshold of data scarcity is reached it is noted that even if the best algorithm is used the imputed value is always lower than the peak observed value this paper reports important implications for the choice of imputation methods and the use of h nps models for solving data scarcity problems for watershed studies keywords emb algorithm multiple imputation methods non point source pollution scarce data swat 1 introduction one of the major concerns facing hydrological researchers can be characterized by variability in rainfall and frequent rainstorms worldwide castillo et al 2003 marzen et al 2017 ringham et al 2016 while continuous rainfall data input certainly represents the indispensable part of hydrology and water quality studies typically rainfall data are obtained from ground stations and remote sensing systems lakshmi 2004 at the same time rainfall stations often serve as the basic source of long term rainfall data kashani et al 2016 however even if the best equipment is used researchers often face data scarcity dilemmas due to electromagnetic lightning pulses and reduced performance of the instrument as well as the effects of human error this data scarcity dilemma can cause enormous problems in obtaining continuous rainfall data series and impact hydrological and water quality analyses which are indeed important for water resource management ram√≠rez et al 2005 junninen et al 2004 during the last few decades hydrologists and mathematicians have worked hard to explore the statistical characteristics of rainfall data sets and have tried to generate specific interpolating approaches to solve the scarcity problem several hypotheses including data scarcity mechanisms have been proposed by dividing the typical data scarcity into the three categories 1 missing completely at random mcar 2 missing at random mar and 3 missing not at random mnar little and rubin 1987 specifically recent model based methods such as maximum likelihood procedures and multiple imputations often assume that the data scarcity follows the mcar or the mar patterns so that specific data distribution could be assumed kalteh and hjorth 2009 according to the number of substitute values for each missing value the method of imputation can be divided into two categories single imputation and multiple imputation acock 2005 single imputation means that only one reasonable substitute value is constructed for each missing value in a certain way and interpolates it to the location of the original scarce data to construct a complete data set a general multiple imputation method for data scarcity was introduced by dempster et al 1977 in their unique research on the expectation maximization em algorithm the expectation maximization algorithm can take into account the pattern of scarce data to further estimate impute the statistical values that will be calculated in order to complete the data set the core of this imputation method is to interpolate missing values from their adjacent data points produce a couple of imputation values and then ultimately constructs complete sample data sets dempster et al 1977 the use of multiple imputations represents an effective method to address scarce data and has been used in various fields nieh et al 2014 evaluated a number of methods such as multiple imputations data deletion and mean interpolation for testing their interpolation effects of missing microbial data the results showed that the multiple imputation method resulted in the least bias compared to the other methods because it can obtain a better parameter estimate via several times imputations previous studies have also indicated that the em algorithm was more preferred than linear regression for imputation of monthly meteorological data sahi n and ci gi zoglu 2010 several studies have also been conducted to compare these imputation methods for example one study compared the effects of linear and non linear techniques on the interpolation of rainfall data and found that the hybrid approach showed the best performance mir√≥ et al 2017 however there are some problems with these methods for solving data scarcity such as the inability to calculate the standard error of parameters and the amount of calculation is too large to solve the problem of previous methods the expectation maximization with bootstrap emb algorithm was developed for data scarcity conditions by combining the advantages of both the multiple imputation and em algorithm approaches however few studies have applied this new method to the interpolation of rainfall data scarcity and as far as we know no study has focused on the comparison of the emb algorithm with other traditional imputation methods which is valuable and we believe is the main message of this paper non point source nps pollution has attracted global attention recently because of its threat to water quality liu et al 2015 chen et al 2016a and watershed models such as the topgraphy based hydrological model topmodel beven and kirkby 1979 the hydrological simulation program fortran hspf model which is developed by u s environmental protection agency in 1981 bicknell et al 2005 and the soil and water assessment tool swat developed by jeff arnold in 1994 arnold and srinivasan et al 1998 which has been used for hydrological and nps h nps analyses liu et al 2017 ouyang et al 2017 among these approaches the swat model has been widely used due to its good function and open source code and the swat has been used to predict h nps cycles under different climates and underlying surface characteristics as rainfall acts as the driving factor of runoff and nps processes rainfall data often serve as the core input and are crucial for these h nps models hrachowitz and clark 2017 for dealing with these scarce rainfall data weather generator tools have been incorporated into models and six meteorological variables including rainfall highest temperature lowest temperature daily average relative humidity daily radiation and daily average wind speed can be interpolated from the available high and low values by the weather generator kilsby et al 2007 several studies have focused on the impacts of rainfall data scarcity on h nps simulations and have reported that the weather generator produces a great deal of deviation because of the extreme values chen et al 2017 coulibaly and evora 2007 studies have also explored the transfer of the spatial distribution of rainfall data to h nps simulations via the quantification of changes in goodness of fit indicators during model calibration chaubey et al 1999 cho et al 2009 shen et al 2012 although the effect of rainfall input on h nps modelling is a well studied field few studies have focused on the effects of rainfall data scarcity on h nps modelling and have failed to compare the impacts of different imputation methods on h nps outputs furthermore the uncertainty is currently considered one of the core dilemmas in h nps studies due to human beings understanding limitations about the complex water environment it is necessary to study the impact of input data on the model uncertainty as it can not only improve the reliability of simulation results but also reduce human errors chen et al 2018a most researches focus on assessing the uncertainty of complex water quality models and its influence on the allowable pollutant load via improving bayesian analytical approach liang et al 2016 jia et al 2018 and few studies have quantified the impacts of rainfall data temporal scarcity nor have compared different imputation methods to reduce the model uncertainty caused by rainfall data scarcity this study focuses on the impacts of imputation methods on rainfall input data and then on h nps simulations the objectives of this study were to 1 compare the effects of different imputation methods on rainfall data scarcity 2 investigate the effects of data scarcity on model uncertainty and 3 evaluate and analyse the effects of the different imputed data sets on h nps results the case study was performed in the daning river watershed three gorges reservoir region china 2 methodology in this study the scarce rainfall datasets were interpolated by using two commonly used imputation methods and then the comparison of imputed values and observed values was used to identify the impacts of two imputation methods on rainfall data sets these interpolated rainfall datasets were then used as a watershed model input for quantifying the choice of different imputation methods on h nps simulations the overall process of this research is shown in fig 1 2 1 description of imputation methods 2 1 1 basic principle of the multiple imputation method the multiple imputation method often assumes that the data scarcity follows the pattern of mcar or mar the core of this method is to produce a number of imputation values and form complete sample data sets olinsky et al 2003 in addition via statistical analyses of the complete data set that was generated by the multiple imputation the expected value and the variance of the data sets are ultimately obtained donders et al 2006 this method generates several imputed values for each missing value and these imputed values are combined to create a number of complete data sets this process is then repeated several times for processing each data set which leads to several results that are used to estimate the target variable and estimate its precision the above process is divided into three steps imputation analysis and merging and the analysis and merger processes are described below in the next 4 equations the key variables of the analysis and merger processes q b u and t are defined 1 q 1 m i 1 m q i where m is the number of data sets generated by multiple imputation q i is the ith interpolated data set and q is the average value of the imputation data sets 2 b 1 m 1 i 1 m q i q 2 where b is the variance within the imputation set 3 u 1 m i 1 m u i where u i is the square of the standard error of the ith interpolated set and u is the sum of variances between imputation sets i e the group variance 4 t 1 1 m b u where t is the variance estimate of the total of the imputation sets 2 1 2 commonly used imputation methods the data augmentation da algorithm is an iterative optimization and sampling algorithm method that can introduces latent variables imtiaz and shah 2008 lanza and collins et al 2005 this method is a special kind of markov chain monte carlo mcmc method and is characterized by random alternate interpolate of each block of scarce data unlike the em algorithm ho et al 2001 firstly the da algorithm assumes two tentative initial values for missing data y miss and parameter Œ∏ and then a markov chain y miss 1 Œ∏ 1 y miss 2 Œ∏ 2 can be obtained from the subsequent iterations of imputed step i step and posterior step p step to be specific according to initial y miss and Œ∏ iteration operations are repeated until this markov chain satisfies a special convergence condition which means that in the final markov chain y miss and Œ∏ are obtained a final parameter Œ∏ is extracted randomly from the posteriori distribution of the parameter set the final imputed values are obtained from the final i step according to Œ∏ the two important steps of the da algorithm can be summarized as follows in the first step i e i step a sample is randomly selected from the posteriori distribution of the missing data for the next step if the observed sample data and the previous parameters are known as y miss t 1 f y miss y obs Œ∏ t where y miss t 1 is a sample data obtained by random extraction y miss is a missing value y obs is an observation Œ∏ is the parameter used for iteration in the second step i e p step a sample is randomly selected from the posterior distribution of the parameters for the next iteration in the case of the observation sample and the previous extraction of the missing sample data the emb algorithm is Œ∏ t 1 f Œ∏ y obs y miss t 1 a combination of the em algorithm and the bootstrap method the emb algorithm is used to estimate parameters of the representative bootstrapped data sets delleji et al 2008 among them the bootstrap step is mainly to obtain bootstrapped data sets of the original incomplete samples and the em step iteratively estimates parameters and scarce data makhuvha et al 1997a when this algorithm is used two basic assumptions should be satisfied 1 the interpolated data sets need to obey the multivariate normal distribution y n p Œº if necessary by transformation where y is the data set of the n p dimensional sample and is composed of scarce data and observation data n and p are the two dimensions that form the sample data sets that is y y obs y miss 2 the response mechanism of scarce data should be mar that is f m y œï f m y obs œï where m m ij n p is the matrix of the missing data pattern in m if y ij y miss then mij 1 and if y ij y obs then mij 0 where yij is the element in the sample data set in satisfying the above two basic assumptions such that Œ∏ Œº and œï is the parameter for determining the distributions of missing data patterns then the likelihood function l Œ∏ œï y obs m f y obs m Œ∏ œï of Œ∏ and œï may be broken down as follows 5 f y obs m Œ∏ œï f y obs Œ∏ f m y obs œï in this case the maximum likelihood estimate of parameter Œ∏ depends entirely on f y obs Œ∏ œï l Œ∏ y obs f y obs Œ∏ when the prior distribution of the parameter Œ∏ is an uninformative prior distribution eq 6 is used 6 f Œ∏ y obs f y obs Œ∏ f y Œ∏ d y miss the main difficulty in the analysis of an incomplete sample data set is the acquisition of the posterior distribution f Œ∏ y obs of the parameter Œ∏ the posterior estimate of the parameters can be obtained by the em algorithm the emb algorithm is based on the em algorithm and using the bootstrap method to sample the posterior distribution f Œ∏ y obs formally the whole calculating process of emb algorithm is shown in fig 2 the first step of the emb algorithm is to obtain m sample sets of bootstrapped rainfall data and the implementation of the return of the sample is subsequently combined with bootstrap algorithm for the missing rainfall data samples the em algorithm is then used to estimate the parameters of the m sample sets of bootstrapped rainfall data the third step is to calculate the expected value of the corresponding missing rainfall data and interpolate the missing period finally all samples are statistically analysed to obtain the expected total variance of each imputed rainfall data set the overall variance estimation was used to evaluate the effects of different imputation methods and was calculated as follows 7 x 1 n i 1 n x i 8 œÉ 2 x i x 2 n 1 where xi is rainfall data after interpolation x is the mean imputation and n is the number of simulations the standard deviation was then calculated as follows 9 sd i 1 n x i x 2 n 1 2 2 description of the swat model the swat model developed by the agriculture research service of the united states department of agriculture was chosen to quantify the effects of different imputation methods on h nps predictions the swat model as a physically based and semi distributed model has been widely used to predict nps pollution loads in large scale watersheds and at the catchment scale because of the heterogeneity of watershed processes chen et al 2014a by dividing the watershed into a number of sub watersheds the swat model considers the spatial variability of rainfall and underlying surface processes after which the model divides each sub watershed into several hydrological response units hrus with the same underlying surface factors that are the same soil type have the same vegetation cover and so on the h nps calculations are quantified for each hru and then aggregated in each sub watershed to obtain a final result for more information about the swat model please refer to the reports by douglas mankin et al 2010 and chen et al 2016b whether the rainfall data of at least one precipitation station is used in the swat model depends on the thiessen polygons which rely on the overall mean when data such as rainfall maximum temperature minimum temperature daily average relative humidity daily radiation and daily average wind speed are scarce the swat model will use its own weather generator to interpolate the missing values in addition the formula for calculating daily rainfall uses the markov chain skewed distribution function of the swat model which is as follows 10 r day Œº mon 2 œÉ mon snd day g mon 6 g mon 6 1 3 1 g mon where rday is one day rainfall Œº mon is average daily rainfall in one month œÉmon is a standard deviation of Œº mon sndday is the calculated standard normal deviation of one day and gmon is the skewness coefficient of Œº mon this study evaluated the impacts of different imputation methods on model accuracy and uncertainty in this study model precision is defined as the difference between a simulated value and its measured value the sequential uncertainty fitting version 2 sufi 2 method within the swat calibration and uncertainty programs swat cup was used for calibration and validation traditional evaluation methods including correlation coefficient r2 values legates and mccabe 1999 and the nash sutcliffe coefficient nse nash and sutcliffe 1970 were used for assessing model prediction the correlation coefficient r2 was calculated as follows 11 r 2 i 1 n o i o p i p i 1 n o i o i 1 n p i p 2 2 where oi is the ith observed value pi is the ith simulated value p and q are the average of the observed and simulated values and n is the number of observed values or simulated values similarly the nash sutcliffe coefficient nse was calculated as follows 12 ens 1 i 1 n o i p i 2 i 1 n o i o 2 different rainfall data scarcity will cause changes in both simulation results and parameter ranges to some extent therefore the generalized likelihood uncertainty estimation glue method beven and binley 1992 which is based on the regionalized sensitivity analysis rsa algorithm hornberger and spear 1981 was chosen to quantify the parameter uncertainty in hydrology simulations beven and freer 2001 and model evaluations chen et al 2014b the glue method eliminates the influence between parameters by sampling and simulating a set of parameters for more information about this process please refer to our previous study shen et al 2011 the 95 confidence interval 95ci was chosen to quantify prediction uncertainty the 95ci can characterize the uncertainty of the output via the identification of the 2 5 and 97 5 threshold values of the results the coefficient of variation cv was used to quantify the magnitude of uncertainty as a supplement to the 95ci to characterize the uncertainty of the simulation results the related calculation formulae are as follows 13 cv sd x 3 case study 3 1 description of the study area and available dataset the daning river watershed is a significant tributary of the three gorges reservoir area and is located in wushan and wuxi counties in the municipality of chongqing china the daning river watershed suffers from severe nps pollution and phosphorus is the limiting nutrient causing eutrophication in the three gorges reservoir region shen et al 2015 the wuxi section of the watershed covers an area of 2422 km2 the following gis data have been collected a digital elevation model dem on a scale of 1 250 000 a land use map at a resolution of 1 100 000 and a soil type map at a resolution of 1 50 000 several attribute databases have been constructed via the collection of daily rainfall data at 13 rainfall stations from 1998 to 2008 and of other meteorological data such as daily maximum and minimum air temperature relative humidity etc daily flow data and monthly water quality data were collected for model calibration physical and chemical properties of the soil such as soil density initial phosphorus content etc as well as crop management measures were obtained from wuxi county and from field investigations and the location of the daning river watershed as well as nine precipitation gages inside it are shown in fig 3 this study analysed the observed daily rainfall data from 1998 to 2008 at 13 rainfall stations to explore the actual rainfall data scarcity situation in the daning river watershed in summary the actual data scarcity situation comprises mainly continuous scarcity and partly short term series scarcity in this research in combination with the actual data scarcity conditions in the watershed the xining precipitation data a complete observed rainfall data set was used as an experimental design object the data scarcity scenarios were divided into two categories missing rates and missing periods to quantify the impact of missing data more accurately in addition the data scarcities were designed to be in accordance with the actual data scarcity in the daning river watershed i e every scenario contains continuous and short term scarcity on the one hand in the missing rate design we designed six missing rate scenarios of 10 20 30 40 50 and 60 among them the 10 missing rate scenario omits the daily data from a large part of 2007 and short parts of 2004 and 2005 in addition the 20 missing rates scenario omits daily data from the whole part of 2007 a large part of 2003 and short parts of 2004 and 2005 the 60 max missing rate scenario omits daily data from the whole parts of 2002 2003 2006 and 2007 a large part of 2004 and short parts of 2000 2001 2005 and 2008 moreover the design scenarios involving relatively high missing rates always encompass the design scenarios of the relatively lower missing rates on the other hand for the missing period designs we designed five missing periods of patterns 1 5 that cover high normal and low flow years rainfall from 1998 to 2008 ranged from 210 to 1582 mm and the precipitation in 2002 was 841 mm this rainfall value is the centre of the range so 2002 was defined as a normal flow year in this study similarly 2000 2005 and 2007 were defined as high flow years and 2004 was defined as a low flow year specifically patterns 1 4 and 5 contain data scarcity for 2000 2005 and 2007 respectively which are high flow years pattern 2 contains data scarcity for 2002 which is a normal flow year and pattern 3 contains data scarcity for 2004 which is a low flow year more detailed information regarding the data scarcity designs can be found in the work by chen et al 2018b 3 2 impacts of different imputation methods on rainfall data series the da algorithm and the emb algorithm were used to interpolate censored rainfall data with six missing rate scenarios and the overall variance estimation was used to evaluate the imputation results and in this paper variance represents the numerical interval of rainfall data series to reflect the degree of dispersion of the data sequence the imputation performances are shown in fig 4 and table 1 it could be observed that with the increase in scarcity rate from 0 to 60 the total variance estimation of the imputation sets gradually decreased for both the da and emb algorithms on the whole the variance in the da algorithm and emb algorithm decreased from 106 50 to 58 31 and from 106 50 to 90 67 respectively however the variance in the emb algorithm was not less than 86 00 and was fitted better to the observed value than the da algorithm when the missing rates increased from 0 to 10 the da algorithm decreased significantly and the subsequent performance of the da algorithm stayed relatively stable indicating that there exists a threshold effect of data scarcity associated with the da method in contrast the performance of emb algorithm exhibits a gentle reduction from the 0 to 60 missing rates even when the missing rate increased to the upper limit of 60 the emb algorithm could still maintain a relatively good imputation effect whereas the variance estimation of the da algorithm dropped to less than 59 00 therefore the data set that was interpolated by the emb algorithm was far better than that interpolated by the da algorithm and was closer to the observed value this advance is more obvious when the missing rate exceeded 30 and the imputation effect of the emb remained essentially unchanged to evaluate the credibility of the data set obtained by the emb algorithm specifically the probability density curve and the over imputation method were used the relative density curve and results of the over imputation analysis of the data set interpolated by the emb algorithm with different missing rates are shown in fig a 1 in addition the relative density curve and the results of the over imputation analysis with different missing periods are shown in fig a 2 the relative density curve on the left side of fig a 1 reflects the relationship between the distribution of the mean imputations and the distribution of the observed values it is clear that the distribution of imputations is consistent with the distribution of observations for the 10 60 missing rates the goodness of fit between the imputed vales and the observed values indicates that the actual distribution of the observed values is well reproduced via the imputation process however when the relative density of observations exceeded 1 5 the relative density of the imputations could be below 0 5 that is the imputed value is always lower than the peak observed value indicating that the rare peak value is difficult to capture with the emb algorithm this phenomenon might result in prediction errors in extreme rainfall simulations coincidentally it can be seen from fig a 2 that the distribution of imputation values is consistent with the distribution of observations in the case of different missing periods in addition this finding suggests that the emb imputation method can be applied to the imputation of missing data during high flow periods normal flow periods or low flow periods this fact should be considered an important advantage of the emb algorithm the right side of fig a 2 shows the results of the over imputation analysis of rainfall data for the study area the over imputation analysis means that each observed data of the data sets will be omitted and become a missing value and then this missing value will be imputed by multiple imputation approach afterward the 90 confidence level and the mean imputations could be obtained if the imputed values can fully follow the mar and normal distribution assumption all the scattered points in the figure will fall on the diagonal line that passes through the origin with a slope of 1 00 it is generally believed that the result is considered satisfactory as long as the 90 confidence interval level is able to cover this line is that at least 90 of the confidence interval the vertical lines intersects with the diagonal lines this method is an evaluation method for whether the imputed values can effectively replace the true values the right side of figs a 1 and a 2 shows that the 90 confidence interval of the imputed values can cover this theoretical line both at different missing rates and at different missing periods this result suggests that the emb algorithm can effectively solve the scarcity problem regardless of the different missing rates and different missing periods of rainfall data furthermore this result may be due to the simple calculation and stable convergence of the em algorithm as well as the combination of the bootstrap method which has high accuracy than the sample variance and assumption of normality chen et al 2016b 3 3 impacts of different data scarcity on model uncertainty the daily rainfall data with 10 60 missing rates were used for the uncertainty analysis and the results for different data scarcity are shown in fig 5 for the swat model output the 95 confidence interval 95ci was used for the uncertainty analysis the uncertainties of flow and total phosphorus tp were slightly different as shown in fig 5 a censored parts of the observed values all fall outside the 95ci of the simulated flow results in particular the scarce data during the high flow period caused large deviations in the simulation of the peak values of flow when the missing rate increased from 10 to 60 the 95ci of x increased from 0 79 151 47 to 0 19 164 03 and the sd and cv increased from 0 16 11 70 to 0 14 15 33 and from 0 03 1 06 to 0 05 3 35 respectively this result indicates that more missing rainfall data would result in greater flow prediction uncertainty as shown in fig 5 b the impact of rainfall data scarcity on tp prediction is similar to the flow prediction while the data scarcity would cause larger uncertainty in the tp output during the low flow period when the missing rate increased from 10 to 60 the interval of the cv increased from 0 04 0 64 to 0 34 2 32 presenting an increasingly larger change trend however the rainfall data scarcity had little effect on the peak values of tp simulation no matter what missing rates were considered with the increase in missing data the intervals of x and sd increased from 145 67 23692 06 to 24 36 19020 90 and from 36 44 9172 50 to 53 82 6551 84 respectively 3 4 impacts of different imputation data sets on nps pollution simulations to verify that the imputed data set could be applied to the nps pollution simulation we used the interpolated data set as input data and ran the swat model the simulation results are shown in figs 6 and 7 it can be seen from fig 6 that the estimated effect of the imputed data set of the flow and the tp loads with different missing rates improved moreover it can be seen from fig 6 that the simulated values obtained by the imputation are slightly lower than the measured values the gap was not significant if the missing rate was less than 30 that is the simulation results were satisfactory when the rainfall data were interpolated by the emb algorithm within a specific missing rate threshold it can also be seen from fig 7 that the simulated values in the normal flow years and the low flow years are closer to the baseline values than are those in the high flow years this finding is mainly because the overall mean of the rainfall data obtained by the emb algorithm is lower than the observed values this phenomenon resulted in a poor estimated effect of the peak and maximum values but a satisfactory overall effect could be reached the simulation results of flow and tp load before and after imputation in the cases of different missing rates and different missing periods are shown in tables 2 and 3 as shown in table 2 if the missing rates increased from 10 to 60 the nse values of the flow decreased from 0 69 to 0 56 after imputation and the nse values of tp decreased from 0 86 to 0 80 before imputation the nse values for the simulated flows and tp decreased from 0 61 to 0 55 and from 0 87 to 0 75 respectively indicating a better model performance if the imputed rainfall data were used after imputation the r2 values were approximately 0 70 and 0 90 for the simulated flows and tp respectively while the nse values were 0 60 and 0 80 respectively these small changes in nse and r2 values after imputation indicate that the imputation effect of rainfall data is less affected by the changes in missing rates coincidentally as shown in table 3 the model performance of the imputed values in different missing periods are also better than the simulation results before imputation specifically the nse values in normal flow years and low flow years patterns 2 and 3 both were 0 74 for the simulated flow and the nse values were 0 93 and 0 94 for the simulated tp in normal flow years and low flow years respectively however in normal flow years and low flow years the nse values were 0 73 and 0 92 for the simulated flow and tp before interpolation respectively in addition the simulation results of pattern 3 are very close to the baseline simulation results even if the maximum value of tp is missing in pattern 4 the model performance after the imputation is still satisfactory in addition this finding suggests that the emb imputation method is better than swat s weather generator this finding could be explained by the weather generator being susceptible to extreme values and by the adjacent values of the scarce data however the emb algorithm combines the advantages of the em algorithm and the multiple imputation method so this method could produce stable and relatively accurate interpolation data sets thus this paper suggests this new imputation method could be added to h nps models to obtain satisfactory results during data scarcity conditions 3 5 implications many studies have focused on the use of the emb algorithm in generating missing data and these studies have also provided evidence that this algorithm performs better than other traditional approaches for example honaker et al 2011 proposed that the emb algorithm is superior to various mcmc approaches for scarce data this proposal is consistent with our findings in which the emb algorithm is superior to the da algorithm and the weather generator incorporated into the swat in the study of missing value imputing the single imputation is often considered the gold standard takahashi 2017a instead the multiple imputation method includes all the advantages of the single imputation and solves the problem of the underestimated standard error caused by single imputation from this paper it is believed that the results of the emb algorithm are better than those of the other multiple imputation methods e g the da algorithm and other traditional imputation methods e g the weather generator for different missing rates and missing periods takahashi 2017b and ghapor et al 2017 also suggested that the emb algorithm produces satisfactory results regarding scarce data estimation because the emb algorithm has the advantages of the em algorithm as well as the multiple imputation method thus the emb algorithm can produce stable and highly accurate interpolation data sets due to the stable convergence of the em step lin 2010 and highly accurate results of the bootstrap step zhang et al 2017 in summary this paper suggests improving the ability of h nps models to estimate scarce data via the introduction of new imputation methods i e the use of emb algorithms instead of the weather generator to improve the scarce data processing capabilities of the swat on the other hand it should be noted that no single algorithm could solve all data scarcity problems for example it could be seen that the emb algorithm showed the worst imputation effect for peak rainfall values in addition the data scarcity during the low flow period would result in larger model prediction uncertainty and this effect would be transferred from the flow prediction to the tp simulation thus even with the best algorithm used more accurate rainfall monitoring data during extreme conditions such as high flow and low flow periods is also necessary it should be considered whether a modification of the em algorithm such as the smallest deviation the most accurate and by far the fastest pseudo em algorithm makhuvha et al 1997b can be used to impute peak rainfall values and according to pegram 1997 it can also be considered as a pre processing method for imputed data that uses the covariance biplot for preliminary repair of monthly scarce data additional studies are needed that consider the combination of data from multiple sources such as radar remote sensing precipitation monitoring and so on lakshmi 2004 li et al 2012 4 conclusion in this study the impacts of imputation methods on rainfall data were quantified under different data scarcity conditions and the effects of different rainfall scarcity scenarios on model uncertainty have been analysed after the investigation of rainfall data series and model performance the response of the output results of h nps simulation to the different imputed data sets was evaluated via traditional evaluation indicators e g nse and r2 the results indicated that the more scarce rainfall data would result in poorer model performance and larger prediction uncertainty and that this impact would be amplified from flow predictions to the h nps simulations in addition the rainfall data scarcity would cause larger h nps prediction uncertainty during the low flow periods compared to high and normal flow periods specifically the minimum values of total phosphorus and flow during low flow periods are more responsive to data scarcity and the maximum values of flow during high flow periods are more susceptible compared to the traditional da and weather generator methods the emb algorithm shows better performance in imputing rainfall data and h nps predictions regardless of missing rates and missing periods this advantage of the emb algorithm would be clearer if the specific threshold of data scarcity is reached which highlights the incorporation of the emb into the h nps models because emb can adapt well to the variability of rainfall data however this paper also noted that even if the best algorithm is used the imputed value is always lower than the large peak observations which highlights the importance of extreme rainfall monitoring during high flow periods this paper reports important implications for both the choice of imputation methods and the use of h nps models and provides an optimal scheme for solving data scarcity problems in watershed studies acknowledgements this research was funded by the national natural science foundation of china nos 51579011 and 51779010 the newton fund grant ref bb n013484 1 key laboratory of nonpoint source pollution control ministry of agriculture p r china 1610132016005 and the interdiscipline research funds of beijing normal university declaration of interest statement the authors declare that there are no conflicts of interest regarding this manuscript appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j jhydrol 2019 03 025 appendix a supplementary data the following are the supplementary data to this article supplementary data 1 
6603,developing hydrologic models based on data driven approaches dda is very complicated due to the complex nature of meteorological data for example a high degree of irregularities periodicities jumps and other forms of stochastic behavior influence the accuracy of river flow forecasting in this study m5 model tree m5tree and multivariate adaptive regression spline mars models were developed to forecast one and multi day ahead river flow moreover ensemble empirical mode decomposition eemd a robust data pre processing technique was used to enhance m5tree and mars models forecasting also mallows coefficient cp one of the procedures to determine the input variables was used to obtain the optimum values of hydrological time series the developed models were validated using two different meteorological stations e g kordkheyl in iran and hongcheon in south korea forecasting performance of developed models e g m5tree mars eemd m5tree and eemd mars was evaluated using six different statistical criteria comparing the results between standalone and hybrid models indicated that a data pre processing technique can enhance the performance of standalone models e g m5tree and mars eemd mars model nse 0 819 and rmse 7 206 m3 s kordkheyl station and nse 0 738 and rmse 50 426 m3 s hongcheon station outperformed m5tree mars and eemd m5tree models based on two day ahead river flow forecasting in validation stage respectively results showed that eemd mars model was an efficient and robust tool to forecast one and multi day ahead e g two three and four day ahead river flow keywords river flow forecasting m5 model tree multivariate adaptive regression spline ensemble empirical mode decomposition mallows coefficient cp 1 introduction providing a reliable model to forecast river flow can be instrumental for water resources planning and management kisi et al 2014 rezaie balf and kisi 2017 ghorbani et al 2018 karimi et al 2018 seo et al 2018 zakhrouf et al 2018 however forecasting is a complex task due to the non linear nature of river flow and dependence on a large number of parameters including temporal and spatial variations some of these parameters such as rainfall evaporation water stage and groundwater etc have uncertain and complex characteristics the complexity of problem increases when the models are applied for forecasting in scales of several days months in advance forecasting of river flow using available lead time series is a common task in hydrology rezaie balf and kisi 2017 in general data driven approaches dda can be divided into artificial intelligence and statistical models this study focuses on artificial intelligence models during the last decades dda such as adaptive neuro fuzzy inference system anfis shiri and kisi 2010 chang et al 2017 kisi et al 2017 artificial neural network ann rezaie balf and kisi 2017 zounemat kermani et al 2018 model tree mt solomatine and dulal 2003 solomatine and xue 2004 bhattacharya and solomatine 2005 najafzadeh et al 2016 rezaie balf et al 2017a gene expression programming gep lu et al 2018 mohsenzadeh karimi et al 2018 najafzadeh et al 2018 extreme learning machine elm mouatadid and adamowski 2017 rezaie balf and kisi 2017 and support vector machine svm kim et al 2017 seo et al 2018 have been employed to solve a wide range of environmental and water engineering problems particularly many studies have also been conducted using dda for river flow forecasting seo et al 2015 alizadeh et al 2017 liu et al 2017 partal 2017 ahani et al 2018 ghorbani et al 2018 the development of powerful approaches and methods however with high levels of reliability to achieve accurate forecasting techniques remains a major challenge since time series data for river flow and precipitation are generally highly non linear and seasonal applying the raw data directly to the model may not produce reliable and accurate results in this context using data pre processing technique may enhance the model s performance wu et al 2010 different data pre processing techniques have been successfully used to improve the forecasting accuracy for addressing hydrologic problems these include principal component analysis pca hu et al 2007 ravikumar and somashekar 2017 continuous wavelet transform cwt sang et al 2013 deo et al 2017 rezaie balf et al 2017b moving average ma yuan et al 2017 wavelet multi resolution analysis wmra zakhrouf et al 2018 maximum entropy spectral analysis mesa benedetto et al 2015 and singular spectrum analysis ssa wu et al 2009 baydaroƒülu et al 2018 more recently new noise assisted data analysis techniques e g empirical mode decomposition emd and ensemble empirical mode decomposition eemd have been pioneered by huang et al 1998 and wu and huang 2009 respectively the eemd approach is an advanced form of traditional decomposition techniques e g wavelet and fourier based decomposition and is an empirical intuitive and self adaptive data processing tool which is developed for non linear and non stationary signal sequences huang and wu 2008 hu et al 2013 several successful applications of emd and eemd have been reported to forecast river flow karthikeyan and kumar 2013 kisi et al 2014 barge and sharif 2016 for instance napolitano et al 2011 explored several aspects of ann for forecasting daily streamflow using emd they showed that the analysis using emd technique increases ann prediction accuracy and reliability likewise wang et al 2013 applied eemd as an adaptive data analysis tool for decomposing an annual rainfall dataset based on a svm model they reported the developed model s efficiency and capability using optimal svm parameters for runoff forecasting the present study aims to propose a novel data pre processing technique to improve forecasting accuracy of dda the forecasting accuracy of standalone models e g m5tree and mars is investigated integrating standalone models with eemd and mallows coefficient cp the new hybrid models e g eemd m5tree and eemd mars can be utilized to improve the accuracy and reliability of proposed standalone models using a historical dataset of predictor variables as the input the new hybrid system is tested in a real problem for forecasting river flow over two different meteorological stations e g kordkheyl in iran and hongcheon in south korea the contributions of this research can be expressed from two points of view the first attempt is to couple eemd and mallows coefficient cp based on m5tree as well as mars models for forecasting river flow secondly the authors aim to present the robust model for forecasting one and multi day ahead river flow of addressed stations the performance of proposed models is assessed using six different statistical criteria and diagnostic plots finally the conclusion and future research are given in the last part 2 methodology 2 1 m5 model tree m5tree m5tree model is a state of the art hierarchical algorithm to evaluate the relation between input and output parameters quinlan 1992 solomatine and dulal 2003 solomatine and xue 2004 talebi et al 2017 it divides the problem into sub problems sub spaces and constructs piecewise linear regression models for each sub space when used for classification all records are classified using tree sorting from the root to several leaves m5tree model is based on the well known classification and regression trees cart algorithm breiman et al 1984 which deals with continuous class learning attributes a schematic of m5tree model is illustrated in fig 1 a m5tree stores a linear model at each branch which predicts the class values of portion of dataset reaching to the leaf according to certain attributes of the data the records split into different portions solomatine and dulal 2003 solomatine and xue 2004 to determine the best attribute for splitting the dataset at each node standard deviation is utilized to construct m5tree model the tree can be obtained using standard deviation reduction sdr which maximizes the expected error reduction for each node using the following eq 1 1 sdr s d e i e i e s d e i where e stands for a set of instances that reaches the leaf node and ei is a subset of input data to parent node to overcome the overfitting problem and gain precise generalization pruning methods are employed to prune back the overgrown trees in the next stage the inner nodes sub trees in the pruning procedure are transformed into leaf nodes by replacing them with linear regression functions the disjointed linear models of neighboring leaves after pruning are subjected to a smoothing process in the course of the smoothing process to achieve the final model all the leaf models are combined along the path back to the root solomatine and dulal 2003 solomatine and xue 2004 information on m5tree model its applications and the model building procedure can be found in wang and witten 1996 and talebi et al 2017 in this study m5tree model was constructed using the weka 3 7 software 2 2 multivariate adaptive regression spline mars mars friedman 1991 does not need specific assumptions of functional relationships between predictors and output variables endpoints of the segments e g nodes determine the endpoint of each region as shown in fig 1 b kisi 2015 the results of splines provide higher flexibility for the model than linear functions considering curvature and thresholds amongst other features the mars creates basis functions bfs using a two step method first in the primary phase performance is improved until probabilistic nodes are found the second step involves removal of minimum real terms secondary phase suppose y is a deterministic output and x x 1 x p is an input variable thus it can be expressed that data can be obtained from an unknown real model consequently the response is as following eq 2 zhang and goh 2013 2 y f x 1 xp e f x e where e is the error distribution mars acquires the approximate function ∆í using the bfs bfs are referred to as splines e g smooth polynomials including the piecewise cubic and piecewise linear functions mars is a linear combination of the bfs and their mutual relations are extracted from following eq 3 3 f x Œ≤ 0 m 1 m Œ≤ m Œª m x where each Œª m x is a basis function bf which might be one spline function or a product of two or more spline functions coefficients Œ≤ are constant values and can be evaluated by the least squares ls method by cutting off the Œ≤0 and basis pair one model can be built that has the maximum reduction of training error the next pair is added to the current model on the basis of the m basis functions as following equation zhang and goh 2016 4 Œ≤ m 1 Œª 1 x max 0 x j t Œ≤ m 2 Œª 1 x max 0 t x j where the ls technique is used for estimating Œ≤ in addition mutual interactions among the bfs in the model are considered carefully when a new bf is added to the model space bfs are added into the model to achieve the maximum specified number of terms that bring a suitable fitness model after that a backward removal discipline is applied to decrease the number of terms the main purpose of this deletion approach is to find the closest to optimal model by getting rid of unessential variables in the backward method to determine the best sub model bfs with the lowest effectiveness on the model are removed generalized cross validation gcv a less computationally expensive technique for comparing subsets of model is expressed as following eq 5 zhang and goh 2016 5 gcv mse 1 n dn m 2 where m and n represent the number of observations and bfs respectively mse is mean squared error and d expresses the penalty of each bf 2 3 ensemble empirical mode decomposition eemd eemd is a special kind of mathematical function which represents a non linear and non stationary signal from original data wu and huang 2009 this data pre processing technique is an improvement of emd which is utilized to decompose the original data into a finite and small number of oscillatory modes based on the local characteristic time scale yeh et al 2010 the oscillatory modes can be expressed using intrinsic mode functions imfs embedded in the data emd is a self adaptive time frequency procedure and using imfs it represents a signal as a sum of zero mean well behaved fast and slow oscillation modes huang et al 1998 wu and huang 2009 in general an imf represents a simple oscillatory mode compared with the simple harmonic function based on the definition a shifting process of the original time series can be briefly expressed as follows yeh et al 2010 step 1 identify all extrema local maxima and minima points of the given time series y t step 2 connect local maxima points to form an upper envelope emax t and all minima points to form a lower envelope emin t with spline interpolation respectively step 3 calculate the mean m t of the two envelopes m t e max t e min t 2 step 4 subtract the mean from the data to establish an imf candidate h t y t m t step 5 if h t meets predefined stopping criteria h t is denoted as the first imf written as c1 t and 1 is its index if h t is not an imf y t is replaced with h t and steps 1 4 are iterated until h t meets the two conditions of an imf step 6 the residual r1 t y t c1 t is then treated as new data subjected to the same shifting process as described above for the next imf from r1 t in the final step the shifting procedure can be stopped when the residue r t becomes a monotonic trend or has a single local maxima and minima point from which no more imfs can be extracted huang et al 2003 at the end of this shifting process the original signal y t can be reconstructed as the sum of imfs and residual using the following eq 6 6 y t i 1 n c i t r n t where rn t represents the final residue n is the number of imf and ci t are nearly orthogonal to each other and all have zero means more information for emd and the stopping criterion can be found in huang et al 1998 and huang et al 2003 the investigations illustrated that emd was unstable due to a mode mixing drawback wu and huang 2009 the mode mixing drawback is expressed as either a single imf consisting of components of disparate scales or a component of a similar scale residing in different imfs lei et al 2009 to overcome the difficult problem of mode mixing drawback in emd eemd is modified as an enhanced method based on to this argument the whole time frequency space is uniformly filled by adding white noise wu and huang 2009 2 4 variable selection criterion based on mallows coefficient cp in practical problems many independent parameters may exist and the choice of the most influencing parameters among them is necessary for the development of forecasting models the forecasting ability of developed models depends on the chosen parameter subset george 2000 for instance the problem of subset selection arises when a researcher attempts to achieve a forecasting model that cross validates well or when there is redundancy among the independent parameters leading to multicollinearity fox 1991 there are some procedures to determine the predictor influencing subset including forward selection backward elimination akaike information criterion aic bayesian information criterion bic and mallows coefficient cp cohen et al 2014 among them mallows coefficient cp has been successful in a choice of the least number of input variables a small mallows coefficient cp implies a good selection of predictor subset and a strong forecasting model if p predictors are selected from a set of k available predictors for a specific problem k p mallows coefficient cp for this particular subset can be calculated using the following eq 7 mallows 1973 7 c p rs s p ms e k 2p n where rssp is the residual sum of squares for a model with p predictors msek represents the mean squared error for regression of the model with the complete set of k predictors and n is the sample size therefore mallows coefficient cp indexes the mean squared error of forecasting for subset models relative to the full model with a penalty for the inclusion of insignificant predictors kobayashi and sakata 1990 3 case study and available data the preparation of hydro meteorological data is one of the most significant issues for hydrologists and water resources engineers jajarmizadeh et al 2016 using data from diverse regions with different meteorological conditions e g dry and temperate climate can enhance the robustness of proposed models therefore the measurements of river flow applied were acquired from two different stations i e kordkheyl in iran and hongcheon in south korea tajan river catchment latitude 36 48 n and longitude 53 06 e has experienced several floods including the most catastrophic one in 1997 it originates from tizabad mountain and eventually enters the caspian sea in mazandaran province in the north of iran it is about 170 km long and its catchment covers an area of 4147 km2 with an annual average streamflow of 207 4 m3 s the catchment elevation varies between 26 and 3728 m the climate is semi humid and cold humid with annual rainfall and discharge of 539 mm and 20 m3 s respectively rezaie balf et al 2017a b the meteorological data available for tajan river were downloaded from of meteorological organization of mazandaran province momp http www mazmet ir en contact us a challenging issue is that the models can forecast river flow accurately by chance or perform well only in some ranges of input and output variables to respond to this circumstance the current research considers another river with different characteristics of input and output variables the hongcheon river basin located in gangwon province of south korea has a humid climate and covers an area of about 1818 km2 with an average height of 140 92 m above mean sea level msl climatic conditions are considered extreme as its location is away from the sea the daily rainfall and runoff data of hongcheon station latitude 37 68 n and longitude 127 88 w were utilized in current research average precipitation and relative humidity of this catchment were reported as 1405 mm and 69 5 per year respectively the rainfall data has been collected and managed by the korea meteorological administration kma since september 1971 https data kma go kr the runoff data was available since 1997 and has been recorded and arranged in the water resources management information system wamis the complete data for the underlying period however was not available because the some periods were missing from wamis http www wamis go kr the data available e g daily river flow and precipitation were divided into two phases approximately 75 2003 2010 of the dataset was used for the calibration stage and the remainder of the dataset was kept for the validation stage 2011 2013 until now the public agencies e g momp kma and wamis have not used m5tree and mars models for forecasting river flow in iran and south korea the schematic diagrams of kordkheyl and hongcheon stations are presented in fig 2 a and b in addition the other details of dataset and statistics of parameters used for proposed models are given in table 1 it is apparent from the table 1 that the rainfall data in hongcheon station has the highest skewed distribution 4 model assessment criteria to evaluate eemd m5tree and eemd mars models performance compared with m5tree and mars models six different statistical criteria were used which are defined as 1 nash sutcliffe efficiency nse this criterion is taken into account to evaluate the ability of hydrological models with higher values indicated better fit between observed and forecasted river flow nash and sutcliffe 1970 if the squared difference between observed and forecasted river flow is relatively large to concur with the variance in observed river flow values nse criterion will be zero if nse criterion is negative the results indicate that the observed mean is a better predictor than the model wilcox et al 1990 if nse criterion is equal to one it indicates a perfect model legates and mccabe 1999 however garrick et al 1978 reported that even for poorly fitted models nse criterion can be relatively high 0 8 while the best models may not generate high values it can be formulated as following eq 8 8 nse 1 i 1 n q obs q for 2 i 1 n q obs q obs 2 2 root mean square error rmse the discrepancy between observed and forecasted values can be shown using rmse criterion a value of zero reflects perfect forecasting the rmse criterion can provide information for model s forecasting skill and quantify the goodness of fit relevant to high river flow values chai and draxler 2014 the rmse criterion can be biased in favor of the peaks and higher magnitudes that will exhibit the greatest error dawson et al 2007 consequently rmse criterion must be used for model evaluation to obtain accuracy in absolute units deo et al 2019 it can be expressed as following eq 9 9 rmse 1 n i 1 n q obs q for 2 3 willmott s index of agreement wi wi criterion as a standardized measure of the degree for model estimation error varies between zero and one so that a value of zero asserts no agreement at all while one indicates a perfect match willmott et al 2012 wi calculates the ratio of mean square error mse and can provide an advantage over rmse willmott 1984 it can be shown as following eq 10 10 wi 1 i 1 n q obs q for 2 i 1 n q for q obs q obs q obs 2 4 legates mccabe s index lmi this criterion considers absolute values for computation and gives errors and differences the appropriate weights legates and mccabe 1999 lmi criterion is not inflated by the squared values and is less sensitive to outliers making it simple and easy to interpret this criterion varies from zero to one with higher values indicating a better fit of the model deo et al 2019 it can be arranged as following eq 11 11 lmi 1 i 1 n q for q for i 1 n q obs q obs 5 peak flow criterion pfc this value plays a significant role in monitoring the extreme events including flood which can be evaluated by pfc to achieve the efficient model a pfc equals to zero represents a perfect fit of model it can be provided as following eq 12 12 pfc i 1 np q obs q for 2 q obs 2 0 25 i 1 np q obs q for 2 0 5 6 low flow criterion lfc lfc is used to assess the accuracy of proposed forecasting models and has same conception of pfc with the difference that it considers low quantities of flow also lower lfc means that the model provides precise predictions and a better fit it can be presented as following eq 13 13 lfc i 1 nl q obs q for 2 q obs 2 0 25 i 1 nl q obs q for 2 0 5 where q obs and q for are observed and forecasted values respectively q obs and q for are the average of observed and forecasted values respectively np is the number of river flow observations greater than one third of mean flow nl is the number of river flow observations less than one third of mean flow and n is the length of time series data the underlying performance criteria can be found as common statistical criteria from the previous literature nash and sutcliffe 1970 legates and mccabe 1999 willmott et al 2012 and each of them has been proposed for a specific purpose the addressed statistical criteria e g nse rmse wi and lmi used in current research may not illustrate the models performance in terms of extreme events such as flood and lack of river flow that is one model may forecast the mean values of streamflow accurately but cannot forecast the high and low values of river flow i e extreme events also it is crucial to assess and monitor the extreme values using pfc and lfc criteria for forecasting of extreme events 5 model applications 5 1 optimum model structure the selection of optimum input variables can be considered as one of the most important tasks for developing accurate forecasting model coulibaly et al 2001 jain et al 2008 kim and singh 2014 the characteristics of river flow cannot be distinguished with small number of input variables accurately similarly the excessive number of input variables can cause overfitting problems rezaie balf et al 2017a b it is clear that the input variables in current study are precipitation and river flow the antecedent values of input variables can be interpreted to represent catchment characteristics for the purpose of developing a data driven model the purpose of this study is to forecast one and multi day ahead river flow of kordkheyl and hongcheon stations so antecedent river flow can be used as a predictor first precipitation and river flow with different lag time are considered as a basic input structure 14 q t 1 f p t 2 p t 1 p t q t 4 q t 3 q t 2 q t 1 q t where t 1 t 2 t 3 and t 4 denote the different daily antecedent values of time series t then to assess the optimum input variables for forecasting each lag time series diverse linear regression functions were accomplished using the best subsets regression tool of minitab software the effective input variables were determined using mallows coefficient cp since it is an efficient approach to determine the subset of effective subseries where there are a number of potential input variables thereafter the selected input variables were ready for next phase 5 2 models implementation based on data pre processing technique the main purpose of eemd m5tree and eemd mars models is to forecast one and multi day ahead river flow for different catchments fig 3 demonstrates the proposed approach for three main steps to enhance the river flow forecasting where p is daily precipitation q is daily river flow res represents the residual error k means kordkheyl station and h is hongcheon station step 1 eemd procedure is utilized to decompose the original input and output observed time series y t into several imf components ci t i 1 2 3 n and one residual component rn t step 2 for each extracted imf component and the residual component for example imf1 the m5tree and mars models are established as river flow forecasting tools to simulate the decomposed imf and residual components and to calculate each component using the same sub series imf1 of input variables respectively step 3 the forecasted values of all extracted imf and residue components using m5tree and mars models are aggregated to generate the river flow to sum up eemd m5tree and eemd mars models established the main idea of decomposition and ensemble the forecasting processes can be simplified by decomposition and the ensemble is utilized to formulate a consensus forecasting on the original time series 5 3 standalone and hybrid models for river flow forecasting to construct m5tree model the regression and model trees were utilized for model tree the number of trees should be selected to improve the non linear performance of one day ahead river flow forecasting in advance in current research the number of trees was varied between 1 and 200 for m5tree model in addition m5tree model also applied trial and error to decide the optimum model s parameters since two stations and four step ahead were used for forecasting river flow eight output variables can be calculated using m5tree model in addition each output was decomposed from imf1 to imf13 for forecasting one and multi day ahead river flow that is 104 13 8 m5tree model s parameters e g leaves and number of equations were calculated this study used an open source code areslab to implement the main functions of mars model in mars model three main parameters i e degree of interaction d the number of bfs in the forward step nk and the smoothing parameter s in gcv affect its performance significantly and hence they should be optimized it is a generally accepted procedure to perform a sensitivity analysis by changing these three parameters to obtain the optimum values for model fitting with training data overall when the difference between observed and forecasted river flow is minimal the values of parameter were chosen as the optimal ones in this study the parameters e g d nk and s were varied from 1 to 2 d 10 to 50 nk and 2 to 4 s by increments of 1 1 and 0 5 respectively a 10 fold cross validation method was also applied to avoid model performance appraisal bias results of linear regression analysis to forecast q t 1 for both stations are illustrated in fig 4 six kordkheyl station and five hongcheon station variables were selected to minimize mallows coefficient cp and variance coefficients in this regard the effective input variables for both stations were defined as following eqs 15 and 16 river flow at kordkheyl station 15 q k t 1 f p k t 1 p k t q k t 3 q k t 2 q k t 1 q k t river flow at hongcheon station 16 q h t 1 f p h t 1 p h t q h t 2 q h t 1 q h t the secondary objective of present research was to investigate the proposed model s ability to forecast multi day ahead river flow of both stations thus standalone and hybrid models were employed to forecast two three and four day ahead river flow to implement the proposed models six input combinations were considered for each output variable corresponding to two three and four day ahead river flow forecasting respectively 6 results and discussion 6 1 results for one day ahead river flow forecasting results from standalone m5tree and mars and hybrid eemd m5tree and eemd mars models were evaluated for forecasting one day ahead river flow at two different stations table 2 shows the statistical criteria for one day ahead river flow forecasting in calibration and validation stages the obtained statistical criteria indicated that eemd m5tree and eemd mars models were able to forecast accurate one day ahead river flow as compared to standalone models for both stations at kordkheyl station the performance of mars model nse 0 791 rmse 10 584 m3 s wi 0 930 and lmi 0 718 was slightly better than that of m5tree model in calibration stage in validation stage however m5tree model nse 0 781 rmse 7 943 m3 s wi 0 940 and lmi 0 684 forecasted river flow accurately better than mars model in the case of hybrid models eemd m5tree model enhanced m5tree model by a 26 reduction in rmse criterion and increased accuracy for nse wi and lmi criterion by 13 5 and 12 in calibration stage in validation stage eemd mars model improved model accuracy by 9 in the nse criterion compared to mars model as can be seen from table 2 the hybrid models i e eemd m5tree and eemd mars were able to forecast the extreme values resulting in small values of pfc 0 311 and 0 249 and lfc 0 295 and 0 236 at hongcheon station the performance of mars model nse 0 768 rmse 53 293 m3 s and wi 0 930 was better than that of m5tree model results of lmi criterion however indicated that the performance of m5tree model was better than that of mars model 0 784 vs 0 751 in calibration stage in validation stage the performance of mars model nse 0 736 rmse 50 469 m3 s and wi 0 916 was better than that of m5tree model results of lmi criterion however indicated that the performance of m5tree model was better than that of mars model 0 711 vs 0 694 results of lmi criterion provided the same pattern in calibration and validation stages in the case of hybrid models eemd mars model improved m5tree mars and eemd m5tree models in nse criterion by 8 7 and 4 in validation stage in addition eemd mars model enhanced m5tree mars and eemd m5tree models by 12 32 10 36 and 9 92 reduction in rmse criterion respectively applying eemd in conjunction with m5tree and mars models as a consequence improved the models performance significantly in validation stage the scatter plots between observed and forecasted river flow values for one day ahead forecasting using standalone and hybrid models for kordkheyl and hongcheon stations are provided in fig 5 the scatter plots also illustrated that the forecasted values of eemd m5tree and eemd mars models for both stations followed the observations better than those of standalone models it can be seen from fig 6 that eemd mars model can forecast river flow better than other models for one day ahead time series 6 2 results for two three and four day ahead river flow forecasting results of subsets regression using the proposed input combinations are shown in fig 7 the minimum values of mallows coefficient cp and variance coefficients indicated the optimum combination for river flow forecasting the optimum input variables using mallows coefficient cp based on all possible regression analyses are presented in table 3 the obtained results of multi day ahead river flow forecasting are illustrated in tables 4 and 5 for kordkheyl and hongcheon stations respectively it seems that the proposed data pre processing technique could improve the performances of m5tree and mars models effectively for different time steps and both stations it can be concluded that the hybrid models could depict the efficient and reliable values for forecasting river flow up to two day ahead including acceptable values that can give a good indication for three and four day ahead comparing tables 4 and 5 indicates that eemd based heuristic regression models e g eemd m5tree and eemd mars produce better forecasting accuracy compared to m5tree and mars models based on six different statistical criteria in current study in general the optimum performances of m5tree and mars models for both stations asserted that mars models in validation stage yielded better accuracy compared to m5tree models for two three and four day ahead river flow forecasting e g higher nse wl lmi and lower rmse at kordkheyl station it can be observed from standalone models that the performance of mars model was slightly better than that of m5tree model except for four day ahead river flow forecasting in validation stage furthermore the hybrid models eemd m5tree and eemd mars performed better compared to standalone models m5tree and mars in validation stage more specially rmse criterion of eemd mars model was reduced by 26 22 22 72 and 21 21 for two three and four day ahead river flow forecasting compared to mars model respectively in addition rmse criterion of eemd m5tree model was reduced by 6 37 17 06 and 7 98 for two three and four day ahead river flow forecasting compared to m5tree model respectively at hongcheon station mars model implementing eemd was accurate compared to other models from table 5 for instance a comparison among standalone and hybrid models indicated that eemd mars model nse 0 738 rmse 50 426 m3 s wi 0 930 and lmi 0 568 outperformed eemd m5tree nse 0 678 rmse 55 966 m3 s wi 0 910 and lmi 0 559 mars nse 0 617 rmse 60 985 m3 s wi 0 860 and lmi 0 530 and m5tree nse 0 623 rmse 60 576 m3 s wi 0 890 and lmi 0 546 models for two day ahead river flow forecasting in validation stage respectively to confirm the accuracy of proposed models graphically figs 8 and 9 provide the scatter plots of observed and forecasted values analyzed as a regression between the two datasets for two three and four day ahead river flow forecasting note that in each scatter plot the rmse value and linear equation for standalone and hybrid models have been proposed the scatter plots in figs 8 and 9 illustrate that the forecasted values by m5tree mars eemd m5tree and eemd mars models for two day ahead river flow forecasting were much closer to corresponding observed values than those of three and four day ahead river flow forecasting it was evident from fig 8 that eemd mars model s line of linear agreement between forecasted y and observed x values i e y 0 938x 1 85 where 0 938 and 1 85 are equal to constants a and b was much closer to the ideal line 1 1 compared to m5tree mars and eemd m5tree models in other words eemd mars model can provide accurate forecasting values since the corresponding gradient of 1 1 line of eemd mars model was much closer to the observed values compared to the other three models m5tree mars and eemd m5tree and the bias is small according to results and discussions so far it can be said that unlike eemd mars model other models failed to forecast the extreme conditions i e peak flows this problem can be investigated by comparing the highest ten peak flows based on four day ahead forecasted values of both stations table 6 shows that the maximum difference between forecasted and observed peak flows provided the value of 36 for eemd mars model whereas this error was up to the values of 76 for eemd m5tree model 93 and 89 for mars and m5tree models respectively for kordkheyl station in addition the maximum difference between forecasted and observed peak flows for eemd mars eemd m5tree mars and m5tree models produced the values of 48 70 74 and 75 respectively for hongcheon station it was found that results of one day ahead river flow forecasting produced better accuracy than those of multi day ahead i e two three and four day ahead river flow forecasting of both stations the comparative evaluation of one and multi day ahead river flow forecasting proved that applying eemd in conjunction with m5tree and mars models could reduce model error and produce more accurate forecasting for kordkheyl and hongcheon stations the model accuracy was also appraised using boxplots illustrating the spread of observed and forecasted values obtained by standalone and hybrid models for one and multi day ahead river flow at kordkheyl station the spread of observed and forecasted values by eemd m5tree model appeared similar and the distribution of values forecasted by eemd mars model compared reasonably to observations however m5tree and mars models displayed an unacceptable spread for river flow forecasting in validation stage at hongcheon station where high values of river flow produced the main features in validation stage eemd m5tree model performed better forecasting distribution for median values while eemd mars showed much scattered spread while the standalone models e g m5tree and mars almost led to similar distributions for one and multi day ahead river flow forecasting such distributions did not clearly follow the spread of observed values overall the findings of present research ascertained that with appropriate input selection eemd based heuristic regression models captured the non linear and non stationary dynamics of river flow effectively and could provide the combined and stabilized river flow forecasting although m5tree and mars models forecasted the extreme values of one day ahead river flow successfully they could not be efficient and reliable for two three and four day ahead river flow forecasting since eemd m5tree and eemd mars models produced better results these models could provide a very useful and accurate forecasting tool therefore eemd based heuristic regression models can improve the development and management of water resources using accurate river flow forecasting of both catchments see fig 10 although this study was the first approach to develop and assess eemd m5tree and eemd mars models for river flow forecasting the identified limitations should be addressed for future studies as can be seen from analysis and figures m5tree and mars models can improve the accuracy of one and multi day ahead river flow forecasting when eemd technique is integrated to decompose data time series one of the disadvantages of eemd technique is that its implementation is time consuming because it produces the large numbers of imfs another limitation of eemd technique is the different length of imfs since the features of input variables are different the number of imfs for each variable is not same following the previous researches al musaylh et al 2018 wen et al 2019 imf1 with high frequency values of decomposed river flow had the highest forecasting error among the other imfs therefore it was recommended to apply the other data pre processing techniques to decompose imf1 with several sub series dataset which can enhance model s performance 7 conclusion and future research accurate forecasting of daily river flow is a key problem of interest to water resources engineers scientists and hydrologists this study implements and evaluates the efficiency of standalone m5tree and mars models for two different river basins including kordkheyl station in iran and hongcheon station in south korea mallows coefficient cp is applied to determine the most effective input variables for forecasting one and multi day ahead river flow then the optimum input variables for four forecasting periods one and multi day ahead are decomposed by eemd as a novel data pre processing technique which can reduce the non linear and non stationary nature of river flow and improve the accuracy of river flow forecasting significantly comparing the results of standalone m5tree and mars and hybrid eemd m5tree and eemd mars models using six different statistical criteria and diagnostic plots confirms the robustness and efficiency of the proposed models eemd mars model provides the best accuracy among the proposed models within the range of historical time series one and multi day ahead in current study this indicates that the decomposition and ensemble strategy can improve the accuracy of river flow forecasting effectively thus it can be concluded that eemd mars model may be an alternative tool and promising forecasting technique for complex problems such as river flow with high non linear and non stationary nature moreover this study reveals that the application of a robust data pre processing technique e g eemd has a key role to achieve accurate forecasting of hydrological processes which can lead to reliable and efficient water management for future research the hybrid model coupling data driven approach data pre processing technique and physically based hydrologic model can be suggested as a potential alternative to enhance the forecasting accuracy of hydrological processes i e streamflow rainfall water stage groundwater etc to achieve this goal additional data pre processing techniques such as complete ensemble empirical mode decomposition ceemd improved ceemd and variational mode decomposition vmd can be addressed also the periodic component of input combination and watershed characteristics may be applied to increase the standalone model s accuracy in addition it can be suggested that two phase decomposition can be applied to enhance imf1 forecasting values which consist of high pass signals for future works conflict of interest none declared acknowledgment the authors would like to reveal our extreme appreciation and gratitude to the regional water organization of mazandaran province and meteorological organization of mazandaran province momp in iran and korea meteorological administration kma and water resources management information system wamis south korea this is for providing the meteorological information the second author professor sungwon kim would like to appreciate the financial support from dongyang university south korea in addition the authors also sincerely thank the editor and reviewers for their admirable revision and scientific suggestions 
6603,developing hydrologic models based on data driven approaches dda is very complicated due to the complex nature of meteorological data for example a high degree of irregularities periodicities jumps and other forms of stochastic behavior influence the accuracy of river flow forecasting in this study m5 model tree m5tree and multivariate adaptive regression spline mars models were developed to forecast one and multi day ahead river flow moreover ensemble empirical mode decomposition eemd a robust data pre processing technique was used to enhance m5tree and mars models forecasting also mallows coefficient cp one of the procedures to determine the input variables was used to obtain the optimum values of hydrological time series the developed models were validated using two different meteorological stations e g kordkheyl in iran and hongcheon in south korea forecasting performance of developed models e g m5tree mars eemd m5tree and eemd mars was evaluated using six different statistical criteria comparing the results between standalone and hybrid models indicated that a data pre processing technique can enhance the performance of standalone models e g m5tree and mars eemd mars model nse 0 819 and rmse 7 206 m3 s kordkheyl station and nse 0 738 and rmse 50 426 m3 s hongcheon station outperformed m5tree mars and eemd m5tree models based on two day ahead river flow forecasting in validation stage respectively results showed that eemd mars model was an efficient and robust tool to forecast one and multi day ahead e g two three and four day ahead river flow keywords river flow forecasting m5 model tree multivariate adaptive regression spline ensemble empirical mode decomposition mallows coefficient cp 1 introduction providing a reliable model to forecast river flow can be instrumental for water resources planning and management kisi et al 2014 rezaie balf and kisi 2017 ghorbani et al 2018 karimi et al 2018 seo et al 2018 zakhrouf et al 2018 however forecasting is a complex task due to the non linear nature of river flow and dependence on a large number of parameters including temporal and spatial variations some of these parameters such as rainfall evaporation water stage and groundwater etc have uncertain and complex characteristics the complexity of problem increases when the models are applied for forecasting in scales of several days months in advance forecasting of river flow using available lead time series is a common task in hydrology rezaie balf and kisi 2017 in general data driven approaches dda can be divided into artificial intelligence and statistical models this study focuses on artificial intelligence models during the last decades dda such as adaptive neuro fuzzy inference system anfis shiri and kisi 2010 chang et al 2017 kisi et al 2017 artificial neural network ann rezaie balf and kisi 2017 zounemat kermani et al 2018 model tree mt solomatine and dulal 2003 solomatine and xue 2004 bhattacharya and solomatine 2005 najafzadeh et al 2016 rezaie balf et al 2017a gene expression programming gep lu et al 2018 mohsenzadeh karimi et al 2018 najafzadeh et al 2018 extreme learning machine elm mouatadid and adamowski 2017 rezaie balf and kisi 2017 and support vector machine svm kim et al 2017 seo et al 2018 have been employed to solve a wide range of environmental and water engineering problems particularly many studies have also been conducted using dda for river flow forecasting seo et al 2015 alizadeh et al 2017 liu et al 2017 partal 2017 ahani et al 2018 ghorbani et al 2018 the development of powerful approaches and methods however with high levels of reliability to achieve accurate forecasting techniques remains a major challenge since time series data for river flow and precipitation are generally highly non linear and seasonal applying the raw data directly to the model may not produce reliable and accurate results in this context using data pre processing technique may enhance the model s performance wu et al 2010 different data pre processing techniques have been successfully used to improve the forecasting accuracy for addressing hydrologic problems these include principal component analysis pca hu et al 2007 ravikumar and somashekar 2017 continuous wavelet transform cwt sang et al 2013 deo et al 2017 rezaie balf et al 2017b moving average ma yuan et al 2017 wavelet multi resolution analysis wmra zakhrouf et al 2018 maximum entropy spectral analysis mesa benedetto et al 2015 and singular spectrum analysis ssa wu et al 2009 baydaroƒülu et al 2018 more recently new noise assisted data analysis techniques e g empirical mode decomposition emd and ensemble empirical mode decomposition eemd have been pioneered by huang et al 1998 and wu and huang 2009 respectively the eemd approach is an advanced form of traditional decomposition techniques e g wavelet and fourier based decomposition and is an empirical intuitive and self adaptive data processing tool which is developed for non linear and non stationary signal sequences huang and wu 2008 hu et al 2013 several successful applications of emd and eemd have been reported to forecast river flow karthikeyan and kumar 2013 kisi et al 2014 barge and sharif 2016 for instance napolitano et al 2011 explored several aspects of ann for forecasting daily streamflow using emd they showed that the analysis using emd technique increases ann prediction accuracy and reliability likewise wang et al 2013 applied eemd as an adaptive data analysis tool for decomposing an annual rainfall dataset based on a svm model they reported the developed model s efficiency and capability using optimal svm parameters for runoff forecasting the present study aims to propose a novel data pre processing technique to improve forecasting accuracy of dda the forecasting accuracy of standalone models e g m5tree and mars is investigated integrating standalone models with eemd and mallows coefficient cp the new hybrid models e g eemd m5tree and eemd mars can be utilized to improve the accuracy and reliability of proposed standalone models using a historical dataset of predictor variables as the input the new hybrid system is tested in a real problem for forecasting river flow over two different meteorological stations e g kordkheyl in iran and hongcheon in south korea the contributions of this research can be expressed from two points of view the first attempt is to couple eemd and mallows coefficient cp based on m5tree as well as mars models for forecasting river flow secondly the authors aim to present the robust model for forecasting one and multi day ahead river flow of addressed stations the performance of proposed models is assessed using six different statistical criteria and diagnostic plots finally the conclusion and future research are given in the last part 2 methodology 2 1 m5 model tree m5tree m5tree model is a state of the art hierarchical algorithm to evaluate the relation between input and output parameters quinlan 1992 solomatine and dulal 2003 solomatine and xue 2004 talebi et al 2017 it divides the problem into sub problems sub spaces and constructs piecewise linear regression models for each sub space when used for classification all records are classified using tree sorting from the root to several leaves m5tree model is based on the well known classification and regression trees cart algorithm breiman et al 1984 which deals with continuous class learning attributes a schematic of m5tree model is illustrated in fig 1 a m5tree stores a linear model at each branch which predicts the class values of portion of dataset reaching to the leaf according to certain attributes of the data the records split into different portions solomatine and dulal 2003 solomatine and xue 2004 to determine the best attribute for splitting the dataset at each node standard deviation is utilized to construct m5tree model the tree can be obtained using standard deviation reduction sdr which maximizes the expected error reduction for each node using the following eq 1 1 sdr s d e i e i e s d e i where e stands for a set of instances that reaches the leaf node and ei is a subset of input data to parent node to overcome the overfitting problem and gain precise generalization pruning methods are employed to prune back the overgrown trees in the next stage the inner nodes sub trees in the pruning procedure are transformed into leaf nodes by replacing them with linear regression functions the disjointed linear models of neighboring leaves after pruning are subjected to a smoothing process in the course of the smoothing process to achieve the final model all the leaf models are combined along the path back to the root solomatine and dulal 2003 solomatine and xue 2004 information on m5tree model its applications and the model building procedure can be found in wang and witten 1996 and talebi et al 2017 in this study m5tree model was constructed using the weka 3 7 software 2 2 multivariate adaptive regression spline mars mars friedman 1991 does not need specific assumptions of functional relationships between predictors and output variables endpoints of the segments e g nodes determine the endpoint of each region as shown in fig 1 b kisi 2015 the results of splines provide higher flexibility for the model than linear functions considering curvature and thresholds amongst other features the mars creates basis functions bfs using a two step method first in the primary phase performance is improved until probabilistic nodes are found the second step involves removal of minimum real terms secondary phase suppose y is a deterministic output and x x 1 x p is an input variable thus it can be expressed that data can be obtained from an unknown real model consequently the response is as following eq 2 zhang and goh 2013 2 y f x 1 xp e f x e where e is the error distribution mars acquires the approximate function ∆í using the bfs bfs are referred to as splines e g smooth polynomials including the piecewise cubic and piecewise linear functions mars is a linear combination of the bfs and their mutual relations are extracted from following eq 3 3 f x Œ≤ 0 m 1 m Œ≤ m Œª m x where each Œª m x is a basis function bf which might be one spline function or a product of two or more spline functions coefficients Œ≤ are constant values and can be evaluated by the least squares ls method by cutting off the Œ≤0 and basis pair one model can be built that has the maximum reduction of training error the next pair is added to the current model on the basis of the m basis functions as following equation zhang and goh 2016 4 Œ≤ m 1 Œª 1 x max 0 x j t Œ≤ m 2 Œª 1 x max 0 t x j where the ls technique is used for estimating Œ≤ in addition mutual interactions among the bfs in the model are considered carefully when a new bf is added to the model space bfs are added into the model to achieve the maximum specified number of terms that bring a suitable fitness model after that a backward removal discipline is applied to decrease the number of terms the main purpose of this deletion approach is to find the closest to optimal model by getting rid of unessential variables in the backward method to determine the best sub model bfs with the lowest effectiveness on the model are removed generalized cross validation gcv a less computationally expensive technique for comparing subsets of model is expressed as following eq 5 zhang and goh 2016 5 gcv mse 1 n dn m 2 where m and n represent the number of observations and bfs respectively mse is mean squared error and d expresses the penalty of each bf 2 3 ensemble empirical mode decomposition eemd eemd is a special kind of mathematical function which represents a non linear and non stationary signal from original data wu and huang 2009 this data pre processing technique is an improvement of emd which is utilized to decompose the original data into a finite and small number of oscillatory modes based on the local characteristic time scale yeh et al 2010 the oscillatory modes can be expressed using intrinsic mode functions imfs embedded in the data emd is a self adaptive time frequency procedure and using imfs it represents a signal as a sum of zero mean well behaved fast and slow oscillation modes huang et al 1998 wu and huang 2009 in general an imf represents a simple oscillatory mode compared with the simple harmonic function based on the definition a shifting process of the original time series can be briefly expressed as follows yeh et al 2010 step 1 identify all extrema local maxima and minima points of the given time series y t step 2 connect local maxima points to form an upper envelope emax t and all minima points to form a lower envelope emin t with spline interpolation respectively step 3 calculate the mean m t of the two envelopes m t e max t e min t 2 step 4 subtract the mean from the data to establish an imf candidate h t y t m t step 5 if h t meets predefined stopping criteria h t is denoted as the first imf written as c1 t and 1 is its index if h t is not an imf y t is replaced with h t and steps 1 4 are iterated until h t meets the two conditions of an imf step 6 the residual r1 t y t c1 t is then treated as new data subjected to the same shifting process as described above for the next imf from r1 t in the final step the shifting procedure can be stopped when the residue r t becomes a monotonic trend or has a single local maxima and minima point from which no more imfs can be extracted huang et al 2003 at the end of this shifting process the original signal y t can be reconstructed as the sum of imfs and residual using the following eq 6 6 y t i 1 n c i t r n t where rn t represents the final residue n is the number of imf and ci t are nearly orthogonal to each other and all have zero means more information for emd and the stopping criterion can be found in huang et al 1998 and huang et al 2003 the investigations illustrated that emd was unstable due to a mode mixing drawback wu and huang 2009 the mode mixing drawback is expressed as either a single imf consisting of components of disparate scales or a component of a similar scale residing in different imfs lei et al 2009 to overcome the difficult problem of mode mixing drawback in emd eemd is modified as an enhanced method based on to this argument the whole time frequency space is uniformly filled by adding white noise wu and huang 2009 2 4 variable selection criterion based on mallows coefficient cp in practical problems many independent parameters may exist and the choice of the most influencing parameters among them is necessary for the development of forecasting models the forecasting ability of developed models depends on the chosen parameter subset george 2000 for instance the problem of subset selection arises when a researcher attempts to achieve a forecasting model that cross validates well or when there is redundancy among the independent parameters leading to multicollinearity fox 1991 there are some procedures to determine the predictor influencing subset including forward selection backward elimination akaike information criterion aic bayesian information criterion bic and mallows coefficient cp cohen et al 2014 among them mallows coefficient cp has been successful in a choice of the least number of input variables a small mallows coefficient cp implies a good selection of predictor subset and a strong forecasting model if p predictors are selected from a set of k available predictors for a specific problem k p mallows coefficient cp for this particular subset can be calculated using the following eq 7 mallows 1973 7 c p rs s p ms e k 2p n where rssp is the residual sum of squares for a model with p predictors msek represents the mean squared error for regression of the model with the complete set of k predictors and n is the sample size therefore mallows coefficient cp indexes the mean squared error of forecasting for subset models relative to the full model with a penalty for the inclusion of insignificant predictors kobayashi and sakata 1990 3 case study and available data the preparation of hydro meteorological data is one of the most significant issues for hydrologists and water resources engineers jajarmizadeh et al 2016 using data from diverse regions with different meteorological conditions e g dry and temperate climate can enhance the robustness of proposed models therefore the measurements of river flow applied were acquired from two different stations i e kordkheyl in iran and hongcheon in south korea tajan river catchment latitude 36 48 n and longitude 53 06 e has experienced several floods including the most catastrophic one in 1997 it originates from tizabad mountain and eventually enters the caspian sea in mazandaran province in the north of iran it is about 170 km long and its catchment covers an area of 4147 km2 with an annual average streamflow of 207 4 m3 s the catchment elevation varies between 26 and 3728 m the climate is semi humid and cold humid with annual rainfall and discharge of 539 mm and 20 m3 s respectively rezaie balf et al 2017a b the meteorological data available for tajan river were downloaded from of meteorological organization of mazandaran province momp http www mazmet ir en contact us a challenging issue is that the models can forecast river flow accurately by chance or perform well only in some ranges of input and output variables to respond to this circumstance the current research considers another river with different characteristics of input and output variables the hongcheon river basin located in gangwon province of south korea has a humid climate and covers an area of about 1818 km2 with an average height of 140 92 m above mean sea level msl climatic conditions are considered extreme as its location is away from the sea the daily rainfall and runoff data of hongcheon station latitude 37 68 n and longitude 127 88 w were utilized in current research average precipitation and relative humidity of this catchment were reported as 1405 mm and 69 5 per year respectively the rainfall data has been collected and managed by the korea meteorological administration kma since september 1971 https data kma go kr the runoff data was available since 1997 and has been recorded and arranged in the water resources management information system wamis the complete data for the underlying period however was not available because the some periods were missing from wamis http www wamis go kr the data available e g daily river flow and precipitation were divided into two phases approximately 75 2003 2010 of the dataset was used for the calibration stage and the remainder of the dataset was kept for the validation stage 2011 2013 until now the public agencies e g momp kma and wamis have not used m5tree and mars models for forecasting river flow in iran and south korea the schematic diagrams of kordkheyl and hongcheon stations are presented in fig 2 a and b in addition the other details of dataset and statistics of parameters used for proposed models are given in table 1 it is apparent from the table 1 that the rainfall data in hongcheon station has the highest skewed distribution 4 model assessment criteria to evaluate eemd m5tree and eemd mars models performance compared with m5tree and mars models six different statistical criteria were used which are defined as 1 nash sutcliffe efficiency nse this criterion is taken into account to evaluate the ability of hydrological models with higher values indicated better fit between observed and forecasted river flow nash and sutcliffe 1970 if the squared difference between observed and forecasted river flow is relatively large to concur with the variance in observed river flow values nse criterion will be zero if nse criterion is negative the results indicate that the observed mean is a better predictor than the model wilcox et al 1990 if nse criterion is equal to one it indicates a perfect model legates and mccabe 1999 however garrick et al 1978 reported that even for poorly fitted models nse criterion can be relatively high 0 8 while the best models may not generate high values it can be formulated as following eq 8 8 nse 1 i 1 n q obs q for 2 i 1 n q obs q obs 2 2 root mean square error rmse the discrepancy between observed and forecasted values can be shown using rmse criterion a value of zero reflects perfect forecasting the rmse criterion can provide information for model s forecasting skill and quantify the goodness of fit relevant to high river flow values chai and draxler 2014 the rmse criterion can be biased in favor of the peaks and higher magnitudes that will exhibit the greatest error dawson et al 2007 consequently rmse criterion must be used for model evaluation to obtain accuracy in absolute units deo et al 2019 it can be expressed as following eq 9 9 rmse 1 n i 1 n q obs q for 2 3 willmott s index of agreement wi wi criterion as a standardized measure of the degree for model estimation error varies between zero and one so that a value of zero asserts no agreement at all while one indicates a perfect match willmott et al 2012 wi calculates the ratio of mean square error mse and can provide an advantage over rmse willmott 1984 it can be shown as following eq 10 10 wi 1 i 1 n q obs q for 2 i 1 n q for q obs q obs q obs 2 4 legates mccabe s index lmi this criterion considers absolute values for computation and gives errors and differences the appropriate weights legates and mccabe 1999 lmi criterion is not inflated by the squared values and is less sensitive to outliers making it simple and easy to interpret this criterion varies from zero to one with higher values indicating a better fit of the model deo et al 2019 it can be arranged as following eq 11 11 lmi 1 i 1 n q for q for i 1 n q obs q obs 5 peak flow criterion pfc this value plays a significant role in monitoring the extreme events including flood which can be evaluated by pfc to achieve the efficient model a pfc equals to zero represents a perfect fit of model it can be provided as following eq 12 12 pfc i 1 np q obs q for 2 q obs 2 0 25 i 1 np q obs q for 2 0 5 6 low flow criterion lfc lfc is used to assess the accuracy of proposed forecasting models and has same conception of pfc with the difference that it considers low quantities of flow also lower lfc means that the model provides precise predictions and a better fit it can be presented as following eq 13 13 lfc i 1 nl q obs q for 2 q obs 2 0 25 i 1 nl q obs q for 2 0 5 where q obs and q for are observed and forecasted values respectively q obs and q for are the average of observed and forecasted values respectively np is the number of river flow observations greater than one third of mean flow nl is the number of river flow observations less than one third of mean flow and n is the length of time series data the underlying performance criteria can be found as common statistical criteria from the previous literature nash and sutcliffe 1970 legates and mccabe 1999 willmott et al 2012 and each of them has been proposed for a specific purpose the addressed statistical criteria e g nse rmse wi and lmi used in current research may not illustrate the models performance in terms of extreme events such as flood and lack of river flow that is one model may forecast the mean values of streamflow accurately but cannot forecast the high and low values of river flow i e extreme events also it is crucial to assess and monitor the extreme values using pfc and lfc criteria for forecasting of extreme events 5 model applications 5 1 optimum model structure the selection of optimum input variables can be considered as one of the most important tasks for developing accurate forecasting model coulibaly et al 2001 jain et al 2008 kim and singh 2014 the characteristics of river flow cannot be distinguished with small number of input variables accurately similarly the excessive number of input variables can cause overfitting problems rezaie balf et al 2017a b it is clear that the input variables in current study are precipitation and river flow the antecedent values of input variables can be interpreted to represent catchment characteristics for the purpose of developing a data driven model the purpose of this study is to forecast one and multi day ahead river flow of kordkheyl and hongcheon stations so antecedent river flow can be used as a predictor first precipitation and river flow with different lag time are considered as a basic input structure 14 q t 1 f p t 2 p t 1 p t q t 4 q t 3 q t 2 q t 1 q t where t 1 t 2 t 3 and t 4 denote the different daily antecedent values of time series t then to assess the optimum input variables for forecasting each lag time series diverse linear regression functions were accomplished using the best subsets regression tool of minitab software the effective input variables were determined using mallows coefficient cp since it is an efficient approach to determine the subset of effective subseries where there are a number of potential input variables thereafter the selected input variables were ready for next phase 5 2 models implementation based on data pre processing technique the main purpose of eemd m5tree and eemd mars models is to forecast one and multi day ahead river flow for different catchments fig 3 demonstrates the proposed approach for three main steps to enhance the river flow forecasting where p is daily precipitation q is daily river flow res represents the residual error k means kordkheyl station and h is hongcheon station step 1 eemd procedure is utilized to decompose the original input and output observed time series y t into several imf components ci t i 1 2 3 n and one residual component rn t step 2 for each extracted imf component and the residual component for example imf1 the m5tree and mars models are established as river flow forecasting tools to simulate the decomposed imf and residual components and to calculate each component using the same sub series imf1 of input variables respectively step 3 the forecasted values of all extracted imf and residue components using m5tree and mars models are aggregated to generate the river flow to sum up eemd m5tree and eemd mars models established the main idea of decomposition and ensemble the forecasting processes can be simplified by decomposition and the ensemble is utilized to formulate a consensus forecasting on the original time series 5 3 standalone and hybrid models for river flow forecasting to construct m5tree model the regression and model trees were utilized for model tree the number of trees should be selected to improve the non linear performance of one day ahead river flow forecasting in advance in current research the number of trees was varied between 1 and 200 for m5tree model in addition m5tree model also applied trial and error to decide the optimum model s parameters since two stations and four step ahead were used for forecasting river flow eight output variables can be calculated using m5tree model in addition each output was decomposed from imf1 to imf13 for forecasting one and multi day ahead river flow that is 104 13 8 m5tree model s parameters e g leaves and number of equations were calculated this study used an open source code areslab to implement the main functions of mars model in mars model three main parameters i e degree of interaction d the number of bfs in the forward step nk and the smoothing parameter s in gcv affect its performance significantly and hence they should be optimized it is a generally accepted procedure to perform a sensitivity analysis by changing these three parameters to obtain the optimum values for model fitting with training data overall when the difference between observed and forecasted river flow is minimal the values of parameter were chosen as the optimal ones in this study the parameters e g d nk and s were varied from 1 to 2 d 10 to 50 nk and 2 to 4 s by increments of 1 1 and 0 5 respectively a 10 fold cross validation method was also applied to avoid model performance appraisal bias results of linear regression analysis to forecast q t 1 for both stations are illustrated in fig 4 six kordkheyl station and five hongcheon station variables were selected to minimize mallows coefficient cp and variance coefficients in this regard the effective input variables for both stations were defined as following eqs 15 and 16 river flow at kordkheyl station 15 q k t 1 f p k t 1 p k t q k t 3 q k t 2 q k t 1 q k t river flow at hongcheon station 16 q h t 1 f p h t 1 p h t q h t 2 q h t 1 q h t the secondary objective of present research was to investigate the proposed model s ability to forecast multi day ahead river flow of both stations thus standalone and hybrid models were employed to forecast two three and four day ahead river flow to implement the proposed models six input combinations were considered for each output variable corresponding to two three and four day ahead river flow forecasting respectively 6 results and discussion 6 1 results for one day ahead river flow forecasting results from standalone m5tree and mars and hybrid eemd m5tree and eemd mars models were evaluated for forecasting one day ahead river flow at two different stations table 2 shows the statistical criteria for one day ahead river flow forecasting in calibration and validation stages the obtained statistical criteria indicated that eemd m5tree and eemd mars models were able to forecast accurate one day ahead river flow as compared to standalone models for both stations at kordkheyl station the performance of mars model nse 0 791 rmse 10 584 m3 s wi 0 930 and lmi 0 718 was slightly better than that of m5tree model in calibration stage in validation stage however m5tree model nse 0 781 rmse 7 943 m3 s wi 0 940 and lmi 0 684 forecasted river flow accurately better than mars model in the case of hybrid models eemd m5tree model enhanced m5tree model by a 26 reduction in rmse criterion and increased accuracy for nse wi and lmi criterion by 13 5 and 12 in calibration stage in validation stage eemd mars model improved model accuracy by 9 in the nse criterion compared to mars model as can be seen from table 2 the hybrid models i e eemd m5tree and eemd mars were able to forecast the extreme values resulting in small values of pfc 0 311 and 0 249 and lfc 0 295 and 0 236 at hongcheon station the performance of mars model nse 0 768 rmse 53 293 m3 s and wi 0 930 was better than that of m5tree model results of lmi criterion however indicated that the performance of m5tree model was better than that of mars model 0 784 vs 0 751 in calibration stage in validation stage the performance of mars model nse 0 736 rmse 50 469 m3 s and wi 0 916 was better than that of m5tree model results of lmi criterion however indicated that the performance of m5tree model was better than that of mars model 0 711 vs 0 694 results of lmi criterion provided the same pattern in calibration and validation stages in the case of hybrid models eemd mars model improved m5tree mars and eemd m5tree models in nse criterion by 8 7 and 4 in validation stage in addition eemd mars model enhanced m5tree mars and eemd m5tree models by 12 32 10 36 and 9 92 reduction in rmse criterion respectively applying eemd in conjunction with m5tree and mars models as a consequence improved the models performance significantly in validation stage the scatter plots between observed and forecasted river flow values for one day ahead forecasting using standalone and hybrid models for kordkheyl and hongcheon stations are provided in fig 5 the scatter plots also illustrated that the forecasted values of eemd m5tree and eemd mars models for both stations followed the observations better than those of standalone models it can be seen from fig 6 that eemd mars model can forecast river flow better than other models for one day ahead time series 6 2 results for two three and four day ahead river flow forecasting results of subsets regression using the proposed input combinations are shown in fig 7 the minimum values of mallows coefficient cp and variance coefficients indicated the optimum combination for river flow forecasting the optimum input variables using mallows coefficient cp based on all possible regression analyses are presented in table 3 the obtained results of multi day ahead river flow forecasting are illustrated in tables 4 and 5 for kordkheyl and hongcheon stations respectively it seems that the proposed data pre processing technique could improve the performances of m5tree and mars models effectively for different time steps and both stations it can be concluded that the hybrid models could depict the efficient and reliable values for forecasting river flow up to two day ahead including acceptable values that can give a good indication for three and four day ahead comparing tables 4 and 5 indicates that eemd based heuristic regression models e g eemd m5tree and eemd mars produce better forecasting accuracy compared to m5tree and mars models based on six different statistical criteria in current study in general the optimum performances of m5tree and mars models for both stations asserted that mars models in validation stage yielded better accuracy compared to m5tree models for two three and four day ahead river flow forecasting e g higher nse wl lmi and lower rmse at kordkheyl station it can be observed from standalone models that the performance of mars model was slightly better than that of m5tree model except for four day ahead river flow forecasting in validation stage furthermore the hybrid models eemd m5tree and eemd mars performed better compared to standalone models m5tree and mars in validation stage more specially rmse criterion of eemd mars model was reduced by 26 22 22 72 and 21 21 for two three and four day ahead river flow forecasting compared to mars model respectively in addition rmse criterion of eemd m5tree model was reduced by 6 37 17 06 and 7 98 for two three and four day ahead river flow forecasting compared to m5tree model respectively at hongcheon station mars model implementing eemd was accurate compared to other models from table 5 for instance a comparison among standalone and hybrid models indicated that eemd mars model nse 0 738 rmse 50 426 m3 s wi 0 930 and lmi 0 568 outperformed eemd m5tree nse 0 678 rmse 55 966 m3 s wi 0 910 and lmi 0 559 mars nse 0 617 rmse 60 985 m3 s wi 0 860 and lmi 0 530 and m5tree nse 0 623 rmse 60 576 m3 s wi 0 890 and lmi 0 546 models for two day ahead river flow forecasting in validation stage respectively to confirm the accuracy of proposed models graphically figs 8 and 9 provide the scatter plots of observed and forecasted values analyzed as a regression between the two datasets for two three and four day ahead river flow forecasting note that in each scatter plot the rmse value and linear equation for standalone and hybrid models have been proposed the scatter plots in figs 8 and 9 illustrate that the forecasted values by m5tree mars eemd m5tree and eemd mars models for two day ahead river flow forecasting were much closer to corresponding observed values than those of three and four day ahead river flow forecasting it was evident from fig 8 that eemd mars model s line of linear agreement between forecasted y and observed x values i e y 0 938x 1 85 where 0 938 and 1 85 are equal to constants a and b was much closer to the ideal line 1 1 compared to m5tree mars and eemd m5tree models in other words eemd mars model can provide accurate forecasting values since the corresponding gradient of 1 1 line of eemd mars model was much closer to the observed values compared to the other three models m5tree mars and eemd m5tree and the bias is small according to results and discussions so far it can be said that unlike eemd mars model other models failed to forecast the extreme conditions i e peak flows this problem can be investigated by comparing the highest ten peak flows based on four day ahead forecasted values of both stations table 6 shows that the maximum difference between forecasted and observed peak flows provided the value of 36 for eemd mars model whereas this error was up to the values of 76 for eemd m5tree model 93 and 89 for mars and m5tree models respectively for kordkheyl station in addition the maximum difference between forecasted and observed peak flows for eemd mars eemd m5tree mars and m5tree models produced the values of 48 70 74 and 75 respectively for hongcheon station it was found that results of one day ahead river flow forecasting produced better accuracy than those of multi day ahead i e two three and four day ahead river flow forecasting of both stations the comparative evaluation of one and multi day ahead river flow forecasting proved that applying eemd in conjunction with m5tree and mars models could reduce model error and produce more accurate forecasting for kordkheyl and hongcheon stations the model accuracy was also appraised using boxplots illustrating the spread of observed and forecasted values obtained by standalone and hybrid models for one and multi day ahead river flow at kordkheyl station the spread of observed and forecasted values by eemd m5tree model appeared similar and the distribution of values forecasted by eemd mars model compared reasonably to observations however m5tree and mars models displayed an unacceptable spread for river flow forecasting in validation stage at hongcheon station where high values of river flow produced the main features in validation stage eemd m5tree model performed better forecasting distribution for median values while eemd mars showed much scattered spread while the standalone models e g m5tree and mars almost led to similar distributions for one and multi day ahead river flow forecasting such distributions did not clearly follow the spread of observed values overall the findings of present research ascertained that with appropriate input selection eemd based heuristic regression models captured the non linear and non stationary dynamics of river flow effectively and could provide the combined and stabilized river flow forecasting although m5tree and mars models forecasted the extreme values of one day ahead river flow successfully they could not be efficient and reliable for two three and four day ahead river flow forecasting since eemd m5tree and eemd mars models produced better results these models could provide a very useful and accurate forecasting tool therefore eemd based heuristic regression models can improve the development and management of water resources using accurate river flow forecasting of both catchments see fig 10 although this study was the first approach to develop and assess eemd m5tree and eemd mars models for river flow forecasting the identified limitations should be addressed for future studies as can be seen from analysis and figures m5tree and mars models can improve the accuracy of one and multi day ahead river flow forecasting when eemd technique is integrated to decompose data time series one of the disadvantages of eemd technique is that its implementation is time consuming because it produces the large numbers of imfs another limitation of eemd technique is the different length of imfs since the features of input variables are different the number of imfs for each variable is not same following the previous researches al musaylh et al 2018 wen et al 2019 imf1 with high frequency values of decomposed river flow had the highest forecasting error among the other imfs therefore it was recommended to apply the other data pre processing techniques to decompose imf1 with several sub series dataset which can enhance model s performance 7 conclusion and future research accurate forecasting of daily river flow is a key problem of interest to water resources engineers scientists and hydrologists this study implements and evaluates the efficiency of standalone m5tree and mars models for two different river basins including kordkheyl station in iran and hongcheon station in south korea mallows coefficient cp is applied to determine the most effective input variables for forecasting one and multi day ahead river flow then the optimum input variables for four forecasting periods one and multi day ahead are decomposed by eemd as a novel data pre processing technique which can reduce the non linear and non stationary nature of river flow and improve the accuracy of river flow forecasting significantly comparing the results of standalone m5tree and mars and hybrid eemd m5tree and eemd mars models using six different statistical criteria and diagnostic plots confirms the robustness and efficiency of the proposed models eemd mars model provides the best accuracy among the proposed models within the range of historical time series one and multi day ahead in current study this indicates that the decomposition and ensemble strategy can improve the accuracy of river flow forecasting effectively thus it can be concluded that eemd mars model may be an alternative tool and promising forecasting technique for complex problems such as river flow with high non linear and non stationary nature moreover this study reveals that the application of a robust data pre processing technique e g eemd has a key role to achieve accurate forecasting of hydrological processes which can lead to reliable and efficient water management for future research the hybrid model coupling data driven approach data pre processing technique and physically based hydrologic model can be suggested as a potential alternative to enhance the forecasting accuracy of hydrological processes i e streamflow rainfall water stage groundwater etc to achieve this goal additional data pre processing techniques such as complete ensemble empirical mode decomposition ceemd improved ceemd and variational mode decomposition vmd can be addressed also the periodic component of input combination and watershed characteristics may be applied to increase the standalone model s accuracy in addition it can be suggested that two phase decomposition can be applied to enhance imf1 forecasting values which consist of high pass signals for future works conflict of interest none declared acknowledgment the authors would like to reveal our extreme appreciation and gratitude to the regional water organization of mazandaran province and meteorological organization of mazandaran province momp in iran and korea meteorological administration kma and water resources management information system wamis south korea this is for providing the meteorological information the second author professor sungwon kim would like to appreciate the financial support from dongyang university south korea in addition the authors also sincerely thank the editor and reviewers for their admirable revision and scientific suggestions 
6604,in identifying groundwater contaminant sources given that the simulation model is computationally inefficient an ensemble surrogate model is proposed to improve the accuracy and robustness of results the proposed ensemble surrogate model in this paper consists of the following three individual surrogate models kriging radial basis functions and least squares support vector machines the adaptive metropolis markov chain monte carlo method is used to assign weights to the three models accuracy and robustness of the ensemble surrogate model were tested on not only conservative contaminants but also contaminants containing chemical reaction the results indicated that the proposed ensemble surrogate model is an effective method to solve the inverse contaminant source identification problems with a high degree of accuracy and short computation time keywords ensemble surrogate model contaminant source identification groundwater contaminant 1 introduction groundwater contaminant source identification gcsi is critical for controlling groundwater contamination protecting groundwater resources effectively and designing remediation strategies for example gcsi is important in environmental forensics and in contamination characterization for regulatory enforcement and liability assessment purposes mirghani et al 2009 a large number of scholars have begun to find ways to use gcsi over the last 30 years through optimization approaches gorelick et al 1983 mahar and datta 2001 zhao et al 2015 probabilistic and geostatistical simulation approaches bagtzoglou et al 1992 neupauer and wilson 2001 butera et al 2013 cupola et al 2015 xu and g√≥mez hern√°ndez 2018 analytical solution and regression approaches sidauruk et al 2010 alapati and kabala 2015 and direct approaches skaggs and kabala 1994 liu and ball 1999 among the solution approaches simulation optimization is one of the most widely used approaches to solving this problem as an equal constraint in the optimization model the simulation model describes the relationship between the source characteristics of the observation wells and the concentration the optimization model is used to find the optimal solution by minimizing the difference between the observed values and model predicted values zhao et al 2015 the simulation codes modflow mcdonald and harbough 1988 and mt3dms zheng and wang 1999 are commonly used to simulate the groundwater flow and solute transport processes respectively the genetic algorithm ga singh and datta 2006 is used in the optimization model to determine the locations and release history of the potential contaminant sources although the simulation optimization method is universal and robust the optimization process must repetitively invoke the simulation model thereby making this technique computationally burdensome hou et al 2015 to improve the operational efficiency in recent research the use of surrogate models instead of simulation models has attracted a great deal of attention research shows that the surrogate models can improve the operational efficiency while ensuring accuracy zhao et al 2016 many models have been used to identify the release history of groundwater contaminant sources during surrogate model construction razavi et al 2012 ouyang et al 2017a such as polynomials fen et al 2009 wang 2003 kriging simpson et al 2001 luo and lu 2014 artificial neural networks ann behzadian et al 2009 khu and werner 2003 radial basis functions rbf mullur and messac 2006 regis and shoemaker 2007 support vector machines svm zhang et al 2009 multivariate adaptive regression splines mars barron and xiao 1991 jin et al 2001 high dimensional model representation ratto et al 2007 sobol 2003 and kernel extreme learning machines jiang et al 2015 etc in most previous studies the most common practice was to construct multiple surrogate models using different individual techniques and then select the best one while discarding the rest hou and lu 2018 however each surrogate model has its limitations and thus the ensemble surrogate models have attracted more and more attention from scholars acar and rais rohani 2009 viana et al 2009 acar 2010 ouyang et al 2017b in this paper we discuss three learning models kriging rbf and least squares support vector machines lssvm wan et al 2005 the models were selected because they are widely used by researchers and having higher accuracy when integrated into an ensemble surrogate model kriging is used because it can model and predict spatial fields and processes at the same time it has the ability to approximate and map complex nonlinear functions rbf has strong classification ability and a fast convergence speed lssvm is capable of capturing complex relations between input and output as well as efficiently extracting structural information from noisy data on the basis of the merits of each model multiple models are assembled the most important way to assemble multiple models is to assign weights to each model to give full play to the advantages of different methods and to make up for the shortcomings of individual model methods a novel ensemble method adaptive metropolis markov chain monte carlo am mcmc is addressed in this paper markov chain monte carlo mcmc was originally used for uncertainty analyses of model parameters tierney 1994 however since the mcmc method can effectively deal with complex high dimensional parameter estimation problems without assuming a normal distribution of predictive variables the global optimal solution of the likelihood function can be obtained with a small amount of computation and therefore this method is used as a random sampling method tian et al 2011 with respect to the weights estimated using the mcmc algorithm the accuracy will largely depend on the sampling method common sampling algorithms include the metropolis hasting m h siddhartha and edward 1995 gibbs sampling smith and roberts 1993 and adaptive metropolis am algorithms haario et al 2001 etc compared to traditional m h and gibbs sampling am no longer needs to determine the recommended distribution of variables in advance but rather it determines the covariance of the initial samples during the sampling process the recommended density i e covariance matrix is adaptively adjusted according to the historical sampling information of the markov chain and parallel computing can be performed to improve the convergence speed of the algorithm the primary purpose of this paper is to improve the accuracy and robustness of gcsi estimation results in previous studies bagtzoglou and hossain 2009 zhao et al 2015 srivastava and singh 2015 an individual surrogate model was often used to solve gcsi problems this study proposes an ensemble surrogate model to prevent the shortcomings of individual surrogate models when searching for the source release history the ensemble surrogate model consists of three individual popular surrogate models the kriging surrogate model the rbf surrogate model and the lssvm surrogate model and is weighted by the am mcmc method this model is assembled with an optimization model to determine the release history of groundwater contaminants to test the accuracy and robustness of the ensemble surrogate model and to ensure that the comparisons of results with the individual surrogate model are more objective all the surrogate models will be applied to two complex cases case one is a situation that many scholars consider to be complicated ayvaz 2010 involving irregular geometry inhomogeneous media transient complex flow and steady state transport conditions in case two the contaminants are no longer stable and conservative but active contaminants with adsorption moreover in case two the study area is larger the number of contaminant sources increases and the contaminant time is prolonged this status indicates that the number of unknown solutions required for the optimization model increases 2 methodology in this research several tools and methods were employed fig 1 depicts the study process with the identification of groundwater contaminant source release history as flowcharts the simulation model is used at the beginning of the study to generate the relationship between source flux and concentrations then three surrogate models are assembled to replace the simulation model finally the optimization model is used to find the release history of groundwater contaminant sources 2 1 simulation model the simulation model is the principal part of the simulation optimization method it acts as an equality constraint in the optimization formulation datta et al 2011 the governing partial differential equation for the steady state flow in a two dimensional aquifer system can be given as follows 1 x i t ij h x j q i j 1 2 where x is the gradient operator vector t ij is the transmissivity tensor and q is the volumetric flux per unit area the partial differential equation that describes the transport of a contaminant in a two dimensional aquifer system can be given as follows pinder and bredehoeft 1968 2 Œ∏ c t x i Œ∏ d ij c x i x i Œ∏ c v i c s q b p n i j 1 2 where Œ∏ is the effective porosity c is the dissolved concentration d i j d u f d i j is the hydrodynamic dispersion tensor vi is the average linear seepage velocity c s is the dissolved concentration in a source or sink flux b is the thickness of the aquifer and in this study the product of the liquid volume disposal rate and the solute concentration of the source is treated as a single variable called source flux singh and datta 2006 and p n is the chemical reaction term and its specific description is as follows 3 p n œÅ b c t œÑ 1 Œ∏ c œÑ 2 œÅ b c where œÅ b is the bulk density of groundwater medium c is the underground solid phase adsorption contaminant concentration œÑ 1 is the first reaction rate of the dissolution term œÑ 2 is the first reaction rate of adsorption term note that eq 2 is related to eq 1 through darcy s law as follows 4 v i k ij Œ∏ h x j i j 1 2 where kij is the hydraulic conductivity and h is the hydraulic head 2 2 surrogate models to decrease the computation time a surrogate model is introduced queipo et al 2005 it is established using the same source flux and concentration that are used in the simulation model 2 3 kriging in this study the kriging surrogate model is one of the three methods which introduced to solve gcsi problems its function can be described as follows 5 f k q g q z q where q is the input data which stands for source flux f k q is the output of the kriging model which stands for concentration g q stands for the global model of the original function and z q represents the gaussian random function the covariance of the z q can be calculated as 6 c o v z q jj z q kk œÉ 2 r r q jj q kk j j k k 1 2 3 n where œÉ 2 is the variance of z q r stands for the correlation function n is the total number of samples and r q jj q kk represents the spatial correlation equation between qjj and qkk of any two sampling points its form is as follows 7 r q jj q kk e x p ii 1 n Œª ii q ii jj q ii kk 2 where q ii jj and q ii kk stand for the iith component of the qjj and qkk n is the number of observation wells Œª ii is the to be known parameter the best fitting value of Œª ii can be estimated using search algorithms that minimize the deviation between the actual observations and the model predictions 2 4 rbf the rbf network consists of three layers of feedforward neural networks including the input layer the hidden layer and the output layer the transformation from the input layer to the hidden layer is nonlinear and the transformation from the hidden layer to the output layer is linear which approximates the minimum error through continuous weight adjustment in the structure the result from the iith hidden unit for the input data q takes the following form 8 y ii q œà q m ii i i 1 2 e where œà is radial basis function mii is a center of the iith unit in the hidden layer q m ii denotes the euclidean norm e is the number of hidden units the gaussian function is the most commonly used activation function and it can be written as 9 y ii q exp q m ii 2 2 Œ≤ 2 i i 1 2 e where Œ≤ is the center width eq 9 indicates that the shorter the distance is between the input vector and the center of the gaussian function the larger the outcome of the hidden unit the activity of the zth output unit in output layer f q will be calculated using the following equation govindaraju and rao 2000 10 f z q ii 1 e w r y ii q Œ∂ z z 1 2 m where w r is the connecting weight between the iith hidden unit and the zth output unit y i q y i i q is the iith hidden unit resulting from all the input data Œ∂ z is the bias term m is the number of total output data 2 5 lssvm lssvm is used to solve the problems of pattern classification and function estimation all the samples are fitted by least square error its function can be described as follows zhang et al 2017 11 f l q w l t œï q d where wl is the weight vector t is the transpose operations d is the bias and œï is mapping to the high dimensional feature space according to the principles of structural risk minimization seeking wl is intended to minimize the results of eq 12 12 u min 1 2 w l 2 1 2 Œ≥ kk 1 n Œæ kk 2 where Œ≥ is a penalty factor and Œæ kk is a function for error control the restrictions are given in eq 13 13 f l q w l t œï q kk d Œæ kk by introducing lagrange multiplier Œº k we can then obtain eq 14 14 l 1 2 w l 2 1 2 Œ≥ kk 1 n Œæ kk 2 kk 1 n Œº kk w l t œï q kk d Œæ kk c kk according to the karush kuhn tucker kkt conditions lssvm takes the partial derivative for both sides of eq 14 as follows 15 l w l 0 w l kk 1 n Œº kk œï q kk l d 0 kk 1 n kk 0 l Œæ kk 0 Œº kk 1 2 Œ≥ kk 1 n Œæ kk l Œº kk 0 w l t œï q kk d Œæ kk c kk 0 then w l and Œæ kk Œæ k are eliminated to obtain linear eq 16 16 œâ Œ≥ 1 i l v l v t œâ Œº d c 0 where c c 1 c 2 cn t Œº Œº 1 Œº 2 Œº n t lv 1 1 1 t i is the unit matrix and œâ is the adjacent matrix œâ qjj qkk œï q jj t œï q kk œï q j j œï q k is the nuclear matrix which is satisfied by the mercer theory figuera et al 2014 the radial basis function has a wide convergence domain and strong generalization ability and thus it is an ideal regression nuclear function it can be expressed as follows in eq 17 17 g q q exp q q 2 2 o 2 where o is the nuclear width parameter which affects the generalization ability of lssvm to obtain the optimal classification hyperplane in the feature space finally the prediction model for lssvm can be expressed as performed in eq 18 18 f l q kk 1 n Œº kk g q kk q d 2 6 ensemble surrogate method to build an ensemble surrogate model we need to weight the three individual surrogate models described above the am mcmc is selected to obtain the weights of the three individual models under the uncertainty f f 1 f 2 fk are collections of surrogate models and the posterior probability of the predicted solute concentrations using multiple simulation models is obtained through the am mcmc according to the law of total probability the specific calculation steps are as follows 1 first initialize t 0 2 randomly produce an initial state w 1 w 2 and w 3 within 0 1 and use it as follows a to calculate the covariance ccov t according to eq 19 19 c cov t c cov 0 t t 0 s d c o v c 0 c 1 c t 1 s d Œµ i d t t 0 where ccov 0 was treated as a diagonal matrix and is equal to 10 of the length of the search range sd is a scaling parameter that depends only on dimension d ensuring an acceptance probability in a reasonable range Œµ is a constant and id denotes a d dimensional identity matrix gelman et al 1996 suggested that sd equals 2 42 d c 0 c 1 ct 1 is the estimated value according to the given samples at time t 1 where c 0 is the initial state b to produce a candidate state w 1 w 2 w 3 c the candidate state ct is accepted with probability Œ¥ in the am algorithm xing et al 2011 20 Œ¥ c t c t 1 m i n 1 p s n c n t c 0 p c n t c 0 p s n c n t 1 c 0 p c n t 1 c 0 where sn is the posterior information at lead time n cn t 1 is the sample value at time t 1 cn t is the new sample value at time t d to generate Œ¥ uniform random number u from 0 1 e if u Œ¥ then accept w 1 w 2 and w 3 otherwise reject w 1 w 2 w 3 3 for t t 1 repeat a e until the number of samples is sufficiently large 4 according to the criterion of the adaptive metropolis convergence gelman and rubin 1992 when am mcmc converges the maximum value of the joint density fmax w 1 w 2 and w 3 of w 1 w 2 and w 3 can be determined at the optimal point w 1 w 2 and w 3 or the mean values of w1 w2 and w3 the ensemble surrogate model can be constructed as follows 21 c ensemble t w 1 c kriging t w 2 c rbf t w 3 c lssvm t t 1 2 n where c ensemble is the output of the ensemble surrogate model based on am mcmc c kriging t is the output of the kriging model c rbf t is the output of the rbf model and c lssvm t is the output of the lssvm model 2 7 optimization model the optimization model uses search algorithms to estimate the best source characteristics that minimize the deviation between the actual observations and model predictions the optimal source identification model can be written as follows 22 m i n t 1 n t k 1 n d c k t c k t 2 as subject to 23 c f q 24 c min c c max 25 q min q q max eq 22 shows the objective function nt is the number of stress periods and nd is the number of sampling locations in eq 23 f q represents the surrogate model that transforms the source flux q into concentrations c in the aquifer it is embedded with the optimization model eqs 24 and 25 present the lower and upper bounds of the concentrations and source fluxes respectively 2 8 criteria for surrogate evaluation some statistical indicators namely the relative error mean absolute percentage error and coefficient of determination were used to test the performance of the models 1 the relative error re for a particular parameter is defined as follows and is expressed as a percent 26 re q t q t q t 100 where t is the number of data points q t is actual source flux and q t is the identified source flux 2 the coefficient of determination r2 reflects the agreement between the actual and predicted values and ranges from 0 to 1 the r2 can be expressed as follows 27 r 2 1 t 1 n q t q t 2 t 1 n q t q t 2 where q t is the mean of the actual values 2 9 incorporating measurement errors the performance of the simulation optimization models is usually evaluated for the measurement error conditions this is a realistic approach since the entire field and or laboratory studies may include some measurement errors and thus the observed concentration involves uncertainty singh and datta 2006 therefore the actual concentrations shown in eq 13 are perturbed by a normally distributed error the perturbed concentrations are stated as follows 28 c c Œµ a c where c is the perturbed concentrations Œµ is the error matrix which represents the level of noise present in the data if a 0 10 the noise level is low if 0 10 a 0 15 the noise level is moderate if a 0 15 the noise level is high 2 10 numerical applications to test the applicability of the proposed methodology two relatively complex cases were selected tamer ayvaz 2010 zhao et al 2016 the first one is a well known aquifer which was chosen in many studies in the literature ayvaz 2010 zhao et al 2015 the second case is more complicated because chemical reactions are considered it is assumed that the initial conditions the boundary conditions aquifer parameters and the length of the release time are known 2 11 basic introduction of the aquifer fig 2 a shows the plan view for case one there are two contaminant sources and seven sampling locations one pumping well is present at the center of this aquifer the entire simulation time is 10 years 120 months which was divided into 20 equal stress periods such that each stress period had duration of 6 months the sources were assumed to release contaminants during the first four stress periods that is the unknown parameters of the optimization model 8 unknown parameters to be identified for 2 sources 4 stress periods the aquifer has specified head boundary conditions on the upper left ab and lower right cd sides and there is no flow at the other sides the head values on both sides ab and cd are 100 0 m and 80 0 m respectively the aquifer is divided to five different hydraulic conductivity zones and the hydraulic conductivity values in each zone are constant the hydraulic conductivity values are as follows k 1 34 56 m d k 2 17 28 m d k 3 8 64 m d k 4 25 92 m d and k 5 60 48 m d table 1a lists the actual values for the source fluxes the hydrogeological parameters of the aquifer are shown in table 2 fig 2b shows the plan view for case two there are three contaminant sources and eight sampling locations at this aquifer table 1b lists the actual values for the source fluxes the contaminant transport process contains chemical reactions which differs from case one table 3 gives the parameters of the chemical reactions the release of contaminants during the first six stress periods is longer than it is in case one which indicates that the number of unknowns to optimize will increase 18 unknown parameters to be identified for 3 sources 6 stress periods based on the given parameters above the transport process of groundwater contaminants can be obtained by running simulation model fig 3 shows the contaminant plumes in the first tenth and twentieth stress periods for two cases fig 4 shows the breakthrough curves of all observation wells for two cases 2 12 ensemble surrogate method before building the ensemble surrogate models we need to train the surrogate models and we obtained the weight using the output from the trained surrogate models a training set of 450 samples and a test set of 50 samples were used in case one and a training set of 540 samples and a test set of 60 samples were used in case two as obtained by optimal latin hypercube sampling based on the training set kriging rbf and lssvm were employed to construct the ensemble surrogate models the weight set w 1 w 2 w 3 of the three individual models above was sampled using the am mcmc the initial values of weights w1 w2 and w3 were uniformly and randomly generated over the range of 0 1 the number of iterations were set to 10 000 during the initial period and parallel sampling was performed five times we monitored the convergence by estimating the factor by which the scale of the conservative posterior distribution was r 1 2 as shown in fig 5 the value of r 1 2 decreases to 1 after the 4000th iteration this result suggests that the multiple sampling series converged i e the statistical characteristics of the samples have converged 2 13 optimization model for the optimization model the genetic algorithm ga was applied to search the release history in the present study the objective was to minimize the deviation between the actual observations and the model predictions with eight injection well concentrations as the decision variables when the objective function achieved convergence the corresponding decision variables were identified in this paper the ga based solution model solved the problem in 20 generations by setting the population size to 500 therefore it required 10 000 simulations population size number of generations to solve the problem 2 14 robustness of the ensemble surrogate model noise was added to the measurement data to simulate measurement errors resulting from the field and or laboratory tests this task tests the robustness of the proposed solution for different noise levels noise level a ranged from 0 05 to 0 20 3 results and discussion 3 1 analysis of the ensemble surrogate model training sets are the inputs for training three surrogate models when the training is completed its parameters and network are preserved to establish the optimization model in using the weights for one observation well in each case as an example the results are shown in tables 4a and 4b fig 6 a c and d f show the posterior marginal densities histogram of the fifth observation well weights in case one and the first observation well weights in case two respectively based on a kolmogorov smirnov test at the 0 05 significance level the null hypotheses of the normal density are accepted the results show that the weight obtained here is reasonable fig 7 shows the posterior mean and variance iterated traces of the weights from the iterative curve it is known that after 5000 iterations both the mean and variance of the weights have been stable the weights mentioned above were used to establish an ensemble surrogate model to evaluate the stability and the simulation accuracy at different stages of three surrogate models and the ensemble model two prediction values were obtained using the training set and detection set respectively as shown in tables 5a and 5b normally a model can be considered accurate if the r 2 criterion is higher than 0 8 the re measures the degree to which two types of variables are linearly related a perfect fit between the observed and predicted values would give an re of 0 during the training period the coefficients of determination for the three surrogate models were all above 0 98 only when lssvm trains the first observation well of case one is about 0 97 which is within the acceptable range the re values of all the models are approximately 6 during the testing period except for the r 2 of the rbf at the location of the fifth observation well in case one which is 0 8942 the r 2 are all above 0 9 and the r 2 can reach above 0 99 when the individual surrogate model performs well the re value of the predicted results is more than 20 which is reasonable 10 5 is satisfactory and below 5 is excellent from tables 5a and 5b the predicted results of the surrogate models are acceptable and satisfactory overall the surrogate models after training are excellent and can be used for optimization it can be seen from the table that the ensemble surrogate model performs better than the single surrogate models according to further comparisons and analyses we found the following results 1 the kriging surrogate model performs well but it is slightly worse at predicting near distance observation wells for example the first observation well of the three surrogate models was checked for case one and case two the kriging performance is inferior to the other two models 2 when testing two cases the lssvm surrogate model is not the best during the verification period but the prediction period is still very stable that is lssvm is suitable for dealing with less complex data 3 by comparing the performance of the rbf surrogate model in the two cases it can be observed that it generally performs better in case two than in case one this result also occurs because the numbers of training data are different between the two cases as the number of training data increases the rbf performance is better 4 assembling three surrogate models improved the performance of the surrogate models the ensemble surrogate model is applied to the prediction and by means of analyzing and comparing the forecast error the validity of this model is verified through the two criteria standards the prediction results of the ensemble surrogate model are satisfactory 5 for the same observation well although the prediction accuracy levels of the individual surrogate models are quite different the performance of the ensemble surrogate model is still excellent 6 by comparing the performance of the ensemble surrogate model in two cases the number of values needed to be predicted increases in case two which leads to the decrease of prediction accuracy however the difference in prediction accuracy between the two cases is small it still has a high accuracy in case two thus the proposed surrogate model could also describe the relationship between source flux and concentrations accurately when the simulation model contains chemical reactions the training results of a model should be evaluated from two perspectives namely the overall fitting degree and the extreme value prediction the above section compares and analyzes the overall quality of the four surrogate models fig 8 shows an analysis of an extreme values prediction for each surrogate model fig 8 shows that the prediction obtained using kriging rbf lssvm and the ensemble model during the validation period the match between the observed values and the predicted values by the four models was also investigated using scatter plots with 1 1 lines in fig 8 a g clearly the simulated points of the predictive values yielded by the ensemble model are closer to the 1 1 line in all the wells which indicates that the ensemble model is better than the other three models at predictions in this study as well it can be concluded that models such as the kriging rbf and lssvm show high prediction accuracy for stable data series in which there are few peaks and valleys but they were unable to maintain their accuracy for the data series owing to many abnormal values to predict the maximum value lssvm and kriging overestimate the outcomes but the rbf estimation is low to predict the minimum value the three model predictions are basically the same in other words compared with the individual surrogate models the ensemble surrogate model performs more stably and its prediction accuracy is higher 3 2 analysis of optimization results fig 9 shows the optimization results by the genetic algorithm from the 300th generation the iteration process tends to be smooth and steady after then the identified results are obtained table 6 shows that the accuracy of ensemble surrogate model is the highest compared with three other individual surrogate models for different hydrogeological conditions ensemble surrogate models show favorable stability and adaptability therefore this ensemble surrogate model was selected to replace the individual surrogate models in the optimization model fig 10 shows the result of this optimization considering the uncertainty of the final optimization results the probability of the confidence interval is 95 the confidence interval shows the extent to which the true value of the parameter has a certain probability of falling around the result this result is also shown in fig 10 however the actual natural environment is very complex and so the noise was considered in this paper additional criteria were used to test the strategy 5 10 15 and 20 the corresponding results are shown in table 7 as the noise level values increased the corresponding values of r 2 decreased and re increased it can be observed in table 7 that the optimal results of the ensemble surrogate model are almost unaffected at lower noise levels for two cases under the high noise level the model optimization results are still in the acceptable range in comparing case one and case two it can be observed that the result of case two is worse than that of case one this finding occurs because the complexity and number of parameters in case two is the larger however the differences of the identified results between the two cases are quite small the identified results for case two are also satisfactory it reflects that the proposed ensemble surrogate model not only can apply to the well known aquifer model case one under all kinds of noise level but also the more complex aquifer containing chemical reactions when the noise level is low or moderate 4 conclusions to identify the release history of groundwater contaminant sources efficiently and accurately an ensemble surrogate model is proposed in this study it assembles three individual surrogate models kriging rbf and lssvm by am mcmc method the performance evaluations for different scenarios of ensemble surrogate model availability are presented it is expected that the proposed ensemble surrogate model overcomes some of the individual surrogate model accuracy problem moreover the proposed methodology is capable of solving the source identification problem for both conservative contaminant and contaminant containing chemical reaction this is an important advantage compared with other studies because many contaminants have chemical reactions in practice in most cases in terms of the disadvantage of the proposed ensemble surrogate model the time spent on data generation is long if the number of decision variables is large acknowledgments this research was supported by the national key r d program of china no 2017yfc0406004 the national natural science foundation of china no 51109036 no 41807196 no 41672232 the postdoctoral science foundation of china no 2018m641793 the natural science foundation of heilongjiang province of china no e2015024 the research fund for the doctoral program of higher education of china no 20112325120009 the projects for science and technology development of water conservancy bureau in heilongjiang province of china no 201402 no 201404 no 201501 and the academic backbones foundation of northeast agricultural university no 16xg11 
6604,in identifying groundwater contaminant sources given that the simulation model is computationally inefficient an ensemble surrogate model is proposed to improve the accuracy and robustness of results the proposed ensemble surrogate model in this paper consists of the following three individual surrogate models kriging radial basis functions and least squares support vector machines the adaptive metropolis markov chain monte carlo method is used to assign weights to the three models accuracy and robustness of the ensemble surrogate model were tested on not only conservative contaminants but also contaminants containing chemical reaction the results indicated that the proposed ensemble surrogate model is an effective method to solve the inverse contaminant source identification problems with a high degree of accuracy and short computation time keywords ensemble surrogate model contaminant source identification groundwater contaminant 1 introduction groundwater contaminant source identification gcsi is critical for controlling groundwater contamination protecting groundwater resources effectively and designing remediation strategies for example gcsi is important in environmental forensics and in contamination characterization for regulatory enforcement and liability assessment purposes mirghani et al 2009 a large number of scholars have begun to find ways to use gcsi over the last 30 years through optimization approaches gorelick et al 1983 mahar and datta 2001 zhao et al 2015 probabilistic and geostatistical simulation approaches bagtzoglou et al 1992 neupauer and wilson 2001 butera et al 2013 cupola et al 2015 xu and g√≥mez hern√°ndez 2018 analytical solution and regression approaches sidauruk et al 2010 alapati and kabala 2015 and direct approaches skaggs and kabala 1994 liu and ball 1999 among the solution approaches simulation optimization is one of the most widely used approaches to solving this problem as an equal constraint in the optimization model the simulation model describes the relationship between the source characteristics of the observation wells and the concentration the optimization model is used to find the optimal solution by minimizing the difference between the observed values and model predicted values zhao et al 2015 the simulation codes modflow mcdonald and harbough 1988 and mt3dms zheng and wang 1999 are commonly used to simulate the groundwater flow and solute transport processes respectively the genetic algorithm ga singh and datta 2006 is used in the optimization model to determine the locations and release history of the potential contaminant sources although the simulation optimization method is universal and robust the optimization process must repetitively invoke the simulation model thereby making this technique computationally burdensome hou et al 2015 to improve the operational efficiency in recent research the use of surrogate models instead of simulation models has attracted a great deal of attention research shows that the surrogate models can improve the operational efficiency while ensuring accuracy zhao et al 2016 many models have been used to identify the release history of groundwater contaminant sources during surrogate model construction razavi et al 2012 ouyang et al 2017a such as polynomials fen et al 2009 wang 2003 kriging simpson et al 2001 luo and lu 2014 artificial neural networks ann behzadian et al 2009 khu and werner 2003 radial basis functions rbf mullur and messac 2006 regis and shoemaker 2007 support vector machines svm zhang et al 2009 multivariate adaptive regression splines mars barron and xiao 1991 jin et al 2001 high dimensional model representation ratto et al 2007 sobol 2003 and kernel extreme learning machines jiang et al 2015 etc in most previous studies the most common practice was to construct multiple surrogate models using different individual techniques and then select the best one while discarding the rest hou and lu 2018 however each surrogate model has its limitations and thus the ensemble surrogate models have attracted more and more attention from scholars acar and rais rohani 2009 viana et al 2009 acar 2010 ouyang et al 2017b in this paper we discuss three learning models kriging rbf and least squares support vector machines lssvm wan et al 2005 the models were selected because they are widely used by researchers and having higher accuracy when integrated into an ensemble surrogate model kriging is used because it can model and predict spatial fields and processes at the same time it has the ability to approximate and map complex nonlinear functions rbf has strong classification ability and a fast convergence speed lssvm is capable of capturing complex relations between input and output as well as efficiently extracting structural information from noisy data on the basis of the merits of each model multiple models are assembled the most important way to assemble multiple models is to assign weights to each model to give full play to the advantages of different methods and to make up for the shortcomings of individual model methods a novel ensemble method adaptive metropolis markov chain monte carlo am mcmc is addressed in this paper markov chain monte carlo mcmc was originally used for uncertainty analyses of model parameters tierney 1994 however since the mcmc method can effectively deal with complex high dimensional parameter estimation problems without assuming a normal distribution of predictive variables the global optimal solution of the likelihood function can be obtained with a small amount of computation and therefore this method is used as a random sampling method tian et al 2011 with respect to the weights estimated using the mcmc algorithm the accuracy will largely depend on the sampling method common sampling algorithms include the metropolis hasting m h siddhartha and edward 1995 gibbs sampling smith and roberts 1993 and adaptive metropolis am algorithms haario et al 2001 etc compared to traditional m h and gibbs sampling am no longer needs to determine the recommended distribution of variables in advance but rather it determines the covariance of the initial samples during the sampling process the recommended density i e covariance matrix is adaptively adjusted according to the historical sampling information of the markov chain and parallel computing can be performed to improve the convergence speed of the algorithm the primary purpose of this paper is to improve the accuracy and robustness of gcsi estimation results in previous studies bagtzoglou and hossain 2009 zhao et al 2015 srivastava and singh 2015 an individual surrogate model was often used to solve gcsi problems this study proposes an ensemble surrogate model to prevent the shortcomings of individual surrogate models when searching for the source release history the ensemble surrogate model consists of three individual popular surrogate models the kriging surrogate model the rbf surrogate model and the lssvm surrogate model and is weighted by the am mcmc method this model is assembled with an optimization model to determine the release history of groundwater contaminants to test the accuracy and robustness of the ensemble surrogate model and to ensure that the comparisons of results with the individual surrogate model are more objective all the surrogate models will be applied to two complex cases case one is a situation that many scholars consider to be complicated ayvaz 2010 involving irregular geometry inhomogeneous media transient complex flow and steady state transport conditions in case two the contaminants are no longer stable and conservative but active contaminants with adsorption moreover in case two the study area is larger the number of contaminant sources increases and the contaminant time is prolonged this status indicates that the number of unknown solutions required for the optimization model increases 2 methodology in this research several tools and methods were employed fig 1 depicts the study process with the identification of groundwater contaminant source release history as flowcharts the simulation model is used at the beginning of the study to generate the relationship between source flux and concentrations then three surrogate models are assembled to replace the simulation model finally the optimization model is used to find the release history of groundwater contaminant sources 2 1 simulation model the simulation model is the principal part of the simulation optimization method it acts as an equality constraint in the optimization formulation datta et al 2011 the governing partial differential equation for the steady state flow in a two dimensional aquifer system can be given as follows 1 x i t ij h x j q i j 1 2 where x is the gradient operator vector t ij is the transmissivity tensor and q is the volumetric flux per unit area the partial differential equation that describes the transport of a contaminant in a two dimensional aquifer system can be given as follows pinder and bredehoeft 1968 2 Œ∏ c t x i Œ∏ d ij c x i x i Œ∏ c v i c s q b p n i j 1 2 where Œ∏ is the effective porosity c is the dissolved concentration d i j d u f d i j is the hydrodynamic dispersion tensor vi is the average linear seepage velocity c s is the dissolved concentration in a source or sink flux b is the thickness of the aquifer and in this study the product of the liquid volume disposal rate and the solute concentration of the source is treated as a single variable called source flux singh and datta 2006 and p n is the chemical reaction term and its specific description is as follows 3 p n œÅ b c t œÑ 1 Œ∏ c œÑ 2 œÅ b c where œÅ b is the bulk density of groundwater medium c is the underground solid phase adsorption contaminant concentration œÑ 1 is the first reaction rate of the dissolution term œÑ 2 is the first reaction rate of adsorption term note that eq 2 is related to eq 1 through darcy s law as follows 4 v i k ij Œ∏ h x j i j 1 2 where kij is the hydraulic conductivity and h is the hydraulic head 2 2 surrogate models to decrease the computation time a surrogate model is introduced queipo et al 2005 it is established using the same source flux and concentration that are used in the simulation model 2 3 kriging in this study the kriging surrogate model is one of the three methods which introduced to solve gcsi problems its function can be described as follows 5 f k q g q z q where q is the input data which stands for source flux f k q is the output of the kriging model which stands for concentration g q stands for the global model of the original function and z q represents the gaussian random function the covariance of the z q can be calculated as 6 c o v z q jj z q kk œÉ 2 r r q jj q kk j j k k 1 2 3 n where œÉ 2 is the variance of z q r stands for the correlation function n is the total number of samples and r q jj q kk represents the spatial correlation equation between qjj and qkk of any two sampling points its form is as follows 7 r q jj q kk e x p ii 1 n Œª ii q ii jj q ii kk 2 where q ii jj and q ii kk stand for the iith component of the qjj and qkk n is the number of observation wells Œª ii is the to be known parameter the best fitting value of Œª ii can be estimated using search algorithms that minimize the deviation between the actual observations and the model predictions 2 4 rbf the rbf network consists of three layers of feedforward neural networks including the input layer the hidden layer and the output layer the transformation from the input layer to the hidden layer is nonlinear and the transformation from the hidden layer to the output layer is linear which approximates the minimum error through continuous weight adjustment in the structure the result from the iith hidden unit for the input data q takes the following form 8 y ii q œà q m ii i i 1 2 e where œà is radial basis function mii is a center of the iith unit in the hidden layer q m ii denotes the euclidean norm e is the number of hidden units the gaussian function is the most commonly used activation function and it can be written as 9 y ii q exp q m ii 2 2 Œ≤ 2 i i 1 2 e where Œ≤ is the center width eq 9 indicates that the shorter the distance is between the input vector and the center of the gaussian function the larger the outcome of the hidden unit the activity of the zth output unit in output layer f q will be calculated using the following equation govindaraju and rao 2000 10 f z q ii 1 e w r y ii q Œ∂ z z 1 2 m where w r is the connecting weight between the iith hidden unit and the zth output unit y i q y i i q is the iith hidden unit resulting from all the input data Œ∂ z is the bias term m is the number of total output data 2 5 lssvm lssvm is used to solve the problems of pattern classification and function estimation all the samples are fitted by least square error its function can be described as follows zhang et al 2017 11 f l q w l t œï q d where wl is the weight vector t is the transpose operations d is the bias and œï is mapping to the high dimensional feature space according to the principles of structural risk minimization seeking wl is intended to minimize the results of eq 12 12 u min 1 2 w l 2 1 2 Œ≥ kk 1 n Œæ kk 2 where Œ≥ is a penalty factor and Œæ kk is a function for error control the restrictions are given in eq 13 13 f l q w l t œï q kk d Œæ kk by introducing lagrange multiplier Œº k we can then obtain eq 14 14 l 1 2 w l 2 1 2 Œ≥ kk 1 n Œæ kk 2 kk 1 n Œº kk w l t œï q kk d Œæ kk c kk according to the karush kuhn tucker kkt conditions lssvm takes the partial derivative for both sides of eq 14 as follows 15 l w l 0 w l kk 1 n Œº kk œï q kk l d 0 kk 1 n kk 0 l Œæ kk 0 Œº kk 1 2 Œ≥ kk 1 n Œæ kk l Œº kk 0 w l t œï q kk d Œæ kk c kk 0 then w l and Œæ kk Œæ k are eliminated to obtain linear eq 16 16 œâ Œ≥ 1 i l v l v t œâ Œº d c 0 where c c 1 c 2 cn t Œº Œº 1 Œº 2 Œº n t lv 1 1 1 t i is the unit matrix and œâ is the adjacent matrix œâ qjj qkk œï q jj t œï q kk œï q j j œï q k is the nuclear matrix which is satisfied by the mercer theory figuera et al 2014 the radial basis function has a wide convergence domain and strong generalization ability and thus it is an ideal regression nuclear function it can be expressed as follows in eq 17 17 g q q exp q q 2 2 o 2 where o is the nuclear width parameter which affects the generalization ability of lssvm to obtain the optimal classification hyperplane in the feature space finally the prediction model for lssvm can be expressed as performed in eq 18 18 f l q kk 1 n Œº kk g q kk q d 2 6 ensemble surrogate method to build an ensemble surrogate model we need to weight the three individual surrogate models described above the am mcmc is selected to obtain the weights of the three individual models under the uncertainty f f 1 f 2 fk are collections of surrogate models and the posterior probability of the predicted solute concentrations using multiple simulation models is obtained through the am mcmc according to the law of total probability the specific calculation steps are as follows 1 first initialize t 0 2 randomly produce an initial state w 1 w 2 and w 3 within 0 1 and use it as follows a to calculate the covariance ccov t according to eq 19 19 c cov t c cov 0 t t 0 s d c o v c 0 c 1 c t 1 s d Œµ i d t t 0 where ccov 0 was treated as a diagonal matrix and is equal to 10 of the length of the search range sd is a scaling parameter that depends only on dimension d ensuring an acceptance probability in a reasonable range Œµ is a constant and id denotes a d dimensional identity matrix gelman et al 1996 suggested that sd equals 2 42 d c 0 c 1 ct 1 is the estimated value according to the given samples at time t 1 where c 0 is the initial state b to produce a candidate state w 1 w 2 w 3 c the candidate state ct is accepted with probability Œ¥ in the am algorithm xing et al 2011 20 Œ¥ c t c t 1 m i n 1 p s n c n t c 0 p c n t c 0 p s n c n t 1 c 0 p c n t 1 c 0 where sn is the posterior information at lead time n cn t 1 is the sample value at time t 1 cn t is the new sample value at time t d to generate Œ¥ uniform random number u from 0 1 e if u Œ¥ then accept w 1 w 2 and w 3 otherwise reject w 1 w 2 w 3 3 for t t 1 repeat a e until the number of samples is sufficiently large 4 according to the criterion of the adaptive metropolis convergence gelman and rubin 1992 when am mcmc converges the maximum value of the joint density fmax w 1 w 2 and w 3 of w 1 w 2 and w 3 can be determined at the optimal point w 1 w 2 and w 3 or the mean values of w1 w2 and w3 the ensemble surrogate model can be constructed as follows 21 c ensemble t w 1 c kriging t w 2 c rbf t w 3 c lssvm t t 1 2 n where c ensemble is the output of the ensemble surrogate model based on am mcmc c kriging t is the output of the kriging model c rbf t is the output of the rbf model and c lssvm t is the output of the lssvm model 2 7 optimization model the optimization model uses search algorithms to estimate the best source characteristics that minimize the deviation between the actual observations and model predictions the optimal source identification model can be written as follows 22 m i n t 1 n t k 1 n d c k t c k t 2 as subject to 23 c f q 24 c min c c max 25 q min q q max eq 22 shows the objective function nt is the number of stress periods and nd is the number of sampling locations in eq 23 f q represents the surrogate model that transforms the source flux q into concentrations c in the aquifer it is embedded with the optimization model eqs 24 and 25 present the lower and upper bounds of the concentrations and source fluxes respectively 2 8 criteria for surrogate evaluation some statistical indicators namely the relative error mean absolute percentage error and coefficient of determination were used to test the performance of the models 1 the relative error re for a particular parameter is defined as follows and is expressed as a percent 26 re q t q t q t 100 where t is the number of data points q t is actual source flux and q t is the identified source flux 2 the coefficient of determination r2 reflects the agreement between the actual and predicted values and ranges from 0 to 1 the r2 can be expressed as follows 27 r 2 1 t 1 n q t q t 2 t 1 n q t q t 2 where q t is the mean of the actual values 2 9 incorporating measurement errors the performance of the simulation optimization models is usually evaluated for the measurement error conditions this is a realistic approach since the entire field and or laboratory studies may include some measurement errors and thus the observed concentration involves uncertainty singh and datta 2006 therefore the actual concentrations shown in eq 13 are perturbed by a normally distributed error the perturbed concentrations are stated as follows 28 c c Œµ a c where c is the perturbed concentrations Œµ is the error matrix which represents the level of noise present in the data if a 0 10 the noise level is low if 0 10 a 0 15 the noise level is moderate if a 0 15 the noise level is high 2 10 numerical applications to test the applicability of the proposed methodology two relatively complex cases were selected tamer ayvaz 2010 zhao et al 2016 the first one is a well known aquifer which was chosen in many studies in the literature ayvaz 2010 zhao et al 2015 the second case is more complicated because chemical reactions are considered it is assumed that the initial conditions the boundary conditions aquifer parameters and the length of the release time are known 2 11 basic introduction of the aquifer fig 2 a shows the plan view for case one there are two contaminant sources and seven sampling locations one pumping well is present at the center of this aquifer the entire simulation time is 10 years 120 months which was divided into 20 equal stress periods such that each stress period had duration of 6 months the sources were assumed to release contaminants during the first four stress periods that is the unknown parameters of the optimization model 8 unknown parameters to be identified for 2 sources 4 stress periods the aquifer has specified head boundary conditions on the upper left ab and lower right cd sides and there is no flow at the other sides the head values on both sides ab and cd are 100 0 m and 80 0 m respectively the aquifer is divided to five different hydraulic conductivity zones and the hydraulic conductivity values in each zone are constant the hydraulic conductivity values are as follows k 1 34 56 m d k 2 17 28 m d k 3 8 64 m d k 4 25 92 m d and k 5 60 48 m d table 1a lists the actual values for the source fluxes the hydrogeological parameters of the aquifer are shown in table 2 fig 2b shows the plan view for case two there are three contaminant sources and eight sampling locations at this aquifer table 1b lists the actual values for the source fluxes the contaminant transport process contains chemical reactions which differs from case one table 3 gives the parameters of the chemical reactions the release of contaminants during the first six stress periods is longer than it is in case one which indicates that the number of unknowns to optimize will increase 18 unknown parameters to be identified for 3 sources 6 stress periods based on the given parameters above the transport process of groundwater contaminants can be obtained by running simulation model fig 3 shows the contaminant plumes in the first tenth and twentieth stress periods for two cases fig 4 shows the breakthrough curves of all observation wells for two cases 2 12 ensemble surrogate method before building the ensemble surrogate models we need to train the surrogate models and we obtained the weight using the output from the trained surrogate models a training set of 450 samples and a test set of 50 samples were used in case one and a training set of 540 samples and a test set of 60 samples were used in case two as obtained by optimal latin hypercube sampling based on the training set kriging rbf and lssvm were employed to construct the ensemble surrogate models the weight set w 1 w 2 w 3 of the three individual models above was sampled using the am mcmc the initial values of weights w1 w2 and w3 were uniformly and randomly generated over the range of 0 1 the number of iterations were set to 10 000 during the initial period and parallel sampling was performed five times we monitored the convergence by estimating the factor by which the scale of the conservative posterior distribution was r 1 2 as shown in fig 5 the value of r 1 2 decreases to 1 after the 4000th iteration this result suggests that the multiple sampling series converged i e the statistical characteristics of the samples have converged 2 13 optimization model for the optimization model the genetic algorithm ga was applied to search the release history in the present study the objective was to minimize the deviation between the actual observations and the model predictions with eight injection well concentrations as the decision variables when the objective function achieved convergence the corresponding decision variables were identified in this paper the ga based solution model solved the problem in 20 generations by setting the population size to 500 therefore it required 10 000 simulations population size number of generations to solve the problem 2 14 robustness of the ensemble surrogate model noise was added to the measurement data to simulate measurement errors resulting from the field and or laboratory tests this task tests the robustness of the proposed solution for different noise levels noise level a ranged from 0 05 to 0 20 3 results and discussion 3 1 analysis of the ensemble surrogate model training sets are the inputs for training three surrogate models when the training is completed its parameters and network are preserved to establish the optimization model in using the weights for one observation well in each case as an example the results are shown in tables 4a and 4b fig 6 a c and d f show the posterior marginal densities histogram of the fifth observation well weights in case one and the first observation well weights in case two respectively based on a kolmogorov smirnov test at the 0 05 significance level the null hypotheses of the normal density are accepted the results show that the weight obtained here is reasonable fig 7 shows the posterior mean and variance iterated traces of the weights from the iterative curve it is known that after 5000 iterations both the mean and variance of the weights have been stable the weights mentioned above were used to establish an ensemble surrogate model to evaluate the stability and the simulation accuracy at different stages of three surrogate models and the ensemble model two prediction values were obtained using the training set and detection set respectively as shown in tables 5a and 5b normally a model can be considered accurate if the r 2 criterion is higher than 0 8 the re measures the degree to which two types of variables are linearly related a perfect fit between the observed and predicted values would give an re of 0 during the training period the coefficients of determination for the three surrogate models were all above 0 98 only when lssvm trains the first observation well of case one is about 0 97 which is within the acceptable range the re values of all the models are approximately 6 during the testing period except for the r 2 of the rbf at the location of the fifth observation well in case one which is 0 8942 the r 2 are all above 0 9 and the r 2 can reach above 0 99 when the individual surrogate model performs well the re value of the predicted results is more than 20 which is reasonable 10 5 is satisfactory and below 5 is excellent from tables 5a and 5b the predicted results of the surrogate models are acceptable and satisfactory overall the surrogate models after training are excellent and can be used for optimization it can be seen from the table that the ensemble surrogate model performs better than the single surrogate models according to further comparisons and analyses we found the following results 1 the kriging surrogate model performs well but it is slightly worse at predicting near distance observation wells for example the first observation well of the three surrogate models was checked for case one and case two the kriging performance is inferior to the other two models 2 when testing two cases the lssvm surrogate model is not the best during the verification period but the prediction period is still very stable that is lssvm is suitable for dealing with less complex data 3 by comparing the performance of the rbf surrogate model in the two cases it can be observed that it generally performs better in case two than in case one this result also occurs because the numbers of training data are different between the two cases as the number of training data increases the rbf performance is better 4 assembling three surrogate models improved the performance of the surrogate models the ensemble surrogate model is applied to the prediction and by means of analyzing and comparing the forecast error the validity of this model is verified through the two criteria standards the prediction results of the ensemble surrogate model are satisfactory 5 for the same observation well although the prediction accuracy levels of the individual surrogate models are quite different the performance of the ensemble surrogate model is still excellent 6 by comparing the performance of the ensemble surrogate model in two cases the number of values needed to be predicted increases in case two which leads to the decrease of prediction accuracy however the difference in prediction accuracy between the two cases is small it still has a high accuracy in case two thus the proposed surrogate model could also describe the relationship between source flux and concentrations accurately when the simulation model contains chemical reactions the training results of a model should be evaluated from two perspectives namely the overall fitting degree and the extreme value prediction the above section compares and analyzes the overall quality of the four surrogate models fig 8 shows an analysis of an extreme values prediction for each surrogate model fig 8 shows that the prediction obtained using kriging rbf lssvm and the ensemble model during the validation period the match between the observed values and the predicted values by the four models was also investigated using scatter plots with 1 1 lines in fig 8 a g clearly the simulated points of the predictive values yielded by the ensemble model are closer to the 1 1 line in all the wells which indicates that the ensemble model is better than the other three models at predictions in this study as well it can be concluded that models such as the kriging rbf and lssvm show high prediction accuracy for stable data series in which there are few peaks and valleys but they were unable to maintain their accuracy for the data series owing to many abnormal values to predict the maximum value lssvm and kriging overestimate the outcomes but the rbf estimation is low to predict the minimum value the three model predictions are basically the same in other words compared with the individual surrogate models the ensemble surrogate model performs more stably and its prediction accuracy is higher 3 2 analysis of optimization results fig 9 shows the optimization results by the genetic algorithm from the 300th generation the iteration process tends to be smooth and steady after then the identified results are obtained table 6 shows that the accuracy of ensemble surrogate model is the highest compared with three other individual surrogate models for different hydrogeological conditions ensemble surrogate models show favorable stability and adaptability therefore this ensemble surrogate model was selected to replace the individual surrogate models in the optimization model fig 10 shows the result of this optimization considering the uncertainty of the final optimization results the probability of the confidence interval is 95 the confidence interval shows the extent to which the true value of the parameter has a certain probability of falling around the result this result is also shown in fig 10 however the actual natural environment is very complex and so the noise was considered in this paper additional criteria were used to test the strategy 5 10 15 and 20 the corresponding results are shown in table 7 as the noise level values increased the corresponding values of r 2 decreased and re increased it can be observed in table 7 that the optimal results of the ensemble surrogate model are almost unaffected at lower noise levels for two cases under the high noise level the model optimization results are still in the acceptable range in comparing case one and case two it can be observed that the result of case two is worse than that of case one this finding occurs because the complexity and number of parameters in case two is the larger however the differences of the identified results between the two cases are quite small the identified results for case two are also satisfactory it reflects that the proposed ensemble surrogate model not only can apply to the well known aquifer model case one under all kinds of noise level but also the more complex aquifer containing chemical reactions when the noise level is low or moderate 4 conclusions to identify the release history of groundwater contaminant sources efficiently and accurately an ensemble surrogate model is proposed in this study it assembles three individual surrogate models kriging rbf and lssvm by am mcmc method the performance evaluations for different scenarios of ensemble surrogate model availability are presented it is expected that the proposed ensemble surrogate model overcomes some of the individual surrogate model accuracy problem moreover the proposed methodology is capable of solving the source identification problem for both conservative contaminant and contaminant containing chemical reaction this is an important advantage compared with other studies because many contaminants have chemical reactions in practice in most cases in terms of the disadvantage of the proposed ensemble surrogate model the time spent on data generation is long if the number of decision variables is large acknowledgments this research was supported by the national key r d program of china no 2017yfc0406004 the national natural science foundation of china no 51109036 no 41807196 no 41672232 the postdoctoral science foundation of china no 2018m641793 the natural science foundation of heilongjiang province of china no e2015024 the research fund for the doctoral program of higher education of china no 20112325120009 the projects for science and technology development of water conservancy bureau in heilongjiang province of china no 201402 no 201404 no 201501 and the academic backbones foundation of northeast agricultural university no 16xg11 
