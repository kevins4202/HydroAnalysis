index,text
25415,air quality monitoring is insufficient in most countries even though air pollution is a major environmental threat to human health this paper presents the collaborative low cost environmental air quality network clean which consists of a series of guides and tools to develop the hardware and software for building a network of low cost air quality monitors our web platform provides an application programming interface api that allows developers to send the data from their devices to a remote server all data will be available for any user and data consumers collaborators can replicate hardware and firmware components and contribute to the network we present the hardware prototypes for demonstration providing the developed firmware and libraries based on the arduino framework all guides and documentation for contributing to the initiative are freely available clean can be a foundation for building knowledge about air quality issues from a local to global scale and serve for educational purposes keywords air quality monitoring arduino low cost monitoring web application internet of things data availability data will be made available on request 1 introduction air pollution is a major environmental risk to health causing death to an estimated seven million people worldwide every year world health organization 2021 the world health organization world health organization 2021 has evidenced the multiple damages of air pollution on human health the risk of suffering diseases or even death is higher for more vulnerable and marginalized populations such as racial ethnic minorities and people of low socioeconomic status jbaily et al 2022 even though almost 99 of the global population breathes air that exceeds who guideline limits world health organization 2023 air quality monitoring coverage is still low and insufficient munir et al 2019 especially in low and middle income countries spatially representative coverage of air pollution monitoring is essential for effectivelly management of air quality given the high spatio temporal variability of the pollutant concentration especially in urban environments kumar and coauthors 2015 mead and coauthors 2013 air pollutants have been monitored using complex and expensive equipment at fixed locations kang et al 2022 the high costs of these instruments limit their deployment to only a few stations per city leaving large geographical areas uncovered munir et al 2019 and restraining the spatial resolution and distribution of the monitoring networks jiao and coauthors 2016 kumar and coauthors 2015 in brazil for instance the monitoring networks cover a few brazilian cities and their long term performance is often compromised by a lack of maintenance and qualified personnel vormittag et al 2014 2021 seeking a better understanding of the air pollution process and its impacts the idea of ubiquitous sensing has attracted the attention of the air quality community and the use of low cost monitors has gained popularity motlagh and coauthors 2020 kumar and coauthors 2015 due to their versatility and low cost both for purchasing and operation these devices can supplement the spatial and temporal scarcity of certified air quality networks and enable new monitoring applications cite lewis and coauthors 2016 some manufacturers of these low cost devices like aqmesh i blades and libelium provide along with their sensor platforms services for data visualization and analysis which are offered as software as a service saas or platform as a service paas research communities and institutions have provided similar resources as open access tools some of these are the habitat map habitat map 2023 the air quality eggs portal air quality egg 2023 the luft daten project map sensor community 2023 the purpleair map purpleair 2023 together with the foss airsensor r package and the dataviewer web application feenstra et al 2020 collier oxandale et al 2022 for data visualization and analysis the smart citizen map smart citizen 2023 and the uradmonitor network map urad monitor 2023 the cost of purchasing the sensor platforms of these initiatives varies between 60 00 and 3 500 00 usd some of them luft daten project and smart citizen map provide guides for replicating the monitoring devices according to a set of hardware and software instructions as established by their creators these efforts are expanding the air pollution monitoring networks and incorporating communities and citizens into the global debate on air quality topics however access to devices is still limited especially in developing countries the current open source initiatives lack reusability because the hardware software specificities are not flexible modular and documented enough to be used in various scenarios and air quality monitoring applications here we present the collaborative low cost environmental air quality network clean the initiative includes hardware solutions with documentation and tutorials well documented firmware libraries and a web application for real time data visualization and analysis clean facilitates the incorporation of new hardware peripherals and sensors independently from api requirements guaranteeing interaction with the web application independent from the hardware developed for specific monitoring applications 2 methods 2 1 clean overview the clean initiative is a collaborative platform for promoting and facilitating the development of low cost air quality monitors and remote access to air quality information in real time it has four major components i hardware devices ii reusable firmware and object oriented programming libraries based on the arduino framework iii guides and documentation for reproducing the hardware and joining the network and iv and the renovar web application for remote data visualization and access in real time all guides and documentation regarding the development of the hardware and firmware of the devices conceived until now the libraries implemented and the development tools are open and freely available on the clean homepage clean allows the collaboration of other groups and individuals interested in the development of low cost monitoring devices and open data for environmental analysis the renovar web application provides an api that allows several low cost monitoring devices to send their data to a remote server for real time and geo located visualization and storage these data remain openly available for further processing and analysis given the great versatility of low cost sensors many applications could be monitored from diverse scenarios contributing to the availability of high volumes of data fig 1 illustrates the main components of the clean initiative which will be described below 2 2 low cost air quality monitor prototypes we developed two low cost prototypes for measuring air pollutants campo et al 2020 one for fixed monitoring and another for mobile monitoring the hardware of both devices as shown in fig 2 is composed of three main blocks 1 gas transportation 2 sensing and 3 microcontroller the gas transportation stage captures ambient air into the sensors which produce an analog signal proportional to the gas concentration the microcontroller which is a microchip atmega2560 embedded into an arduino mega platform captures the responses of the sensors and transforms them into gas concentration data the hardware also obtains the time and location where each measurement was collected the microcontroller stores this information into a micro sd card and transmits it to a web server hosted at the university superintendence of information technology and communication running the renovar web application a wi fi connection is established by an esp8266 microcontroller for data transmission a real time clock and a gps module provide date time and geolocation information respectively the fixed version of the monitoring devices fig 3 a uses six electrochemical sensors from alphasense and four electrochemical sensors from spec sensors table 1 of appendix a shows the sensor models of each manufacturer used in both the fixed and mobile versions of the air quality monitors the power supply for the fixed node comes from a 12 vdc power source the mobile node fig 3b on the other hand is powered by a 5 vdc power bank via a usb connection the hardware of the devices is open and freely available under license 512022001116 6 from the brazilian national institute of industrial property detailed documentation about the hardware components of the devices and mounting guides can be found in supplementary materials b 1 and b 2 2 3 the clean arduino mega board based on the results obtained by the prototypes and the experiences achieved a more compact and upgraded version was developed for fixed monitoring we named it the clean arduino mega board after the arduino mega microcontroller it uses as the main processor the composition of the hardware is very similar to the prototypes but the modules were mounted on a single pcb fig 4 a 4b and 4c illustrate the pcb project and one of the manufactured boards the pcb was created on eagle software and the project files are available at the repository on this link https codigos ufsc br lcqar hw monit gar sc table 1 in appendix a shows the main hardware components used in the board which requires a supply voltage of 12 v 2a via a p4 power jack it has analogical inputs for 6 alphasense isb sensor boards an rs 485 bus for future expansions three digital outputs and connectors for powering 12 v and 5 v fans depending on the application the board can connect to the internet via wi fi or gprs those connections cannot be used simultaneously which will depend on each application the user can configure the board for using one or the other and will have to adapt the firmware of the arduino microcontroller accordingly 2 4 firmware the firmware of the devices was developed on the arduino framework the code was programmed on the platform io ide installed in the microsoft visual studio code editor vscode currently there are two versions of the firmware one for the fixed monitor and another for the mobile monitor a firmware was also developed for the esp8266 microcontroller embedded on an esp 01 board which in both devices establishes the wi fi connection to the internet for code modularity and re usability a set of libraries was developed which can be utilized for different versions of the firmware and other microcontroller platforms supported in the arduino framework like the nodemcu board for detailed documentation on how to set up the development environment using vscode platformio and the esp 01 board please refer to supplementary material b 3 for the current versions of the monitoring prototypes two main programs were developed for the arduino mega microcontroller the basic sequence of the programs is illustrated in fig 5 a and a more detailed description can be found in supplementary material b 4 the main program of each device is executed within the loop function of the arduino framework this function is executed as an infinite loop that contains the main sequence of instructions to make the code easier to understand review and reuse the main loop was separated into four sequences that run periodically at time intervals labeled as usd time for hard storing the data into the micro sd card http time for sending the data to the remote server sample time for sampling the gas sensors and gps time for reading the geo location of the mobile device for demonstration fig 5a illustrates the flowchart of the main loop with its four sequences new devices with similar hardware configuration could use the same code but are not limited to it the firmware libraries developed aim at modularity and reusability in other microcontroller platforms and other hardware components supported in the arduino framework such as esp8266 from espressif it is also desirable that new collaborators could contribute with libraries for the hardware components they use in their projects in order to expand the range of possible configurations and monitoring applications the libraries developed for the project are distributed in four main packages as shown in fig 5b the packages are the hardware interfaces package the system drivers package the sensors package and the data package the hardware interfaces package groups all the classes and structures used for interfacing the peripheral hardware such as sensors timing modules geo location modules and storage modules the drivers implement functionalities that can be used by the main program independently of the hardware used in each device the sensors package is at the same level as the drivers and could be interpreted as a set of special drivers for the sensors but with the peculiarity of being specific for each manufacturer finally the data package encompasses all the functionalities that are related to the preparation of sensor data for storage and transmission this package abstracts the concentration information acquired by the gas sensors from specific details about the functioning and operation of their hardware as an illustration of how to use the firmware libraries and how they contribute to the re use of the code and ease their maintenance we explain how the rtcdriver class is used in the code of the devices the class rtcdriver is a template class for controlling the functionalities related to an rtc module like updating its time for instance the rtcdriver uses a reference to an rtcinterface which interfaces the hardware of an rtc module the diagram in fig 6 presents an example of implementation the object my rtcinterface interfaces the rtc module with our code while the class rtcdriver controls the module s functionalities within our libraries code this way the core of our code is abstracted from the hardware libraries that a users could choose for more details on the libraries and packages developed please refer to supplementary material b 5 2 5 renovar web application renovar is a web platform that provides air sensor data for visualization and an api for the integration of iot air sensor devices the platform consists of a database a back end service developed in java language using spring boot and a front end application created on angular with ionic and typescript its access is free and open for environmental research and analysis fig 7 a illustrates the main panel of the platform which consists of a map showing the location of the monitoring devices the system receives data from iot devices such as air pollutants concentration temperature and relative humidity the data is stored in a database as time series which can be visualized online on the web platform the software consists of 1 a mysql database that stores the device s readings and other data necessary for the platform such as users registered devices pollutants and units 2 a restful back end developed in java using spring boot which is responsible for gathering data from the database and preparing it for the frontend and 3 the front end developed to be multi platform providing the interface with the user as illustrated in fig 7a and 7b the database backend and frontend are hosted on a server at the federal university of santa catarina fig 8 illustrates the functioning of the service in general the devices collect environmental data and send them into the database through the internet the backend receives requests from the frontend and gathers the necessary data from the database in case the data require some treatment e g calculating average values or filtering the backend executes the required operations and send the processed information back to the frontend the frontend on the other hand implements the interface with the user and outputs the results of the operations requested like visualizing the data as time series and downloading data as csv 3 how to use and collaborate collaborators could make use of our guides and tutorials for reproducing air quality monitors all the monitors registered in the platform will have their data available on clean for visualization and analysis for all users for registering a monitor to clean a username and a password will be required without any cost and then the device and its sensors must be registered a registration form has been made available on the clean homepage 4 conclusions in this work we took the first steps towards a collaborative framework for low cost air quality monitoring we developed the renovar web platform for connecting iot devices and made their data freely available to researchers local authorities and communities we also deployed two monitoring prototypes and presented documentation and guides for replication and upgrade we introduce the clean initiative which promotes collaboration for developing low cost monitoring platforms at lower costs and greater flexibility than the current open access initiatives this initiative can incorporate a broader and more diverse group of environmental monitoring specialists and enthusiasts increase the amount publicly available data and improve the spatio temporal coverage of air quality monitoring especially in developing countries the clean initiative will promote citizen science facilitate the development process of the devices to a larger public and make air quality data available for analysis and visualization the network of collaborators can expand to developers researchers hobbyists and students who could use the tools available at clean for education prototyping personal use and research broader coverage of air quality monitoring will make a considerable amount of data available facilitating access to environmental information to make cities and settlements more inclusive safe resilient and sustainable access to representative and reliable information helps researchers and decision makers to find solutions for smarter and healthier cities in developing countries where access to expensive technology is more limited initiatives like clean are affordable and scalable alternatives to highly expensive monitoring networks that can contribute to sustained economic growth and increased quality of life we believe this initiative would potentiate the expansion of the air monitoring community especially within brazilian territory facilitating access to air pollution information in regions where regulatory data is scarce or even missing software availability the code for the firmware of the prototypes and the firmware libraries is available at https codigos ufsc br lcqar clean firmware the code for the renovar platform is available at https codigos ufsc br lcqar renovar declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments authors would like to thank the fundação de amparo à pesquisa e inovação de santa catarina fapesc for financial support of project number 2018tr499 avaliação do impacto das emissões veiculares queimadas industriais e naturais na qualidade do ar em santa catarina appendix a supplementary data the following is the supplementary data to this article multimedia component 1 multimedia component 1 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2023 105664 
25415,air quality monitoring is insufficient in most countries even though air pollution is a major environmental threat to human health this paper presents the collaborative low cost environmental air quality network clean which consists of a series of guides and tools to develop the hardware and software for building a network of low cost air quality monitors our web platform provides an application programming interface api that allows developers to send the data from their devices to a remote server all data will be available for any user and data consumers collaborators can replicate hardware and firmware components and contribute to the network we present the hardware prototypes for demonstration providing the developed firmware and libraries based on the arduino framework all guides and documentation for contributing to the initiative are freely available clean can be a foundation for building knowledge about air quality issues from a local to global scale and serve for educational purposes keywords air quality monitoring arduino low cost monitoring web application internet of things data availability data will be made available on request 1 introduction air pollution is a major environmental risk to health causing death to an estimated seven million people worldwide every year world health organization 2021 the world health organization world health organization 2021 has evidenced the multiple damages of air pollution on human health the risk of suffering diseases or even death is higher for more vulnerable and marginalized populations such as racial ethnic minorities and people of low socioeconomic status jbaily et al 2022 even though almost 99 of the global population breathes air that exceeds who guideline limits world health organization 2023 air quality monitoring coverage is still low and insufficient munir et al 2019 especially in low and middle income countries spatially representative coverage of air pollution monitoring is essential for effectivelly management of air quality given the high spatio temporal variability of the pollutant concentration especially in urban environments kumar and coauthors 2015 mead and coauthors 2013 air pollutants have been monitored using complex and expensive equipment at fixed locations kang et al 2022 the high costs of these instruments limit their deployment to only a few stations per city leaving large geographical areas uncovered munir et al 2019 and restraining the spatial resolution and distribution of the monitoring networks jiao and coauthors 2016 kumar and coauthors 2015 in brazil for instance the monitoring networks cover a few brazilian cities and their long term performance is often compromised by a lack of maintenance and qualified personnel vormittag et al 2014 2021 seeking a better understanding of the air pollution process and its impacts the idea of ubiquitous sensing has attracted the attention of the air quality community and the use of low cost monitors has gained popularity motlagh and coauthors 2020 kumar and coauthors 2015 due to their versatility and low cost both for purchasing and operation these devices can supplement the spatial and temporal scarcity of certified air quality networks and enable new monitoring applications cite lewis and coauthors 2016 some manufacturers of these low cost devices like aqmesh i blades and libelium provide along with their sensor platforms services for data visualization and analysis which are offered as software as a service saas or platform as a service paas research communities and institutions have provided similar resources as open access tools some of these are the habitat map habitat map 2023 the air quality eggs portal air quality egg 2023 the luft daten project map sensor community 2023 the purpleair map purpleair 2023 together with the foss airsensor r package and the dataviewer web application feenstra et al 2020 collier oxandale et al 2022 for data visualization and analysis the smart citizen map smart citizen 2023 and the uradmonitor network map urad monitor 2023 the cost of purchasing the sensor platforms of these initiatives varies between 60 00 and 3 500 00 usd some of them luft daten project and smart citizen map provide guides for replicating the monitoring devices according to a set of hardware and software instructions as established by their creators these efforts are expanding the air pollution monitoring networks and incorporating communities and citizens into the global debate on air quality topics however access to devices is still limited especially in developing countries the current open source initiatives lack reusability because the hardware software specificities are not flexible modular and documented enough to be used in various scenarios and air quality monitoring applications here we present the collaborative low cost environmental air quality network clean the initiative includes hardware solutions with documentation and tutorials well documented firmware libraries and a web application for real time data visualization and analysis clean facilitates the incorporation of new hardware peripherals and sensors independently from api requirements guaranteeing interaction with the web application independent from the hardware developed for specific monitoring applications 2 methods 2 1 clean overview the clean initiative is a collaborative platform for promoting and facilitating the development of low cost air quality monitors and remote access to air quality information in real time it has four major components i hardware devices ii reusable firmware and object oriented programming libraries based on the arduino framework iii guides and documentation for reproducing the hardware and joining the network and iv and the renovar web application for remote data visualization and access in real time all guides and documentation regarding the development of the hardware and firmware of the devices conceived until now the libraries implemented and the development tools are open and freely available on the clean homepage clean allows the collaboration of other groups and individuals interested in the development of low cost monitoring devices and open data for environmental analysis the renovar web application provides an api that allows several low cost monitoring devices to send their data to a remote server for real time and geo located visualization and storage these data remain openly available for further processing and analysis given the great versatility of low cost sensors many applications could be monitored from diverse scenarios contributing to the availability of high volumes of data fig 1 illustrates the main components of the clean initiative which will be described below 2 2 low cost air quality monitor prototypes we developed two low cost prototypes for measuring air pollutants campo et al 2020 one for fixed monitoring and another for mobile monitoring the hardware of both devices as shown in fig 2 is composed of three main blocks 1 gas transportation 2 sensing and 3 microcontroller the gas transportation stage captures ambient air into the sensors which produce an analog signal proportional to the gas concentration the microcontroller which is a microchip atmega2560 embedded into an arduino mega platform captures the responses of the sensors and transforms them into gas concentration data the hardware also obtains the time and location where each measurement was collected the microcontroller stores this information into a micro sd card and transmits it to a web server hosted at the university superintendence of information technology and communication running the renovar web application a wi fi connection is established by an esp8266 microcontroller for data transmission a real time clock and a gps module provide date time and geolocation information respectively the fixed version of the monitoring devices fig 3 a uses six electrochemical sensors from alphasense and four electrochemical sensors from spec sensors table 1 of appendix a shows the sensor models of each manufacturer used in both the fixed and mobile versions of the air quality monitors the power supply for the fixed node comes from a 12 vdc power source the mobile node fig 3b on the other hand is powered by a 5 vdc power bank via a usb connection the hardware of the devices is open and freely available under license 512022001116 6 from the brazilian national institute of industrial property detailed documentation about the hardware components of the devices and mounting guides can be found in supplementary materials b 1 and b 2 2 3 the clean arduino mega board based on the results obtained by the prototypes and the experiences achieved a more compact and upgraded version was developed for fixed monitoring we named it the clean arduino mega board after the arduino mega microcontroller it uses as the main processor the composition of the hardware is very similar to the prototypes but the modules were mounted on a single pcb fig 4 a 4b and 4c illustrate the pcb project and one of the manufactured boards the pcb was created on eagle software and the project files are available at the repository on this link https codigos ufsc br lcqar hw monit gar sc table 1 in appendix a shows the main hardware components used in the board which requires a supply voltage of 12 v 2a via a p4 power jack it has analogical inputs for 6 alphasense isb sensor boards an rs 485 bus for future expansions three digital outputs and connectors for powering 12 v and 5 v fans depending on the application the board can connect to the internet via wi fi or gprs those connections cannot be used simultaneously which will depend on each application the user can configure the board for using one or the other and will have to adapt the firmware of the arduino microcontroller accordingly 2 4 firmware the firmware of the devices was developed on the arduino framework the code was programmed on the platform io ide installed in the microsoft visual studio code editor vscode currently there are two versions of the firmware one for the fixed monitor and another for the mobile monitor a firmware was also developed for the esp8266 microcontroller embedded on an esp 01 board which in both devices establishes the wi fi connection to the internet for code modularity and re usability a set of libraries was developed which can be utilized for different versions of the firmware and other microcontroller platforms supported in the arduino framework like the nodemcu board for detailed documentation on how to set up the development environment using vscode platformio and the esp 01 board please refer to supplementary material b 3 for the current versions of the monitoring prototypes two main programs were developed for the arduino mega microcontroller the basic sequence of the programs is illustrated in fig 5 a and a more detailed description can be found in supplementary material b 4 the main program of each device is executed within the loop function of the arduino framework this function is executed as an infinite loop that contains the main sequence of instructions to make the code easier to understand review and reuse the main loop was separated into four sequences that run periodically at time intervals labeled as usd time for hard storing the data into the micro sd card http time for sending the data to the remote server sample time for sampling the gas sensors and gps time for reading the geo location of the mobile device for demonstration fig 5a illustrates the flowchart of the main loop with its four sequences new devices with similar hardware configuration could use the same code but are not limited to it the firmware libraries developed aim at modularity and reusability in other microcontroller platforms and other hardware components supported in the arduino framework such as esp8266 from espressif it is also desirable that new collaborators could contribute with libraries for the hardware components they use in their projects in order to expand the range of possible configurations and monitoring applications the libraries developed for the project are distributed in four main packages as shown in fig 5b the packages are the hardware interfaces package the system drivers package the sensors package and the data package the hardware interfaces package groups all the classes and structures used for interfacing the peripheral hardware such as sensors timing modules geo location modules and storage modules the drivers implement functionalities that can be used by the main program independently of the hardware used in each device the sensors package is at the same level as the drivers and could be interpreted as a set of special drivers for the sensors but with the peculiarity of being specific for each manufacturer finally the data package encompasses all the functionalities that are related to the preparation of sensor data for storage and transmission this package abstracts the concentration information acquired by the gas sensors from specific details about the functioning and operation of their hardware as an illustration of how to use the firmware libraries and how they contribute to the re use of the code and ease their maintenance we explain how the rtcdriver class is used in the code of the devices the class rtcdriver is a template class for controlling the functionalities related to an rtc module like updating its time for instance the rtcdriver uses a reference to an rtcinterface which interfaces the hardware of an rtc module the diagram in fig 6 presents an example of implementation the object my rtcinterface interfaces the rtc module with our code while the class rtcdriver controls the module s functionalities within our libraries code this way the core of our code is abstracted from the hardware libraries that a users could choose for more details on the libraries and packages developed please refer to supplementary material b 5 2 5 renovar web application renovar is a web platform that provides air sensor data for visualization and an api for the integration of iot air sensor devices the platform consists of a database a back end service developed in java language using spring boot and a front end application created on angular with ionic and typescript its access is free and open for environmental research and analysis fig 7 a illustrates the main panel of the platform which consists of a map showing the location of the monitoring devices the system receives data from iot devices such as air pollutants concentration temperature and relative humidity the data is stored in a database as time series which can be visualized online on the web platform the software consists of 1 a mysql database that stores the device s readings and other data necessary for the platform such as users registered devices pollutants and units 2 a restful back end developed in java using spring boot which is responsible for gathering data from the database and preparing it for the frontend and 3 the front end developed to be multi platform providing the interface with the user as illustrated in fig 7a and 7b the database backend and frontend are hosted on a server at the federal university of santa catarina fig 8 illustrates the functioning of the service in general the devices collect environmental data and send them into the database through the internet the backend receives requests from the frontend and gathers the necessary data from the database in case the data require some treatment e g calculating average values or filtering the backend executes the required operations and send the processed information back to the frontend the frontend on the other hand implements the interface with the user and outputs the results of the operations requested like visualizing the data as time series and downloading data as csv 3 how to use and collaborate collaborators could make use of our guides and tutorials for reproducing air quality monitors all the monitors registered in the platform will have their data available on clean for visualization and analysis for all users for registering a monitor to clean a username and a password will be required without any cost and then the device and its sensors must be registered a registration form has been made available on the clean homepage 4 conclusions in this work we took the first steps towards a collaborative framework for low cost air quality monitoring we developed the renovar web platform for connecting iot devices and made their data freely available to researchers local authorities and communities we also deployed two monitoring prototypes and presented documentation and guides for replication and upgrade we introduce the clean initiative which promotes collaboration for developing low cost monitoring platforms at lower costs and greater flexibility than the current open access initiatives this initiative can incorporate a broader and more diverse group of environmental monitoring specialists and enthusiasts increase the amount publicly available data and improve the spatio temporal coverage of air quality monitoring especially in developing countries the clean initiative will promote citizen science facilitate the development process of the devices to a larger public and make air quality data available for analysis and visualization the network of collaborators can expand to developers researchers hobbyists and students who could use the tools available at clean for education prototyping personal use and research broader coverage of air quality monitoring will make a considerable amount of data available facilitating access to environmental information to make cities and settlements more inclusive safe resilient and sustainable access to representative and reliable information helps researchers and decision makers to find solutions for smarter and healthier cities in developing countries where access to expensive technology is more limited initiatives like clean are affordable and scalable alternatives to highly expensive monitoring networks that can contribute to sustained economic growth and increased quality of life we believe this initiative would potentiate the expansion of the air monitoring community especially within brazilian territory facilitating access to air pollution information in regions where regulatory data is scarce or even missing software availability the code for the firmware of the prototypes and the firmware libraries is available at https codigos ufsc br lcqar clean firmware the code for the renovar platform is available at https codigos ufsc br lcqar renovar declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments authors would like to thank the fundação de amparo à pesquisa e inovação de santa catarina fapesc for financial support of project number 2018tr499 avaliação do impacto das emissões veiculares queimadas industriais e naturais na qualidade do ar em santa catarina appendix a supplementary data the following is the supplementary data to this article multimedia component 1 multimedia component 1 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2023 105664 
25416,the behaviour and rate of spread of a wildfire is strongly affected by local wind conditions depending on topography and surrounding vegetation the wind speed within dense vegetation can be substantially lower than the open wind speed above the vegetation this is commonly accounted for by applying a local correction factor to the local wind speed which can be called the wind adjustment factor waf wafs are often difficult to calculate and are usually based estimates for a particular vegetation type variation in the vegetation may result in spatially varying wafs we implement a model for spatially varying wafs based on leaf area index lai and vegetation height data which can be derived from remote sensing data sources the model is implemented within spark an operational wildfire prediction framework simulations of historical fires using the spatially varying waf model generally provided improved predictions compared to recorded fire extents than simulations without a spatially varying waf keywords wildfire wind adjustment factor vegetation height leaf area index fire rate of spread data availability data will be made available on request 1 introduction wildfires are one of the most devastating natural hazards in australia sharples et al 2016 bowman et al 2020 predicting how the wildfires spread under given fuel and weather conditions is essential for the risk assessment and operational fire management to effectively control fires as well as estimate potential damage and losses however prediction is complex as factors including weather fuel conditions and local terrain have differing and interacting degrees of influences on fire behaviour in addition predictions must ideally be performed on the rapid time scales required for emergency management operational prediction requires the ability to interactively adapt simulations to changing conditions and suppression activities a balance between prediction accuracy and calculation speed is currently achieved by implementing empirical wildfire models sullivan 2009 which can be rapidly computed within seconds over landscape scales empirical models such as the mcarthur 1967 or rothermel 1972 models use a number of parameters to determine the forward rate of spread of the fire including local wind speed fuel conditions such as amount and dryness and factors such as the local terrain slope the mcarthur forest fire mcarthur 1967 model is based on empirical correlations of observed fire behaviour and measured fuel and environmental variables obtained from field scale experimental wildfires sullivan 2009 in the united states rothermel s model rothermel 1972 is the foundation of the national fire danger rating system deeming et al 1977 burgan 1988 the fire behaviour analytics toolkit behave andrews 1986 and behaveplus andrews 2007 andrews et al 2008 the model was developed based on experimental data obtained from wind tunnel experiments conducted with artificial fuel beds of varying characteristics rothermel and anderson 1966 the local wind speed is a common component of all empirical wildfire models and strongly affects the forward spread rate gould et al 2008 it is commonly recognised that wind speed is reduced within vegetation by drag from the surrounding material dong et al 2001 fischenich 2000 and a wind adjustment factor waf is used in wildfire models to account for this effect the wind adjustment factor is spatially varying in nature and changes depending on the local vegetation type and density to derive a waf the sub canopy wind speed at some height within the canopy and the open wind speed are determined usually by measurement typically the sub canopy wind speed is determined at 2 m as a nominal mid flame height and the open wind speed is measured by a weather station at 10 m above the ground or above the canopy the waf is calculated as the open wind speed divided by the sub canopy wind speed moon et al 2016 field measurements of sub canopy wind speeds were performed in australian vegetation by moon et al 2016 related studies have also been conducted by dupont and brunet 2008a b dupont et al 2008 employing similar experimental practices regarding site selection and measurement techniques moon et al also measured vertical transects of leaf area density lad which demonstrated the variance in vegetation density over different forest types such as open woodland open regrowth forest 30 year old pine plantation open regrowth forest 110 year old open scrub 4 year old open scrub 30 year old and tall open eucalypt forest moon et al 2016 measurements made in su et al 1998 dupont and brunet 2008a b dupont et al 2008 dupont and brunet 2009 and dupont et al 2011 showed a gaussian profile of lad and several studies have used this as a standard lad profile cassiani et al 2008 kanani sühring and raasch 2015 2017 moon et al 2016 showed the waf for many australian forest types deviate considerably from previous estimates and demonstrated variation in waf with height in different forests although the half flame height wind speed is considered the most relevant wind speed to characterise the fire spread moon et al 2016 the vegetation height is also critical in waf calculation in the united states the another wind adjustment factor w a f u s is used in the rothermel model rothermel 1972 the w a f u s is defined the open or unsheltered wind speed divided by the mid flame wind speed the unsheltered or open wind speed is measured at 20 ft 6 1 m above the ground or above the vegetation for the w a f u s unlike the standard meteorological height of 10 m used in australia andrews 2012 modelled mid flame wind speed to estimate w a f u s assuming a uniform wind speed within the canopy mueller et al 2014 massman et al 2017 and neglecting the impacts of canopy structure on the sub canopy wind profile the influence of w a f u s on fire behaviour modelling was verified by applying w a f u s in behaveplus andrews 2007 2014 massman et al 2017 presents an analytic model for waf but did not attempt any comparison between simulated fires and observed wildfires as we attempt here current empirical wildfire rate of spread models were derived for specific wind and fuel conditions the inputs to the models are selected to be the open wind speed as this is more practical to measure using a weather station than sub canopy wind speeds which are inherently more difficult to capture due to their spatial and temporal variability nonetheless mcarthur 1967 observed the significance of the sub canopy wind speed reduction on the fire spread and made attempted to correlate the sub canopy effect open wind speed and stocking density a qualitative measure of the amount of plant material the correlations derived by mcarthur therefore implicitly include the density of the forest table 1 shows waf values used in phoenix rapidfire tolhurst et al 2008 an operational fire prediction system used in australia incorporating the mcarthur forest fire spread empirical model these values are used to estimate the sub canopy wind speed in fuel types different to the eucalypt forest for which the mcarthur model was developed mcarthur 1967 the open wind speed either from a forecast or measured in the field is multiplied by the ratio of the standard waf for eucalypt forest 3 to the waf of the local fuel type this gives a wind speed used in the mcarthur model and resultant forward rate of spread values less than 3 in phoenix increase the wind speed used in the model whereas values greater than 3 decrease the wind speed although fixed factors such as those in table 1 can provide an estimate of wind reduction the vegetation with a forest is complex and non uniform seasonal or locational variations exist within forests of the same fuel types and past fires result in long term structural variation in the vegetation the waf is therefore spatially and temporarily heterogeneous in nature and more complicated than can be described by a constant value moon et al 2016 sutherland et al 2018 sutherland et al 2018 showed the strong influence of the lad on the wind flow in regions below and immediately above the canopy using large eddy simulations it is important that a dynamic spatially and temporarily varying waf model be implemented in an operational model and assess the improvement in fire propagation prediction during natural fires in this study we use mathematical models to calculate spatially varying waf within the canopy of eucalypt forests and implement these in an operational wildfire model miller et al 2015 simulations of forest fires with and without calculated spatially varying waf were performed to investigate potential improvements in predictive accuracy using spatial wind adjustment factors 2 methods 2 1 model description the spatially varying waf model was implemented using the spark wildfire modelling system miller et al 2015 hilton et al 2019 the methodology is presented here in several phases firstly the spatially varying waf model itself is described and test calculations are presented secondly we used spatial lai data from the landscape visualiser anon 2020 and forest height data for australia simard et al 2011 to calculate a spatial lad and resulting spatially varying waf data the spatially varying waf data was then applied to eucalypt rate of spread models for a hypothetical wildfire simulation in spark finally we compared the results to a number of australian fires to investigate differences between simulations with and without a spatially varying waf model 2 2 defining the wind adjustment model the definition of the model parameters are listed in table 2 following moon et al 2016 we consider the waf as the sub canopy wind divided by an equivalent open wind speed both of which are functions of height z the waf that is applied in the simulations is then evaluated at some specific height z h analytic models of sub canopy and above canopy wind velocity can therefore be used to construct a model of spatially varying waf the sub canopy wind velocity model used in this study was originally due to inoue 1963 and was significantly extended by harman and finnigan 2007 the inoue model assumes a very large forest free of any forest edges or inhomogeneity in the forest canopy the harman and finnigan model requires only the canopy top velocity and the leaf area index lai of the forest to predict the sub canopy profile in neutral atmospheric conditions where the lai is the integral of the lad ranging from ground to the canopy top all other parameters may be estimated from empirical measurements as the velocity at the canopy top can be estimated once the vegetation height h is known it becomes redundant and the only parameters in the spatially varying waf model used in this study are the forest l a i and h the inoue 1963 model is derived from the navier stokes equations which are averaged in time and in space for a forest that is uniform in both the x and y directions the forest canopy is thought of as infinitely deep and the influence of the bottom boundary is considered negligible this averaging process removes the time derivative and the advection terms from the navier stokes equations the pressure gradient term is also assumed negligible relative to the turbulent stress term and the drag term the turbulent stress term is then be modelled using the mixing length approximation the mixing length concept is that a parcel of fluid retains its properties over a characteristic length scale the mixing length before mixing with the surrounding fluid while the mixing length approximation is a relatively crude model it allows analytical progress to be made the drag of the forest is modelled using the standard aerospatially varying model where drag is proportional to the frontal area multiplied by the fluid velocity squared the frontal area of the plants within a forest is called leaf area density lad amiro 1990 to allow analytical progress it is assumed that the canopy has uniform lad a while this is a crude approximation harman and finnigan 2007 demonstrates that the model gives good agreement to the measured wind velocities within a number of forest types the model for sub canopy wind speed u s is then 1 d d z l 2 d d z u s c d a u s 2 0 the parameter l is the mixing length and c d is the drag coefficient both of which will be estimated later from empirical measurements boundary conditions are required to solve this second order ordinary differential equation the conditions chosen are that the velocity derivative vanishes as z and the canopy top velocity u h is known the solution of eq 1 for the sub canopy wind velocity is 2 u s u h exp β l z h where 3 β u τ u h where u τ is the friction velocity at the canopy top and u h is the velocity at the canopy top the most obviously violated assumption of the inoue model is the canopy has infinite depth in practical terms the inoue model works for the top part of the canopy and progressively makes poor predictions near the ground because w a f is computed at height corresponding to the mid canopy the poor predictions near the ground is not an issue unless the canopy is very short for short canopies the w a f would likely be irrelevant because the flame length would greatly exceed the canopy height harman and finnigan 2007 extended the inoue 1963 model to smoothly unify with the logarithmic model of the boundary layer above the canopy as well as capture the effects of atmospheric stability on the flow profiles for this work we consider only neutral atmospheric conditions and so a simplified version of the harman and finnigan model for neutral conditions is used harman and finnigan divide the flow into three layers firstly the sub canopy layer where the inoue model is valid immediately above the canopy there is a roughness sublayer with an exponential profile far above the canopy is a standard log law boundary layer flow the parameters of roughness sublayer above the canopy and the log law are either determined simultaneously by continuity and smoothness conditions or by additional physical arguments or empirical measurements the profile that harman and finnigan fit to the sub and above canopy wind velocity for 0 z is 4 u z u h θ h z exp β z h l θ z h u h β κ log z h d z 0 log d z 0 ψ z l d β where θ is the heaviside function h is the canopy height d is the displacement length z 0 is the equivalent roughness length at the canopy top u h is the canopy top wind speed a parameter in the harman and finnigan 2007 model in their case is measured from eq 4 it can be seen that u h is a scaling of the sub and above canopy wind velocity profile the first term on the right of eq 4 is the inoue sub canopy wind velocity model eq 1 the second term is the above canopy wind velocity made of two components the log law model and contribution of the roughness sublayer to the wind velocity ψ z l d β the computation of the displacement length d the equivalent roughness length z 0 the mixing length l the parameter β and the roughness sublayer function ψ will be discussed later the profile eq 4 can be matched to a model of wind profile over canopy free or open ground which allows the computation of a modelled wind adjustment factor we adopt a log layer model of the open wind speed as 5 u o z u τ o κ log z z 0 o where u τ o is the friction velocity at the ground which is not necessarily the same as the friction velocity at the canopy top κ 0 4 is von karman s constant and z 0 o is the roughness length over the open ground which is not necessarily the same as the z 0 at the canopy top values of z 0 o have been estimated for a wide range of atmospheric flows and the value of z 0 o 0 03 which is realistic for a flow over bare ground bou zeid et al 2009 is adopted here at some critical blending height above the canopy z b it is assumed that the surface layer adjusts to a homogeneous turbulent wake region the blending height is usually taken at 3 times the feature height measured from the top of the feature bou zeid et al 2009 in the wake region it is assumed that the velocity is constant with height the full model of open wind speed as a function of height is 6 u o z u τ o κ log z z 0 o 0 z z b u τ o κ log z b z 0 o z z b the parameters β c d l d z 0 u h and the function ψ z l d β must be determined β and c d are estimated from experimental measurements harman and finnigan 2007 notes that in neutral conditions β 0 3 for a range of forests considered and so we simply adopt β 0 3 c d has been measured e g amiro 1990 harman and finnigan 2007 for a range of forests and while there is some variability the value of c d 0 15 is adopted as a reasonable estimate amiro 1990 scaling arguments of harman and finnigan 2007 show that the mixing length is 7 l 2 β c d a by comparison with field data the authors demonstrate that the resulting exponential profile works sufficiently well for many large canopies the displacement length d is the distance inside the canopy where the above canopy log layer velocity would equal zero following jackson 1981 d is equated to the canopy drag centroid in the idealised case of a uniform canopy this gives harman and finnigan 2007 8 d β 2 c d a z 0 is an equivalent roughness length at the canopy top this roughness length is chosen by forcing continuity of the profile at the canopy interface and therefore depends on the function ψ z l d β harman and finnigan adopt a function form of ψ as shown in appendix the equivalent roughness length harman and finnigan 2007 then simplifies to 9 z 0 d exp ψ h κ β the value of k is chosen in this implementation as 1 2 the wind velocity above the canopy at the blending height from eq 4 is then 10 u z b u h β κ ψ z b ln z b d z 0 u z b can then be matched to the open wind velocity u o z at the blending height by eliminating u τ o from eq 5 and setting u o z b u z b gives the open wind speed 11 u o z u z b ln z z 0 o ln z b z 0 o as previously discussed the value of z 0 o 0 03 is based on measurements of roughness length over bare ground the sub canopy wind speed at z h h follows from eq 4 12 u h u h exp β l h h the waf can then be calculated as 13 waf u o h u h u h and u o h via u z b both contain multiplicative factors of u h therefore the factor of u h is eliminated from the definition of waf and the canopy top velocity is not actually required to determine waf in our implementation we simply set u h 1 we evaluate ψ h and ψ z b eqs a 2 and a 3 respectively d from 8 and z 0 from 9 this and the choice of u h 1 allows the computation of wind velocities u z b from eq 10 open wind velocity u o h from eq 11 the sub canopy wind velocity from eq 12 and finally waf from eq 13 2 3 determining the wind adjustment factor the waf depends on leaf area density a z as well as vegetation canopy height h leaf area index lai is a dimensionless measure of one sided vegetation area per unit of ground area lai is defined as 14 l a i 0 h a z d z because the leaf area density a z of the forest is assumed to be uniform in the vertical direction i e a z a then l a i a h it is therefore trivial to obtain a z once the vegetation height h is known the spatially varying waf becomes a function of l a i and h which in turn depend on ground position that implies w a f x y where x and y are the ground position coordinates based on eq 13 the waf can be estimated at different heights h as fractions of h within multiple forest types fig 1 a shows the waf contours fig 1 b shows the sub and above canopy wind profile from eq 4 and fig 1 c shows the open wind profile from eq 6 calculated for an arbitrary l a i 2 0 and canopy height h 10 the horizontal dotted line in fig 1 b represents the canopy top where z h 1 2 4 implementing the model in spark a schematic of the model implementation processes are presented in fig 2 the waf model requires two geospatial input files with vegetation properties one for lai and another one for vegetation height as shown in fig 3 we extracted the vegetation data bounded by the longitude and latitude of the area under consideration we used python 3 and r scripting for pre processing the input data and qgis for post processing of the simulation outcomes the raster maps were generated using the raster and rgdal libraries of r with a resolution of 0 01 decimal degrees for all fire spread simulations a 30 m x 30 m grid is used this resolution is sufficient as demonstrated in previous works e g hilton et al 2018 the spatially varying waf was implemented in spark using the model described in section 2 2 through a user defined script written in opencl c the spatially varying waf model is only applied to cells with tall vegetation cells where the vegetation was grass or was not vegetation did not have a waf applied this script uses the two required inputs l a i and h from two raster maps to calculate the per cell waf using the numerical integral given in eq a 4 at an initialisation stage this waf value is then used to calculate a new sub canopy wind speed in each cell at each time step for the rate of spread model using the same process as phoenix namely by multiplying the open wind by w a f s w a f where w a f s is the standard waf 3 as per table 1 for eucalypt forest if no vegetation was found in a particular cell due to missing data w a f w a f s was used for a preliminary example we considered a representative point source ignition chosen at coordinates 150 16 7 o 33 67 9 o in new south wales australia where the fire spread was modelled using the dry eucalypt cheney et al 2012 and grasslands rate of spread cheney et al 1998 models based on the australia land use map the wind speed wind direction relative humidity and temperature were taken from automated weather station readings from a high fire danger day although simulated forecast can be used in this study observation from physical weather station is used the hourly isochrones of the simulation without applying a spatially varying waf model are shown in fig 4 a here a constant w a f s is used the same case using the spatially varying waf model is shown in fig 4 b using a constant representative l a i 2 0 and h 10 m a comparison of hourly isochrones with and without the waf model is shown in fig 4 c the isochrones show the variation in fire spread due to the spatially varying waf model affecting the propagation speed in areas of eucalypt forest 3 test case setup and determination of waf height 3 1 test case setup we conducted a series of simulations based on five real fire incidents in australia to compare observations of these fires to the simulation results with and without the waf model the fire events selected were the forcett fire 2013 of tasmania the kilmore fire 2009 of victoria the lithgow fire 2013 of new south wales and the mount cooke fire 2003 and the pickering brook fire 2005 of western australia the spatially varying waf was calculated for each region as described in the previous section the lai map was sourced for each of the fire regions and provided as a spatial input to spark the waf height map h was calculated for each region using a parametric study discussed in the next section and provided as a second spatial input to spark 3 2 determining waf height there are significant uncertainties in modelling flame height and subsequently the height at which waf is calculated h to use in complex fuel environments such as eucalypt forest must be determined the common metric of the sub canopy wind speed is the mid flame wind speed however the existing approaches for estimating mid flame speed have some significant shortcomings massman et al 2017 we therefore selected h using an iterative method to find the best agreement between the final simulated and actual fire burnt area see 6 for a description of how agreement between the simulated and actual fire lines was evaluated to determine the appropriate value of h we ran 100 variants of each case varying h from 1 to 100 of h by an increment of 1 we observed that h closer to the 100 of h typically produces the best predictions therefore the results presented later in this section are based on h h i e 100 of h this observation is may appear counter intuitive since the result implies the wind speed at the canopy top is the most significant for determining the ros however all of the fires considered here had significant crown fire activity and so the flame heights were either comparable to or even exceeded h this implies that h h is likely simply a crude estimate of mean mid flame height 4 case studies the case studies used here were drawn from a set of studies in spark designed for validation purposes swedosh et al 2018 we selected the forcett fire 2013 the kilmore fire 2009 the lithgow fire 2013 the mount cooke fire 2003 and the pickering brook fire 2005 the setup for the cases follows swedosh et al 2018 however we only used wind data from nearby automatic weather stations aws whereas swedosh et al tested the effect of different wind data on the fire isochrones all simulations used the dry eucalypt cheney et al 2012 and csiro grasslands cheney et al 1998 rate of spread models 4 1 case 1 forcett fire 2013 tas the ignition was caused by a campfire which was started at 14 00 on january 3 2013 and remained active until january 18 2013 this fire burned about 23 960 hectares with a perimeter of around 310 km marsden smedley 2014 the wind data used for this simulation was from the dunalley stroud point aws the geosciences australia digital elevation model 30 m raster was used for the dem data the curing was assumed to be 80 everywhere as a reasonable estimate for the time of year that the fire occurred a constant fuel age of 8 years was assumed for the simulation and fuel hazard scores were then estimated from standard fuel growth curves cheney et al 2012 an initial fire of radius 100 m was assumed at 147 6751 e 42 8065 s marsden smedley 2014 the fire was simulated for a 24 hour period starting on 14 30 on january 3 2013 the simulation covered the initial development of the fire the overnight decrease in fire activity and subsequent intensification of fire the fire spread rapidly as a crown fire from approximately 12 30 to 14 30 on january 4 2013 4 2 case 2 kilmore fire 2009 vic the kilmore fire in 2009 was part of the devastating black saturday fires the ignition time was approximately 11 45 am on 7 february caused by failed power line the fire was driven by extreme northwesterly winds and travelled around 50 km southeast with a narrow fire front cruz et al 2012 provides a detailed description of this fire the intense fire had generated a considerable number of downwind spotfires which later merged into a long fire front these were not simulated in this analysis for this simulation wallan kilmore gap aws m data was used along with a 30 m topographical raster curing was taken to be 100 based on the time of year and the fuel age was assumed to be a constant 15 years with the fuel hazard scores estimated from fuel growth curves the initial condition was based on the isochrones at 1300 in cruz et al 2012 and the fire was simulated for 4 hours 4 3 case 3 lithgow fire 2013 nsw the lithgow fire a part of the state mine fire started as a minor fire on 16 october 2013 near a defence force training base at marrangaroo and travelled up to 25 km on 17 october the fire burnt out more than 55 000 hectares between lithgow and bilpin the simulation assumed a curing value of 60 and a fuel age of 18 years the initial fire shape was based upon a reconstruction of the fire at 1159 on 17 october 2013 and the simulation was ended at 2115 on 17 october 2013 mount boyce aws weather data was used in these simulations 4 4 case 4 mount cooke fire 2003 wa the mount cooke fire started from a lightning strike at night and grew to approximately 30 hectares by the morning the fire rapidly intensified on the morning of 10 january 2003 and burnt through jarrah eucalyptus marginata forest the simulation here relies on information in johnston et al 2008 the initial fire was assumed to be at 116 295 e 32 375 s with a radius of 310 m at 0800 on 10 january 2003 and the fire was simulated until 0500 11 january 2003 this fire event was complicated with rapid intensification crown fire and spotting activity suppression activity and natural easing of fire conditions overnight fuel age and hence hazard scores were based upon the maps shown in johnston et al 2008 the curing coefficient was assumed to be 80 weather data from bickley aws were used as was the standard 30 m topographical raster from ga 4 5 case 5 pickering brook fire 2005 wa this wildfire burnt 27 700 hectares of jarrah forest in the pickering brook area of perth hills in western australia during january 2005 cheney 2010 the fires were started by a series of deliberate ignitions fuel age data was sourced from cheney 2010 and mapped to fuel hazard scores using fuel growth curves bickley aws weather data were used for this simulation as was the usual 30 m ga raster for topography the curing level was assumed to be 80 three initial fires were used to initialise the simulations based on cheney 2010 the first fire started at 1800 on 15 january 2005 and the simulation was run for 25 h successful and partially successful suppression efforts influenced the shape of the western and southern fire line however no suppression effects were included in the model 5 simulation results the simulated isochrones for all case studies with and without the spatially varying waf model and the simulated burnt area as a function of time are shown in figs 5 to 9 the jaccard score is used to provide quantitative comparisons between the predicted and the observed final fires the jaccard score jaccard 1912 is 15 jaccard score t p t p f p f n where t p true positive are grid cells where both the simulated fire and the observed fire were burnt f p false positive are grid cells where the simulated fire is burnt and the observed fire was not burnt f n false negative are grid cells where the simulated fire is not burnt but the observed fire was burnt the jaccard score is a ratio between the true positive and the union of simulated and observed burnt areas and for perfectly overlapping simulated and observed fires the jaccard score is one for perfectly disjoint fires the jaccard score is zero impossible since the simulated ignition is always within the observed fire burnt area the jaccard score does not quantify errors in the fire prediction made over time nor does it provide significant insight into the reason for any errors such as the selection of wind data or the lack of spotting or suppression models in the simulation the jaccard scores presented in fig 10 a show that the predicted quantities for with and with non spatially varying waf are quite similar except for the mount cooke fire case for the individual cases the forcett case is worse using spatially varying waf kilmore shows a slight improvement using spatially varying waf lithgow is marginally worse using spatially varying waf mount cooke shows a large improvement using spatially varying waf and pickering brook is worse when using spatially varying waf the tp and fn scores are shown in figs 10b and 10c respectively the tp predictions are improved using the spatially varying waf model in all cases except pickering brook where the spatially varying waf model performs slightly worse than the simulation without the spatially varying waf model mount cook shows considerable improvement the fn predictions are also typically better with the spatially varying waf model than without the spatially varying waf model 6 discussion the inclusion of a physically motivated spatially varying waf provides some improved prediction of the overall fire line when compared to the predictions without the spatially varying waf however the use of a spatially varying waf still does not significantly improve predictions in all cases compared to the final fire perimeters there are a number of possible reasons for the disagreement between the predictions and the final perimeter irrespective of whether with or without the spatially varying waf model was used factors such as driving wind wind changes fuel conditions suppression activities and spotting are not well represented by the fire spread model and these factors dominate the prediction of the final fire perimeter over the wind adjustment factor the lithgow fire simulations are a particularly good example of the sensitivity of the predictions to the input weather data the weather data from an aws at mt boyce was used as inputs to the simulation mt boyce was approximately 20 km from the fire and the wind direction of mt boyce data was south westerly rather than a westerly as suggested by the final fire perimeter swedosh et al 2018 also simulated this fire using gridded numerical weather data from the bureau of meteorology predicted by the bureau s weather model in this case they found the final fire perimeter was greatly over predicted to the south east they attributed this discrepancy primarily to the north westerly wind direction in the gridded data rather than the westerly direction suggested by the final fire perimeter the other cases used aws data at significant distances from the fire to obtain driving wind data the aws data used for the mt cooke simulation also likely over estimated the relative humidity and fuel moisture content resulting in a considerably reduced ros the aws was located in bickley approximately 60 km north west of the fire spotting was not included in any of these simulations except to the extent implicitly included in the dry eucalypt model cheney et al 2012 spotting was significant in all cases swedosh et al 2018 there were two initial burn areas in the lithgow fire suggesting spotting activity which could increase the ros to the east the kilmore fire exhibited long range spotting up to a distance of several kilometers to the south east similarly mt cooke exhibited spotting of up to 5 km while pickering brooke exhibited shorter range spotting of up to 1 km an improved explicit treatment of spotting may improve the fire spread predictions with or without a spatially varying waf the forcett fire occurred in complicated vegetation which was classified into a reduced number of standard fuel types and a constant fuel age was assumed swedosh et al 2018 the simulated final fire perimeter may improve if the representation of the fuel type and load were used no attempt was made to represent the suppression efforts nor control lines in any of the fires the most prominent example is the pickering brooke fire which was subjected to control lines which slowed or stopped fire spread along the southern flank the lack of suppression in the simulation led to over prediction of the extent of the southern flank the suppression efforts in the actual fire led to the irregular fingered shape of the observed southern flank the western flank of the forcett fire was also controlled and so the simulations tend to over predict the fire spread in this location most if not all of these fires involved significant crown fire spread cheney 2010 cruz et al 2012 marsden smedley 2014 which justifies the selection of h h because the flame length from a crown fire exceeds the forest height crown fire flames are driven by less obstructed winds than sub canopy fires the above canopy wind profile is affected by the canopy there is a shear layer present above the canopy harman and finnigan 2007 however far above the canopy the wind profile tends to a logarithmic profile and eventually to a blended layer as shown in fig 1 b c therefore the waf will decrease to unity above the canopy and therefore predictions of crown fires will be less effected by the selected value of waf the foundation of this study was an idealised model of sub and above canopy flow due to inoue 1963 harman and finnigan 2007 this model assumes that the forest is infinitely deep and homogeneous and in this implementation we also neglect any variations in atmospheric stability the assumption of an infinitely deep forest is of course unrealistic however harman and finnigan 2007 showed that their model provided good agreement to observations for a significant part of the canopy for finite depth canopies belcher et al 2003 provides some estimates of near ground velocity profile which could be adopted because the fires considered here were large and the relevant height h for the waf was found to be on the order of the canopy height h this additional step appears unnecessary near ground corrections may be required for small fires contained entirely within the canopy forests are inhomogeneous considerable variation in lai and h are shown in the raster maps of lithgow fig 3 this work assumes that the wind and fire spread rate adjust quickly to variations in the forest in an empirical and simulation study at the edge of a pine forest dupont and brunet 2008a found the wind flow adjusted to the forest over a distance in excess of 10 h and dependent on the lad i e the vertical heterogeneity of the forest is also important for this adjustment length it is likely that at clearing forest boundaries the wind speed is underestimated by using the developed sub canopy wind speed profile and thus including a transition zone make improve model predictions the effect of small changes in lai rather than a step change at a forest boundary is not well understood and further research will be required to completely characterise the effects of forest heterogeneity additionally further work will be required to understand how the fire spread rate responds to a change in sub canopy wind speed we have also assumed that the fire itself does not modify the lai obviously a crown fire will consume much of the leafy material within a tree crown the assumption that the lai of a spatial location does not change with time is equivalent to the assumption that the lai is dominated by the large branches and trunks of the trees which do not burn away easily the spread model used here is based on wind speed away from the fire and any effects of pyrogenic winds resulting from in drafts into the fire plume are not explicitly included in the fire spread model used here therefore it is fundamentally difficult to incorporate any waf into the fire spread model that includes degradation of the forest and pyrogenic winds for other models such as hilton et al 2018 such a refinement may be possible we have only used a constant lad approximation whereas the model of massman et al 2017 allows variation of lad with height because we found the near top and above canopy flow to be the most relevant to the fire spread the model of massman et al 2017 will likely only give marginal improvements the resolution of the lai and height maps used here were at the maximum available of 0 01 decimal degrees which corresponds to an approximately 500 m by 500 m cell on the ground the relative coarseness of this data compared to the fire simulation resolution of 30 m by 30 m cells means that the effect of spatially varying waf resulting from these maps is a very coarse grained correction to the rate of spread a possible advantage of the waf model is the ability to correct the fire isochrones at fine scales which would allow more accurate time of arrivals to be predicted for sensitive areas such as peri urban areas however this would require much higher resolution lai and height maps which are not currently available but may be in the future as new remote sensing technology becomes available additionally the waf model may need to be extended to include edge effects where the forest transitions from one lai to another causing the sub canopy wind profile to vary over some streamwise distance primarily the case studies consisted of large crown fires in smaller less intense fires such as hazard reduction burns the flame length may be significantly smaller than the canopy height for such fires the waf may be much larger than the values identified for these case studies the waf may then have a more significant effect on the predicted isochrones for less intense fires than observed in this study a worthwhile extension of this research is to consider compare the predictions with low intensity fires however comprehensive datasets for such fires in australia appears sparse as such the question about the effect of the waf on the spread of low intensity fires remains open 7 conclusion in this study we implemented the harman finnigan waf model in an operational model spark this waf model takes into consideration of vegetation height and canopy density at every computational cell as the waf varies spatially with vegetation and temporally with wind we term it as spatially varying waf through this research we assess to what extent the wind flow is affected by the canopy density and forest height on the day of fire incidents the height at which the waf is calculated was determined by trial simulations that moved to minimise disagreement between the simulated and observed fires in all cases we found the relevant height to calculate the waf h was equal to the canopy height h this could be due to significant crown fire activity in the cases studied in our case studies we observed a significant improvement in the mount cooke fire prediction largely by increasing the true positive agreement between the simulated and observed fire areas with the exception of the pickering brook case all other true positive predictions improved using the spatially varying waf model reductions in false negative predictions were observed for all but the pickering brook case however the jaccard scores only improved with the spatially varying waf model in the mount cooke and kilmore cases all other cases the jaccard scores were slightly worse with the spatially varying waf despite improvements in the true positive and false negative predictions this suggests that false positive predictions increase with the spatially varying waf model this can be considered conservative i e having a safer margin a possible interpretation of this result is that the waf which is used heuristically in operational practice cannot sufficiently correct the empirical fire spread model such that the predictions closely match the observations and other more detailed modelling approaches may be required for fire spread under a canopy overall the spatially varying waf appears to improve the prediction of fire propagation further improvement may be made by implementing the massman model massman et al 2017 which accounts for non uniform leaf area density however for practical applications high resolution lad data will be required for all forest locations furthermore plant area index pai data is needed for the massman model with increasing use of lidar technology the required lad and pai data may be available in future software availability the wildfire framework spark is available at https research csiro au spark the geospatial framework geostack used in spark is available as an open source library at https gitlab com geostack library and as a python package available on conda forge declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgement we acknowledge the contribution of the bushfire and natural hazard cooperative research centre bnhcrc through the project fire spread across fuel types we would like to acknowledge the valuable feedback of dr nazmul khan of the institute for sustainable industries and liveable cities at victoria university melbourne during the implementation of the spatially varying waf model appendix harman and finnigan adopt the following function form of ψ as a good fit to the data ψ z z d 1 ϕ z l β z d z ϕ z 1 c exp β z d 2 l a 1 c 1 κ 2 β exp 1 4 this is an indefinite integral which may be evaluated with a numerical quadrature scheme in these cases eq a 1 was evaluated with a cumulative riemann sum the models of canopy and open wind velocity are functions of height z however there are actually only three relevant values of z namely the blending height z z b the canopy top z h and the height at which w a f is calculated z h therefore if h is constant such as in operational models it is unnecessary to compute ψ z instead all that is required is ψ h and ψ z b which after simplification become a 2 ψ h c h d e c z z d z a 3 ψ z b c z b d t e c z z d z these two integrals eqs a 2 and a 3 can be efficiently calculated using exponentially weighted quadrature due to the presence of the e z term a 4 z 0 e c z z d z i e c z i z i δ z i where δ z i k i δ z 0 with δ z 0 the initial quadrature spacing chosen here to be 1 0 e 3 and z i z 0 i δ z i the value of k is the exponential step increase chosen in this implementation to be 1 2 in the quadrature scheme the upper bound is taken as z 50 which is sufficiently far above the canopy as to be effectively infinite 
25416,the behaviour and rate of spread of a wildfire is strongly affected by local wind conditions depending on topography and surrounding vegetation the wind speed within dense vegetation can be substantially lower than the open wind speed above the vegetation this is commonly accounted for by applying a local correction factor to the local wind speed which can be called the wind adjustment factor waf wafs are often difficult to calculate and are usually based estimates for a particular vegetation type variation in the vegetation may result in spatially varying wafs we implement a model for spatially varying wafs based on leaf area index lai and vegetation height data which can be derived from remote sensing data sources the model is implemented within spark an operational wildfire prediction framework simulations of historical fires using the spatially varying waf model generally provided improved predictions compared to recorded fire extents than simulations without a spatially varying waf keywords wildfire wind adjustment factor vegetation height leaf area index fire rate of spread data availability data will be made available on request 1 introduction wildfires are one of the most devastating natural hazards in australia sharples et al 2016 bowman et al 2020 predicting how the wildfires spread under given fuel and weather conditions is essential for the risk assessment and operational fire management to effectively control fires as well as estimate potential damage and losses however prediction is complex as factors including weather fuel conditions and local terrain have differing and interacting degrees of influences on fire behaviour in addition predictions must ideally be performed on the rapid time scales required for emergency management operational prediction requires the ability to interactively adapt simulations to changing conditions and suppression activities a balance between prediction accuracy and calculation speed is currently achieved by implementing empirical wildfire models sullivan 2009 which can be rapidly computed within seconds over landscape scales empirical models such as the mcarthur 1967 or rothermel 1972 models use a number of parameters to determine the forward rate of spread of the fire including local wind speed fuel conditions such as amount and dryness and factors such as the local terrain slope the mcarthur forest fire mcarthur 1967 model is based on empirical correlations of observed fire behaviour and measured fuel and environmental variables obtained from field scale experimental wildfires sullivan 2009 in the united states rothermel s model rothermel 1972 is the foundation of the national fire danger rating system deeming et al 1977 burgan 1988 the fire behaviour analytics toolkit behave andrews 1986 and behaveplus andrews 2007 andrews et al 2008 the model was developed based on experimental data obtained from wind tunnel experiments conducted with artificial fuel beds of varying characteristics rothermel and anderson 1966 the local wind speed is a common component of all empirical wildfire models and strongly affects the forward spread rate gould et al 2008 it is commonly recognised that wind speed is reduced within vegetation by drag from the surrounding material dong et al 2001 fischenich 2000 and a wind adjustment factor waf is used in wildfire models to account for this effect the wind adjustment factor is spatially varying in nature and changes depending on the local vegetation type and density to derive a waf the sub canopy wind speed at some height within the canopy and the open wind speed are determined usually by measurement typically the sub canopy wind speed is determined at 2 m as a nominal mid flame height and the open wind speed is measured by a weather station at 10 m above the ground or above the canopy the waf is calculated as the open wind speed divided by the sub canopy wind speed moon et al 2016 field measurements of sub canopy wind speeds were performed in australian vegetation by moon et al 2016 related studies have also been conducted by dupont and brunet 2008a b dupont et al 2008 employing similar experimental practices regarding site selection and measurement techniques moon et al also measured vertical transects of leaf area density lad which demonstrated the variance in vegetation density over different forest types such as open woodland open regrowth forest 30 year old pine plantation open regrowth forest 110 year old open scrub 4 year old open scrub 30 year old and tall open eucalypt forest moon et al 2016 measurements made in su et al 1998 dupont and brunet 2008a b dupont et al 2008 dupont and brunet 2009 and dupont et al 2011 showed a gaussian profile of lad and several studies have used this as a standard lad profile cassiani et al 2008 kanani sühring and raasch 2015 2017 moon et al 2016 showed the waf for many australian forest types deviate considerably from previous estimates and demonstrated variation in waf with height in different forests although the half flame height wind speed is considered the most relevant wind speed to characterise the fire spread moon et al 2016 the vegetation height is also critical in waf calculation in the united states the another wind adjustment factor w a f u s is used in the rothermel model rothermel 1972 the w a f u s is defined the open or unsheltered wind speed divided by the mid flame wind speed the unsheltered or open wind speed is measured at 20 ft 6 1 m above the ground or above the vegetation for the w a f u s unlike the standard meteorological height of 10 m used in australia andrews 2012 modelled mid flame wind speed to estimate w a f u s assuming a uniform wind speed within the canopy mueller et al 2014 massman et al 2017 and neglecting the impacts of canopy structure on the sub canopy wind profile the influence of w a f u s on fire behaviour modelling was verified by applying w a f u s in behaveplus andrews 2007 2014 massman et al 2017 presents an analytic model for waf but did not attempt any comparison between simulated fires and observed wildfires as we attempt here current empirical wildfire rate of spread models were derived for specific wind and fuel conditions the inputs to the models are selected to be the open wind speed as this is more practical to measure using a weather station than sub canopy wind speeds which are inherently more difficult to capture due to their spatial and temporal variability nonetheless mcarthur 1967 observed the significance of the sub canopy wind speed reduction on the fire spread and made attempted to correlate the sub canopy effect open wind speed and stocking density a qualitative measure of the amount of plant material the correlations derived by mcarthur therefore implicitly include the density of the forest table 1 shows waf values used in phoenix rapidfire tolhurst et al 2008 an operational fire prediction system used in australia incorporating the mcarthur forest fire spread empirical model these values are used to estimate the sub canopy wind speed in fuel types different to the eucalypt forest for which the mcarthur model was developed mcarthur 1967 the open wind speed either from a forecast or measured in the field is multiplied by the ratio of the standard waf for eucalypt forest 3 to the waf of the local fuel type this gives a wind speed used in the mcarthur model and resultant forward rate of spread values less than 3 in phoenix increase the wind speed used in the model whereas values greater than 3 decrease the wind speed although fixed factors such as those in table 1 can provide an estimate of wind reduction the vegetation with a forest is complex and non uniform seasonal or locational variations exist within forests of the same fuel types and past fires result in long term structural variation in the vegetation the waf is therefore spatially and temporarily heterogeneous in nature and more complicated than can be described by a constant value moon et al 2016 sutherland et al 2018 sutherland et al 2018 showed the strong influence of the lad on the wind flow in regions below and immediately above the canopy using large eddy simulations it is important that a dynamic spatially and temporarily varying waf model be implemented in an operational model and assess the improvement in fire propagation prediction during natural fires in this study we use mathematical models to calculate spatially varying waf within the canopy of eucalypt forests and implement these in an operational wildfire model miller et al 2015 simulations of forest fires with and without calculated spatially varying waf were performed to investigate potential improvements in predictive accuracy using spatial wind adjustment factors 2 methods 2 1 model description the spatially varying waf model was implemented using the spark wildfire modelling system miller et al 2015 hilton et al 2019 the methodology is presented here in several phases firstly the spatially varying waf model itself is described and test calculations are presented secondly we used spatial lai data from the landscape visualiser anon 2020 and forest height data for australia simard et al 2011 to calculate a spatial lad and resulting spatially varying waf data the spatially varying waf data was then applied to eucalypt rate of spread models for a hypothetical wildfire simulation in spark finally we compared the results to a number of australian fires to investigate differences between simulations with and without a spatially varying waf model 2 2 defining the wind adjustment model the definition of the model parameters are listed in table 2 following moon et al 2016 we consider the waf as the sub canopy wind divided by an equivalent open wind speed both of which are functions of height z the waf that is applied in the simulations is then evaluated at some specific height z h analytic models of sub canopy and above canopy wind velocity can therefore be used to construct a model of spatially varying waf the sub canopy wind velocity model used in this study was originally due to inoue 1963 and was significantly extended by harman and finnigan 2007 the inoue model assumes a very large forest free of any forest edges or inhomogeneity in the forest canopy the harman and finnigan model requires only the canopy top velocity and the leaf area index lai of the forest to predict the sub canopy profile in neutral atmospheric conditions where the lai is the integral of the lad ranging from ground to the canopy top all other parameters may be estimated from empirical measurements as the velocity at the canopy top can be estimated once the vegetation height h is known it becomes redundant and the only parameters in the spatially varying waf model used in this study are the forest l a i and h the inoue 1963 model is derived from the navier stokes equations which are averaged in time and in space for a forest that is uniform in both the x and y directions the forest canopy is thought of as infinitely deep and the influence of the bottom boundary is considered negligible this averaging process removes the time derivative and the advection terms from the navier stokes equations the pressure gradient term is also assumed negligible relative to the turbulent stress term and the drag term the turbulent stress term is then be modelled using the mixing length approximation the mixing length concept is that a parcel of fluid retains its properties over a characteristic length scale the mixing length before mixing with the surrounding fluid while the mixing length approximation is a relatively crude model it allows analytical progress to be made the drag of the forest is modelled using the standard aerospatially varying model where drag is proportional to the frontal area multiplied by the fluid velocity squared the frontal area of the plants within a forest is called leaf area density lad amiro 1990 to allow analytical progress it is assumed that the canopy has uniform lad a while this is a crude approximation harman and finnigan 2007 demonstrates that the model gives good agreement to the measured wind velocities within a number of forest types the model for sub canopy wind speed u s is then 1 d d z l 2 d d z u s c d a u s 2 0 the parameter l is the mixing length and c d is the drag coefficient both of which will be estimated later from empirical measurements boundary conditions are required to solve this second order ordinary differential equation the conditions chosen are that the velocity derivative vanishes as z and the canopy top velocity u h is known the solution of eq 1 for the sub canopy wind velocity is 2 u s u h exp β l z h where 3 β u τ u h where u τ is the friction velocity at the canopy top and u h is the velocity at the canopy top the most obviously violated assumption of the inoue model is the canopy has infinite depth in practical terms the inoue model works for the top part of the canopy and progressively makes poor predictions near the ground because w a f is computed at height corresponding to the mid canopy the poor predictions near the ground is not an issue unless the canopy is very short for short canopies the w a f would likely be irrelevant because the flame length would greatly exceed the canopy height harman and finnigan 2007 extended the inoue 1963 model to smoothly unify with the logarithmic model of the boundary layer above the canopy as well as capture the effects of atmospheric stability on the flow profiles for this work we consider only neutral atmospheric conditions and so a simplified version of the harman and finnigan model for neutral conditions is used harman and finnigan divide the flow into three layers firstly the sub canopy layer where the inoue model is valid immediately above the canopy there is a roughness sublayer with an exponential profile far above the canopy is a standard log law boundary layer flow the parameters of roughness sublayer above the canopy and the log law are either determined simultaneously by continuity and smoothness conditions or by additional physical arguments or empirical measurements the profile that harman and finnigan fit to the sub and above canopy wind velocity for 0 z is 4 u z u h θ h z exp β z h l θ z h u h β κ log z h d z 0 log d z 0 ψ z l d β where θ is the heaviside function h is the canopy height d is the displacement length z 0 is the equivalent roughness length at the canopy top u h is the canopy top wind speed a parameter in the harman and finnigan 2007 model in their case is measured from eq 4 it can be seen that u h is a scaling of the sub and above canopy wind velocity profile the first term on the right of eq 4 is the inoue sub canopy wind velocity model eq 1 the second term is the above canopy wind velocity made of two components the log law model and contribution of the roughness sublayer to the wind velocity ψ z l d β the computation of the displacement length d the equivalent roughness length z 0 the mixing length l the parameter β and the roughness sublayer function ψ will be discussed later the profile eq 4 can be matched to a model of wind profile over canopy free or open ground which allows the computation of a modelled wind adjustment factor we adopt a log layer model of the open wind speed as 5 u o z u τ o κ log z z 0 o where u τ o is the friction velocity at the ground which is not necessarily the same as the friction velocity at the canopy top κ 0 4 is von karman s constant and z 0 o is the roughness length over the open ground which is not necessarily the same as the z 0 at the canopy top values of z 0 o have been estimated for a wide range of atmospheric flows and the value of z 0 o 0 03 which is realistic for a flow over bare ground bou zeid et al 2009 is adopted here at some critical blending height above the canopy z b it is assumed that the surface layer adjusts to a homogeneous turbulent wake region the blending height is usually taken at 3 times the feature height measured from the top of the feature bou zeid et al 2009 in the wake region it is assumed that the velocity is constant with height the full model of open wind speed as a function of height is 6 u o z u τ o κ log z z 0 o 0 z z b u τ o κ log z b z 0 o z z b the parameters β c d l d z 0 u h and the function ψ z l d β must be determined β and c d are estimated from experimental measurements harman and finnigan 2007 notes that in neutral conditions β 0 3 for a range of forests considered and so we simply adopt β 0 3 c d has been measured e g amiro 1990 harman and finnigan 2007 for a range of forests and while there is some variability the value of c d 0 15 is adopted as a reasonable estimate amiro 1990 scaling arguments of harman and finnigan 2007 show that the mixing length is 7 l 2 β c d a by comparison with field data the authors demonstrate that the resulting exponential profile works sufficiently well for many large canopies the displacement length d is the distance inside the canopy where the above canopy log layer velocity would equal zero following jackson 1981 d is equated to the canopy drag centroid in the idealised case of a uniform canopy this gives harman and finnigan 2007 8 d β 2 c d a z 0 is an equivalent roughness length at the canopy top this roughness length is chosen by forcing continuity of the profile at the canopy interface and therefore depends on the function ψ z l d β harman and finnigan adopt a function form of ψ as shown in appendix the equivalent roughness length harman and finnigan 2007 then simplifies to 9 z 0 d exp ψ h κ β the value of k is chosen in this implementation as 1 2 the wind velocity above the canopy at the blending height from eq 4 is then 10 u z b u h β κ ψ z b ln z b d z 0 u z b can then be matched to the open wind velocity u o z at the blending height by eliminating u τ o from eq 5 and setting u o z b u z b gives the open wind speed 11 u o z u z b ln z z 0 o ln z b z 0 o as previously discussed the value of z 0 o 0 03 is based on measurements of roughness length over bare ground the sub canopy wind speed at z h h follows from eq 4 12 u h u h exp β l h h the waf can then be calculated as 13 waf u o h u h u h and u o h via u z b both contain multiplicative factors of u h therefore the factor of u h is eliminated from the definition of waf and the canopy top velocity is not actually required to determine waf in our implementation we simply set u h 1 we evaluate ψ h and ψ z b eqs a 2 and a 3 respectively d from 8 and z 0 from 9 this and the choice of u h 1 allows the computation of wind velocities u z b from eq 10 open wind velocity u o h from eq 11 the sub canopy wind velocity from eq 12 and finally waf from eq 13 2 3 determining the wind adjustment factor the waf depends on leaf area density a z as well as vegetation canopy height h leaf area index lai is a dimensionless measure of one sided vegetation area per unit of ground area lai is defined as 14 l a i 0 h a z d z because the leaf area density a z of the forest is assumed to be uniform in the vertical direction i e a z a then l a i a h it is therefore trivial to obtain a z once the vegetation height h is known the spatially varying waf becomes a function of l a i and h which in turn depend on ground position that implies w a f x y where x and y are the ground position coordinates based on eq 13 the waf can be estimated at different heights h as fractions of h within multiple forest types fig 1 a shows the waf contours fig 1 b shows the sub and above canopy wind profile from eq 4 and fig 1 c shows the open wind profile from eq 6 calculated for an arbitrary l a i 2 0 and canopy height h 10 the horizontal dotted line in fig 1 b represents the canopy top where z h 1 2 4 implementing the model in spark a schematic of the model implementation processes are presented in fig 2 the waf model requires two geospatial input files with vegetation properties one for lai and another one for vegetation height as shown in fig 3 we extracted the vegetation data bounded by the longitude and latitude of the area under consideration we used python 3 and r scripting for pre processing the input data and qgis for post processing of the simulation outcomes the raster maps were generated using the raster and rgdal libraries of r with a resolution of 0 01 decimal degrees for all fire spread simulations a 30 m x 30 m grid is used this resolution is sufficient as demonstrated in previous works e g hilton et al 2018 the spatially varying waf was implemented in spark using the model described in section 2 2 through a user defined script written in opencl c the spatially varying waf model is only applied to cells with tall vegetation cells where the vegetation was grass or was not vegetation did not have a waf applied this script uses the two required inputs l a i and h from two raster maps to calculate the per cell waf using the numerical integral given in eq a 4 at an initialisation stage this waf value is then used to calculate a new sub canopy wind speed in each cell at each time step for the rate of spread model using the same process as phoenix namely by multiplying the open wind by w a f s w a f where w a f s is the standard waf 3 as per table 1 for eucalypt forest if no vegetation was found in a particular cell due to missing data w a f w a f s was used for a preliminary example we considered a representative point source ignition chosen at coordinates 150 16 7 o 33 67 9 o in new south wales australia where the fire spread was modelled using the dry eucalypt cheney et al 2012 and grasslands rate of spread cheney et al 1998 models based on the australia land use map the wind speed wind direction relative humidity and temperature were taken from automated weather station readings from a high fire danger day although simulated forecast can be used in this study observation from physical weather station is used the hourly isochrones of the simulation without applying a spatially varying waf model are shown in fig 4 a here a constant w a f s is used the same case using the spatially varying waf model is shown in fig 4 b using a constant representative l a i 2 0 and h 10 m a comparison of hourly isochrones with and without the waf model is shown in fig 4 c the isochrones show the variation in fire spread due to the spatially varying waf model affecting the propagation speed in areas of eucalypt forest 3 test case setup and determination of waf height 3 1 test case setup we conducted a series of simulations based on five real fire incidents in australia to compare observations of these fires to the simulation results with and without the waf model the fire events selected were the forcett fire 2013 of tasmania the kilmore fire 2009 of victoria the lithgow fire 2013 of new south wales and the mount cooke fire 2003 and the pickering brook fire 2005 of western australia the spatially varying waf was calculated for each region as described in the previous section the lai map was sourced for each of the fire regions and provided as a spatial input to spark the waf height map h was calculated for each region using a parametric study discussed in the next section and provided as a second spatial input to spark 3 2 determining waf height there are significant uncertainties in modelling flame height and subsequently the height at which waf is calculated h to use in complex fuel environments such as eucalypt forest must be determined the common metric of the sub canopy wind speed is the mid flame wind speed however the existing approaches for estimating mid flame speed have some significant shortcomings massman et al 2017 we therefore selected h using an iterative method to find the best agreement between the final simulated and actual fire burnt area see 6 for a description of how agreement between the simulated and actual fire lines was evaluated to determine the appropriate value of h we ran 100 variants of each case varying h from 1 to 100 of h by an increment of 1 we observed that h closer to the 100 of h typically produces the best predictions therefore the results presented later in this section are based on h h i e 100 of h this observation is may appear counter intuitive since the result implies the wind speed at the canopy top is the most significant for determining the ros however all of the fires considered here had significant crown fire activity and so the flame heights were either comparable to or even exceeded h this implies that h h is likely simply a crude estimate of mean mid flame height 4 case studies the case studies used here were drawn from a set of studies in spark designed for validation purposes swedosh et al 2018 we selected the forcett fire 2013 the kilmore fire 2009 the lithgow fire 2013 the mount cooke fire 2003 and the pickering brook fire 2005 the setup for the cases follows swedosh et al 2018 however we only used wind data from nearby automatic weather stations aws whereas swedosh et al tested the effect of different wind data on the fire isochrones all simulations used the dry eucalypt cheney et al 2012 and csiro grasslands cheney et al 1998 rate of spread models 4 1 case 1 forcett fire 2013 tas the ignition was caused by a campfire which was started at 14 00 on january 3 2013 and remained active until january 18 2013 this fire burned about 23 960 hectares with a perimeter of around 310 km marsden smedley 2014 the wind data used for this simulation was from the dunalley stroud point aws the geosciences australia digital elevation model 30 m raster was used for the dem data the curing was assumed to be 80 everywhere as a reasonable estimate for the time of year that the fire occurred a constant fuel age of 8 years was assumed for the simulation and fuel hazard scores were then estimated from standard fuel growth curves cheney et al 2012 an initial fire of radius 100 m was assumed at 147 6751 e 42 8065 s marsden smedley 2014 the fire was simulated for a 24 hour period starting on 14 30 on january 3 2013 the simulation covered the initial development of the fire the overnight decrease in fire activity and subsequent intensification of fire the fire spread rapidly as a crown fire from approximately 12 30 to 14 30 on january 4 2013 4 2 case 2 kilmore fire 2009 vic the kilmore fire in 2009 was part of the devastating black saturday fires the ignition time was approximately 11 45 am on 7 february caused by failed power line the fire was driven by extreme northwesterly winds and travelled around 50 km southeast with a narrow fire front cruz et al 2012 provides a detailed description of this fire the intense fire had generated a considerable number of downwind spotfires which later merged into a long fire front these were not simulated in this analysis for this simulation wallan kilmore gap aws m data was used along with a 30 m topographical raster curing was taken to be 100 based on the time of year and the fuel age was assumed to be a constant 15 years with the fuel hazard scores estimated from fuel growth curves the initial condition was based on the isochrones at 1300 in cruz et al 2012 and the fire was simulated for 4 hours 4 3 case 3 lithgow fire 2013 nsw the lithgow fire a part of the state mine fire started as a minor fire on 16 october 2013 near a defence force training base at marrangaroo and travelled up to 25 km on 17 october the fire burnt out more than 55 000 hectares between lithgow and bilpin the simulation assumed a curing value of 60 and a fuel age of 18 years the initial fire shape was based upon a reconstruction of the fire at 1159 on 17 october 2013 and the simulation was ended at 2115 on 17 october 2013 mount boyce aws weather data was used in these simulations 4 4 case 4 mount cooke fire 2003 wa the mount cooke fire started from a lightning strike at night and grew to approximately 30 hectares by the morning the fire rapidly intensified on the morning of 10 january 2003 and burnt through jarrah eucalyptus marginata forest the simulation here relies on information in johnston et al 2008 the initial fire was assumed to be at 116 295 e 32 375 s with a radius of 310 m at 0800 on 10 january 2003 and the fire was simulated until 0500 11 january 2003 this fire event was complicated with rapid intensification crown fire and spotting activity suppression activity and natural easing of fire conditions overnight fuel age and hence hazard scores were based upon the maps shown in johnston et al 2008 the curing coefficient was assumed to be 80 weather data from bickley aws were used as was the standard 30 m topographical raster from ga 4 5 case 5 pickering brook fire 2005 wa this wildfire burnt 27 700 hectares of jarrah forest in the pickering brook area of perth hills in western australia during january 2005 cheney 2010 the fires were started by a series of deliberate ignitions fuel age data was sourced from cheney 2010 and mapped to fuel hazard scores using fuel growth curves bickley aws weather data were used for this simulation as was the usual 30 m ga raster for topography the curing level was assumed to be 80 three initial fires were used to initialise the simulations based on cheney 2010 the first fire started at 1800 on 15 january 2005 and the simulation was run for 25 h successful and partially successful suppression efforts influenced the shape of the western and southern fire line however no suppression effects were included in the model 5 simulation results the simulated isochrones for all case studies with and without the spatially varying waf model and the simulated burnt area as a function of time are shown in figs 5 to 9 the jaccard score is used to provide quantitative comparisons between the predicted and the observed final fires the jaccard score jaccard 1912 is 15 jaccard score t p t p f p f n where t p true positive are grid cells where both the simulated fire and the observed fire were burnt f p false positive are grid cells where the simulated fire is burnt and the observed fire was not burnt f n false negative are grid cells where the simulated fire is not burnt but the observed fire was burnt the jaccard score is a ratio between the true positive and the union of simulated and observed burnt areas and for perfectly overlapping simulated and observed fires the jaccard score is one for perfectly disjoint fires the jaccard score is zero impossible since the simulated ignition is always within the observed fire burnt area the jaccard score does not quantify errors in the fire prediction made over time nor does it provide significant insight into the reason for any errors such as the selection of wind data or the lack of spotting or suppression models in the simulation the jaccard scores presented in fig 10 a show that the predicted quantities for with and with non spatially varying waf are quite similar except for the mount cooke fire case for the individual cases the forcett case is worse using spatially varying waf kilmore shows a slight improvement using spatially varying waf lithgow is marginally worse using spatially varying waf mount cooke shows a large improvement using spatially varying waf and pickering brook is worse when using spatially varying waf the tp and fn scores are shown in figs 10b and 10c respectively the tp predictions are improved using the spatially varying waf model in all cases except pickering brook where the spatially varying waf model performs slightly worse than the simulation without the spatially varying waf model mount cook shows considerable improvement the fn predictions are also typically better with the spatially varying waf model than without the spatially varying waf model 6 discussion the inclusion of a physically motivated spatially varying waf provides some improved prediction of the overall fire line when compared to the predictions without the spatially varying waf however the use of a spatially varying waf still does not significantly improve predictions in all cases compared to the final fire perimeters there are a number of possible reasons for the disagreement between the predictions and the final perimeter irrespective of whether with or without the spatially varying waf model was used factors such as driving wind wind changes fuel conditions suppression activities and spotting are not well represented by the fire spread model and these factors dominate the prediction of the final fire perimeter over the wind adjustment factor the lithgow fire simulations are a particularly good example of the sensitivity of the predictions to the input weather data the weather data from an aws at mt boyce was used as inputs to the simulation mt boyce was approximately 20 km from the fire and the wind direction of mt boyce data was south westerly rather than a westerly as suggested by the final fire perimeter swedosh et al 2018 also simulated this fire using gridded numerical weather data from the bureau of meteorology predicted by the bureau s weather model in this case they found the final fire perimeter was greatly over predicted to the south east they attributed this discrepancy primarily to the north westerly wind direction in the gridded data rather than the westerly direction suggested by the final fire perimeter the other cases used aws data at significant distances from the fire to obtain driving wind data the aws data used for the mt cooke simulation also likely over estimated the relative humidity and fuel moisture content resulting in a considerably reduced ros the aws was located in bickley approximately 60 km north west of the fire spotting was not included in any of these simulations except to the extent implicitly included in the dry eucalypt model cheney et al 2012 spotting was significant in all cases swedosh et al 2018 there were two initial burn areas in the lithgow fire suggesting spotting activity which could increase the ros to the east the kilmore fire exhibited long range spotting up to a distance of several kilometers to the south east similarly mt cooke exhibited spotting of up to 5 km while pickering brooke exhibited shorter range spotting of up to 1 km an improved explicit treatment of spotting may improve the fire spread predictions with or without a spatially varying waf the forcett fire occurred in complicated vegetation which was classified into a reduced number of standard fuel types and a constant fuel age was assumed swedosh et al 2018 the simulated final fire perimeter may improve if the representation of the fuel type and load were used no attempt was made to represent the suppression efforts nor control lines in any of the fires the most prominent example is the pickering brooke fire which was subjected to control lines which slowed or stopped fire spread along the southern flank the lack of suppression in the simulation led to over prediction of the extent of the southern flank the suppression efforts in the actual fire led to the irregular fingered shape of the observed southern flank the western flank of the forcett fire was also controlled and so the simulations tend to over predict the fire spread in this location most if not all of these fires involved significant crown fire spread cheney 2010 cruz et al 2012 marsden smedley 2014 which justifies the selection of h h because the flame length from a crown fire exceeds the forest height crown fire flames are driven by less obstructed winds than sub canopy fires the above canopy wind profile is affected by the canopy there is a shear layer present above the canopy harman and finnigan 2007 however far above the canopy the wind profile tends to a logarithmic profile and eventually to a blended layer as shown in fig 1 b c therefore the waf will decrease to unity above the canopy and therefore predictions of crown fires will be less effected by the selected value of waf the foundation of this study was an idealised model of sub and above canopy flow due to inoue 1963 harman and finnigan 2007 this model assumes that the forest is infinitely deep and homogeneous and in this implementation we also neglect any variations in atmospheric stability the assumption of an infinitely deep forest is of course unrealistic however harman and finnigan 2007 showed that their model provided good agreement to observations for a significant part of the canopy for finite depth canopies belcher et al 2003 provides some estimates of near ground velocity profile which could be adopted because the fires considered here were large and the relevant height h for the waf was found to be on the order of the canopy height h this additional step appears unnecessary near ground corrections may be required for small fires contained entirely within the canopy forests are inhomogeneous considerable variation in lai and h are shown in the raster maps of lithgow fig 3 this work assumes that the wind and fire spread rate adjust quickly to variations in the forest in an empirical and simulation study at the edge of a pine forest dupont and brunet 2008a found the wind flow adjusted to the forest over a distance in excess of 10 h and dependent on the lad i e the vertical heterogeneity of the forest is also important for this adjustment length it is likely that at clearing forest boundaries the wind speed is underestimated by using the developed sub canopy wind speed profile and thus including a transition zone make improve model predictions the effect of small changes in lai rather than a step change at a forest boundary is not well understood and further research will be required to completely characterise the effects of forest heterogeneity additionally further work will be required to understand how the fire spread rate responds to a change in sub canopy wind speed we have also assumed that the fire itself does not modify the lai obviously a crown fire will consume much of the leafy material within a tree crown the assumption that the lai of a spatial location does not change with time is equivalent to the assumption that the lai is dominated by the large branches and trunks of the trees which do not burn away easily the spread model used here is based on wind speed away from the fire and any effects of pyrogenic winds resulting from in drafts into the fire plume are not explicitly included in the fire spread model used here therefore it is fundamentally difficult to incorporate any waf into the fire spread model that includes degradation of the forest and pyrogenic winds for other models such as hilton et al 2018 such a refinement may be possible we have only used a constant lad approximation whereas the model of massman et al 2017 allows variation of lad with height because we found the near top and above canopy flow to be the most relevant to the fire spread the model of massman et al 2017 will likely only give marginal improvements the resolution of the lai and height maps used here were at the maximum available of 0 01 decimal degrees which corresponds to an approximately 500 m by 500 m cell on the ground the relative coarseness of this data compared to the fire simulation resolution of 30 m by 30 m cells means that the effect of spatially varying waf resulting from these maps is a very coarse grained correction to the rate of spread a possible advantage of the waf model is the ability to correct the fire isochrones at fine scales which would allow more accurate time of arrivals to be predicted for sensitive areas such as peri urban areas however this would require much higher resolution lai and height maps which are not currently available but may be in the future as new remote sensing technology becomes available additionally the waf model may need to be extended to include edge effects where the forest transitions from one lai to another causing the sub canopy wind profile to vary over some streamwise distance primarily the case studies consisted of large crown fires in smaller less intense fires such as hazard reduction burns the flame length may be significantly smaller than the canopy height for such fires the waf may be much larger than the values identified for these case studies the waf may then have a more significant effect on the predicted isochrones for less intense fires than observed in this study a worthwhile extension of this research is to consider compare the predictions with low intensity fires however comprehensive datasets for such fires in australia appears sparse as such the question about the effect of the waf on the spread of low intensity fires remains open 7 conclusion in this study we implemented the harman finnigan waf model in an operational model spark this waf model takes into consideration of vegetation height and canopy density at every computational cell as the waf varies spatially with vegetation and temporally with wind we term it as spatially varying waf through this research we assess to what extent the wind flow is affected by the canopy density and forest height on the day of fire incidents the height at which the waf is calculated was determined by trial simulations that moved to minimise disagreement between the simulated and observed fires in all cases we found the relevant height to calculate the waf h was equal to the canopy height h this could be due to significant crown fire activity in the cases studied in our case studies we observed a significant improvement in the mount cooke fire prediction largely by increasing the true positive agreement between the simulated and observed fire areas with the exception of the pickering brook case all other true positive predictions improved using the spatially varying waf model reductions in false negative predictions were observed for all but the pickering brook case however the jaccard scores only improved with the spatially varying waf model in the mount cooke and kilmore cases all other cases the jaccard scores were slightly worse with the spatially varying waf despite improvements in the true positive and false negative predictions this suggests that false positive predictions increase with the spatially varying waf model this can be considered conservative i e having a safer margin a possible interpretation of this result is that the waf which is used heuristically in operational practice cannot sufficiently correct the empirical fire spread model such that the predictions closely match the observations and other more detailed modelling approaches may be required for fire spread under a canopy overall the spatially varying waf appears to improve the prediction of fire propagation further improvement may be made by implementing the massman model massman et al 2017 which accounts for non uniform leaf area density however for practical applications high resolution lad data will be required for all forest locations furthermore plant area index pai data is needed for the massman model with increasing use of lidar technology the required lad and pai data may be available in future software availability the wildfire framework spark is available at https research csiro au spark the geospatial framework geostack used in spark is available as an open source library at https gitlab com geostack library and as a python package available on conda forge declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgement we acknowledge the contribution of the bushfire and natural hazard cooperative research centre bnhcrc through the project fire spread across fuel types we would like to acknowledge the valuable feedback of dr nazmul khan of the institute for sustainable industries and liveable cities at victoria university melbourne during the implementation of the spatially varying waf model appendix harman and finnigan adopt the following function form of ψ as a good fit to the data ψ z z d 1 ϕ z l β z d z ϕ z 1 c exp β z d 2 l a 1 c 1 κ 2 β exp 1 4 this is an indefinite integral which may be evaluated with a numerical quadrature scheme in these cases eq a 1 was evaluated with a cumulative riemann sum the models of canopy and open wind velocity are functions of height z however there are actually only three relevant values of z namely the blending height z z b the canopy top z h and the height at which w a f is calculated z h therefore if h is constant such as in operational models it is unnecessary to compute ψ z instead all that is required is ψ h and ψ z b which after simplification become a 2 ψ h c h d e c z z d z a 3 ψ z b c z b d t e c z z d z these two integrals eqs a 2 and a 3 can be efficiently calculated using exponentially weighted quadrature due to the presence of the e z term a 4 z 0 e c z z d z i e c z i z i δ z i where δ z i k i δ z 0 with δ z 0 the initial quadrature spacing chosen here to be 1 0 e 3 and z i z 0 i δ z i the value of k is the exponential step increase chosen in this implementation to be 1 2 in the quadrature scheme the upper bound is taken as z 50 which is sufficiently far above the canopy as to be effectively infinite 
25417,modeling is an essential tool for studying environmental systems due to the interdisciplinary nature several models should be integrated to consider the nexus between different natural processes however the computational cost and accuracy of physics based numerical models are always limited even contradictory in this study a novel paradigm for integrating numerical models and machine learning models is proposed based on a previous study which integrates wofost hydrus and modflow in the paradigm the modflow is replaced by radial basis function neural network to reduce the computational costs with comparable accuracy object modeling system is used as modeling framework which provides a run time environment for the components a synthetic case is implemented with data collected from linze inland river basin research station in northwestern of china furthermore the computation efficiency and generalization ability are discussed by refining the modflow grids and changing the driving forces precipitation the proposed paradigm shows significant better performance on flexibility and computational efficiency with similar accuracy and acceptable generalization ability which would be further improved by larger dataset this paper explores a computational efficiency paradigm by integrating physics based and machine learning models which also provides a reference for physics informed guided model integration keywords model integration physics based model machine learning eco hydrological data availability data will be made available on request 1 introduction earth system modeling is a basic research method for earth system science and has been regarded as the second copernican revolution schellnhuber 1999 modeling can be considered a comprehensive approach to formalizing existing knowledge of earth system science models are capable of characterizing global and local behaviors of earth system to improve the understanding of the system models can reproduce the past states and forecast the future states of the earth system through scenario analysis based on different assumptions furthermore models can help researchers stakeholders and decision makers be prepared for possible changes in the future to obtain optimized and or sustainable solutions reid et al 2010 weaver et al 2013 since 1960s a wide variety of numerical models diersch 2014 hughes et al 2017 schneider et al 2017 steffen et al 2020 trenberth 1992 have been developed and applied to simulate the subsystems of the earth systems amanambu et al 2020 chen et al 2020b cuthbert et al 2019 erler et al 2019 yang et al 2019 zhou et al 2020 such as atmospheric circulation cotton et al 1995 biogeochemical processes in the sea soetaert et al 2000 mantle dynamics yoshida and santosh 2011 tsunamis impacts gelfenbaum et al 2011 etc increasingly detailed models finer resolutions and larger domains are simulated with the increasing availability of computing resources and the increasing availability of earth observations which stimulates complexity of the model because of the potentially high level of interactions between variables and parameters wagener and pianosi 2019 furthermore there is a consensus that the study of earth system should have a global vision cheng and li 2015 which considers the interaction between atmosphere ocean cryosphere land surfaces and the biosphere cheng et al proposed the watershed as a basic system unit of the complex earth to study water land air plant human system and their nexus cheng and li 2015 cheng et al 2014 li et al 2018 thus considered a watershed model as a basin scale earth system model integrated numerical models have been developed to incorporate connected processes arnold and fohrer 2005 bardazzi and bosello 2021 brunner and simmons 2012 feng et al 2018 kollet and maxwell 2006 markstrom et al 2008 furthermore agent based model bonabeau 2002 and modeling environment e g object modeling system oms ahuja et al 2005 open modeling interface openmi gregersen et al 2005 were developed to facilitate the integration of different models for simulating the multidisciplinary nature of natural systems and human systems e g integrating climate system and socio economic behavior of society stanton et al 2009 integrating crop growth and production model unsaturated flow model and saturated flow model peña haro et al 2012 the aforementioned aspects the big data challenges asch et al 2018 and the demise of the laws of dennard and moore khan et al 2018 require a rethinking of the way of simulating earth system bauer et al 2021 in recent years the machine learning algorithms have been thoroughly investigated in many researches jordan and mitchell 2015 mjolsness and decoste 2001 and applied in domains of science business and government e g computer vision natural language processing automatic driving knowledge discovery etc theoretically machine learning models can approximate any earth system with any precision by training machine learning algorithms with big data barron 1993 cybenko 1989 kolmogorov 1957 the application of machine learning algorithms in earth system dates back to almost 30 years ago in which the neural networks were conducted to classify the land cover benediktsson et al 1990 and clouds lee et al 1990 using landsat mss imagery a three layer neural network was then developed to forecast rainfall intensity fields in space and time french et al 1992 another class of problem where machine learning algorithms have been good at is regression three machine learning algorithms multi layer perceptron mlp radial basis function rbf network and support vector machine svm were used to simulate the groundwater dynamics in a study area located in the northwestern china chen et al 2020a furthermore deep learning algorithms have been gradually used to exploit spatiotemporal structures features and information in the data chen et al 2021 in general the machine learning models are projections between model inputs and outputs after training process which consume limited computing resources despite the opportunities of deep learning in earth system modeling certain challenges and avenues interpretability physical consistency complex and uncertain data limited labels computational demand exist therefore physics informed physics guided or theory informed theory guided machine learning algorithms are proposed karniadakis et al 2021 karpatne et al 2017a read et al 2019 a lot of progresses have been made in this area such as the physics informed neural network pinn rueden et al 2021 which coupled physical equations into the loss function of the neural network the physics guided neural network pgnn karpatne et al 2017b and the physics guided recursive neural network pgrnn jia et al 2021 which added extra reward and punishment mechanisms to the loss function according to the physical consistency to enhance the robustness of the training process physical feature enhancement which combined physical parameters with the hidden layer pawar et al 2021 post processing of surface water classification results by adding an elevation constraint to avoid inconsistencies in classification labels khandelwal et al 2017 furthermore reichstein et al suggested that the theory driven and data driven approaches i e physical models and machine learning models should be integrated to incorporate the interpretability and extrapolation capability of the physical models with the flexibility in adapting data and the capability of discovering unexpected patterns of machine learning models reichstein et al 2019 on the one hand the numerical model outputs can be used to obtain a mixed model cho et al applied lstm to simulate the residual errors between the observations and the numerical model outputs so as to obtain a mixed model by merging the lstm and numerical model outputs to improve the accuracy and generalization ability cho and kim 2022 on the other hand the inputs and outputs of the numerical model can be used to train a new machine learning model to be merged into a hybrid framework to avoid the shortcomings of the two separated models hanachi et al 2019 in this paper a new paradigm for integrating physics driven and data driven models is proposed to incorporate the advantages of both physics based models and machine learning models an eco hydrological model is developed to validate the paradigm by integrating a crop growth and production model wofost an unsaturated flow model hydrus and a saturated flow model based on a previous study peña haro et al 2012 the saturated flow model which was modflow in peña haro et al 2012 is replaced by a well trained machine learning model in this study the integration of the three models is conducted in oms which is a modeling framework providing a run time environment for the components the data meteorological soil characteristics crop etc collected from linze inland river basin research station in northwestern of china was used to drive the integrated model finally the flexibility computational efficiency accuracy and generalization ability of the proposed paradigm is discussed 2 methods 2 1 object modeling system object modeling system oms is a pure java object oriented and lightweight environmental modeling framework which is now jointly developed by the u s department of agriculture usda agricultural research service ars usda natural resources conservation service nrcs and colorado state university csu david et al 2010 the principal idea of oms lies in the principles of component based software engineering and java native access based platform which enables the modularity and interoperability components are main building blocks of simulation models in oms a component represents an executable simulation with a sufficient level of complexity e g a model model developers can continue to use their familiar programming languages e g fortran c or c to create models while model integrators use java integrated development environments ide to create components and integrated models 2 2 wofost the world food studies wofost model is a mechanistic dynamic model for the quantitative analysis of the growth and production of field crops major processes of wofost include i phenological development which serves as a controlling and steering mechanism for plant growth by a dimensionless variable dvs development stage which affected by vernalization photoperiod reduction factors and temperature ii co2 assimilation and respiration which is calculated from the absorbed radiation the photosynthesis light response curve of individual leaves and the ambient temperature iii leaf growth and senescence which affected by initial dry weight ambient temperature potential limited resources and life span for leaves self shading water stress respectively iv roots which implemented as a fixed daily increase until either a crop specific maximum depth or a soil defined maximum depth is reached v transpiration which applied the penman monteith reference evapotranspiration de wit et al 2019 monteith 1965 to calculate the water loss from a crop to the atmosphere vi soil moisture which calculated by a soil water balance model in order to estimate potential water limited production de wit et al 2019 to facilitate the simulation of any crop growing wofost reads the parameters for crop soil and weather and additional information running information default values and other support information in separated data files these input files include crop files which provide a specific set of parameters for the simulated crops soil files which contain information on physical soil characteristics weather files which contain the meteorological data the simulation results of wofost are written to four different output files includes out file which contains detailed output of simulation pps which contains summary output for potential growth wps which contains summary output for water limited growth and sum which contains summary output the output variables of wofost include the crop development stage dvs the leaf area index of the crop lai the total weight roots twrt the crop transpiration tra the crop rooting depth rd the root zone soil moisture sm the amount of water available in the rooted zone wwlow etc detail introduction of the wofost refer to boogaard et al 2021 2 3 hydrus hydrus is a physically based model for simulating water heat and solute movement in unsaturated partially saturated or fully saturated porous media hydrus 1d is used in the experiments which only considers the water flows from surface to groundwater in the vertical direction under rainfall a modified form of the richards equation which neglects water flow due to thermal gradient is numerically solved for saturated and unsaturated water flow eq 1 1 θ t z k h z 1 s where θ is the volumetric water content h is the water pressure head t is time z is the spatial coordinate positive upward k is the unsaturated hydraulic conductivity function given by 2 k h x k s x k r h x where k r is the relative hydraulic conductivity and k s is the saturated hydraulic conductivity the flow equation eq 1 incorporates a sink term s to account for water uptake by plant roots the water flow part of the model considers prescribed head and flux boundaries as well as boundaries controlled by atmospheric conditions free drainage or flow to horizontal drains the processes of evaporation and plant transpiration also exert a major influence on water distributions in near surface environments potential evapotranspiration is calculated with the penman monteith combination equation the input parameters and data for hydrus 1d are included in five separate input files i selector in which includes some basic information water flow information time information root growth information and root water uptake information ii profile dat which includes the nodal information iii atmosph in which includes atmospheric information iv fit in inverse solution information v meteo in which includes meteorological information the simulated pressure head water content and temperature at specified observation nodes are saved in several output files t level out run inf out obs node out rassam et al 2018 2 4 rbf neural network the rbf neural network is a feedforward neural network fnn with strong nonlinear fitting ability simple learning rule and convenient implementation haykin 1998 in the context of machine learning a neural network with the radial basis function as activation function is called a rbf neural network which can approximate any nonlinear function with strong generalization ability the rbf neural network uses linear combinations of a radially symmetric function based on euclidean distance the structure of rbf neural network is a three layer fnn with input layer reading the inputs hidden layer mapping the input vectors to high dimension space output layer calculating a linear combination of the hidden layer outputs fig 1 generally the outputs of rbf neural network are calculated as 3 y n p 1 p w p n φ r p c where p represents the number of nodes in the hidden layer and the corresponding number of radial basis functions r p x x p is the euclidean distance in which x is the center of input variables x p is vector of input variables at the ith sampling point c is a non negative constant φ is the radial basis function w p n p 1 2 p n 1 2 n are unknown weighting coefficients to be determined the non negative constant c is always omitted thus the outputs of rbf neural network are calculated as 4 y n p 1 p w p n φ r p p 1 p w p n φ x x p the basic idea of rbf is to transform the data into a high dimensional space so that it is linearly separable in the high dimensional space francois chollet 2017 the commonly used basis functions include linear cubic thin plate spline gaussian multi quadric inverse multi quadric etc gutmann 2001 the gaussian radial basis function is adopted as 5 φ r e r 2 σ 2 where σ is the standard deviation 2 5 integration of physics based and machine learning models in this study we propose a paradigm to integrate the physics based numerical model with machine learning models this paradigm based on the component based software engineering which considers an executable simulation with a sufficient level of complexity as a component oms3 is a lightweight framework different from the traditional framework that provides a large number of apis it allows plain objects plain old java objects that are annotated to be used within the framework in one of our previous studies we coupled wofost hydrus 1d and modflow by reprogramming them as components in oms to simulate the interaction between crop growth and unsaturated saturated flow processes peña haro et al 2012 in the present study one component of the coupled model is replaced by a machine learning model fig 2 ensuring that the interfaces and behaviors of the machine learning model are identical or approximate to the original component to illustrate this idea the modflow which simulates saturated flow is replaced by a machine learning model rbf model by training rbf neural network in the coupled model the data flow is identical to the previous study shown in fig 3 peña haro et al 2012 wofost is used to calculate the growth of crops with the inputs of crop parameters and meteorological data wofost model outputs crop characteristic parameters such as lai root depth rd and plant height ch lai and rd are used as inputs to hydrus 1d hydrus 1d applies penman monteith formula to calculate potential evaporation and inputs data such as irrigation precipitation daily net radiation daily maximum minimum temperature daily wind speed and daily relative humidity feddes equation is used to calculate the water uptake of crop roots hydrus 1d also calculates the outflow from the unsaturated zone and transfers it to the groundwater model as the input of boundary conditions the groundwater model uses the outflow value calculated by hydrus 1d to calculate the groundwater head value the obtained groundwater head value is converted into pressure head and transmitted back to hydrus 1d as the lower boundary condition the rbf model is used to replace the original calculation file in modflow the coupled model is implemented in oms modeling framework in order to meet the requirements of oms framework the metadata mechanism of the framework is used to expose component interfaces and ensure data exchange wofost hydrus 1d and rbf neural network models are originally developed independently for the purpose of executable programs in order to couple wofost hydrus 1d and rbf in oms3 these three models must be modified and encapsulated into components of oms3 the packaging of wofost and hydrus 1d refers to peña haro et al 2012 the rbf neural network is implemented using python which cannot be directly called by oms3 therefore rbf is encapsulated into java component table 1 domain specific language dsl is used to describe the component among which the input output variables of the component are marked with metadata in out the main executive function of the module is marked with execute the data transfer scheme between wofost and hydrus 1d adopts the methods of li et al 2011 and jian et al 2012 and the data transfer between hydrus 1d and rbf adopts the boundary condition updating scheme proposed by peña haro peña haro et al 2012 3 experiments a synthetic case is conducted to validate the proposed paradigm the data and settings used in this experiment are described in this section 3 1 data the linze inland river basin research station 100 07 e 39 20 n amsl 1374 m of the chinese academy of sciences irbrs cas which is located in the middle reaches of heihe river basin northwestern china is selected as study area it is a semi arid area with high shallow groundwater level less precipitation large evaporation and fragile ecosystem the water resources in this area are experiencing unbalanced development and utilization high pressure of irrigation water and further shortage of water resources caused by heihe river water transfer therefore comprehensive consideration of the interaction between crop growth unsaturated zone and saturated zone would provide relative holistic view of the eco hydrological processes the daily meteorological data irradiation air temperature vapor pressure wind speed and precipitation collected at the irbrs cas from january 1st 1991 to december 31st 1991 is used wofost has developed most of the crop files which contains a specific set of parameters that are used for describing the phenology assimilation and respiration characteristics and partitioning of assimilates to plant organs and so on creating new crop files can be cumbersome because of the necessary large number of data collecting therefore the crop file maiz w41 in wofost database is selected in this study we intend to replace modflow in the previous study peña haro et al 2012 with rbf neural network the rbf neural network should be trained and tested before coupling with wofost and hydrus in the coupled model rbf model should provide the same interface as modflow does therefore we first extract the inputs and outputs of modflow by decoupling the model in the previous study peña haro et al 2012 two inputs variables of groundwater recharge rech and actual evapotranspiration aet and one output variable of groundwater head hb are extracted to form the dataset data collected in a complete growth period are divided into training and test sets with the proportion of 70 and 30 respectively because of the different units of groundwater level groundwater recharge and actual evapotranspiration the data is normalized as follows 6 x x μ σ where x represents the normalized dataset μ is the average value of x σ is the variance of x the pre processing rescales the variance and mean value of the normalized data to 1 and 0 3 2 experiments setup the model assumes a two dimensional vertical aquifer with a height of 20 m a length of 200 m and a width of 5 m the schematic diagram of the simulated system is shown in fig 4 the aquifer is discretized by grids with size of 5 m 5 m the initial hydraulic heads vary from 17 m on the left to 18 m on the right side of the domain the hydraulic heads between the left and right side are interpolated the saturated hydraulic conductivity and specific yield of the aquifer are 2 m d and 0 14 the simulated depth of hydrus 1d is 3 5 m with the soil profile being discretized into 351 nodes the soil columns are assumed to be homogeneous with residual water content saturated water content air entry pressure head and the pore size distribution index after brooks corey being θ r 0 11 θ s 0 32 h b 0 168 m and λ 0 17 respectively the total study area is divided into two sub areas i e zone 1 and zone 2 the crops grown in the two sub areas are assumed to be maize with crop parameters stored in maiz w41 an irrigation of 452 mm and a transient precipitation of 32 mm a are applied the model simulates a complete growth process of crops from early may to mid september with 122 days from sowing to harvest daily interval is used in the simulation due to the availability of data as mentioned above the inputs and outputs of rbf model should be identical to modflow therefore only two input variables which include groundwater recharge rech and actual evapotranspiration aet and one output variable which is groundwater head hb are used in rbf neural networks the center of rbf x in eq 4 the weights w in eqs 3 and 4 and the number of nodes in hidden layer p in eqs 3 and 4 are trained and determined separately the k means clustering algorithm is used to determine the center vector through self organizing learning the selected center is located in the important area of the input space and the iterative least square method lms is used to adjust the connection weights of the hidden layer and the output layer the optimal structure of rbf model is obtained within the training dataset shown in fig 5 human experience is also used while training rbf neural network such as only 10 nodes in the hidden layer are implemented because of the relatively small dataset the rbf neural network is then tested with the test dataset rbf neural network after training rbf model is used to replace the saturated flow model modflow the performance of rbf model is evaluated using root mean square error rmse shown in eq 7 7 r m s e 1 n i 1 n m i o i 2 where n is the total number of observations m and o represent the output values from rbf model and modflow respectively 3 3 results 3 3 1 the training and validation of rbf neural network in this study rbf neural network is used to replace modflow to provide interfaces for wofost and hydrus therefore the rbf neural network should be trained and validated to ensure the approximation of the modflow behavior the inputs and outputs of modflow are considered as the inputs and labels of rbf neural network after training the outputs of rbf model in two sub areas are analyzed with modflow outputs the results in the training stage are represented by scatter plots fig 6 which indicates the rbf model match the modflow reasonably in which red line represents a perfect match a quantitative comparison between the outputs of rbf model and modflow in the training stage is conducted by rmse the rmse values in training stage are 0 0527 and 0 0970 for zone 1 and zone 2 the parameters of rbf model are optimized by the training dataset and tested using test dataset the results of rbf model and modflow in the test stage are shown in fig 7 although larger differences exist the blue points distribute near the red line perfect match which indicates that the outputs of rbf model approximate the outputs of modflow a quantitative comparison between the outputs of rbf model and modflow in the test stage is also conducted by calculating rmse for each sub area the rmse values for zone 1 and zone 2 in the test stage are 0 0646 and 0 1391 respectively it is well known that the amount of data is an important factor for rbf model and all data driven method therefore the difference between rbf model and modflow could be further improved by larger dataset e g more growth periods 3 3 2 integration of wofost hydrus and rbf model after training and testing the rbf model is integrated with wofost and hydrus to provide feedbacks specifically the rbf model offers lower boundary condition by transforming groundwater level into pressure head for the unsaturated zone model hydrus the groundwater levels in the two sub areas are both simulated by rbf model the results of the integrated model in the previous study peña haro et al 2012 which coupled wofost hydrus and modflow refer to whm hereafter are also demonstrated to evaluate the integrated model in this study refer to whr hereafter as shown in fig 4 the initial hydraulic heads of the two sub areas are different due to the different groundwater levels and pressure heads in the unsaturated zone the soil water content in the unsaturated zone is different which would have certain impacts on the water uptake process of crop roots and then affects the growth of crops the results of the lai in the two sub areas are shown in fig 8 which indicates a reasonable match between whr and whm the general laws of crop growth are also captured by whr which is reflected by the same trend of the results from whr and whm in fig 8 one can find the relative high impacts of groundwater levels on lai in different growth periods from about one month after sowing the crops are in seedling stage and panicle stage successively in these two stages the plants are mainly engaged in root development and plant growth which requires little water jian he et al 2010 therefore little impact of different groundwater levels on the lai is observed from the beginning of june to the middle of july the crops are between the flowering and grain stage during this period the leaves expand one after another which results in the rapid increasing of lai the lai of crops in zone 2 with higher groundwater level is significantly higher than those in zone 1 the crops are in the mature stage from mid july to the end of the experiment in which the lai gradually decreased in the coupling scheme the groundwater recharge of saturated flow model modflow or rbf model is calculated by the unsaturated flow model hydrus which is affected by the crop growth the different lai between zone 1 and zone 2 shown in fig 8 indicates the different growth phase which results in different recharge for saturated flow model shown in fig 9 the groundwater recharge calculated by hydrus in zone 2 is higher than that in zone 1 due to the higher initial hydraulic heads in zone 2 fig 9 indicates that the groundwater recharge obtained by whr is similar to whm which proves that the substitution of integrated model components is reliable the pressure heads in zone 1 and zone 2 simulated by whr and whm are illustrated in fig 10 for both zone 1 and zone 2 the general trends of pressure heads simulated by whr are consistent with those simulated by whm which indicates that the behaviors of rbf model in the integration are similar to modflow the interface that rbf model provide to hydrus is reasonable figs 9 and 10 also indicate that the pressure heads generated by rbf model respond to the groundwater recharge correctly however certain fluctuation of the pressure heads calculated by rbf model can be observed in fig 10 there are peaks in the curves generated by rbf model where no groundwater recharge exists this may be resolved by larger datasets for training and testing which cannot be satisfied in this study 3 4 discussion 3 4 1 computational efficiency to evaluate the computational efficiency of the proposed paradigm the aquifer of the synthetic case is refined to 1 m 1 m which results in the increase of grids from 1 40 to 5 200 in modflow apart from the discretization scheme other configurations are identical with experiments in section 3 2 the refined modflow is then integrated with wofost and hydrus the rbf model trained in section 3 2 is then used to replace the refined modflow the refinement of grids changes the values of the exchanged data among different models the results of the new whr model which replaces the refined modflow by rbf model show acceptable performance for simplicity only pressure heads which are simulated by whr and whm are shown fig 11 fig 11 indicates that even without further training whr is able to capture the dynamics of the whole system this may benefit from the mechanism and constraint that the rbf model learn from the physics based numerical model modflow the computational cost of the integrated model is then analyzed and evaluated by the running time on a specified computer a relative obsoleted set of hardware and software shown in table 2 is selected purposefully to show the limited requirements to run the integrated model the average running time of ten model runs of whr and whm is used to eliminate random errors on average the running time of whr and whm are 63 5 s and 67 8 s in the experiment in section 3 2 respectively about 5 s which account for 6 of the running time is reduced due to the model replacement from numerical modflow to machine learning rbf models there are two possible reasons which prevent further reduction of the computational cost the first reason may be the java based oms framework the intrinsic computational expensive drawbacks of java language e g java virtual machine decreases the difference between whr and whm because the main contribution of this study is the proposal of a new paradigm which integrating the numerical and machine learning model this may be improved by integrating models with more efficient modeling framework or programming languages e g python c etc the second reason may lie in the relatively small scale only 1 40 grids with grid size of 5 m 5 m in modflow of the synthetic case in the experiment in section 3 2 the running time of rbf model and modflow for simulating one time interval one day alone without being integrated in oms are 0 060 s and 0 065 s respectively the difference between the rbf model and modflow is already relatively small therefore more grids 5 200 grids with grid size of 1 m 1 m are refined in modflow in this section although the number of grids in this synthetic case 1000 is far less than in the real case always hundreds of thousands of grids depending on the study area and spatial resolution the increase of computation cost for whm is significant in this experiment the running time of whr and whm are 67 4 s and 322 0 s respectively the running time costs for whm increases 254 2 s which is five times more than that in experiment 1 on the contrary the running time of whr remains the same magnitude which account for 20 of the computational resources that whm costs it is well known that there is a strong positive correlation even exponential correlation between the computational cost of numerical models and the scale and resolution of the problem therefore the decrease of computational cost can be more significant in a more complex problem and real case 3 4 2 generalization ability generalization ability which indicates the model performance in an unknown new dataset is an important issue for machine learning models there are several levels of generalization 1 the adaptability across different datasets e g training dataset and test dataset 2 the adaptability of different driving forces 3 the adaptability of different initial boundary conditions 4 the adaptability of different research fields the adaptability of rbf model across different datasets i e training dataset and test dataset is evaluated in section 3 3 1 and 3 3 2 furthermore the generalization ability of trained rbf model to work in different driving forces is evaluated by applying the model in several scenarios with changed precipitation increase a constant value of 0 5 mm which is one of the most important driving forces because of the existence of two sub areas i e zone 1 and zone 2 the precipitation is increased in two cases the precipitation only in zone 1 increases with 0 5 mm and the precipitation in both zone 1 and zone 2 increases with 0 5 mm these two cases are used to evaluate the response of the models to different precipitation it should be noted that either the precipitation in single zone zone 1 or both zones is increased the models are applied in both zones to evaluate the generalization ability the rbf model is executed alone without wofost and hydrus and within whr respectively the different scenarios and corresponding applied models are summarized in table 3 the results of whm are used as baseline for different scenarios the outputs of groundwater model i e the pressure heads are used as performance indicators shown in fig 12 the changed precipitation influences the pressure heads by changing the groundwater recharge as expected the pressure heads increase with the precipitation in all scenarios and applied models comparing with the status quo because zone 1 directly receives increased precipitation in all scenarios the pressure heads response the increasing precipitation in the first few days in scenario 2 only precipitation in zone 1 increases with 0 5 mm which results in the significant increasing pressure heads with 20 cm compared with the results from whm in scenario 3 the pressure heads in scenario 2 are relatively lower fig 12 a this may be caused by the reduced horizontal flow between zone 1 and zone 2 the increased pressure heads in zone 1 decrease the hydraulic gradient between the two zones which reduces the lateral flow from zone 2 to zone 1 in scenario 3 because both zone 1 and zone 2 directly receive the increased precipitation the pressure heads in both zones significantly increase with quick responses in the first few days as stated the pressure heads in zone 1 from whm in scenario 3 are higher than those in scenario 2 rbf model is executed alone to evaluate the generalization ability fig 12 a indicates that even with some errors the rbf model is able to capture the trend of pressure heads from whm the errors may come from 1 the intrinsic errors of the rbf model 2 the random errors in the dataset in scenario 3 the rmse values of the pressure heads in zone 1 between the rbf model and the whm is 0 0736 the whr which integrates wofost hydrus and rbf model is then applied in scenario 3 the pressure heads generated from whr are compared with those from whm which indicates a reasonable match the difference of pressure heads between rbf model and whr may be caused by the interaction between three models in scenario 3 the rmse values of pressure heads in zone 1 between whr and whm is calculated with the value of 0 0790 it should be noted that the performance of rbf model is better than that of whr this may be caused by the error accumulation while the rbf model interacting with the other models in scenario 2 the response of pressure heads in zone 2 on the increased precipitation in zone 1 is relatively slow about one month which is reasonable considering the indirectly increased precipitation fig 12 b the increase of pressure heads in zone 2 is also not as significant as in zone 1 in scenario 3 the pressure heads in zone 2 significantly increase due to the directly increased precipitation in both zones compared with whm the performance of rbf model and whr are fairly good by analyzing the pressure heads the rmse values of pressure heads in zone 2 between the rbf model whr and whm are 0 4564 and 0 4605 respectively to clearly demonstrate the results from whm and whr the pressure heads from whm and whr are separated and shown in fig 13 the comparisons between different models in zone 1 and zone 2 indicate that all the models are able to capture the principles of the three interacted systems and prove the acceptable generalization ability of the machine learning models in different driving forces 3 4 3 other aspects flexibility is an advantage of the proposed paradigm in the study the rbf neural network is trained using the same inputs and outputs of modflow however the inputs and labels of rbf neural network is not limited to the inputs and outputs of modflow on the one hand the rbf neural network can be trained using observed data groundwater recharge actual evapotranspiration and groundwater head in this study while available which would also have positive influences on the model performance for instance chen et al trained rbf neural network with observed groundwater pumping rates irrigation rates streamflow rates and groundwater levels to simulate groundwater dynamics chen et al 2020a on the other hand training rbf neural networks more generally machine learning algorithms with the inputs and outputs of numerical physics based models more generally under physical constraints is an important branch of machine learning physics informed guided machine learning which produces interpretable physical consistent results furthermore in the proposed paradigm wofost and hydrus can be replaced by machine learning models depending on the data availability to further reduce the computational costs with acceptable accuracy however this may lead to another issue generalization ability which was discussed in section 3 4 2 although the trade off between accuracy generalization ability and computational costs still exists the computation costs are significantly reduced with acceptable accuracy generalization ability considering the accuracy and generalization ability would be improved by implementing larger dataset in the proposed paradigm by replacing one of the physics based numerical models i e modflow with a machine learning model i e rbf model a surrogate model wang et al 2014 is built there are four main categories of problems utilizing surrogate models including prediction surrogate assisted optimization decision support and sensitivity uncertainty analysis reducing runtime and improving computational efficiency are the main advantages of applying surrogate models in any categories commonly used approaches for surrogate modeling are response surface model rsm radial basis function rbf model polynomial chaos expansion pce model kriging model support vector regression svr model artificial neural network ann model boosted trees and random forests model kudela and matousek 2022 the rbf model surrogate for modflow which act as a component of the integrated model could significantly reduce the computational costs this may benefit possible applications in all the above mentioned four categories especially in real time model predictions decision support and sensitivity uncertainty analysis shortcomings of surrogate models do exist in the accuracy and generalization ability essentially the surrogate model rbf model approximates the complex model modflow which can be regarded as a second level of abstraction of the real world problems in this study although the rmse values between rbf model and modflow are relatively low the errors between the surrogate model and the real world problems are inevitable besides improving the approaches for surrogate modeling considering the real world observations while constructing the surrogate model may be an effective solution constructing surrogate models based on real world observations may reduce a level of abstraction which is always more accurate than the physics based numerical models generalization ability is another issue that should be considered while applying surrogate models as discussed in chen et al 2020a physics based numerical models are constructed based on the physical mechanisms and equations e g modflow is based on the groundwater flow equation however surrogate models are mostly mappings between the inputs and outputs based on statistics without considering the physical mechanisms and equations besides increasing training data and improving modeling methods involving physical mechanisms while constructing surrogate models which results in physics informed physics guided or theory informed theory guided machine learning algorithms may be a feasible solution for this issue 4 conclusions in this paper a new paradigm for integrating physics based numerical and machine learning models was proposed to improve computational efficiency of the integrated model with comparable accuracy an eco hydrological model which integrated crop growth unsaturated flow and saturated flow processes was developed to consider the interactions and influences between different processes based on one of our previous studies which integrated wofost hydrus and modflow we replaced the physics based numerical model modflow with a machine learning model rbf model furthermore a synthetic case was implemented with data collected from linze inland river basin research station in northwestern of china the performance of rbf model and modflow were compared and analyzed which indicated a reasonable match between them the eco hydrological model which integrated wofost hydrus and rbf model whr was then implemented in the oms modeling framework with the rbf model providing interfaces for wofost and hydrus the results from whr were then compared and analyzed with the eco hydrological model which integrated wofost hydrus and modflow whm which indicated the feasibility of the proposed paradigm furthermore the pros and cons computational efficiency accuracy and generalization ability of the proposed paradigm were discussed the model grids in modflow was refined to form a relatively realistic example the computational costs of whr remained while those of whm increased dramatically with the refinement of model grids the generalization ability was revealed by changing the driving forces of the models although only one component modflow was replaced by machine learning model rbf model the proposed paradigm was not supposed to be limited to in the era of big geoscience data all the components of an integrated model can be approximated and replaced by machine learning models with the balance between accuracy generalization ability and computational efficiency declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements the authors would like to sincerely thank prof buren qian for sharing the office with the authors this work was supported by the national natural science foundation of china grant number 62006247 the national key r d program of china grant number 2019yfc1510501 
25417,modeling is an essential tool for studying environmental systems due to the interdisciplinary nature several models should be integrated to consider the nexus between different natural processes however the computational cost and accuracy of physics based numerical models are always limited even contradictory in this study a novel paradigm for integrating numerical models and machine learning models is proposed based on a previous study which integrates wofost hydrus and modflow in the paradigm the modflow is replaced by radial basis function neural network to reduce the computational costs with comparable accuracy object modeling system is used as modeling framework which provides a run time environment for the components a synthetic case is implemented with data collected from linze inland river basin research station in northwestern of china furthermore the computation efficiency and generalization ability are discussed by refining the modflow grids and changing the driving forces precipitation the proposed paradigm shows significant better performance on flexibility and computational efficiency with similar accuracy and acceptable generalization ability which would be further improved by larger dataset this paper explores a computational efficiency paradigm by integrating physics based and machine learning models which also provides a reference for physics informed guided model integration keywords model integration physics based model machine learning eco hydrological data availability data will be made available on request 1 introduction earth system modeling is a basic research method for earth system science and has been regarded as the second copernican revolution schellnhuber 1999 modeling can be considered a comprehensive approach to formalizing existing knowledge of earth system science models are capable of characterizing global and local behaviors of earth system to improve the understanding of the system models can reproduce the past states and forecast the future states of the earth system through scenario analysis based on different assumptions furthermore models can help researchers stakeholders and decision makers be prepared for possible changes in the future to obtain optimized and or sustainable solutions reid et al 2010 weaver et al 2013 since 1960s a wide variety of numerical models diersch 2014 hughes et al 2017 schneider et al 2017 steffen et al 2020 trenberth 1992 have been developed and applied to simulate the subsystems of the earth systems amanambu et al 2020 chen et al 2020b cuthbert et al 2019 erler et al 2019 yang et al 2019 zhou et al 2020 such as atmospheric circulation cotton et al 1995 biogeochemical processes in the sea soetaert et al 2000 mantle dynamics yoshida and santosh 2011 tsunamis impacts gelfenbaum et al 2011 etc increasingly detailed models finer resolutions and larger domains are simulated with the increasing availability of computing resources and the increasing availability of earth observations which stimulates complexity of the model because of the potentially high level of interactions between variables and parameters wagener and pianosi 2019 furthermore there is a consensus that the study of earth system should have a global vision cheng and li 2015 which considers the interaction between atmosphere ocean cryosphere land surfaces and the biosphere cheng et al proposed the watershed as a basic system unit of the complex earth to study water land air plant human system and their nexus cheng and li 2015 cheng et al 2014 li et al 2018 thus considered a watershed model as a basin scale earth system model integrated numerical models have been developed to incorporate connected processes arnold and fohrer 2005 bardazzi and bosello 2021 brunner and simmons 2012 feng et al 2018 kollet and maxwell 2006 markstrom et al 2008 furthermore agent based model bonabeau 2002 and modeling environment e g object modeling system oms ahuja et al 2005 open modeling interface openmi gregersen et al 2005 were developed to facilitate the integration of different models for simulating the multidisciplinary nature of natural systems and human systems e g integrating climate system and socio economic behavior of society stanton et al 2009 integrating crop growth and production model unsaturated flow model and saturated flow model peña haro et al 2012 the aforementioned aspects the big data challenges asch et al 2018 and the demise of the laws of dennard and moore khan et al 2018 require a rethinking of the way of simulating earth system bauer et al 2021 in recent years the machine learning algorithms have been thoroughly investigated in many researches jordan and mitchell 2015 mjolsness and decoste 2001 and applied in domains of science business and government e g computer vision natural language processing automatic driving knowledge discovery etc theoretically machine learning models can approximate any earth system with any precision by training machine learning algorithms with big data barron 1993 cybenko 1989 kolmogorov 1957 the application of machine learning algorithms in earth system dates back to almost 30 years ago in which the neural networks were conducted to classify the land cover benediktsson et al 1990 and clouds lee et al 1990 using landsat mss imagery a three layer neural network was then developed to forecast rainfall intensity fields in space and time french et al 1992 another class of problem where machine learning algorithms have been good at is regression three machine learning algorithms multi layer perceptron mlp radial basis function rbf network and support vector machine svm were used to simulate the groundwater dynamics in a study area located in the northwestern china chen et al 2020a furthermore deep learning algorithms have been gradually used to exploit spatiotemporal structures features and information in the data chen et al 2021 in general the machine learning models are projections between model inputs and outputs after training process which consume limited computing resources despite the opportunities of deep learning in earth system modeling certain challenges and avenues interpretability physical consistency complex and uncertain data limited labels computational demand exist therefore physics informed physics guided or theory informed theory guided machine learning algorithms are proposed karniadakis et al 2021 karpatne et al 2017a read et al 2019 a lot of progresses have been made in this area such as the physics informed neural network pinn rueden et al 2021 which coupled physical equations into the loss function of the neural network the physics guided neural network pgnn karpatne et al 2017b and the physics guided recursive neural network pgrnn jia et al 2021 which added extra reward and punishment mechanisms to the loss function according to the physical consistency to enhance the robustness of the training process physical feature enhancement which combined physical parameters with the hidden layer pawar et al 2021 post processing of surface water classification results by adding an elevation constraint to avoid inconsistencies in classification labels khandelwal et al 2017 furthermore reichstein et al suggested that the theory driven and data driven approaches i e physical models and machine learning models should be integrated to incorporate the interpretability and extrapolation capability of the physical models with the flexibility in adapting data and the capability of discovering unexpected patterns of machine learning models reichstein et al 2019 on the one hand the numerical model outputs can be used to obtain a mixed model cho et al applied lstm to simulate the residual errors between the observations and the numerical model outputs so as to obtain a mixed model by merging the lstm and numerical model outputs to improve the accuracy and generalization ability cho and kim 2022 on the other hand the inputs and outputs of the numerical model can be used to train a new machine learning model to be merged into a hybrid framework to avoid the shortcomings of the two separated models hanachi et al 2019 in this paper a new paradigm for integrating physics driven and data driven models is proposed to incorporate the advantages of both physics based models and machine learning models an eco hydrological model is developed to validate the paradigm by integrating a crop growth and production model wofost an unsaturated flow model hydrus and a saturated flow model based on a previous study peña haro et al 2012 the saturated flow model which was modflow in peña haro et al 2012 is replaced by a well trained machine learning model in this study the integration of the three models is conducted in oms which is a modeling framework providing a run time environment for the components the data meteorological soil characteristics crop etc collected from linze inland river basin research station in northwestern of china was used to drive the integrated model finally the flexibility computational efficiency accuracy and generalization ability of the proposed paradigm is discussed 2 methods 2 1 object modeling system object modeling system oms is a pure java object oriented and lightweight environmental modeling framework which is now jointly developed by the u s department of agriculture usda agricultural research service ars usda natural resources conservation service nrcs and colorado state university csu david et al 2010 the principal idea of oms lies in the principles of component based software engineering and java native access based platform which enables the modularity and interoperability components are main building blocks of simulation models in oms a component represents an executable simulation with a sufficient level of complexity e g a model model developers can continue to use their familiar programming languages e g fortran c or c to create models while model integrators use java integrated development environments ide to create components and integrated models 2 2 wofost the world food studies wofost model is a mechanistic dynamic model for the quantitative analysis of the growth and production of field crops major processes of wofost include i phenological development which serves as a controlling and steering mechanism for plant growth by a dimensionless variable dvs development stage which affected by vernalization photoperiod reduction factors and temperature ii co2 assimilation and respiration which is calculated from the absorbed radiation the photosynthesis light response curve of individual leaves and the ambient temperature iii leaf growth and senescence which affected by initial dry weight ambient temperature potential limited resources and life span for leaves self shading water stress respectively iv roots which implemented as a fixed daily increase until either a crop specific maximum depth or a soil defined maximum depth is reached v transpiration which applied the penman monteith reference evapotranspiration de wit et al 2019 monteith 1965 to calculate the water loss from a crop to the atmosphere vi soil moisture which calculated by a soil water balance model in order to estimate potential water limited production de wit et al 2019 to facilitate the simulation of any crop growing wofost reads the parameters for crop soil and weather and additional information running information default values and other support information in separated data files these input files include crop files which provide a specific set of parameters for the simulated crops soil files which contain information on physical soil characteristics weather files which contain the meteorological data the simulation results of wofost are written to four different output files includes out file which contains detailed output of simulation pps which contains summary output for potential growth wps which contains summary output for water limited growth and sum which contains summary output the output variables of wofost include the crop development stage dvs the leaf area index of the crop lai the total weight roots twrt the crop transpiration tra the crop rooting depth rd the root zone soil moisture sm the amount of water available in the rooted zone wwlow etc detail introduction of the wofost refer to boogaard et al 2021 2 3 hydrus hydrus is a physically based model for simulating water heat and solute movement in unsaturated partially saturated or fully saturated porous media hydrus 1d is used in the experiments which only considers the water flows from surface to groundwater in the vertical direction under rainfall a modified form of the richards equation which neglects water flow due to thermal gradient is numerically solved for saturated and unsaturated water flow eq 1 1 θ t z k h z 1 s where θ is the volumetric water content h is the water pressure head t is time z is the spatial coordinate positive upward k is the unsaturated hydraulic conductivity function given by 2 k h x k s x k r h x where k r is the relative hydraulic conductivity and k s is the saturated hydraulic conductivity the flow equation eq 1 incorporates a sink term s to account for water uptake by plant roots the water flow part of the model considers prescribed head and flux boundaries as well as boundaries controlled by atmospheric conditions free drainage or flow to horizontal drains the processes of evaporation and plant transpiration also exert a major influence on water distributions in near surface environments potential evapotranspiration is calculated with the penman monteith combination equation the input parameters and data for hydrus 1d are included in five separate input files i selector in which includes some basic information water flow information time information root growth information and root water uptake information ii profile dat which includes the nodal information iii atmosph in which includes atmospheric information iv fit in inverse solution information v meteo in which includes meteorological information the simulated pressure head water content and temperature at specified observation nodes are saved in several output files t level out run inf out obs node out rassam et al 2018 2 4 rbf neural network the rbf neural network is a feedforward neural network fnn with strong nonlinear fitting ability simple learning rule and convenient implementation haykin 1998 in the context of machine learning a neural network with the radial basis function as activation function is called a rbf neural network which can approximate any nonlinear function with strong generalization ability the rbf neural network uses linear combinations of a radially symmetric function based on euclidean distance the structure of rbf neural network is a three layer fnn with input layer reading the inputs hidden layer mapping the input vectors to high dimension space output layer calculating a linear combination of the hidden layer outputs fig 1 generally the outputs of rbf neural network are calculated as 3 y n p 1 p w p n φ r p c where p represents the number of nodes in the hidden layer and the corresponding number of radial basis functions r p x x p is the euclidean distance in which x is the center of input variables x p is vector of input variables at the ith sampling point c is a non negative constant φ is the radial basis function w p n p 1 2 p n 1 2 n are unknown weighting coefficients to be determined the non negative constant c is always omitted thus the outputs of rbf neural network are calculated as 4 y n p 1 p w p n φ r p p 1 p w p n φ x x p the basic idea of rbf is to transform the data into a high dimensional space so that it is linearly separable in the high dimensional space francois chollet 2017 the commonly used basis functions include linear cubic thin plate spline gaussian multi quadric inverse multi quadric etc gutmann 2001 the gaussian radial basis function is adopted as 5 φ r e r 2 σ 2 where σ is the standard deviation 2 5 integration of physics based and machine learning models in this study we propose a paradigm to integrate the physics based numerical model with machine learning models this paradigm based on the component based software engineering which considers an executable simulation with a sufficient level of complexity as a component oms3 is a lightweight framework different from the traditional framework that provides a large number of apis it allows plain objects plain old java objects that are annotated to be used within the framework in one of our previous studies we coupled wofost hydrus 1d and modflow by reprogramming them as components in oms to simulate the interaction between crop growth and unsaturated saturated flow processes peña haro et al 2012 in the present study one component of the coupled model is replaced by a machine learning model fig 2 ensuring that the interfaces and behaviors of the machine learning model are identical or approximate to the original component to illustrate this idea the modflow which simulates saturated flow is replaced by a machine learning model rbf model by training rbf neural network in the coupled model the data flow is identical to the previous study shown in fig 3 peña haro et al 2012 wofost is used to calculate the growth of crops with the inputs of crop parameters and meteorological data wofost model outputs crop characteristic parameters such as lai root depth rd and plant height ch lai and rd are used as inputs to hydrus 1d hydrus 1d applies penman monteith formula to calculate potential evaporation and inputs data such as irrigation precipitation daily net radiation daily maximum minimum temperature daily wind speed and daily relative humidity feddes equation is used to calculate the water uptake of crop roots hydrus 1d also calculates the outflow from the unsaturated zone and transfers it to the groundwater model as the input of boundary conditions the groundwater model uses the outflow value calculated by hydrus 1d to calculate the groundwater head value the obtained groundwater head value is converted into pressure head and transmitted back to hydrus 1d as the lower boundary condition the rbf model is used to replace the original calculation file in modflow the coupled model is implemented in oms modeling framework in order to meet the requirements of oms framework the metadata mechanism of the framework is used to expose component interfaces and ensure data exchange wofost hydrus 1d and rbf neural network models are originally developed independently for the purpose of executable programs in order to couple wofost hydrus 1d and rbf in oms3 these three models must be modified and encapsulated into components of oms3 the packaging of wofost and hydrus 1d refers to peña haro et al 2012 the rbf neural network is implemented using python which cannot be directly called by oms3 therefore rbf is encapsulated into java component table 1 domain specific language dsl is used to describe the component among which the input output variables of the component are marked with metadata in out the main executive function of the module is marked with execute the data transfer scheme between wofost and hydrus 1d adopts the methods of li et al 2011 and jian et al 2012 and the data transfer between hydrus 1d and rbf adopts the boundary condition updating scheme proposed by peña haro peña haro et al 2012 3 experiments a synthetic case is conducted to validate the proposed paradigm the data and settings used in this experiment are described in this section 3 1 data the linze inland river basin research station 100 07 e 39 20 n amsl 1374 m of the chinese academy of sciences irbrs cas which is located in the middle reaches of heihe river basin northwestern china is selected as study area it is a semi arid area with high shallow groundwater level less precipitation large evaporation and fragile ecosystem the water resources in this area are experiencing unbalanced development and utilization high pressure of irrigation water and further shortage of water resources caused by heihe river water transfer therefore comprehensive consideration of the interaction between crop growth unsaturated zone and saturated zone would provide relative holistic view of the eco hydrological processes the daily meteorological data irradiation air temperature vapor pressure wind speed and precipitation collected at the irbrs cas from january 1st 1991 to december 31st 1991 is used wofost has developed most of the crop files which contains a specific set of parameters that are used for describing the phenology assimilation and respiration characteristics and partitioning of assimilates to plant organs and so on creating new crop files can be cumbersome because of the necessary large number of data collecting therefore the crop file maiz w41 in wofost database is selected in this study we intend to replace modflow in the previous study peña haro et al 2012 with rbf neural network the rbf neural network should be trained and tested before coupling with wofost and hydrus in the coupled model rbf model should provide the same interface as modflow does therefore we first extract the inputs and outputs of modflow by decoupling the model in the previous study peña haro et al 2012 two inputs variables of groundwater recharge rech and actual evapotranspiration aet and one output variable of groundwater head hb are extracted to form the dataset data collected in a complete growth period are divided into training and test sets with the proportion of 70 and 30 respectively because of the different units of groundwater level groundwater recharge and actual evapotranspiration the data is normalized as follows 6 x x μ σ where x represents the normalized dataset μ is the average value of x σ is the variance of x the pre processing rescales the variance and mean value of the normalized data to 1 and 0 3 2 experiments setup the model assumes a two dimensional vertical aquifer with a height of 20 m a length of 200 m and a width of 5 m the schematic diagram of the simulated system is shown in fig 4 the aquifer is discretized by grids with size of 5 m 5 m the initial hydraulic heads vary from 17 m on the left to 18 m on the right side of the domain the hydraulic heads between the left and right side are interpolated the saturated hydraulic conductivity and specific yield of the aquifer are 2 m d and 0 14 the simulated depth of hydrus 1d is 3 5 m with the soil profile being discretized into 351 nodes the soil columns are assumed to be homogeneous with residual water content saturated water content air entry pressure head and the pore size distribution index after brooks corey being θ r 0 11 θ s 0 32 h b 0 168 m and λ 0 17 respectively the total study area is divided into two sub areas i e zone 1 and zone 2 the crops grown in the two sub areas are assumed to be maize with crop parameters stored in maiz w41 an irrigation of 452 mm and a transient precipitation of 32 mm a are applied the model simulates a complete growth process of crops from early may to mid september with 122 days from sowing to harvest daily interval is used in the simulation due to the availability of data as mentioned above the inputs and outputs of rbf model should be identical to modflow therefore only two input variables which include groundwater recharge rech and actual evapotranspiration aet and one output variable which is groundwater head hb are used in rbf neural networks the center of rbf x in eq 4 the weights w in eqs 3 and 4 and the number of nodes in hidden layer p in eqs 3 and 4 are trained and determined separately the k means clustering algorithm is used to determine the center vector through self organizing learning the selected center is located in the important area of the input space and the iterative least square method lms is used to adjust the connection weights of the hidden layer and the output layer the optimal structure of rbf model is obtained within the training dataset shown in fig 5 human experience is also used while training rbf neural network such as only 10 nodes in the hidden layer are implemented because of the relatively small dataset the rbf neural network is then tested with the test dataset rbf neural network after training rbf model is used to replace the saturated flow model modflow the performance of rbf model is evaluated using root mean square error rmse shown in eq 7 7 r m s e 1 n i 1 n m i o i 2 where n is the total number of observations m and o represent the output values from rbf model and modflow respectively 3 3 results 3 3 1 the training and validation of rbf neural network in this study rbf neural network is used to replace modflow to provide interfaces for wofost and hydrus therefore the rbf neural network should be trained and validated to ensure the approximation of the modflow behavior the inputs and outputs of modflow are considered as the inputs and labels of rbf neural network after training the outputs of rbf model in two sub areas are analyzed with modflow outputs the results in the training stage are represented by scatter plots fig 6 which indicates the rbf model match the modflow reasonably in which red line represents a perfect match a quantitative comparison between the outputs of rbf model and modflow in the training stage is conducted by rmse the rmse values in training stage are 0 0527 and 0 0970 for zone 1 and zone 2 the parameters of rbf model are optimized by the training dataset and tested using test dataset the results of rbf model and modflow in the test stage are shown in fig 7 although larger differences exist the blue points distribute near the red line perfect match which indicates that the outputs of rbf model approximate the outputs of modflow a quantitative comparison between the outputs of rbf model and modflow in the test stage is also conducted by calculating rmse for each sub area the rmse values for zone 1 and zone 2 in the test stage are 0 0646 and 0 1391 respectively it is well known that the amount of data is an important factor for rbf model and all data driven method therefore the difference between rbf model and modflow could be further improved by larger dataset e g more growth periods 3 3 2 integration of wofost hydrus and rbf model after training and testing the rbf model is integrated with wofost and hydrus to provide feedbacks specifically the rbf model offers lower boundary condition by transforming groundwater level into pressure head for the unsaturated zone model hydrus the groundwater levels in the two sub areas are both simulated by rbf model the results of the integrated model in the previous study peña haro et al 2012 which coupled wofost hydrus and modflow refer to whm hereafter are also demonstrated to evaluate the integrated model in this study refer to whr hereafter as shown in fig 4 the initial hydraulic heads of the two sub areas are different due to the different groundwater levels and pressure heads in the unsaturated zone the soil water content in the unsaturated zone is different which would have certain impacts on the water uptake process of crop roots and then affects the growth of crops the results of the lai in the two sub areas are shown in fig 8 which indicates a reasonable match between whr and whm the general laws of crop growth are also captured by whr which is reflected by the same trend of the results from whr and whm in fig 8 one can find the relative high impacts of groundwater levels on lai in different growth periods from about one month after sowing the crops are in seedling stage and panicle stage successively in these two stages the plants are mainly engaged in root development and plant growth which requires little water jian he et al 2010 therefore little impact of different groundwater levels on the lai is observed from the beginning of june to the middle of july the crops are between the flowering and grain stage during this period the leaves expand one after another which results in the rapid increasing of lai the lai of crops in zone 2 with higher groundwater level is significantly higher than those in zone 1 the crops are in the mature stage from mid july to the end of the experiment in which the lai gradually decreased in the coupling scheme the groundwater recharge of saturated flow model modflow or rbf model is calculated by the unsaturated flow model hydrus which is affected by the crop growth the different lai between zone 1 and zone 2 shown in fig 8 indicates the different growth phase which results in different recharge for saturated flow model shown in fig 9 the groundwater recharge calculated by hydrus in zone 2 is higher than that in zone 1 due to the higher initial hydraulic heads in zone 2 fig 9 indicates that the groundwater recharge obtained by whr is similar to whm which proves that the substitution of integrated model components is reliable the pressure heads in zone 1 and zone 2 simulated by whr and whm are illustrated in fig 10 for both zone 1 and zone 2 the general trends of pressure heads simulated by whr are consistent with those simulated by whm which indicates that the behaviors of rbf model in the integration are similar to modflow the interface that rbf model provide to hydrus is reasonable figs 9 and 10 also indicate that the pressure heads generated by rbf model respond to the groundwater recharge correctly however certain fluctuation of the pressure heads calculated by rbf model can be observed in fig 10 there are peaks in the curves generated by rbf model where no groundwater recharge exists this may be resolved by larger datasets for training and testing which cannot be satisfied in this study 3 4 discussion 3 4 1 computational efficiency to evaluate the computational efficiency of the proposed paradigm the aquifer of the synthetic case is refined to 1 m 1 m which results in the increase of grids from 1 40 to 5 200 in modflow apart from the discretization scheme other configurations are identical with experiments in section 3 2 the refined modflow is then integrated with wofost and hydrus the rbf model trained in section 3 2 is then used to replace the refined modflow the refinement of grids changes the values of the exchanged data among different models the results of the new whr model which replaces the refined modflow by rbf model show acceptable performance for simplicity only pressure heads which are simulated by whr and whm are shown fig 11 fig 11 indicates that even without further training whr is able to capture the dynamics of the whole system this may benefit from the mechanism and constraint that the rbf model learn from the physics based numerical model modflow the computational cost of the integrated model is then analyzed and evaluated by the running time on a specified computer a relative obsoleted set of hardware and software shown in table 2 is selected purposefully to show the limited requirements to run the integrated model the average running time of ten model runs of whr and whm is used to eliminate random errors on average the running time of whr and whm are 63 5 s and 67 8 s in the experiment in section 3 2 respectively about 5 s which account for 6 of the running time is reduced due to the model replacement from numerical modflow to machine learning rbf models there are two possible reasons which prevent further reduction of the computational cost the first reason may be the java based oms framework the intrinsic computational expensive drawbacks of java language e g java virtual machine decreases the difference between whr and whm because the main contribution of this study is the proposal of a new paradigm which integrating the numerical and machine learning model this may be improved by integrating models with more efficient modeling framework or programming languages e g python c etc the second reason may lie in the relatively small scale only 1 40 grids with grid size of 5 m 5 m in modflow of the synthetic case in the experiment in section 3 2 the running time of rbf model and modflow for simulating one time interval one day alone without being integrated in oms are 0 060 s and 0 065 s respectively the difference between the rbf model and modflow is already relatively small therefore more grids 5 200 grids with grid size of 1 m 1 m are refined in modflow in this section although the number of grids in this synthetic case 1000 is far less than in the real case always hundreds of thousands of grids depending on the study area and spatial resolution the increase of computation cost for whm is significant in this experiment the running time of whr and whm are 67 4 s and 322 0 s respectively the running time costs for whm increases 254 2 s which is five times more than that in experiment 1 on the contrary the running time of whr remains the same magnitude which account for 20 of the computational resources that whm costs it is well known that there is a strong positive correlation even exponential correlation between the computational cost of numerical models and the scale and resolution of the problem therefore the decrease of computational cost can be more significant in a more complex problem and real case 3 4 2 generalization ability generalization ability which indicates the model performance in an unknown new dataset is an important issue for machine learning models there are several levels of generalization 1 the adaptability across different datasets e g training dataset and test dataset 2 the adaptability of different driving forces 3 the adaptability of different initial boundary conditions 4 the adaptability of different research fields the adaptability of rbf model across different datasets i e training dataset and test dataset is evaluated in section 3 3 1 and 3 3 2 furthermore the generalization ability of trained rbf model to work in different driving forces is evaluated by applying the model in several scenarios with changed precipitation increase a constant value of 0 5 mm which is one of the most important driving forces because of the existence of two sub areas i e zone 1 and zone 2 the precipitation is increased in two cases the precipitation only in zone 1 increases with 0 5 mm and the precipitation in both zone 1 and zone 2 increases with 0 5 mm these two cases are used to evaluate the response of the models to different precipitation it should be noted that either the precipitation in single zone zone 1 or both zones is increased the models are applied in both zones to evaluate the generalization ability the rbf model is executed alone without wofost and hydrus and within whr respectively the different scenarios and corresponding applied models are summarized in table 3 the results of whm are used as baseline for different scenarios the outputs of groundwater model i e the pressure heads are used as performance indicators shown in fig 12 the changed precipitation influences the pressure heads by changing the groundwater recharge as expected the pressure heads increase with the precipitation in all scenarios and applied models comparing with the status quo because zone 1 directly receives increased precipitation in all scenarios the pressure heads response the increasing precipitation in the first few days in scenario 2 only precipitation in zone 1 increases with 0 5 mm which results in the significant increasing pressure heads with 20 cm compared with the results from whm in scenario 3 the pressure heads in scenario 2 are relatively lower fig 12 a this may be caused by the reduced horizontal flow between zone 1 and zone 2 the increased pressure heads in zone 1 decrease the hydraulic gradient between the two zones which reduces the lateral flow from zone 2 to zone 1 in scenario 3 because both zone 1 and zone 2 directly receive the increased precipitation the pressure heads in both zones significantly increase with quick responses in the first few days as stated the pressure heads in zone 1 from whm in scenario 3 are higher than those in scenario 2 rbf model is executed alone to evaluate the generalization ability fig 12 a indicates that even with some errors the rbf model is able to capture the trend of pressure heads from whm the errors may come from 1 the intrinsic errors of the rbf model 2 the random errors in the dataset in scenario 3 the rmse values of the pressure heads in zone 1 between the rbf model and the whm is 0 0736 the whr which integrates wofost hydrus and rbf model is then applied in scenario 3 the pressure heads generated from whr are compared with those from whm which indicates a reasonable match the difference of pressure heads between rbf model and whr may be caused by the interaction between three models in scenario 3 the rmse values of pressure heads in zone 1 between whr and whm is calculated with the value of 0 0790 it should be noted that the performance of rbf model is better than that of whr this may be caused by the error accumulation while the rbf model interacting with the other models in scenario 2 the response of pressure heads in zone 2 on the increased precipitation in zone 1 is relatively slow about one month which is reasonable considering the indirectly increased precipitation fig 12 b the increase of pressure heads in zone 2 is also not as significant as in zone 1 in scenario 3 the pressure heads in zone 2 significantly increase due to the directly increased precipitation in both zones compared with whm the performance of rbf model and whr are fairly good by analyzing the pressure heads the rmse values of pressure heads in zone 2 between the rbf model whr and whm are 0 4564 and 0 4605 respectively to clearly demonstrate the results from whm and whr the pressure heads from whm and whr are separated and shown in fig 13 the comparisons between different models in zone 1 and zone 2 indicate that all the models are able to capture the principles of the three interacted systems and prove the acceptable generalization ability of the machine learning models in different driving forces 3 4 3 other aspects flexibility is an advantage of the proposed paradigm in the study the rbf neural network is trained using the same inputs and outputs of modflow however the inputs and labels of rbf neural network is not limited to the inputs and outputs of modflow on the one hand the rbf neural network can be trained using observed data groundwater recharge actual evapotranspiration and groundwater head in this study while available which would also have positive influences on the model performance for instance chen et al trained rbf neural network with observed groundwater pumping rates irrigation rates streamflow rates and groundwater levels to simulate groundwater dynamics chen et al 2020a on the other hand training rbf neural networks more generally machine learning algorithms with the inputs and outputs of numerical physics based models more generally under physical constraints is an important branch of machine learning physics informed guided machine learning which produces interpretable physical consistent results furthermore in the proposed paradigm wofost and hydrus can be replaced by machine learning models depending on the data availability to further reduce the computational costs with acceptable accuracy however this may lead to another issue generalization ability which was discussed in section 3 4 2 although the trade off between accuracy generalization ability and computational costs still exists the computation costs are significantly reduced with acceptable accuracy generalization ability considering the accuracy and generalization ability would be improved by implementing larger dataset in the proposed paradigm by replacing one of the physics based numerical models i e modflow with a machine learning model i e rbf model a surrogate model wang et al 2014 is built there are four main categories of problems utilizing surrogate models including prediction surrogate assisted optimization decision support and sensitivity uncertainty analysis reducing runtime and improving computational efficiency are the main advantages of applying surrogate models in any categories commonly used approaches for surrogate modeling are response surface model rsm radial basis function rbf model polynomial chaos expansion pce model kriging model support vector regression svr model artificial neural network ann model boosted trees and random forests model kudela and matousek 2022 the rbf model surrogate for modflow which act as a component of the integrated model could significantly reduce the computational costs this may benefit possible applications in all the above mentioned four categories especially in real time model predictions decision support and sensitivity uncertainty analysis shortcomings of surrogate models do exist in the accuracy and generalization ability essentially the surrogate model rbf model approximates the complex model modflow which can be regarded as a second level of abstraction of the real world problems in this study although the rmse values between rbf model and modflow are relatively low the errors between the surrogate model and the real world problems are inevitable besides improving the approaches for surrogate modeling considering the real world observations while constructing the surrogate model may be an effective solution constructing surrogate models based on real world observations may reduce a level of abstraction which is always more accurate than the physics based numerical models generalization ability is another issue that should be considered while applying surrogate models as discussed in chen et al 2020a physics based numerical models are constructed based on the physical mechanisms and equations e g modflow is based on the groundwater flow equation however surrogate models are mostly mappings between the inputs and outputs based on statistics without considering the physical mechanisms and equations besides increasing training data and improving modeling methods involving physical mechanisms while constructing surrogate models which results in physics informed physics guided or theory informed theory guided machine learning algorithms may be a feasible solution for this issue 4 conclusions in this paper a new paradigm for integrating physics based numerical and machine learning models was proposed to improve computational efficiency of the integrated model with comparable accuracy an eco hydrological model which integrated crop growth unsaturated flow and saturated flow processes was developed to consider the interactions and influences between different processes based on one of our previous studies which integrated wofost hydrus and modflow we replaced the physics based numerical model modflow with a machine learning model rbf model furthermore a synthetic case was implemented with data collected from linze inland river basin research station in northwestern of china the performance of rbf model and modflow were compared and analyzed which indicated a reasonable match between them the eco hydrological model which integrated wofost hydrus and rbf model whr was then implemented in the oms modeling framework with the rbf model providing interfaces for wofost and hydrus the results from whr were then compared and analyzed with the eco hydrological model which integrated wofost hydrus and modflow whm which indicated the feasibility of the proposed paradigm furthermore the pros and cons computational efficiency accuracy and generalization ability of the proposed paradigm were discussed the model grids in modflow was refined to form a relatively realistic example the computational costs of whr remained while those of whm increased dramatically with the refinement of model grids the generalization ability was revealed by changing the driving forces of the models although only one component modflow was replaced by machine learning model rbf model the proposed paradigm was not supposed to be limited to in the era of big geoscience data all the components of an integrated model can be approximated and replaced by machine learning models with the balance between accuracy generalization ability and computational efficiency declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements the authors would like to sincerely thank prof buren qian for sharing the office with the authors this work was supported by the national natural science foundation of china grant number 62006247 the national key r d program of china grant number 2019yfc1510501 
25418,flow analysis model is useful in designing hydraulic structures understanding hydraulic processes and predicting the impacts of development works and natural events accordingly needs for hydraulic models that can accurately simulate hydraulic phenomena in natural and engineered rivers are increasing here hdm 2di a predictive two dimensional flow simulation model was developed and integrated in rams a mesh generation and visualization tool rams has intuitive user interface in generating and editing meshes and setting boundary conditions with the geometry construction module rams g river cross sections and bank stations can be automatically extracted by applying linear interpolation based spatial functions to the generated mesh the tool was applied to a meandering experimental flume and a natural river to evaluate its accuracy the tool was able to successfully reproduce observed flows in channel and river rams is currently downloadable at www ramsplus net detailed tutorials a user guide and examples are also provided keywords flow analysis model hdm 2di rams predictive tool geometry construction data availability i have shared the software at software availability section software availability name of the software rams developer chang geun song il won seo heon uk im contact information baybreeze119 inu ac kr hardware required basic computer year first available 2020 program language c and fortran cost free software availability http www ramsplus net program size 93 22 mb 1 introduction because of anthropogenic climate change and land use change associated with urbanization damage caused by flooding is increasing to manage rivers and prepare for floods efficiently it is necessary to predict and analyze the hydraulic behavior of river flows however applying modeling results to river maintenance or development works without knowing restrictions can cause inaccurate and inappropriate designs as for the major rivers in south korea they generally feature long reaches wide widths moderate degrees of meandering many in river islands numerous tributaries and wild fluctuation of flow rate therefore multi dimensional hydraulic models featuring low flow analysis and stability are necessary to quantify river flow dynamics in korea several flow analysis models are currently being used to design hydraulic structures understand hydraulic processes predict the impacts of development works and to minimize the damage caused by flood events since flows near channel bends tributaries and around estuary banks weirs and other river structures exhibit multi dimensional characteristics the water level and velocity distribution in such regions cannot be predicted accurately with one dimensional 1d hydraulic models two dimensional 2d flow models can be more useful predictors of flow behavior in waterways several 2d flow codes exist including rma 2 wes 1996 feswms 2dh fhwa 1989 leendertse 1967 and teach 2e gosmam and ideriah 1976 in particular iber model developed by the collaboration of flumen institute polytechnic university of catalonia the water engineering group geama cimne and cedex gains attentions because of its versatility user friendliness and free distribution caro camargo and bayona romero 2018 iber is a numerical modelling tool for turbulent flow in an unsteady state and environmental processes in river hydrodynamics dam break simulation flood zone assessment sediment transport calculation and tidal flow in estuaries gutiérrez garcía et al 2022 the iber s capabilities and performances are thoroughly investigated in recent publication cueva portal et al 2021 these codes apply numerical methods for discretizing the governing flow equations among the various numerical schemes the finite element method and the finite volume method are often favored because they are frequently used in recent in cfd communities with verified performance but there is an ongoing argument in the preference between fem and fvm it is generally accepted that fvm enjoys faster computational time and thus requires less computational resources lukáčová medviďová and teschke 2006 jeong and seong 2014 vasconcellos et al 2020 in contrast fem is a more mathematically robust approach and can use higher order function to approximate the solution in each element wendt and anderson 2008 botti et al 2018 in addition fem allow easier meshing of irregular and complex geometries molina aiz et al 2010 to be specific the finite element method has been often applied for the analysis of hydrodynamic flow and pollutant diffusion behavior in rivers kim et al 2017 2021 choi et al 2020 because irregular terrain and river morphology can be replicated more efficiently and accurately compared with other numerical methods king and norton 1978 brebbia et al 1978 norton 1980 huebner et al 1995 davies 1980 pinder and gray 1977 in addition mesh characteristics such as cell sizes and the number of nodes can be optimized with respect to the accuracy of the numerical solution using terrain adaptive meshes heinrich and pepper 1999 karniadakis and sherwin 2005 chung 1992 furthermore flux type boundary conditions can be imposed in a straightforward manner gresho and sani 1998 fletcher 1984 and solid mathematical foundation of fe approximation enables systematic analyses on error convergence and solution accuracy strang and fix 1973 axelsson and barker 1984 thomee 1984 wait and mitchell 1985 because the finite element method is based on elements rather than nodes it is possible to obtain accurate numerical solutions for nodes inside elements by adopting various interpolation functions ghanem 1995 in this study hdm 2di a predictive simulation model for 2d flow analysis was developed and integrated with rams a user friendly tool for mesh generation and visualization then the predictive tool was applied to both a meandering experimental stream and a natural river and its applicability tested by comparing the numerical simulation results with measured data 2 tool description 2 1 hdm 2di hdm 2di is a 2d numerical model that calculates the flow variables under complex topography with river structures and can be incorporated with the pollutant transport analysis model the model produces velocities and depth results by solving following equations representing mass and momentum conservations 1 h t h u u h 0 2 u t u u g h h 1 h h ν u g n 2 h 4 3 u u where h water depth t time u u 1 u 2 is the depth averaged velocity vector in the x and y directions respectively g gravitational acceleration h bottom elevation ν kinematic eddy viscosity n roughness coefficient and u velocity magnitude equations 1 and 2 were discretized by adopting the streamline upwind petrov galerkin scheme song and oh 2016 the non linear advection term included in eq 2 was linearized by the newton raphson method and the total system of algebraic equations was solved by frontal method with fully implicit time integration applicable fields of the hdm 2di include the simulation and subsequent analysis of subcritical transcritical and supercritical flow rapid flow change dry wet transition and flow around hydraulic structures in particular 2d flow models have been frequently applied to analyze dam or dyke break problems in spite of some arguments concerning their theoretical applicability guan et al 2014 singh et al 2011 developed a 2d shallow flow solver using an explicit central upwind scheme and applied it to the malpasset dam break problem the absolute errors between the predicted front arrival time which is defined as the time when the outermost boundary of flood wave generated by dam break arrives first at a certain point and the field observed data were less than 0 04 song et al 2018a tested their 2d flow simulation code to a dam break with triangular hump problem and compared the predicted results with experimental measurements which showed very good agreements except some discrepancies immediately after opening the sluice gate hien and chien hien and van chien 2021 investigated the flow waves generated by dam break flow and checked the ability of 2d and 3d numerical model they reported that although the 2d and 3d numerical models produced quite similar profiles of water depth and velocity the 3d model can capture the peak values more accurately based on these descriptions it can be concluded that 2d model can be applied to dam break problems with reasonable accuracy however when the analysis is focused on the prediction of the exact peak values or right after the gate opening sophisticated 3d modelling should be implemented the model can successfully represent complex geometries with triangular or rectangular elements or their combination and features various distinctive functions as shown in fig 1 since its first development in 2011 by song 2011 it has been verified and applied to various river flow analysis problems for example the effect of wall boundary conditions and lateral velocity boundary conditions on the flow field was analyzed seo and song 2010 seo song the model produced accurate results with maximum mass conservation error less than 0 7 song et al 2013 however it yielded less accurate predictions as reynolds number increased to improve the accuracy of the hdm 2di simulation for the lateral flow velocity deviation due to secondary currents occurring in the curved part of the river the dispersion stress term was included in the governing equation song et al 2012 the solver for the shallow water equations with dispersion stress term provided satisfactory solutions and the root mean square error rmse of the resultant velocities in a natural river application had mean value of 0 027 however when it applied to a confluent channel the model showed inaccurate performance because three dimensional flow structure prevailed around the joint area of the main and the tributary channels floodplain erosion and the spatial distribution of sediments have been considered song et al 2017 it was shown that the kinematic flow information such as flow depth and velocity obtained by 2d flow modelling can be used to determine the deposition and erosion spots in extreme flood conditions but acceleration which is an onerous parameter to compute was necessary in estimating the spatial distribution of the deposition and erosion and the characteristics of pollutant dispersion associated with tidal current induced flow direction changes in a large natural river have been analyzed park and song 2018 hdm 2di adequately reproduced the rising and falling limbs of the water surface level and the mean absolute percentage errors at two water level gaging stations were 2 18 and 3 19 respectively recently spatiotemporal predictions of inland flooding and river inundation using a wet dry handling technique have been conducted song et al 2018b shin et al 2019 the 2d modelling results matched reasonably well with the measured data over the entire simulation region except the propagating front this was because the 2d model has limitations in capturing the moving front with singularity and discontinuity 2 2 rams as shown in fig 2 rams consists of two submodules rams g and rams gui rams g is a tool to provide subdivided and powerful functions related to mesh generation it can create and edit topographic meshes used in 2d river simulation and incorporates them into various types of geographic file formats it supports flexible meshes triangular meshes quadrilateral meshes and hybrid mesh provides various practically useful functions such as the integration of external geometric files hec ras sms gis cad and reduces the pre processing effort required for mesh generation rams gui provides pre and post processing functions for the simulation engine hdm 2di in particular compared with existing 2d river analysis software mesh generation and editing integrating meshes with the simulation engine and setting boundary conditions can be intuitively configured to increase usability rams is currently downloadable at www ramsplus net at this website users can find tutorials with detailed instructions a user guide and examples an important task in implementing a 2d flow modellings is producing the river topography geometric data in hec ras format are widely used to characterize 2d topography in the topography generation process a digital elevation model dem and a cad file are typically employed together however this process can be demanding and laborious because of its manual nature for example a commercial hydraulic software sms surface water modeling system requires several procedures to proceed 2d modeling based on hec ras survey data first you need to export the cross section location geometry and river centerline data from hec ras to create a sdf file after that in sms the river centerline is generated as cl and the cross section is produced as an xsec file name in order to advance 2d modeling it is necessary to activate the xsec coverage and convert the elevation data of the cross section into scatter based on the transformed cross sectional scatter data a stream line is built and the surface is checked through the triangulation option and the unnecessary parts such as vertical walls are removed from the initial topography according to the scatter data set then you need to construct the outer boundary of the river through the feature point and feature arc menus to complete the mesh generation to ease these burdensome procedures rams g is equipped with a function that can create 2d meshes by directly linking existing topographic data format of the hec ras as shown in fig 3 with rams g river cross sections and bank station can be automatically extracted by applying linear interpolation based spatial distribution treatment to the generated mesh as such this convenient function enables 2d topographic data to be constructed more easily and efficiently 3 applications 3 1 meandering experimental flume a series of tracer tests were performed at the andong river experiment center of the korea institute of civil engineering and building technology see fig 4 the test flumes in the river experiment center reproduce the river shape without distortion by accommodating moderate channel length and width with physical structures the river experiment center has a channel in which three meanders with different curvatures are continuously connected each meander has four apices and the bottom of the waterway is made of sand in addition the channel has naturally created cross sections by flowing water for a certain period the total length of the channel is 682 m the lengths of channels with sinuosities of 1 2 1 5 and 1 7 are 115 m 134 m and 155 m respectively the initial design slope is 0 00125 for the channel bottom and 0 5 for the walls hydraulic measurements were conducted and the results are summarized in table 1 in this table q u w and h stand for the discharge flow velocity channel width and water depth respectively the hydraulic data at six lateral lines were recorded using sontek s adcp s5 for the channel with a sinuosity of 1 5 the average flow rate and mean cross sectional velocity were 1 45 m3 s and 0 70 m s respectively for the channel with a sinuosity of 1 7 the corresponding values were 1 26 m3 s and 0 52 m s before running the flow analysis engine hdm 2di the finite element meshes were constructed using rams g as shown in fig 5 as for the geometry construction of sinuosity 1 5 triangular meshes with 7624 elements and 4126 nodes were produced for sinuosity 1 7 the meshes with 7697 elements and 4182 nodes were generated fig 6 shows the rams windows for inputting boundary conditions model controls and material properties a variety of variables and optional values for model simulation can be entered here the boundary conditions such as the flow rate and water level were determined to be same as the experimental values used in the field measurements in the model control widow parameters such as the time control option wall boundary condition wetting drying adjustment can be specified the viscosity and roughness coefficients were spatially differentiated by assigning anomaly material properties throughout the entire meshes to reflect the variation of vegetation and resistance in the experiment because the bed materials of the natural rivers are usually non uniformly graded sediments the assignment of spatially anomalistic roughness coefficient could be useful in reflecting the gradation of non cohesive sediment size figs 7 and 8 show the flow simulation results with the channels having sinuosities of 1 5 and 1 7 the changes in flow velocity and depth due to irregular vegetation distribution were evidently shown and the velocity configurations and resultant depth formations were well reproduced it can be observed in fig 8 that the water was deep along the center of the channel after section 1 because of the channel geometry shown in fig 4 as shown in table 1 the velocities in the channel with a sinuosity of 1 7 were lower than that with sinuosity 1 5 while the flow depths were deeper with sinuosity 1 7 these can be confirmed when comparing fig 8 with fig 7 in addition the velocity variations along the transverse directions were narrower in 1 7 sinuosity channel because the mean channel width was 0 36 m wider than that with 1 5 sinuosity and the consequent lower velocity induced decelerated flow around the curved parts figs 9 and 10 compare the simulation results and hydraulic measurements of the depth averaged flow velocity and water depth for each section with sinuosity 1 5 and 1 7 respectively in fig 9 the predicted velocities matched well with the measured ones and the predicted water depths at left and right bank showed similar tendency to that of adcp result despite of some errors that existed near mid part of the channel in particular the water depth at y w 0 5 in section 3 and y w 0 5 in section 4 had relatively large errors compared with other regions the cause of that is presumed to be as follows referring to the water depth contour of fig 7 the water depth deepened in the right part between section 2 and section 3 and in the central part along section 4 some errors occurred at these positions while measuring the depth with the deep digging bottom using the ultrasonic wave of adcp the comparisons between simulated and measured values for the average velocity and water depth for each section are listed in table 2 the section averaged coefficient of determination r2 for velocity with sinuosity 1 5 is 0 960 and that for velocity with sinuosity 1 7 is 0 953 the section averaged rmse for depth with sinuosity 1 5 is 0 031 and that for depth with sinuosity 1 7 is 0 031 therefore it can be concluded that the flow predictive tool performs well in reproducing the observed flows in the meandering experimental channel 3 2 a natural river to apply the predictive tool to a natural river the reach from the gumi weir to chilgok weir in the nakdong river the republic of korea was selected as the study area see fig 11 the length of the study reach is 27 3 km in total this reach was selected because the gumi national industrial complexes are located nearby and there have been several accidents in which the river was severely polluted by the chemical spills after the completion of four rivers maintenance project by the korean government in 2011 the residence time of waters flowing from the gumi weir to the chilgok weir reduced from 0 63 days before the construction of the weir to 24 45 days after the construction of the weir therefore when a pollutant is introduced in that reach it may remain longer than before and the environmental impacts will be prolonged simulating pollutant transport in river environments is usually approached using the advection dispersion framework prediction of pollutant transport should be preceded by a flow simulation to generate the flow velocity field and water depth vectors accordingly the flow velocity and water depth should be computed before water quality is assessed and the flow predictive tool including rams g and hdm 2di was used to generate a mesh for the study area the following procedures were followed after calculating the spectral indices of landsat 8 satellite images from 11 12 october 2013 which is the date of the adcp measurements the river extent was extracted according to the normalized difference water index ndwi ndwi is an index used for waterbody detection and it is calculated using the green and the near infrared bands since green light is strongly reflected by waterbody while the near infrared radiation is strongly absorbed mcfeeters 1996 in this study ndwi was calculated using the band 3 green and band 5 near infrared of the landsat 8 satellite images the topographic data were constructed according to the procedures shown in fig 12 fig 12 a and b show the images of band 3 and band 5 of landsat 8 for the study area then the boundary of the river was extracted using the gis tool as show in fig 12 c the fill extent was imported in rams g and using the convert map function the terrain was created as shown in fig 12 d as noted in previous researches calomino et al 2015 lauria et al 2020 tafarojnoruz and lauria 2020 the mesh dependency should be checked through preliminary modellings or sensitivity analysis accordingly five mesh layouts were produced and solution convergence and accuracy by comparing with field measurements were tested after that mesh layout with 32 127 elements and 17 730 nodes were finally selected fig 12 e to run hdm 2di boundary conditions and flow parameters have to be determined the boundary conditions were set as follows using the observed data set provided by the water resources management information system wamis of the nakdong river flood control center the flow conditions recorded at the same time as the adcp geometry measurement were adopted the discharge at the upstream boundary of gumi weir was 197 389 m3 s and the water level at the downstream boundary of chilgok weir was 18 64 el m in addition the inflow from the tributary gamcheon stream was 28 78 m3 s the flow parameters were set as follows the roughness and viscosity coefficients of the study area were determined by referring to the previous studies fig 13 shows the parameter values for model simulation the boundary conditions obtained by the field measurements were assigned and the numerical simulation was conducted with the unsteady flow option the flow rate from gamcheon stream was also considered and the roughness coefficient and viscosity coefficient of the main stream of the nakdonggang river were determined by comparing the numerical results with measured data fig 14 shows the contours of the bottom elevation water level water depth and flow velocity results of the hdm 2di simulation the hollow parts in the depth contour represent the dry areas where water depth becomes zero the water depth increased at the front of in river island and decreased after the island in addition the water depth rose near the downstream boundary of the chilgok weir which is a drainage effect of the weir structure the velocity meanwhile increased at the narrow part of the channel and the acceleration at the curved part was well reproduced examining the depth and velocity contours simultaneously reveals that where the flow velocity decreases the water depth increases and vice versa to meet the continuity condition to verify the prediction accuracy of the hdm 2di the simulated depth averaged flow velocity and water depth were compared with the adcp measurement data the adcp field measurements were conducted at the six sections from the gumi weir to chilgok weir as shown in fig 15 in general the results of hdm 2di closely match the values measured by the adcp detailed descriptions on the flow behaviors and comparisons with field measured data according to each section are as follows at section 1 the velocity increased at the mid channel and decreased near both banks just before section 1 as shown in fig 11 the gamcheon stream flowed in at the right side of the main stream here because of the sandbank located near the confluence the water depth on the left side was deep at section 2 the main flow direction changed from left to right and the velocity and water depth showed an almost uniform distribution across the channel width at section 3 the velocity was nearly uniform and the water was shallower near the right bank at section 4 the in river island located downstream influenced the flow characteristics and the velocity magnitude increased slightly compared with the upstream sections at section 5 the velocity and depth values were approximately 0 3 m s and 3 5 m respectively and the water in the midsection of the channel was deep at section 6 the average velocity was about 0 3 m s and the water deepened because the chilgok weir acted as a barrier that held back water comparing fig 15 with fig 10 the model performed better when it applied to the natural river than the meandering experimental flume the reasons are as follows the strong curvatures of the meandering experimental flume induced complex velocity structures over the entire cross section and velocity distribution varied widely across the channel width accordingly the velocity field and its subsequent depth formation were significantly complicated as shown in fig 10 and there was some discrepancy between prediction and measurement however the target natural river features almost straight reach with several moderate meander as shown in fig 11 and the bottom slope in transverse direction is not steep as shown in fig 15 therefore the velocity profiles across the channel widths were almost flat or varied weakly within the range of 0 2 m s to 0 4 m s these enable the present model to make better prediction in natural river application to quantify the agreement between the hdm 2di predictions and adcp measured data the sectional r2 and rmse are presented in table 3 the simulated results matched reasonably well with the measured data especially at sections 1 and 5 although the coefficient of determination r2 of velocity was slightly lower at section 3 and depth rmse was largest at section 4 the error rates were very low therefore it is concluded that the 2d flow predictive tool developed in this study showed satisfactory performance in the natural river 4 conclusions and future directions korean rivers feature long reaches wide river widths moderate meandering the existence of many in river islands confluences and large river regime coefficients when estimating the impact of development work predicting potential damage by natural events or forecasting pollutant migration it is often helpful to conduct flow simulations in implementing a flow simulation three significant factors should be considered 1 the ease of the topographic data generation procedure 2 the stability and accuracy of the simulation engine 3 and post processing of the modeling results this study was motivated by these considerations in this study hdm 2di a predictive simulation model for 2d flow was developed and integrated with rams a user friendly tool for mesh generation and visualization rams was designed for convenience in the modeling process it consists of two submodules rams g and rams gui rams g is a tool to provide sub divided and powerful functions related to mesh generation while rams gui provides pre and post processing functions of the simulation engine in particular compared with existing 2d river analysis software mesh generation editing connection with the analysis engine simulation condition setting are intuitively configured to increase usability rams g is a software that creates and edits topographic meshes used in 2d river analysis and incorporates them into various types of geographic data formats it supports flexible meshes triangular meshes quadrilateral meshes and hybrid meshes provides various practical functions such as linking to external geometric files hec ras sms gis cad and reduces the effort involved in the pre processing processing required for mesh generation typically mesh generation can be demanding and laborious because of its manual nature considering these difficulties this study developed a mesh generation tool rams g which is equipped with a function that can create 2d meshes by directly linking the existing topographic data in hec ras format with rams g it is possible to automatically extract river cross sections and bank stations by applying linear interpolation based spatial distribution treatment to the generated mesh therefore it enables 2d topographic data to be constructed more easily the tool developed was tested using two applications i two large test flumes with different sinuosities and four meander apices and ii a natural river with a tributary and in river island in test flume application the changes in flow velocity and depth due to irregular vegetation distribution were evidently shown and the velocity configurations and resultant depth formations were well reproduced in natural river application the depth formation associated with a sandbank the flow behavior associated with bifurcation near in river island and drainage effect by a weir were clearly reproduced for both applications the accuracy of modeling results was assessed by comparing them with adcp measurement data the coefficient of determinations and rmse were 0 946 and 0 0325 on average respectively which showed that the 2d flow predictive tool developed in this study showed satisfactory performance in both an experimental flume and a natural river rams are currently downloadable at www ramsplus net at this website users can also find tutorials with detailed instructions user guide and examples demonstrating how to begin using the solver currently the research to improve the simulation speed of the hdm 2di model is being conducted in relation to the scalability of the model research and development are continuing not only for natural river flow analysis but also for application regarding urban flooding and inundation declaration of competing interest the authors declare the following financial interests personal relationships which may be considered as potential competing interests chang geun song reports financial support was provided by korea environment industry technology institute keiti through r d program for innovative flood protection technologies against climate crisis project funded by korea ministry of environment moe 2022003470001 chang geun song reports a relationship with korea environmental industry and technology institute that includes acknowledgement this work was supported by korea environment industry technology institute keiti through r d program for innovative flood protection technologies against climate crisis project funded by korea ministry of environment moe 2022003470001 
25418,flow analysis model is useful in designing hydraulic structures understanding hydraulic processes and predicting the impacts of development works and natural events accordingly needs for hydraulic models that can accurately simulate hydraulic phenomena in natural and engineered rivers are increasing here hdm 2di a predictive two dimensional flow simulation model was developed and integrated in rams a mesh generation and visualization tool rams has intuitive user interface in generating and editing meshes and setting boundary conditions with the geometry construction module rams g river cross sections and bank stations can be automatically extracted by applying linear interpolation based spatial functions to the generated mesh the tool was applied to a meandering experimental flume and a natural river to evaluate its accuracy the tool was able to successfully reproduce observed flows in channel and river rams is currently downloadable at www ramsplus net detailed tutorials a user guide and examples are also provided keywords flow analysis model hdm 2di rams predictive tool geometry construction data availability i have shared the software at software availability section software availability name of the software rams developer chang geun song il won seo heon uk im contact information baybreeze119 inu ac kr hardware required basic computer year first available 2020 program language c and fortran cost free software availability http www ramsplus net program size 93 22 mb 1 introduction because of anthropogenic climate change and land use change associated with urbanization damage caused by flooding is increasing to manage rivers and prepare for floods efficiently it is necessary to predict and analyze the hydraulic behavior of river flows however applying modeling results to river maintenance or development works without knowing restrictions can cause inaccurate and inappropriate designs as for the major rivers in south korea they generally feature long reaches wide widths moderate degrees of meandering many in river islands numerous tributaries and wild fluctuation of flow rate therefore multi dimensional hydraulic models featuring low flow analysis and stability are necessary to quantify river flow dynamics in korea several flow analysis models are currently being used to design hydraulic structures understand hydraulic processes predict the impacts of development works and to minimize the damage caused by flood events since flows near channel bends tributaries and around estuary banks weirs and other river structures exhibit multi dimensional characteristics the water level and velocity distribution in such regions cannot be predicted accurately with one dimensional 1d hydraulic models two dimensional 2d flow models can be more useful predictors of flow behavior in waterways several 2d flow codes exist including rma 2 wes 1996 feswms 2dh fhwa 1989 leendertse 1967 and teach 2e gosmam and ideriah 1976 in particular iber model developed by the collaboration of flumen institute polytechnic university of catalonia the water engineering group geama cimne and cedex gains attentions because of its versatility user friendliness and free distribution caro camargo and bayona romero 2018 iber is a numerical modelling tool for turbulent flow in an unsteady state and environmental processes in river hydrodynamics dam break simulation flood zone assessment sediment transport calculation and tidal flow in estuaries gutiérrez garcía et al 2022 the iber s capabilities and performances are thoroughly investigated in recent publication cueva portal et al 2021 these codes apply numerical methods for discretizing the governing flow equations among the various numerical schemes the finite element method and the finite volume method are often favored because they are frequently used in recent in cfd communities with verified performance but there is an ongoing argument in the preference between fem and fvm it is generally accepted that fvm enjoys faster computational time and thus requires less computational resources lukáčová medviďová and teschke 2006 jeong and seong 2014 vasconcellos et al 2020 in contrast fem is a more mathematically robust approach and can use higher order function to approximate the solution in each element wendt and anderson 2008 botti et al 2018 in addition fem allow easier meshing of irregular and complex geometries molina aiz et al 2010 to be specific the finite element method has been often applied for the analysis of hydrodynamic flow and pollutant diffusion behavior in rivers kim et al 2017 2021 choi et al 2020 because irregular terrain and river morphology can be replicated more efficiently and accurately compared with other numerical methods king and norton 1978 brebbia et al 1978 norton 1980 huebner et al 1995 davies 1980 pinder and gray 1977 in addition mesh characteristics such as cell sizes and the number of nodes can be optimized with respect to the accuracy of the numerical solution using terrain adaptive meshes heinrich and pepper 1999 karniadakis and sherwin 2005 chung 1992 furthermore flux type boundary conditions can be imposed in a straightforward manner gresho and sani 1998 fletcher 1984 and solid mathematical foundation of fe approximation enables systematic analyses on error convergence and solution accuracy strang and fix 1973 axelsson and barker 1984 thomee 1984 wait and mitchell 1985 because the finite element method is based on elements rather than nodes it is possible to obtain accurate numerical solutions for nodes inside elements by adopting various interpolation functions ghanem 1995 in this study hdm 2di a predictive simulation model for 2d flow analysis was developed and integrated with rams a user friendly tool for mesh generation and visualization then the predictive tool was applied to both a meandering experimental stream and a natural river and its applicability tested by comparing the numerical simulation results with measured data 2 tool description 2 1 hdm 2di hdm 2di is a 2d numerical model that calculates the flow variables under complex topography with river structures and can be incorporated with the pollutant transport analysis model the model produces velocities and depth results by solving following equations representing mass and momentum conservations 1 h t h u u h 0 2 u t u u g h h 1 h h ν u g n 2 h 4 3 u u where h water depth t time u u 1 u 2 is the depth averaged velocity vector in the x and y directions respectively g gravitational acceleration h bottom elevation ν kinematic eddy viscosity n roughness coefficient and u velocity magnitude equations 1 and 2 were discretized by adopting the streamline upwind petrov galerkin scheme song and oh 2016 the non linear advection term included in eq 2 was linearized by the newton raphson method and the total system of algebraic equations was solved by frontal method with fully implicit time integration applicable fields of the hdm 2di include the simulation and subsequent analysis of subcritical transcritical and supercritical flow rapid flow change dry wet transition and flow around hydraulic structures in particular 2d flow models have been frequently applied to analyze dam or dyke break problems in spite of some arguments concerning their theoretical applicability guan et al 2014 singh et al 2011 developed a 2d shallow flow solver using an explicit central upwind scheme and applied it to the malpasset dam break problem the absolute errors between the predicted front arrival time which is defined as the time when the outermost boundary of flood wave generated by dam break arrives first at a certain point and the field observed data were less than 0 04 song et al 2018a tested their 2d flow simulation code to a dam break with triangular hump problem and compared the predicted results with experimental measurements which showed very good agreements except some discrepancies immediately after opening the sluice gate hien and chien hien and van chien 2021 investigated the flow waves generated by dam break flow and checked the ability of 2d and 3d numerical model they reported that although the 2d and 3d numerical models produced quite similar profiles of water depth and velocity the 3d model can capture the peak values more accurately based on these descriptions it can be concluded that 2d model can be applied to dam break problems with reasonable accuracy however when the analysis is focused on the prediction of the exact peak values or right after the gate opening sophisticated 3d modelling should be implemented the model can successfully represent complex geometries with triangular or rectangular elements or their combination and features various distinctive functions as shown in fig 1 since its first development in 2011 by song 2011 it has been verified and applied to various river flow analysis problems for example the effect of wall boundary conditions and lateral velocity boundary conditions on the flow field was analyzed seo and song 2010 seo song the model produced accurate results with maximum mass conservation error less than 0 7 song et al 2013 however it yielded less accurate predictions as reynolds number increased to improve the accuracy of the hdm 2di simulation for the lateral flow velocity deviation due to secondary currents occurring in the curved part of the river the dispersion stress term was included in the governing equation song et al 2012 the solver for the shallow water equations with dispersion stress term provided satisfactory solutions and the root mean square error rmse of the resultant velocities in a natural river application had mean value of 0 027 however when it applied to a confluent channel the model showed inaccurate performance because three dimensional flow structure prevailed around the joint area of the main and the tributary channels floodplain erosion and the spatial distribution of sediments have been considered song et al 2017 it was shown that the kinematic flow information such as flow depth and velocity obtained by 2d flow modelling can be used to determine the deposition and erosion spots in extreme flood conditions but acceleration which is an onerous parameter to compute was necessary in estimating the spatial distribution of the deposition and erosion and the characteristics of pollutant dispersion associated with tidal current induced flow direction changes in a large natural river have been analyzed park and song 2018 hdm 2di adequately reproduced the rising and falling limbs of the water surface level and the mean absolute percentage errors at two water level gaging stations were 2 18 and 3 19 respectively recently spatiotemporal predictions of inland flooding and river inundation using a wet dry handling technique have been conducted song et al 2018b shin et al 2019 the 2d modelling results matched reasonably well with the measured data over the entire simulation region except the propagating front this was because the 2d model has limitations in capturing the moving front with singularity and discontinuity 2 2 rams as shown in fig 2 rams consists of two submodules rams g and rams gui rams g is a tool to provide subdivided and powerful functions related to mesh generation it can create and edit topographic meshes used in 2d river simulation and incorporates them into various types of geographic file formats it supports flexible meshes triangular meshes quadrilateral meshes and hybrid mesh provides various practically useful functions such as the integration of external geometric files hec ras sms gis cad and reduces the pre processing effort required for mesh generation rams gui provides pre and post processing functions for the simulation engine hdm 2di in particular compared with existing 2d river analysis software mesh generation and editing integrating meshes with the simulation engine and setting boundary conditions can be intuitively configured to increase usability rams is currently downloadable at www ramsplus net at this website users can find tutorials with detailed instructions a user guide and examples an important task in implementing a 2d flow modellings is producing the river topography geometric data in hec ras format are widely used to characterize 2d topography in the topography generation process a digital elevation model dem and a cad file are typically employed together however this process can be demanding and laborious because of its manual nature for example a commercial hydraulic software sms surface water modeling system requires several procedures to proceed 2d modeling based on hec ras survey data first you need to export the cross section location geometry and river centerline data from hec ras to create a sdf file after that in sms the river centerline is generated as cl and the cross section is produced as an xsec file name in order to advance 2d modeling it is necessary to activate the xsec coverage and convert the elevation data of the cross section into scatter based on the transformed cross sectional scatter data a stream line is built and the surface is checked through the triangulation option and the unnecessary parts such as vertical walls are removed from the initial topography according to the scatter data set then you need to construct the outer boundary of the river through the feature point and feature arc menus to complete the mesh generation to ease these burdensome procedures rams g is equipped with a function that can create 2d meshes by directly linking existing topographic data format of the hec ras as shown in fig 3 with rams g river cross sections and bank station can be automatically extracted by applying linear interpolation based spatial distribution treatment to the generated mesh as such this convenient function enables 2d topographic data to be constructed more easily and efficiently 3 applications 3 1 meandering experimental flume a series of tracer tests were performed at the andong river experiment center of the korea institute of civil engineering and building technology see fig 4 the test flumes in the river experiment center reproduce the river shape without distortion by accommodating moderate channel length and width with physical structures the river experiment center has a channel in which three meanders with different curvatures are continuously connected each meander has four apices and the bottom of the waterway is made of sand in addition the channel has naturally created cross sections by flowing water for a certain period the total length of the channel is 682 m the lengths of channels with sinuosities of 1 2 1 5 and 1 7 are 115 m 134 m and 155 m respectively the initial design slope is 0 00125 for the channel bottom and 0 5 for the walls hydraulic measurements were conducted and the results are summarized in table 1 in this table q u w and h stand for the discharge flow velocity channel width and water depth respectively the hydraulic data at six lateral lines were recorded using sontek s adcp s5 for the channel with a sinuosity of 1 5 the average flow rate and mean cross sectional velocity were 1 45 m3 s and 0 70 m s respectively for the channel with a sinuosity of 1 7 the corresponding values were 1 26 m3 s and 0 52 m s before running the flow analysis engine hdm 2di the finite element meshes were constructed using rams g as shown in fig 5 as for the geometry construction of sinuosity 1 5 triangular meshes with 7624 elements and 4126 nodes were produced for sinuosity 1 7 the meshes with 7697 elements and 4182 nodes were generated fig 6 shows the rams windows for inputting boundary conditions model controls and material properties a variety of variables and optional values for model simulation can be entered here the boundary conditions such as the flow rate and water level were determined to be same as the experimental values used in the field measurements in the model control widow parameters such as the time control option wall boundary condition wetting drying adjustment can be specified the viscosity and roughness coefficients were spatially differentiated by assigning anomaly material properties throughout the entire meshes to reflect the variation of vegetation and resistance in the experiment because the bed materials of the natural rivers are usually non uniformly graded sediments the assignment of spatially anomalistic roughness coefficient could be useful in reflecting the gradation of non cohesive sediment size figs 7 and 8 show the flow simulation results with the channels having sinuosities of 1 5 and 1 7 the changes in flow velocity and depth due to irregular vegetation distribution were evidently shown and the velocity configurations and resultant depth formations were well reproduced it can be observed in fig 8 that the water was deep along the center of the channel after section 1 because of the channel geometry shown in fig 4 as shown in table 1 the velocities in the channel with a sinuosity of 1 7 were lower than that with sinuosity 1 5 while the flow depths were deeper with sinuosity 1 7 these can be confirmed when comparing fig 8 with fig 7 in addition the velocity variations along the transverse directions were narrower in 1 7 sinuosity channel because the mean channel width was 0 36 m wider than that with 1 5 sinuosity and the consequent lower velocity induced decelerated flow around the curved parts figs 9 and 10 compare the simulation results and hydraulic measurements of the depth averaged flow velocity and water depth for each section with sinuosity 1 5 and 1 7 respectively in fig 9 the predicted velocities matched well with the measured ones and the predicted water depths at left and right bank showed similar tendency to that of adcp result despite of some errors that existed near mid part of the channel in particular the water depth at y w 0 5 in section 3 and y w 0 5 in section 4 had relatively large errors compared with other regions the cause of that is presumed to be as follows referring to the water depth contour of fig 7 the water depth deepened in the right part between section 2 and section 3 and in the central part along section 4 some errors occurred at these positions while measuring the depth with the deep digging bottom using the ultrasonic wave of adcp the comparisons between simulated and measured values for the average velocity and water depth for each section are listed in table 2 the section averaged coefficient of determination r2 for velocity with sinuosity 1 5 is 0 960 and that for velocity with sinuosity 1 7 is 0 953 the section averaged rmse for depth with sinuosity 1 5 is 0 031 and that for depth with sinuosity 1 7 is 0 031 therefore it can be concluded that the flow predictive tool performs well in reproducing the observed flows in the meandering experimental channel 3 2 a natural river to apply the predictive tool to a natural river the reach from the gumi weir to chilgok weir in the nakdong river the republic of korea was selected as the study area see fig 11 the length of the study reach is 27 3 km in total this reach was selected because the gumi national industrial complexes are located nearby and there have been several accidents in which the river was severely polluted by the chemical spills after the completion of four rivers maintenance project by the korean government in 2011 the residence time of waters flowing from the gumi weir to the chilgok weir reduced from 0 63 days before the construction of the weir to 24 45 days after the construction of the weir therefore when a pollutant is introduced in that reach it may remain longer than before and the environmental impacts will be prolonged simulating pollutant transport in river environments is usually approached using the advection dispersion framework prediction of pollutant transport should be preceded by a flow simulation to generate the flow velocity field and water depth vectors accordingly the flow velocity and water depth should be computed before water quality is assessed and the flow predictive tool including rams g and hdm 2di was used to generate a mesh for the study area the following procedures were followed after calculating the spectral indices of landsat 8 satellite images from 11 12 october 2013 which is the date of the adcp measurements the river extent was extracted according to the normalized difference water index ndwi ndwi is an index used for waterbody detection and it is calculated using the green and the near infrared bands since green light is strongly reflected by waterbody while the near infrared radiation is strongly absorbed mcfeeters 1996 in this study ndwi was calculated using the band 3 green and band 5 near infrared of the landsat 8 satellite images the topographic data were constructed according to the procedures shown in fig 12 fig 12 a and b show the images of band 3 and band 5 of landsat 8 for the study area then the boundary of the river was extracted using the gis tool as show in fig 12 c the fill extent was imported in rams g and using the convert map function the terrain was created as shown in fig 12 d as noted in previous researches calomino et al 2015 lauria et al 2020 tafarojnoruz and lauria 2020 the mesh dependency should be checked through preliminary modellings or sensitivity analysis accordingly five mesh layouts were produced and solution convergence and accuracy by comparing with field measurements were tested after that mesh layout with 32 127 elements and 17 730 nodes were finally selected fig 12 e to run hdm 2di boundary conditions and flow parameters have to be determined the boundary conditions were set as follows using the observed data set provided by the water resources management information system wamis of the nakdong river flood control center the flow conditions recorded at the same time as the adcp geometry measurement were adopted the discharge at the upstream boundary of gumi weir was 197 389 m3 s and the water level at the downstream boundary of chilgok weir was 18 64 el m in addition the inflow from the tributary gamcheon stream was 28 78 m3 s the flow parameters were set as follows the roughness and viscosity coefficients of the study area were determined by referring to the previous studies fig 13 shows the parameter values for model simulation the boundary conditions obtained by the field measurements were assigned and the numerical simulation was conducted with the unsteady flow option the flow rate from gamcheon stream was also considered and the roughness coefficient and viscosity coefficient of the main stream of the nakdonggang river were determined by comparing the numerical results with measured data fig 14 shows the contours of the bottom elevation water level water depth and flow velocity results of the hdm 2di simulation the hollow parts in the depth contour represent the dry areas where water depth becomes zero the water depth increased at the front of in river island and decreased after the island in addition the water depth rose near the downstream boundary of the chilgok weir which is a drainage effect of the weir structure the velocity meanwhile increased at the narrow part of the channel and the acceleration at the curved part was well reproduced examining the depth and velocity contours simultaneously reveals that where the flow velocity decreases the water depth increases and vice versa to meet the continuity condition to verify the prediction accuracy of the hdm 2di the simulated depth averaged flow velocity and water depth were compared with the adcp measurement data the adcp field measurements were conducted at the six sections from the gumi weir to chilgok weir as shown in fig 15 in general the results of hdm 2di closely match the values measured by the adcp detailed descriptions on the flow behaviors and comparisons with field measured data according to each section are as follows at section 1 the velocity increased at the mid channel and decreased near both banks just before section 1 as shown in fig 11 the gamcheon stream flowed in at the right side of the main stream here because of the sandbank located near the confluence the water depth on the left side was deep at section 2 the main flow direction changed from left to right and the velocity and water depth showed an almost uniform distribution across the channel width at section 3 the velocity was nearly uniform and the water was shallower near the right bank at section 4 the in river island located downstream influenced the flow characteristics and the velocity magnitude increased slightly compared with the upstream sections at section 5 the velocity and depth values were approximately 0 3 m s and 3 5 m respectively and the water in the midsection of the channel was deep at section 6 the average velocity was about 0 3 m s and the water deepened because the chilgok weir acted as a barrier that held back water comparing fig 15 with fig 10 the model performed better when it applied to the natural river than the meandering experimental flume the reasons are as follows the strong curvatures of the meandering experimental flume induced complex velocity structures over the entire cross section and velocity distribution varied widely across the channel width accordingly the velocity field and its subsequent depth formation were significantly complicated as shown in fig 10 and there was some discrepancy between prediction and measurement however the target natural river features almost straight reach with several moderate meander as shown in fig 11 and the bottom slope in transverse direction is not steep as shown in fig 15 therefore the velocity profiles across the channel widths were almost flat or varied weakly within the range of 0 2 m s to 0 4 m s these enable the present model to make better prediction in natural river application to quantify the agreement between the hdm 2di predictions and adcp measured data the sectional r2 and rmse are presented in table 3 the simulated results matched reasonably well with the measured data especially at sections 1 and 5 although the coefficient of determination r2 of velocity was slightly lower at section 3 and depth rmse was largest at section 4 the error rates were very low therefore it is concluded that the 2d flow predictive tool developed in this study showed satisfactory performance in the natural river 4 conclusions and future directions korean rivers feature long reaches wide river widths moderate meandering the existence of many in river islands confluences and large river regime coefficients when estimating the impact of development work predicting potential damage by natural events or forecasting pollutant migration it is often helpful to conduct flow simulations in implementing a flow simulation three significant factors should be considered 1 the ease of the topographic data generation procedure 2 the stability and accuracy of the simulation engine 3 and post processing of the modeling results this study was motivated by these considerations in this study hdm 2di a predictive simulation model for 2d flow was developed and integrated with rams a user friendly tool for mesh generation and visualization rams was designed for convenience in the modeling process it consists of two submodules rams g and rams gui rams g is a tool to provide sub divided and powerful functions related to mesh generation while rams gui provides pre and post processing functions of the simulation engine in particular compared with existing 2d river analysis software mesh generation editing connection with the analysis engine simulation condition setting are intuitively configured to increase usability rams g is a software that creates and edits topographic meshes used in 2d river analysis and incorporates them into various types of geographic data formats it supports flexible meshes triangular meshes quadrilateral meshes and hybrid meshes provides various practical functions such as linking to external geometric files hec ras sms gis cad and reduces the effort involved in the pre processing processing required for mesh generation typically mesh generation can be demanding and laborious because of its manual nature considering these difficulties this study developed a mesh generation tool rams g which is equipped with a function that can create 2d meshes by directly linking the existing topographic data in hec ras format with rams g it is possible to automatically extract river cross sections and bank stations by applying linear interpolation based spatial distribution treatment to the generated mesh therefore it enables 2d topographic data to be constructed more easily the tool developed was tested using two applications i two large test flumes with different sinuosities and four meander apices and ii a natural river with a tributary and in river island in test flume application the changes in flow velocity and depth due to irregular vegetation distribution were evidently shown and the velocity configurations and resultant depth formations were well reproduced in natural river application the depth formation associated with a sandbank the flow behavior associated with bifurcation near in river island and drainage effect by a weir were clearly reproduced for both applications the accuracy of modeling results was assessed by comparing them with adcp measurement data the coefficient of determinations and rmse were 0 946 and 0 0325 on average respectively which showed that the 2d flow predictive tool developed in this study showed satisfactory performance in both an experimental flume and a natural river rams are currently downloadable at www ramsplus net at this website users can also find tutorials with detailed instructions user guide and examples demonstrating how to begin using the solver currently the research to improve the simulation speed of the hdm 2di model is being conducted in relation to the scalability of the model research and development are continuing not only for natural river flow analysis but also for application regarding urban flooding and inundation declaration of competing interest the authors declare the following financial interests personal relationships which may be considered as potential competing interests chang geun song reports financial support was provided by korea environment industry technology institute keiti through r d program for innovative flood protection technologies against climate crisis project funded by korea ministry of environment moe 2022003470001 chang geun song reports a relationship with korea environmental industry and technology institute that includes acknowledgement this work was supported by korea environment industry technology institute keiti through r d program for innovative flood protection technologies against climate crisis project funded by korea ministry of environment moe 2022003470001 
25419,applying extreme temperature events for future conditions is not straightforward for infrastructure resilience analyses this work introduces a stochastic model that fills this gap the model uses at least 50 years of daily extreme temperature records climate normals with 10 90 confidence intervals and shifts offsets for increased frequency and intensity of heat wave events intensity and frequency are shifted based on surface temperature anomaly from 1850 1900 for 32 models from cmip6 a case study for worcester massachusetts passed 85 of cases using the two sided kolmogorov smirnov p value test with 95 confidence for both temperature and duration future shifts for several climate scenarios to 2020 2040 2060 and 2080 had acceptable errors between the shifted model and 10 and 50 year extreme temperature event thresholds with the largest error being 2 67 c the model is likely to be flexible enough for other patterns of extreme weather such as extreme precipitation and hurricanes keywords climate change extreme temperatures heat waves shifts in frequency duration and intensity infrastructure building energy modeling resilience stochastic models data availability all data is publicly available copies of the input and output for the case study have been stored in the github repository examples folder https github com sandialabs mews tree main examples example data worcester and to a center for open science data share https doi org 10 17605 osf io ts9e8 1 introduction the present climate crisis increases the need to understand how future infrastructure systems will be affected by extreme temperature conditions measurements of the last 30 years have shown statistically significant increases globally in extreme high temperature events robinson et al 2021 in addition modeling of earth s climate has progressed enough to give confidence that future extreme heat events will increase even further in frequency duration and intensity fdi domeisen et al 2023 this result can be derived from global circulation models global circulation models gcms and regional climate models rcms output of these models needs further processing for application to infrastructure systems analyses research to assess climate effects on infrastructure is abundant jasiunas et al 2021 zhou 2023 brockway et al 2022 even so there are very few tools and methods to assess stochastic climate futures of extreme events on infrastructure as a result we have developed a software tool called the multiscenario extreme weather simulator multiscenario extreme weather simulator mews villa et al 2022b villa 2023 that creates energyplus u s doe 2022 hourly weather files in epw format with changes to dry bulb temperature based on intergovernmental panel on climate change intergovernmental panel on climate change ipcc fdi for extreme temperature days heat wave heat wave hw and cold snap cold snap cs events though our previous research focus has been buildings villa 2021 villa et al 2022b the resulting algorithm applies to any infrastructure analysis that uses zero dimensional hourly temperature input the algorithm is also symmetric and includes cs events but without future fdi changes mews is significant because the stochastic approach enables full exploration of interdependence between future extreme temperature events and infrastructure issues such as thermal comfort power outages and changes to energy consumption requirements villa et al 2022a 1 1 goals mews model emphasizes direct numerical fitting and characterization of the tails of temperature distributions to historical data this fitting involves optimization that can complete in less than one day on 60 processors or in 4 days on a laptop computer with 11 cores once this fit is complete solution files can be used in an infrastructure analysis to rapidly generate dry bulb temperature histories for different climate scenarios and future years the following goals guided the development of the software 1 produce statistically accurate extreme events given historical data and hw and cs increases in fdi projected by rcms and gcms 2 use data driven stochastic modeling that keeps the computational burden much less than running rcms such that infrastructure and environmental system dynamics modelers can readily generate and use temperature profiles with extreme temperature events as boundary conditions 3 enable application across a broad range of locations the national oceanic and atmospheric association national oceanic and atmospheric association noaa climate normals and daily summaries were chosen to provide many hundreds of locations with publicly available data over the fifty or more years of data needed this study answers the following research question can the mews model provide good fits to a real case we hypothesize that our algorithm can match historic distributions with 95 confidence based on the kolmogorov smirnov test statistic for fits between random variables we also show that shifting into the future for 10 and 50 year events have errors that are acceptable for the future climate assessment types of analyses targeted for the application of mews finally we show the model performance for worcester massachusetts ma in the united states u s as our example case 1 2 outline this paper gives a brief literature review of the use of future weather i e estimated weather conditions decades into the future for building energy models bems this shows how our approach is an extension of previous work it then introduces the methods outlining 1 the stochastic dynamic model in section 2 1 2 how historic histograms are fit by optimization in section 2 2 3 how the sixth coupled model intercomparison project the sixth coupled model intercomparison project cmip6 ensemble data is used to establish surface temperature change for a specific latitude and longitude 4 how distributions from the stochastic dynamic model are shifted into the future 5 what shape function is used to add future hw and cs events to existing weather data and 6 describes the case study results are presented for 1 linear regression fits for duration normalization duration normalized temperature change and duration normalized energy data 2 example cases of histogram fits and future shifts and 3 kolmogorov smirnov test statistics and p values across all historic fits to the case study the future fit errors and an example case of bem output are also provided the discussion then addresses how the mews tool algorithms can be extended the limitations of the current study and how mews can be applied we conclude that mews is ready for application to many additional cases and is likely to obtain the closeness of fit needed for the majority of locations that have sufficient historical data 1 3 literature review the focus of our review is to relate the history of how future weather has been applied to bem and infrastructure analysis it shows how extreme heat event simulators are an area of research that is naturally needed for multi objective resilience analysis of infrastructure it then gives the closest studies found in our search that have capabilities similar to mews it has been thirty four years since the first rcms were created giorgi 2019 dickinson et al 1989 these models take gcm output and dynamically downscale them to a more suitable resolution to regional infrastructure analyses xu et al 2019 tapiador et al 2020 wolf and collins 2015 as rcm techniques have progressed they have become increasingly useful to inform building sector analyses involving energy resilience and thermal comfort assessments tootkaboni et al 2021 predictions have progressed to include extreme heat increases of frequency and intensity zobel et al 2017 heavy precipitation park et al 2021 strong winds jeong et al 2020 and severe storms gutowski et al 2020 even so filling the gap between gridded models and local conditions is a significant challenge there are both technical challenges and taxonomical challenges in understanding how to connect rcms results to bems current efforts in dynamic downscaling are reaching toward including population and infrastructures directly in rcms giorgi 2022 even so such models are unlikely to reach the scales of resolution of interest for detailed infrastructure analysis and lean more towards policy development than engineering insight regardless using rcms to explore local infrastructure issues pertinent to designers is not practical and tools that harness information from rcms and or gcms are therefore increasingly important as the climate crisis becomes more urgent more methods to statistically downscale extreme event information from gcms and rcms supplemented by historical data are therefore needed application of future weather projections to buildings is a difficult problem that has been explored for nearly three decades in parallel to the development of climate models cole and kernan were some of the first to acknowledge the importance of climate change in building life cycle assessment cole and kernan 1996 at nearly the same time crawley and huang showed how weather variations can make a difference in building energy simulations huang and crawley 1996 crawley 1998 this question has continued to be assessed over several decades with the conclusion that extreme weather years are needed alongside typical conditions to understand variations of building performance barnaby and crawley 2011 crawley and lawrie 2020 pernigotto et al 2020 there are many variations of such extreme weather year files with competing interests concerning how they are formed for example gasparella et al observe that extreme reference years defined by pernigotto et al 2020 focus on overall hot years and overall cold years whereas extreme meteorological years defined by crawley crawley and lawrie 2020 focus on a year that simultaneously maximizes heating and cooling and a second year that simultaneously minimizes heating and cooling gasparella et al 2021 this emphasizes that the dynamic weather for a year cannot be represented by an upper and lower bound if more than one priority exists this highlights that stochastic approaches like ours are needed to be able to assess competing priorities simultaneously methods to produce typical weather for future years are abundant belcher et al applied statistical offsets and stretching of historical weather to produce morphed future weather files belcher et al 2005 this method continues to be used extensively troup and fannon 2016 it does not provide a mechanism for including uncertainty in future weather though uncertainty through variations in climate models was addressed a few years later jones et al 2009 bass et al have extended the typical meteorological year tmy wilcox and marion 2008 to future typical meteorological years ftmy bass et al 2022 bass and new 2023 several other studies take a non stochastic approach to extreme heat herrera et al explored how a quantile regression ensemble could be used to quantify hw extremes and how it applies to building cooling and heating design conditions herrera et al 2018 the method provides insight on how to relate extreme heat to other weather variables such as wind and humidity doutreloup et al used rcm data to generate typical and extreme meteorological year files applied to belgium doutreloup et al 2022 guo et al propose another extreme year type defined as the typical hot year and typical cold year unlike statistical selection methods the method selects entire years from rcm reanalysis outputs guo et al 2020 they analyze two extreme years consisting of the two years with the single most extreme events for cold and hot conditions and the two years with the highest intensity of extreme events though they analyzed both extremes their work does not consider increasing fdi of hw and cs work during the last decade has extended future weather to include generating files using stochastic methods williams et al reviewed stochastic weather generators for normal weather at the earliest stages of their inception williams et al 2011 rastogi produced stochastic variations in weather that do not include extreme heat waves rastogi and andersen 2016 rastogi 2016 more recently several initiatives besides ours take on a stochastic or statistical approach to hw fdi zhang et al introduces a software tool called the heat wave tracker and applies it to australia zhang et al 2022 this tool uses reanalysis products from rcm to characterize fdi and spatial characteristics of historic and future hw using extreme event theory this temporal spatial characterization of hws is comprehensive but requires large amounts of processing power via cloud computing on big data our analysis is designed to benefit from products like theirs so that weather can be characterized at a single location without such an extensive computational requirement li et al used a statistical approach to hws but does not extend the resulting distribution shifts to the generation of hourly data suitable for infrastructure analysis li et al 2021 their work is similar to our approach through the use of daily temperature data to characterize hw abadie et al use a three distribution stochastic approach where a poisson distribution models hw frequency duration is modeled as a gamma process and intensity is modeled as truncated gaussian distribution similar to our approach abadie et al 2019 their work demonstrates increases in fdi for the fifth coupled model intercomparison project cmip5 climate data the model is lower resolution than our efforts with only two periods of parameter evaluation rather than 12 and does not focus on css as well hosseini et al have created tools and methods that compete with the mews approach they take a weather classification approach that uses machine learning to capture increases in fdi from gcm output hosseini et al 2021 they apply their work to bem similar to ours but do not consider discreet events making their method less likely to capture detailed shifts in fdi there method is also tied to individual gcm making generalizations to ensembles of gcms prohibitively expensive for analyses using bems a heat risk assessment tool performing similar functions as mews to address fdi of extreme heat is the jupiter heatscore tool which addresses probabilistic assessment of increased risk from extreme heat jupiter intelligence 2023 unfortunately the methods used in this tool are not available in the literature making understanding how this study might relate to it infeasible the study closest to ours we could find was that of shaby et al 2016 shaby et al used a markov switching model to simulate hws but only focus on summer time extremes neglecting winter hws and all css shaby et al 2016 this is the closest algorithm to those presented here but differs in the markov process lacks css and uses a daily time step instead of hourly our application to the building sector in the mews software also makes our algorithms immediately available to bem practitioners the models and results presented here are an extension of our previous work on hw effects on infrastructure villa 2021 and stochastic generation of hws villa et al 2022b a our algorithms have been completely reformulated because we found that algebraic estimation of statistical quantities in villa et al 2022a did not work beyond 2060 therefore an optimization approach was formulated that did not assume independence of event frequency and duration parameters 2 methods we propose an approach that uses a stochastic dynamic process to model extreme days css and hws events in this process have time varying probabilities of an extreme temperature event being sustained when time variation is not present the process events are markovian the stochastic event process is coupled to duration normalized truncated gaussian distributions for total temperature and energy of extreme events historic noaa daily summary data has maximum and minimum values alongside noaa climate normals with 90 hourly maximum temperatures these sources of historical information provide adequate amounts of data and a standard to compare against so that statistics of the historic fdi of extreme temperature events can be characterized for a specific location these historical statistics can then be altered based on future ipcc projections concerning increased frequency and intensity the parameters of the coupled stochastic and truncated gaussian distributions are determined historically via multi objective stochastic optimization followed by an algebraic shift to meet future conditions fig 1 shows the inputs algorithms and outputs of the mews approach each of the algorithms is elaborated further in the following sections we start by describing the stochastic model in section 2 1 this is followed by a description of the historic fit optimization in section 2 2 the polynomial fits needed from cmip6 data for future changes in section 2 3 and the corresponding shift to future conditions in section 2 4 finally we finish with a definition for the shape function used to add new hws to hourly weather files in the software in section 2 5 2 1 stochastic dynamic model the model used in this study consists of a stochastic time dependent transition matrix and truncated gaussian distributions over the interval 1 1 these distributions are derived from duration normalized peak temperature change δ t and total change in energy δ e above the climate normal 50 confidence interval confidence interval ci the model must be fit to historic data for each month of the year m 1 2 12 2 1 1 transition matrix for each month a 3 3 right handed state transition stochastic matrix must be determined the states of the matrix i e row position are 0 normal conditions 1 in a cs i e c s in the index and 2 in a hw i e h w in the index the probabilities of the first row for normal conditions are constant whereas the probabilities for the other states vary as functions of time 1 m m δ t 1 p h w m p c s m p c s m p h w m 1 p s c s m δ t p s c s m δ t 0 1 p s h w m δ t 0 p s h w m δ t here δ t is the time in hours since an event started also the subscript c s and h w are generally represented by the wave index w c s h w the p s w m δ t functions can have many forms but must stay between zero and one for all δ t the form focused on in this work consists of a quadratic increase followed by an exponential decay function that has four parameters as seen in box i the parameters are defined below and the functional form is illustrated in fig 2 1 p 0 w m the initial probability of sustaining a wave 2 δ t p w m the time to peak probability p m a x w m 3 p m a x w m peak probability reached by the function if less than p 0 w m this is not a peak 4 δ t c w m cutoff time at which probability drops to zero the temporary increase of this function provides the capacity to extend wave durations to better fit the historical duration histograms that often do not exhibit immediate decay 2 1 2 truncated gaussian distributions performing statistical fits of the raw historical data would lead to poor results because peak temperatures and energy both grow as event duration increases it is therefore necessary to normalize both temperature and energy by a function of duration for this analysis it is assumed that energy added and peak temperature change grow as linear functions of duration these linear functions are determined by regressing normalized heat wave energy δ e w m δ e w m max δ e w m and normalized temperature δ τ w m δ t w m max δ t w m 3 δ e w m d α e w m d max d w m 4 δ τ w m d α t w d max d w m β t w the 6 coefficients α e w α t w and β t w are determined by linear regression of duration divided by the maximum duration event for a given m and w these linear regressions are used to form two new sets of dimensionless duration normalized values 5 ɛ w m d δ e w m max δ e w m δ e w m d 6 t w m d δ t w m max δ t w m δ τ w m d here ɛ is the duration normalized energy and t is the duration normalized change in temperature the sets of duration normalized energies and temperatures for historic hws are represented by ɛ w m and t w m whereas ɛ w m d and t w m d refer to the functions used to evaluate future sampled hws these sets are mapped to the interval 1 1 using the following linear transform 7 i x 2 x min x max x min x 1 here x is either of the sets in eqs 5 and 6 and i is the corresponding 1 1 transformed data the inverse transform to return to x is 8 x i i 1 max x min x 2 min x the maximum and minimum values of each set e g min ɛ w m max ɛ w m have to therefore be retained so the reverse transformation can be accomplished the mean μ and standard deviation σ of truncated gaussian distributions on the interval 1 1 are used as variables in the optimization to fit historical wave histograms initial values of these parameters are the sample mean and standard deviation of their corresponding sets for example σ ɛ w m is given an initial value in optimizing the standard deviation of i ɛ w m finally the maximum and minimum values of ɛ w m and t w m are taken as the boundaries a and b of a truncated gaussian distribution these values are transformed linearly with the μ and σ parameters as described in section 2 4 2 1 3 model parameters for optimization as defined in the algorithm the model has 14 parameters per month that must be determined the parameters are 1 two parameters for the probability of starting an extreme event p h w p c s 2 two initial probabilities of sustaining an extreme event p s h w p s c s 3 four truncated gaussian parameters for temperature and energy the boundaries of the distributions are fixed at a 1 and b 1 and 4 six function form parameters p m a x δ t p δ t c for each w the time variant function p 0 term of eq 2 are the p s terms the model takes a sample of uniformly distributed random numbers on the interval 0 1 for each month these are considered to be an hourly time series for the optimization a period of several million samples for each month is simulated to characterize statistical distributions this is changed to an hourly time series over a year when creating weather files with m m varying as each month passes the series of random numbers is applied to the transition matrix m m which provides a history of extreme temperature events the history is filtered to make all events at least one day long and rounds events greater than one day to the nearest day for each event the corresponding truncated gaussian distribution is sampled and the wave duration is then used to produce a peak temperature and total energy these values are then provided as the inputs to section 2 5 the model output histograms of temperature and duration can be compared to historic distributions or ipcc shifted thresholds in an optimization framework to get the model parameters to produce a fit to the historic data or future shifted conditions model output has to be unscaled and reverse normalized to produce temperature and energy values this is accomplished by sampling from the truncated gaussian distributions for the appropriate month transforming via x in eq 8 and using eqs 5 and 6 in reverse 2 2 historic fit of stochastic model the historic fit requires processing of the noaa inputs in fig 1 to form histograms of peak temperature change δ t and duration d of sets of consecutive extreme temperature days for both hws and css these historic histograms become the target for an optimization that finds the best fit for the stochastic dynamic model parameters mews uses an evolutionary algorithm for this optimization with a least squares difference between the mews output and the historic histograms of d and δ t 2 2 1 historic histograms the noaa daily summaries provide maximum and minimum daily temperature noaa 2021a the noaa climate normals contain hourly data for one year with 10 50 and 90 ci thresholds noaa 2021b this study used the 1991 to 2020 version of the climate normals the climate normals are aggregated to daily minimum and maximum values for each ci for comparison to the daily summaries extreme temperature days from the daily summaries are then classified as 1 extreme hot days the daily maximum is greater than the climate norm daily maximum 90 ci hot daytime or the daily minimum is hotter than the climate norm daily minimum 90 ci hot nighttime 2 extreme cold days the daily maximum is less than the climate norm daily maximum 10 ci cold daytime or the daily minimum is less than the climate norm daily minimum 10 cold nighttime both sets of days are grouped into subsets for any consecutive extreme days the lengths of these subsets form a duration histogram for hws and css for hws the maximum daily temperature minus the 50 daily mean climate normal for each subset forms a maximum temperature change histogram for css the minimum temperature minus the 50 daily mean forms a minimum temperature change histogram the histogram s number of bins is set to the number of extreme temperature events divided by ten this strikes a balance between resolution of temperature bins and number of events in each bin it gives a good shape for numeric histograms for the 50 or more years of data required 2 3 climate scenario polynomials mews uses gcm output from cmip6 eyring et al 2016 to produce a historical polynomial for surface temperature and polynomials for shared socio economic pathways shared socio economic pathways ssps riahi et al 2017 that need to be simulated these polynomials establish the baseline temperature change from 1850 1900 needed in the ipcc hw shift factors described in section 2 4 they also provide the amount to shift temperatures for future years mews uses a two dimensional interpolation scheme to produce a specific set of profiles for a given latitude and longitude the set of cmip6 models used can be changed for this study the largest available set of models for each ssp was used as seen in table 1 the order of polynomial can be increased to produce smooth fits generally speaking a quadratic is sufficient for future scenarios while a fifth to sixth order polynomial is needed to properly match the historic variations in climate from 1850 to 2014 2 4 shift historic to future the shift to future conditions is accomplished by assuming that only the truncated gaussian distribution for temperatures change it is important to note that many other ways of shifting in the future could be used but our approach was chosen as the best method with the currently available information also mewss has the capacity to shift cs but information about such shifts is lacking we therefore keep historical cs fdi constant into the future for hws the mean and standard deviation are assumed to scale equally so that the minimum bound of the distribution remains fixed while the maximum is retained the ipcc fdi shifts are given for 10 and 50 year events to define the hourly probability of a 10 year or 50 year event happening we assume that historic hw rates are uniform we also require that at least 50 years of historic data be available for the analysis to be valid we then linearly interpolate the number of hws expected in 10 years and 50 years 9 p y δ t h t h y m δ t y n h n y here p y is the probability of an event of either 10 or 50 year events i e through index y 10 50 δ t h is the total time interval of the historic noaa daily summaries data t h y m is the total time in month m in the historic time interval δ t y is the number of hours in a year n h is the number of hw events in the historic time interval and n y is the number of hours in y years the probabilities p y are then used to calculate the threshold temperature δ t y where the historic histogram s cumulative distribution function cdf of changes in temperature are linearly interpolated between bin average points the ipcc physical basis report for policy makers masson delmotte et al 2021a estimates how extreme events will shift and stretch probability distributions with changes in surface temperature at a specific latitude and longitude the input to these shifts is the surface temperature change δ t g from the 1850 1900 baseline established by the polynomials calculated from cmip6 data in section 2 3 this temperature change is calculated for each year for which future weather is needed histories that increase temperature beyond δ t g of 4 c are linearly extrapolated by mews beyond the table values beyond 6 05 c are rejected the threshold was moved up to 6 05 from 6 so that the worcester case could include 2080 figure spm 6 of masson delmotte et al 2021a contains numerical information that is summarized in table 2 the ci represents the spread of results across different climate models averaged over the earth s entire land surface these bounds therefore do not necessarily reflect a specific location s spread but are a reasonable estimation for a specific location the mews software allows use of one or more of the ci in table 2 given a δ t g we interpolate linearly to get values for δ t y c i p c c and f y c i p c c from table 2 we then have to adjust this value because the time period for the noaa daily summaries noaa climate normals and ipcc hw increase in fdi all are different this is illustrated in fig 3 the values shown are not real data and are only to illustrate the three periods and how their average temperature varies between the three periods mews neglects the temperature difference between the hw and climate normals baselines to account for the differences in the dates for the noaa daily summaries and ipcc baseline of 1850 1900 the temperature anomaly for the average temperature of the hw period δ t g h w is used to interpolate factors from table 2 which we will call δ t y c h w i p c c and f y c h w i p c c we then use these values to alter future targets for frequency and intensity of 10 year and 50 year hw events for frequency the future probability of the two events are taken as 10 p f y c p y d i v i d e c i f y c i p c c f y c h w i p c c the function d i v i d e c i is needed because cis cannot have simple division applied to them it takes random samples of 100 000 values of the two implied normal distributions for the cis and then reconstructs the new resulting ci this has more than sufficient precision to divide the cis numerically for intensity we take the new temperature to be 11 δ t f y c δ t y δ t y c i p c c δ t y c h w i p c c once these target thresholds are known we can shift the mews model temperature distributions by changing the mean and standard deviation of each month s truncated gaussian temperature distributions the algorithm uses newton s method by shifting the distribution a small increment finding the sensitivity and then calculating a target shift to cause the distribution 10 and 50 year threshold to meet the target thresholds the model distribution thresholds δ t f m y m c are the temperature anomalies for which the probability of temperature being greater than the threshold is p f y c the newton algorithm typically converges in 3 4 iterations several instances of the model are averaged for each interation since the model is stochastic the error between the model threshold and shifted historic threshold serves as a measure of the accuracy of the future shift fit 12 e r r f u t y m c δ t f m y m c δ t f y c 2 5 shape function the mews model produces waves with d and δ t a shape function is needed to apply this to an hourly weather file a useful function to fulfill this need is shown below 13 δ t t d δ t m i n a sin π t d o d d b 1 cos 2 π t δ t m i n c t d o d d b 1 cos 2 π t δ t m i n c t d o d d this function contains a constant shift c a daily increase in temperature b and a term for the entire hw duration a here δ t is the change from the mean daily climate norm temperature and δ t m i n is 24 hr the parameter d o d d is the closest odd multiple of δ t m i n that is less than the hw duration d 14 d o d d δ t m i n d δ t m i n δ d δ t m i n mod 2 here mod is the modulus operator is the floor function or closest integer less than the input and δ is the dirac delta function using d o d d instead of d in eq 13 avoids erratic variations in the maximum temperature condition with respect to the hw duration enabling mapping δ t to sampled values δ t s and δ e s integration of eq 13 from 0 to d produces the following total energy in degrees temperature times time 15 δ e s 2 a d o d d π b d b δ t m i n 2 π sin 2 π d δ t m i n though d o d d makes the energy relationship more complex it makes the heat wave maximum temperature much more simple 16 δ t s a 2 b c the relationships in eqs 15 and 16 can be solved to provide values for a and b the offset c is set to zero in the current version of mews 17 a δ t s π 2 d o d d δ e s 2 π d 2 d o d d δ t m i n 4 d o d d sin 2 π d δ t m i n 18 b δ t s a 2 the values of a b and c must be greater than zero for hws or less than zero for css for the solution to be physically meaningful if such a solution cannot be found then b 0 and c 0 with a remaining as the only unknown in such cases only the sampled temperature is met and the energy constraint is not satisfied 2 6 case study the mews model was applied to worcester ma in the united states tmy version 3 tmy3 williams et al 2011 climate normals noaa 2021b and daily summaries data from worcester regional airport from june 1st 1946 to october 28 2022 was used within the proposed framework to produce temperature and distribution fits the data sources used are summarized in table 3 the historic daily summary data for extreme days hws and css were compared to three random trials of our stochastic model using the 2 sided kolmogorov smirnov test statistic p values to determine if the stochastic dynamic model output distributions are the same as the historic distributions with 95 confidence the historic temperature distributions were then shifted and stretched to reflect the ipcc intensity and frequency increases seen in table 2 for ssp2 4 5 3 7 0 and 5 8 5 values were evaluated for 2020 2040 2060 and 2080 3 results the linear regressions of duration normalized energy and temperature are shown in fig 4 with scale 0 1 for both axes where the maximum values of duration and temperature are normalizing the x and y axes respectively also depicted are the extreme event energy and temperature histograms normalized by the linear regressions and scaled to the interval 1 1 all scales and data descriptions are provided in the figure caption as expected the temperature regressions have a non zero offset while the energy regressions do not as a result some of the distributions still appear to have a skew but most cases appear symmetric the historic and future projected change in surface temperature extracted from cmip6 for the ssps analyzed is shown in fig 5 the cis shown reflect the variations of the many cmip6 models shown in table 1 the polynomial fits depicted by yellow red and dark red lines were used at 2020 2040 2060 and 2080 to create shifts in hw fdi the fit for the historic optimization was run for 30 60 and 120 iterations using evolutionary optimization with the conclusion that 30 iterations were sufficient for convergence to greater than 1 accuracy the optimization results for temperature and duration were evaluated via the kolmogorov smirnov statistics and p values given in tables 4 and 5 cases where the null hypothesis was rejected i e the null hypothesis is that the distributions are the same with 95 confidence have red boxes surrounding them for temperature these worst cases are shown in fig 6 fig 7 gives an example case for how the temperature distribution for august 2080 is altered to meet the 10 year and 50 year heat wave thresholds shifted per table 2 shown are the original temperature threshold δ t y with cyan representing the 10 year threshold and orange red representing the 50 year historic threshold these are then shifted by δ t y c i p c c where the black arrows show the shift into the future the mews actual thresholds δ t f m y m c are shown in dotted lines of the same color as the target thresholds shown in dashed lines as previously stated the duration distributions are not shifted for this study the frequency increase for 10 and 50 year events is accounted for by increasing the area in front of the 10 and 50 year thresholds for the temperature distributions the errors between target and actual mews model temperature thresholds e r r f u t y m c for all future projections are shown in table 6 the average error across all cases is 0 037 c with a sample standard deviation of 0 841 c the worst case negative and positive errors are highlighted with red boxes around them as illustrated in fig 8 the primary cause of error is differences in the size of band between 10 and 50 year events for earlier years the 50 year events are over predicted and 10 year events are under predicted while the average of the 10 and 50 year events between the model and actual shifts is nearly zero this problem decreases for 2060 and 2080 because the frequency multipliers from table 2 become much larger the evaluation of error was executed for three cases of the stochastic model and fig 8 clearly shows how three evaluations have slightly different errors finally fig 9 shows how the application of the shape function provided in section 2 5 alters dry bulb temperature for practical energyplus file use the realization includes a constant offset from the temperature polynomials shown in fig 5 it also includes the hws and css added by the stochastic model the hws and css are referenced by the same climate normals regardless of the future year so that the offset does not get double counted this exhibits single realizations of the stochastic model characterized by the distributions shown in the previous figures hundreds or even thousands of such realizations are needed to characterize stochastic response of bems or other infrastructure models the mews stochastic model historic fits pass with 95 confidence for the kolmogorov smirnov test p values for 85 of the cases provided in tables 4 and 5 this firmly establishes that the proposed model provides statistically significant fits for the worcester noaa historical extreme temperature data the distributions that did not pass the test with 95 confidence illustrated in fig 6 all have sudden jumps in the historic worcester histograms that are unlikely to be phenomena that actually warrant fitting 4 discussion 4 1 model fit the cases that did not pass the statistical test were reviewed to verify the model behavior was acceptable all the distributions for css in july august and october have a spike for mild css that is probably due to transition between the rest of temperatures and the 90 threshold since this model only focuses on the 10 lowest and 10 highest temperature records on a daily basis we would expect that adding an overlapping normal conditions distribution for the 80 remaining temperature data would smooth out this discrepancy the hw fit for august is poor for this reason as well regardless the 86 year period of data recorded 1946 2022 for extreme temperature event data does not provide smooth daily histograms making it acceptable for some of the fits to not meet 95 confidence this problem of extreme events not having enough historical basis to characterize distributions in a changing climate is even more pronounced for less frequent events like hurricanes the errors seen in table 6 are deemed acceptable for future shifting since the mean between 10 and 50 year events are equivalent the stochastic nature of the maximum errors of 2 67 c are within the range of physically realistic events and are for 50 year events for which threshold uncertainty is expected to be high the mews input structure allows for extreme temperature events that are unrealistic for some of the fits but these can be removed since they are low probability events that occur for the model fit procedure which uses 285 years of simulation 2 5 millions hours per month to characterize the distributions even so the difficulties associated with baselining the historic data versus ipcc fdi shift and offset factors illustrated in fig 3 points to the need for perfect alignment of the climate normals extreme temperature data and climate baseline to do this an rcm based approach is necessary ideally rcms could be run with stochastic changes to boundary conditions so that actual distributions for extreme temperature could become the target the mews algorithm is ready to fit such cases since it can fit any arbitrary distribution via nonlinear optimization producing such a product running rcms hundreds or thousands of times per ssp is prohibitively expensive computationally though instead multiple cases of rcms across different models and different run cases will be needed to gather a couple dozen points to estimate means and standard deviations of extreme phenomena a new fit procedure would be needed but its tools are already in the mews software ideally runs that continue for several decades in the same climate condition are needed so that more than 50 years of information is available for climate models even though this is desirable it would significantly increase the amount of time needed to download and process large amounts of data regardless a properly fit mews stochastic dynamic model is needed so that extreme events can be accurately replicated rapidly with a low computational burden for infrastructure analysis purposes this extends resilience analysis into a framework that can incorporate complex combinations of design basis threats dbts that are correlated to extreme heat or cold without the need for big data processing and super computing our goals are to replicate temporal patterns exhibited by climate models with moderate precision so detailed energy and resilience models can be evaluated against extreme events in a stochastic way this will enable planners to overcome the shortcomings of the scenario based approach most commonly used to assess how infrastructure might perform in the future the scenario based approach involves creating several scenarios of environmental variables that often attempt to bound the problem with best case average and worst case futures in taking this approach researchers overlook the majority of statistical possibilities for the future and are likely to overlook cascading events such as power outages caused by a combination of increased demand and increased heat even when such studies consider such events baniassadi et al 2018 exploring possibilities is very limited this can be acceptable for analyses of a stable system but can overlook event combinations that destabilize an infrastructure system that do not appear to be worst case 4 2 future work our work needs to be extended to include estimation techniques for how extreme heat can effect other weather variables such as wind speed wind direction and humidity similar to herrera et al 2018 to do so the use of rcm outputs are certainly needed because not enough data exists for other variables for the lengths of time needed for hw characterization quantifying heat wave intensity is an active area of research mazdiyasni et al 2019 therefore we hope to perform increasingly sophisticated experiments to assure that the shape function we use with mews spans the statistical distribution of hw shapes in addition future research could use machine learning to learn valid shapes of hws based on rcm analysis we think the approaches outlined in this paper are sufficiently flexible to capture extremes for precipitation storms and others our time varying stochastic transition matrix has the parameters needed to keep events to varying scales of duration to avoid physically impossible events for earth s climate system we have intentionally placed bounding parameters cutoff times truncated gaussian distributions so that future projections only include increases due to rcm or gcm predictions rather than random sampling events this is especially important when altering the tails of temperature distributions stochastic approaches in resilience analyses are common e g see rocchetta et al 2018 balakrishnan and cassottana 2022 the mews software has potential to naturally incorporate fdi of extreme heat into such analyses there is a need for infrastructure mews like analysis to map out how fdi of extreme heat will affect resilience metrics for competing technologies such as micro grids and energy efficiency or sustainability measures such as electrification of buildings or use of ground source heat pumps the stochastic approach for specific locations provides valuable information if one infrastructure design option is more likely to produce good outcomes than another design option the approach can also show if differences between designs are not statistically significant 5 conclusion the mews model presented has been shown to objectively fit data for worcester ma with 95 confidence for most cases using the kolmogorov smirnov two sided p value test cases that did not meet 95 confidence have been shown to be acceptable the future shifting of the model has been shown to have acceptable thresholds of error between the model and ipcc fdi shifts offsets we conclude that the mews model is useful for extreme temperature event simulation for future climate conditions the optimization tools and models developed in the mews open source repository villa 2023 are more broadly applicable to other extreme event models also several studies are underway to apply mews to additional locations including houston texas and kodiak island alaska even so the continuation of this research requires demonstration that the statistical approaches used do not introduce significantly different statistical distributions than dynamic downscaling approaches using rcms the models we have presented can most likely be adjusted to overcome inaccuracies in this respect but a comparison has not yet been made declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments this work was funded by the u s department of energy building technologies office bto future weather project u s department of energy bto 2023 under contract number nl0039169 software and data availability mews version 1 1 0 february 28 2023 is available as open source software under the revised bsd license it also contains third party libraries approved for unlimited redistribution per the license file specifications in the mews github repository https github com sandialabs mews this paper s results were generated using the worcester example py and verify mews py scripts in the examples folder of the repository all data used in this research is freely available to the public through the noaa download websites for climate normals https www ncei noaa gov data normals hourly 1991 2020 archive and daily summaries https www ncei noaa gov data global historical climatology network daily archive for station usw00094746 for the worcester regional airport the ipcc hw shift information is also freely available and is replicated in this paper data from the sixth coupled model intercomparison project used by this work has been placed at https osf io ts9e8 files osfstorage disclaimer sandia national laboratories is a multimission laboratory managed and operated by national technology and engineering solutions of sandia llc a wholly owned subsidiary of honeywell international inc for the u s department of energy s national nuclear security administration under contract de na0003525 this paper describes objective technical results and analysis any subjective views or opinions that might be expressed in the paper do not necessarily represent the views of the u s department of energy or the u s government abbreviations bto building technologies office bem building energy model cdf cumulative distribution function ci confidence interval cmip5 the fifth coupled model intercomparison project cmip6 the sixth coupled model intercomparison project cs cold snap dbt design basis threat fdi frequency duration and intensity ftmy future typical meteorological year gcm global circulation model hw heat wave ipcc intergovernmental panel on climate change ma massachusetts mews multiscenario extreme weather simulator noaa national oceanic and atmospheric association rcm regional climate model ssp shared socio economic pathway tmy typical meteorological year tmy3 typical meteorological year version 3 u s united states 
25419,applying extreme temperature events for future conditions is not straightforward for infrastructure resilience analyses this work introduces a stochastic model that fills this gap the model uses at least 50 years of daily extreme temperature records climate normals with 10 90 confidence intervals and shifts offsets for increased frequency and intensity of heat wave events intensity and frequency are shifted based on surface temperature anomaly from 1850 1900 for 32 models from cmip6 a case study for worcester massachusetts passed 85 of cases using the two sided kolmogorov smirnov p value test with 95 confidence for both temperature and duration future shifts for several climate scenarios to 2020 2040 2060 and 2080 had acceptable errors between the shifted model and 10 and 50 year extreme temperature event thresholds with the largest error being 2 67 c the model is likely to be flexible enough for other patterns of extreme weather such as extreme precipitation and hurricanes keywords climate change extreme temperatures heat waves shifts in frequency duration and intensity infrastructure building energy modeling resilience stochastic models data availability all data is publicly available copies of the input and output for the case study have been stored in the github repository examples folder https github com sandialabs mews tree main examples example data worcester and to a center for open science data share https doi org 10 17605 osf io ts9e8 1 introduction the present climate crisis increases the need to understand how future infrastructure systems will be affected by extreme temperature conditions measurements of the last 30 years have shown statistically significant increases globally in extreme high temperature events robinson et al 2021 in addition modeling of earth s climate has progressed enough to give confidence that future extreme heat events will increase even further in frequency duration and intensity fdi domeisen et al 2023 this result can be derived from global circulation models global circulation models gcms and regional climate models rcms output of these models needs further processing for application to infrastructure systems analyses research to assess climate effects on infrastructure is abundant jasiunas et al 2021 zhou 2023 brockway et al 2022 even so there are very few tools and methods to assess stochastic climate futures of extreme events on infrastructure as a result we have developed a software tool called the multiscenario extreme weather simulator multiscenario extreme weather simulator mews villa et al 2022b villa 2023 that creates energyplus u s doe 2022 hourly weather files in epw format with changes to dry bulb temperature based on intergovernmental panel on climate change intergovernmental panel on climate change ipcc fdi for extreme temperature days heat wave heat wave hw and cold snap cold snap cs events though our previous research focus has been buildings villa 2021 villa et al 2022b the resulting algorithm applies to any infrastructure analysis that uses zero dimensional hourly temperature input the algorithm is also symmetric and includes cs events but without future fdi changes mews is significant because the stochastic approach enables full exploration of interdependence between future extreme temperature events and infrastructure issues such as thermal comfort power outages and changes to energy consumption requirements villa et al 2022a 1 1 goals mews model emphasizes direct numerical fitting and characterization of the tails of temperature distributions to historical data this fitting involves optimization that can complete in less than one day on 60 processors or in 4 days on a laptop computer with 11 cores once this fit is complete solution files can be used in an infrastructure analysis to rapidly generate dry bulb temperature histories for different climate scenarios and future years the following goals guided the development of the software 1 produce statistically accurate extreme events given historical data and hw and cs increases in fdi projected by rcms and gcms 2 use data driven stochastic modeling that keeps the computational burden much less than running rcms such that infrastructure and environmental system dynamics modelers can readily generate and use temperature profiles with extreme temperature events as boundary conditions 3 enable application across a broad range of locations the national oceanic and atmospheric association national oceanic and atmospheric association noaa climate normals and daily summaries were chosen to provide many hundreds of locations with publicly available data over the fifty or more years of data needed this study answers the following research question can the mews model provide good fits to a real case we hypothesize that our algorithm can match historic distributions with 95 confidence based on the kolmogorov smirnov test statistic for fits between random variables we also show that shifting into the future for 10 and 50 year events have errors that are acceptable for the future climate assessment types of analyses targeted for the application of mews finally we show the model performance for worcester massachusetts ma in the united states u s as our example case 1 2 outline this paper gives a brief literature review of the use of future weather i e estimated weather conditions decades into the future for building energy models bems this shows how our approach is an extension of previous work it then introduces the methods outlining 1 the stochastic dynamic model in section 2 1 2 how historic histograms are fit by optimization in section 2 2 3 how the sixth coupled model intercomparison project the sixth coupled model intercomparison project cmip6 ensemble data is used to establish surface temperature change for a specific latitude and longitude 4 how distributions from the stochastic dynamic model are shifted into the future 5 what shape function is used to add future hw and cs events to existing weather data and 6 describes the case study results are presented for 1 linear regression fits for duration normalization duration normalized temperature change and duration normalized energy data 2 example cases of histogram fits and future shifts and 3 kolmogorov smirnov test statistics and p values across all historic fits to the case study the future fit errors and an example case of bem output are also provided the discussion then addresses how the mews tool algorithms can be extended the limitations of the current study and how mews can be applied we conclude that mews is ready for application to many additional cases and is likely to obtain the closeness of fit needed for the majority of locations that have sufficient historical data 1 3 literature review the focus of our review is to relate the history of how future weather has been applied to bem and infrastructure analysis it shows how extreme heat event simulators are an area of research that is naturally needed for multi objective resilience analysis of infrastructure it then gives the closest studies found in our search that have capabilities similar to mews it has been thirty four years since the first rcms were created giorgi 2019 dickinson et al 1989 these models take gcm output and dynamically downscale them to a more suitable resolution to regional infrastructure analyses xu et al 2019 tapiador et al 2020 wolf and collins 2015 as rcm techniques have progressed they have become increasingly useful to inform building sector analyses involving energy resilience and thermal comfort assessments tootkaboni et al 2021 predictions have progressed to include extreme heat increases of frequency and intensity zobel et al 2017 heavy precipitation park et al 2021 strong winds jeong et al 2020 and severe storms gutowski et al 2020 even so filling the gap between gridded models and local conditions is a significant challenge there are both technical challenges and taxonomical challenges in understanding how to connect rcms results to bems current efforts in dynamic downscaling are reaching toward including population and infrastructures directly in rcms giorgi 2022 even so such models are unlikely to reach the scales of resolution of interest for detailed infrastructure analysis and lean more towards policy development than engineering insight regardless using rcms to explore local infrastructure issues pertinent to designers is not practical and tools that harness information from rcms and or gcms are therefore increasingly important as the climate crisis becomes more urgent more methods to statistically downscale extreme event information from gcms and rcms supplemented by historical data are therefore needed application of future weather projections to buildings is a difficult problem that has been explored for nearly three decades in parallel to the development of climate models cole and kernan were some of the first to acknowledge the importance of climate change in building life cycle assessment cole and kernan 1996 at nearly the same time crawley and huang showed how weather variations can make a difference in building energy simulations huang and crawley 1996 crawley 1998 this question has continued to be assessed over several decades with the conclusion that extreme weather years are needed alongside typical conditions to understand variations of building performance barnaby and crawley 2011 crawley and lawrie 2020 pernigotto et al 2020 there are many variations of such extreme weather year files with competing interests concerning how they are formed for example gasparella et al observe that extreme reference years defined by pernigotto et al 2020 focus on overall hot years and overall cold years whereas extreme meteorological years defined by crawley crawley and lawrie 2020 focus on a year that simultaneously maximizes heating and cooling and a second year that simultaneously minimizes heating and cooling gasparella et al 2021 this emphasizes that the dynamic weather for a year cannot be represented by an upper and lower bound if more than one priority exists this highlights that stochastic approaches like ours are needed to be able to assess competing priorities simultaneously methods to produce typical weather for future years are abundant belcher et al applied statistical offsets and stretching of historical weather to produce morphed future weather files belcher et al 2005 this method continues to be used extensively troup and fannon 2016 it does not provide a mechanism for including uncertainty in future weather though uncertainty through variations in climate models was addressed a few years later jones et al 2009 bass et al have extended the typical meteorological year tmy wilcox and marion 2008 to future typical meteorological years ftmy bass et al 2022 bass and new 2023 several other studies take a non stochastic approach to extreme heat herrera et al explored how a quantile regression ensemble could be used to quantify hw extremes and how it applies to building cooling and heating design conditions herrera et al 2018 the method provides insight on how to relate extreme heat to other weather variables such as wind and humidity doutreloup et al used rcm data to generate typical and extreme meteorological year files applied to belgium doutreloup et al 2022 guo et al propose another extreme year type defined as the typical hot year and typical cold year unlike statistical selection methods the method selects entire years from rcm reanalysis outputs guo et al 2020 they analyze two extreme years consisting of the two years with the single most extreme events for cold and hot conditions and the two years with the highest intensity of extreme events though they analyzed both extremes their work does not consider increasing fdi of hw and cs work during the last decade has extended future weather to include generating files using stochastic methods williams et al reviewed stochastic weather generators for normal weather at the earliest stages of their inception williams et al 2011 rastogi produced stochastic variations in weather that do not include extreme heat waves rastogi and andersen 2016 rastogi 2016 more recently several initiatives besides ours take on a stochastic or statistical approach to hw fdi zhang et al introduces a software tool called the heat wave tracker and applies it to australia zhang et al 2022 this tool uses reanalysis products from rcm to characterize fdi and spatial characteristics of historic and future hw using extreme event theory this temporal spatial characterization of hws is comprehensive but requires large amounts of processing power via cloud computing on big data our analysis is designed to benefit from products like theirs so that weather can be characterized at a single location without such an extensive computational requirement li et al used a statistical approach to hws but does not extend the resulting distribution shifts to the generation of hourly data suitable for infrastructure analysis li et al 2021 their work is similar to our approach through the use of daily temperature data to characterize hw abadie et al use a three distribution stochastic approach where a poisson distribution models hw frequency duration is modeled as a gamma process and intensity is modeled as truncated gaussian distribution similar to our approach abadie et al 2019 their work demonstrates increases in fdi for the fifth coupled model intercomparison project cmip5 climate data the model is lower resolution than our efforts with only two periods of parameter evaluation rather than 12 and does not focus on css as well hosseini et al have created tools and methods that compete with the mews approach they take a weather classification approach that uses machine learning to capture increases in fdi from gcm output hosseini et al 2021 they apply their work to bem similar to ours but do not consider discreet events making their method less likely to capture detailed shifts in fdi there method is also tied to individual gcm making generalizations to ensembles of gcms prohibitively expensive for analyses using bems a heat risk assessment tool performing similar functions as mews to address fdi of extreme heat is the jupiter heatscore tool which addresses probabilistic assessment of increased risk from extreme heat jupiter intelligence 2023 unfortunately the methods used in this tool are not available in the literature making understanding how this study might relate to it infeasible the study closest to ours we could find was that of shaby et al 2016 shaby et al used a markov switching model to simulate hws but only focus on summer time extremes neglecting winter hws and all css shaby et al 2016 this is the closest algorithm to those presented here but differs in the markov process lacks css and uses a daily time step instead of hourly our application to the building sector in the mews software also makes our algorithms immediately available to bem practitioners the models and results presented here are an extension of our previous work on hw effects on infrastructure villa 2021 and stochastic generation of hws villa et al 2022b a our algorithms have been completely reformulated because we found that algebraic estimation of statistical quantities in villa et al 2022a did not work beyond 2060 therefore an optimization approach was formulated that did not assume independence of event frequency and duration parameters 2 methods we propose an approach that uses a stochastic dynamic process to model extreme days css and hws events in this process have time varying probabilities of an extreme temperature event being sustained when time variation is not present the process events are markovian the stochastic event process is coupled to duration normalized truncated gaussian distributions for total temperature and energy of extreme events historic noaa daily summary data has maximum and minimum values alongside noaa climate normals with 90 hourly maximum temperatures these sources of historical information provide adequate amounts of data and a standard to compare against so that statistics of the historic fdi of extreme temperature events can be characterized for a specific location these historical statistics can then be altered based on future ipcc projections concerning increased frequency and intensity the parameters of the coupled stochastic and truncated gaussian distributions are determined historically via multi objective stochastic optimization followed by an algebraic shift to meet future conditions fig 1 shows the inputs algorithms and outputs of the mews approach each of the algorithms is elaborated further in the following sections we start by describing the stochastic model in section 2 1 this is followed by a description of the historic fit optimization in section 2 2 the polynomial fits needed from cmip6 data for future changes in section 2 3 and the corresponding shift to future conditions in section 2 4 finally we finish with a definition for the shape function used to add new hws to hourly weather files in the software in section 2 5 2 1 stochastic dynamic model the model used in this study consists of a stochastic time dependent transition matrix and truncated gaussian distributions over the interval 1 1 these distributions are derived from duration normalized peak temperature change δ t and total change in energy δ e above the climate normal 50 confidence interval confidence interval ci the model must be fit to historic data for each month of the year m 1 2 12 2 1 1 transition matrix for each month a 3 3 right handed state transition stochastic matrix must be determined the states of the matrix i e row position are 0 normal conditions 1 in a cs i e c s in the index and 2 in a hw i e h w in the index the probabilities of the first row for normal conditions are constant whereas the probabilities for the other states vary as functions of time 1 m m δ t 1 p h w m p c s m p c s m p h w m 1 p s c s m δ t p s c s m δ t 0 1 p s h w m δ t 0 p s h w m δ t here δ t is the time in hours since an event started also the subscript c s and h w are generally represented by the wave index w c s h w the p s w m δ t functions can have many forms but must stay between zero and one for all δ t the form focused on in this work consists of a quadratic increase followed by an exponential decay function that has four parameters as seen in box i the parameters are defined below and the functional form is illustrated in fig 2 1 p 0 w m the initial probability of sustaining a wave 2 δ t p w m the time to peak probability p m a x w m 3 p m a x w m peak probability reached by the function if less than p 0 w m this is not a peak 4 δ t c w m cutoff time at which probability drops to zero the temporary increase of this function provides the capacity to extend wave durations to better fit the historical duration histograms that often do not exhibit immediate decay 2 1 2 truncated gaussian distributions performing statistical fits of the raw historical data would lead to poor results because peak temperatures and energy both grow as event duration increases it is therefore necessary to normalize both temperature and energy by a function of duration for this analysis it is assumed that energy added and peak temperature change grow as linear functions of duration these linear functions are determined by regressing normalized heat wave energy δ e w m δ e w m max δ e w m and normalized temperature δ τ w m δ t w m max δ t w m 3 δ e w m d α e w m d max d w m 4 δ τ w m d α t w d max d w m β t w the 6 coefficients α e w α t w and β t w are determined by linear regression of duration divided by the maximum duration event for a given m and w these linear regressions are used to form two new sets of dimensionless duration normalized values 5 ɛ w m d δ e w m max δ e w m δ e w m d 6 t w m d δ t w m max δ t w m δ τ w m d here ɛ is the duration normalized energy and t is the duration normalized change in temperature the sets of duration normalized energies and temperatures for historic hws are represented by ɛ w m and t w m whereas ɛ w m d and t w m d refer to the functions used to evaluate future sampled hws these sets are mapped to the interval 1 1 using the following linear transform 7 i x 2 x min x max x min x 1 here x is either of the sets in eqs 5 and 6 and i is the corresponding 1 1 transformed data the inverse transform to return to x is 8 x i i 1 max x min x 2 min x the maximum and minimum values of each set e g min ɛ w m max ɛ w m have to therefore be retained so the reverse transformation can be accomplished the mean μ and standard deviation σ of truncated gaussian distributions on the interval 1 1 are used as variables in the optimization to fit historical wave histograms initial values of these parameters are the sample mean and standard deviation of their corresponding sets for example σ ɛ w m is given an initial value in optimizing the standard deviation of i ɛ w m finally the maximum and minimum values of ɛ w m and t w m are taken as the boundaries a and b of a truncated gaussian distribution these values are transformed linearly with the μ and σ parameters as described in section 2 4 2 1 3 model parameters for optimization as defined in the algorithm the model has 14 parameters per month that must be determined the parameters are 1 two parameters for the probability of starting an extreme event p h w p c s 2 two initial probabilities of sustaining an extreme event p s h w p s c s 3 four truncated gaussian parameters for temperature and energy the boundaries of the distributions are fixed at a 1 and b 1 and 4 six function form parameters p m a x δ t p δ t c for each w the time variant function p 0 term of eq 2 are the p s terms the model takes a sample of uniformly distributed random numbers on the interval 0 1 for each month these are considered to be an hourly time series for the optimization a period of several million samples for each month is simulated to characterize statistical distributions this is changed to an hourly time series over a year when creating weather files with m m varying as each month passes the series of random numbers is applied to the transition matrix m m which provides a history of extreme temperature events the history is filtered to make all events at least one day long and rounds events greater than one day to the nearest day for each event the corresponding truncated gaussian distribution is sampled and the wave duration is then used to produce a peak temperature and total energy these values are then provided as the inputs to section 2 5 the model output histograms of temperature and duration can be compared to historic distributions or ipcc shifted thresholds in an optimization framework to get the model parameters to produce a fit to the historic data or future shifted conditions model output has to be unscaled and reverse normalized to produce temperature and energy values this is accomplished by sampling from the truncated gaussian distributions for the appropriate month transforming via x in eq 8 and using eqs 5 and 6 in reverse 2 2 historic fit of stochastic model the historic fit requires processing of the noaa inputs in fig 1 to form histograms of peak temperature change δ t and duration d of sets of consecutive extreme temperature days for both hws and css these historic histograms become the target for an optimization that finds the best fit for the stochastic dynamic model parameters mews uses an evolutionary algorithm for this optimization with a least squares difference between the mews output and the historic histograms of d and δ t 2 2 1 historic histograms the noaa daily summaries provide maximum and minimum daily temperature noaa 2021a the noaa climate normals contain hourly data for one year with 10 50 and 90 ci thresholds noaa 2021b this study used the 1991 to 2020 version of the climate normals the climate normals are aggregated to daily minimum and maximum values for each ci for comparison to the daily summaries extreme temperature days from the daily summaries are then classified as 1 extreme hot days the daily maximum is greater than the climate norm daily maximum 90 ci hot daytime or the daily minimum is hotter than the climate norm daily minimum 90 ci hot nighttime 2 extreme cold days the daily maximum is less than the climate norm daily maximum 10 ci cold daytime or the daily minimum is less than the climate norm daily minimum 10 cold nighttime both sets of days are grouped into subsets for any consecutive extreme days the lengths of these subsets form a duration histogram for hws and css for hws the maximum daily temperature minus the 50 daily mean climate normal for each subset forms a maximum temperature change histogram for css the minimum temperature minus the 50 daily mean forms a minimum temperature change histogram the histogram s number of bins is set to the number of extreme temperature events divided by ten this strikes a balance between resolution of temperature bins and number of events in each bin it gives a good shape for numeric histograms for the 50 or more years of data required 2 3 climate scenario polynomials mews uses gcm output from cmip6 eyring et al 2016 to produce a historical polynomial for surface temperature and polynomials for shared socio economic pathways shared socio economic pathways ssps riahi et al 2017 that need to be simulated these polynomials establish the baseline temperature change from 1850 1900 needed in the ipcc hw shift factors described in section 2 4 they also provide the amount to shift temperatures for future years mews uses a two dimensional interpolation scheme to produce a specific set of profiles for a given latitude and longitude the set of cmip6 models used can be changed for this study the largest available set of models for each ssp was used as seen in table 1 the order of polynomial can be increased to produce smooth fits generally speaking a quadratic is sufficient for future scenarios while a fifth to sixth order polynomial is needed to properly match the historic variations in climate from 1850 to 2014 2 4 shift historic to future the shift to future conditions is accomplished by assuming that only the truncated gaussian distribution for temperatures change it is important to note that many other ways of shifting in the future could be used but our approach was chosen as the best method with the currently available information also mewss has the capacity to shift cs but information about such shifts is lacking we therefore keep historical cs fdi constant into the future for hws the mean and standard deviation are assumed to scale equally so that the minimum bound of the distribution remains fixed while the maximum is retained the ipcc fdi shifts are given for 10 and 50 year events to define the hourly probability of a 10 year or 50 year event happening we assume that historic hw rates are uniform we also require that at least 50 years of historic data be available for the analysis to be valid we then linearly interpolate the number of hws expected in 10 years and 50 years 9 p y δ t h t h y m δ t y n h n y here p y is the probability of an event of either 10 or 50 year events i e through index y 10 50 δ t h is the total time interval of the historic noaa daily summaries data t h y m is the total time in month m in the historic time interval δ t y is the number of hours in a year n h is the number of hw events in the historic time interval and n y is the number of hours in y years the probabilities p y are then used to calculate the threshold temperature δ t y where the historic histogram s cumulative distribution function cdf of changes in temperature are linearly interpolated between bin average points the ipcc physical basis report for policy makers masson delmotte et al 2021a estimates how extreme events will shift and stretch probability distributions with changes in surface temperature at a specific latitude and longitude the input to these shifts is the surface temperature change δ t g from the 1850 1900 baseline established by the polynomials calculated from cmip6 data in section 2 3 this temperature change is calculated for each year for which future weather is needed histories that increase temperature beyond δ t g of 4 c are linearly extrapolated by mews beyond the table values beyond 6 05 c are rejected the threshold was moved up to 6 05 from 6 so that the worcester case could include 2080 figure spm 6 of masson delmotte et al 2021a contains numerical information that is summarized in table 2 the ci represents the spread of results across different climate models averaged over the earth s entire land surface these bounds therefore do not necessarily reflect a specific location s spread but are a reasonable estimation for a specific location the mews software allows use of one or more of the ci in table 2 given a δ t g we interpolate linearly to get values for δ t y c i p c c and f y c i p c c from table 2 we then have to adjust this value because the time period for the noaa daily summaries noaa climate normals and ipcc hw increase in fdi all are different this is illustrated in fig 3 the values shown are not real data and are only to illustrate the three periods and how their average temperature varies between the three periods mews neglects the temperature difference between the hw and climate normals baselines to account for the differences in the dates for the noaa daily summaries and ipcc baseline of 1850 1900 the temperature anomaly for the average temperature of the hw period δ t g h w is used to interpolate factors from table 2 which we will call δ t y c h w i p c c and f y c h w i p c c we then use these values to alter future targets for frequency and intensity of 10 year and 50 year hw events for frequency the future probability of the two events are taken as 10 p f y c p y d i v i d e c i f y c i p c c f y c h w i p c c the function d i v i d e c i is needed because cis cannot have simple division applied to them it takes random samples of 100 000 values of the two implied normal distributions for the cis and then reconstructs the new resulting ci this has more than sufficient precision to divide the cis numerically for intensity we take the new temperature to be 11 δ t f y c δ t y δ t y c i p c c δ t y c h w i p c c once these target thresholds are known we can shift the mews model temperature distributions by changing the mean and standard deviation of each month s truncated gaussian temperature distributions the algorithm uses newton s method by shifting the distribution a small increment finding the sensitivity and then calculating a target shift to cause the distribution 10 and 50 year threshold to meet the target thresholds the model distribution thresholds δ t f m y m c are the temperature anomalies for which the probability of temperature being greater than the threshold is p f y c the newton algorithm typically converges in 3 4 iterations several instances of the model are averaged for each interation since the model is stochastic the error between the model threshold and shifted historic threshold serves as a measure of the accuracy of the future shift fit 12 e r r f u t y m c δ t f m y m c δ t f y c 2 5 shape function the mews model produces waves with d and δ t a shape function is needed to apply this to an hourly weather file a useful function to fulfill this need is shown below 13 δ t t d δ t m i n a sin π t d o d d b 1 cos 2 π t δ t m i n c t d o d d b 1 cos 2 π t δ t m i n c t d o d d this function contains a constant shift c a daily increase in temperature b and a term for the entire hw duration a here δ t is the change from the mean daily climate norm temperature and δ t m i n is 24 hr the parameter d o d d is the closest odd multiple of δ t m i n that is less than the hw duration d 14 d o d d δ t m i n d δ t m i n δ d δ t m i n mod 2 here mod is the modulus operator is the floor function or closest integer less than the input and δ is the dirac delta function using d o d d instead of d in eq 13 avoids erratic variations in the maximum temperature condition with respect to the hw duration enabling mapping δ t to sampled values δ t s and δ e s integration of eq 13 from 0 to d produces the following total energy in degrees temperature times time 15 δ e s 2 a d o d d π b d b δ t m i n 2 π sin 2 π d δ t m i n though d o d d makes the energy relationship more complex it makes the heat wave maximum temperature much more simple 16 δ t s a 2 b c the relationships in eqs 15 and 16 can be solved to provide values for a and b the offset c is set to zero in the current version of mews 17 a δ t s π 2 d o d d δ e s 2 π d 2 d o d d δ t m i n 4 d o d d sin 2 π d δ t m i n 18 b δ t s a 2 the values of a b and c must be greater than zero for hws or less than zero for css for the solution to be physically meaningful if such a solution cannot be found then b 0 and c 0 with a remaining as the only unknown in such cases only the sampled temperature is met and the energy constraint is not satisfied 2 6 case study the mews model was applied to worcester ma in the united states tmy version 3 tmy3 williams et al 2011 climate normals noaa 2021b and daily summaries data from worcester regional airport from june 1st 1946 to october 28 2022 was used within the proposed framework to produce temperature and distribution fits the data sources used are summarized in table 3 the historic daily summary data for extreme days hws and css were compared to three random trials of our stochastic model using the 2 sided kolmogorov smirnov test statistic p values to determine if the stochastic dynamic model output distributions are the same as the historic distributions with 95 confidence the historic temperature distributions were then shifted and stretched to reflect the ipcc intensity and frequency increases seen in table 2 for ssp2 4 5 3 7 0 and 5 8 5 values were evaluated for 2020 2040 2060 and 2080 3 results the linear regressions of duration normalized energy and temperature are shown in fig 4 with scale 0 1 for both axes where the maximum values of duration and temperature are normalizing the x and y axes respectively also depicted are the extreme event energy and temperature histograms normalized by the linear regressions and scaled to the interval 1 1 all scales and data descriptions are provided in the figure caption as expected the temperature regressions have a non zero offset while the energy regressions do not as a result some of the distributions still appear to have a skew but most cases appear symmetric the historic and future projected change in surface temperature extracted from cmip6 for the ssps analyzed is shown in fig 5 the cis shown reflect the variations of the many cmip6 models shown in table 1 the polynomial fits depicted by yellow red and dark red lines were used at 2020 2040 2060 and 2080 to create shifts in hw fdi the fit for the historic optimization was run for 30 60 and 120 iterations using evolutionary optimization with the conclusion that 30 iterations were sufficient for convergence to greater than 1 accuracy the optimization results for temperature and duration were evaluated via the kolmogorov smirnov statistics and p values given in tables 4 and 5 cases where the null hypothesis was rejected i e the null hypothesis is that the distributions are the same with 95 confidence have red boxes surrounding them for temperature these worst cases are shown in fig 6 fig 7 gives an example case for how the temperature distribution for august 2080 is altered to meet the 10 year and 50 year heat wave thresholds shifted per table 2 shown are the original temperature threshold δ t y with cyan representing the 10 year threshold and orange red representing the 50 year historic threshold these are then shifted by δ t y c i p c c where the black arrows show the shift into the future the mews actual thresholds δ t f m y m c are shown in dotted lines of the same color as the target thresholds shown in dashed lines as previously stated the duration distributions are not shifted for this study the frequency increase for 10 and 50 year events is accounted for by increasing the area in front of the 10 and 50 year thresholds for the temperature distributions the errors between target and actual mews model temperature thresholds e r r f u t y m c for all future projections are shown in table 6 the average error across all cases is 0 037 c with a sample standard deviation of 0 841 c the worst case negative and positive errors are highlighted with red boxes around them as illustrated in fig 8 the primary cause of error is differences in the size of band between 10 and 50 year events for earlier years the 50 year events are over predicted and 10 year events are under predicted while the average of the 10 and 50 year events between the model and actual shifts is nearly zero this problem decreases for 2060 and 2080 because the frequency multipliers from table 2 become much larger the evaluation of error was executed for three cases of the stochastic model and fig 8 clearly shows how three evaluations have slightly different errors finally fig 9 shows how the application of the shape function provided in section 2 5 alters dry bulb temperature for practical energyplus file use the realization includes a constant offset from the temperature polynomials shown in fig 5 it also includes the hws and css added by the stochastic model the hws and css are referenced by the same climate normals regardless of the future year so that the offset does not get double counted this exhibits single realizations of the stochastic model characterized by the distributions shown in the previous figures hundreds or even thousands of such realizations are needed to characterize stochastic response of bems or other infrastructure models the mews stochastic model historic fits pass with 95 confidence for the kolmogorov smirnov test p values for 85 of the cases provided in tables 4 and 5 this firmly establishes that the proposed model provides statistically significant fits for the worcester noaa historical extreme temperature data the distributions that did not pass the test with 95 confidence illustrated in fig 6 all have sudden jumps in the historic worcester histograms that are unlikely to be phenomena that actually warrant fitting 4 discussion 4 1 model fit the cases that did not pass the statistical test were reviewed to verify the model behavior was acceptable all the distributions for css in july august and october have a spike for mild css that is probably due to transition between the rest of temperatures and the 90 threshold since this model only focuses on the 10 lowest and 10 highest temperature records on a daily basis we would expect that adding an overlapping normal conditions distribution for the 80 remaining temperature data would smooth out this discrepancy the hw fit for august is poor for this reason as well regardless the 86 year period of data recorded 1946 2022 for extreme temperature event data does not provide smooth daily histograms making it acceptable for some of the fits to not meet 95 confidence this problem of extreme events not having enough historical basis to characterize distributions in a changing climate is even more pronounced for less frequent events like hurricanes the errors seen in table 6 are deemed acceptable for future shifting since the mean between 10 and 50 year events are equivalent the stochastic nature of the maximum errors of 2 67 c are within the range of physically realistic events and are for 50 year events for which threshold uncertainty is expected to be high the mews input structure allows for extreme temperature events that are unrealistic for some of the fits but these can be removed since they are low probability events that occur for the model fit procedure which uses 285 years of simulation 2 5 millions hours per month to characterize the distributions even so the difficulties associated with baselining the historic data versus ipcc fdi shift and offset factors illustrated in fig 3 points to the need for perfect alignment of the climate normals extreme temperature data and climate baseline to do this an rcm based approach is necessary ideally rcms could be run with stochastic changes to boundary conditions so that actual distributions for extreme temperature could become the target the mews algorithm is ready to fit such cases since it can fit any arbitrary distribution via nonlinear optimization producing such a product running rcms hundreds or thousands of times per ssp is prohibitively expensive computationally though instead multiple cases of rcms across different models and different run cases will be needed to gather a couple dozen points to estimate means and standard deviations of extreme phenomena a new fit procedure would be needed but its tools are already in the mews software ideally runs that continue for several decades in the same climate condition are needed so that more than 50 years of information is available for climate models even though this is desirable it would significantly increase the amount of time needed to download and process large amounts of data regardless a properly fit mews stochastic dynamic model is needed so that extreme events can be accurately replicated rapidly with a low computational burden for infrastructure analysis purposes this extends resilience analysis into a framework that can incorporate complex combinations of design basis threats dbts that are correlated to extreme heat or cold without the need for big data processing and super computing our goals are to replicate temporal patterns exhibited by climate models with moderate precision so detailed energy and resilience models can be evaluated against extreme events in a stochastic way this will enable planners to overcome the shortcomings of the scenario based approach most commonly used to assess how infrastructure might perform in the future the scenario based approach involves creating several scenarios of environmental variables that often attempt to bound the problem with best case average and worst case futures in taking this approach researchers overlook the majority of statistical possibilities for the future and are likely to overlook cascading events such as power outages caused by a combination of increased demand and increased heat even when such studies consider such events baniassadi et al 2018 exploring possibilities is very limited this can be acceptable for analyses of a stable system but can overlook event combinations that destabilize an infrastructure system that do not appear to be worst case 4 2 future work our work needs to be extended to include estimation techniques for how extreme heat can effect other weather variables such as wind speed wind direction and humidity similar to herrera et al 2018 to do so the use of rcm outputs are certainly needed because not enough data exists for other variables for the lengths of time needed for hw characterization quantifying heat wave intensity is an active area of research mazdiyasni et al 2019 therefore we hope to perform increasingly sophisticated experiments to assure that the shape function we use with mews spans the statistical distribution of hw shapes in addition future research could use machine learning to learn valid shapes of hws based on rcm analysis we think the approaches outlined in this paper are sufficiently flexible to capture extremes for precipitation storms and others our time varying stochastic transition matrix has the parameters needed to keep events to varying scales of duration to avoid physically impossible events for earth s climate system we have intentionally placed bounding parameters cutoff times truncated gaussian distributions so that future projections only include increases due to rcm or gcm predictions rather than random sampling events this is especially important when altering the tails of temperature distributions stochastic approaches in resilience analyses are common e g see rocchetta et al 2018 balakrishnan and cassottana 2022 the mews software has potential to naturally incorporate fdi of extreme heat into such analyses there is a need for infrastructure mews like analysis to map out how fdi of extreme heat will affect resilience metrics for competing technologies such as micro grids and energy efficiency or sustainability measures such as electrification of buildings or use of ground source heat pumps the stochastic approach for specific locations provides valuable information if one infrastructure design option is more likely to produce good outcomes than another design option the approach can also show if differences between designs are not statistically significant 5 conclusion the mews model presented has been shown to objectively fit data for worcester ma with 95 confidence for most cases using the kolmogorov smirnov two sided p value test cases that did not meet 95 confidence have been shown to be acceptable the future shifting of the model has been shown to have acceptable thresholds of error between the model and ipcc fdi shifts offsets we conclude that the mews model is useful for extreme temperature event simulation for future climate conditions the optimization tools and models developed in the mews open source repository villa 2023 are more broadly applicable to other extreme event models also several studies are underway to apply mews to additional locations including houston texas and kodiak island alaska even so the continuation of this research requires demonstration that the statistical approaches used do not introduce significantly different statistical distributions than dynamic downscaling approaches using rcms the models we have presented can most likely be adjusted to overcome inaccuracies in this respect but a comparison has not yet been made declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments this work was funded by the u s department of energy building technologies office bto future weather project u s department of energy bto 2023 under contract number nl0039169 software and data availability mews version 1 1 0 february 28 2023 is available as open source software under the revised bsd license it also contains third party libraries approved for unlimited redistribution per the license file specifications in the mews github repository https github com sandialabs mews this paper s results were generated using the worcester example py and verify mews py scripts in the examples folder of the repository all data used in this research is freely available to the public through the noaa download websites for climate normals https www ncei noaa gov data normals hourly 1991 2020 archive and daily summaries https www ncei noaa gov data global historical climatology network daily archive for station usw00094746 for the worcester regional airport the ipcc hw shift information is also freely available and is replicated in this paper data from the sixth coupled model intercomparison project used by this work has been placed at https osf io ts9e8 files osfstorage disclaimer sandia national laboratories is a multimission laboratory managed and operated by national technology and engineering solutions of sandia llc a wholly owned subsidiary of honeywell international inc for the u s department of energy s national nuclear security administration under contract de na0003525 this paper describes objective technical results and analysis any subjective views or opinions that might be expressed in the paper do not necessarily represent the views of the u s department of energy or the u s government abbreviations bto building technologies office bem building energy model cdf cumulative distribution function ci confidence interval cmip5 the fifth coupled model intercomparison project cmip6 the sixth coupled model intercomparison project cs cold snap dbt design basis threat fdi frequency duration and intensity ftmy future typical meteorological year gcm global circulation model hw heat wave ipcc intergovernmental panel on climate change ma massachusetts mews multiscenario extreme weather simulator noaa national oceanic and atmospheric association rcm regional climate model ssp shared socio economic pathway tmy typical meteorological year tmy3 typical meteorological year version 3 u s united states 
