index,text
26085,coupled wave 3d hydrodynamics model runs are performed to investigate thermal discharge release to coastal areas by means of including nearshore effects of wave current dynamics the study area comprises the vicinity of a power plant at cerano in south italy where cooling industrial waters are released to the sea the implemented model is calibrated by using temperature measurements and sensitivity analyses are carried out for various relevant drivers and input parameters afterwards the effect of thermal discharge is investigated through distinct hypothetical scenarios for a combination of metocean conditions and operational features of the power plant modifying water discharge and temperature at its outlet the model results of this representative array of conditions are intercompared and evaluated on the basis of heat dispersion rate and areas of influence providing with useful insights on the numerical simulation of the process and the potential effects for the specific coastal area keywords thermal pollution 3d numerical model sea temperature wave action mixing area 1 introduction sea surface temperature sst is considered as one of the most important parameters in observing ecosystem conditions for marine coastal environments azmi et al 2015 brando et al 2015 with the sustainability of their habitats being largely dependent on water quality thermal pollution is defined as any deviation of the environment temperature due to industrial cooling cycles or natural discharges of cold fluid into water bodies dodds and whiles 2010 although thermal pollution may refer to the decrease of water temperature as well the term is usually associated with the effects of the increase in water temperature in rivers lakes and coastal areas as well as the consequent decrease of the concentration of dissolved oxygen do degrading water quality generally the mean water body temperature may not be significantly affected by the introduction of thermal discharges due to its large heat capacity but eventually steep rises in local temperature disturb the aquatic flora and fauna in it modifying the ecological balance in the affected areas indeed abrupt changes in water temperature known as thermal shocks deeply damage the marine ecosystem leading to significant decrease of do concentration in water up to biodiversity alteration such as increased bacteria levels changes in metabolism reproduction denaturing of life supporting enzymes and increase of mortality for aquatic species lardicci et al 1999 chuang et al 2009 li et al 2014 hence thermal pollution is seen as a severe threat for ecological composition in coastal waters around the world and their industrial use as cooling agent is identified as its main cause indeed power plants typically use water from nearby bodies to cool their machinery discharging it back at elevated temperatures in a range of 5 100 c water used in industries for cooling purposes is released back to environment as thermal effluent nuclear power plants require from 30 to 100 more cooling water rates than other types of plants of comparable power output and their releases have been often investigated in terms of impacts on biodiversity li et al 2014 and tourism development rosen et al 2015 therefore governments and international organizations set various standards regarding effluent temperatures from such facilities in order to regulate their operation and minimize their environmental impacts manivanan and singh 2013 to address the impacts of thermal shocks on the aquatic environment it is generally required to analyze the mechanism of dispersion of the thermal effluent in the water body nearfield of its release and to establish a zone of influence where a certain temperature increment is acceptable each eu country developed its own regulation regarding environmental standards for effluent temperatures in water bodies implementing the specifics reported in the principal eu directives such as water framework habitats and marine strategy framework directives a review of the regulations adopted by some eu countries is listed in appendix a considering the above depicted european framework italian regulation follows the general trend as far as both maximum effluent temperature and acceptable ambient effluent temperature difference are concerned d lgs n 152 2006 dictates that i the maximum water temperature at the outlet should not be higher than 35 c and ii the maximum temperature increment in the coastal field at a distance of 1 km from the outlet should be lower than 3 c in temperate coastal and estuarine areas cooling water discharge from power stations is typically between 8 and 12 c above the ambient water temperature and recirculation of effluent water should be therefore granted by designing intake and outlet structures in order to allow a proper re cooling of sea water dur√°n colmenares et al 2016 the warmer water of the effluent is of course less dense than the receiving body water and therefore it tends to rise at the surface a persistent interface is so generated because of this density difference and dynamics like advection and diffusion tend to tone down such an interface leading to complex mixing mechanisms depending on climate conditions the environmental impact of thermal release into the sea can be more or less localized and inducing some modifications in nearshore currents by power plant intake and thermal discharge systems elwany et al 1990 in addition the diffusion of the cooling fluid in sea could also affect the efficiency of the heat exchangers of the power plants since the inflow temperature eventually influenced by the thermal outflow should be taken into account during the design of the plant machines more site specific analysis on the developing thermal plume is then needed for multi units at the same site to assure sufficient separation between the inflow and outflow waters to be evaluated together with operational limitations due to the seawater temperature kim and jeong 2013 accordingly the estimation of the mixing area or thermal plume seems to be critical to ensure compliance with the national regulation limits and is largely influenced by parameters such as outflow discharge and velocity cooling water temperature and sea and coastal ambient conditions sst currents tides wave and wind rmes infrastructures economical facilities etc furthermore effects induced by the present climate variability and the expected climate changes should not be neglected considering the dire consequences those already have will have on mean sea temperature schaltout and omstedt 2014 along with the design life time of power plants exceeding 30 50 years in most cases the present study focuses on dynamics induced by thermal discharge release in the coastal area of cerano south italy in the vicinity of a power plant which uses seawater as a cooling agent the investigation is carried out by means of a coupled wave 3d hydrodynamics numerical model telemac and the paper is organized as described in the following section 2 presents the state of the art in the numerical modelling of coastal water quality with a focus on thermal discharge the case study of cerano is described in section 3 where site characteristics collected field data to implement and verify the model and numerical setup are illustrated section 4 presents the model calibration and sensitivity analysis specifics discussing in detail all data and model parameters involved by means of typical performance indices results of the calibrated model runs are shown in section 5 for the 3 hypothetical representative scenarios of environmental and operational conditions namely sc1 focusing on tidal and wave conditions sc2 focusing on water temperature stratification in the coastal field and sc3 focusing on the operational characteristics of the power plant i e water discharge and effluent temperature the presented numerical investigation is the first authors step towards the better understanding of thermal discharge dynamics in coastal areas by means of numerical modelling of thermal pollution furthermore the study is among the few in relevant literature that focuses directly on thermal discharge dynamics rather than studying it indirectly through its biological impacts and is deemed to set the bases for future work on the same path 2 modelling coastal water quality with focus on thermal discharge a state of the art as also mentioned in the previous coastal water quality is essential for the preservation coastal ecosystems and the sustainable evolution of human activities in coastal and marine areas as such it has been extensively studied over the years by various means ranging from simple observational and monitoring techniques to entire frameworks and from simple empirical equations to entire computational systems given the advances in computational power nowadays numerical modelling has become the main tool for the investigation of the effects that various natural and or human forcings have on water quality by being able to i incorporate the representation of a wide array of processes and interactions and ii examine multiple scenarios of the aforementioned forcings for research and operational purposes in the following the theoretical background and advances in coastal water quality modelling is presented with a clear focus on thermal discharges in coastal marine waters regarding natural forcings the ones referred to as metocean forcings are quintessential for coastal water quality modelling comprising wind air temperature humidity precipitation etc accurate meteorological data are required at the water surface to act as boundary conditions james 2002 driving the currents through wind stress changing the temperature through heat fluxes and salinity through precipitation and evaporation wave effects are important in the surface layer and in shallow water where coupled wave hydrodynamics modelling is required in order to account for increased mixing due to breaking waves and wave induced enhancement of bed stress in addition wave driven residual flows such as stokes drift and longshore currents are particularly important when studying thermal discharge in coastal areas with the second ones constituting probably the strongest drive for thermal plumes within the breaker zone regarding human forcings one may refer in general to the addition of pollutants and or contaminants to the coastal marine environment that deteriorate water quality sediments metals oil spills chemicals toxins thermal discharges plastics etc as results of various economical and recreational man activities focusing on thermal discharge studies and in the context of this work a basic division can be made into i studies that examine the problem indirectly though its biological impact on coastal ecosystems and ii studies that examine the direct effects of thermal discharge on water temperature sst distribution depth stratification and local hydrodynamics the second ones when based on numerical modelling like the present work offer a better understanding of the actual processes involved in the studied phenomena and are more versatile in the sense that they can be used both for quantitative comparative evaluations and as bases for other studies on the indirect effects of thermal pollution falling into the first of the above categories one could refer indicatively but not exclusively to the works of chuang et al 2009 who studied the effects of elevated water temperatures and residual chlorine from thermal discharge by a coastal nuclear power plant in taiwan on aquatic flora characteristics ingleton and mcminn 2012 who proposed a multidisciplinary approach for assessing effects of thermal pollution on estuaries based on biotic indices and satellite observations lardicci et al 1999 and li et al 2014 who studied the impact of thermal discharge on benthic communities based on field measurements and statistical analyses and poornima et al 2005 who studied the impact of thermal effluents from a coastal power plant in india on phytoplankton based on field observations and laboratory experiments regarding studies falling into the second category one could start from the ones utilizing field measurements and observational data referring to the works of elwany et al 1990 who studied the modification of coastal currents by a power plant s intake and thermal discharge systems in southern california based on current measurements trajectory experiments and their statistical analyses and muthulakshmi et al 2013 who studied the plume dispersion from the thermal outfall of a nuclear power in india using thermal infrared images along with field measurements moving forward works on the bridging of near and far field analysis are also worth mentioning with the work of israelsson et al 2006 and suh 2001 2006 2014 standing out in relevant literature israelsson et al 2006 proposed the use of lagrangian approaches independently or along eulerian models in order to extend the domain of near field analysis of contaminant mixing near pollution sources in inland coastal water bodies while suh 2001 2006 2014 proposed a hybrid technique to simulate the dispersion of heat from thermal discharges combining near field models e g cormix and adcirc and far field eulerian lagrangian transport models although the above constitute valuable background knowledge for researchers working on such relevant phenomena studies on the numerical modelling of thermal discharge are the most essential for research and operational purposes nowadays and helped formulate the study design and realization of this work as well listing a number of different modelling approaches used for the simulation of temperature and hydrodynamics evolutions due to thermal discharge one could refer to the works of wu et al 2001 who applied the 3d hydrodynamic and transport model gllvht generalized longitudinal lateral vertical hydrodynamic and transport edinger and buchak 1980 schreiner et al 2002 who put in test the cornell mixing zone expert system cormix jirka et al 1996 kolluru et al 2003 who used the generalized environmental modelling system for surface waters gemss and a probabilistic approach for the definition of the discharge s mixing zone chen et al 2003 who used the 3d finite volume coastal ocean model fvcom maderich et al 2008 who used the 3d numerical model they developed named threetox and cardoso mohedano et al 2015 who used the stony brook parallel ocean model presented by jordi and wang 2012 indicative studies on the comparative evaluation of different models include those of stamou and nikiforakis 2013 who presented an integrated model for the simulation of thermal effluent discharges to coastal waters consisting of the near field model corjet and the far field model flow 3dl and compared their results to predictions from the cormix model and tang et al 2008 who developed 3d reynolds averaged navier stokes model for the simulation of initial mixing in the near field of thermal discharges and tested it in various configurations comparing their results to the results of both cormix and visual plumes frick et al 2002 finally and due to its connection to the numerical tools used in this work reference should also be made to the work of i bedri et al 2014 who used the 2d and 3d hydrodynamics modules of the telemac suite telemac 2d and 3d respectively in combination to the environmental model subief 3d luck and guesmia 2002 in order to investigate the impact of a mixed sewage treatment plant and power plant effluence on coastal water quality and of ii matta et al 2017 who investigated the 3d flows induced by moderate or extreme winds in a brazilian bay 3 the case study of cerano south italy 3 1 site description the study comprises the coastal area in the immediate vicinity of the enel federico ii power plant at cerano about 12 km south the city of brindisi in south italy see fig 1 the coastline near cerano is made up of cliffs with a very small sandy beach at the foot the cliffs are made up of sandy clayey soils sometimes weakly cemented however easily erodible by the aggression of the incident wave motion with no beach at the foot along the northern sections of cerano and very small sandy strips along the southern slope delle rose 2015 mattm 2018 the coastline is under strong erosion due to the continuous dismantling action of the storms which abrading the foot of the cliff s sides establish precarious equilibrium conditions which result in collapses and erosion tendency the substantial shoreline retreat has led over time to the construction of some coastal defense works already before 1992 the littoral stretch south the plant had been protected by a revetment made from natural boulders and some years later rubble mound emerged breakwaters and groins were constructed along the coast inducing modifications in the sediment transport of the area indeed breakwaters induced a considerable accumulation of sediment over the years in the area protected by the cliffs leading to the formation of typical tombolos while groins trapped sand on their north side but caused beach erosion on the south side of the structures due to the prevailing direction of the solid longitudinal transport from north to south the wave data collected in the period 1989 2012 by the ron italian data buoy network franco and archetti 1993 franco 1995 archetti et al 2016 buoy placed offshore monopoli around 70 km north the study site are analytically transposed offshore the study site according to the geographic fetch based approach contini and de girolamo 1998 therefore the reconstructed typical wave climate at cerano is found to be characterized by nnw and ese storms mainly generated by mistral cold and dry wind usually blowing during the winter and by sirocco warm and humid wind generally inducing high storm surge respectively in the study site the net annual longshore sediment transport is directed from nw to se according to the italian atlas of beaches 1997 while sediment exchange with nearby coastal areas is limited constrained by the headland located at the north i e south of brindisi delle rose 2015 covering an area of about 270 ha and with a total installed capacity of 2640 mw the thermoelectric coal power plant federico ii is one of the largest in europe with an electrical mean efficiency estimated to be of 34 78 in 2014 environmental declaration 2015 seawater is withdrawn at around 300 m from the shoreline and at a water depth of 5 m with an inlet system of four pipelines heat exchangers are cooled by this sea water transported by a single pump of 1 000 m3 h capacity and at the end of the production cycle the cooling fluid comes back to sea thought a discharge channel with an outlet width of 80 m reaching a total estimated outflow of 3 bn tons of water per year corresponding to an annual average rate of 100 m3 s environmental declaration 2015 based on the plant s design conditions the maximum temperature difference between inflow and outflow waters is therefore estimated to be set to 12 c 3 2 field data instantaneous values of conductivity temperature and density ctd have been provided by the environmental agency of the apulia ron arpa regarding a monitoring campaign carried out on the day 17 july 2002 during this campaign ctd profiles were obtained at 23 points in the sea offshore the cerano power plant as presented in fig 2 the geographical coordinates east north of the measurement points together to the values of the depth average temperature and standard deviation are listed in table 1 due to the spatial distribution of the measurement points the dataset is divided into 5 categories reported in the last column of the table in relation to the distance from the outlet channel and the shoreline classes are channel including the 3 points located inside outlet channel close to the channel 1 km including points at a distance less than 1 km from the channel far from the channel in the nearshore north and far from the channel in the nearshore south including points close to the shoreline water depth 5 m and located at north and south of the channel respectively and finally far from the channel towards the offshore including points offshore water depth 5 m waves were acquired from ron at the monopoli buoy and propagated at the offshore boundary of study site wind time series have been retrieved from the italian national tide gauge network rmn while the global database tpxo egbert and erofeeva 2002 was used for tides 3 3 numerical modelling approaching the coastal ron waves generated offshore are influenced by shoaling refraction and loss of energy either due to bottom friction and wave breaking buccino et al 2014 cavaleri et al 2018 bonaldo et al 2019 to simulate all these physical processes including wave induced currents the present study of thermal discharge dispersion to the coastal area of cerano is carried out using the numerical modules of the telemac mascaret suite available at telemac 2019 that is distributed under a general public license the suite comprises finite element based solvers to simulate shallow water hydrodynamics and wave propagation and is able to model inshore water levels and wave spectra under different drivers the different modules comprised in the suite can simulate wind wave propagation ground water flows tracer transport sediment transport and morphodynamics the wave and 3d hydrodynamics modules of telemac mascaret suite are tomawac and telemac3d respectively and they were implemented in the proposed approach in order to propagate offshore waves and currents and reproduce nearshore dynamics influencing the thermal release at cerano tomawac module henceforth denoted as tom benoit et al 1996 solves a simplified equation for the spectro angular density of wave action by means of a finite element type method booij et al 1999 in order to describe wave propagation and dynamics in coastal areas this module is properly set up for the studied area based on previous experience on coupled wave 2d hydrodynamics runs for the representation of nearshore processes as presented in samaras et al 2016 gaeta et al 2016 and gaeta et al 2018 the processes included in the wave model simulations are i energy dissipation due to wave breaking according to battjes and janssen 1978 ii energy dissipation due to bottom friction according to hasselmann et al 1973 and iii nonlinear transfer of energy due to triad three wave interactions according to eldeberky and battjes 1995 no movable seabed no defense breaching and no past subsidence induced movements were assumed in the study the 3 d navier stokes equations are solved in telemac 3d henceforth denoted as tel3d hervouet 2007 with the option of the non hydrostatic pressure hypothesis and includes i the use of a finite element unstructured grid which allows selective refinement of the mesh at key locations in the domain and boundary fitting method for vertical discretization ii the transport diffusion equations of intrinsic quantities temperature salinity concentration in order to reproduce 3 d hydrodynamics including the trans port of active and passive tracers iii a wide range of options for vertical turbulence modelling the governing equation of the tracer transport is reported in equation 1 as 1 œÅ t t u œÅ t x v œÅ t y w œÅ t z u x œÅ t x u y œÅ t y u z œÅ t z where t and œÅ represent the water temperature and the water density respectively u v and w are the water velocity components and ux uy and uz are the turbulent thermal diffusivity components along the x y and z the spatial coordinate system respectively t is time the latest release of telemac version 7 0 includes the implementation in tel3d of thermal exchange fluxes between sea and atmosphere including typical processes of net solar radiation cooper 1969 long wave radiation berliand and berliand 1952 sensible heat rosati and miyakoda 1988 and latent heat due to evaporation panin and brezgunov 2007 tel3d can be directly coupled two way coupling with the spectral module tom on the same computational mesh in order to reproduce the dynamics of wave driven currents the gradients of the radiation stress induced by waves are computed using the theory of longuet higgins and steward 1964 as part of the hydrodynamics equations the updated values of current velocities and water depths calculated in tel3d are transferred to tom while tom solves the wave action density conservation equation and returns the updated values of the wave driving forces acting on the current to the hydrodynamics modules hervouet 2007 the initialization strategy of the implemented high resolution model followed the methodology described in federico et al 2017 where initial condition fields on temperature were provided by the mediterranean forecasting system mfs produced by data assimilation which supplies operational forecasting products in the framework of e u copernicus marine service information http marine copernicus eu cmems 2019 fig 3 shows the sea temperature field at day 17 july 2002 as obtained by mfs and the profiles at 4 selected nodes the ones falling into the study area and therefore used to set initial conditions of the present implemented model according to the experience and validation shown at large and coastal scale cucco et al 2012 trotta et al 2016 and at harbor scale gaeta et al 2016 a spin up time of 3 days was considered to be a reasonable choice in order to ensure the development of internal dynamics by the nested model therefore simulations were carried out for the period between 14 and 17 july 2002 that is the day when measurements were collected interpolating the oceanic fields over the new higher resolution grid is solved following the procedure suggested in de dominicis et al 2013 and validated in samaras et al 2014 although the local effects on sea temperature and hydrodynamics field induced by the cooling discharge from the plant are not captured in the mfs model due both to its large mesh resolution 7 7 km2 and the absence of any imposed liquid conditions at the shoreline boundary no run off and release into coastal waters details of the drivers used for the model runs are listed in table 2 describing the initial conditions ic offshore boundary conditions obc surface conditions sc and the imposed conditions at the channel outlet of the modelled variables in tel3d and in tom following the implemented multiple nesting procedure gaeta et al 2016 in the model finite element techniques were used to solve the hydrodynamic equations adopting the z vertical discretization to follow the surface and lower boundaries the multidimensional upwind residual distribution method was applied for the advection of three dimensional variables under tel3d and the boundary conditions were applied following the method of characteristics the advection of tracers was solved using the distributive murd psi method and the prescription of tracers at the open channel was applied using imposition by the dirichlet boundary condition while a neumann type condition at the offshore was prescribed imposing a zero gradient of temperature the mesh for the implementation of the models in this work was generated using the freely available pre processing tool blue kenue chc 2010 a series of tests proceeded the final mesh generation in order to evaluate optimal mesh edge dimension in terms of both representing the processes of interest satisfactorily and keeping computational times to reasonable levels the selected variable density unstructured mesh see fig 4 consisted of two density rons a coarse one 100 m edge length extending from the offshore boundary and up to the plant outflow area and a fine one 10 m edge length in the immediate vicinity of the plant outlet and the nearby area aiming to better capture the mixing dynamics following the thermal release the number of nodes in the mesh equals to around 14 000 the used bathymetric and shoreline data were obtained by digitizing nautical charts acquired from the italian national hydrographic military service fig 4 the resulting map clearly indicates a gradually changing bathymetric pattern moving offshore in particular depths in the range of 2 3 m right in front of the shoreline remain respectively low up to 500 m to the offshore followed by the central mesh area which is characterized by depths ranging from 12 m to 18 m eventually transitioning to depth reaching around 30 m near the offshore boundary of the modelled ron i e the boundary where waves are generated as well 4 model calibration and discussion 4 1 sensitivity analysis on hydrodynamics drivers a sensitivity analysis was performed on hydrodynamics drivers included in the simulations in order to define which of them mostly influenced models performance as being sensitive to the variation of the simulated processes the analysis was carried out at the 7 points of the arpa apulia measuring campaign namely f2 f3 m1 close to the outlet channel x2 w6 far from the channel in the nearshore north and south respectively w3 and f4 far from the channel towards the offshore these points were selected among the measurements as they are representative of the defined location groups table 1 the analysis was applied to different drivers included in the simulations as listed in table 3 accounting for wave propagation w tidal elevation t and wind forcing u for a total of 8 performed runs to investigate the model performance the root mean square error rmse was evaluated allowing a comparison between the measured and the simulated temperature values these parameters were calculated by using the expression 2 r m s e 1 n t s t m 2 n where n is the total number of the measured points table 1 along the collected vertical profiles the index s is for the simulated value and m for the measured ctd data for each of the performed simulations default values as defined by hervouet 2007 samaras et al 2016 and gaeta et al 2016 were kept constant as well as the number of horizontal layers equal to 5 fig 5 presents the results of the sensitivity analysis in terms of rmse for temperature values the influence of the simulated processes strongly depends on the investigated location s distance from thermal discharge release point with minimum errors resulting when all drivers were included in the run i e w1t1u1 in particular points located close the channel f2 m1 present the highest errors ranging between 2 and 5 when waves and wind were excluded by the simulations that were reported as crucial drivers to simulate the thermal diffusion in nearshore areas therefore coupling wave and wind modelling despite rising the computational time by a factor 10 improves the quality of the results and might be accounted for in these complex coastal processes 4 2 sensitivity analysis on governing equations parameters a significant array of simulations was run in order to get a proper calibration of the models used in this work on the basis of the available temperature profiles provided by arpa apulia by means of investigating the influence of the tel3d parameters in order to reach the better agreement between data and simulations for the local sensitivity analysis the performed simulations were carried out using the one at the time approach simmons et al 2015 by increasing each parameter by a given percentage while leaving all others constant and quantifying the change in model output the parameters identified for the analysis characterize the processes involved in coastal thermal pollution such as the number of the horizontal layers the friction bottom coefficient the horizontal and vertical turbulence models the diffusion coefficients for velocity the horizontal diffusion for temperature and wind drag coefficient all the other parameters mainly regarding wave propagation and wave current interactions were kept equal to the calibrated values as defined by hervouet 2007 samaras et al 2016 and gaeta et al 2016 table 4 shows the list of analysed parameters their tested range and their corresponding final values adopted for the calibrated model runs for this analysis the physical processes by coupled wave 3d hydrodynamics modelling including wind influence were simulated according to the results shown in the global sensitivity analysis presented in the previous section results of the 45 runs were summarized for the 7 selected measurement locations in fig 6 where each panel shows the differences in depth average temperature between measurements and model results for each of the 9 investigated parameters in fig 6 red lines represent mean values top and bottom box edges represent the 75th and 25th percentiles line limits represent maximum and minimum values and red crosses indicate outliers the results show that the parameters that mainly influence the model quality thus improving the numerical reproduction of the thermal mixing processes are the choice of the vertical and horizontal turbulence models kv kh the vertical diffusion coefficient uz and the wind drag coefficient cd the model sensitivity to the adopted turbulence models and velocity diffusivity appears to decrease with the increase of distance from the shoreline and from the release channel points f2 f3 m1 x2 and w6 present the greatest variance in the agreement with measured temperatures the influence of the drag coefficient for wind appears on the other hand to be spatially uniform ranging from 0 2 to 0 1 following the analysis on the parameters the model can be assessed as calibrated with reference to the thermal dynamics and climate of the study area 4 3 discussion on the numerical results from the calibrated model results of the final calibrated model as obtained after the sensitivity analysis i e run w1t1u1 are presented in fig 7 where temperature profiles at the 7 reference points analysed in the previous sub sections were extracted from the numerical simulation and compared to the measurements although some uncertainties are present in the present numerical study about initial and boundary conditions implemented in the simulations the calibration results in a satisfactory agreement to data especially considering the complexity of the studied phenomenon as well as the equally complex dynamics of the various processes interactions involved in its numerical simulation in particular regarding the depth average temperature the following convergence results are observed maximum deviation from data reaching 2 c in just 2 of the examined locations deviation less than 1 c for 19 points maximum deviation of 0 2 c achieved for 6 out of the 23 examined points the greater difference is observed at points located offshore i e w3 and f4 where the low temperatures around 25 c are not captured well by the model overestimating them for water depths 3 m this can be attributed to the initial conditions used as temperatures extracted by mfs are overall bigger than 26 c see fig 3 the temperature increment Œ¥t due to the power plant release with respect to the ambient values is shown in fig 8 where the evolution of the plume development as discretized at the 23 points of the arpa apulia campaign is shown the model results and the measured values at a water depth equal to 1 m the overall diffusion of the thermal release is found to be well reproduced by the calibrated model where the computed mixing area is slightly greater extending to the southern direction revealing the flow directionality driven by wind and waves is satisfactory predicted in the simulation in accordance to the previous fig 9 shows the computed thermal plume in the studied coastal area where the sea temperature at the end of the simulation reveals the prevalent dispersion of the cooling waters towards sse and mainly along the southern shoreline arrows represent the velocity vectors the prevalent longshore current is driven by wind and waves and directed from north to south with speed reaching up to about 0 20 m s and that strongly affects the direction of the thermal plume both the presence of the two groins delimiting the outlet channel and the outflow speed of the cooling waters induce a perturbation in the actual current patterns resulting in the development of i a seaward directed plume with a velocity of about 1 m s and ii eddies downward the channel eventually contributing to the observed shoreline retreat in this specific area under the simulated conditions the directionality developed by the thermal plume is desirable in terms of power plant efficiency performance as the cooling waters move away from the morphologically constrained area north of the channel see also fig 1 which could lead to its entrapment and consequent increment in local temperature affecting both the water temperature at the intake i e reducing the plant efficiency and the acceptable thermal increment imposed by the national regulation fig 10 presents the sea temperature along the 3 selected planes namely s1 s2 and s3 located at the vicinity of the release channel as shown in fig 9 for a water depth of around 6 m section s2 in the middle panel up to the shoreline for sections s1 and s3 left and right panels respectively the presence of the cooling waters is clearly observed at section s2 that is located just in front of the outlet at a distance of around 100 m there the vertical gradient of the temperature slightly develops probably due to the low wave intensity occurring during the simulated period the horizontal dispersion of the thermal plume is observed as well with temperature decreasing up of 6 c along a 500 m long stretch results along section s3 instead reveal high temperatures of around 30 c at shallow waters from depth of 1 m up to shoreline persisting along the 1 km long shoreline at the southeast of the outlet as also represented in fig 9 5 numerical experiments under different environmental and operational scenarios after models sensitivity analyses and calibration a set of scenarios for different environmental and operational conditions were setup so that a proper assessment of the cooling water release from the cerano power plant could contribute towards i implementing an effective monitoring program of the thermal plume and ii establishing mitigation measures for its eventual environmental effects the aforementioned set of scenarios was setup in order to represent expected seasonal and industrial events in order to test them against the reference scenario for the real on site conditions described in section 4 this set as listed in table 5 comprised sc1 representing the worst metocean conditions for dispersion meaning no wind and therefore no waves while maintaining the same operational features with the reference scenario sc2 representing the maximum power production conditions of the plant reference to data during the day 17 july 2002 with cooling waters at the outlet with a temperature of 35 c and a temperature increment of 12 c with respect to the ambient temperature therefore set to be equal to 23 c while maintaining the metocean reference conditions sc3 representing sirocco conditions i e overturning the wind and wave direction while maintaining the same operational features with the reference scenario fig 11 illustrates the computed sea temperature field at a water depth of 1 m at the end of each numerical experiment carried out for the period 14 17 july 2002 as in the reference scenario for scenario sc1 fig 11a the absence of any metocean forcings tends to make the thermal plume stay in the area the temperature spread towards the south being lower than in the reference scenario see fig 9 for comparison furthermore the plume is limited to the nearshore spreading out in a similar way towards the areas located north and south of the outlet consequently affecting the former one which was not polluted in the reference scenario scenario sc2 see fig 11b is characterized by a thermal discharge with a temperature increment of 12 c with respect to the ambient temperature and cooling waters at the outlet with a temperature of 35 c in comparison with the reference scenario see fig 9 sc2 results reveal a similar plume distribution pattern although as expected a wider zone of influence was present expanding northwards of the outlet as well where impact was minimal in the reference scenario in sc3 fig 11c the prevalent direction of the thermal plume propagation reverses from south southeast to north northwest although evidence of spreading is observed north of the outlet as well the northern area also mentioned in subsection 4 3 constrains plume propagation and leads to thermal amassing in the area in this scenario the efficiency of the power plant is expected to be significantly affected due to the dual effect of the water intake s location being in the area and the environmental implications of the temperature difference increase following the analysis by dur√°n colmenares et al 2016 the spatial decay of the depth average temperature with respect to the ambient condition t0 was estimated along a cross shore section starting at the outlet and extending up to 1500 m offshore for all the 3 scenarios and the reference simulation i e on the day 17 july 2002 the values of the decay presented in fig 12 were normalized with the maximum temperature value tmax at the outlet changing with the simulated scenarios along the analysed cross shore section for the reference sc1 and sc3 scenarios the percentage of dissipated temperature decreases up to 50 at a distance of around 300 m from the outlet and arrives to be around null within 1000 1500 and 1600 m respectively the change of the metocean forcings in sc1 null drivers and sc3 overturning drivers is primarily responsible for the lower heat dispersion offshore in comparison to the reference conditions while closer to the shoreline the sea temperature increment remains fairly unaltered the variation of the industrial conditions in sc2 instead has the main influence on heat dispersion into the sea reducing the decay rate of the temperature towards the offshore the areas of influence for all scenarios table 6 were calculated for each simulation and per range of temperature increment referring to the computed sea surface temperature the values were normalized with the total volume of hot waters released by the power plant since the industrial conditions varied between scenarios considering the geographical extent of the implemented numerical domain fig 4 wind from the north allows the greater rates of heat dispersion mainly occurring towards the south fig 11 and giving up to 50 for a temperature increment less than 1 c while still persisting the same metocean conditions in sc2 areas of influence are greater from 1 5 for higher values greater than 8 c up to 69 for lower values of temperature greater than 1 c the maximum area of influence for lower temperature increments is then obtained for scenario sc1 97 i e in absence of any metocean forcing and for scenario sc3 94 i e for the south wind the case where the cooling waters would have difficulties to spread offshore and outside the numerical domain due to the northern coastal morphology cove like formation of the area that leads to an eventual thermal amassing 6 conclusions this work presents the setup sensitivity analysis calibration validation and implementation of a coupled wave 3d hydrodynamics model for the investigation of thermal discharge dynamics in coastal areas the model setup is based on the respective spectral and 3d hydrodynamics modules of the open source telemac suite the implemented case regards a thermal power plant in south italy which uses seawater intake for cooling purposes and releases hot water back to the sea through a coastal outlet after an extended sensitivity analysis the coupled model was calibrated by using the sea temperature data collected during the field campaign of arpa apulia in the area sensitivity analyses highlighted the importance for model performance in the representation of thermal discharge to coastal areas of i wave wind combination regarding model drivers and ii vertical horizontal turbulence models vertical diffusion coefficient and wind drag coefficient regarding governing equations parameters model calibration validation is deemed to have been successful managing to capture satisfactorily both the horizontal and vertical distribution of water temperature variations the final runs of the calibrated model regarded scenarios of environmental and operational conditions in order to investigate representative states of the specific coastal system in terms of both environmental issues i e temperature increment in the nearshore area and plant efficiency issues i e performance related to inflow temperatures the 3 simulated scenarios revealed the range of expected changes in the thermal plume s distribution for representative conditions in the field environmental operational providing useful insights on the specific case study s dynamics all in all the present work is among the relatively few in relevant literature that focuses directly on thermal discharge dynamics rather than studying it indirectly through its biological impacts and is deemed to constitute a useful basis for other studies on the topic either direct or indirect future versions of this work could be further improved by investigating case studies with more extensive field measurement datasets e g current wind data and or coupling the wave and 3d hydrodynamics modules with the water quality module of telemac available in the suite s last release in order to extend this approach s applicability nonetheless the systematic literature review analysis of the studied phenomenon and modelling approach followed in this work contribute to setting the framework for the direct numerical simulation of thermal discharges to coastal areas while also setting the bases for the specific case study s evaluation and environmental technical assessment acknowledgments the authors would like to thank mr ungaro from arpa apulia for proving the ctd data of the field campaign in 2002 the research was partially funded within the framework of the italian flagship project tessa development of technologies for the situational sea awareness supported by the pon01 02823 2 ricerca competitivit√† 2007 2013 program of the italian ministry for education university and research appendix a regulations in eu countries for thermal release in coastal area the principal eu directives such as water framework habitats and marine strategy framework directives do not require a trans european thermal standard therefore each eu country developed its own regulation british energy estuarine studies 2011 regarding environmental standards for effluent temperatures in estuarine coastal and marine systems and in particular their allowed temperature values was here listed and reviewed in france on the gironde estuary the temperature increment must be not greater than 11 c at the outflow point with a maximum temperature of 30 c with a special permission it can reach up to 36 5 c in summer in norway the temperature increment must be not exceeding 10 c at the outflow point with no more than 1 c of temperature increment in the mixing zone with some flexibilities up to 3 c at particular sites in spain the temperature increment must not exceed 3 c or 1 c as integrated throughout the water column at a distance of 50 m or more from the discharge point in germany maximum limit for cooling water discharges is set at 30 c and the temperature increment must be not exceeding 10 c for existing sites and 7 c for new plants in the netherlands discharges into marine waters must not exceed 30 c within a mixing zone bounded by the 25 c isotherm in italy the maximum water temperature at the outlet must not be higher than 35 c and the maximum temperature increment in the coastal field at a distance of 1 km from the outlet must be lower than 3 c 
26085,coupled wave 3d hydrodynamics model runs are performed to investigate thermal discharge release to coastal areas by means of including nearshore effects of wave current dynamics the study area comprises the vicinity of a power plant at cerano in south italy where cooling industrial waters are released to the sea the implemented model is calibrated by using temperature measurements and sensitivity analyses are carried out for various relevant drivers and input parameters afterwards the effect of thermal discharge is investigated through distinct hypothetical scenarios for a combination of metocean conditions and operational features of the power plant modifying water discharge and temperature at its outlet the model results of this representative array of conditions are intercompared and evaluated on the basis of heat dispersion rate and areas of influence providing with useful insights on the numerical simulation of the process and the potential effects for the specific coastal area keywords thermal pollution 3d numerical model sea temperature wave action mixing area 1 introduction sea surface temperature sst is considered as one of the most important parameters in observing ecosystem conditions for marine coastal environments azmi et al 2015 brando et al 2015 with the sustainability of their habitats being largely dependent on water quality thermal pollution is defined as any deviation of the environment temperature due to industrial cooling cycles or natural discharges of cold fluid into water bodies dodds and whiles 2010 although thermal pollution may refer to the decrease of water temperature as well the term is usually associated with the effects of the increase in water temperature in rivers lakes and coastal areas as well as the consequent decrease of the concentration of dissolved oxygen do degrading water quality generally the mean water body temperature may not be significantly affected by the introduction of thermal discharges due to its large heat capacity but eventually steep rises in local temperature disturb the aquatic flora and fauna in it modifying the ecological balance in the affected areas indeed abrupt changes in water temperature known as thermal shocks deeply damage the marine ecosystem leading to significant decrease of do concentration in water up to biodiversity alteration such as increased bacteria levels changes in metabolism reproduction denaturing of life supporting enzymes and increase of mortality for aquatic species lardicci et al 1999 chuang et al 2009 li et al 2014 hence thermal pollution is seen as a severe threat for ecological composition in coastal waters around the world and their industrial use as cooling agent is identified as its main cause indeed power plants typically use water from nearby bodies to cool their machinery discharging it back at elevated temperatures in a range of 5 100 c water used in industries for cooling purposes is released back to environment as thermal effluent nuclear power plants require from 30 to 100 more cooling water rates than other types of plants of comparable power output and their releases have been often investigated in terms of impacts on biodiversity li et al 2014 and tourism development rosen et al 2015 therefore governments and international organizations set various standards regarding effluent temperatures from such facilities in order to regulate their operation and minimize their environmental impacts manivanan and singh 2013 to address the impacts of thermal shocks on the aquatic environment it is generally required to analyze the mechanism of dispersion of the thermal effluent in the water body nearfield of its release and to establish a zone of influence where a certain temperature increment is acceptable each eu country developed its own regulation regarding environmental standards for effluent temperatures in water bodies implementing the specifics reported in the principal eu directives such as water framework habitats and marine strategy framework directives a review of the regulations adopted by some eu countries is listed in appendix a considering the above depicted european framework italian regulation follows the general trend as far as both maximum effluent temperature and acceptable ambient effluent temperature difference are concerned d lgs n 152 2006 dictates that i the maximum water temperature at the outlet should not be higher than 35 c and ii the maximum temperature increment in the coastal field at a distance of 1 km from the outlet should be lower than 3 c in temperate coastal and estuarine areas cooling water discharge from power stations is typically between 8 and 12 c above the ambient water temperature and recirculation of effluent water should be therefore granted by designing intake and outlet structures in order to allow a proper re cooling of sea water dur√°n colmenares et al 2016 the warmer water of the effluent is of course less dense than the receiving body water and therefore it tends to rise at the surface a persistent interface is so generated because of this density difference and dynamics like advection and diffusion tend to tone down such an interface leading to complex mixing mechanisms depending on climate conditions the environmental impact of thermal release into the sea can be more or less localized and inducing some modifications in nearshore currents by power plant intake and thermal discharge systems elwany et al 1990 in addition the diffusion of the cooling fluid in sea could also affect the efficiency of the heat exchangers of the power plants since the inflow temperature eventually influenced by the thermal outflow should be taken into account during the design of the plant machines more site specific analysis on the developing thermal plume is then needed for multi units at the same site to assure sufficient separation between the inflow and outflow waters to be evaluated together with operational limitations due to the seawater temperature kim and jeong 2013 accordingly the estimation of the mixing area or thermal plume seems to be critical to ensure compliance with the national regulation limits and is largely influenced by parameters such as outflow discharge and velocity cooling water temperature and sea and coastal ambient conditions sst currents tides wave and wind rmes infrastructures economical facilities etc furthermore effects induced by the present climate variability and the expected climate changes should not be neglected considering the dire consequences those already have will have on mean sea temperature schaltout and omstedt 2014 along with the design life time of power plants exceeding 30 50 years in most cases the present study focuses on dynamics induced by thermal discharge release in the coastal area of cerano south italy in the vicinity of a power plant which uses seawater as a cooling agent the investigation is carried out by means of a coupled wave 3d hydrodynamics numerical model telemac and the paper is organized as described in the following section 2 presents the state of the art in the numerical modelling of coastal water quality with a focus on thermal discharge the case study of cerano is described in section 3 where site characteristics collected field data to implement and verify the model and numerical setup are illustrated section 4 presents the model calibration and sensitivity analysis specifics discussing in detail all data and model parameters involved by means of typical performance indices results of the calibrated model runs are shown in section 5 for the 3 hypothetical representative scenarios of environmental and operational conditions namely sc1 focusing on tidal and wave conditions sc2 focusing on water temperature stratification in the coastal field and sc3 focusing on the operational characteristics of the power plant i e water discharge and effluent temperature the presented numerical investigation is the first authors step towards the better understanding of thermal discharge dynamics in coastal areas by means of numerical modelling of thermal pollution furthermore the study is among the few in relevant literature that focuses directly on thermal discharge dynamics rather than studying it indirectly through its biological impacts and is deemed to set the bases for future work on the same path 2 modelling coastal water quality with focus on thermal discharge a state of the art as also mentioned in the previous coastal water quality is essential for the preservation coastal ecosystems and the sustainable evolution of human activities in coastal and marine areas as such it has been extensively studied over the years by various means ranging from simple observational and monitoring techniques to entire frameworks and from simple empirical equations to entire computational systems given the advances in computational power nowadays numerical modelling has become the main tool for the investigation of the effects that various natural and or human forcings have on water quality by being able to i incorporate the representation of a wide array of processes and interactions and ii examine multiple scenarios of the aforementioned forcings for research and operational purposes in the following the theoretical background and advances in coastal water quality modelling is presented with a clear focus on thermal discharges in coastal marine waters regarding natural forcings the ones referred to as metocean forcings are quintessential for coastal water quality modelling comprising wind air temperature humidity precipitation etc accurate meteorological data are required at the water surface to act as boundary conditions james 2002 driving the currents through wind stress changing the temperature through heat fluxes and salinity through precipitation and evaporation wave effects are important in the surface layer and in shallow water where coupled wave hydrodynamics modelling is required in order to account for increased mixing due to breaking waves and wave induced enhancement of bed stress in addition wave driven residual flows such as stokes drift and longshore currents are particularly important when studying thermal discharge in coastal areas with the second ones constituting probably the strongest drive for thermal plumes within the breaker zone regarding human forcings one may refer in general to the addition of pollutants and or contaminants to the coastal marine environment that deteriorate water quality sediments metals oil spills chemicals toxins thermal discharges plastics etc as results of various economical and recreational man activities focusing on thermal discharge studies and in the context of this work a basic division can be made into i studies that examine the problem indirectly though its biological impact on coastal ecosystems and ii studies that examine the direct effects of thermal discharge on water temperature sst distribution depth stratification and local hydrodynamics the second ones when based on numerical modelling like the present work offer a better understanding of the actual processes involved in the studied phenomena and are more versatile in the sense that they can be used both for quantitative comparative evaluations and as bases for other studies on the indirect effects of thermal pollution falling into the first of the above categories one could refer indicatively but not exclusively to the works of chuang et al 2009 who studied the effects of elevated water temperatures and residual chlorine from thermal discharge by a coastal nuclear power plant in taiwan on aquatic flora characteristics ingleton and mcminn 2012 who proposed a multidisciplinary approach for assessing effects of thermal pollution on estuaries based on biotic indices and satellite observations lardicci et al 1999 and li et al 2014 who studied the impact of thermal discharge on benthic communities based on field measurements and statistical analyses and poornima et al 2005 who studied the impact of thermal effluents from a coastal power plant in india on phytoplankton based on field observations and laboratory experiments regarding studies falling into the second category one could start from the ones utilizing field measurements and observational data referring to the works of elwany et al 1990 who studied the modification of coastal currents by a power plant s intake and thermal discharge systems in southern california based on current measurements trajectory experiments and their statistical analyses and muthulakshmi et al 2013 who studied the plume dispersion from the thermal outfall of a nuclear power in india using thermal infrared images along with field measurements moving forward works on the bridging of near and far field analysis are also worth mentioning with the work of israelsson et al 2006 and suh 2001 2006 2014 standing out in relevant literature israelsson et al 2006 proposed the use of lagrangian approaches independently or along eulerian models in order to extend the domain of near field analysis of contaminant mixing near pollution sources in inland coastal water bodies while suh 2001 2006 2014 proposed a hybrid technique to simulate the dispersion of heat from thermal discharges combining near field models e g cormix and adcirc and far field eulerian lagrangian transport models although the above constitute valuable background knowledge for researchers working on such relevant phenomena studies on the numerical modelling of thermal discharge are the most essential for research and operational purposes nowadays and helped formulate the study design and realization of this work as well listing a number of different modelling approaches used for the simulation of temperature and hydrodynamics evolutions due to thermal discharge one could refer to the works of wu et al 2001 who applied the 3d hydrodynamic and transport model gllvht generalized longitudinal lateral vertical hydrodynamic and transport edinger and buchak 1980 schreiner et al 2002 who put in test the cornell mixing zone expert system cormix jirka et al 1996 kolluru et al 2003 who used the generalized environmental modelling system for surface waters gemss and a probabilistic approach for the definition of the discharge s mixing zone chen et al 2003 who used the 3d finite volume coastal ocean model fvcom maderich et al 2008 who used the 3d numerical model they developed named threetox and cardoso mohedano et al 2015 who used the stony brook parallel ocean model presented by jordi and wang 2012 indicative studies on the comparative evaluation of different models include those of stamou and nikiforakis 2013 who presented an integrated model for the simulation of thermal effluent discharges to coastal waters consisting of the near field model corjet and the far field model flow 3dl and compared their results to predictions from the cormix model and tang et al 2008 who developed 3d reynolds averaged navier stokes model for the simulation of initial mixing in the near field of thermal discharges and tested it in various configurations comparing their results to the results of both cormix and visual plumes frick et al 2002 finally and due to its connection to the numerical tools used in this work reference should also be made to the work of i bedri et al 2014 who used the 2d and 3d hydrodynamics modules of the telemac suite telemac 2d and 3d respectively in combination to the environmental model subief 3d luck and guesmia 2002 in order to investigate the impact of a mixed sewage treatment plant and power plant effluence on coastal water quality and of ii matta et al 2017 who investigated the 3d flows induced by moderate or extreme winds in a brazilian bay 3 the case study of cerano south italy 3 1 site description the study comprises the coastal area in the immediate vicinity of the enel federico ii power plant at cerano about 12 km south the city of brindisi in south italy see fig 1 the coastline near cerano is made up of cliffs with a very small sandy beach at the foot the cliffs are made up of sandy clayey soils sometimes weakly cemented however easily erodible by the aggression of the incident wave motion with no beach at the foot along the northern sections of cerano and very small sandy strips along the southern slope delle rose 2015 mattm 2018 the coastline is under strong erosion due to the continuous dismantling action of the storms which abrading the foot of the cliff s sides establish precarious equilibrium conditions which result in collapses and erosion tendency the substantial shoreline retreat has led over time to the construction of some coastal defense works already before 1992 the littoral stretch south the plant had been protected by a revetment made from natural boulders and some years later rubble mound emerged breakwaters and groins were constructed along the coast inducing modifications in the sediment transport of the area indeed breakwaters induced a considerable accumulation of sediment over the years in the area protected by the cliffs leading to the formation of typical tombolos while groins trapped sand on their north side but caused beach erosion on the south side of the structures due to the prevailing direction of the solid longitudinal transport from north to south the wave data collected in the period 1989 2012 by the ron italian data buoy network franco and archetti 1993 franco 1995 archetti et al 2016 buoy placed offshore monopoli around 70 km north the study site are analytically transposed offshore the study site according to the geographic fetch based approach contini and de girolamo 1998 therefore the reconstructed typical wave climate at cerano is found to be characterized by nnw and ese storms mainly generated by mistral cold and dry wind usually blowing during the winter and by sirocco warm and humid wind generally inducing high storm surge respectively in the study site the net annual longshore sediment transport is directed from nw to se according to the italian atlas of beaches 1997 while sediment exchange with nearby coastal areas is limited constrained by the headland located at the north i e south of brindisi delle rose 2015 covering an area of about 270 ha and with a total installed capacity of 2640 mw the thermoelectric coal power plant federico ii is one of the largest in europe with an electrical mean efficiency estimated to be of 34 78 in 2014 environmental declaration 2015 seawater is withdrawn at around 300 m from the shoreline and at a water depth of 5 m with an inlet system of four pipelines heat exchangers are cooled by this sea water transported by a single pump of 1 000 m3 h capacity and at the end of the production cycle the cooling fluid comes back to sea thought a discharge channel with an outlet width of 80 m reaching a total estimated outflow of 3 bn tons of water per year corresponding to an annual average rate of 100 m3 s environmental declaration 2015 based on the plant s design conditions the maximum temperature difference between inflow and outflow waters is therefore estimated to be set to 12 c 3 2 field data instantaneous values of conductivity temperature and density ctd have been provided by the environmental agency of the apulia ron arpa regarding a monitoring campaign carried out on the day 17 july 2002 during this campaign ctd profiles were obtained at 23 points in the sea offshore the cerano power plant as presented in fig 2 the geographical coordinates east north of the measurement points together to the values of the depth average temperature and standard deviation are listed in table 1 due to the spatial distribution of the measurement points the dataset is divided into 5 categories reported in the last column of the table in relation to the distance from the outlet channel and the shoreline classes are channel including the 3 points located inside outlet channel close to the channel 1 km including points at a distance less than 1 km from the channel far from the channel in the nearshore north and far from the channel in the nearshore south including points close to the shoreline water depth 5 m and located at north and south of the channel respectively and finally far from the channel towards the offshore including points offshore water depth 5 m waves were acquired from ron at the monopoli buoy and propagated at the offshore boundary of study site wind time series have been retrieved from the italian national tide gauge network rmn while the global database tpxo egbert and erofeeva 2002 was used for tides 3 3 numerical modelling approaching the coastal ron waves generated offshore are influenced by shoaling refraction and loss of energy either due to bottom friction and wave breaking buccino et al 2014 cavaleri et al 2018 bonaldo et al 2019 to simulate all these physical processes including wave induced currents the present study of thermal discharge dispersion to the coastal area of cerano is carried out using the numerical modules of the telemac mascaret suite available at telemac 2019 that is distributed under a general public license the suite comprises finite element based solvers to simulate shallow water hydrodynamics and wave propagation and is able to model inshore water levels and wave spectra under different drivers the different modules comprised in the suite can simulate wind wave propagation ground water flows tracer transport sediment transport and morphodynamics the wave and 3d hydrodynamics modules of telemac mascaret suite are tomawac and telemac3d respectively and they were implemented in the proposed approach in order to propagate offshore waves and currents and reproduce nearshore dynamics influencing the thermal release at cerano tomawac module henceforth denoted as tom benoit et al 1996 solves a simplified equation for the spectro angular density of wave action by means of a finite element type method booij et al 1999 in order to describe wave propagation and dynamics in coastal areas this module is properly set up for the studied area based on previous experience on coupled wave 2d hydrodynamics runs for the representation of nearshore processes as presented in samaras et al 2016 gaeta et al 2016 and gaeta et al 2018 the processes included in the wave model simulations are i energy dissipation due to wave breaking according to battjes and janssen 1978 ii energy dissipation due to bottom friction according to hasselmann et al 1973 and iii nonlinear transfer of energy due to triad three wave interactions according to eldeberky and battjes 1995 no movable seabed no defense breaching and no past subsidence induced movements were assumed in the study the 3 d navier stokes equations are solved in telemac 3d henceforth denoted as tel3d hervouet 2007 with the option of the non hydrostatic pressure hypothesis and includes i the use of a finite element unstructured grid which allows selective refinement of the mesh at key locations in the domain and boundary fitting method for vertical discretization ii the transport diffusion equations of intrinsic quantities temperature salinity concentration in order to reproduce 3 d hydrodynamics including the trans port of active and passive tracers iii a wide range of options for vertical turbulence modelling the governing equation of the tracer transport is reported in equation 1 as 1 œÅ t t u œÅ t x v œÅ t y w œÅ t z u x œÅ t x u y œÅ t y u z œÅ t z where t and œÅ represent the water temperature and the water density respectively u v and w are the water velocity components and ux uy and uz are the turbulent thermal diffusivity components along the x y and z the spatial coordinate system respectively t is time the latest release of telemac version 7 0 includes the implementation in tel3d of thermal exchange fluxes between sea and atmosphere including typical processes of net solar radiation cooper 1969 long wave radiation berliand and berliand 1952 sensible heat rosati and miyakoda 1988 and latent heat due to evaporation panin and brezgunov 2007 tel3d can be directly coupled two way coupling with the spectral module tom on the same computational mesh in order to reproduce the dynamics of wave driven currents the gradients of the radiation stress induced by waves are computed using the theory of longuet higgins and steward 1964 as part of the hydrodynamics equations the updated values of current velocities and water depths calculated in tel3d are transferred to tom while tom solves the wave action density conservation equation and returns the updated values of the wave driving forces acting on the current to the hydrodynamics modules hervouet 2007 the initialization strategy of the implemented high resolution model followed the methodology described in federico et al 2017 where initial condition fields on temperature were provided by the mediterranean forecasting system mfs produced by data assimilation which supplies operational forecasting products in the framework of e u copernicus marine service information http marine copernicus eu cmems 2019 fig 3 shows the sea temperature field at day 17 july 2002 as obtained by mfs and the profiles at 4 selected nodes the ones falling into the study area and therefore used to set initial conditions of the present implemented model according to the experience and validation shown at large and coastal scale cucco et al 2012 trotta et al 2016 and at harbor scale gaeta et al 2016 a spin up time of 3 days was considered to be a reasonable choice in order to ensure the development of internal dynamics by the nested model therefore simulations were carried out for the period between 14 and 17 july 2002 that is the day when measurements were collected interpolating the oceanic fields over the new higher resolution grid is solved following the procedure suggested in de dominicis et al 2013 and validated in samaras et al 2014 although the local effects on sea temperature and hydrodynamics field induced by the cooling discharge from the plant are not captured in the mfs model due both to its large mesh resolution 7 7 km2 and the absence of any imposed liquid conditions at the shoreline boundary no run off and release into coastal waters details of the drivers used for the model runs are listed in table 2 describing the initial conditions ic offshore boundary conditions obc surface conditions sc and the imposed conditions at the channel outlet of the modelled variables in tel3d and in tom following the implemented multiple nesting procedure gaeta et al 2016 in the model finite element techniques were used to solve the hydrodynamic equations adopting the z vertical discretization to follow the surface and lower boundaries the multidimensional upwind residual distribution method was applied for the advection of three dimensional variables under tel3d and the boundary conditions were applied following the method of characteristics the advection of tracers was solved using the distributive murd psi method and the prescription of tracers at the open channel was applied using imposition by the dirichlet boundary condition while a neumann type condition at the offshore was prescribed imposing a zero gradient of temperature the mesh for the implementation of the models in this work was generated using the freely available pre processing tool blue kenue chc 2010 a series of tests proceeded the final mesh generation in order to evaluate optimal mesh edge dimension in terms of both representing the processes of interest satisfactorily and keeping computational times to reasonable levels the selected variable density unstructured mesh see fig 4 consisted of two density rons a coarse one 100 m edge length extending from the offshore boundary and up to the plant outflow area and a fine one 10 m edge length in the immediate vicinity of the plant outlet and the nearby area aiming to better capture the mixing dynamics following the thermal release the number of nodes in the mesh equals to around 14 000 the used bathymetric and shoreline data were obtained by digitizing nautical charts acquired from the italian national hydrographic military service fig 4 the resulting map clearly indicates a gradually changing bathymetric pattern moving offshore in particular depths in the range of 2 3 m right in front of the shoreline remain respectively low up to 500 m to the offshore followed by the central mesh area which is characterized by depths ranging from 12 m to 18 m eventually transitioning to depth reaching around 30 m near the offshore boundary of the modelled ron i e the boundary where waves are generated as well 4 model calibration and discussion 4 1 sensitivity analysis on hydrodynamics drivers a sensitivity analysis was performed on hydrodynamics drivers included in the simulations in order to define which of them mostly influenced models performance as being sensitive to the variation of the simulated processes the analysis was carried out at the 7 points of the arpa apulia measuring campaign namely f2 f3 m1 close to the outlet channel x2 w6 far from the channel in the nearshore north and south respectively w3 and f4 far from the channel towards the offshore these points were selected among the measurements as they are representative of the defined location groups table 1 the analysis was applied to different drivers included in the simulations as listed in table 3 accounting for wave propagation w tidal elevation t and wind forcing u for a total of 8 performed runs to investigate the model performance the root mean square error rmse was evaluated allowing a comparison between the measured and the simulated temperature values these parameters were calculated by using the expression 2 r m s e 1 n t s t m 2 n where n is the total number of the measured points table 1 along the collected vertical profiles the index s is for the simulated value and m for the measured ctd data for each of the performed simulations default values as defined by hervouet 2007 samaras et al 2016 and gaeta et al 2016 were kept constant as well as the number of horizontal layers equal to 5 fig 5 presents the results of the sensitivity analysis in terms of rmse for temperature values the influence of the simulated processes strongly depends on the investigated location s distance from thermal discharge release point with minimum errors resulting when all drivers were included in the run i e w1t1u1 in particular points located close the channel f2 m1 present the highest errors ranging between 2 and 5 when waves and wind were excluded by the simulations that were reported as crucial drivers to simulate the thermal diffusion in nearshore areas therefore coupling wave and wind modelling despite rising the computational time by a factor 10 improves the quality of the results and might be accounted for in these complex coastal processes 4 2 sensitivity analysis on governing equations parameters a significant array of simulations was run in order to get a proper calibration of the models used in this work on the basis of the available temperature profiles provided by arpa apulia by means of investigating the influence of the tel3d parameters in order to reach the better agreement between data and simulations for the local sensitivity analysis the performed simulations were carried out using the one at the time approach simmons et al 2015 by increasing each parameter by a given percentage while leaving all others constant and quantifying the change in model output the parameters identified for the analysis characterize the processes involved in coastal thermal pollution such as the number of the horizontal layers the friction bottom coefficient the horizontal and vertical turbulence models the diffusion coefficients for velocity the horizontal diffusion for temperature and wind drag coefficient all the other parameters mainly regarding wave propagation and wave current interactions were kept equal to the calibrated values as defined by hervouet 2007 samaras et al 2016 and gaeta et al 2016 table 4 shows the list of analysed parameters their tested range and their corresponding final values adopted for the calibrated model runs for this analysis the physical processes by coupled wave 3d hydrodynamics modelling including wind influence were simulated according to the results shown in the global sensitivity analysis presented in the previous section results of the 45 runs were summarized for the 7 selected measurement locations in fig 6 where each panel shows the differences in depth average temperature between measurements and model results for each of the 9 investigated parameters in fig 6 red lines represent mean values top and bottom box edges represent the 75th and 25th percentiles line limits represent maximum and minimum values and red crosses indicate outliers the results show that the parameters that mainly influence the model quality thus improving the numerical reproduction of the thermal mixing processes are the choice of the vertical and horizontal turbulence models kv kh the vertical diffusion coefficient uz and the wind drag coefficient cd the model sensitivity to the adopted turbulence models and velocity diffusivity appears to decrease with the increase of distance from the shoreline and from the release channel points f2 f3 m1 x2 and w6 present the greatest variance in the agreement with measured temperatures the influence of the drag coefficient for wind appears on the other hand to be spatially uniform ranging from 0 2 to 0 1 following the analysis on the parameters the model can be assessed as calibrated with reference to the thermal dynamics and climate of the study area 4 3 discussion on the numerical results from the calibrated model results of the final calibrated model as obtained after the sensitivity analysis i e run w1t1u1 are presented in fig 7 where temperature profiles at the 7 reference points analysed in the previous sub sections were extracted from the numerical simulation and compared to the measurements although some uncertainties are present in the present numerical study about initial and boundary conditions implemented in the simulations the calibration results in a satisfactory agreement to data especially considering the complexity of the studied phenomenon as well as the equally complex dynamics of the various processes interactions involved in its numerical simulation in particular regarding the depth average temperature the following convergence results are observed maximum deviation from data reaching 2 c in just 2 of the examined locations deviation less than 1 c for 19 points maximum deviation of 0 2 c achieved for 6 out of the 23 examined points the greater difference is observed at points located offshore i e w3 and f4 where the low temperatures around 25 c are not captured well by the model overestimating them for water depths 3 m this can be attributed to the initial conditions used as temperatures extracted by mfs are overall bigger than 26 c see fig 3 the temperature increment Œ¥t due to the power plant release with respect to the ambient values is shown in fig 8 where the evolution of the plume development as discretized at the 23 points of the arpa apulia campaign is shown the model results and the measured values at a water depth equal to 1 m the overall diffusion of the thermal release is found to be well reproduced by the calibrated model where the computed mixing area is slightly greater extending to the southern direction revealing the flow directionality driven by wind and waves is satisfactory predicted in the simulation in accordance to the previous fig 9 shows the computed thermal plume in the studied coastal area where the sea temperature at the end of the simulation reveals the prevalent dispersion of the cooling waters towards sse and mainly along the southern shoreline arrows represent the velocity vectors the prevalent longshore current is driven by wind and waves and directed from north to south with speed reaching up to about 0 20 m s and that strongly affects the direction of the thermal plume both the presence of the two groins delimiting the outlet channel and the outflow speed of the cooling waters induce a perturbation in the actual current patterns resulting in the development of i a seaward directed plume with a velocity of about 1 m s and ii eddies downward the channel eventually contributing to the observed shoreline retreat in this specific area under the simulated conditions the directionality developed by the thermal plume is desirable in terms of power plant efficiency performance as the cooling waters move away from the morphologically constrained area north of the channel see also fig 1 which could lead to its entrapment and consequent increment in local temperature affecting both the water temperature at the intake i e reducing the plant efficiency and the acceptable thermal increment imposed by the national regulation fig 10 presents the sea temperature along the 3 selected planes namely s1 s2 and s3 located at the vicinity of the release channel as shown in fig 9 for a water depth of around 6 m section s2 in the middle panel up to the shoreline for sections s1 and s3 left and right panels respectively the presence of the cooling waters is clearly observed at section s2 that is located just in front of the outlet at a distance of around 100 m there the vertical gradient of the temperature slightly develops probably due to the low wave intensity occurring during the simulated period the horizontal dispersion of the thermal plume is observed as well with temperature decreasing up of 6 c along a 500 m long stretch results along section s3 instead reveal high temperatures of around 30 c at shallow waters from depth of 1 m up to shoreline persisting along the 1 km long shoreline at the southeast of the outlet as also represented in fig 9 5 numerical experiments under different environmental and operational scenarios after models sensitivity analyses and calibration a set of scenarios for different environmental and operational conditions were setup so that a proper assessment of the cooling water release from the cerano power plant could contribute towards i implementing an effective monitoring program of the thermal plume and ii establishing mitigation measures for its eventual environmental effects the aforementioned set of scenarios was setup in order to represent expected seasonal and industrial events in order to test them against the reference scenario for the real on site conditions described in section 4 this set as listed in table 5 comprised sc1 representing the worst metocean conditions for dispersion meaning no wind and therefore no waves while maintaining the same operational features with the reference scenario sc2 representing the maximum power production conditions of the plant reference to data during the day 17 july 2002 with cooling waters at the outlet with a temperature of 35 c and a temperature increment of 12 c with respect to the ambient temperature therefore set to be equal to 23 c while maintaining the metocean reference conditions sc3 representing sirocco conditions i e overturning the wind and wave direction while maintaining the same operational features with the reference scenario fig 11 illustrates the computed sea temperature field at a water depth of 1 m at the end of each numerical experiment carried out for the period 14 17 july 2002 as in the reference scenario for scenario sc1 fig 11a the absence of any metocean forcings tends to make the thermal plume stay in the area the temperature spread towards the south being lower than in the reference scenario see fig 9 for comparison furthermore the plume is limited to the nearshore spreading out in a similar way towards the areas located north and south of the outlet consequently affecting the former one which was not polluted in the reference scenario scenario sc2 see fig 11b is characterized by a thermal discharge with a temperature increment of 12 c with respect to the ambient temperature and cooling waters at the outlet with a temperature of 35 c in comparison with the reference scenario see fig 9 sc2 results reveal a similar plume distribution pattern although as expected a wider zone of influence was present expanding northwards of the outlet as well where impact was minimal in the reference scenario in sc3 fig 11c the prevalent direction of the thermal plume propagation reverses from south southeast to north northwest although evidence of spreading is observed north of the outlet as well the northern area also mentioned in subsection 4 3 constrains plume propagation and leads to thermal amassing in the area in this scenario the efficiency of the power plant is expected to be significantly affected due to the dual effect of the water intake s location being in the area and the environmental implications of the temperature difference increase following the analysis by dur√°n colmenares et al 2016 the spatial decay of the depth average temperature with respect to the ambient condition t0 was estimated along a cross shore section starting at the outlet and extending up to 1500 m offshore for all the 3 scenarios and the reference simulation i e on the day 17 july 2002 the values of the decay presented in fig 12 were normalized with the maximum temperature value tmax at the outlet changing with the simulated scenarios along the analysed cross shore section for the reference sc1 and sc3 scenarios the percentage of dissipated temperature decreases up to 50 at a distance of around 300 m from the outlet and arrives to be around null within 1000 1500 and 1600 m respectively the change of the metocean forcings in sc1 null drivers and sc3 overturning drivers is primarily responsible for the lower heat dispersion offshore in comparison to the reference conditions while closer to the shoreline the sea temperature increment remains fairly unaltered the variation of the industrial conditions in sc2 instead has the main influence on heat dispersion into the sea reducing the decay rate of the temperature towards the offshore the areas of influence for all scenarios table 6 were calculated for each simulation and per range of temperature increment referring to the computed sea surface temperature the values were normalized with the total volume of hot waters released by the power plant since the industrial conditions varied between scenarios considering the geographical extent of the implemented numerical domain fig 4 wind from the north allows the greater rates of heat dispersion mainly occurring towards the south fig 11 and giving up to 50 for a temperature increment less than 1 c while still persisting the same metocean conditions in sc2 areas of influence are greater from 1 5 for higher values greater than 8 c up to 69 for lower values of temperature greater than 1 c the maximum area of influence for lower temperature increments is then obtained for scenario sc1 97 i e in absence of any metocean forcing and for scenario sc3 94 i e for the south wind the case where the cooling waters would have difficulties to spread offshore and outside the numerical domain due to the northern coastal morphology cove like formation of the area that leads to an eventual thermal amassing 6 conclusions this work presents the setup sensitivity analysis calibration validation and implementation of a coupled wave 3d hydrodynamics model for the investigation of thermal discharge dynamics in coastal areas the model setup is based on the respective spectral and 3d hydrodynamics modules of the open source telemac suite the implemented case regards a thermal power plant in south italy which uses seawater intake for cooling purposes and releases hot water back to the sea through a coastal outlet after an extended sensitivity analysis the coupled model was calibrated by using the sea temperature data collected during the field campaign of arpa apulia in the area sensitivity analyses highlighted the importance for model performance in the representation of thermal discharge to coastal areas of i wave wind combination regarding model drivers and ii vertical horizontal turbulence models vertical diffusion coefficient and wind drag coefficient regarding governing equations parameters model calibration validation is deemed to have been successful managing to capture satisfactorily both the horizontal and vertical distribution of water temperature variations the final runs of the calibrated model regarded scenarios of environmental and operational conditions in order to investigate representative states of the specific coastal system in terms of both environmental issues i e temperature increment in the nearshore area and plant efficiency issues i e performance related to inflow temperatures the 3 simulated scenarios revealed the range of expected changes in the thermal plume s distribution for representative conditions in the field environmental operational providing useful insights on the specific case study s dynamics all in all the present work is among the relatively few in relevant literature that focuses directly on thermal discharge dynamics rather than studying it indirectly through its biological impacts and is deemed to constitute a useful basis for other studies on the topic either direct or indirect future versions of this work could be further improved by investigating case studies with more extensive field measurement datasets e g current wind data and or coupling the wave and 3d hydrodynamics modules with the water quality module of telemac available in the suite s last release in order to extend this approach s applicability nonetheless the systematic literature review analysis of the studied phenomenon and modelling approach followed in this work contribute to setting the framework for the direct numerical simulation of thermal discharges to coastal areas while also setting the bases for the specific case study s evaluation and environmental technical assessment acknowledgments the authors would like to thank mr ungaro from arpa apulia for proving the ctd data of the field campaign in 2002 the research was partially funded within the framework of the italian flagship project tessa development of technologies for the situational sea awareness supported by the pon01 02823 2 ricerca competitivit√† 2007 2013 program of the italian ministry for education university and research appendix a regulations in eu countries for thermal release in coastal area the principal eu directives such as water framework habitats and marine strategy framework directives do not require a trans european thermal standard therefore each eu country developed its own regulation british energy estuarine studies 2011 regarding environmental standards for effluent temperatures in estuarine coastal and marine systems and in particular their allowed temperature values was here listed and reviewed in france on the gironde estuary the temperature increment must be not greater than 11 c at the outflow point with a maximum temperature of 30 c with a special permission it can reach up to 36 5 c in summer in norway the temperature increment must be not exceeding 10 c at the outflow point with no more than 1 c of temperature increment in the mixing zone with some flexibilities up to 3 c at particular sites in spain the temperature increment must not exceed 3 c or 1 c as integrated throughout the water column at a distance of 50 m or more from the discharge point in germany maximum limit for cooling water discharges is set at 30 c and the temperature increment must be not exceeding 10 c for existing sites and 7 c for new plants in the netherlands discharges into marine waters must not exceed 30 c within a mixing zone bounded by the 25 c isotherm in italy the maximum water temperature at the outlet must not be higher than 35 c and the maximum temperature increment in the coastal field at a distance of 1 km from the outlet must be lower than 3 c 
26086,surface mining irreversibly disturbs the landscape a first order priority is to establish an erosionally stable landscape soil and surface material particle size has a strong influence on soil erosion and a number of erosion models have been developed based on the relationship with particle size here we highlight the practicalities of assessing material particle size for a post mining landscape in particular the caesar lisflood landscape evolution model the focus here requires a defined material particle size as input a key feature which differentiates caesar lisflood is the ability to apply particle size data at the same resolution as the digital elevation model dem representing the landform surface here we develop particle size distributions and demonstrate how they influence erosion for a potential post mining landform field data from the site demonstrates that material particle size distribution changes little over a ten year period and yet a strong influence on erosion rates keywords soil particle size sediment transport landscape evolution mine rehabilitation soil erosion modelling 1 introduction it is well recognised that soil and surface material i e in the study here mine waste material particle size has a large role in soil water holding capacity chemical properties and retention of nutrients therefore greatly influencing plant growth potential hazelton and murphy 2007 material particle size also has a strong influence on soil erosion many researchers have examined the role that soil particle size has on erosion and recognised that statistical relationships can be determined the best example of this is the revised universal soil loss equation rusle k or soil erodibility value which is largely determined based on material particle size wischmeier and smith 1978 in the rusle erodibility is further modified by a cover or c factor which can include an armour component based on rock percentage cover and particle size a number of fluvial sediment transport models have been developed based on the relationship with particle size wilcock and crowe 2003 the seminal work of shields 1936 and hjulstr√∂m 1939 clearly demonstrates how different shear stress and flow velocities are required to entrain the different sediment sizes and then transport it in general for soils with the same chemical properties and composition a soil with a coarse distribution will erode less than one with a fine size distribution this is because a coarse soil will 1 have a higher infiltration rate due to pore spaces between the larger particles that therefore generate less runoff and 2 require a higher shear stress to entrain them which may be due to higher velocities and or larger flow depths hjulstr√∂m 1939 in reality however this is not always the case as finer soils can retain more water and nutrients and therefore can develop a vegetation cover which can reduce erosion the relationship between sediment transport at the hillslope scale is recognised to be complex and non linear hjulstr√∂m 1939 wischmeier and smith 1978 wilcock and crowe 2003 tucker and hancock 2010 bussi et al 2014 juez et al 2018 willgoose 2018 further complexity is added by the soil catena it is well understood that for natural hillslopes soils are generally more coarse and shallow on the ridge crest with soil generally deepening and becoming more fine i e higher silt and clay content towards and at the base of the hillslope birkeland 1984 charman and murphy 2007 the hillslope changes from a zone of erosion to a zone of deposition while numerical relationships on soil particle size and how they vary down a hillslope are scarce in the literature the soil catena concept is generally accepted for fluvially dominated systems by the earth science community however in anthropogenic and disturbed landscape systems such as post mining landscapes this idea of soil catena with some structure is often absent 1 1 the mining process and soil organisation it is well known that natural soils have a vertical structure which are organised into horizons which are recognised to have very different physical and chemical properties hazelton and murphy 2007 charman and murphy 2007 this organisation is a result of physical and chemical weathering illuviation of weathered material from above as well as the slow turnover of soil by bioturbation for disturbed and or rehabilitated sites such as post mining landforms this concept of a vertically and horizontally ordered soil system or structure does not exist as the mining process destroys any previous soil or geological organisation and the material may have come from well below the ground surface the mining process while conceptually simple i e non economic material is removed to recover economic material is in reality very complex as large volumes of soil and rock require removal in an ordered process to access the economic ore the mining process has a large impact on material particle size post mining landscapes are constructed of what is termed waste rock from the pit in most cases this material has been initially dislodged using explosives explosives break up what is termed overburden that is the material overlying the mineral of interest or interburden material that is within the seams of the mineral of interest the blasting process uses sufficient energy to break up the host rock so that it can be easily removed by earthmoving equipment too little blast energy and large rocks result which are difficult for equipment to remove too much energy and a fine material results which can be difficult to handle produces dust i e air pollution as well as fly rock that is flying rock that travels hundreds of metres away from the blast site the amount of explosive used and the resultant blast energy has a large impact on the size of the waste rock particles the over and inter burden can have vastly different physical size as well as chemical properties depending on blast energy or where in the pit the material is removed from the particle size is further changed in transport and placement and the shaping of this material by earthmoving equipment this results in very different particle size distributions of this waste material for example at coal mines waste rock can be directly removed by a large excavator termed a dragline and placed directly in a waste rock dump however for most metalliferous or other mines in the case here uranium the waste rock is removed by an excavator and placed in a truck which transports the waste to a waste rock dump where it is dumped in a series of piles in a process called end dumping these piles whether they have been placed there by dragline or truck are then shaped by a bulldozer according to some predetermined landscape design in many cases the new landscape is constructed as a series of lifts usually about 10 m high as the waste rock is reshaped in layers this material delivery and earthmoving process results in many passes of heavy earthmoving equipment wheels and tracks which pounds and compacts the surface resulting in further breakdown of the waste rock as well as the potential for compaction which can impede water movement the transporting dumping and reshaping with a bulldozer continues to change the material particle size distribution at some mine sites topsoil that has been removed and stockpiled is spread on the reshaped landform and used as a growth medium in other cases a subsoil or weathered waste rock i e lateritic material can be used as a topsoil such weathered material offers the benefits of having a much finer particle size distribution than waste rock and therefore potentially improved water holding capacity the addition of subsoil material can also increase the volume of topsoil if it is required for plant establishment this requires identification of the material pre mining and a thorough understanding of its physical and chemical properties a further complication is that the new surface materials can weather and change their physical and chemical characteristics what the weathering process is what rate it proceeds at together with what the physical and chemical weathering products are will be site specific there is a near complete absence of this type of information for post mining landscapes wells et al 2006 2007 2008 depending on the amount of soil available and the suitability of the waste rock as plant growth medium the depth of spread soil can range from no soil needed to tens of centimetres some mines such as the ranger mine in the northern territory of australia have no topsoil and are therefore not able to spread additional material on the waste rock in many cases both with and without topsoil this surface is then ripped by a bulldozer pulling a large tyne to produce roughness reduce runoff velocity as well as increase infiltration in order to reduce erosion this roughness can also act as a sediment and seed trap if the surface is to be revegetated by seeding this ripping process also mixes underlying waste rock including large boulders with the surface material therefore the construction of a post mining landscape results in a highly heterogeneous landscape consisting of a range of different materials with different particle size distributions with no order that can be predetermined according to our understandings of the natural environment in effect post mining landscapes are new systems constructed of recycled earth materials that may have been pulverised and compacted with no vertical or horizontal particle size structure 1 2 predicting post mining landscape soil erosion and landscape evolution past erosion models such as the rusle have operated at the hillslope and or slope segment scale using lumped parameters in recent years a number of soil erosion and landscape evolution models lems have been developed see tucker and hancock 2010 for a review the models use a digital elevation model dem to represent the landscape and directly calculate an erosion rate based on runoff and the material particle size distribution in particular the caesar lisflood coulthard et al 2013 model has the ability to input particle size for every cell in the dem which may represent different surface cover types in addition the most recent version of caesar lisflood has the ability to represent the particle size characteristics of a sub surface layer caesar is the only lem at present with this capability the siberia lem willgoose et al 1991 has a similar capability where it can spatially distribute erodibility at each dem node hancock et al 2008 the model can then route move sediment from node to node based on flow direction and shear stress required to move the particle flow and velocity of flow is determined based on the upslope contributing area at each point determined from the dem the material particle size is input by the user based on field data where the reliability of the material particle size data is very important for correctly modelling landscape behaviour soil particle or surface material particle size and distribution affects erosion rates in lems hirano 1971 proposed the concept of an active layer where there exists a conservation of mass between sediment on the landscape surface and the sediment that is being transported it is recognised that the thickness of the active layer governs the competition between bed level change and the change in surface particle distribution juez et al 2016 hancock et al 2010 and hancock and coulthard 2012 demonstrated that different particle size distributions influence erosion rates for an agricultural catchment in south east australia they showed that soil particle size distribution had a large impact on erosion rate and type of erosion hancock and coulthard 2012 also demonstrated that different soil particle size distributions can control the position and movement of channels under different rainfall events juez et al 2018 hancock et al 2010 demonstrated that using different field measured particle size distributions for a natural catchment in northern australia produced erosion rates that could vary by a factor of 2 3 the outcome of these modelling exercises demonstrated that a good understanding of the surface material characteristics based on field data is required an important issue not examined was that of the active layer where erosion and deposition occurs if the active layer is too shallow the adjustment of surface grain size distribution is fast and the bed level changes will be constrained by the armouring of the surface a too thick layer may produce excess erosion the caesar lisflood model has the capability to employ an active layer depth across a possible nine size ranges and further described in section 3 1 here we highlight the complexity of post mining landscapes and their surface material properties the questions examined here are given the heterogeneity of a post mining landscape with a coarser particle size than the surrounding natural surface how does particle size influence erosion furthermore given the likely wide range of particle size and surface materials what is the optimal size grading or range to be employed in a lem to model a rehabilitated landform and surrounds can we reliably model soil erosion and landscape evolution of both natural and post mining landscapes we demonstrate here how a lem can be used to better understand disturbed systems such as post mining landscapes 2 site description a proposed rehabilitated landform at the ranger uranium mine is used here as a test case to examine the effects of different material particle size ranges from mined waste rock material and surrounding natural surfaces the rehabilitation requirements for the ranger mine state with respect to erosion and landform stability that the landform should possess erosion characteristics which as far as can reasonably be achieved do not vary significantly from those of comparable landforms in surrounding undisturbed areas australian government 1999 this requires that the disturbed landscape be rehabilitated in a way that restores environmental functions such as the sediment water and nutrient balance supporting local ecosystem diversity these environmental functions include landscape water nutrient and energy balance which can be represented through geomorphic properties such as relief ludwig and tongway 1995 an erosionally stable landform is the first part of this process the ranger mine is located in the tropical north of australia approximately 250 km east of darwin it is in the wet dry tropics of northern australia and receives high intensity storms and tropical monsoons between october and april the wet season with little rain falling for the remainder of the year the dry season fig 1 the annual average rainfall is 1584 mm with high interannual variability ranging between 1038 mm yr to 2623 mm yr www bom gov au the geology of the site consists of igneous rocks and mineralised metasediments of the pine creek geosyncline and the younger mamadawerre formation sandstones east 1996 needham 1988 the area is one of the richest uranium areas globally mineral lease geology consists of a superficial cover of cainozoic soil unconsolidated sands ferruginous material and laterite over lower proterozoic schists of the cahill formation geomorphically the site is characterised as the koolpinyah surface which comprises plains broad valleys and low slopes with isolated ridges and hills of resistant rock east 1996 previous research in the alligator rivers region duggan 1994 has shown that sediment yields decline progressively over at least the first three years following a major surface disturbance as a result of initial washout of fine sediment and the subsequent formation of a gravel armoured surface hancock et al 2016 2017 erosion rates for the koolpinyah surface have been determined by a variety of methods over a number of years cull et al 1992 and erskine and saynor 2000 suggest regional denudation rates of between 0 01 and 0 04 mm yr hancock and lowry 2015 have determined erosion rates of 0 037 0 18 mm yr for a local analogue area tin camp creek a trial landform has been constructed at the site to inform landform design and ecosystem restoration efforts saynor et al 2012 hancock et al 2016 2017 fig 2 this landform of approximately 200 m by 400 m 8 ha was constructed during late 2008 and early 2009 by the mine operator energy resources australia era adjacent to the north western wall of the tailings storage facility at the ranger mine fig 1 four erosion plots 30 m by 30 m were constructed on the trial landform with a maximum slope of 2 during the 2009 dry season with material characteristics representative of what may occur on the reconstructed landform from this trial landform site a series of material particle size ranges were determined for waste rock material and are described in section 4 below fig 2 at the completion of mining and processing of the ore the pits from which the ore was extracted will be filled and the exhumed waste rock reshaped into a new landform a proposed design for one catchment within the mine corridor creek is described below section 3 2 and fig 2 the landform will be vegetated by seeding and potentially hand planted tubestock however there is no stockpiled topsoil to assist with the establishment of this vegetation 3 landscape evolution models while developed for geological time scale applications in recent years landscape evolution models lems have been employed in applied settings i e mine sites at considerably shorter time scales years decades centuries than geological time scales testing of these models across a range of environments has shown that they are generally reliable when employed using appropriate input parameters tucker and hancock 2010 it should be recognised that there are several caveats the landscape representation i e the digital elevation model has to correctly represent the landscape and capture hillslope curvature as well as features such as contour banks hancock 2005 model input parameters require a detailed knowledge of the site and are largely derived from site specific field data and have a large impact on any model outputs evans 2000 hancock et al 2000 2002 2006 model sensitivity has been evaluated mostly by assessing different landforms and differences in elevations willgoose et al 2003 however the models lend themselves to a full monte carlo type assessment with the only limitation being input parameter knowledge and reliability and computing resources willgoose 2005 further assessments have been conducted using comparison with erosion rates determined by environmental tracers such as 137cs hancock et al 2008 and gully position and morphology hancock et al 2000 decadal time scale assessment as well as at annual time scales using field erosion plots lowry et al 2011 2013 saynor et al 2012 centennial to millennial time scale assessments have been conducted by comparing one lem siberia willgoose et al 1991 against another caesar coulthard et al 2013 martinez et al 2009 hancock et al 2010 2011 2015 landform evolution modelling to assess a post mining rehabilitated landform design was first conducted by willgoose and riley 1993 using the siberia model willgoose et al 1991 since 1993 the model has been extensively used to assess erosion and evolution of post mining rehabilitated landforms or small catchments i e willgoose and riley 1998 evans et al 1998 hancock et al 2000 2002 moliere et al 2002 further it has been employed at the nabarlek uranium mine following an initial assessment using the empirically based rusle hancock et al 2006 2008 while a very reliable and useful model to predict erosion fluxes the rusle wischmeier and smith 1978 does not evolve a landscape surface in response to erosion and deposition as do lems a new development is the physically based caesar and subsequent caesar lisflood lem coulthard et al 2013 these models have been used to assess rehabilitation designs on mine sites coulthard et al 2012 lowry et al 2011 2013 the advantage of caesar lisflood is that it operates at a much finer temporal resolution hourly than the siberia or rusle models yearly this paper employs the caesar lisflood lem 3 1 the caesar lisflood landscape and erosion model caesar lisflood is a physically based landscape and river reach evolution model that evolved from the caesar model of coulthard et al 2002 caesar lisflood is an improvement on previous versions of caesar as it included a hydrodynamic 2d flow model based on the lisflood fp code of bates et al 2010 a full description of caesar lisflood is given in coulthard et al 2013 here we use version 1 8g the model simulates erosion and deposition in river catchments and reaches over time scales from hours to 1000 s of years caesar lisflood enables a user to input a dem and uses hourly rainfall data to drive a hydrological model based on topmodel beven and kirkby 1979 a significant advantage of caesar lisflood over other lems is that it is able to utilise rainfall at hourly time steps which allows different rainfall data sets and specific rainfall events to be modelled lisflood fp is a one dimensional inertial model derived from the full set of shallow water equations and is applied in the x and y directions to simulate two dimensional flow over the raster grid the model is second order in space and first order in time bates et al 2010 surface runoff is then routed using the lisflood fp code that calculates flow between cells based on water surface slope and a local inertia term by utilising one dimensional 1d flow models in both the x and y directions this then generates a two dimensional 2d flow field from which shear stress is calculated from flow velocity between cells and then used to calculate sediment entrainment via either the einstein 1950 or wilcock and crowe 2003 sediment transport rules bedload is routed according to the distribution of velocities and resultant shear from a cell while suspended sediment is routed with water flow and then deposited according to the settling velocity sediment transport is calculated over a range of up to nine grain sizes with an active layer defined by the user these are integrated within a user defined set active layers coulthard et al 2013 erosion transport and deposition of the grains will result in spatially variable sediment size distributions the management of this is carried out using a system of layers comprising an active layer representing the stream bed multiple buried layers strata a base layer and if required an immovable bedrock layer the layers have a fixed thickness and their position is fixed relative to the bedrock layer this allows the development of differential surface grainsize patterns and effects such as bed armouring and grainsize changes over depositional features such as bars and or fans coulthard et al 2012 2013 the model can also input up to five different particle size data sets for different surfaces over the study domain on a pixel by pixel basis while the proportions within each of the different particle size distributions can vary the ranges initially employed are fixed therefore if there are materials with different particle size distributions a compromise needs to be found regarding the size ranges to be employed caesar provides two different methods of calculating sediment transport based on the einstein 1950 and the wilcock and crowe 2003 equations the einstein 1950 approach was developed based on predominantly sand based laboratory channels whereas the wilcock and crowe 2003 formula on field and laboratory data from a coarser bed gravel sand mix we evaluate both models here a calculation of shear stress is required to drive each model and this is calculated from flow velocity the choice of size range is an important issue at this study site where the waste rock particle size is a very different particle size to that of the natural undisturbed surface the choice of the particle size is the focus of the work here 3 2 caesar lisflood input parameters the caesar lisflood model requires three key inputs a dem to represent topography long term rainfall data mm hr 1 and surface material particle size distribution data the focus of this study and described in section 4 here we use a proposed rehabilitation design for the corridor creek area of the ranger site the design is one option that may be used and it provides a realistic test case for any model and landscape assessment as it contains waste rock areas approximately two thirds of the catchment and natural areas approximately one third of the catchment a dem of a potential rehabilitated landform for the site was supplied by era fig 3 the original catchment topographic data for corridor creek was in the form of 2 m interval contours which was gridded to a horizontal spatial resolution of 10 m this 10 m grid size was determined to be the optimal resolution at which caesar lisflood could function as well as reliably represent hillslope geomorphology hancock 2005 the dem has been used for previous landscape assessment and has been described in detail elsewhere hancock et al 2016 a key feature of the caesar lisflood is the ability to model sediment output at the storm event time scale using rainfall data at hourly time steps as an input it is well recognised that temporally variable rainfall has a large effect on erosion rate and type juez et al 2018 here we use rainfall data from the nearest site 2 km distance with the longest rainfall record jabiru airport site number 014198 www bom gov au fig 1 pluviograph rainfall data 6 min was obtained from the australian bureau of meteorology bom here only years with complete annual pluviograph data was used from 1972 to 2011 producing a 23 year record years with missing data were excluded more recent data had incomplete and or missing data during this period the study area has been subjected to a range of rainfall events and seasonal variability which may be considered climatically representative this dataset representing 23 years of complete annual rainfall data was added end to end to produce a continuous 100 year record that could be used as input into the caesar lisflood model the caesar lisflood input parameters are listed in table 1 4 material particle size characterisation the rehabilitation of the ranger site will encompass both a disturbed surface with an extensive cover of freshly exhumed rock waste rock as well as significant areas of natural undisturbed areas koolpinyah surface these surface materials and their particle size distribution psd are described below 4 1 waste rock two different sets of waste rock psds were examined here one from the ranger trial landform tlf and a second based on historical data these are described below 4 1 1 trial landform the tlf fig 2 tests two types of potential surface material waste rock and waste rock blended with approximately 30 fine grained weathered horizon material lateritic material for their suitability as cap materials once the placement and shaping of the surface had been completed the surface was ripped by a bulldozer pulling a large tyne to create surface roughness initial detail regarding the present surface has been described elsewhere saynor and houghton 2011 in april 2009 sixteen surface samples between 5766 and 8461 g were collected from the tlf and particle size was determined by sieve analysis however this manual sampling precluded the collection and measurement of larger material fig 2 that was captured by the grid by numbers method described below employed in 2012 2014 and 2018 obtaining bulk samples from specific geomorphic environments is the accepted method of sampling surficial sediments kellerhals and bray 1971 this involves the collection of material from a predetermined volume within a specific geomorphic environment however there are potential problems with bulk sampling that must be recognised very large sample masses are required to obtain reproducible measures of the grain size distributions of samples containing individual large clasts or gravels de vries 1970 church and wolcott 1987 gale and hoare 1992 ferguson and paola 1997 to overcome some of the problems of collecting a large sized sample that would impact greatly on the surface of the plot an alternative solution was required wolman 1954 devised a grid by number surface sampling technique for gravel bars in rivers gravel count method slight variations of this technique have been recommended leopold 1970 kellerhals and bray 1971 and the grid by number method is discussed in erskine and turner 1998 erskine and turner 1998 recommended that the b axis diameter of at least 100 clasts should be measured to obtain a reliable grain size distribution the grid by number method has been adapted here for determining the surface grain size of the erosion plots to determine what clasts were measured on the erosion plots the following methodology was followed twenty 1 m by 1 m square quadrats were made from pvc pipe holes were drilled in the middle of each square and fishing line tied between each opposite side so that the fishing line intersected in the middle of the 1 m quadrat ten of the 1 m quadrats were laid on the tlf plots and another 10 laid next to the first row of the plots where the fishing line intersected a steel rule was used to measure the clast laid directly underneath the b axis of the clast was measured and if picked up the clast was replaced in its original position where the clast was less than 8 mm it was noted as less than 8 mm once the clasts under the 10 quadrats had been measured the quadrats were moved alongside the next row of quadrats this was continued until the opposite side of the erosion plot was reached and then the quadrats were moved down to the next 10 m this method was completed on all four erosion plots for 2012 2014 and 2018 table 2 shows the number of clasts measured where more than 5 of the sample was finer than 8 mm a bulk sample of the finer sediment was collected this bulk sample was dried and sieved and combined with the gravel count data using a weighting factor equivalent to its percentage surface exposure the samples were than averaged to give particle size for waste rock material table 3 fig 4 demonstrates that there has been little change in the psd over a ten year period further visual evidence also demonstrates that there has been little change particularly for the larger material fig 5 this suggests that at least over decadal time scales that there is little variation as a result of weathering and that a particle size determined early on in landscape development is robust over the short to medium term at this site 4 1 2 historic data a psd was developed from historic data for a range of different waste rock surfaces and at different times at the site evans 2000 evans et al 1998 moliere et al 2002 the psd was determined using sieve and hydrometer and represents an average of the different surfaces that were examined prior to the construction of the tlf described below the psd is considered to be a realistic and reliable data set as it encapsulates a range of historic surfaces and psds that are likely to be present on the rehabilitated surface as the data had been collected over several years with different surface treatments such as vegetation and fire as this psd has been used extensively for past assessments lowry et al 2011 2013 hancock et al 2015 it will also be examined here due to its prior use this data set is considered to be the standard distribution and for brevity is named wrd std table 4 4 2 natural koolpinyah surface as the site has extensive areas of undisturbed or natural surface fig 3 a psd for this surface was also examined the determination of psd for the natural or undisturbed areas koolpinyah surface has been well described by saynor et al 2009a b a summary is provided below soils in the gulungul creek catchment immediately to the west of ranger mine were sampled in 2006 as part of a large project looking at the impacts of cyclone monica which passed through the area in april 2006 saynor et al 2009a b detailed particle size analyses were undertaken on samples collected from woodland and open woodland vegetation communities that are indicative of the type of soils on the natural surrounding koolpinyah surface a hand soil auger was used to excavate soil to a depth until refusal or no retention the surface material of 0 1 m depth has had a particle size distribution determined on them using sieve and hydrometer methods gee and bauder 1986 an average of all samples is displayed in table 3 4 3 particle size distribution used for modelling the maximum number of particle size ranges allowable in caesar lisflood is restricted to nine each size range is user defined but has to be the same size range for the different surface types represented on the landform for many sites with a single soil or surface material such as the water rock at a mine site nine particle size ranges is adequate however here we require a psds that can represent the very different size ranges of the waste rock coarse dominated and koolpinyah material fine dominated displayed in table 3 to fit within the 9 size ranges as mentioned previously while the amount of material as a fraction can vary the psd range in caesar lisflood is fixed at the start of the model run it can be seen that the wrd and koolpinyah materials have very different psds table 3 here we are required to choose size ranges that cover both the waste rock and koolpinyah size ranges to fit into the 9 size ranges so that both surface materials can be represented within the model reliably table 4 displays the waste rock and koolpinyah psd from table 3 regrouped into the 9 size range classification as input to caesar lisflood and described in section 4 1 1 and 4 2 the groupings chosen are loosely based on the phi size scale a commonly used size scale in fluvial geomorphology to cover the size ranges of both the waste rock and koolpinyah distributions here the groupings are named wrd a wrd b and wrd c and kool a kool b and kool c for the koolpinyah materials table 4 the groupings are simply different options for model input to test model sensitivity another option of the caesar lisflood model is that while the model employs a maximum 9 range particle size distribution as input it does not require all 9 ranges to be input it can operate at its simplest with a single particle size this is important at the ranger site to enable the modelling of the armoured drainage lines that will have a coarse but narrow size range to assess the sensitivity of the model to a reduced particle size data input the wrd and koolpinyah particle size was compressed to a 6 range distribution from field sampling using cores table 4 this was named wrd comp and kool comp zeros in table 4 indicate that the size range is not being used in this psd a further parameter available is the active layer which is the depth that the particle size is distributed this is user defined in previous work the active layer has been defined as being 0 2 m as this was the depth from which the particle size was determined hancock et al 2010 2011 2015 it is also the depth over which rilling has been observed with fresh material exposed by this process in this study to assess the sensitivity of the model a series of simulations were run with active layers at 0 1 and 0 2 m 5 model setup and parameter input a series of simulations were run using the dem fig 3 and rainfall described in section 3 2 below the various psds input to the caesar lisflood model are explained 5 1 waste rock dump surface particle size distributions there is considerable conjecture regarding how newly exposed material weathers and at what rate analysis of the 2009 2012 2014 and 2018 psds as well as qualitative assessment demonstrated some small change over the ten year period with little change between 2012 and 2018 figs 4 and 5 the graphic particle size statistics folk 1980 have been calculated for each year table 5 and show little change particularly for the years 2012 2018 where the grid by number methods was used therefore a chosen psd could be considered representative of at least a ten to 20 year period or longer and the particle sizes are suitable to be used for long term modelling using lems see table 5 5 2 waste rock dump and koolpinyah surface particle size distribution a series of simulations were run employing the wrd and koolpinyah psds over their respective areas on the rehabilitated and natural surface fig 3 these simulations for simplicity and to keep the size ranges the same for the different surfaces used wrd std and kool std wrd a and kool a wrd b and kool b wrd c and kool c size ranges over their respective areas this resulted in four simulations 5 3 fewer particle size ranges compressed psd to assess the sensitivity of the model to a reduced particle size input the wrd and koolpinyah particle size was compressed to a 6 range distribution table 4 the logic of this was that there may be some instances where only limited psd are used such as a rock armoured drain that is a material may have a bias at one size fraction therefore it may be better to concentrate on describing the psd within that range 5 4 assessment of active layer depth the caesar model can have a series of active layers user defined that represent bedload surface and subsurface layers the active layer is usually 0 1 0 2 m this represents the depth over which most fluvial activity could be expected to occur to assess the sensitivity of the model to different active layer depths two simulations were run using the wrd std and kool std they were run with the active layer set at 0 1 m the sample depth for the psds and 0 2 m respectively 5 5 choice of erosion model the version of caesar lisflood used in this study v1 8g allows the user to select one of two erosion models the wilcock and crowe 2003 or the einstein 1950 a single simulation was run employing the wilcock and crowe model and using the wrd std psd this was compared with the same model input using the einstein model 6 results 6 1 single wrd particle size distribution across the catchment sediment output at annual time steps for the corridor creek catchment simulation using the different wrd psds are shown in fig 6 the peak sediment output is the result of a major storm in 2007 that is included in the 23 year rainfall data set with the 23 year rainfall data set repeated 5 times see section 3 2 each simulation has a unique sediment output however wrd c is considerably higher than for the other psds which is further illustrated in fig 7 top it is difficult to observe much difference between the other psds in fig 6 however fig 5 top and bottom cumulative erosion demonstrates that there are differences in erosion rates for the different wrd psds fig 7 bottom demonstrates that wrd std and has the lowest erosion rate while wrd c not displayed as it has the highest erosion rate and goes of the scale the differences can be attributed to wrd c having the least coarse finest size range while wrd std has the coarsest table 4 the different psds also produce differences in maximum depth of erosion i e maximum gully depth table 6 minimum depth gully depth here is produced with the finest psd i e wrd c this suggests that particle size not only influences erosion rates but also incisive erosion processes such as gullying while the differences are not large maximum depth of deposition did not appear to have any pattern in relation to psd in summary the results demonstrate that materials with a more coarse distribution have the lowest erosion rate and the finest materials have the highest erosion rate which is intuitively correct given the model formulation based on material particle size therefore correct particle size input to the model is important for any hillslope or catchment assessment 6 2 combined wrd and koolpinyah psd the corridor creek catchment is only partially disturbed by mining with approximately two thirds having an undisturbed or natural surface fig 3 these simulations test the capacity of the caesar model to accommodate two different psds over the modelled area the highest erosion rate was for the wrd c and kool c simulations which has the finest least coarse distribution fig 8 top for brevity only the cumulative data is displayed the combined wrd a and b and kool a and b produced similar sediment output with the combined wrd std and kool std the least output 6 3 compressed psd using a compressed or reduced range psd table 4 produces a cyclic pattern similar to that of employing the full data range i e fig 6 however this compressed range has a strong effect on erosion rates being approximately double that of the simulations using the full nine ranges fig 9 therefore if a limited psd is employed any predicted erosion rate should be viewed with this knowledge however an advantage of using this compressed psd is that the simulations take considerably less time to run approximately half the time therefore if data is required quickly this method provides an option but with the caveat that it will potentially double the erosion rate 6 4 selection of active layer selection of a different active layer produces a pattern of erosion similar to that observed for all other simulations however the selection of an active layer of 0 1 m or 0 2 m had a small but noticeable influence on sediment output fig 10 with the output produced by an active layer of 0 2 m slightly higher than an active layer of 0 1 m and is a result of the homogeneous vertical profile assumed here 6 5 selection of erosion model caesar lisflood can employ either the einstein 1950 or wilcock and crowe 2003 sediment transport equation the above results have all used the einstein 1950 equation here to further evaluate the sensitivity of the caesar lisflood model to different psds the wilcock and crowe equation was also used employing the wilcock and crowe model was found to reduce the erosion rate by a factor of approximately 2 3 in all cases fig 11 it also reduced gully depth table 6 it should be noted that the wilcock and crowe model has been developed from flume and stream sediment transport data and is therefore likely to represent channel and stream erosion better than the einstein model which has been applied to hillslope processes at this site coulthard et al 2012 the model is more complex and determines the shear stress on individual size classes whereas the einstein approach assumes a well sorted material either model for a mine site application is a reasonable choice given the heterogeneity of both surface and subsurface materials therefore choice of sediment transport model should be based on the erosion focus of the study 6 6 form of erosion a further issue that affects many disturbed landscape systems is that of gullies and gullying landscape evolution models such as caesar lisflood predict both erosion and deposition and adjust each elevation at each time step according to the divergence of sediment flux consequently they allow the form and extent of erosion to be visualised here gullies can be observed in all simulations interestingly each psd produces a unique pattern of gullies demonstrating the sensitivity of the model to particle size input fig 12 however there is not a lot of difference in maximum gully depth for the wrd std and wrd a b and c or the wrd comp simulations nor for the different active layer depths this demonstrates that the maximum depth of erosion is relatively insensitive to psd and that if this is the only information required from the model then a compressed psd will provide reliable information with a shorter model run time 7 discussion disturbed landscape systems such as post mining landscapes can have very different surface properties to that of the pre mine undisturbed landscape models are being increasingly used to assist in the rehabilitation of these post mining systems and can provide insights into both short and long term landscape behaviour here the sensitivity of the caesar lisflood model to different field determined surface particle size distributions was assessed the goal was to provide insights to how the model performs using different psds the advantage of this site tlf from which the wrd psd data was obtained is that there is an extensive data set of surface material particle size for both the natural koolpinyah and reconstructed surface collected over a ten year period 7 1 weathering and choice of particle size it is well understood and expected that the wrd material will weather and pedogenesis will occur at unknown and different rates hancock et al 2016 weathering will influence the particle size distribution which is currently modelled as a static set of numbers however there is a paucity of data globally examining weathering products and particle sizes wells et al 2006 2007 2008 saynor et al 2006 therefore any assessment based on present material characteristics may not represent the longer term here we find that there is little change in psd over a ten year period this suggests that any modelling using this psd will be reliable at least at decadal time scales this being the period of rapid landscape development and also the period where any erosional features i e gullies can be fixed by earthworks how weathering and soil forming processes will occur and what this will mean in terms of erosion over longer i e centennial time scales is unknown as the site and climate is unique wells et al 2006 2007 2008 saynor et al 2006 however we are confident in these findings over decadal time scales additional evidence of the stability of the particle size is provided by the sediment output from the tlf where eroded material bedload particle size exiting the tlf has been quantified fig 13 analysis of the particle size demonstrates that there is little variation this strongly suggests that there is little change in the plot surface psd 7 2 influence of particle size range for surfaces with a wide range of psds and when using a model with a limited range of size inputs there is a need to find the correct balance between size ranges to allow for large differences in the psd for the different materials further the results here demonstrate that material psd has a strong influence on erosion rates therefore the determination of a reliable psd is an important factor for caear lisflood model input we confirm that a coarser distribution reduces erosion while a finer distribution can produce an increased erosion rate a further interesting finding is that reducing or compressing the particle size range results in the model running quicker however it produces a higher sediment output over the simulation period in this case an increase in sediment output by a factor of approximately two to three fig 9 therefore materials need to have a full and robust particle size analysis and find the correct balance between size ranges to allow for large differences in the psd for the different materials however caesar lisflood is restricted to 9 ranges and therefore there is the need to accommodate large differences in the waste rock very coarse and freshly exposed and the koolpinyah much finer and much older with a lot of erosional processes having already taken place including weathering also reducing the size fractions also reduces the model run time here simulations using the compressed psd were completed in approximately half the time 12 18 h of that using the full nine ranges more than 24 h therefore it may be tempting in some cases to reduce the number of size ranges to obtain a result in less time this may well provide over estimations of erosion and should only be use for initial estimations the result here demonstrates that when reducing the number of size ranges in caesar lisflood one needs to be aware of the implications examining the position of gully heads and gully network found that psd has a strong influence interestingly the finest psd wrd c produced a greater gully network than the other distributions with the other psds producing similar gully networks fig 12 therefore a fine psd will produce not just an elevated erosion rate but also more extensive gullying it should be noted that without site specific field data it is impossible to evaluate any model prediction such an increased erosion finding may be considered conservative and result in improved landform design aimed to reduce erosion however a better argument can be made that using the full nine range psd according to some pre determined and rigorous size range i e the phi scale is more likely to allow the model to better capture the complex interactions between the various size fractions and provide a more defensible result 7 3 spatially distributed particle size distribution caesar lisflood has the capability to distribute different psds across the landscape it allows a maximum of five individual distributions to be input using the same size ranges therefore where there are multiple surfaces then decisions need to be made as to what size distributions should be used here we used four different wrd psds across the rehabilitated surface employing both the wrd and koolpinyah psds across the catchment produces similar patterns to that of employing just the wrd psd however the erosion rate is slightly increased in this case the wrd b and kool b size range provides the best option for the materials at this site 7 4 landscape management applications all simulations of the corridor creek catchment produce gullies a key message is that the landscape will produce sediment by this process this information provides both design and future maintenance guidance this is if the current proposed design is accepted then gullies and their associated high erosion are likely to occur the simulations produce erosion rates an order of magnitude above that of the regional koolpinyah rates 0 01 0 04 mm yr and are above the maximum value found at tin camp creek therefore all simulated landscapes are likely to deliver sediment to the local stream system above background rates for the 100 years modelled in the study the modelling program provides guidance as to what to expect given the current landscape design and surface materials 7 5 limitations and recommendations there is no account of any vegetation establishment and growth on this landscape it is well recognised that vegetation has a large influence on erosion and sediment transport the wrd and koolpinyah materials will have very different vegetation communities with the wrd surface being initially bare it is also likely that the two surfaces will develop different vegetation through time how this affects erosion rates and patterns over the long term is unknown the question here is what is the most appropriate psd to use in caesar lisflood for the ranger mine site the results demonstrate that reducing the number of psd size range produces a high erosion rate compared to that of the employing the nine ranges therefore it is recommended that where possible the full nine ranges be employed in many cases it may not be known as to what is an acceptable erosion rate however if a full range is employed then the model is being utilised to its full potential complexity and likely producing a more reliable result the model has shown its sensitivity to the psd range used there are two choices with determining a psd for input to caesar lisflood the first is to adjust the psd range based on the characteristics of the material this may be appropriate where there is a simple material and the distribution may be adjusted at either the fine end or the coarse end of the distribution secondly if there are a variety of materials with distinctly different distributions i e one distribution clustered at the coarse end and the other clustered at the fine end then a compromise must be employed here we suggest that the phi scale with size class boundaries that have a logarithmic progression in mm allows capture of the wide range of sizes that exist in the study site caesar lisflood is a very powerful model with many functions that provide the capability to better understand and quantify landscape processes however like any model it requires reliable and robust input data here we demonstrate that the model is very sensitive to input psd this being recognised as a major contributing factor for any field or modelling assessment 8 conclusions humans have been disturbing the landscape for the extraction of raw material for millennia mining is now occurring at a greater scale and disturbing more area than ever before here we demonstrate how a computer based landscape evolution model can be used to assess a potential rehabilitation design the results show that the model is sensitive to inputs such as material particle size here all simulations produce high erosion rates above background as well as erosion in the form of gullies which is the main type of erosion therefore the model demonstrates both its sensitivity to initial conditions and usefulness to assist in mine planning and assessment of post mining landforms such assessments allow the redesign of the landform to reduce erosion potential and or the inclusion of sediment retention structures and the need for landscape management at least in the first decade post rehabilitation 
26086,surface mining irreversibly disturbs the landscape a first order priority is to establish an erosionally stable landscape soil and surface material particle size has a strong influence on soil erosion and a number of erosion models have been developed based on the relationship with particle size here we highlight the practicalities of assessing material particle size for a post mining landscape in particular the caesar lisflood landscape evolution model the focus here requires a defined material particle size as input a key feature which differentiates caesar lisflood is the ability to apply particle size data at the same resolution as the digital elevation model dem representing the landform surface here we develop particle size distributions and demonstrate how they influence erosion for a potential post mining landform field data from the site demonstrates that material particle size distribution changes little over a ten year period and yet a strong influence on erosion rates keywords soil particle size sediment transport landscape evolution mine rehabilitation soil erosion modelling 1 introduction it is well recognised that soil and surface material i e in the study here mine waste material particle size has a large role in soil water holding capacity chemical properties and retention of nutrients therefore greatly influencing plant growth potential hazelton and murphy 2007 material particle size also has a strong influence on soil erosion many researchers have examined the role that soil particle size has on erosion and recognised that statistical relationships can be determined the best example of this is the revised universal soil loss equation rusle k or soil erodibility value which is largely determined based on material particle size wischmeier and smith 1978 in the rusle erodibility is further modified by a cover or c factor which can include an armour component based on rock percentage cover and particle size a number of fluvial sediment transport models have been developed based on the relationship with particle size wilcock and crowe 2003 the seminal work of shields 1936 and hjulstr√∂m 1939 clearly demonstrates how different shear stress and flow velocities are required to entrain the different sediment sizes and then transport it in general for soils with the same chemical properties and composition a soil with a coarse distribution will erode less than one with a fine size distribution this is because a coarse soil will 1 have a higher infiltration rate due to pore spaces between the larger particles that therefore generate less runoff and 2 require a higher shear stress to entrain them which may be due to higher velocities and or larger flow depths hjulstr√∂m 1939 in reality however this is not always the case as finer soils can retain more water and nutrients and therefore can develop a vegetation cover which can reduce erosion the relationship between sediment transport at the hillslope scale is recognised to be complex and non linear hjulstr√∂m 1939 wischmeier and smith 1978 wilcock and crowe 2003 tucker and hancock 2010 bussi et al 2014 juez et al 2018 willgoose 2018 further complexity is added by the soil catena it is well understood that for natural hillslopes soils are generally more coarse and shallow on the ridge crest with soil generally deepening and becoming more fine i e higher silt and clay content towards and at the base of the hillslope birkeland 1984 charman and murphy 2007 the hillslope changes from a zone of erosion to a zone of deposition while numerical relationships on soil particle size and how they vary down a hillslope are scarce in the literature the soil catena concept is generally accepted for fluvially dominated systems by the earth science community however in anthropogenic and disturbed landscape systems such as post mining landscapes this idea of soil catena with some structure is often absent 1 1 the mining process and soil organisation it is well known that natural soils have a vertical structure which are organised into horizons which are recognised to have very different physical and chemical properties hazelton and murphy 2007 charman and murphy 2007 this organisation is a result of physical and chemical weathering illuviation of weathered material from above as well as the slow turnover of soil by bioturbation for disturbed and or rehabilitated sites such as post mining landforms this concept of a vertically and horizontally ordered soil system or structure does not exist as the mining process destroys any previous soil or geological organisation and the material may have come from well below the ground surface the mining process while conceptually simple i e non economic material is removed to recover economic material is in reality very complex as large volumes of soil and rock require removal in an ordered process to access the economic ore the mining process has a large impact on material particle size post mining landscapes are constructed of what is termed waste rock from the pit in most cases this material has been initially dislodged using explosives explosives break up what is termed overburden that is the material overlying the mineral of interest or interburden material that is within the seams of the mineral of interest the blasting process uses sufficient energy to break up the host rock so that it can be easily removed by earthmoving equipment too little blast energy and large rocks result which are difficult for equipment to remove too much energy and a fine material results which can be difficult to handle produces dust i e air pollution as well as fly rock that is flying rock that travels hundreds of metres away from the blast site the amount of explosive used and the resultant blast energy has a large impact on the size of the waste rock particles the over and inter burden can have vastly different physical size as well as chemical properties depending on blast energy or where in the pit the material is removed from the particle size is further changed in transport and placement and the shaping of this material by earthmoving equipment this results in very different particle size distributions of this waste material for example at coal mines waste rock can be directly removed by a large excavator termed a dragline and placed directly in a waste rock dump however for most metalliferous or other mines in the case here uranium the waste rock is removed by an excavator and placed in a truck which transports the waste to a waste rock dump where it is dumped in a series of piles in a process called end dumping these piles whether they have been placed there by dragline or truck are then shaped by a bulldozer according to some predetermined landscape design in many cases the new landscape is constructed as a series of lifts usually about 10 m high as the waste rock is reshaped in layers this material delivery and earthmoving process results in many passes of heavy earthmoving equipment wheels and tracks which pounds and compacts the surface resulting in further breakdown of the waste rock as well as the potential for compaction which can impede water movement the transporting dumping and reshaping with a bulldozer continues to change the material particle size distribution at some mine sites topsoil that has been removed and stockpiled is spread on the reshaped landform and used as a growth medium in other cases a subsoil or weathered waste rock i e lateritic material can be used as a topsoil such weathered material offers the benefits of having a much finer particle size distribution than waste rock and therefore potentially improved water holding capacity the addition of subsoil material can also increase the volume of topsoil if it is required for plant establishment this requires identification of the material pre mining and a thorough understanding of its physical and chemical properties a further complication is that the new surface materials can weather and change their physical and chemical characteristics what the weathering process is what rate it proceeds at together with what the physical and chemical weathering products are will be site specific there is a near complete absence of this type of information for post mining landscapes wells et al 2006 2007 2008 depending on the amount of soil available and the suitability of the waste rock as plant growth medium the depth of spread soil can range from no soil needed to tens of centimetres some mines such as the ranger mine in the northern territory of australia have no topsoil and are therefore not able to spread additional material on the waste rock in many cases both with and without topsoil this surface is then ripped by a bulldozer pulling a large tyne to produce roughness reduce runoff velocity as well as increase infiltration in order to reduce erosion this roughness can also act as a sediment and seed trap if the surface is to be revegetated by seeding this ripping process also mixes underlying waste rock including large boulders with the surface material therefore the construction of a post mining landscape results in a highly heterogeneous landscape consisting of a range of different materials with different particle size distributions with no order that can be predetermined according to our understandings of the natural environment in effect post mining landscapes are new systems constructed of recycled earth materials that may have been pulverised and compacted with no vertical or horizontal particle size structure 1 2 predicting post mining landscape soil erosion and landscape evolution past erosion models such as the rusle have operated at the hillslope and or slope segment scale using lumped parameters in recent years a number of soil erosion and landscape evolution models lems have been developed see tucker and hancock 2010 for a review the models use a digital elevation model dem to represent the landscape and directly calculate an erosion rate based on runoff and the material particle size distribution in particular the caesar lisflood coulthard et al 2013 model has the ability to input particle size for every cell in the dem which may represent different surface cover types in addition the most recent version of caesar lisflood has the ability to represent the particle size characteristics of a sub surface layer caesar is the only lem at present with this capability the siberia lem willgoose et al 1991 has a similar capability where it can spatially distribute erodibility at each dem node hancock et al 2008 the model can then route move sediment from node to node based on flow direction and shear stress required to move the particle flow and velocity of flow is determined based on the upslope contributing area at each point determined from the dem the material particle size is input by the user based on field data where the reliability of the material particle size data is very important for correctly modelling landscape behaviour soil particle or surface material particle size and distribution affects erosion rates in lems hirano 1971 proposed the concept of an active layer where there exists a conservation of mass between sediment on the landscape surface and the sediment that is being transported it is recognised that the thickness of the active layer governs the competition between bed level change and the change in surface particle distribution juez et al 2016 hancock et al 2010 and hancock and coulthard 2012 demonstrated that different particle size distributions influence erosion rates for an agricultural catchment in south east australia they showed that soil particle size distribution had a large impact on erosion rate and type of erosion hancock and coulthard 2012 also demonstrated that different soil particle size distributions can control the position and movement of channels under different rainfall events juez et al 2018 hancock et al 2010 demonstrated that using different field measured particle size distributions for a natural catchment in northern australia produced erosion rates that could vary by a factor of 2 3 the outcome of these modelling exercises demonstrated that a good understanding of the surface material characteristics based on field data is required an important issue not examined was that of the active layer where erosion and deposition occurs if the active layer is too shallow the adjustment of surface grain size distribution is fast and the bed level changes will be constrained by the armouring of the surface a too thick layer may produce excess erosion the caesar lisflood model has the capability to employ an active layer depth across a possible nine size ranges and further described in section 3 1 here we highlight the complexity of post mining landscapes and their surface material properties the questions examined here are given the heterogeneity of a post mining landscape with a coarser particle size than the surrounding natural surface how does particle size influence erosion furthermore given the likely wide range of particle size and surface materials what is the optimal size grading or range to be employed in a lem to model a rehabilitated landform and surrounds can we reliably model soil erosion and landscape evolution of both natural and post mining landscapes we demonstrate here how a lem can be used to better understand disturbed systems such as post mining landscapes 2 site description a proposed rehabilitated landform at the ranger uranium mine is used here as a test case to examine the effects of different material particle size ranges from mined waste rock material and surrounding natural surfaces the rehabilitation requirements for the ranger mine state with respect to erosion and landform stability that the landform should possess erosion characteristics which as far as can reasonably be achieved do not vary significantly from those of comparable landforms in surrounding undisturbed areas australian government 1999 this requires that the disturbed landscape be rehabilitated in a way that restores environmental functions such as the sediment water and nutrient balance supporting local ecosystem diversity these environmental functions include landscape water nutrient and energy balance which can be represented through geomorphic properties such as relief ludwig and tongway 1995 an erosionally stable landform is the first part of this process the ranger mine is located in the tropical north of australia approximately 250 km east of darwin it is in the wet dry tropics of northern australia and receives high intensity storms and tropical monsoons between october and april the wet season with little rain falling for the remainder of the year the dry season fig 1 the annual average rainfall is 1584 mm with high interannual variability ranging between 1038 mm yr to 2623 mm yr www bom gov au the geology of the site consists of igneous rocks and mineralised metasediments of the pine creek geosyncline and the younger mamadawerre formation sandstones east 1996 needham 1988 the area is one of the richest uranium areas globally mineral lease geology consists of a superficial cover of cainozoic soil unconsolidated sands ferruginous material and laterite over lower proterozoic schists of the cahill formation geomorphically the site is characterised as the koolpinyah surface which comprises plains broad valleys and low slopes with isolated ridges and hills of resistant rock east 1996 previous research in the alligator rivers region duggan 1994 has shown that sediment yields decline progressively over at least the first three years following a major surface disturbance as a result of initial washout of fine sediment and the subsequent formation of a gravel armoured surface hancock et al 2016 2017 erosion rates for the koolpinyah surface have been determined by a variety of methods over a number of years cull et al 1992 and erskine and saynor 2000 suggest regional denudation rates of between 0 01 and 0 04 mm yr hancock and lowry 2015 have determined erosion rates of 0 037 0 18 mm yr for a local analogue area tin camp creek a trial landform has been constructed at the site to inform landform design and ecosystem restoration efforts saynor et al 2012 hancock et al 2016 2017 fig 2 this landform of approximately 200 m by 400 m 8 ha was constructed during late 2008 and early 2009 by the mine operator energy resources australia era adjacent to the north western wall of the tailings storage facility at the ranger mine fig 1 four erosion plots 30 m by 30 m were constructed on the trial landform with a maximum slope of 2 during the 2009 dry season with material characteristics representative of what may occur on the reconstructed landform from this trial landform site a series of material particle size ranges were determined for waste rock material and are described in section 4 below fig 2 at the completion of mining and processing of the ore the pits from which the ore was extracted will be filled and the exhumed waste rock reshaped into a new landform a proposed design for one catchment within the mine corridor creek is described below section 3 2 and fig 2 the landform will be vegetated by seeding and potentially hand planted tubestock however there is no stockpiled topsoil to assist with the establishment of this vegetation 3 landscape evolution models while developed for geological time scale applications in recent years landscape evolution models lems have been employed in applied settings i e mine sites at considerably shorter time scales years decades centuries than geological time scales testing of these models across a range of environments has shown that they are generally reliable when employed using appropriate input parameters tucker and hancock 2010 it should be recognised that there are several caveats the landscape representation i e the digital elevation model has to correctly represent the landscape and capture hillslope curvature as well as features such as contour banks hancock 2005 model input parameters require a detailed knowledge of the site and are largely derived from site specific field data and have a large impact on any model outputs evans 2000 hancock et al 2000 2002 2006 model sensitivity has been evaluated mostly by assessing different landforms and differences in elevations willgoose et al 2003 however the models lend themselves to a full monte carlo type assessment with the only limitation being input parameter knowledge and reliability and computing resources willgoose 2005 further assessments have been conducted using comparison with erosion rates determined by environmental tracers such as 137cs hancock et al 2008 and gully position and morphology hancock et al 2000 decadal time scale assessment as well as at annual time scales using field erosion plots lowry et al 2011 2013 saynor et al 2012 centennial to millennial time scale assessments have been conducted by comparing one lem siberia willgoose et al 1991 against another caesar coulthard et al 2013 martinez et al 2009 hancock et al 2010 2011 2015 landform evolution modelling to assess a post mining rehabilitated landform design was first conducted by willgoose and riley 1993 using the siberia model willgoose et al 1991 since 1993 the model has been extensively used to assess erosion and evolution of post mining rehabilitated landforms or small catchments i e willgoose and riley 1998 evans et al 1998 hancock et al 2000 2002 moliere et al 2002 further it has been employed at the nabarlek uranium mine following an initial assessment using the empirically based rusle hancock et al 2006 2008 while a very reliable and useful model to predict erosion fluxes the rusle wischmeier and smith 1978 does not evolve a landscape surface in response to erosion and deposition as do lems a new development is the physically based caesar and subsequent caesar lisflood lem coulthard et al 2013 these models have been used to assess rehabilitation designs on mine sites coulthard et al 2012 lowry et al 2011 2013 the advantage of caesar lisflood is that it operates at a much finer temporal resolution hourly than the siberia or rusle models yearly this paper employs the caesar lisflood lem 3 1 the caesar lisflood landscape and erosion model caesar lisflood is a physically based landscape and river reach evolution model that evolved from the caesar model of coulthard et al 2002 caesar lisflood is an improvement on previous versions of caesar as it included a hydrodynamic 2d flow model based on the lisflood fp code of bates et al 2010 a full description of caesar lisflood is given in coulthard et al 2013 here we use version 1 8g the model simulates erosion and deposition in river catchments and reaches over time scales from hours to 1000 s of years caesar lisflood enables a user to input a dem and uses hourly rainfall data to drive a hydrological model based on topmodel beven and kirkby 1979 a significant advantage of caesar lisflood over other lems is that it is able to utilise rainfall at hourly time steps which allows different rainfall data sets and specific rainfall events to be modelled lisflood fp is a one dimensional inertial model derived from the full set of shallow water equations and is applied in the x and y directions to simulate two dimensional flow over the raster grid the model is second order in space and first order in time bates et al 2010 surface runoff is then routed using the lisflood fp code that calculates flow between cells based on water surface slope and a local inertia term by utilising one dimensional 1d flow models in both the x and y directions this then generates a two dimensional 2d flow field from which shear stress is calculated from flow velocity between cells and then used to calculate sediment entrainment via either the einstein 1950 or wilcock and crowe 2003 sediment transport rules bedload is routed according to the distribution of velocities and resultant shear from a cell while suspended sediment is routed with water flow and then deposited according to the settling velocity sediment transport is calculated over a range of up to nine grain sizes with an active layer defined by the user these are integrated within a user defined set active layers coulthard et al 2013 erosion transport and deposition of the grains will result in spatially variable sediment size distributions the management of this is carried out using a system of layers comprising an active layer representing the stream bed multiple buried layers strata a base layer and if required an immovable bedrock layer the layers have a fixed thickness and their position is fixed relative to the bedrock layer this allows the development of differential surface grainsize patterns and effects such as bed armouring and grainsize changes over depositional features such as bars and or fans coulthard et al 2012 2013 the model can also input up to five different particle size data sets for different surfaces over the study domain on a pixel by pixel basis while the proportions within each of the different particle size distributions can vary the ranges initially employed are fixed therefore if there are materials with different particle size distributions a compromise needs to be found regarding the size ranges to be employed caesar provides two different methods of calculating sediment transport based on the einstein 1950 and the wilcock and crowe 2003 equations the einstein 1950 approach was developed based on predominantly sand based laboratory channels whereas the wilcock and crowe 2003 formula on field and laboratory data from a coarser bed gravel sand mix we evaluate both models here a calculation of shear stress is required to drive each model and this is calculated from flow velocity the choice of size range is an important issue at this study site where the waste rock particle size is a very different particle size to that of the natural undisturbed surface the choice of the particle size is the focus of the work here 3 2 caesar lisflood input parameters the caesar lisflood model requires three key inputs a dem to represent topography long term rainfall data mm hr 1 and surface material particle size distribution data the focus of this study and described in section 4 here we use a proposed rehabilitation design for the corridor creek area of the ranger site the design is one option that may be used and it provides a realistic test case for any model and landscape assessment as it contains waste rock areas approximately two thirds of the catchment and natural areas approximately one third of the catchment a dem of a potential rehabilitated landform for the site was supplied by era fig 3 the original catchment topographic data for corridor creek was in the form of 2 m interval contours which was gridded to a horizontal spatial resolution of 10 m this 10 m grid size was determined to be the optimal resolution at which caesar lisflood could function as well as reliably represent hillslope geomorphology hancock 2005 the dem has been used for previous landscape assessment and has been described in detail elsewhere hancock et al 2016 a key feature of the caesar lisflood is the ability to model sediment output at the storm event time scale using rainfall data at hourly time steps as an input it is well recognised that temporally variable rainfall has a large effect on erosion rate and type juez et al 2018 here we use rainfall data from the nearest site 2 km distance with the longest rainfall record jabiru airport site number 014198 www bom gov au fig 1 pluviograph rainfall data 6 min was obtained from the australian bureau of meteorology bom here only years with complete annual pluviograph data was used from 1972 to 2011 producing a 23 year record years with missing data were excluded more recent data had incomplete and or missing data during this period the study area has been subjected to a range of rainfall events and seasonal variability which may be considered climatically representative this dataset representing 23 years of complete annual rainfall data was added end to end to produce a continuous 100 year record that could be used as input into the caesar lisflood model the caesar lisflood input parameters are listed in table 1 4 material particle size characterisation the rehabilitation of the ranger site will encompass both a disturbed surface with an extensive cover of freshly exhumed rock waste rock as well as significant areas of natural undisturbed areas koolpinyah surface these surface materials and their particle size distribution psd are described below 4 1 waste rock two different sets of waste rock psds were examined here one from the ranger trial landform tlf and a second based on historical data these are described below 4 1 1 trial landform the tlf fig 2 tests two types of potential surface material waste rock and waste rock blended with approximately 30 fine grained weathered horizon material lateritic material for their suitability as cap materials once the placement and shaping of the surface had been completed the surface was ripped by a bulldozer pulling a large tyne to create surface roughness initial detail regarding the present surface has been described elsewhere saynor and houghton 2011 in april 2009 sixteen surface samples between 5766 and 8461 g were collected from the tlf and particle size was determined by sieve analysis however this manual sampling precluded the collection and measurement of larger material fig 2 that was captured by the grid by numbers method described below employed in 2012 2014 and 2018 obtaining bulk samples from specific geomorphic environments is the accepted method of sampling surficial sediments kellerhals and bray 1971 this involves the collection of material from a predetermined volume within a specific geomorphic environment however there are potential problems with bulk sampling that must be recognised very large sample masses are required to obtain reproducible measures of the grain size distributions of samples containing individual large clasts or gravels de vries 1970 church and wolcott 1987 gale and hoare 1992 ferguson and paola 1997 to overcome some of the problems of collecting a large sized sample that would impact greatly on the surface of the plot an alternative solution was required wolman 1954 devised a grid by number surface sampling technique for gravel bars in rivers gravel count method slight variations of this technique have been recommended leopold 1970 kellerhals and bray 1971 and the grid by number method is discussed in erskine and turner 1998 erskine and turner 1998 recommended that the b axis diameter of at least 100 clasts should be measured to obtain a reliable grain size distribution the grid by number method has been adapted here for determining the surface grain size of the erosion plots to determine what clasts were measured on the erosion plots the following methodology was followed twenty 1 m by 1 m square quadrats were made from pvc pipe holes were drilled in the middle of each square and fishing line tied between each opposite side so that the fishing line intersected in the middle of the 1 m quadrat ten of the 1 m quadrats were laid on the tlf plots and another 10 laid next to the first row of the plots where the fishing line intersected a steel rule was used to measure the clast laid directly underneath the b axis of the clast was measured and if picked up the clast was replaced in its original position where the clast was less than 8 mm it was noted as less than 8 mm once the clasts under the 10 quadrats had been measured the quadrats were moved alongside the next row of quadrats this was continued until the opposite side of the erosion plot was reached and then the quadrats were moved down to the next 10 m this method was completed on all four erosion plots for 2012 2014 and 2018 table 2 shows the number of clasts measured where more than 5 of the sample was finer than 8 mm a bulk sample of the finer sediment was collected this bulk sample was dried and sieved and combined with the gravel count data using a weighting factor equivalent to its percentage surface exposure the samples were than averaged to give particle size for waste rock material table 3 fig 4 demonstrates that there has been little change in the psd over a ten year period further visual evidence also demonstrates that there has been little change particularly for the larger material fig 5 this suggests that at least over decadal time scales that there is little variation as a result of weathering and that a particle size determined early on in landscape development is robust over the short to medium term at this site 4 1 2 historic data a psd was developed from historic data for a range of different waste rock surfaces and at different times at the site evans 2000 evans et al 1998 moliere et al 2002 the psd was determined using sieve and hydrometer and represents an average of the different surfaces that were examined prior to the construction of the tlf described below the psd is considered to be a realistic and reliable data set as it encapsulates a range of historic surfaces and psds that are likely to be present on the rehabilitated surface as the data had been collected over several years with different surface treatments such as vegetation and fire as this psd has been used extensively for past assessments lowry et al 2011 2013 hancock et al 2015 it will also be examined here due to its prior use this data set is considered to be the standard distribution and for brevity is named wrd std table 4 4 2 natural koolpinyah surface as the site has extensive areas of undisturbed or natural surface fig 3 a psd for this surface was also examined the determination of psd for the natural or undisturbed areas koolpinyah surface has been well described by saynor et al 2009a b a summary is provided below soils in the gulungul creek catchment immediately to the west of ranger mine were sampled in 2006 as part of a large project looking at the impacts of cyclone monica which passed through the area in april 2006 saynor et al 2009a b detailed particle size analyses were undertaken on samples collected from woodland and open woodland vegetation communities that are indicative of the type of soils on the natural surrounding koolpinyah surface a hand soil auger was used to excavate soil to a depth until refusal or no retention the surface material of 0 1 m depth has had a particle size distribution determined on them using sieve and hydrometer methods gee and bauder 1986 an average of all samples is displayed in table 3 4 3 particle size distribution used for modelling the maximum number of particle size ranges allowable in caesar lisflood is restricted to nine each size range is user defined but has to be the same size range for the different surface types represented on the landform for many sites with a single soil or surface material such as the water rock at a mine site nine particle size ranges is adequate however here we require a psds that can represent the very different size ranges of the waste rock coarse dominated and koolpinyah material fine dominated displayed in table 3 to fit within the 9 size ranges as mentioned previously while the amount of material as a fraction can vary the psd range in caesar lisflood is fixed at the start of the model run it can be seen that the wrd and koolpinyah materials have very different psds table 3 here we are required to choose size ranges that cover both the waste rock and koolpinyah size ranges to fit into the 9 size ranges so that both surface materials can be represented within the model reliably table 4 displays the waste rock and koolpinyah psd from table 3 regrouped into the 9 size range classification as input to caesar lisflood and described in section 4 1 1 and 4 2 the groupings chosen are loosely based on the phi size scale a commonly used size scale in fluvial geomorphology to cover the size ranges of both the waste rock and koolpinyah distributions here the groupings are named wrd a wrd b and wrd c and kool a kool b and kool c for the koolpinyah materials table 4 the groupings are simply different options for model input to test model sensitivity another option of the caesar lisflood model is that while the model employs a maximum 9 range particle size distribution as input it does not require all 9 ranges to be input it can operate at its simplest with a single particle size this is important at the ranger site to enable the modelling of the armoured drainage lines that will have a coarse but narrow size range to assess the sensitivity of the model to a reduced particle size data input the wrd and koolpinyah particle size was compressed to a 6 range distribution from field sampling using cores table 4 this was named wrd comp and kool comp zeros in table 4 indicate that the size range is not being used in this psd a further parameter available is the active layer which is the depth that the particle size is distributed this is user defined in previous work the active layer has been defined as being 0 2 m as this was the depth from which the particle size was determined hancock et al 2010 2011 2015 it is also the depth over which rilling has been observed with fresh material exposed by this process in this study to assess the sensitivity of the model a series of simulations were run with active layers at 0 1 and 0 2 m 5 model setup and parameter input a series of simulations were run using the dem fig 3 and rainfall described in section 3 2 below the various psds input to the caesar lisflood model are explained 5 1 waste rock dump surface particle size distributions there is considerable conjecture regarding how newly exposed material weathers and at what rate analysis of the 2009 2012 2014 and 2018 psds as well as qualitative assessment demonstrated some small change over the ten year period with little change between 2012 and 2018 figs 4 and 5 the graphic particle size statistics folk 1980 have been calculated for each year table 5 and show little change particularly for the years 2012 2018 where the grid by number methods was used therefore a chosen psd could be considered representative of at least a ten to 20 year period or longer and the particle sizes are suitable to be used for long term modelling using lems see table 5 5 2 waste rock dump and koolpinyah surface particle size distribution a series of simulations were run employing the wrd and koolpinyah psds over their respective areas on the rehabilitated and natural surface fig 3 these simulations for simplicity and to keep the size ranges the same for the different surfaces used wrd std and kool std wrd a and kool a wrd b and kool b wrd c and kool c size ranges over their respective areas this resulted in four simulations 5 3 fewer particle size ranges compressed psd to assess the sensitivity of the model to a reduced particle size input the wrd and koolpinyah particle size was compressed to a 6 range distribution table 4 the logic of this was that there may be some instances where only limited psd are used such as a rock armoured drain that is a material may have a bias at one size fraction therefore it may be better to concentrate on describing the psd within that range 5 4 assessment of active layer depth the caesar model can have a series of active layers user defined that represent bedload surface and subsurface layers the active layer is usually 0 1 0 2 m this represents the depth over which most fluvial activity could be expected to occur to assess the sensitivity of the model to different active layer depths two simulations were run using the wrd std and kool std they were run with the active layer set at 0 1 m the sample depth for the psds and 0 2 m respectively 5 5 choice of erosion model the version of caesar lisflood used in this study v1 8g allows the user to select one of two erosion models the wilcock and crowe 2003 or the einstein 1950 a single simulation was run employing the wilcock and crowe model and using the wrd std psd this was compared with the same model input using the einstein model 6 results 6 1 single wrd particle size distribution across the catchment sediment output at annual time steps for the corridor creek catchment simulation using the different wrd psds are shown in fig 6 the peak sediment output is the result of a major storm in 2007 that is included in the 23 year rainfall data set with the 23 year rainfall data set repeated 5 times see section 3 2 each simulation has a unique sediment output however wrd c is considerably higher than for the other psds which is further illustrated in fig 7 top it is difficult to observe much difference between the other psds in fig 6 however fig 5 top and bottom cumulative erosion demonstrates that there are differences in erosion rates for the different wrd psds fig 7 bottom demonstrates that wrd std and has the lowest erosion rate while wrd c not displayed as it has the highest erosion rate and goes of the scale the differences can be attributed to wrd c having the least coarse finest size range while wrd std has the coarsest table 4 the different psds also produce differences in maximum depth of erosion i e maximum gully depth table 6 minimum depth gully depth here is produced with the finest psd i e wrd c this suggests that particle size not only influences erosion rates but also incisive erosion processes such as gullying while the differences are not large maximum depth of deposition did not appear to have any pattern in relation to psd in summary the results demonstrate that materials with a more coarse distribution have the lowest erosion rate and the finest materials have the highest erosion rate which is intuitively correct given the model formulation based on material particle size therefore correct particle size input to the model is important for any hillslope or catchment assessment 6 2 combined wrd and koolpinyah psd the corridor creek catchment is only partially disturbed by mining with approximately two thirds having an undisturbed or natural surface fig 3 these simulations test the capacity of the caesar model to accommodate two different psds over the modelled area the highest erosion rate was for the wrd c and kool c simulations which has the finest least coarse distribution fig 8 top for brevity only the cumulative data is displayed the combined wrd a and b and kool a and b produced similar sediment output with the combined wrd std and kool std the least output 6 3 compressed psd using a compressed or reduced range psd table 4 produces a cyclic pattern similar to that of employing the full data range i e fig 6 however this compressed range has a strong effect on erosion rates being approximately double that of the simulations using the full nine ranges fig 9 therefore if a limited psd is employed any predicted erosion rate should be viewed with this knowledge however an advantage of using this compressed psd is that the simulations take considerably less time to run approximately half the time therefore if data is required quickly this method provides an option but with the caveat that it will potentially double the erosion rate 6 4 selection of active layer selection of a different active layer produces a pattern of erosion similar to that observed for all other simulations however the selection of an active layer of 0 1 m or 0 2 m had a small but noticeable influence on sediment output fig 10 with the output produced by an active layer of 0 2 m slightly higher than an active layer of 0 1 m and is a result of the homogeneous vertical profile assumed here 6 5 selection of erosion model caesar lisflood can employ either the einstein 1950 or wilcock and crowe 2003 sediment transport equation the above results have all used the einstein 1950 equation here to further evaluate the sensitivity of the caesar lisflood model to different psds the wilcock and crowe equation was also used employing the wilcock and crowe model was found to reduce the erosion rate by a factor of approximately 2 3 in all cases fig 11 it also reduced gully depth table 6 it should be noted that the wilcock and crowe model has been developed from flume and stream sediment transport data and is therefore likely to represent channel and stream erosion better than the einstein model which has been applied to hillslope processes at this site coulthard et al 2012 the model is more complex and determines the shear stress on individual size classes whereas the einstein approach assumes a well sorted material either model for a mine site application is a reasonable choice given the heterogeneity of both surface and subsurface materials therefore choice of sediment transport model should be based on the erosion focus of the study 6 6 form of erosion a further issue that affects many disturbed landscape systems is that of gullies and gullying landscape evolution models such as caesar lisflood predict both erosion and deposition and adjust each elevation at each time step according to the divergence of sediment flux consequently they allow the form and extent of erosion to be visualised here gullies can be observed in all simulations interestingly each psd produces a unique pattern of gullies demonstrating the sensitivity of the model to particle size input fig 12 however there is not a lot of difference in maximum gully depth for the wrd std and wrd a b and c or the wrd comp simulations nor for the different active layer depths this demonstrates that the maximum depth of erosion is relatively insensitive to psd and that if this is the only information required from the model then a compressed psd will provide reliable information with a shorter model run time 7 discussion disturbed landscape systems such as post mining landscapes can have very different surface properties to that of the pre mine undisturbed landscape models are being increasingly used to assist in the rehabilitation of these post mining systems and can provide insights into both short and long term landscape behaviour here the sensitivity of the caesar lisflood model to different field determined surface particle size distributions was assessed the goal was to provide insights to how the model performs using different psds the advantage of this site tlf from which the wrd psd data was obtained is that there is an extensive data set of surface material particle size for both the natural koolpinyah and reconstructed surface collected over a ten year period 7 1 weathering and choice of particle size it is well understood and expected that the wrd material will weather and pedogenesis will occur at unknown and different rates hancock et al 2016 weathering will influence the particle size distribution which is currently modelled as a static set of numbers however there is a paucity of data globally examining weathering products and particle sizes wells et al 2006 2007 2008 saynor et al 2006 therefore any assessment based on present material characteristics may not represent the longer term here we find that there is little change in psd over a ten year period this suggests that any modelling using this psd will be reliable at least at decadal time scales this being the period of rapid landscape development and also the period where any erosional features i e gullies can be fixed by earthworks how weathering and soil forming processes will occur and what this will mean in terms of erosion over longer i e centennial time scales is unknown as the site and climate is unique wells et al 2006 2007 2008 saynor et al 2006 however we are confident in these findings over decadal time scales additional evidence of the stability of the particle size is provided by the sediment output from the tlf where eroded material bedload particle size exiting the tlf has been quantified fig 13 analysis of the particle size demonstrates that there is little variation this strongly suggests that there is little change in the plot surface psd 7 2 influence of particle size range for surfaces with a wide range of psds and when using a model with a limited range of size inputs there is a need to find the correct balance between size ranges to allow for large differences in the psd for the different materials further the results here demonstrate that material psd has a strong influence on erosion rates therefore the determination of a reliable psd is an important factor for caear lisflood model input we confirm that a coarser distribution reduces erosion while a finer distribution can produce an increased erosion rate a further interesting finding is that reducing or compressing the particle size range results in the model running quicker however it produces a higher sediment output over the simulation period in this case an increase in sediment output by a factor of approximately two to three fig 9 therefore materials need to have a full and robust particle size analysis and find the correct balance between size ranges to allow for large differences in the psd for the different materials however caesar lisflood is restricted to 9 ranges and therefore there is the need to accommodate large differences in the waste rock very coarse and freshly exposed and the koolpinyah much finer and much older with a lot of erosional processes having already taken place including weathering also reducing the size fractions also reduces the model run time here simulations using the compressed psd were completed in approximately half the time 12 18 h of that using the full nine ranges more than 24 h therefore it may be tempting in some cases to reduce the number of size ranges to obtain a result in less time this may well provide over estimations of erosion and should only be use for initial estimations the result here demonstrates that when reducing the number of size ranges in caesar lisflood one needs to be aware of the implications examining the position of gully heads and gully network found that psd has a strong influence interestingly the finest psd wrd c produced a greater gully network than the other distributions with the other psds producing similar gully networks fig 12 therefore a fine psd will produce not just an elevated erosion rate but also more extensive gullying it should be noted that without site specific field data it is impossible to evaluate any model prediction such an increased erosion finding may be considered conservative and result in improved landform design aimed to reduce erosion however a better argument can be made that using the full nine range psd according to some pre determined and rigorous size range i e the phi scale is more likely to allow the model to better capture the complex interactions between the various size fractions and provide a more defensible result 7 3 spatially distributed particle size distribution caesar lisflood has the capability to distribute different psds across the landscape it allows a maximum of five individual distributions to be input using the same size ranges therefore where there are multiple surfaces then decisions need to be made as to what size distributions should be used here we used four different wrd psds across the rehabilitated surface employing both the wrd and koolpinyah psds across the catchment produces similar patterns to that of employing just the wrd psd however the erosion rate is slightly increased in this case the wrd b and kool b size range provides the best option for the materials at this site 7 4 landscape management applications all simulations of the corridor creek catchment produce gullies a key message is that the landscape will produce sediment by this process this information provides both design and future maintenance guidance this is if the current proposed design is accepted then gullies and their associated high erosion are likely to occur the simulations produce erosion rates an order of magnitude above that of the regional koolpinyah rates 0 01 0 04 mm yr and are above the maximum value found at tin camp creek therefore all simulated landscapes are likely to deliver sediment to the local stream system above background rates for the 100 years modelled in the study the modelling program provides guidance as to what to expect given the current landscape design and surface materials 7 5 limitations and recommendations there is no account of any vegetation establishment and growth on this landscape it is well recognised that vegetation has a large influence on erosion and sediment transport the wrd and koolpinyah materials will have very different vegetation communities with the wrd surface being initially bare it is also likely that the two surfaces will develop different vegetation through time how this affects erosion rates and patterns over the long term is unknown the question here is what is the most appropriate psd to use in caesar lisflood for the ranger mine site the results demonstrate that reducing the number of psd size range produces a high erosion rate compared to that of the employing the nine ranges therefore it is recommended that where possible the full nine ranges be employed in many cases it may not be known as to what is an acceptable erosion rate however if a full range is employed then the model is being utilised to its full potential complexity and likely producing a more reliable result the model has shown its sensitivity to the psd range used there are two choices with determining a psd for input to caesar lisflood the first is to adjust the psd range based on the characteristics of the material this may be appropriate where there is a simple material and the distribution may be adjusted at either the fine end or the coarse end of the distribution secondly if there are a variety of materials with distinctly different distributions i e one distribution clustered at the coarse end and the other clustered at the fine end then a compromise must be employed here we suggest that the phi scale with size class boundaries that have a logarithmic progression in mm allows capture of the wide range of sizes that exist in the study site caesar lisflood is a very powerful model with many functions that provide the capability to better understand and quantify landscape processes however like any model it requires reliable and robust input data here we demonstrate that the model is very sensitive to input psd this being recognised as a major contributing factor for any field or modelling assessment 8 conclusions humans have been disturbing the landscape for the extraction of raw material for millennia mining is now occurring at a greater scale and disturbing more area than ever before here we demonstrate how a computer based landscape evolution model can be used to assess a potential rehabilitation design the results show that the model is sensitive to inputs such as material particle size here all simulations produce high erosion rates above background as well as erosion in the form of gullies which is the main type of erosion therefore the model demonstrates both its sensitivity to initial conditions and usefulness to assist in mine planning and assessment of post mining landforms such assessments allow the redesign of the landform to reduce erosion potential and or the inclusion of sediment retention structures and the need for landscape management at least in the first decade post rehabilitation 
26087,hydrodynamic models are commonly used to understand flood risk and inform flood management decisions however their high computational cost can impose practical limits on real time flood forecasting and uncertainty analysis which require fast modelling response or many model runs emulation models have the potential to reduce simulation times while still maintaining acceptable accuracy of the estimates in this study we propose an artificial neural networks anns based emulation modelling framework for flood inundation modelling we investigate the suitability of anns as flood inundation models using a river segment in queensland australia our results show that anns can model the time series behaviour of flood inundation and significantly reduce the simulation times required which facilitates their use in applications requiring fast model response or a large number of model runs based the model development process and results the major challenges and future research directions are discussed keywords flood inundation modelling artificial neural networks emulation models meta models surrogate models 1 introduction flooding is one of the most devastating and frequent natural hazards in the world they account for half of the weather related disasters between 1995 and 2015 verwey et al 2017 and are estimated to affect 1 3 billion people globally by 2050 ligtvoet et al 2014 it is important to understand assess and predict flood events and their impact to prevent loss of life and damages to properties one important tool to achieve this goal is flood inundation models which can provide information of flood events such as the extent depth duration and velocity of floods so that the hazard levels exposure and vulnerability of an area can be estimated verwey et al 2017 flood inundation models have been widely used for flood management including flood risk assessment balica et al 2013 flood forecasting and warning casagrande et al 2017 flood hazard mapping merwade et al 2008 flood related engineering infrastructure design schmitt et al 2004 floodplain sediment transport analysis van manh et al 2015 floodplain ecology studies wilson et al 2007 and urban flood management fewtrell et al 2008 commonly used numerical models for flood inundation modelling include one dimensional 1d hydrodynamic models which treat the flow as one dimensional along the centre line of the river channel such as in a confined channel or in a pipe van manh et al 2015 habert et al 2016 two dimensional 2d hydrodynamic models which simulate conservation of mass and momentum in a 2d plane using shallow water equations and 1d one dimensional and 2d coupled models where a 1d model is used to simulate the river channel and the floodplain is modelled using the 2d component bates et al 2010 morales hern√°ndez et al 2013 these 2d or 1d 2d coupled hydrodynamic models provide adequate representation of flow movement along both river channels and across floodplains kim et al 2014 leandro et al 2009 and can model flood inundation in areas with complex dynamic interactions e g around hydraulic structures near confluences or in estuarine rivers there are several software packages available for flood inundation simulation using these models including hecras2d telemac2d tuflow and mike flood however these hydrodynamic models are computationally expensive teng et al 2015 and are not feasible for applications that require rapid model response and or a large number of model runs such applications include real time river operation based on ensemble forecasts alfieri et al 2013 demeritt et al 2007 multi objective optimisation of river operation using evolutionary algorithms dittmann et al 2009 che and mays 2015 and analyses of flood risks using monte carlo or similar methods rahman et al 2002 nathan and ling 2016 therefore there is a need to develop fast alternative modelling approaches to simulate flood inundation one fast alternative modelling approach considered for flood inundation modelling is simplified conceptual models or 0d models these models are based on simplified hydraulic concepts and do not attempt to represent the complex flood generation processes using mathematical equations and therefore are orders of magnitudes faster than hydrodynamic models mcgrath et al 2018 commonly used simplified conceptual flood inundation models include rapid flood spreading method rfsm lhomme et al 2008 the vtd model based on the bathtub method teng et al 2015 and the height above nearest drainage network hand model nobre et al 2011 these models use information from digital elevation models dems to simulate simple filling and spilling processes they have been found to be able to produce near real time simulation of flood inundation extent and depth mcgrath et al 2018 and are therefore useful to produce the maximum flood inundation maps however these models often do not consider the hydrodynamics of flood propagation and therefore cannot capture the time series behaviour of water levels nor can they provide reliable results in systems with complex dynamic interactions teng et al 2017 alternatively data driven approaches can be used to develop emulation models in place of simplified conceptual models for flood inundation modelling data driven emulation models also called surrogate models or meta models have been applied in place of physically based models in several environmental engineering applications ratto et al 2012 razavi et al 2012 timani and peralta 2017 yan and minsker 2006 used anns as emulation models to replace a process based groundwater model and a contaminant transport model to speed up model simulation in groundwater remediation design optimisation later the same approach was used in an uncertainty analysis of groundwater remediation designs based on monte carlo simulation yan and minsker 2011 broad et al 2015 developed ann based emulation models to replace both hydraulic and water quality models of water distribution systems for system design optimisation beh et al 2017 developed ann based emulation models to calculate robustness and other objectives for the sequencing of water supply infrastructure of in adelaide south australia more recently a variety of other data driven techniques have also been used including polynomial regression for optimising remediation design of heterogeneous dnapl contaminated sites qin et al 2007 support vector regression for investigating the impact of model structural error on calibration and prediction of real world groundwater flows xu et al 2017 and extreme learning machine for multi objective optimisations of coastal aquifer managements song et al 2018 data driven emulation models with different complexities have also been used in flood forecasting and management multivariate linear regression models have been used to replace 2d flood models to estimate the total flood volume and peak discharge downstream of the yellow river multiple reservoir system castro gama et al 2014 around the same time yazdi and neyshabouri 2014 integrated the sued of ann models in a genetic algorithm based optimisation process for the optimal design of a flood control dam in the kan watershed in central iran more recently a simple two parametric gamma distribution function was used as an emulation model to simulate the peak discharge flood loss relationships in a large number flood scenarios with low computational cost zischg et al 2018 alternatively gaussian processes hall et al 2011 and anns yu et al 2015 have been used to map the relationships between channel width roughness manning s coefficient and water levels at selected locations of a river in addition a support vector machine was developed to simulate the maximum water depth and velocity in response to different discharge hydrographs at selected locations liu and pender 2015 in most of these studies flood discharge or volume rather than flood level were modelled in the limited studies where flood levels were considered they were only simulated at selected locations whereas flood inundation extent and depth with time remains a challenge for emulation modelling one important reason for the lack of emulation models for flood inundation modelling is the data availability issue in traditional flood modelling studies observed flood data are used which are often limited to discharge data observed flood level data are generally scarce and when they do get collected it is often for selected sparse locations along the river channel and there are rarely observed flood level data across floodplains with the recent development in 2d hydrodynamic modelling techniques and advancement in computational power especially with high performance computer with graphics processing units gpus becoming more accessible there are more simulated flood inundation data available to inform flood risk assessments this presents a unique opportunity for the development of data driven models for flood inundation modelling this study is proposed to explore this opportunity by using flood inundation data generated from a 2d hydrodynamic model to develop fast flood inundation emulation models although the development of these emulation models relies on outputs from hydrodynamic models once developed they can potentially be used in many flood related decision making applications requiring efficient mapping of flood inundation with time such as real time river operation considering inundation of floodplains and ensemble flood forecasting or uncertainty analysis where hundreds or thousands of model runs may be required this paper aims to contribute to the literature on both flood inundation modelling and emulation modelling using data driven approaches the suitability of one promising emulation modelling approach i e anns is investigated with the hope that it will facilitate the development of practical approaches to inundation modelling although there are many other emulation models anns were selected as it is a commonly used data driven approach and they have a relatively simply structure compared to some deep learning approaches and therefore they are relatively less data intensive the primary objectives of this paper are to i develop a modelling framework based on the state of art ann modelling protocols for flood inundation modelling ii demonstrate the application of the framework via a real world case study and iii outline the major challenges and future research directions in developing ann based emulation models for flood inundation modelling the rest of this paper is organised as follows the real world case study is first introduced in section 2 including description of the study area data availability and data pre processing the ann based modelling framework is then described in section 3 including both the modelling process and the methods used in each step of the process the flood inundation simulation results obtained for the case study system are presented in section 4 based on the modelling process and the results obtained the challenges of applying anns as emulation models for flood inundation modelling and future research directions are discussed in section 5 finally summary and conclusions are presented in section 6 2 case study 2 1 case study description the case study area is part of the burnett river in central queensland australia as shown in fig 1 a the river system has a total catchment area in excess of 33 000 km2 the river flows into the coral sea at burnett heads about 20 km north of bundaberg the flow path is approximately 435 km with a fall of 485 m it is bound to the northeast by the kolan river to the west and southwest by the dawson and condamine rivers and to the south by the brisbane river and the coastal mary river the land use is largely comprised of native grazing vegetation with some native forests a major storage paradise dam is located on the burnett river 80 km upstream from bundaberg and it was completed in 2005 the region has a sub tropical climate with hot moist summers and warm dry winters the basin receives an average of 692 mm of rainfall per year and rainfall is highly seasonal with most rain occurring during the summer season the annual mean potential evaporation 1997 mm is nearly three times the annual mean rainfall over the same period 692 mm the average annual temperature was 22 1 c and it has increased 0 5 c over the last decade from 21 6 c to 22 1 c rising sea levels will increase the risks of coastal hazards such as storm tide inundation this area has always been subject to major flooding flooding of the burnett river in bundaberg peaked at 7 92 m in december 2010 and was above major flood level 7 00 m the river rose to 9 53 m at bundaberg about 7 34 m above highest astronomical tide hat i e 2 19 m in january 2013 http www bom gov au climate current therefore this river has been an area of interest for flood inundation modelling studies the latest of which involves the development of a 2d hydrodynamic model by hydrology and risk consulting harc for sunwater a queensland government owned corporation for water services a 2720 m 2860 m area along the river system downstream of paradise dam and upstream of the city of bundaberg was selected as the study area the pink area in plot a shows the approximate location of the study area and the extent of the study area smaller than the pink area in plot a is shown in plot b this area was selected as it includes a variety of physical features including river channel river bend and an extensive floodplain and a backwater region that is influenced by lateral inflows at downstream locations along the main channel the selected study area is represented by 19 448 20 m 20 m grid cells within the tuflow model the elevation of the selected area is between 0 m and 35 m the blue area in fig 1 a indicates the flood inundation extent of the probable maximum flood pmf of the entire river system the area surrounded by red lines in fig 1 b indicates the flood inundation extent of the pmf in the selected study area 2 2 data description and data pre processing flood data for a total of 10 flood events are available including three historical events i e year 1971 2010 and 2013 and seven design events which represent flood conditions for specific annual exceedance probabilities as shown in fig 2 around 75 of the grid cells i e 14 227 cells out of the 19 448 cells in the study area are inundated within the pmf and the remaining grid cells are dry in all events the flow q data are available at 15 min intervals and water depth d data generated using the tuflow model are available at one or 2 h intervals two hour water depth data were converted into 1 h time intervals from the available data using linear interpolation a summary of the data is provided in table 1 flood inundation data were obtained by using the tuflow 2d hydrodynamic model syme 2001 the model was configured using a grid cell size of 20 m 20 m 19 448 grid cells with a model domain that extended from paradise dam to its discharge point at burnett heads the tuflow model was calibrated to three historical events i e the 1971 2010 and 2013 flood events and run for another seven design events ranging from events with an annual exceedance probability aep of 1 300 up to the probable maximum flood pmf for emulation model development the discharge at paradise dam and tidal conditions at burnett heads are factors relevant to the prediction of flood inundation depths at subsequent time steps obtained from the tuflow model at each grid cell to make full use of the discharge information the original inflows derived at 15 min intervals are used a maximum lag time of 12 h is considered which result in a total of 63 candidate input variables i e 49 inflow input variables with lags at 15 min intervals q t q t 1 q t 48 13 water depth input variables with lags at hourly intervals d t d t 1 d t 12 and the downstream tide level t considered in the input selection process the lead time of the output for ann model is 8 h which was determined from the average time required for water to travel from paradise dam to the centre of the study area 3 ann based emulation modelling framework for flood inundation modelling 3 1 overall framework the ann based emulation modelling framework for flood inundation modelling is shown in fig 3 there are three stages within the framework comprising 1 modelling problem formulation 2 emulation model development and 3 emulation model application in the first stage of modelling problem formulation it is important to first identify the intended purpose of the model then select the study area of interest based on which data will be collected the data are then allocated to the two stages of model development and model application for the second stage of emulation model development the standard ann model development process proposed by wu et al 2014 is recommended the details of this process are discussed in section 3 2 with regard to the third stage of model application it is important to note that most ann modelling studies stop at model validation however in this study we include an additional application step in the modelling to apply the validated models to an independent event that has not been used in any part of the model development process including the validation step as this best represents the manner in which it would be applied in practice the application of the ann based flood inundation modelling framework to the selected study area is further described in detail in the remaining subsections 3 2 stage i modelling problem formulation the overall objective of the modelling exercise is to develop a fast data driven model to simulate the extent and depth of flood inundation over time based on data generated using 2d hydrodynamic models the boundary conditions of tuflow includes the outflows from the dam and the downstream tidal levels the outflow hydrographs from the dam are routed through the tuflow model as a flow over time boundary condition see fig 2 for the major discharges from the 10 events downstream boundary conditions in the tuflow model are represented as a constant water level boundary condition at burnett heads where the water level is set to be at the peak of the highest astronomical tide hat the peak tidal condition is a simplifying assumption that is commonly used in infrastructure design quinn et al 2014 zheng et al 2014 2017 in this study the conservatively high nature of this assumption is of negligible consequence as the majority of the modelled domain is not influenced by tides the input variables include two flood drivers i e discharge at paradise dam and tidal conditions at burnett heads as well as previous flood depth information the output variables are flood depth in 8 h time in the 14 227 wet cells within the study area the adopted resolution of flood inundation levels represented in the emulation models is the same as that of the tuflow model which result in a total of 14 227 data driven models being developed out of the ten flood events available nine were used for model development the 1971 historical event was selected for model application as it is the most complete flood event and its magnitude is within the range of the minimum and maximum size of floods considered in model development in this study the generalized regression neural network grnn model was used specht 1991 as it has a fixed structure and simple development process which suits the calibration of a large number of models in a practical timeframe 3 3 stage ii model development ann models were developed to simulate water depths 8 h ahead i e d t 8 at each of the wet cells using nine out of the ten flood events available excluding the 1971 historical event used for model application the lead time of 8 h was determined by the average travel time of the flood wave from a range of flood events the ann model development process follows the recommendations by wu et al 2014 and is comprised of six major steps input selection data splitting model architecture selection model structure selection model calibration and model validation the ann model development process and associated methods used in each of the major steps are summarised in fig 4 and details on how each of the six major steps carried out in this study are provided in the subsections below 3 3 1 input selection identifying suitable input variables is an important step of ann model development input selection not only identifies the variables that best characterise the functional relationships between inputs and outputs i e input significance but it also prevents redundant information being added into the model i e input independence which avoids the negative impact of these inputs on model performance fernando et al 2009 there are many different algorithms for input selection including search strategies dimensionality reduction wrapper algorithms embedded algorithms filter algorithms and partial mutual information pmi based methods may et al 2011 pmi based methods have proven to be effective for ann development may et al 2008a pmi is estimated based on mutual information mi which measures the mutual dependence between two variables pmi improves dependence estimation over mi in that it also considers the residual information in the two variables based on the selected inputs cigizoglu and alp 2006 which allows the significance of the inputs to be correctly assessed may et al 2008a consequently the pmi based input selection method developed by may et al 2008a was used in this study the akaike information criterion aic akaike 1974 was used as the stopping criterion as used previously by may et al 2008b as mentioned in section 2 there are in total 63 potential input variables considered in the input selection process i e 49 inflow variables with lags at 15 min intervals and 13 water level variables with lags at hourly intervals and the tide level t although reasonably accurate pmi based methods are time consuming to ensure that the input selection process can be completed within a desirable timeframe two assumptions were made 1 the pmi based input selection method was only applied to a sample of cells selected from the total of 14 227 wet cells in total 3244 cells were selected using latin hypercube sampling lhs and the nearest neighbour method was then used to assign input variables to the remaining 10 983 cells 2 the pmi based method was terminated after 10 iterations which assumes that the maximum number of inputs selected for each grid cell does not exceed 10 this assumption is reasonable considering there are approximately 1000 data points available for model development 3 3 2 data splitting to ensure good generalisation ability of ann models the models need to be developed using training data for model calibration and test data for early stopping to prevent overfitting they also need to be validated using other data that have not been seen in the model calibration process maier et al 2010 el shafie and noureldin 2011 bowden et al 2012 data splitting methods are thus required to divide the total datasets into training test and validation sets there are many data splitting methods available including random data splitting methods methods based on the time order of data trial and error methods systematic stratified methods self organizing map based stratified sampling and the duplex method wu et al 2013 the duplex method splits the total datasets based on the maximum euclidean distance to the respective target sets the original duplex method can only generate two subsets including training and test datasets however the method used in this study was extended by may et al 2010 to divide the total datasets into training test and validation datasets based on given proportions it starts to find the pair of data points that lie farthest from each other within the database the first pair is allocated to the training data and the second to the test data and the third to the validation data this process is repeated until the datasets are filled wu et al 2012 although being slightly pessimistic duplex is a deterministic method which does not introduce uncertainty into the data splitting step in the model development process wu et al 2013 and therefore it was selected for this study the duplex method was used to split data from nine events used for model development expect the 1971 event for model application into 60 for training 606 input output samples 20 for test 202 input output samples and 20 202 input output samples for validation 3 3 3 model selection and calibration in this study the generalized regression neural network grnn model was used specht 1991 grnn is a feed forward type of ann which can solve nonlinear approximate problems based on the estimation of a probability distribution function zhou et al 2014 the structure of the grnn model is fixed with four layers input layer pattern layer summation layer and output layer each layer needs to be assigned to a specific computational function when nonlinear regression analysis is performed tayfur et al 2014 grnn has a very fast training process because it only has one parameter the smoothness parameter and its calibration does not require an iterative training procedure as required by back propagation methods ki≈üi 2006 rooki 2016 the grnn was selected for this study so that the large number of models could be developed within a feasible timeframe the grnn models were trained using brent s algorithm press et al 1992 bowden et al 2006 3 3 4 model validation the aim of model validation is to evaluate the usefulness of a developed model when applied for its intended purposes and is an important step of any modelling process it has been well recognised in the environmental modelling community that model validation needs to consider how well a model i predicts the output variable s when applied to data not used to calibrate the model i e predictive validity ii captures the underlying relationships within the calibration data i e replicative validity and iii represents the underlying physical processes of the system i e structure validity humphrey et al 2017 power 1993 wu et al 2014 it is worth noting here that in the majority of ann modelling studies concerning environmental systems only predictive performance is considered while other aspects of model validity especially structure validity are often overlooked humphrey et al 2017 mount et al 2016 wu et al 2014 the examination of all three aspects of model validity is important for developing any models humphrey et al 2017 but it is particularly important for data driven models such as anns since they do not explicitly represent the physical processes of an environmental system solomatine and ostfeld 2008 the details of the methods used to examine each of the three aspects of model validity are described below 1 predictive validity predictive validity is the most commonly considered aspect in the validation of anns in environmental modelling humphrey et al 2017 wu et al 2014 and is examined to ensure the model can generalise over the range of calibration data humphrey et al 2017 predictive validity is assessed by applying the developed ann models to a set of validation data that have not been used in the model calibration process and evaluate their performance using error based or goodness of fit metrics there are numerous metrics available in the literature to assess predictive validity to improve the consistency of these metrics dawson et al 2007 developed a web based tool hydrotest https www hydrotest org uk to evaluate predictive validity using more than 20 metrics two of the metrics included in hydrotest are used in this study fig 4 namely the root mean square error rmse and the median absolute percentage error mdape rmse is based on the squared error and therefore is useful to assess the performance of model over large range of flood magnitudes on the other hand mdape is useful to assess model performance against lower magnitude flood levels and is less sensitive to skewed errors or outliers dawson et al 2007 the formula of rmse and mdape are shown below 1 rmse i 1 m d i d i 2 m 2 mdape median d 1 d 1 d 1 d 2 d 2 d 2 d i d i d i d m d m d m where d i and d i are the simulated flood depth by tuflow model and predicted flood depth by the emulation model at the ith time period respectively and m is the total number data points 2 replicative validity the aim of replicative validation is to ensure the model has captured the underlying relationship in the available data humphrey et al 2017 replicative validity is assessed by examining the residuals of the calibration data to make sure all useful information has been extracted by the model and there is no remaining structure in the residuals that is the residuals represent only white noise and are normally distributed graphical diagnostic plots of the calibration residuals are often used to examine replicative validity in this study plots of standardised residuals against predicted data are used as recommended by humphrey et al 2017 a good replicative standard of validation performance is achieved when most of the standardised calibration residuals are scattered symmetrically about zero and within the 95 confidence bands based on a normal distribution i e 1 96 humphrey et al 2017 3 structure validity structure validation is often omitted in environmental modelling using anns humphrey et al 2017 mount et al 2016 wu et al 2014 structure validation is to ensure the model is plausible when compared with a priori knowledge of the system fig 4 since anns do not explicitly represent the underlying physical processes of environmental systems and the parameters of anns do not directly correspond to any physical variables of the system it is difficult to assess the structure validity of ann models humphrey et al 2017 one commonly used approach to assess structure validity of ann models is to evaluate the relative contributions of different inputs to the derived output s based on connection weights and assess if they make physical sense kingston et al 2005 alternatively sensitivity analysis based approaches can be used to examine the response of the output variables in relation to changes in the input variables the profile method lek et al 1995 is a common sensitivity analysis method used for this purpose and it was adopted in this study as it is quick and easy to apply humphrey et al 2017 3 4 stage iii model application most studies on environmental modelling using anns stop at model validation where performance of the developed model is merely confirmed using validation data the developed models are rarely applied to an independent event with complete time series of rising and falling flooding stages as would occur in real world applications therefore in the framework developed in this study model application is included as the third stage after model development the 1971 historical event was withheld from the model development stage including the validation process and was used to demonstrate the application of the developed models and provide further information on model performance 4 results the performance of the developed ann models was analysed and is presented in terms of predictive replicative and structure validity assessments it should be noted that results of replicative validation and structure validation are only presented for four cells here for ease of description as these results are best assessed visually the four cells are selected to represent areas with different characteristics in the model domain specifically 1 location g1 in the centre of the river channel 2 location g2 in the river channel near significant dem changes 3 location g3 on the floodplain and 4 location g4 in the backwater region that is influenced by lateral inflows at downstream locations along the main channel the locations of these four cells are shown in fig 1 b it should be noted that the inputs were directly selected using pmi for location g3 whereas for the remaining three locations the inputs were assigned using the nearest neighbour method based on inputs selected for a nearby cell as described in section 3 1 in addition for locations g1 and g2 the grid cells are flooded during all ten flood events whereas for locations g3 and g4 the corresponding grid cells are dry 85 and 50 of the time respectively 4 1 predictive validity in order to understand both the overall performance of the developed ann models across all data available as well as their performance over flood events with different magnitudes the collective predictive validation results across all nine events used for model development as well as the predictive performance over each of these nine individual events were examined using the mean rmse and the mean mdape the predicative validation results of three out of these nine events with representative flood magnitudes i e the 2010 historical event and the 1 300 aep and pmf events are presented here for demonstration purposes the results for the other six individual events are included in the supporting material figs s1 and s2 the mean rmse and the mean mdape for all validation data and the three selected events are shown in figs 5 and 6 respectively the mean rmse and mean mdape across all grid cells for all and each of the nine events are summarised in table 2 as can be seen from both figs 5 and 6 overall the models performed well with the mean rmse across all grid cells being under 0 51 m and the mean mdape across all grid cells being under 3 the predictive performance of the models deteriorates as the magnitude of the flood events become larger for example both the rmse and the mdape are significantly higher for the pmf event compared to the other events used for model development figs 5 and 6 and table 2 the model performance over the pmf event reduces the overall performance of the models across all nine events the absolute errors represented by the rmse are larger in the river channels for the pmf due to the larger flood magnitude however these errors are small compared to the total flood depth conversely the relative errors are larger on the floodplain near the flood extent boundaries as flood depths in these regions are small in absolute terms it should also be noted that there is little flood information available for model development in areas which transition between dry and inundated conditions which has contributed to the poorer model performance at these locations the relatively poor performance of the pmf event may also be linked to the grnn model used as grnn is based on density estimation with only one smoothness parameter the little complexity of grnn makes it less suited for simulating extreme events li et al 2014 this issue is also supported by the relatively large difference between the training and validation data it is possible that the adoption of a more complex and flexible ann model architecture such as the multilayer perceptron model could improve model performance however this would likely require more data for training in addition it has always been a challenge to model flood inundation across floodplains using emulation models teng et al 2017 especially in areas where there are rapid changes between dry and inundated conditions the low frequency of flood depth data used in this study i e the hourly or two hourly flood depth data available are not able to capture these rapid changes and thus high frequency flood depth data e g at 15 min intervals as the inflow of good quality are required in order to improve the performance of anns models in these transitional regions 4 2 replicative validity the standardised residual plot was used to investigate the replicative validation performance of the developed models at the four locations selected the results are shown in fig 7 the majority of the standardised residuals for the four models developed for the four locations lie within the 95 confidence interval the grey lines in fig 7 which indicates that the developed models have captured the underlying relationships in the training data used however at locations g3 and g4 there are relatively more data outside the 95 confidence interval at very low flood magnitude i e near zero indicating larger uncertainty in predicting flood levels in areas of the floodplain subject to overbank flows due to insufficient flood data and rapid changes between dry and inundated conditions 4 3 structure validity the structure validity of the developed models was evaluated by examining the profile of changes in the output variable with respect to each input variable generated using the profile method lek et al 1996 young et al 2011 the input variables selected for each of the four selected locations are summarised in table 3 the time series values of these input variables are plotted against that of the output variable i e d t 8 at the four selected locations in fig 8 which shows the correlation between the input variables and the output variable at each selected location it can be seen from the figure that in general there is a positive relationship between predicted water depth and all of the inputs selected this relationship is particularly strong with the current water depth d t at three out of the four locations selected i e g1 g2 and g4 at location g3 which is on the floodplain the relationship between the output variable and the current water depth is more scattered this finding is supported by the fact that on the floodplain the cross section of flooded area becomes larger so a small increase in q does not greatly influence the levels of inundation the selected variables at location g3 include flows at a longer lag compared to locations g1 and g2 i e q t 6 instead of q t in table 3 because it takes a longer time for water to travel to location g3 compared to locations g1 and g2 and only flow above certain threshold values e g flow q t 6 above 40 000 m3 d in fig 8 c result in inundation of the floodplain e g 15 of the time in addition for all locations the output variable responds to all input variables in two distinctive ways as represented by the two different degrees of curvature in the scatter plot between each pair of inputs and outputs these two distinct responses are associated with hysteresis in the rising and falling limbs of the flood hydrographs which implies that these responses may need to be emulated separately the results for structure validity obtained using the profile method for the four selected locations are presented in figs 9 11 each plot shows the input sensitivity profile for a particular variable in each profile plot of input sensitivity a line with a specific colour shows the changes in the output variable when the input variable is varied across its range while the other input variables are kept at a specific constant value the constant values considered include the minimum first quartile median third quartile and maximum values a large positive value indicates a significant positive relationship between the input and the output variables while a large negative value indicates the reverse for location g1 in the centre of the river channel only the current water depth d t and flow q t were selected as inputs see table 3 it is evident from fig 9 that both these inputs have a positive relationship with the output variable which is also confirmed by the scatter plot in fig 8 a in addition when the current flow q t is moderate or in its lower range i e 75 or below the output variable only responds to changes in the current water depth d t up to 75 of its range the yellow red green and purple lines in fig 9 a with very large inflows the output variable is mostly dominated by flow unless the current water depth d t is already above its 80th percentile as shown in fig 9 a similarly the response of the output variable to the low to medium range of flows i e up to 50 is only evident when the current water depth d t is moderate or in its lower range i e 75 or below as shown by the yellow red green and purple lines in fig 9 b when the current water depth d t is at its maximum level the blue line in fig 9 b only increases in the current flows in the range larger than its medium value i e above 50 can lead to increased water depth in 8 h time these results are consistent with the understanding of flood behaviour in the river channel the yellow red green and purple lines in fig 9 a are closely related to the falling limb of the flood events where future flood levels gradually reduce as the current flood level reduces in contrast the yellow red green and purple lines in fig 9 b are closely related to the rising limbs of flood events where future flood depths are related to the increase in both current water depth d t and flow q t the blue lines in both fig 9 a and b are closely related to the most extreme events when the current water depths are already high only very large inflows can result in further increases in flood depth similar results can be found for location g2 see fig s3 in the supporting material which is also in the river channel for location g3 on the floodplain fig 10 the output variable does not respond to the first three most important input variables i e d t d t 1 d t 5 despite the inputs being directly selected based on pmi the only influencing variable is flow 1 5 h before the current time i e q t 6 which results in some responses in the output variable when the other inputs are at relatively high levels i e 75 or above although these results seem to be inconsistent with the input selection results they do match the fact that it takes longer for water to reach location g3 e g 1 5 h longer compared to the other two locations in the river channel in addition the location has a low likelihood of being inundated i e 85 of the time location g3 is completely dry and flooding at the location only occurs in more extreme events i e the blue line in fig 10 d for location g4 in the backwater region see fig 11 the output variable mainly responds to the inputs when they are at their maximum levels these results are supported by the fact that this region is mainly inundated during very large events and it takes a long time e g 2 5 h represented by q t 10 in table 3 for flow to reach the location however there are some modest responses in the output variable to each of the input variables in the lower range of their values indicating this region may also respond to smaller floods with longer durations 4 4 flood inundation mapping the maximum flood inundation extent obtained from the tuflow model and from the ann models for three selected events i e the 2010 1 300 aep and pmf events are plotted in fig 12 a threshold value of 0 01 m was used to filter out low magnitude noise when estimating the number of cells inundated i e a cell with an inundation depth below 0 01 m is considered to be dry the number of grid cells flooded n ranges from a minimum of 3500 cells for the 2010 event to 14 227 cells for the pmf event based on tuflow the corresponding numbers for the ann models are 4 140 4699 and 14 223 respectively the difference in the number of flooded grid cells between tuflow and ann model in these events are 640 392 and 4 respectively which are equivalent to 0 26 0 16 and 0 0016 km2 of flooded area or 4 50 2 75 and 0 03 of the total 5 69 km2 flooded area of the maximum flood extent of the pmf event 14 277 wet cells the results of the other events are included in the supporting material fig s4 4 5 model application the developed models were applied to the 1971 historical event a dataset which was not used in the model development process simulation results for this event across the model domain and at the four selected locations are shown in figs 13 and 14 respectively the total number of flooded grid cells in the 1971 event is 3193 based on the tuflow model and 3546 based on the ann models based on a threshold value of 0 01 m this is equivalent to a 0 14 km2 difference in flooded area or 2 48 of the total inundated area the mean rmse for all the grid cells during model application is 0 16 m and the mean mdape for all grid cells is 17 0 across the 3546 cells flooded in the event the discrepancy in maximum flood extent between tuflow and ann models in the shallow regions in the upstream of the river i e bottom left corner of plots in fig 13 is bigger than that for other locations the time series of water depth simulated using tuflow and anns at locations g1 and g2 in the river channel are shown in fig 14 noting that locations g3 and g4 are completed dry during this event anns generally performed well during the majority time of the flood except for the beginning of the rising limb and the end of the falling limb when anns over estimated the flood depth during the low flow period this is particularly the case for the rising limb at location g2 fig 14 b which is located near the area with significant dem changes in the river channel see fig 1 again this is likely due to the lack of data for model development and the low frequency of data used for model development i e one or 2 h intervals which cannot capture the time series behaviour of floods during times of rapid change the main advantage of emulation models compared to process based numerical models is their efficiency in model application once the models are developed this is particularly the case for 2d hydrodynamic models teng et al 2017 for the case study considered in this study the tuflow model required over 200 h to simulate the 1971 event along the entire length of the river on a server with a cpu intel r xeon r cpu e5 2680 v4 2 40 ghz in contrast the developed ann required only 1 min of cpu to simulate the same event over the study area i e 5 69 km2 or 14 227 grid cells if the ann was used to estimate inundation levels over the entire length of the river i e 644 92 km2 or 1 612 286 grid cells then the simulation time is estimated to take approximately 115 min this difference in simulation times indicates that ann models once developed may be around 100 times faster traditional hydraulic simulations although the efficiency of a 2d hydrodynamic model like tuflow can be improved with a gpu bmt wbm pty ltd 2018 in the short term it may be expected that the increase in speed using gpus will still not be sufficient to facilitate ensemble runs for operational purposes therefore once developed ann based emulation models represent a significant opportunity for improving the efficiency of flood inundation simulations these models have good potential for use in real world applications that require rapid execution time or involve a large number of model runs such as ensemble forecasting or uncertainty analyses 5 challenges and future directions based on the model development process carried out and the results obtained from the case study system five challenges of applying anns to flood inundation modelling are listed and discussed including data availability and quality model development efficiency modelling dynamic effects accounting for uncertainties and modelling future changes potential methods that can be used to address these challenges in future research are also discussed 5 1 data availability and quality 5 1 1 challenges the availability and quality of data used for model development play an important role in the performance and applicability of anns solomatine et al 2008 an ann model can only perform at best as good as the quality of the data used in its development acquiring good quality data with sufficient length and resolution is a challenge for many environmental systems this is particularly the case for flood inundation modelling since water level data are scarce and when they are collected they are never collected in great quantity covering large areas of river channels or floodplains in practice information on inundation levels is commonly obtained from the simulation of historical or design flood events using 2d hydrodynamic models however as the focus of such studies is often on risk analysis or the generation of the maximum flood inundation maps where flood simulations are often stopped after the flood peaks have been reached also water level outputs are often recorded at hourly intervals or longer and this may be too coarse a time step to capture the complex hydraulic dependencies that may occur during times of rapid change accordingly the generation and or collection of data at fine temporal and spatial resolutions will lead to the development of better emulation models 5 1 2 future directions the most expedient way to improve the quality of data available for the development of inundation emulation models is to generate the required input discharge output water depth datasets specifically for this purpose using 2d hydrodynamic models such datasets would overcome any limitations in the range and frequency of results derived from simulations undertaken solely for the purpose of mapping the maximum inundation levels with improved data quality and quantity more complex ann model architecture or other machine learning techniques can be supported in addition the data used need to suit the specific modelling objectives of interest for example for risk based design of infrastructure the peak values of the tidal boundary conditions might be used to derive conservatively high estimates of inundation levels thus the risk is never under estimated however dynamic lower water level boundary conditions do need to be considered if unbiased risk of compound floods leonard et al 2014 wu et al 2018 resulted from multiple drivers is to be estimated which is often the case for operational management in this case the temporal variation for all flood drivers need to be included when generating flood data for emulation model development this approach would require the upskilling of ann modellers in hydrodynamic modelling or else collaboration with the water industry who are familiar with hydrodynamic modelling in addition the utility of the ann models is dependent on the accuracy of the calibrated 2d hydrodynamic model and how well it represents the real world system to apply developed ann models with confidence it is necessary to ensure that the hydraulic models used to develop the anns adequately simulate historical flood behaviour observed water level data are scarce and often sparsely located along the river channel where stream gauges are installed however they do also exist in a variety of other forms for example in the uk high water marks can go back 40 years for most rivers and over a century for major rivers macdonald 2014 photographs and reports from news medias can also be used to produce both quantitative and qualitative flood inundation data smith et al 2012 for future flood events new technologies such as satellites jain et al 2018 or drones may be used to collect flood inundation data across large areas satellite images have already been used to assist flood area identification and flood mapping munasinghe et al 2018 notti et al 2018 skakun 2010 however the use of satellite or drone collected flood data in flood inundation modelling raises a number of theoretical notti et al 2018 and practical restas 2018 issues also the movement of water in rivers is a well understood process and this physical understanding should be included in the development of data driven models to improve model performance creative modelling approaches are required to take advantage of all the information relevant to flood inundation modelling such data may include both quantitative and qualitative information point level data from river gauges and raster data from 2d models and the different temporal and spatial resolutions of flood extent derived from drones or satellite hybrid models that simulate selected components of the system using different modelling methods have shown favourable performance for modelling environmental systems for example moeeni and bonakdari 2017 demonstrate the efficacy of separately simulating the linear and non linear components of the streamflow generation process using a linear regression model and ann in a streamflow forecast study resulting in 12 and 20 improvement in predictive errors compared to either the regression model or ann model alone in a salinity prediction study three key salinity transport processes i e instream salt transport accession of saline groundwater and the flushing of floodplain storages were simulated using a process driven model an ann model and a regression model which resulted in improved performance compared to both the process driven and the ann models used alone hunter et al 2018 in addition fuzzy rules using a series of if then logical expressions have been used to include heuristics in data driven environmental models solomatine et al 2008 they can potentially be used to establish qualitative relationships between qualitative inputs and flood inundation hybrid data driven models which incorporate all available information could thus provide a promising modelling approach for flood inundation modelling 5 2 model development efficiency 5 2 1 challenges to cover the entire model domain a large number of ann models have been developed in this study with one ann model for each 20 m 20 m grid cell i e 14 227 models for the 14 227 inundated grid cells across the study area although the final model application to the 440 h historical event occurred in 1971 was reasonably efficient the model development process was time consuming it is evident from the results presented in section 4 that flood behaviour on many nearby cells are highly correlated which provides considerable scope for improvement of model development efficiency and model application simplicity 5 2 2 future directions perhaps the most straightforward method to improve model development efficiency for flood inundation modelling using anns is to use statistical clustering methods to group nearby cells into small number of regions that are homogeneous with respect to all input and output variables e g flow and water level at different time lags then a single ann model may be developed for each homogenous region thus significantly reducing the number of ann models that need to be developed and applied there are several factors that may influence the successful implementation of this clustering based approach including the choice of the clustering method and the connections between the simulation regions a large number of clustering methods are available in the literature including the traditional statistical model identification methods i e distance based or density based clustering and competitive learning using neural networks i e self organizing map du 2010 given all such approaches are based solely on statistical measures it is prudent to also support the identification of such regions with physical understanding of the hydraulic processes involved although the different simulation regions may be modelled independently in this clustering based approach water levels in these simulation regions are correlated the water surface gradients within each simulation region and the connections between nearby regions need to be considered to avoid unrealistic discontinuities in simulated water levels between nearby regions alternatively dimension reduction methods can be used to reduce the number of variables required to represent a flood inundation area for example instead of representing the flood inundation area using points located at the centre of all inundated grid cells as was done in this study a flood inundation area can be characterised using lines curves polygons or surface patches represented using a few parameters this approach is also referred to as surface parameterisation which has been commonly used in modelling surfaces of objects in computer added design piegl and tiller 1997 the ann models can be developed to map the relationships between flood inputs e g inflows and lower boundary water levels and these derived parameters which allows the relationships between flood inputs and flood inundation to be established however a few issues need to be resolved before dimension reduction approaches can be used first unlike the surfaces of most objects modelled in computer aided design a flood area is highly irregular as it depends on the bathymetry of the river channels and topography of floodplains which makes it difficult to parameterise cook and merwade 2009 second an inundated area is an open surface that can move in any direction across the floodplain which further increases the difficulty of parameterising its extent although photographs have been used to manually delineate shapes representing flooded area smith et al 2012 finding simple practical algorithms to automate the flood surface parameterisation process remains a research challenge due to the irregularity of floodplains a large number of parameters may be required to accurately characterise a flooded area which defeats the purpose of using surface parameterisation to reduce the number of anns required the best way of achieving a balance between the number of parameters used and the accuracy of flood inundation area needs to be investigated if such a dimension reduction approach is used 5 3 dynamic effects 5 3 1 challenges like simplified conceptual models ann models do not explicitly consider the dynamic interactions within a flood system however since the data used to develop ann models can be derived from 2d hydrodynamic models then the hydrodynamic interactions are included in the data and should be implicitly accounted for in ann model development the effectiveness of representing the dynamic interactions in ann models is dependent on the data quality and temporal and spatial resolutions discussed previously a coarse temporal and spatial resolution of the data may not be able to capture the effects of these interactions simulated using a hydrodynamic model 5 3 2 future directions if the temporal and spatial resolution of the data derived from 2d hydrodynamic models are sufficient to capture the effects of the dynamic interactions in the system it should be possible to develop ann flood inundation models that emulate these interactions on a relatively large scale e g across a grid cell of 20 m 20 m the accuracy of these hydrodynamic interactions represented in the ann models is heavily dependent on how well they are represented in the data used for model development the practicability of using anns to capture dynamic interactions in a flood system in a manner that includes consideration of associated variables such as flow velocity remains a future research question 5 4 input and model uncertainties 5 4 1 challenges it is well recognised that environmental models are subject to input model structure and parameter uncertainties vrugt 2016 which have been identified as important factors that are overlooked in early ann modelling practices maier and dandy 2000 in recent studies however methods have been developed to incorporate all three aspects of uncertainty in environmental models developed using anns input uncertainty due to natural variability can be taken into account using probability distributions or ensemble forecasts kasiviswanathan et al 2018 sensitivity analysis based structure validation methods such as the profile method lek et al 1995 used in this study can also be used to provide understanding of contribution of different inputs to predicted output humphrey et al 2017 and therefore can potentially be used to incorporate input uncertainty in ann models on the other hand methods based on bayes theorem are often used to investigate model structure or parameter uncertainty examples include a markov chain monte carlo based optimisation method to investigate model parameter uncertainty guo et al 2013 bayesian neural networks to incorporate input model structure and parameter uncertainties zhang et al 2011 and a bayesian model averaging method to analyse uncertainties resulted from the weights activation functions and number of hidden nodes used in ann models chitsazan et al 2015 one aspect of uncertainty that is often overlooked in environmental modelling not only ann models is human related uncertainty which has been identified to have a significant impact on all stages of the environmental decision making process including modelling ascough et al 2008 uncertainties related to human inputs are related to the knowledge experience and preferences of the modellers refsgaard et al 2005 but can also be extended to cover societal and institutional processes funtowicz and ravetz 1990 de marchi et al 1993 the human aspect of uncertainties is difficult to quantify and its consideration in environmental modelling remains a major challenge 5 4 2 future directions as discussed above methods are available in current literature to incorporate input model structure and model parameter uncertainties into ann based environmental models the applicability and effectiveness of these methods for the development of flood inundation models should be investigated in addition uncertainties related to human inputs should also be considered in flood inundation modelling however significant challenges lie ahead in developing suitable methods to qualify these human related uncertainties before they can be incorporated in the modelling process 5 5 responses to future changes 5 5 1 challenges like any other data driven approaches anns cannot produce reliable extrapolation beyond the range of data used for model calibration therefore it has always been a challenging task to model environmental systems using anns when there are potential future changes to the system here we mainly consider two types of future changes 1 external changes that cause changes to model inputs i e inflows and boundary water levels that may be associated with flood events that are not included in model development and 2 internal changes within the model domain such as changed floodplain topography due to interactions between flood water and floodplain or changes to hydraulic properties of the river due to the construction of new hydraulic structures or failure of existing structures e g dam failure 5 5 2 future directions methods have been developed to cater for short term known changes to system inputs when anns are used for example when new flood data become available in a study by bowden et al 2012 a self organizing map was combined with kernel density estimators to check for new data patterns coming into real time streamflow forecasting models developed using anns once new data patterns that have not been included in model development are identified a warning is issued and the ann models will be retrained to include these new data patterns however for long term unknown changes caused by climate change or population growth a scenario based approach where ann models are trained using data generated under different plausible future scenarios is favoured maier et al 2016 for this approach though the generation of realistic system inputs e g flows and lower boundary water levels under various future scenarios may present a challenge regardless which approach is used retraining of ann models is required in order to cater for external changes in model inputs when there are internal changes to the system within the model domain such as altered floodplain properties due to interactions between flood water and floodplain or changed hydraulic conductivity due to changes to hydraulic structures the underpinning theoretical foundation for data driven models is no longer valid in such situations a process driven model will be more appropriate to model these changes 6 summary and conclusions in this paper an ann based emulation modelling framework for flood inundation modelling has been developed to simulate flood inundation based on information obtained from a 2d hydrodynamic model the modelling framework includes three stages namely modelling problem formulation model development and model application the emulation modelling framework was applied to a segment of a river system in central queensland australia using simulations derived from a 2d hydrodynamic tuflow model in total there are 1010 hourly data points from nine flood events available for emulation model development and 440 hourly data points from one historical flood event available for model application in total 14 227 ann models were developed to cover the inundated region of the study area with each model representing a 20 m 20 m grid cell in the inundated area the results show that overall the ann models can simulate the input output relationships of the system considering all three aspects of model validity i e predictive replicative and structure validity when presented with an independent flood event that was excluded from model development the ann models were found to over estimate flood levels in low flow periods e g the beginning of the rising limb and the end of the falling limb and in transitional regions of dry and wet conditions e g across the normally dry part of a floodplain although this is a common feature for all data driven modelling approaches especially when there are limited data across floodplains the situation could be exacerbated in this study by the simple ann architecture used i e grnn a significant advantage of using ann based emulation models for flood inundation modelling is the significantly reduced computational burden involved in the model application stage therefore anns have the potential to serve as a viable emulation modelling method for flood inundation modelling particularly in real time operation applications requiring rapid model response or in ensemble forecast or uncertainty analyses which require a large number of model runs based on the investigations undertaken for the adopted case study a number of challenges on the application of anns to flood inundation modelling are discussed these include improving data availability and quality better use of existing data improved model development efficiency and application simplicity the accommodation of dynamic effects in flood systems and the need for considering modelling uncertainties due to future changes these challenges need to be addressed or better understood before anns can be used to model flood inundation to assist the analysis of real world problems these challenges can be classified into four categories with each category requiring different levels of complexity in future research development category 1 this category includes challenges that can be addressed using existing methods with little or no modification improving data availability and quality by generating high frequency flood inundation data i e at 15 min intervals using 2d hydraulic dynamic models with all known flood drivers e g flows and storm tides as inputs improved data quantity will also support the use of more complex anns or machine learning techniques further improving model performance simulating dynamic effects and associated variables e g velocity within flood systems by using high quality and high frequency flood data generated from 2d hydrodynamic models where these dynamic effects are already considered this approach is theoretically tractable however the effectiveness of this approach needs to be investigated retraining anns models when new flood information becomes available to respond to known short term changes to system inputs category 2 this category includes challenges that cannot be directly addressed using existing methods however currently available methods can be adapted or revised to deal with these challenges collecting flood inundation data using new technologies such as satellite or drones better utilising all available information including data of different nature coverage and temporal and spatial resolutions using hybrid models improving model development efficiency by using clustering methods to divide flood area into a smaller number of regions based on hydraulic similarity category 3 this category includes challenges that cannot be addressed using existing methods even with significant modification therefore new methods need to be developed to deal with these challenges improving model development efficiency by using surface parameterisation based dimension reduction approaches incorporating into the modelling process the uncertainties due to human inputs related to differences in their knowledge experience and subjective preferences and societal and institutional processes retraining anns models for long term future changes due to climate change or population growth category 4 this category includes challenges that cannot be easily addressed within the data driven modelling paradigm therefore alternative modelling approaches such as process driven models may be required typical examples of this type of challenges include situations where there are physical changes within the flood system that are caused by the interaction between flood water and floodplain during the modelling process or when there is failure of existing hydraulic structures such as dam break which has a significant impact on the hydraulic property of the system in both situations the physical system has undergone significant changes and as a data driven approach anns are no longer applicable solomatine et al 2008 in conclusion we have investigated the applicability of a data driven modelling approach the anns as emulation models for flood inundation modelling the results show that with appropriate modelling process and sufficient data data driven models such as anns can potentially be a fast alternative for flood inundation modelling or the modelling of complex environmental systems in general although there are challenges involved in using data driven methods for flood inundation modelling or environmental modelling we hope that the lessons learned from this study will be useful for ann researchers and data driven modellers in general as they push the frontiers of environmental modelling research declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgement the authors would like to thank hydrology and risk consulting harc for providing the hydraulic simulations on which the analyses are based and sunwater for their permission to use the burnett river as a case study the lead author has been sponsored by the national natural science foundation of china grant number 51459003 as a visitor to the university of melbourne where the research was conducted appendix a supplementary data the following is the supplementary data to this article multimedia component 1 multimedia component 1 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2019 104587 
26087,hydrodynamic models are commonly used to understand flood risk and inform flood management decisions however their high computational cost can impose practical limits on real time flood forecasting and uncertainty analysis which require fast modelling response or many model runs emulation models have the potential to reduce simulation times while still maintaining acceptable accuracy of the estimates in this study we propose an artificial neural networks anns based emulation modelling framework for flood inundation modelling we investigate the suitability of anns as flood inundation models using a river segment in queensland australia our results show that anns can model the time series behaviour of flood inundation and significantly reduce the simulation times required which facilitates their use in applications requiring fast model response or a large number of model runs based the model development process and results the major challenges and future research directions are discussed keywords flood inundation modelling artificial neural networks emulation models meta models surrogate models 1 introduction flooding is one of the most devastating and frequent natural hazards in the world they account for half of the weather related disasters between 1995 and 2015 verwey et al 2017 and are estimated to affect 1 3 billion people globally by 2050 ligtvoet et al 2014 it is important to understand assess and predict flood events and their impact to prevent loss of life and damages to properties one important tool to achieve this goal is flood inundation models which can provide information of flood events such as the extent depth duration and velocity of floods so that the hazard levels exposure and vulnerability of an area can be estimated verwey et al 2017 flood inundation models have been widely used for flood management including flood risk assessment balica et al 2013 flood forecasting and warning casagrande et al 2017 flood hazard mapping merwade et al 2008 flood related engineering infrastructure design schmitt et al 2004 floodplain sediment transport analysis van manh et al 2015 floodplain ecology studies wilson et al 2007 and urban flood management fewtrell et al 2008 commonly used numerical models for flood inundation modelling include one dimensional 1d hydrodynamic models which treat the flow as one dimensional along the centre line of the river channel such as in a confined channel or in a pipe van manh et al 2015 habert et al 2016 two dimensional 2d hydrodynamic models which simulate conservation of mass and momentum in a 2d plane using shallow water equations and 1d one dimensional and 2d coupled models where a 1d model is used to simulate the river channel and the floodplain is modelled using the 2d component bates et al 2010 morales hern√°ndez et al 2013 these 2d or 1d 2d coupled hydrodynamic models provide adequate representation of flow movement along both river channels and across floodplains kim et al 2014 leandro et al 2009 and can model flood inundation in areas with complex dynamic interactions e g around hydraulic structures near confluences or in estuarine rivers there are several software packages available for flood inundation simulation using these models including hecras2d telemac2d tuflow and mike flood however these hydrodynamic models are computationally expensive teng et al 2015 and are not feasible for applications that require rapid model response and or a large number of model runs such applications include real time river operation based on ensemble forecasts alfieri et al 2013 demeritt et al 2007 multi objective optimisation of river operation using evolutionary algorithms dittmann et al 2009 che and mays 2015 and analyses of flood risks using monte carlo or similar methods rahman et al 2002 nathan and ling 2016 therefore there is a need to develop fast alternative modelling approaches to simulate flood inundation one fast alternative modelling approach considered for flood inundation modelling is simplified conceptual models or 0d models these models are based on simplified hydraulic concepts and do not attempt to represent the complex flood generation processes using mathematical equations and therefore are orders of magnitudes faster than hydrodynamic models mcgrath et al 2018 commonly used simplified conceptual flood inundation models include rapid flood spreading method rfsm lhomme et al 2008 the vtd model based on the bathtub method teng et al 2015 and the height above nearest drainage network hand model nobre et al 2011 these models use information from digital elevation models dems to simulate simple filling and spilling processes they have been found to be able to produce near real time simulation of flood inundation extent and depth mcgrath et al 2018 and are therefore useful to produce the maximum flood inundation maps however these models often do not consider the hydrodynamics of flood propagation and therefore cannot capture the time series behaviour of water levels nor can they provide reliable results in systems with complex dynamic interactions teng et al 2017 alternatively data driven approaches can be used to develop emulation models in place of simplified conceptual models for flood inundation modelling data driven emulation models also called surrogate models or meta models have been applied in place of physically based models in several environmental engineering applications ratto et al 2012 razavi et al 2012 timani and peralta 2017 yan and minsker 2006 used anns as emulation models to replace a process based groundwater model and a contaminant transport model to speed up model simulation in groundwater remediation design optimisation later the same approach was used in an uncertainty analysis of groundwater remediation designs based on monte carlo simulation yan and minsker 2011 broad et al 2015 developed ann based emulation models to replace both hydraulic and water quality models of water distribution systems for system design optimisation beh et al 2017 developed ann based emulation models to calculate robustness and other objectives for the sequencing of water supply infrastructure of in adelaide south australia more recently a variety of other data driven techniques have also been used including polynomial regression for optimising remediation design of heterogeneous dnapl contaminated sites qin et al 2007 support vector regression for investigating the impact of model structural error on calibration and prediction of real world groundwater flows xu et al 2017 and extreme learning machine for multi objective optimisations of coastal aquifer managements song et al 2018 data driven emulation models with different complexities have also been used in flood forecasting and management multivariate linear regression models have been used to replace 2d flood models to estimate the total flood volume and peak discharge downstream of the yellow river multiple reservoir system castro gama et al 2014 around the same time yazdi and neyshabouri 2014 integrated the sued of ann models in a genetic algorithm based optimisation process for the optimal design of a flood control dam in the kan watershed in central iran more recently a simple two parametric gamma distribution function was used as an emulation model to simulate the peak discharge flood loss relationships in a large number flood scenarios with low computational cost zischg et al 2018 alternatively gaussian processes hall et al 2011 and anns yu et al 2015 have been used to map the relationships between channel width roughness manning s coefficient and water levels at selected locations of a river in addition a support vector machine was developed to simulate the maximum water depth and velocity in response to different discharge hydrographs at selected locations liu and pender 2015 in most of these studies flood discharge or volume rather than flood level were modelled in the limited studies where flood levels were considered they were only simulated at selected locations whereas flood inundation extent and depth with time remains a challenge for emulation modelling one important reason for the lack of emulation models for flood inundation modelling is the data availability issue in traditional flood modelling studies observed flood data are used which are often limited to discharge data observed flood level data are generally scarce and when they do get collected it is often for selected sparse locations along the river channel and there are rarely observed flood level data across floodplains with the recent development in 2d hydrodynamic modelling techniques and advancement in computational power especially with high performance computer with graphics processing units gpus becoming more accessible there are more simulated flood inundation data available to inform flood risk assessments this presents a unique opportunity for the development of data driven models for flood inundation modelling this study is proposed to explore this opportunity by using flood inundation data generated from a 2d hydrodynamic model to develop fast flood inundation emulation models although the development of these emulation models relies on outputs from hydrodynamic models once developed they can potentially be used in many flood related decision making applications requiring efficient mapping of flood inundation with time such as real time river operation considering inundation of floodplains and ensemble flood forecasting or uncertainty analysis where hundreds or thousands of model runs may be required this paper aims to contribute to the literature on both flood inundation modelling and emulation modelling using data driven approaches the suitability of one promising emulation modelling approach i e anns is investigated with the hope that it will facilitate the development of practical approaches to inundation modelling although there are many other emulation models anns were selected as it is a commonly used data driven approach and they have a relatively simply structure compared to some deep learning approaches and therefore they are relatively less data intensive the primary objectives of this paper are to i develop a modelling framework based on the state of art ann modelling protocols for flood inundation modelling ii demonstrate the application of the framework via a real world case study and iii outline the major challenges and future research directions in developing ann based emulation models for flood inundation modelling the rest of this paper is organised as follows the real world case study is first introduced in section 2 including description of the study area data availability and data pre processing the ann based modelling framework is then described in section 3 including both the modelling process and the methods used in each step of the process the flood inundation simulation results obtained for the case study system are presented in section 4 based on the modelling process and the results obtained the challenges of applying anns as emulation models for flood inundation modelling and future research directions are discussed in section 5 finally summary and conclusions are presented in section 6 2 case study 2 1 case study description the case study area is part of the burnett river in central queensland australia as shown in fig 1 a the river system has a total catchment area in excess of 33 000 km2 the river flows into the coral sea at burnett heads about 20 km north of bundaberg the flow path is approximately 435 km with a fall of 485 m it is bound to the northeast by the kolan river to the west and southwest by the dawson and condamine rivers and to the south by the brisbane river and the coastal mary river the land use is largely comprised of native grazing vegetation with some native forests a major storage paradise dam is located on the burnett river 80 km upstream from bundaberg and it was completed in 2005 the region has a sub tropical climate with hot moist summers and warm dry winters the basin receives an average of 692 mm of rainfall per year and rainfall is highly seasonal with most rain occurring during the summer season the annual mean potential evaporation 1997 mm is nearly three times the annual mean rainfall over the same period 692 mm the average annual temperature was 22 1 c and it has increased 0 5 c over the last decade from 21 6 c to 22 1 c rising sea levels will increase the risks of coastal hazards such as storm tide inundation this area has always been subject to major flooding flooding of the burnett river in bundaberg peaked at 7 92 m in december 2010 and was above major flood level 7 00 m the river rose to 9 53 m at bundaberg about 7 34 m above highest astronomical tide hat i e 2 19 m in january 2013 http www bom gov au climate current therefore this river has been an area of interest for flood inundation modelling studies the latest of which involves the development of a 2d hydrodynamic model by hydrology and risk consulting harc for sunwater a queensland government owned corporation for water services a 2720 m 2860 m area along the river system downstream of paradise dam and upstream of the city of bundaberg was selected as the study area the pink area in plot a shows the approximate location of the study area and the extent of the study area smaller than the pink area in plot a is shown in plot b this area was selected as it includes a variety of physical features including river channel river bend and an extensive floodplain and a backwater region that is influenced by lateral inflows at downstream locations along the main channel the selected study area is represented by 19 448 20 m 20 m grid cells within the tuflow model the elevation of the selected area is between 0 m and 35 m the blue area in fig 1 a indicates the flood inundation extent of the probable maximum flood pmf of the entire river system the area surrounded by red lines in fig 1 b indicates the flood inundation extent of the pmf in the selected study area 2 2 data description and data pre processing flood data for a total of 10 flood events are available including three historical events i e year 1971 2010 and 2013 and seven design events which represent flood conditions for specific annual exceedance probabilities as shown in fig 2 around 75 of the grid cells i e 14 227 cells out of the 19 448 cells in the study area are inundated within the pmf and the remaining grid cells are dry in all events the flow q data are available at 15 min intervals and water depth d data generated using the tuflow model are available at one or 2 h intervals two hour water depth data were converted into 1 h time intervals from the available data using linear interpolation a summary of the data is provided in table 1 flood inundation data were obtained by using the tuflow 2d hydrodynamic model syme 2001 the model was configured using a grid cell size of 20 m 20 m 19 448 grid cells with a model domain that extended from paradise dam to its discharge point at burnett heads the tuflow model was calibrated to three historical events i e the 1971 2010 and 2013 flood events and run for another seven design events ranging from events with an annual exceedance probability aep of 1 300 up to the probable maximum flood pmf for emulation model development the discharge at paradise dam and tidal conditions at burnett heads are factors relevant to the prediction of flood inundation depths at subsequent time steps obtained from the tuflow model at each grid cell to make full use of the discharge information the original inflows derived at 15 min intervals are used a maximum lag time of 12 h is considered which result in a total of 63 candidate input variables i e 49 inflow input variables with lags at 15 min intervals q t q t 1 q t 48 13 water depth input variables with lags at hourly intervals d t d t 1 d t 12 and the downstream tide level t considered in the input selection process the lead time of the output for ann model is 8 h which was determined from the average time required for water to travel from paradise dam to the centre of the study area 3 ann based emulation modelling framework for flood inundation modelling 3 1 overall framework the ann based emulation modelling framework for flood inundation modelling is shown in fig 3 there are three stages within the framework comprising 1 modelling problem formulation 2 emulation model development and 3 emulation model application in the first stage of modelling problem formulation it is important to first identify the intended purpose of the model then select the study area of interest based on which data will be collected the data are then allocated to the two stages of model development and model application for the second stage of emulation model development the standard ann model development process proposed by wu et al 2014 is recommended the details of this process are discussed in section 3 2 with regard to the third stage of model application it is important to note that most ann modelling studies stop at model validation however in this study we include an additional application step in the modelling to apply the validated models to an independent event that has not been used in any part of the model development process including the validation step as this best represents the manner in which it would be applied in practice the application of the ann based flood inundation modelling framework to the selected study area is further described in detail in the remaining subsections 3 2 stage i modelling problem formulation the overall objective of the modelling exercise is to develop a fast data driven model to simulate the extent and depth of flood inundation over time based on data generated using 2d hydrodynamic models the boundary conditions of tuflow includes the outflows from the dam and the downstream tidal levels the outflow hydrographs from the dam are routed through the tuflow model as a flow over time boundary condition see fig 2 for the major discharges from the 10 events downstream boundary conditions in the tuflow model are represented as a constant water level boundary condition at burnett heads where the water level is set to be at the peak of the highest astronomical tide hat the peak tidal condition is a simplifying assumption that is commonly used in infrastructure design quinn et al 2014 zheng et al 2014 2017 in this study the conservatively high nature of this assumption is of negligible consequence as the majority of the modelled domain is not influenced by tides the input variables include two flood drivers i e discharge at paradise dam and tidal conditions at burnett heads as well as previous flood depth information the output variables are flood depth in 8 h time in the 14 227 wet cells within the study area the adopted resolution of flood inundation levels represented in the emulation models is the same as that of the tuflow model which result in a total of 14 227 data driven models being developed out of the ten flood events available nine were used for model development the 1971 historical event was selected for model application as it is the most complete flood event and its magnitude is within the range of the minimum and maximum size of floods considered in model development in this study the generalized regression neural network grnn model was used specht 1991 as it has a fixed structure and simple development process which suits the calibration of a large number of models in a practical timeframe 3 3 stage ii model development ann models were developed to simulate water depths 8 h ahead i e d t 8 at each of the wet cells using nine out of the ten flood events available excluding the 1971 historical event used for model application the lead time of 8 h was determined by the average travel time of the flood wave from a range of flood events the ann model development process follows the recommendations by wu et al 2014 and is comprised of six major steps input selection data splitting model architecture selection model structure selection model calibration and model validation the ann model development process and associated methods used in each of the major steps are summarised in fig 4 and details on how each of the six major steps carried out in this study are provided in the subsections below 3 3 1 input selection identifying suitable input variables is an important step of ann model development input selection not only identifies the variables that best characterise the functional relationships between inputs and outputs i e input significance but it also prevents redundant information being added into the model i e input independence which avoids the negative impact of these inputs on model performance fernando et al 2009 there are many different algorithms for input selection including search strategies dimensionality reduction wrapper algorithms embedded algorithms filter algorithms and partial mutual information pmi based methods may et al 2011 pmi based methods have proven to be effective for ann development may et al 2008a pmi is estimated based on mutual information mi which measures the mutual dependence between two variables pmi improves dependence estimation over mi in that it also considers the residual information in the two variables based on the selected inputs cigizoglu and alp 2006 which allows the significance of the inputs to be correctly assessed may et al 2008a consequently the pmi based input selection method developed by may et al 2008a was used in this study the akaike information criterion aic akaike 1974 was used as the stopping criterion as used previously by may et al 2008b as mentioned in section 2 there are in total 63 potential input variables considered in the input selection process i e 49 inflow variables with lags at 15 min intervals and 13 water level variables with lags at hourly intervals and the tide level t although reasonably accurate pmi based methods are time consuming to ensure that the input selection process can be completed within a desirable timeframe two assumptions were made 1 the pmi based input selection method was only applied to a sample of cells selected from the total of 14 227 wet cells in total 3244 cells were selected using latin hypercube sampling lhs and the nearest neighbour method was then used to assign input variables to the remaining 10 983 cells 2 the pmi based method was terminated after 10 iterations which assumes that the maximum number of inputs selected for each grid cell does not exceed 10 this assumption is reasonable considering there are approximately 1000 data points available for model development 3 3 2 data splitting to ensure good generalisation ability of ann models the models need to be developed using training data for model calibration and test data for early stopping to prevent overfitting they also need to be validated using other data that have not been seen in the model calibration process maier et al 2010 el shafie and noureldin 2011 bowden et al 2012 data splitting methods are thus required to divide the total datasets into training test and validation sets there are many data splitting methods available including random data splitting methods methods based on the time order of data trial and error methods systematic stratified methods self organizing map based stratified sampling and the duplex method wu et al 2013 the duplex method splits the total datasets based on the maximum euclidean distance to the respective target sets the original duplex method can only generate two subsets including training and test datasets however the method used in this study was extended by may et al 2010 to divide the total datasets into training test and validation datasets based on given proportions it starts to find the pair of data points that lie farthest from each other within the database the first pair is allocated to the training data and the second to the test data and the third to the validation data this process is repeated until the datasets are filled wu et al 2012 although being slightly pessimistic duplex is a deterministic method which does not introduce uncertainty into the data splitting step in the model development process wu et al 2013 and therefore it was selected for this study the duplex method was used to split data from nine events used for model development expect the 1971 event for model application into 60 for training 606 input output samples 20 for test 202 input output samples and 20 202 input output samples for validation 3 3 3 model selection and calibration in this study the generalized regression neural network grnn model was used specht 1991 grnn is a feed forward type of ann which can solve nonlinear approximate problems based on the estimation of a probability distribution function zhou et al 2014 the structure of the grnn model is fixed with four layers input layer pattern layer summation layer and output layer each layer needs to be assigned to a specific computational function when nonlinear regression analysis is performed tayfur et al 2014 grnn has a very fast training process because it only has one parameter the smoothness parameter and its calibration does not require an iterative training procedure as required by back propagation methods ki≈üi 2006 rooki 2016 the grnn was selected for this study so that the large number of models could be developed within a feasible timeframe the grnn models were trained using brent s algorithm press et al 1992 bowden et al 2006 3 3 4 model validation the aim of model validation is to evaluate the usefulness of a developed model when applied for its intended purposes and is an important step of any modelling process it has been well recognised in the environmental modelling community that model validation needs to consider how well a model i predicts the output variable s when applied to data not used to calibrate the model i e predictive validity ii captures the underlying relationships within the calibration data i e replicative validity and iii represents the underlying physical processes of the system i e structure validity humphrey et al 2017 power 1993 wu et al 2014 it is worth noting here that in the majority of ann modelling studies concerning environmental systems only predictive performance is considered while other aspects of model validity especially structure validity are often overlooked humphrey et al 2017 mount et al 2016 wu et al 2014 the examination of all three aspects of model validity is important for developing any models humphrey et al 2017 but it is particularly important for data driven models such as anns since they do not explicitly represent the physical processes of an environmental system solomatine and ostfeld 2008 the details of the methods used to examine each of the three aspects of model validity are described below 1 predictive validity predictive validity is the most commonly considered aspect in the validation of anns in environmental modelling humphrey et al 2017 wu et al 2014 and is examined to ensure the model can generalise over the range of calibration data humphrey et al 2017 predictive validity is assessed by applying the developed ann models to a set of validation data that have not been used in the model calibration process and evaluate their performance using error based or goodness of fit metrics there are numerous metrics available in the literature to assess predictive validity to improve the consistency of these metrics dawson et al 2007 developed a web based tool hydrotest https www hydrotest org uk to evaluate predictive validity using more than 20 metrics two of the metrics included in hydrotest are used in this study fig 4 namely the root mean square error rmse and the median absolute percentage error mdape rmse is based on the squared error and therefore is useful to assess the performance of model over large range of flood magnitudes on the other hand mdape is useful to assess model performance against lower magnitude flood levels and is less sensitive to skewed errors or outliers dawson et al 2007 the formula of rmse and mdape are shown below 1 rmse i 1 m d i d i 2 m 2 mdape median d 1 d 1 d 1 d 2 d 2 d 2 d i d i d i d m d m d m where d i and d i are the simulated flood depth by tuflow model and predicted flood depth by the emulation model at the ith time period respectively and m is the total number data points 2 replicative validity the aim of replicative validation is to ensure the model has captured the underlying relationship in the available data humphrey et al 2017 replicative validity is assessed by examining the residuals of the calibration data to make sure all useful information has been extracted by the model and there is no remaining structure in the residuals that is the residuals represent only white noise and are normally distributed graphical diagnostic plots of the calibration residuals are often used to examine replicative validity in this study plots of standardised residuals against predicted data are used as recommended by humphrey et al 2017 a good replicative standard of validation performance is achieved when most of the standardised calibration residuals are scattered symmetrically about zero and within the 95 confidence bands based on a normal distribution i e 1 96 humphrey et al 2017 3 structure validity structure validation is often omitted in environmental modelling using anns humphrey et al 2017 mount et al 2016 wu et al 2014 structure validation is to ensure the model is plausible when compared with a priori knowledge of the system fig 4 since anns do not explicitly represent the underlying physical processes of environmental systems and the parameters of anns do not directly correspond to any physical variables of the system it is difficult to assess the structure validity of ann models humphrey et al 2017 one commonly used approach to assess structure validity of ann models is to evaluate the relative contributions of different inputs to the derived output s based on connection weights and assess if they make physical sense kingston et al 2005 alternatively sensitivity analysis based approaches can be used to examine the response of the output variables in relation to changes in the input variables the profile method lek et al 1995 is a common sensitivity analysis method used for this purpose and it was adopted in this study as it is quick and easy to apply humphrey et al 2017 3 4 stage iii model application most studies on environmental modelling using anns stop at model validation where performance of the developed model is merely confirmed using validation data the developed models are rarely applied to an independent event with complete time series of rising and falling flooding stages as would occur in real world applications therefore in the framework developed in this study model application is included as the third stage after model development the 1971 historical event was withheld from the model development stage including the validation process and was used to demonstrate the application of the developed models and provide further information on model performance 4 results the performance of the developed ann models was analysed and is presented in terms of predictive replicative and structure validity assessments it should be noted that results of replicative validation and structure validation are only presented for four cells here for ease of description as these results are best assessed visually the four cells are selected to represent areas with different characteristics in the model domain specifically 1 location g1 in the centre of the river channel 2 location g2 in the river channel near significant dem changes 3 location g3 on the floodplain and 4 location g4 in the backwater region that is influenced by lateral inflows at downstream locations along the main channel the locations of these four cells are shown in fig 1 b it should be noted that the inputs were directly selected using pmi for location g3 whereas for the remaining three locations the inputs were assigned using the nearest neighbour method based on inputs selected for a nearby cell as described in section 3 1 in addition for locations g1 and g2 the grid cells are flooded during all ten flood events whereas for locations g3 and g4 the corresponding grid cells are dry 85 and 50 of the time respectively 4 1 predictive validity in order to understand both the overall performance of the developed ann models across all data available as well as their performance over flood events with different magnitudes the collective predictive validation results across all nine events used for model development as well as the predictive performance over each of these nine individual events were examined using the mean rmse and the mean mdape the predicative validation results of three out of these nine events with representative flood magnitudes i e the 2010 historical event and the 1 300 aep and pmf events are presented here for demonstration purposes the results for the other six individual events are included in the supporting material figs s1 and s2 the mean rmse and the mean mdape for all validation data and the three selected events are shown in figs 5 and 6 respectively the mean rmse and mean mdape across all grid cells for all and each of the nine events are summarised in table 2 as can be seen from both figs 5 and 6 overall the models performed well with the mean rmse across all grid cells being under 0 51 m and the mean mdape across all grid cells being under 3 the predictive performance of the models deteriorates as the magnitude of the flood events become larger for example both the rmse and the mdape are significantly higher for the pmf event compared to the other events used for model development figs 5 and 6 and table 2 the model performance over the pmf event reduces the overall performance of the models across all nine events the absolute errors represented by the rmse are larger in the river channels for the pmf due to the larger flood magnitude however these errors are small compared to the total flood depth conversely the relative errors are larger on the floodplain near the flood extent boundaries as flood depths in these regions are small in absolute terms it should also be noted that there is little flood information available for model development in areas which transition between dry and inundated conditions which has contributed to the poorer model performance at these locations the relatively poor performance of the pmf event may also be linked to the grnn model used as grnn is based on density estimation with only one smoothness parameter the little complexity of grnn makes it less suited for simulating extreme events li et al 2014 this issue is also supported by the relatively large difference between the training and validation data it is possible that the adoption of a more complex and flexible ann model architecture such as the multilayer perceptron model could improve model performance however this would likely require more data for training in addition it has always been a challenge to model flood inundation across floodplains using emulation models teng et al 2017 especially in areas where there are rapid changes between dry and inundated conditions the low frequency of flood depth data used in this study i e the hourly or two hourly flood depth data available are not able to capture these rapid changes and thus high frequency flood depth data e g at 15 min intervals as the inflow of good quality are required in order to improve the performance of anns models in these transitional regions 4 2 replicative validity the standardised residual plot was used to investigate the replicative validation performance of the developed models at the four locations selected the results are shown in fig 7 the majority of the standardised residuals for the four models developed for the four locations lie within the 95 confidence interval the grey lines in fig 7 which indicates that the developed models have captured the underlying relationships in the training data used however at locations g3 and g4 there are relatively more data outside the 95 confidence interval at very low flood magnitude i e near zero indicating larger uncertainty in predicting flood levels in areas of the floodplain subject to overbank flows due to insufficient flood data and rapid changes between dry and inundated conditions 4 3 structure validity the structure validity of the developed models was evaluated by examining the profile of changes in the output variable with respect to each input variable generated using the profile method lek et al 1996 young et al 2011 the input variables selected for each of the four selected locations are summarised in table 3 the time series values of these input variables are plotted against that of the output variable i e d t 8 at the four selected locations in fig 8 which shows the correlation between the input variables and the output variable at each selected location it can be seen from the figure that in general there is a positive relationship between predicted water depth and all of the inputs selected this relationship is particularly strong with the current water depth d t at three out of the four locations selected i e g1 g2 and g4 at location g3 which is on the floodplain the relationship between the output variable and the current water depth is more scattered this finding is supported by the fact that on the floodplain the cross section of flooded area becomes larger so a small increase in q does not greatly influence the levels of inundation the selected variables at location g3 include flows at a longer lag compared to locations g1 and g2 i e q t 6 instead of q t in table 3 because it takes a longer time for water to travel to location g3 compared to locations g1 and g2 and only flow above certain threshold values e g flow q t 6 above 40 000 m3 d in fig 8 c result in inundation of the floodplain e g 15 of the time in addition for all locations the output variable responds to all input variables in two distinctive ways as represented by the two different degrees of curvature in the scatter plot between each pair of inputs and outputs these two distinct responses are associated with hysteresis in the rising and falling limbs of the flood hydrographs which implies that these responses may need to be emulated separately the results for structure validity obtained using the profile method for the four selected locations are presented in figs 9 11 each plot shows the input sensitivity profile for a particular variable in each profile plot of input sensitivity a line with a specific colour shows the changes in the output variable when the input variable is varied across its range while the other input variables are kept at a specific constant value the constant values considered include the minimum first quartile median third quartile and maximum values a large positive value indicates a significant positive relationship between the input and the output variables while a large negative value indicates the reverse for location g1 in the centre of the river channel only the current water depth d t and flow q t were selected as inputs see table 3 it is evident from fig 9 that both these inputs have a positive relationship with the output variable which is also confirmed by the scatter plot in fig 8 a in addition when the current flow q t is moderate or in its lower range i e 75 or below the output variable only responds to changes in the current water depth d t up to 75 of its range the yellow red green and purple lines in fig 9 a with very large inflows the output variable is mostly dominated by flow unless the current water depth d t is already above its 80th percentile as shown in fig 9 a similarly the response of the output variable to the low to medium range of flows i e up to 50 is only evident when the current water depth d t is moderate or in its lower range i e 75 or below as shown by the yellow red green and purple lines in fig 9 b when the current water depth d t is at its maximum level the blue line in fig 9 b only increases in the current flows in the range larger than its medium value i e above 50 can lead to increased water depth in 8 h time these results are consistent with the understanding of flood behaviour in the river channel the yellow red green and purple lines in fig 9 a are closely related to the falling limb of the flood events where future flood levels gradually reduce as the current flood level reduces in contrast the yellow red green and purple lines in fig 9 b are closely related to the rising limbs of flood events where future flood depths are related to the increase in both current water depth d t and flow q t the blue lines in both fig 9 a and b are closely related to the most extreme events when the current water depths are already high only very large inflows can result in further increases in flood depth similar results can be found for location g2 see fig s3 in the supporting material which is also in the river channel for location g3 on the floodplain fig 10 the output variable does not respond to the first three most important input variables i e d t d t 1 d t 5 despite the inputs being directly selected based on pmi the only influencing variable is flow 1 5 h before the current time i e q t 6 which results in some responses in the output variable when the other inputs are at relatively high levels i e 75 or above although these results seem to be inconsistent with the input selection results they do match the fact that it takes longer for water to reach location g3 e g 1 5 h longer compared to the other two locations in the river channel in addition the location has a low likelihood of being inundated i e 85 of the time location g3 is completely dry and flooding at the location only occurs in more extreme events i e the blue line in fig 10 d for location g4 in the backwater region see fig 11 the output variable mainly responds to the inputs when they are at their maximum levels these results are supported by the fact that this region is mainly inundated during very large events and it takes a long time e g 2 5 h represented by q t 10 in table 3 for flow to reach the location however there are some modest responses in the output variable to each of the input variables in the lower range of their values indicating this region may also respond to smaller floods with longer durations 4 4 flood inundation mapping the maximum flood inundation extent obtained from the tuflow model and from the ann models for three selected events i e the 2010 1 300 aep and pmf events are plotted in fig 12 a threshold value of 0 01 m was used to filter out low magnitude noise when estimating the number of cells inundated i e a cell with an inundation depth below 0 01 m is considered to be dry the number of grid cells flooded n ranges from a minimum of 3500 cells for the 2010 event to 14 227 cells for the pmf event based on tuflow the corresponding numbers for the ann models are 4 140 4699 and 14 223 respectively the difference in the number of flooded grid cells between tuflow and ann model in these events are 640 392 and 4 respectively which are equivalent to 0 26 0 16 and 0 0016 km2 of flooded area or 4 50 2 75 and 0 03 of the total 5 69 km2 flooded area of the maximum flood extent of the pmf event 14 277 wet cells the results of the other events are included in the supporting material fig s4 4 5 model application the developed models were applied to the 1971 historical event a dataset which was not used in the model development process simulation results for this event across the model domain and at the four selected locations are shown in figs 13 and 14 respectively the total number of flooded grid cells in the 1971 event is 3193 based on the tuflow model and 3546 based on the ann models based on a threshold value of 0 01 m this is equivalent to a 0 14 km2 difference in flooded area or 2 48 of the total inundated area the mean rmse for all the grid cells during model application is 0 16 m and the mean mdape for all grid cells is 17 0 across the 3546 cells flooded in the event the discrepancy in maximum flood extent between tuflow and ann models in the shallow regions in the upstream of the river i e bottom left corner of plots in fig 13 is bigger than that for other locations the time series of water depth simulated using tuflow and anns at locations g1 and g2 in the river channel are shown in fig 14 noting that locations g3 and g4 are completed dry during this event anns generally performed well during the majority time of the flood except for the beginning of the rising limb and the end of the falling limb when anns over estimated the flood depth during the low flow period this is particularly the case for the rising limb at location g2 fig 14 b which is located near the area with significant dem changes in the river channel see fig 1 again this is likely due to the lack of data for model development and the low frequency of data used for model development i e one or 2 h intervals which cannot capture the time series behaviour of floods during times of rapid change the main advantage of emulation models compared to process based numerical models is their efficiency in model application once the models are developed this is particularly the case for 2d hydrodynamic models teng et al 2017 for the case study considered in this study the tuflow model required over 200 h to simulate the 1971 event along the entire length of the river on a server with a cpu intel r xeon r cpu e5 2680 v4 2 40 ghz in contrast the developed ann required only 1 min of cpu to simulate the same event over the study area i e 5 69 km2 or 14 227 grid cells if the ann was used to estimate inundation levels over the entire length of the river i e 644 92 km2 or 1 612 286 grid cells then the simulation time is estimated to take approximately 115 min this difference in simulation times indicates that ann models once developed may be around 100 times faster traditional hydraulic simulations although the efficiency of a 2d hydrodynamic model like tuflow can be improved with a gpu bmt wbm pty ltd 2018 in the short term it may be expected that the increase in speed using gpus will still not be sufficient to facilitate ensemble runs for operational purposes therefore once developed ann based emulation models represent a significant opportunity for improving the efficiency of flood inundation simulations these models have good potential for use in real world applications that require rapid execution time or involve a large number of model runs such as ensemble forecasting or uncertainty analyses 5 challenges and future directions based on the model development process carried out and the results obtained from the case study system five challenges of applying anns to flood inundation modelling are listed and discussed including data availability and quality model development efficiency modelling dynamic effects accounting for uncertainties and modelling future changes potential methods that can be used to address these challenges in future research are also discussed 5 1 data availability and quality 5 1 1 challenges the availability and quality of data used for model development play an important role in the performance and applicability of anns solomatine et al 2008 an ann model can only perform at best as good as the quality of the data used in its development acquiring good quality data with sufficient length and resolution is a challenge for many environmental systems this is particularly the case for flood inundation modelling since water level data are scarce and when they are collected they are never collected in great quantity covering large areas of river channels or floodplains in practice information on inundation levels is commonly obtained from the simulation of historical or design flood events using 2d hydrodynamic models however as the focus of such studies is often on risk analysis or the generation of the maximum flood inundation maps where flood simulations are often stopped after the flood peaks have been reached also water level outputs are often recorded at hourly intervals or longer and this may be too coarse a time step to capture the complex hydraulic dependencies that may occur during times of rapid change accordingly the generation and or collection of data at fine temporal and spatial resolutions will lead to the development of better emulation models 5 1 2 future directions the most expedient way to improve the quality of data available for the development of inundation emulation models is to generate the required input discharge output water depth datasets specifically for this purpose using 2d hydrodynamic models such datasets would overcome any limitations in the range and frequency of results derived from simulations undertaken solely for the purpose of mapping the maximum inundation levels with improved data quality and quantity more complex ann model architecture or other machine learning techniques can be supported in addition the data used need to suit the specific modelling objectives of interest for example for risk based design of infrastructure the peak values of the tidal boundary conditions might be used to derive conservatively high estimates of inundation levels thus the risk is never under estimated however dynamic lower water level boundary conditions do need to be considered if unbiased risk of compound floods leonard et al 2014 wu et al 2018 resulted from multiple drivers is to be estimated which is often the case for operational management in this case the temporal variation for all flood drivers need to be included when generating flood data for emulation model development this approach would require the upskilling of ann modellers in hydrodynamic modelling or else collaboration with the water industry who are familiar with hydrodynamic modelling in addition the utility of the ann models is dependent on the accuracy of the calibrated 2d hydrodynamic model and how well it represents the real world system to apply developed ann models with confidence it is necessary to ensure that the hydraulic models used to develop the anns adequately simulate historical flood behaviour observed water level data are scarce and often sparsely located along the river channel where stream gauges are installed however they do also exist in a variety of other forms for example in the uk high water marks can go back 40 years for most rivers and over a century for major rivers macdonald 2014 photographs and reports from news medias can also be used to produce both quantitative and qualitative flood inundation data smith et al 2012 for future flood events new technologies such as satellites jain et al 2018 or drones may be used to collect flood inundation data across large areas satellite images have already been used to assist flood area identification and flood mapping munasinghe et al 2018 notti et al 2018 skakun 2010 however the use of satellite or drone collected flood data in flood inundation modelling raises a number of theoretical notti et al 2018 and practical restas 2018 issues also the movement of water in rivers is a well understood process and this physical understanding should be included in the development of data driven models to improve model performance creative modelling approaches are required to take advantage of all the information relevant to flood inundation modelling such data may include both quantitative and qualitative information point level data from river gauges and raster data from 2d models and the different temporal and spatial resolutions of flood extent derived from drones or satellite hybrid models that simulate selected components of the system using different modelling methods have shown favourable performance for modelling environmental systems for example moeeni and bonakdari 2017 demonstrate the efficacy of separately simulating the linear and non linear components of the streamflow generation process using a linear regression model and ann in a streamflow forecast study resulting in 12 and 20 improvement in predictive errors compared to either the regression model or ann model alone in a salinity prediction study three key salinity transport processes i e instream salt transport accession of saline groundwater and the flushing of floodplain storages were simulated using a process driven model an ann model and a regression model which resulted in improved performance compared to both the process driven and the ann models used alone hunter et al 2018 in addition fuzzy rules using a series of if then logical expressions have been used to include heuristics in data driven environmental models solomatine et al 2008 they can potentially be used to establish qualitative relationships between qualitative inputs and flood inundation hybrid data driven models which incorporate all available information could thus provide a promising modelling approach for flood inundation modelling 5 2 model development efficiency 5 2 1 challenges to cover the entire model domain a large number of ann models have been developed in this study with one ann model for each 20 m 20 m grid cell i e 14 227 models for the 14 227 inundated grid cells across the study area although the final model application to the 440 h historical event occurred in 1971 was reasonably efficient the model development process was time consuming it is evident from the results presented in section 4 that flood behaviour on many nearby cells are highly correlated which provides considerable scope for improvement of model development efficiency and model application simplicity 5 2 2 future directions perhaps the most straightforward method to improve model development efficiency for flood inundation modelling using anns is to use statistical clustering methods to group nearby cells into small number of regions that are homogeneous with respect to all input and output variables e g flow and water level at different time lags then a single ann model may be developed for each homogenous region thus significantly reducing the number of ann models that need to be developed and applied there are several factors that may influence the successful implementation of this clustering based approach including the choice of the clustering method and the connections between the simulation regions a large number of clustering methods are available in the literature including the traditional statistical model identification methods i e distance based or density based clustering and competitive learning using neural networks i e self organizing map du 2010 given all such approaches are based solely on statistical measures it is prudent to also support the identification of such regions with physical understanding of the hydraulic processes involved although the different simulation regions may be modelled independently in this clustering based approach water levels in these simulation regions are correlated the water surface gradients within each simulation region and the connections between nearby regions need to be considered to avoid unrealistic discontinuities in simulated water levels between nearby regions alternatively dimension reduction methods can be used to reduce the number of variables required to represent a flood inundation area for example instead of representing the flood inundation area using points located at the centre of all inundated grid cells as was done in this study a flood inundation area can be characterised using lines curves polygons or surface patches represented using a few parameters this approach is also referred to as surface parameterisation which has been commonly used in modelling surfaces of objects in computer added design piegl and tiller 1997 the ann models can be developed to map the relationships between flood inputs e g inflows and lower boundary water levels and these derived parameters which allows the relationships between flood inputs and flood inundation to be established however a few issues need to be resolved before dimension reduction approaches can be used first unlike the surfaces of most objects modelled in computer aided design a flood area is highly irregular as it depends on the bathymetry of the river channels and topography of floodplains which makes it difficult to parameterise cook and merwade 2009 second an inundated area is an open surface that can move in any direction across the floodplain which further increases the difficulty of parameterising its extent although photographs have been used to manually delineate shapes representing flooded area smith et al 2012 finding simple practical algorithms to automate the flood surface parameterisation process remains a research challenge due to the irregularity of floodplains a large number of parameters may be required to accurately characterise a flooded area which defeats the purpose of using surface parameterisation to reduce the number of anns required the best way of achieving a balance between the number of parameters used and the accuracy of flood inundation area needs to be investigated if such a dimension reduction approach is used 5 3 dynamic effects 5 3 1 challenges like simplified conceptual models ann models do not explicitly consider the dynamic interactions within a flood system however since the data used to develop ann models can be derived from 2d hydrodynamic models then the hydrodynamic interactions are included in the data and should be implicitly accounted for in ann model development the effectiveness of representing the dynamic interactions in ann models is dependent on the data quality and temporal and spatial resolutions discussed previously a coarse temporal and spatial resolution of the data may not be able to capture the effects of these interactions simulated using a hydrodynamic model 5 3 2 future directions if the temporal and spatial resolution of the data derived from 2d hydrodynamic models are sufficient to capture the effects of the dynamic interactions in the system it should be possible to develop ann flood inundation models that emulate these interactions on a relatively large scale e g across a grid cell of 20 m 20 m the accuracy of these hydrodynamic interactions represented in the ann models is heavily dependent on how well they are represented in the data used for model development the practicability of using anns to capture dynamic interactions in a flood system in a manner that includes consideration of associated variables such as flow velocity remains a future research question 5 4 input and model uncertainties 5 4 1 challenges it is well recognised that environmental models are subject to input model structure and parameter uncertainties vrugt 2016 which have been identified as important factors that are overlooked in early ann modelling practices maier and dandy 2000 in recent studies however methods have been developed to incorporate all three aspects of uncertainty in environmental models developed using anns input uncertainty due to natural variability can be taken into account using probability distributions or ensemble forecasts kasiviswanathan et al 2018 sensitivity analysis based structure validation methods such as the profile method lek et al 1995 used in this study can also be used to provide understanding of contribution of different inputs to predicted output humphrey et al 2017 and therefore can potentially be used to incorporate input uncertainty in ann models on the other hand methods based on bayes theorem are often used to investigate model structure or parameter uncertainty examples include a markov chain monte carlo based optimisation method to investigate model parameter uncertainty guo et al 2013 bayesian neural networks to incorporate input model structure and parameter uncertainties zhang et al 2011 and a bayesian model averaging method to analyse uncertainties resulted from the weights activation functions and number of hidden nodes used in ann models chitsazan et al 2015 one aspect of uncertainty that is often overlooked in environmental modelling not only ann models is human related uncertainty which has been identified to have a significant impact on all stages of the environmental decision making process including modelling ascough et al 2008 uncertainties related to human inputs are related to the knowledge experience and preferences of the modellers refsgaard et al 2005 but can also be extended to cover societal and institutional processes funtowicz and ravetz 1990 de marchi et al 1993 the human aspect of uncertainties is difficult to quantify and its consideration in environmental modelling remains a major challenge 5 4 2 future directions as discussed above methods are available in current literature to incorporate input model structure and model parameter uncertainties into ann based environmental models the applicability and effectiveness of these methods for the development of flood inundation models should be investigated in addition uncertainties related to human inputs should also be considered in flood inundation modelling however significant challenges lie ahead in developing suitable methods to qualify these human related uncertainties before they can be incorporated in the modelling process 5 5 responses to future changes 5 5 1 challenges like any other data driven approaches anns cannot produce reliable extrapolation beyond the range of data used for model calibration therefore it has always been a challenging task to model environmental systems using anns when there are potential future changes to the system here we mainly consider two types of future changes 1 external changes that cause changes to model inputs i e inflows and boundary water levels that may be associated with flood events that are not included in model development and 2 internal changes within the model domain such as changed floodplain topography due to interactions between flood water and floodplain or changes to hydraulic properties of the river due to the construction of new hydraulic structures or failure of existing structures e g dam failure 5 5 2 future directions methods have been developed to cater for short term known changes to system inputs when anns are used for example when new flood data become available in a study by bowden et al 2012 a self organizing map was combined with kernel density estimators to check for new data patterns coming into real time streamflow forecasting models developed using anns once new data patterns that have not been included in model development are identified a warning is issued and the ann models will be retrained to include these new data patterns however for long term unknown changes caused by climate change or population growth a scenario based approach where ann models are trained using data generated under different plausible future scenarios is favoured maier et al 2016 for this approach though the generation of realistic system inputs e g flows and lower boundary water levels under various future scenarios may present a challenge regardless which approach is used retraining of ann models is required in order to cater for external changes in model inputs when there are internal changes to the system within the model domain such as altered floodplain properties due to interactions between flood water and floodplain or changed hydraulic conductivity due to changes to hydraulic structures the underpinning theoretical foundation for data driven models is no longer valid in such situations a process driven model will be more appropriate to model these changes 6 summary and conclusions in this paper an ann based emulation modelling framework for flood inundation modelling has been developed to simulate flood inundation based on information obtained from a 2d hydrodynamic model the modelling framework includes three stages namely modelling problem formulation model development and model application the emulation modelling framework was applied to a segment of a river system in central queensland australia using simulations derived from a 2d hydrodynamic tuflow model in total there are 1010 hourly data points from nine flood events available for emulation model development and 440 hourly data points from one historical flood event available for model application in total 14 227 ann models were developed to cover the inundated region of the study area with each model representing a 20 m 20 m grid cell in the inundated area the results show that overall the ann models can simulate the input output relationships of the system considering all three aspects of model validity i e predictive replicative and structure validity when presented with an independent flood event that was excluded from model development the ann models were found to over estimate flood levels in low flow periods e g the beginning of the rising limb and the end of the falling limb and in transitional regions of dry and wet conditions e g across the normally dry part of a floodplain although this is a common feature for all data driven modelling approaches especially when there are limited data across floodplains the situation could be exacerbated in this study by the simple ann architecture used i e grnn a significant advantage of using ann based emulation models for flood inundation modelling is the significantly reduced computational burden involved in the model application stage therefore anns have the potential to serve as a viable emulation modelling method for flood inundation modelling particularly in real time operation applications requiring rapid model response or in ensemble forecast or uncertainty analyses which require a large number of model runs based on the investigations undertaken for the adopted case study a number of challenges on the application of anns to flood inundation modelling are discussed these include improving data availability and quality better use of existing data improved model development efficiency and application simplicity the accommodation of dynamic effects in flood systems and the need for considering modelling uncertainties due to future changes these challenges need to be addressed or better understood before anns can be used to model flood inundation to assist the analysis of real world problems these challenges can be classified into four categories with each category requiring different levels of complexity in future research development category 1 this category includes challenges that can be addressed using existing methods with little or no modification improving data availability and quality by generating high frequency flood inundation data i e at 15 min intervals using 2d hydraulic dynamic models with all known flood drivers e g flows and storm tides as inputs improved data quantity will also support the use of more complex anns or machine learning techniques further improving model performance simulating dynamic effects and associated variables e g velocity within flood systems by using high quality and high frequency flood data generated from 2d hydrodynamic models where these dynamic effects are already considered this approach is theoretically tractable however the effectiveness of this approach needs to be investigated retraining anns models when new flood information becomes available to respond to known short term changes to system inputs category 2 this category includes challenges that cannot be directly addressed using existing methods however currently available methods can be adapted or revised to deal with these challenges collecting flood inundation data using new technologies such as satellite or drones better utilising all available information including data of different nature coverage and temporal and spatial resolutions using hybrid models improving model development efficiency by using clustering methods to divide flood area into a smaller number of regions based on hydraulic similarity category 3 this category includes challenges that cannot be addressed using existing methods even with significant modification therefore new methods need to be developed to deal with these challenges improving model development efficiency by using surface parameterisation based dimension reduction approaches incorporating into the modelling process the uncertainties due to human inputs related to differences in their knowledge experience and subjective preferences and societal and institutional processes retraining anns models for long term future changes due to climate change or population growth category 4 this category includes challenges that cannot be easily addressed within the data driven modelling paradigm therefore alternative modelling approaches such as process driven models may be required typical examples of this type of challenges include situations where there are physical changes within the flood system that are caused by the interaction between flood water and floodplain during the modelling process or when there is failure of existing hydraulic structures such as dam break which has a significant impact on the hydraulic property of the system in both situations the physical system has undergone significant changes and as a data driven approach anns are no longer applicable solomatine et al 2008 in conclusion we have investigated the applicability of a data driven modelling approach the anns as emulation models for flood inundation modelling the results show that with appropriate modelling process and sufficient data data driven models such as anns can potentially be a fast alternative for flood inundation modelling or the modelling of complex environmental systems in general although there are challenges involved in using data driven methods for flood inundation modelling or environmental modelling we hope that the lessons learned from this study will be useful for ann researchers and data driven modellers in general as they push the frontiers of environmental modelling research declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgement the authors would like to thank hydrology and risk consulting harc for providing the hydraulic simulations on which the analyses are based and sunwater for their permission to use the burnett river as a case study the lead author has been sponsored by the national natural science foundation of china grant number 51459003 as a visitor to the university of melbourne where the research was conducted appendix a supplementary data the following is the supplementary data to this article multimedia component 1 multimedia component 1 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2019 104587 
26088,we present in this paper the development of a new open source matlab toolbox srs gda that aims to provide random spatial sampling of grid based hydro climatic datasets for environmental change studies this toolbox addresses the needs of quantifying how hydro climatic responses which are often driven by grid based forcing datasets such as climate model projections vary with location and scale the toolbox can be used to carry out random spatial sampling of grid based quantities with various constraints shape size location dominant orientation and resolution a case study of a large dataset the gear rainfall dataset is supplied to demonstrate the typical uses case of this toolbox the provision of the toolbox for downloading together with the sample data is also presented keywords spatial random sampling grid based data analysis environment change matlab toolbox open source software 1 introduction research in environmental changes has been increasingly relying on computer models driven by external forcing field and conditions that can represent changing factors such as temperature precipitation and land uses erler et al 2019 alamou et al 2017 historically the inputs used to drive these models were often relatively scarce and only available at limited number of locations data collection was often restricted by technical conditions instruments and means of storage to make full use of such finite data many methods have been proposed and applied in terms of rainfall data there are many methods for translating point rainfall records which are usually collected from hydrological gauges or stations to the basin rainfall for example the areal reduction factor arf has been widely used possibly under different names in different countries weather bureau 1958 nerc 1977 more recently however with the rapid advances in environmental monitoring technology spatially disaggregated grid based hydro climatic datasets have become gradually available with steady improvements in both accuracy and spatial temporal resolutions a typical case is the nimrod weather radar system deployed by the uk met office which can provide up to 1km 5min precipitation distribution over the country golding 1998 fairman et al 2017 similarly satellite borne observations such as the global precipitation measurements gpm islam et al 2014 ning et al 2016 can now provide large scale coverage of the precipitation coverage in near real time many environmental models nowadays are also tuned to make use of those new grid based high resolution datasets e g the grid to grid version of the pdm model has been developed and operationally used by the environment agency in the uk cole and moore 2008 kay et al 2009 another important source of external forcing data is model simulated hydro climatic fields in this case rainfall temperature as well as soil moisture fields generated by numerical weather models or climate models can be used to drive other model simulations practices of using the so called coupled model approach started to gain momentum in the early 2000 s when numerical weather models and climate models were able to produce simulation with high enough spatial resolutions e g at tens of kilometres as such there have been plenty of studies since then such as bauer et al 2015 moufouma okia jones 2015 and many more inspired by the hydrological ensemble prediction experiments hepex schaake et al 2007 initiative datasets such as the era40 uppala et al 2005 have been widely used since then although these datasets are not originally produced over sets of grids or at least not the commonly recognised types of grids they often are interpolated onto regular grids nevertheless in order to facilitate further analysis and to be used as other model inputs for instance global numerical weather models tend to use the gaussian grids e g ear40 grids local area model lam such as the weather research and forecasting model wrf skamarock et al 2001 uses regular grids spatially but does so only on a projected plane the importance and popularity of using those grid based forcing data are underlined by the needs of many climate change impact studies where climate projections such as those from the coupled model inter comparison project giorgetta et al 2013 covey et al 2003 are normally provided over a set of regular longitude latitude grids over the globe to better facilitate the community in using these grid based data and encourage the interoperability among models the network common data format netcdf rew and davis 1990 has become the de facto standard in climate change impact studies although other traditional formats such as gridded binary grib rutledge et al 2006 or hierarchical data format hdf duane et al 2000 are well supported as well in the context of using grid based hydro climatic datasets for providing external forcing field an important step is to understand quantify and if possible to correct the errors and or bias in these fields the spatially variant nature of these data remains as the centre of the process for example rojas et al 2011 applied a statistical bias correction to improve the regional climate model rcm driven climate simulations across the europe rabiei and haberlandt 2015 proposed to merge rain gauge measurements and weather radar data which is grid based data by bias correction specifically for weather radar adjustment many algorithms such as the mean field bias mfb method and the kriging with external drift ked method adjust the radar data solely by a multiplicative factor which does not vary spatially however more recently the conditional merge algorithm introduced by sinclair and pegram 2005 and implemented by guenzi et al 2016 considers the spatial impacts by conditioning the gauge adjustment on the radar precipitation values at gauge locations silver et al 2019 apart from being used as inputs to the models the grid based hydro climatic datasets are also a foundation to support further analyses on environment change both spatially and temporally it is not surprising that nearly all published studies in this field have been done so on grid based datasets e g du and zhang 2019 identified the spatiotemporal variations and trends of precipitation and streamflow extremes in the xiang river basin with gridded data of resolution 0 5 and concluded that intensified summer extreme precipitation occurs mainly in the upper and middle of the basin and extreme streamflow has an increasing trend at the same region fairman et al 2017 analysed the climatology of size shape and intensity of grid based precipitation features over great britain and ireland more application on grid based data can be seen in drusch et al 2004 thorndahl et al 2017 chen et al 2015 it is clear from the above examples that the grid based hydro climatic data have spatial patterns and characteristics with regards to certain changing factors that need to be diagnosed such diagnosis without exception is done over analysing targeted variable s and or their combinations sampled spatially within predefined boundaries such as political regions bell et al 2009 and river catchments monteiro et al 2016 further to understand the random nature of the errors and uncertainties associated with the spatial data the monte carlo simulation approach is commonly used together with geo statistical stochastic simulation for uncertainty quantification a simple procedure is to perform simulations of points can be data or events randomly distributed in the predefined area calculate the empirical distribution function of such inter point distances in each case and then obtain further values of the statistic by goodness of fit besag and diggle 1977 following this approach some applications have been published e g smith and cheeseman 1986 xu et al 2005 and wu et al 2018 however application on hydro climatic grid based data remains scarce and many previous studies on spatial distributions of hydro climatic variables were conducted over predefined areas apparently the substantial overhead of computer programming of spatial random sampling over often large size hydro climatic datasets has affected researchers capacity of studying spatial temporal variation of climatic features to address this issue we developed a spatial random sampling toolbox for grid based data analysis srs gda which can generate arbitrary samples from any grid based dataset automatically as an open source matlab toolbox it can assist users in spatial random sampling with various constraints such as shape size location dominant orientation and resolution in the field of environmental change impact studies where the spatial properties of grid based datasets remain as the focus this toolbox addresses the needs of quantifying how hydro climatic responses vary with location and scale the grid size used by the srs gda toolbox can be defined in line with any resolution of the base grid map to increase the applicability of this toolbox users can customise various sampling conditions and their combinations which can be directly applied to many environmental change studies this paper is structured as follows first a brief introduction of the study background and the main objective are provided followed by the presentation of the methodology section an example using case of analysing hydro climatic extremes i e precipitation over great britain using the gear dataset is provided to demonstrate the application of the toolbox finally discussion on further applicability and availability of the model is presented 2 the design and implementation of the srs gda toolbox the main aim of the srs gda toolbox is to enable random spatial sampling of grid based data within a pre defined region of interest roi of different sizes shapes locations and resolutions the sampling procedure starts with a user supplied grid dataset with spatial reference it is also common to have an overall boundary ob from which the sampling is to be conducted as many grid datasets have coverage normally much larger than that of the user s interest such as the general circulation model gcm output around the globe normally the ob should be set large enough for studying how the variation of locations can affect certain quantities represented by an roi the randomisation of the sampling process is manifested by the ways of how the roi is constructed 1 randomisation of the shape of the roi the shape of an area often plays an important role in various applications for example in hydrology a so called donor catchment is often desired to have a shape analogous to that of the ungauged target one understandably this process sets to be the most complex one in the srs gda toolbox we offer two options with regards to whether the shape of the roi is of concern the shape unconstrained sampling which randomises the shape of roi and the shape constrained sampling that makes use of a predefined geometric shape supplied by the user e g a polygon at a given scale a special case is point or single grid sampling whose roi reduces to a single grid this is also useful for example when studying the variation of point measured quantities 2 randomisation of the location of the roi the location of an roi can be varied by changing the coordinates of its centroid for predefined roi s or its origin for randomly generated roi s this operation is done by randomly setting a point or grid within the ob as the new location for the roi to be moved to an extra step is usually applied to ensure the entire region of the roi falling within the ob 3 randomisation of the size of the roi variation of the roi size can help users to identify whether the aggregated data value over an area exhibits notable behaviour a typical case for example is to study the extreme value distribution of a hydrometeorological variable temperature or precipitation at regional national and global scale this operation depends on whether the roi is shape constrained or not if a predefined shape is used a buffering operation chang 2008 is used to either increase or reduce the size whilst maintaining the shape unchanged whereas for a shape unconstrained case the desired roi is randomly produced with a given location and specified size these three operations can be combined to achieve the various levels of randomisation required by users the implementation of the toolbox involves a series of steps that are described below and shown in fig 1 which includes a grid map generation which sets the overall boundary ob spatial coverage constraint and the resolution for the study sampling area b sampling setup that determines whether one or more constraints are used and sets the corresponding values and or features for example location fixed or floated shape unconstrained or shape constrained size fixed or not etc and c sampling processing and validation which are automatically carried out by the srs gda toolbox based on the ob grid map and the constraint setups with extra filters applied to the results depending on extra conditions where appropriate 2 1 generating the grid based overall boundary ob map as mentioned previously the underlying dataset normally comes with a coverage larger than that of users interest in other words a subset based on an ob needs to be produced this ob needs to be specified by the user e g by using either a raster file or a vector based map such as shapefiles that define the boundary if no ob is specified the entire coverage of the underlying grid dataset will be used to conduct the sampling process it should be mentioned that the sampling process often happens inside the ob however different from ob the boundaries specified by the roi s are deemed to be restrictive and arbitrary as far as a natural process is concerned such as rainfall and wind speed the logic behind sampling roi in ob is because many times only the quantity of certain hydro climatic variables falling in such given boundary is of concern for example rainfall over the urban area of a city is a key element for urban drainage design a grid based map is then generated by rasterising the ob if it comes as a vector map using the same projection and grid resolution as the underlying dataset the grids inside the ob are regarded as valid grids while those outside are invalid grids once this is completed the toolbox will automatically exclude those invalid grids and activate the valid grids for example in the example case given in this paper the national grid reference ngr ordnance survey 1946 is used to refer to the coordinates of the grids of the gear dataset the base map is processed to distinguish ocean so called invalid grids outside the gb boundary and land so called valid grids inside the gb boundary it is also further refined to have several versions with different spatial resolutions which are normally multiples exact divisions of the grid size of the underlying dataset these refined ob s will be used for further study on aggregation upscaling and disaggregation downscaling the toolbox provides three resolutions to match the underlying grid dataset 1 km 1 km 5 km 5 km and 10 km 10 km for user applying and the base maps of the gb are produced with these three resolutions respectively as shown in fig 5 where 1 km 1 km is chosen for demonstrating the example case for being consistent with the resolution of dataset details in 3 1 in addition to setting the ob another important task at this step is to spatially index the data grids and label those that contain valid data from now on all subsequent spatial sampling is conducted over or within to be more precise the base map 2 2 sampling setup there are four initial settings also seen in fig 1b that need to be specified before starting the sampling process which are 1 total number of samples required 2 the desired location of the samples which is only applicable in the case where users wish to fix the location while randomising other properties such as shapes and sizes 3 sample size in the unit of km 2 which is translated into numbers of grids at the finest grid resolution used note that this is only required if a size constrained sampling is desired 4 spatial index of the roi shape i e samples which is needed when a shape constrained sampling is required in this case the roi shapes are randomly generated as convex hulls having their spatial index sp value set by the user in the case of shape unconstrained sampling the shape of the roi s will be randomised the spatial index sp is defined to indicate dominant spatial extension direction e g north south or west east 1 s p d ns d we where d ns and d we refer to the north south dimension and the west east dimension of a sample represented by a matrix the reason of having sp as an attached indictor is that in many climate studies the direction of an area such as a river catchment plays a crucial role in determining the amount of quantity such as rainfall viviroli et al 2003 svensson and rakhecha 1998 obviously other indexes such as the direction of the major axis can be easily defined if required 2 3 sampling processing and validation this is the final step fig 1c where samples are generated according to the initial settings the methods discussed below correspond to the three main functions of srs gda toolbox sampling with randomised locations this function randomly selects different locations to set the centroids of the samples within the ob base map the sampling is relatively straightforward first x and y coordinates are sampled from the range of the ob maps in the two directions using a joint uniform distribution u x y followed by filtering out those samples that are not entirely within the ob sampling with randomised sizes the second function is to randomly generate samples with different sizes which is mainly used in the cases where the behaviour of aggregated quantity over the area of a sample is desired since the grid resolution a grid in km2 is known the size of sample a sample can be translated into the number of grids n grids of sample of the roi the equation below shows the translation 2 n grids of sample a sample a grid the variation of the area of the roi the sample is realized by applying a buffering operation while keeping the centroid location unchanged i e it only increases or decreases the main axis of the sample proportionately fig 2 shows an example of shape generation sampling with randomised shape of roi unconstrained and constrained the third main function is to randomly generate samples in different shapes varying in both sizes and locations depending on the user s initial settings this function can conduct both shape unconstrained and shape constrained sampling in the former case the location and the size of the sample roi are both obtained from the two previous functions for each combination of the location and the size the shape is randomised using the size as a constraint two principles are applied in this process 1 all grids should be interconnected i e no isolated grids are allowed 2 any growth must not go over the boundary set by the ob map the sampling starts at the given location and follows a random run to the neighbouring grid and records it until the number of grids equals n grids of sample all the grids covered by the path are selected to comprise the sample an extra validation step is applied to remove samples with holes inside the so called ill set samples and rerun the process until the required number of samples is met as presented in fig 3 for the case of shape constrained random sampling it focuses on sampling with the shapes of convex polygons as seen in many hydrological catchments in environmental or climatic research the working flow is shown in fig 4 unlike the shape unconstrained method the shape constrained random sampling method produces more regular samples such as convex polygons the main parameters such as the initial centred location l sample size s and number n are the same as those required by the shape unconstrained method in addition the shape constrained method uses one more major parameter the spatial index sp as a further constraint if required three optional parameters can also be set to further refine the control of the polygon generation i e the number of angles usually is greater than or equal to 3 the irregularity that indicates how much variance there is in the angular distance of vertices with a range of 0 1 the spikiness which indicates how much variance there is in each vertex from the average radius with a range of 0 1 however as in the setup of the main parameters l s n and sp specification of these additional parameters are not compulsory unless otherwise specified explicitly by the user the toolbox automatically generates default values for them e g irregularity 0 3 and spikiness 0 1 to avoid producing extremely weird irregularity 1 or sharp spikiness 1 polygons compared with the shape unconstrained random sampling method it runs substantially faster because there is no need for random walking to grow the grids nor does it have any possibility of producing ill set areas 3 an example application of the toolbox 3 1 dataset one of the motivations of this example is to investigate how areal rainfall extremes in terms of their distributions can vary with locations size and shapes of the roi in fact there has been consensus about the impact of the size of catchment when producing areal rainfall at certain return levels this is normally acknowledged by applying a so called areal reduction factor arf bell 1976 to the value obtained at the location of the centroid of the catchment whilst variation of hydro climatic variables is commonly recognised to be associated with the climatology impact of the locations as well as the shape of the catchment have not been fully studied in a quantitative way in our case the 1 km gridded estimates of daily rainfall for great britain are analysed using a map of great britain roughly sized as 700 1250 km2 the rainfall estimates are derived from the met office national database of observed precipitation by using the uk rain gauge network the natural neighbour interpolation methodology including a normalization step based on average annual rainfall was used to generate the daily estimates from 9am until 9am on the following day tanguy et al 2016 3 2 application of the srs gda toolbox to be consistent with the precision of dataset the ob base map is produced as the same grid size of 1 km2 the production of the ob map undergoes two steps first a rough sketch of the boundary of great britain gb is used to generate grids with very coarse resolution set as 100 km2 this is to ensure the boundary is properly covered secondly the grid map is then refined by subdividing every grid with a number of smaller ones so that the grid resolution gradually increases to 5 km 5 km and 10 km 10 km which allows for the detection and removal of those grids falling outside of the boundary this process is shown in fig 5 including a 638607 valid grids marked as green with the size of 1 km2 b 9464 valid grids with the size of 25 km2 c 2368 valid grids with the size of 100 km2 meanwhile the location of the sample in this example study is chosen to be in london with the coordinate of l 520 km 1070 km two random sampling methods e g shape unconstrained and shape constrained are used to generate 5 different samples n 5 at this location with the same size of 25 km2 according to eq 2 the number of grids in each sample s is calculated as 25 km 2 1 km 2 25 n l and s are the basic inputs for srs gda toolbox 3 2 1 shape unconstrained random sampling method table 1 presents the 5 different samples around the initial location l grey grid generated by the shape unconstrained random sampling method it can be observed that all samples have grids interconnected with no hole inside however the shapes of the sample can be very irregular as there is no requirement that they need to be a convex polygon which is used in the shape constrained sampling method the shape unconstrained sampling offers maximum freedom however it can inevitably introduce shapes with holes inside which have to be rejected fig 6 shows the steps involved to detect and remove those ill set sample shapes first the original sample is presented to the validation function fig 6 a before it is converted into a binary image fig 6b secondly the inner area of the binary image is flooded to remove the potential holes which results in a hole free image as shown in fig 6c finally by comparing the areas of the two images the location and the size of the hole s can be detected which in turn triggers the removal process to discard the ill set sample in our test the whole process of shape unconstrained random sampling method takes 7 0 s on a low configuration laptop to randomly generate five accepted samples with sizes of 25 km 2 specified as an initial constraint while three samples are abandoned 3 2 2 shape constrained random sampling method five samples at same location l grey grid generated by using shape constrained random sampling method are shown in table 2 with various spatial indexes sp defined by the toolbox comparing with those samples listed in table 1 clearly the shapes are more regular here as convex polygons which can be directly used to simulate hydrological catchments the whole process is recorded to have finished in 2 0s on our test pc which is shorter than that from the former method however the tests show that the larger size and number are the more efficient and time saving the shape constrained method is compared with the shape unconstrained method in table 3 fig 7 summarises the steps taken for shape constrained sampling starting with an arbitrary but convex polygon with sp irregularity and spikiness all set by the toolbox set at the same location index l grey grid the effect of the spatial index sp in the process of shape constrained sampling is shown in fig 8 with larger values of sp having more north south direction dominated shapes while smaller values indicate west east direction dominated samples apparently other shape related constraints can be defined and applied subject to the needs of different applications the value of the toolbox can well be appreciated in the analysis results partly shown in fig 9 in finding the spatial variation of extreme rainfall over the gb the entire analysis is not presented here however with the help of the srs gda toolbox we were able to reveal patterns never reported before for example a west east variation of the rainfall distribution at different quantiles is clearly seen as west high east low in fig 9a what is more interesting is the symmetric pattern shown in fig 9b around sp 1 0 with regards to the sample shape which implies that sampled areas with slight elongation in north south direction are expected to have a higher amount of rainfall than those spread more in east west direction at given frequency return period for samples with the same size and location there is a remarkable difference in areal averaged rainfall between more elongated e g sp 0 2 or 5 0 and rounded shape e g sp 1 0 which can be attributed to heterogeneity of the grid rainfall distribution that cannot compensate to the areal average the relationship between the sample size and the annual maximum daily rainfall fig 9c is shown to have largely followed what is expected e g decrease of areal rainfall as sample size grows 4 conclusions and availability of the toolbox in this paper we discuss the development of a new matlab toolbox for spatial random sampling in grid based data analysis srs gda the main aim of the toolbox is to address the very needs of many climate change related studies on spatial temporal diagnostics of hydro climatic datasets an example application case is given in which the implementation details are discussed our initial applications show that with this toolbox several important variation patterns of extreme rainfall due to be published separately over gb that have yet to be reported are clearly identified based on the promising results we expect this toolbox thanks to the availability of its source code will help the related research community in their analyses of grid data sets and gain further insight into the underlying science the source code of the toolbox as well as the example case given above are available at the github https github com wanghan924 srs gda toolbox git the source code is provided subject to a gpl v3 licence use fork of the toolbox is subject to proper acknowledgement as stated on the webpage of the toolbox declaration of competing interest the authors declare no conflict of interest acknowledgements the authors would like to thank the centre of hydrology and ecology ceh uk for providing the gear dataset to test the toolbox the co author han wang s phd study is jointly sponsored by the two scholarships offered by the chinese scholarship council csc china and the college of engineering swansea university uk which are both gratefully acknowledged this study is supported by the uk china urban flooding programme grant ref uufrip 10021 from the royal academy of engineering united kingdom 
26088,we present in this paper the development of a new open source matlab toolbox srs gda that aims to provide random spatial sampling of grid based hydro climatic datasets for environmental change studies this toolbox addresses the needs of quantifying how hydro climatic responses which are often driven by grid based forcing datasets such as climate model projections vary with location and scale the toolbox can be used to carry out random spatial sampling of grid based quantities with various constraints shape size location dominant orientation and resolution a case study of a large dataset the gear rainfall dataset is supplied to demonstrate the typical uses case of this toolbox the provision of the toolbox for downloading together with the sample data is also presented keywords spatial random sampling grid based data analysis environment change matlab toolbox open source software 1 introduction research in environmental changes has been increasingly relying on computer models driven by external forcing field and conditions that can represent changing factors such as temperature precipitation and land uses erler et al 2019 alamou et al 2017 historically the inputs used to drive these models were often relatively scarce and only available at limited number of locations data collection was often restricted by technical conditions instruments and means of storage to make full use of such finite data many methods have been proposed and applied in terms of rainfall data there are many methods for translating point rainfall records which are usually collected from hydrological gauges or stations to the basin rainfall for example the areal reduction factor arf has been widely used possibly under different names in different countries weather bureau 1958 nerc 1977 more recently however with the rapid advances in environmental monitoring technology spatially disaggregated grid based hydro climatic datasets have become gradually available with steady improvements in both accuracy and spatial temporal resolutions a typical case is the nimrod weather radar system deployed by the uk met office which can provide up to 1km 5min precipitation distribution over the country golding 1998 fairman et al 2017 similarly satellite borne observations such as the global precipitation measurements gpm islam et al 2014 ning et al 2016 can now provide large scale coverage of the precipitation coverage in near real time many environmental models nowadays are also tuned to make use of those new grid based high resolution datasets e g the grid to grid version of the pdm model has been developed and operationally used by the environment agency in the uk cole and moore 2008 kay et al 2009 another important source of external forcing data is model simulated hydro climatic fields in this case rainfall temperature as well as soil moisture fields generated by numerical weather models or climate models can be used to drive other model simulations practices of using the so called coupled model approach started to gain momentum in the early 2000 s when numerical weather models and climate models were able to produce simulation with high enough spatial resolutions e g at tens of kilometres as such there have been plenty of studies since then such as bauer et al 2015 moufouma okia jones 2015 and many more inspired by the hydrological ensemble prediction experiments hepex schaake et al 2007 initiative datasets such as the era40 uppala et al 2005 have been widely used since then although these datasets are not originally produced over sets of grids or at least not the commonly recognised types of grids they often are interpolated onto regular grids nevertheless in order to facilitate further analysis and to be used as other model inputs for instance global numerical weather models tend to use the gaussian grids e g ear40 grids local area model lam such as the weather research and forecasting model wrf skamarock et al 2001 uses regular grids spatially but does so only on a projected plane the importance and popularity of using those grid based forcing data are underlined by the needs of many climate change impact studies where climate projections such as those from the coupled model inter comparison project giorgetta et al 2013 covey et al 2003 are normally provided over a set of regular longitude latitude grids over the globe to better facilitate the community in using these grid based data and encourage the interoperability among models the network common data format netcdf rew and davis 1990 has become the de facto standard in climate change impact studies although other traditional formats such as gridded binary grib rutledge et al 2006 or hierarchical data format hdf duane et al 2000 are well supported as well in the context of using grid based hydro climatic datasets for providing external forcing field an important step is to understand quantify and if possible to correct the errors and or bias in these fields the spatially variant nature of these data remains as the centre of the process for example rojas et al 2011 applied a statistical bias correction to improve the regional climate model rcm driven climate simulations across the europe rabiei and haberlandt 2015 proposed to merge rain gauge measurements and weather radar data which is grid based data by bias correction specifically for weather radar adjustment many algorithms such as the mean field bias mfb method and the kriging with external drift ked method adjust the radar data solely by a multiplicative factor which does not vary spatially however more recently the conditional merge algorithm introduced by sinclair and pegram 2005 and implemented by guenzi et al 2016 considers the spatial impacts by conditioning the gauge adjustment on the radar precipitation values at gauge locations silver et al 2019 apart from being used as inputs to the models the grid based hydro climatic datasets are also a foundation to support further analyses on environment change both spatially and temporally it is not surprising that nearly all published studies in this field have been done so on grid based datasets e g du and zhang 2019 identified the spatiotemporal variations and trends of precipitation and streamflow extremes in the xiang river basin with gridded data of resolution 0 5 and concluded that intensified summer extreme precipitation occurs mainly in the upper and middle of the basin and extreme streamflow has an increasing trend at the same region fairman et al 2017 analysed the climatology of size shape and intensity of grid based precipitation features over great britain and ireland more application on grid based data can be seen in drusch et al 2004 thorndahl et al 2017 chen et al 2015 it is clear from the above examples that the grid based hydro climatic data have spatial patterns and characteristics with regards to certain changing factors that need to be diagnosed such diagnosis without exception is done over analysing targeted variable s and or their combinations sampled spatially within predefined boundaries such as political regions bell et al 2009 and river catchments monteiro et al 2016 further to understand the random nature of the errors and uncertainties associated with the spatial data the monte carlo simulation approach is commonly used together with geo statistical stochastic simulation for uncertainty quantification a simple procedure is to perform simulations of points can be data or events randomly distributed in the predefined area calculate the empirical distribution function of such inter point distances in each case and then obtain further values of the statistic by goodness of fit besag and diggle 1977 following this approach some applications have been published e g smith and cheeseman 1986 xu et al 2005 and wu et al 2018 however application on hydro climatic grid based data remains scarce and many previous studies on spatial distributions of hydro climatic variables were conducted over predefined areas apparently the substantial overhead of computer programming of spatial random sampling over often large size hydro climatic datasets has affected researchers capacity of studying spatial temporal variation of climatic features to address this issue we developed a spatial random sampling toolbox for grid based data analysis srs gda which can generate arbitrary samples from any grid based dataset automatically as an open source matlab toolbox it can assist users in spatial random sampling with various constraints such as shape size location dominant orientation and resolution in the field of environmental change impact studies where the spatial properties of grid based datasets remain as the focus this toolbox addresses the needs of quantifying how hydro climatic responses vary with location and scale the grid size used by the srs gda toolbox can be defined in line with any resolution of the base grid map to increase the applicability of this toolbox users can customise various sampling conditions and their combinations which can be directly applied to many environmental change studies this paper is structured as follows first a brief introduction of the study background and the main objective are provided followed by the presentation of the methodology section an example using case of analysing hydro climatic extremes i e precipitation over great britain using the gear dataset is provided to demonstrate the application of the toolbox finally discussion on further applicability and availability of the model is presented 2 the design and implementation of the srs gda toolbox the main aim of the srs gda toolbox is to enable random spatial sampling of grid based data within a pre defined region of interest roi of different sizes shapes locations and resolutions the sampling procedure starts with a user supplied grid dataset with spatial reference it is also common to have an overall boundary ob from which the sampling is to be conducted as many grid datasets have coverage normally much larger than that of the user s interest such as the general circulation model gcm output around the globe normally the ob should be set large enough for studying how the variation of locations can affect certain quantities represented by an roi the randomisation of the sampling process is manifested by the ways of how the roi is constructed 1 randomisation of the shape of the roi the shape of an area often plays an important role in various applications for example in hydrology a so called donor catchment is often desired to have a shape analogous to that of the ungauged target one understandably this process sets to be the most complex one in the srs gda toolbox we offer two options with regards to whether the shape of the roi is of concern the shape unconstrained sampling which randomises the shape of roi and the shape constrained sampling that makes use of a predefined geometric shape supplied by the user e g a polygon at a given scale a special case is point or single grid sampling whose roi reduces to a single grid this is also useful for example when studying the variation of point measured quantities 2 randomisation of the location of the roi the location of an roi can be varied by changing the coordinates of its centroid for predefined roi s or its origin for randomly generated roi s this operation is done by randomly setting a point or grid within the ob as the new location for the roi to be moved to an extra step is usually applied to ensure the entire region of the roi falling within the ob 3 randomisation of the size of the roi variation of the roi size can help users to identify whether the aggregated data value over an area exhibits notable behaviour a typical case for example is to study the extreme value distribution of a hydrometeorological variable temperature or precipitation at regional national and global scale this operation depends on whether the roi is shape constrained or not if a predefined shape is used a buffering operation chang 2008 is used to either increase or reduce the size whilst maintaining the shape unchanged whereas for a shape unconstrained case the desired roi is randomly produced with a given location and specified size these three operations can be combined to achieve the various levels of randomisation required by users the implementation of the toolbox involves a series of steps that are described below and shown in fig 1 which includes a grid map generation which sets the overall boundary ob spatial coverage constraint and the resolution for the study sampling area b sampling setup that determines whether one or more constraints are used and sets the corresponding values and or features for example location fixed or floated shape unconstrained or shape constrained size fixed or not etc and c sampling processing and validation which are automatically carried out by the srs gda toolbox based on the ob grid map and the constraint setups with extra filters applied to the results depending on extra conditions where appropriate 2 1 generating the grid based overall boundary ob map as mentioned previously the underlying dataset normally comes with a coverage larger than that of users interest in other words a subset based on an ob needs to be produced this ob needs to be specified by the user e g by using either a raster file or a vector based map such as shapefiles that define the boundary if no ob is specified the entire coverage of the underlying grid dataset will be used to conduct the sampling process it should be mentioned that the sampling process often happens inside the ob however different from ob the boundaries specified by the roi s are deemed to be restrictive and arbitrary as far as a natural process is concerned such as rainfall and wind speed the logic behind sampling roi in ob is because many times only the quantity of certain hydro climatic variables falling in such given boundary is of concern for example rainfall over the urban area of a city is a key element for urban drainage design a grid based map is then generated by rasterising the ob if it comes as a vector map using the same projection and grid resolution as the underlying dataset the grids inside the ob are regarded as valid grids while those outside are invalid grids once this is completed the toolbox will automatically exclude those invalid grids and activate the valid grids for example in the example case given in this paper the national grid reference ngr ordnance survey 1946 is used to refer to the coordinates of the grids of the gear dataset the base map is processed to distinguish ocean so called invalid grids outside the gb boundary and land so called valid grids inside the gb boundary it is also further refined to have several versions with different spatial resolutions which are normally multiples exact divisions of the grid size of the underlying dataset these refined ob s will be used for further study on aggregation upscaling and disaggregation downscaling the toolbox provides three resolutions to match the underlying grid dataset 1 km 1 km 5 km 5 km and 10 km 10 km for user applying and the base maps of the gb are produced with these three resolutions respectively as shown in fig 5 where 1 km 1 km is chosen for demonstrating the example case for being consistent with the resolution of dataset details in 3 1 in addition to setting the ob another important task at this step is to spatially index the data grids and label those that contain valid data from now on all subsequent spatial sampling is conducted over or within to be more precise the base map 2 2 sampling setup there are four initial settings also seen in fig 1b that need to be specified before starting the sampling process which are 1 total number of samples required 2 the desired location of the samples which is only applicable in the case where users wish to fix the location while randomising other properties such as shapes and sizes 3 sample size in the unit of km 2 which is translated into numbers of grids at the finest grid resolution used note that this is only required if a size constrained sampling is desired 4 spatial index of the roi shape i e samples which is needed when a shape constrained sampling is required in this case the roi shapes are randomly generated as convex hulls having their spatial index sp value set by the user in the case of shape unconstrained sampling the shape of the roi s will be randomised the spatial index sp is defined to indicate dominant spatial extension direction e g north south or west east 1 s p d ns d we where d ns and d we refer to the north south dimension and the west east dimension of a sample represented by a matrix the reason of having sp as an attached indictor is that in many climate studies the direction of an area such as a river catchment plays a crucial role in determining the amount of quantity such as rainfall viviroli et al 2003 svensson and rakhecha 1998 obviously other indexes such as the direction of the major axis can be easily defined if required 2 3 sampling processing and validation this is the final step fig 1c where samples are generated according to the initial settings the methods discussed below correspond to the three main functions of srs gda toolbox sampling with randomised locations this function randomly selects different locations to set the centroids of the samples within the ob base map the sampling is relatively straightforward first x and y coordinates are sampled from the range of the ob maps in the two directions using a joint uniform distribution u x y followed by filtering out those samples that are not entirely within the ob sampling with randomised sizes the second function is to randomly generate samples with different sizes which is mainly used in the cases where the behaviour of aggregated quantity over the area of a sample is desired since the grid resolution a grid in km2 is known the size of sample a sample can be translated into the number of grids n grids of sample of the roi the equation below shows the translation 2 n grids of sample a sample a grid the variation of the area of the roi the sample is realized by applying a buffering operation while keeping the centroid location unchanged i e it only increases or decreases the main axis of the sample proportionately fig 2 shows an example of shape generation sampling with randomised shape of roi unconstrained and constrained the third main function is to randomly generate samples in different shapes varying in both sizes and locations depending on the user s initial settings this function can conduct both shape unconstrained and shape constrained sampling in the former case the location and the size of the sample roi are both obtained from the two previous functions for each combination of the location and the size the shape is randomised using the size as a constraint two principles are applied in this process 1 all grids should be interconnected i e no isolated grids are allowed 2 any growth must not go over the boundary set by the ob map the sampling starts at the given location and follows a random run to the neighbouring grid and records it until the number of grids equals n grids of sample all the grids covered by the path are selected to comprise the sample an extra validation step is applied to remove samples with holes inside the so called ill set samples and rerun the process until the required number of samples is met as presented in fig 3 for the case of shape constrained random sampling it focuses on sampling with the shapes of convex polygons as seen in many hydrological catchments in environmental or climatic research the working flow is shown in fig 4 unlike the shape unconstrained method the shape constrained random sampling method produces more regular samples such as convex polygons the main parameters such as the initial centred location l sample size s and number n are the same as those required by the shape unconstrained method in addition the shape constrained method uses one more major parameter the spatial index sp as a further constraint if required three optional parameters can also be set to further refine the control of the polygon generation i e the number of angles usually is greater than or equal to 3 the irregularity that indicates how much variance there is in the angular distance of vertices with a range of 0 1 the spikiness which indicates how much variance there is in each vertex from the average radius with a range of 0 1 however as in the setup of the main parameters l s n and sp specification of these additional parameters are not compulsory unless otherwise specified explicitly by the user the toolbox automatically generates default values for them e g irregularity 0 3 and spikiness 0 1 to avoid producing extremely weird irregularity 1 or sharp spikiness 1 polygons compared with the shape unconstrained random sampling method it runs substantially faster because there is no need for random walking to grow the grids nor does it have any possibility of producing ill set areas 3 an example application of the toolbox 3 1 dataset one of the motivations of this example is to investigate how areal rainfall extremes in terms of their distributions can vary with locations size and shapes of the roi in fact there has been consensus about the impact of the size of catchment when producing areal rainfall at certain return levels this is normally acknowledged by applying a so called areal reduction factor arf bell 1976 to the value obtained at the location of the centroid of the catchment whilst variation of hydro climatic variables is commonly recognised to be associated with the climatology impact of the locations as well as the shape of the catchment have not been fully studied in a quantitative way in our case the 1 km gridded estimates of daily rainfall for great britain are analysed using a map of great britain roughly sized as 700 1250 km2 the rainfall estimates are derived from the met office national database of observed precipitation by using the uk rain gauge network the natural neighbour interpolation methodology including a normalization step based on average annual rainfall was used to generate the daily estimates from 9am until 9am on the following day tanguy et al 2016 3 2 application of the srs gda toolbox to be consistent with the precision of dataset the ob base map is produced as the same grid size of 1 km2 the production of the ob map undergoes two steps first a rough sketch of the boundary of great britain gb is used to generate grids with very coarse resolution set as 100 km2 this is to ensure the boundary is properly covered secondly the grid map is then refined by subdividing every grid with a number of smaller ones so that the grid resolution gradually increases to 5 km 5 km and 10 km 10 km which allows for the detection and removal of those grids falling outside of the boundary this process is shown in fig 5 including a 638607 valid grids marked as green with the size of 1 km2 b 9464 valid grids with the size of 25 km2 c 2368 valid grids with the size of 100 km2 meanwhile the location of the sample in this example study is chosen to be in london with the coordinate of l 520 km 1070 km two random sampling methods e g shape unconstrained and shape constrained are used to generate 5 different samples n 5 at this location with the same size of 25 km2 according to eq 2 the number of grids in each sample s is calculated as 25 km 2 1 km 2 25 n l and s are the basic inputs for srs gda toolbox 3 2 1 shape unconstrained random sampling method table 1 presents the 5 different samples around the initial location l grey grid generated by the shape unconstrained random sampling method it can be observed that all samples have grids interconnected with no hole inside however the shapes of the sample can be very irregular as there is no requirement that they need to be a convex polygon which is used in the shape constrained sampling method the shape unconstrained sampling offers maximum freedom however it can inevitably introduce shapes with holes inside which have to be rejected fig 6 shows the steps involved to detect and remove those ill set sample shapes first the original sample is presented to the validation function fig 6 a before it is converted into a binary image fig 6b secondly the inner area of the binary image is flooded to remove the potential holes which results in a hole free image as shown in fig 6c finally by comparing the areas of the two images the location and the size of the hole s can be detected which in turn triggers the removal process to discard the ill set sample in our test the whole process of shape unconstrained random sampling method takes 7 0 s on a low configuration laptop to randomly generate five accepted samples with sizes of 25 km 2 specified as an initial constraint while three samples are abandoned 3 2 2 shape constrained random sampling method five samples at same location l grey grid generated by using shape constrained random sampling method are shown in table 2 with various spatial indexes sp defined by the toolbox comparing with those samples listed in table 1 clearly the shapes are more regular here as convex polygons which can be directly used to simulate hydrological catchments the whole process is recorded to have finished in 2 0s on our test pc which is shorter than that from the former method however the tests show that the larger size and number are the more efficient and time saving the shape constrained method is compared with the shape unconstrained method in table 3 fig 7 summarises the steps taken for shape constrained sampling starting with an arbitrary but convex polygon with sp irregularity and spikiness all set by the toolbox set at the same location index l grey grid the effect of the spatial index sp in the process of shape constrained sampling is shown in fig 8 with larger values of sp having more north south direction dominated shapes while smaller values indicate west east direction dominated samples apparently other shape related constraints can be defined and applied subject to the needs of different applications the value of the toolbox can well be appreciated in the analysis results partly shown in fig 9 in finding the spatial variation of extreme rainfall over the gb the entire analysis is not presented here however with the help of the srs gda toolbox we were able to reveal patterns never reported before for example a west east variation of the rainfall distribution at different quantiles is clearly seen as west high east low in fig 9a what is more interesting is the symmetric pattern shown in fig 9b around sp 1 0 with regards to the sample shape which implies that sampled areas with slight elongation in north south direction are expected to have a higher amount of rainfall than those spread more in east west direction at given frequency return period for samples with the same size and location there is a remarkable difference in areal averaged rainfall between more elongated e g sp 0 2 or 5 0 and rounded shape e g sp 1 0 which can be attributed to heterogeneity of the grid rainfall distribution that cannot compensate to the areal average the relationship between the sample size and the annual maximum daily rainfall fig 9c is shown to have largely followed what is expected e g decrease of areal rainfall as sample size grows 4 conclusions and availability of the toolbox in this paper we discuss the development of a new matlab toolbox for spatial random sampling in grid based data analysis srs gda the main aim of the toolbox is to address the very needs of many climate change related studies on spatial temporal diagnostics of hydro climatic datasets an example application case is given in which the implementation details are discussed our initial applications show that with this toolbox several important variation patterns of extreme rainfall due to be published separately over gb that have yet to be reported are clearly identified based on the promising results we expect this toolbox thanks to the availability of its source code will help the related research community in their analyses of grid data sets and gain further insight into the underlying science the source code of the toolbox as well as the example case given above are available at the github https github com wanghan924 srs gda toolbox git the source code is provided subject to a gpl v3 licence use fork of the toolbox is subject to proper acknowledgement as stated on the webpage of the toolbox declaration of competing interest the authors declare no conflict of interest acknowledgements the authors would like to thank the centre of hydrology and ecology ceh uk for providing the gear dataset to test the toolbox the co author han wang s phd study is jointly sponsored by the two scholarships offered by the chinese scholarship council csc china and the college of engineering swansea university uk which are both gratefully acknowledged this study is supported by the uk china urban flooding programme grant ref uufrip 10021 from the royal academy of engineering united kingdom 
26089,we developed a new global model to predict biogeochemical cycling of mercury in the ocean we describe and evaluate the model and discuss mercury levels distribution and budgets based on a simulation with a total time span of 260 years the model is based on a fully coupled atmosphere ocean chemical transport model and considers methylated mercury production in the water column followed by biotransfer to lower order marine organisms including spatial and temporal variations in partitioning properties model validation shows that we can simulate total dissolved mercury hgt concentrations in the surface ocean with model data differences at a maximum of one order of magnitude the simulated oceanic hgt content is currently 2010 1 6 16 9 times larger than previously modeled estimates the estimated overall turnover time of oceanic hgt determined by our model is 320 years which is shorter than suggested by previous modeling studies keywords mercury ocean biotransfer global model software availability name of software fate hg developer t kawai contact email kawai toru nies go jp year first available 2019 hardware required unix linux software required netcdf availability development version available for contact developers through github cost free language fortran90 program size 3 0 mb source files 1 introduction mercury especially in its methylated form is a notorious neurotoxin and numerous studies have documented its toxic effects on human health ha et al 2017 additionally exposure to mercury can induce a variety of adverse effects in wildlife such as birds scheuhammer et al 2007 mammals scheuhammer et al 2007 and fishes scheuhammer et al 2007 depew et al 2012 morcillo et al 2017 at physiologic histologic bio chemical enzymatic and genetic levels mercury is emitted from both anthropogenic and geogenic sources and cycles through the atmosphere ocean soil vegetation and biosphere taking elemental oxidized and methylated forms anthropogenic mercury emissions likely began around 2700 bc with the advent of mining precious metals lacerda 1997 and has increased since the industrial revolution with peaks around 1890 and 1970 streets et al 2017 north america and europe previously generated the highest levels of atmospheric emissions 75 in 1890 and 60 in 1970 if all source sectors are combined but asia has become the largest polluter in recent years approx 60 in 2010 mercury emitted to the atmosphere is transported to remote regions via atmospheric general circulation the residence time of gaseous elemental mercury gem i e the dominant form found in the atmosphere is approximately 0 5 1 year holmes et al 2010 which allows the dispersion of gem over the entire global atmosphere diffusive transport from continental areas and the oceans to the atmosphere is also significant because mercury is a highly volatile heavy metal the release of mercury from continental areas and the ocean is several times greater than anthropogenic emissions to the atmosphere amap unep 2013 amap un environment 2019 mercury cycles on a global scale and across diverse environmental media sets this element apart from other heavy metals in terms of environmental dynamics in october 2013 the minamata convention on mercury was finalized and enforced starting in august 2017 the convention was one of several international efforts devoted to reducing anthropogenic mercury emissions numerical modeling is useful for evaluating the effects that reduced anthropogenic mercury emissions which arise from such efforts have on the levels and distributions of mercury in environmental media and biota several 3 d dynamic global to regional mercury models have been developed based on the atmospheric general circulation or air quality models pirrone and keating 2010 on the other hand global ocean models are less developed and studied interfacial transport is often treated as secondary emissions the ocean plays a central role in mercury biogeochemical cycles and are a large reservoir for mercury which suggests the possibility that transport from the ocean to atmosphere is a primary source of atmospheric mercury the oceans also produce methylated mercury and accumulation is common in fish consumption of this food source is a primary pathway for human exposure to mercury mergler et al 2007 therefore ocean modeling is important not only for discussing the role of the mercury cycle in the environment but also to link dynamic modeling and risk assessments the geos chem selin et al 2008 strode et al 2007 implements a mixed layer slab ocean submodel soerensen et al 2010 referred to as so10 zhang et al 2014a referred to as zh14 for the first time developed a global mercury ocean model offtrac hg based on the ocean general circulation model ogcm using this model they analyzed and discussed preindustrial background concentrations based on a 10 000 year simulation without anthropogenic emissions they also coupled geos chem and offtrac hg and showed that the coupled atmosphere ocean model increases the air sea exchange flux by 12 compared to the uncoupled model zhang et al 2019 previous studies have observed methylated mercury in open ocean seawater with maximum concentrations typically near the oxycline sunderland et al 2009 mason et al 2012 kim et al 2017 although there is a lack of quantitative understanding of the source and pathways of methylated mercury more recent models have considered explicit methylated mercury cycles such as the arctic box model developed in soerensen et al 2016 referred to a so16 and the ogcm based global ocean model reported in semeniuk and dastoor 2017 referred to as called sd17 previously we developed a finely advanced transboundary environmental model known as fate to model chlorinated and brominated organic compounds such as polychlorinated biphenyls kawai et al 2014 handoh and kawai 2014 in this study we incorporated mercury processes into fate to be able to model mercury fate hg the fate hg model takes into account methylated mercury production in the water column followed by biotransfers to lower order marine organisms i e particle organic matter pom and the subsequent export to the deeper ocean via the biological pump the major differences in the fate hg model with respect to preceding models soerensen et al 2010 2016 zhang et al 2014a 2014b semeniuk and dastoor 2017 horowitz et al 2017 mason et al 1994 lamborg et al 2002 mason and sheu 2002 sunderland and mason 2007 streets et al 2011 amos et al 2013 2015 are twofold 1 the model is based on a fully coupled atmosphere ocean chemical transport model and 2 the model considers spatial and temporal variations in the seawater pom partitioning coefficient observational studies lamborg et al 2016 have indicated that this parameter varies significantly in both time and space the aim of this study is to describe and evaluate fate hg and to investigate the human impact on the fate of mercury with a particular focus on the ocean using fate hg we discuss long term 1850 2010 trends in mercury levels and the global distribution and budget of mercury based on a centurial simulation with changes in historical anthropogenic emissions 2 methods 2 1 model description fate hg is a global model capable of simulating the biogeochemical cycles of mercury in and across the troposphere continental areas ocean sediment and lower order marine organisms the transport and ecosystem sub models implemented by fate hg provide the basis for the physical and biological processes the coupled atmosphere ocean chemical transport model computes 3 d non steady advective and diffusive transport of gaseous elemental divalent and particle associated mercury in the troposphere as well as dissolved elemental hg0 hgii mmhg and dimethyl dmhg mercury in the ocean the spatial resolutions of the troposphere and ocean are 0 75 horizontal 35 œÉ layers vertical and 1 0 50 layers 0 5500 m respectively the governing equations and physical schemes are identical to those in fate kawai et al 2014 handoh and kawai 2014 but fate hg uses different climate forcing data the ecosystem model uses satellite data to estimate pom concentrations pom export flux via the biological pump pom remineralization flux in seawater pom burial flux to sediment and pom remineralization flux from sediments fig 1 summarizes the mercury processes that are considered in the model fate hg also takes into account transformations in air and cloud water interfacial transports transformations in seawater and sediment transport from sediment to seawater via diffusion and resuspension and the biotransfer to lower order marine organisms i e pom followed by deep sea export via the biologically driven carbon cycle processes for interfacial transport at land sea surfaces consist of dry and wet deposition and the bidirectional diffusive transport of elemental mercury dry deposition is further classified into the deposition of particle associated mercury and gaseous mercury atmospheric chemistry is based on o3 and oh chemistry as major gem oxidants shia et al 1999 seigneur et al 2006 the model does not explicitly consider terrestrial compartments but does calculate interfacial transport we assume that surface soil vegetation the cryosphere and seasonal snowpack are two dimensional layers with no volume the total depositional flux to the terrestrial surface is divided into a burial flux in the geological strata and a flux from the surface to atmosphere the global burial flux is assumed to be equivalent to global geogenic emissions the global flux from terrestrial surfaces to the atmosphere is distributed such that it is proportional to the total depositional flux i e at the terrestrial surface relative to the magnitude of the evasion between grids which is assumed to be identical to that of the total deposition between grids appendix a sections a1 a2 a3 and a4 describe in detail the transport model ecosystem model interfacial transport and atmospheric chemistry the ocean chemistry and ocean biotransfer are described in the following subsections 2 1 1 ocean chemistry in seawater the model considers dissolved hg0 hgii mmhg and dmhg and in sediment it considers the dissolved and particle associated hgii and mmhg fig 1 sediment is defined as an active layer with a height of 2 cm ikehara 1997 at which there are occurrences of substance exchange with the overlying seawater sediment burial to the deeper layer i e strata is assumed to be the final sink in the model table 1 provides a list of the mercury transformations in seawater and sediment the interfacial transport between seawater and sediment and sediment burial to strata that are considered in fate hg interfacial transport is considered only in coastal sediment because turbulence near the sea floor in the open ocean is assumed to be weak in this study the coastal ocean corresponds to depths shallower than 200 m the transformations and transports listed in table 1 are formulated by the first order equation the kinetic preprocessor kpp sandu and sander 2006 is used to create a module that numerically computes simultaneous differential equations for numerical solutions the model uses a second order rosenbrock method the time integration interval used to calculate seawater transformations is 1 h the rate coefficients listed in table 1 are the key model parameters for model performance we obtained these rate coefficients from previous studies that performed observational and modeling analyses with several modifications as described below the improvement of available parameterizations for rate coefficients or proposal of new parameterizations is not the focus of this study but we discuss how the available parameterizations work in our model the model considers seawater transformations as a redox between hg0 and hgii and as methylation and demethylation between hgii mmhg and dmhg photolytic reactions occur in the euphotic zone while only biotic reactions occur in deeper layers i e aphotic zone demethylation may occur from mmhg to hg0 however this reaction is relatively minor sharif et al 2014 and is therefore not considered in our model rate coefficients for the photolytic reactions of oxidation and reduction i e kpox and kpred in table 1 are parameterized as linear functions of net shortwave radiation at specific depths which is equivalent to that adopted in so10 zh15 so16 and sd17 attenuation of shortwave radiation within the water column is calculated using an empirical model developed in paulson and simpson 1977 dark oxidation with a rate coefficient kdox is also considered this reaction occurs in euphotic zones after sunset generated by the oxidants produced during the day rate coefficients for biotic reactions i e kbox and kbred for redox km1 km2 and km3 for methylation and kdm1 and kdm2 for demethylation are parameterized as linear functions of the poc remineralization rate ocrr for redox net primary production npp for methylation and demethylation in the euphotic zone and ocrr for methylation and demethylation in the deeper oceans i e aphotic zone respectively the parameterizations of kbox and kbred and km1 km2 km3 kdm1 and kdm2 are identical to those used in zh15 and sd17 respectively table a3 in appendix a provides a list of the observational studies that reported the rate coefficients for methylation and demethylation in seawater with available rate coefficients and the estimation method the observed rate coefficients and values adopted in previous models are summarized in fig 2 observations of methylation and demethylation rates between hgii and mmhg have increased in recent years on the other hand studies on other rate coefficients are scarce with only one or two data sources available in addition to transformations in seawater fate hg considers demethylation from dmhg to mmhg in the lower troposphere in other words dmhg in the upper ocean diffuses into the atmosphere where it rapidly transforms to mmhg niki et al 1983a 1983b or hgii sommar et al 1997 thomsen and egsgaard 1986 and is then deposited in the upper ocean in this study we assumed that mmhg is a dominant product of gaseous dmhg demethylation in the lower troposphere black et al 2009 indicated that the dmhg loss rate via diffusion is possibly on the same order of magnitude as photodecomposition rate coefficients for sediment transformations i e ksm ksdm and interfacial transport i e kdiff1 kdiff2 krs1 krs2 ksb1 ksb2 were obtained from a modeling study performed in the canadian coastal ocean sunderland et al 2010 2 1 2 oceanic biotransfer fate hg computes the mercury concentration in pom as steadily existing exporting and remineralizing fractions for each ocean grid using the seawater pom partitioning coefficient kd kg wet l the concentrations of mmhg in pom are explicitly calculated while the concentrations of inorganic mercury are calculated using the ratio of mmhg to hgt in pom van der velden et al 2013 on a wet weight basis the mercury in both remineralizing pom and exporting pom are subsequently converted to mercury fluxes via pom remineralization within the grid and pom removal from the grid respectively we assume that pom consists of both living and dead phytoplankton and zooplankton that phytoplankton in the euphotic zone can be either alive or dead all phytoplankton are dead in the aphotic zone and that all zooplankton are alive and dead in euphotic zone and aphotic zone respectively phytoplankton cell viability in the euphotic zone is set at a constant value of 0 8 which coincides with the reported range for natural surface waters agust√≠ and s√°nchez 2002 alonso laita and agust√≠ 2006 hayakawa et al 2008 we determined the mmhg bioconcentration factor bcf for living phytoplankton using a model developed in schartup et al 2018 in this model both the bcf dependency of living phytoplankton on dissolved organic carbon concentration and phytoplankton cell surface area to the volume ratio were taken into account the phytoplankton are divided into three size classes picoplankton diameter of 0 2 2 Œºm nanoplankton 2 20 Œºm and microplankton 20 200 Œºm and their fractions are determined by empirical formulas with chl see table a4 in appendix a for practical purposes we assumed that the bcf of living zooplankton is a constant value i e log10bcf 5 this value is an average of the observed values at the continental margin of the northwest atlantic ocean hammerschmidt et al 2013 and in the central pacific ocean gosnell and mason 2015 experimental studies have suggested that the bcf of living phytoplankton is larger than that of dead phytoplankton lee and fisher 2016 pickhardt and fisher 2007 tada and marumoto 2019 this is because mmhg uptake by phytoplankton consists of two pathways i e penetration into the cytoplasm active uptake and passive absorption onto the cell surface where the former pathway does not occur in dead cells we assume that the bcf of dead phytoplankton is 0 4 times smaller than that of living phytoplankton this scaling factor is based on the observed range for diverse marine microalgae 0 28 0 56 tada and marumoto 2019 and is consistent with the observation that 63 of mmhg in marine diatoms is found in the cytoplasm mason et al 1996 section a5 table a4 and fig a2 in appendix a provide full descriptions of the formulas and parameters used as well as a comparison of the modeled and observed mmhg kd fig 3 shows the modeled long term mean of the mean annual mmhg kd the modeled kd in the coastal ocean is generally smaller than that of the open ocean by several orders of magnitude in the open ocean this parameter had a range of approximately half an order of magnitude with relatively smaller values in tropical oceans and the northern part of the north atlantic the distribution of kd values is due to different productivities in the various regions in more productive areas of the oceans doc is higher larger phytoplankton dominate and thus the rate of mmhg uptake decreases the global average of the modeled log10kd is 5 66 which is slightly smaller than that of the mmhg in phytoplankton at log10bcf 5 78 fig a1b and larger than that used in sd17 5 3 2 2 simulation design and data simulations were performed for the period 1750 2010 the first 100 years were used as a spin up period and were not analyzed in this study fate hg is computationally expensive at a full spatial resolution and therefore its use with respect to long term simulations is impractical instead we used a low resolution model the horizontal resolution of the low resolution model is 3 the internal processes considered vertical resolution and data used are identical to those used in the full resolution model for initial background hgt concentrations in the ocean we used the results of zh14 and sd17 in which they performed 10 000 and 5000 year simulations at natural conditions we obtained concentrations at three depths i e the mixed layer depth as well as 1000 and 3000 m depths from figure 4 in zh14 and fiture 3 in sd17 with 10 of horizontal resolution we used the average of these two data sources which were then linearly interpolated into our model grids fig a3 in appendix a the initial concentration of gaseous hg0 is set to 0 22 ng hg m3 throughout the troposphere zhang et al 2014a all other initial concentrations were set to zero in fate hg we used four categories of global data i e emission climate reactant and satellite data fate hg is an offline model not based on any general circulation or air quality models therefore fate hg is flexible when selecting data concerning climate and reactants streets et al 2017 estimated long term 1850 2010 anthropogenic emissions in seven regions oceania asia africa mid east former ussr europe and south and north america we incorporated these estimates for the 10 year interval and linearly interpolated this to estimate yearly emissions we then distributed the yearly emissions from the seven regions onto the model grid by assuming that the spatial distribution i e the relative amount of emission between grids is equivalent to the available inventory for 2010 amap unep 2013 in a specific region i e similar patterns were applied to each specific region fig a4 in appendix a summarizes the prepared gridded inventory this inventory is used in simulations for the period from 1850 to 2010 for anthropogenic emissions from 1750 to 1850 we used a constant global emission of 342 5 mg year distributed to grids with historical silver and gold mining activities lacerda 1997 the value 342 5 mg year is an average of global anthropogenic emission for 1450 1850 used in a previous modeling study streets et al 2011 climate data were obtained from the european centre for medium range weather forecasting ecmwf era interim 1979 to present ecmwf 2017 and the geophysical fluid dynamics laboratory gfdl ocean data assimilation experiments oda 1970 2007 zhang et al 2007 the reactant data used in atmospheric chemistry such as the o3 1979 to present and so2 2003 2012 mixing ratios were obtained from the ecmwf era interim and monitoring atmospheric composition and climate macc ecmwf 2017 satellite data were obtained from nasa seawifs 1997 2002 and modis terra 2003 to present nasa 2017 to retain data consistency throughout the simulation period we prepared mean long term data for years with available data which was repeatedly used except for the anthropogenic emissions and wind velocities for wind velocities which were used for advection calculations in the atmosphere we repeatedly used data from 2010 the input and forcing data used in this study are described in more detail in appendix a section a6 3 results and discussion 3 1 modeled concentration evaluation to perform model validation observational data for the concentrations of dissolved hgt mmhg dmhg and hg0 dgm in the ocean and gaseous hg0 gem above the ocean were compiled from available databases see table a5 and fig a5 in appendix a for this validation we exclude data obtained near the coast where the maximum water depth is shallower than 1000 m because our model does not consider riverine mercury input fig 4 shows the validation results for concentrations of hgt mmhg and dmhg we excluded data obtained in years earlier than 2000 appendix a provides a statistical summary of the validation of individual cruises table a6 and the same correlation diagrams as shown in fig 4 but also includes data obtained earlier prior to 2000 fig a6 also fig a6 shows the results for the gem in the lower troposphere and dissolved hg0 results from recent observations are superior to those from earlier observations and earlier observed concentrations tend to be overestimated compared with modeled concentrations the ocean is classified into three depth ranges the surface 0 70 m intermediate 70 1000 m and deep 1000 5500 m oceans hereinafter we use this classification we used log transformed hg concentrations in the validations included in this section and in discussions of global distribution in section 3 2 this is because 1 seawater concentrations in our dataset can vary over 100 fold depending on the month and region although in the open ocean the degree of such variation is decreased and 2 uncertainty in some key model parameters such as kd produces order of magnitude differences in the simulated hgt concentrations as discussed in section 3 3 in the surface ocean fate hg generally simulated hgt concentrations the upper panels in fig 4 for most of the recent cruises where differences in the modeled and observed concentrations are within one order of magnitude in the intermediate ocean the modeled concentrations are slightly underestimated compared with the observed concentrations in the deep ocean the modeled concentrations spanned approximately two orders of magnitude on the other hand the observed concentrations ranged over approximately one order of magnitude this is shown as vertically long plots in fig 4 and is evident in the larger standard deviations of modeled concentrations shown in table a6 our model could not successfully simulate such uniform hgt distribution in the deep ocean the results for dmhg were worse than the results for hgt and mmhg for all depths as indicated by the considerable variation in the plots in the surface intermediate ocean and deep ocean the modeled mmhg concentrations tend to be underestimated and overestimated respectively compared with the observed concentrations as indicated by the positive and negative biases shown in table a6 we note that our model simulated gem concentrations in the atmospheric planetary boundary layer apbl with a much smaller variation in the plots see fig a6 in appendix a based on the results for the hgt concentrations in the surface ocean we can partly explain differences in model data by interlaboratory variability in the observed data interlaboratory variability could explain a maximum of 40 of these differences lamborg et al 2012 neglecting the interannual variability in concentrations induced by the interannual variability in climate i e the use of mean long term input data is possibly another reason for scatter in the plot underestimation of the modeled hgt concentrations in the intermediate and deep oceans is possibly due to overestimation of the seawater pom partitioning coefficient because this process is sensitive to simulated mercury levels the uncertainty in the initial background concentration used in the simulation is another possible cause these issues are discussed later with respect to fig 6 in section 3 3 the poor fit between the measured and modeled concentrations of methylated mercury may be caused by insufficient parameterization of the rate coefficients for seawater transformations in this study we adopted the parameterizations used in previous models without modification or used constant observed values there is uncertainty in the rate coefficients for biotic reductions for the rate coefficients of biotic oxidation and reduction i e kbox and kbred in table 1 we adopted the parameterization used in zh14 the constant proportionalities that relate kbox and kbred as well as ocrr are adjusted values determined to force the modeled hgii hg0 ratio to be consistent with the observed ratio therefore these parameterizations are not supported by observation but are model dependent uncertainty exists as to whether biotic oxidation occurs in the ocean for example so10 and so16 do not take into account this reaction however if this reaction is not considered simulated hg0 concentrations become too low compared with the observed concentrations not shown recent years have seen an increase in the number of observational studies on rate coefficients for methylation and demethylation in the water column using stable isotopes fig 2 table a3 even though studies performed in the open ocean are limited e g the mediterranean sea monperrus et al 2007 and canadian arctic archipelago lehnherr et al 2011 on the other hand there are only a handful of studies for the other rates earlier studies on the creation and decomposition of dmhg in seawater mason and sullivan 1999 are more uncertain because these studies are not based on stable isotope analysis estimating the npp and ocrr from satellite observations with satellite based empirical models is relatively straightforward however their use as relevant variables for biotic methylation and demethylation is not sufficiently supported by observation for example in the canadian arctic archipelago the rates of methylation and demethylation at the depths of surface chlorophyll maxima and oxycline are similar lehnherr et al 2011 as shown in fig 2 observed methylation and demethylation rates differed significantly these site to site differences could not be fully explained by the differences in the npp or ocrr the poor fit between the measured and modeled methylated mercury concentrations may also be caused by problems in the methods used for analysis dmhg is known to decompose to mmhg at low ph values black et al 2009 when acidification is used as a preservation technique this will yield a mmhg dmhg ratio that is too high in general the fit with the measured values is not as good when predicting the concentrations of specific forms of mercury as compared with predicting the hgt concentrations therefore in the following discussion on the ocean we focus on hgt concentrations the ability to improve the predictions of methylated mercury concentrations requires more observational studies on the rate coefficients of methylation and demethylation in seawater also we do not discuss hgt distribution in the deeper ocean because fate hg overpredicts the range of hgt concentrations in the deeper ocean 3 2 global distribution fig 5 shows the simulated distributions of hgt in the surface ocean along with the distributions of gem in the apbl and the dry and wet deposition of hgt mean annual concentrations or annual fluxes for 2010 are shown when available the observed concentrations are plotted on the modeled concentrations although the observed data are corrected for various years and months the gem is characterized by higher concentrations near terrestrial emission sources and is uniformly distributed in remote regions gem concentrations are generally higher in the northern hemisphere than in the southern hemisphere fig 5a the dry and wet deposition fluxes had higher values at low latitudes and correlate with higher concentrations of the major gem oxidants i e o3 and oh and the amount of precipitation fig 5b the distribution of hgt in the surface ocean fig 5c does not significantly correlate with the distribution of dry and wet deposition fluxes although atmospheric deposition is a principal source of oceanic mercury mason et al 2012 this indicates that not only deposition from the atmosphere but also several internal processes within the interior of the ocean such as physical transport and export associated with the biologically driven carbon cycle determine the hgt distribution in the surface ocean hgt concentrations tend to be higher in regions with higher deposition and upwelling flows fig a7 e g high latitude regions in the north pacific and north atlantic as well as near the antarctic circumpolar current so10 reported similar analyses for global distributions of hg concentrations and fluxes in the surface ocean in which the results are obtained from geos chem simulations figures 3 and 4 in soerensen et al 2010 the major difference between the results in so10 and our results can be found in the distribution of atmospheric deposition in so10 higher deposition occurs at higher latitudes which is likely due to the fact that so10 incorporates br chemistry in the atmosphere br chemistry shifts high deposition regions to higher latitudes holmes et al 2010 oxidation of gem by o3 and oh are thermodynamically unfavorable and no longer considered to be accurate the differences in hgt concentrations between this study and so10 are particularly apparent in the north atlantic in so10 inorganic hg concentrations in the north atlantic are higher than concentrations in other regions however inorganic hg concentrations are lower in our simulation except for off the coast of greenland where deep water formation occurs this difference may be caused by different settings in the initial background concentration and seawater pom partitioning coefficient the initial background concentrations of the intermediate and deep ocean in the north atlantic used in this study are lower than those in other sections of the northern hemisphere fig a3 as discussed later i e the discussion for fig 7 in section 3 3 with respect to the hgt levels in the ocean the north atlantic is the most sensitive region to historical anthropogenic emissions however the simulated hgt concentrations may still not reach their current levels in the north atlantic in fact hgt concentrations from recent exploratory cruises in the north atlantic positive biases in geotraces ga01 and ga03 shown in table a6 showed higher values than those simulated figs 4 and 5c also as shown in fig 3 estimates for the mmhg kd levels at mid latitudes in the north atlantic are relatively higher than the values in other sections therefore hgt in the surface ocean throughout the north atlantic is more likely to be exported by particle removal in our model calculations consequently we estimate that mercury export from the surface to deeper ocean via particle removal in this area is dominant as compared with other areas 3 3 long term variation fig 6 shows the simulated long term variation in the global concentrations of hgt at the apbl surface ocean intermediate ocean and deep ocean fig 6 also shows the annual global anthropogenic emissions into the atmosphere and the results from three sensitivity runs sr1 3 in addition to results from a reference run ref in sr1 the mmhg bcf for dead phytoplankton is assumed to be equivalent to that for living phytoplankton i e in sr1 the lack of active uptake in dead phytoplankton considered in ref is not considered in sr2 a prescribed constant value for the seawater pom partitioning coefficient log10kd 5 3 is used for both hgii and mmhg in sr3 initial background concentrations in the ocean are doubled for all depths prior to the simulation the objective with respect to sensitivity analyses is to investigate how uncertainties in the initial background concentration settings in the ocean and parameterization of the seawater pom partitioning coefficient affect the simulated hgt levels as we described in the preceding section section 2 2 the initial background concentrations used in this study are not based on simulations at natural conditions over a period of thousands of years but rather are based on results from previous modeling studies therefore uncertainty exists in the initial background concentration settings to discuss the impact of past anthropogenic emissions on present day mercury content herein we define the relative increase in mercury content as the 2010 mercury content normalized by the 1850 mercury content when anthropogenic emissions began to clearly increase in ref the relative increase of hgt in the apbl surface ocean intermediate ocean and deep ocean are 2 3 1 9 1 8 and 2 2 respectively the relative increase of the hgt in the apbl 2 3 is consistent with that of atmospheric deposition increased 2 5 fold as inferred by observations from lake sediments peat bogs and ice cores fitzgerald et al 2007 morel et al 1998 estimations for the increase in oceanic hgt due to historical anthropogenic emissions vary considerably in different models the result for the relative increase of hgt in the surface ocean 1 9 is consistent with increases from previous modeling studies covering the years from 1850 onwards 1 25 3 selin et al 2008 zhang et al 2014b mason et al 1994 lamborg et al 2002 mason and sheu 2002 sunderland and mason 2007 the relative increase of hgt in the intermediate and deep ocean from previous studies which incorporate pre 1850 emissions is 1 5 5 3 for the intermediate ocean and 1 2 2 1 for the deep ocean zhang et al 2014b streets et al 2011 amos et al 2013 by taking the influence of pre 1850 emissions into account our estimates for the intermediate 1 8 and deep oceans 2 2 are consistent with and larger than estimates from previous studies respectively the hgt concentration in the apbl rapidly tracks changing anthropogenic emissions due to its relatively short residence time in the atmosphere on the other hand the hgt contents in the ocean showed phase lags after anthropogenic emissions this is evident even in the surface ocean for example the local maximum for hgt contents in the surface ocean that correspond to emissions from 1970 roughly appeared around 1975 these phase lags are comparable in both the surface and intermediate ocean and longer in the deep ocean due to its larger inertia compared with the results of ref in sr1 and sr2 hgt contents in the apbl surface ocean and intermediate ocean showed significantly lower values despite a relative increase similar to that of the ref this may have been induced by an overly large kd in sr1 and sr2 if we do not consider the decrease of the bcf in dead phytoplankton mercury removal via particle removal becomes too large which causes the apparent depletion of mercury from the water column in our model simulated mercury levels in the lower troposphere and surface intermediate ocean are highly sensitive to the seawater pom partitioning coefficient given that particle removal is a major pathway by which mercury is removed from the ocean this is expected the uncertainty in the seawater pom partitioning modeling may be a cause of the disagreement between the modeled and observed concentrations in fig 4 and a6 in sr3 hgt contents had higher values than those in ref at all depths and this difference decreased with time at present 2010 hgt contents in the apbl surface ocean intermediate ocean and deep ocean in sr3 are 1 04 1 05 1 05 and 1 17 times larger than the concentrations in ref respectively based on this result we find that at present the uncertainty in the initial background concentrations in the ocean is insensitive to hgt concentrations in both the apbl and surface intermediate ocean on the other hand this may be a possible cause that produces model data disagreements in the hgt levels in the deep ocean fig 7 is similar to fig 6 but shows the results for ref in the seven global oceans north pacific south pacific north atlantic south atlantic indian ocean arctic ocean and antarctic ocean see fig a8 in appendix a for geographical classifications where we replace the y axis with the relative increase in hgt throughout the analyzed period 1850 2010 both the arctic ocean and north atlantic are sensitive to anthropogenic emissions i e the relative hgt increase in these oceans is larger than increases in other oceans on the other hand the estimated hgt concentrations tend to be underestimated in the surface intermediate ocean when compared with observed concentrations from recent cruises in the north atlantic positive biases in geotraces ga01 and ga03 in table a6 if the initial background concentrations for the north atlantic fig a3 are properly set the estimated relative hgt increase in the north atlantic is still underestimated compared with actual values the geographic proximity to emissions is one of the factors that produces differences in the relative increases between oceans in fact oceans in the southern hemisphere and indian ocean are characterized by a lower relative increase the well known characteristic that chemicals characterized by long range transport tend to accumulate in polar regions wania and mackay 1995 is possibly another factor for the relatively high sensitivity of the arctic ocean to anthropogenic emissions given the relatively low sensitivity of the north pacific to anthropogenic emissions compared with other oceans in the northern hemisphere water age may have a certain influence the high latitude north atlantic is a well known location for deep water formation water in the north atlantic and surface of the arctic ocean is relatively young semeniuk and dastoor 2017 whereas water in the north pacific is older oceans with younger water are more sensitive to historical anthropogenic emissions in our simulations the atmospheric gem rapidly follows changing anthropogenic emissions in magnitude and location historically emissions from north america and europe are greater than that from other regions fig a4 in appendix a therefore the increase in gem in the arctic ocean and north atlantic is larger than other oceans recent years have seen an increasing amount of anthropogenic emissions from asia which has rendered the observation of differences between oceans unclear the hgt contents in the oceans showed phase lags from the anthropogenic emissions as in fig 6 and the degree of these phase lags differed between the seven oceans the arctic ocean responds quickly to changing anthropogenic emissions as compared with the north atlantic oceans in the southern hemisphere also tend to respond slowly compared with oceans in the northern hemisphere zhang et al 2014b reported similar analyses of long term variations in oceanic hgt levels with changes in anthropogenic emissions they estimated that the contribution of historical emissions to present day concentrations are relatively smaller in the arctic ocean these results are opposite of this study s results on the other hand the contributions are relatively higher in the intermediate deep ocean in the north atlantic and relatively smaller in the pacific ocean in zhang et al 2014b these results are qualitatively consistent with the results of this study so16 also reported similar analyses for the arctic ocean they estimated that hgt in the polar mixed layer subsurface water and deep ocean have increased by 1 5 1 4 and 1 2 fold respectively from 1850 to the present these relative increases are smaller than those in this study the relative increase for the arctic ocean in this study is generally larger than that in previous studies which is possibly due to the fact that the fate hg does not consider sea ice cover 3 4 global budgets fig 8 shows the global hgt budget for 2010 simulated by the fate hg the simulated dry and wet deposition to the sea surface 3590 mg year 1 and hgt contents in the surface ocean 2900 mg are comparable with those in the amap unep amap un environment 2019 3800 mg year 1 and 2600 mg respectively on the other hand the simulated net evasion from the sea surface to the atmosphere 1730 mg year 1 and hgt contents in the intermediate deep ocean 406 7 gg are smaller and larger than those in the amap unep 3400 mg year 1 and 310 gg respectively the estimated mercury contents in the ocean from previous modeling studies varies significantly selin et al 2008 zhang et al 2014b mason et al 1994 lamborg et al 2002 mason and sheu 2002 sunderland and mason 2007 streets et al 2011 amos et al 2013 the estimated hgt content in the ocean in this study 409 6 gg is 1 6 16 9 times larger than previous estimates however on average the simulated hgt concentrations in the ocean in this study never overestimated large numbers of observed concentrations but rather slightly underestimated them fig 4 therefore the actual content is likely larger than the estimate in this study the estimated overall turnover time for hgt in the ocean is 320 years which is considerably shorter than estimates from previous modeling studies 3000 years in mason and sheu 2002 600 years in selin et al 2008 1400 years in amos et al 2013 2000 years in zh14 1082 years in sd17 the turnover time in this study is defined identically to sd17 i e as the amount of hgt in an ocean layer divided by the export flux from the base of the layer the turnover time in the surface ocean in this study is 0 6 years which is comparable with that reported in zh14 7 months and shorter than that in sd17 2 3 years results suggest that the turnover time is 13 and 320 years in the intermediate and the deep ocean respectively which are both shorter than those in zh14 120 and 2000 years and sd17 214 and 1476 years in the ocean vertical transport via the biological pump is the dominant removal pathway for mercury zhang et al 2014a semeniuk and dastoor 2017 therefore the estimated turnover time is highly dependent on the poc export flux different formulations for the poc export profile can possibly explain the different estimations for the hgt turnover time in the ocean between models in this study the non dimensional flux profile is calculated with the following equation 1 s z s 1 e k s z s 2 where s z is the poc export flux at depth z m normalized by the poc export flux at the depth of the euphotic zone and s1 1 39 s2 0 046 and ks 1 317 are empirical parameters determined by averaging regional estimates lutz et al 2002 formulas that assume a simple logarithmic decay for the poc export flux with depth such as the model developed in martin et al 1987 are commonly used in biogeochemical models however such formulations underestimate the poc export flux on the sea floor lutz et al 2002 and therefore underestimate the hgt export flux on the sea floor and overestimate the overall turnover time in the ocean in fact the hgt export flux on the sea floor estimated in this study 1190 mg year 1 is approximately 5 times larger than similar estimates from the offtrac hg 240 mg year 1 zhang et al 2014b which uses the formulation reported in martin et al 1987 for the poc export flux profile considering refractory material in the poc against remineralization i e including s2 in equation 1 will improve estimations of the poc export flux on the sea floor however the empirical parameters i e s1 s2 and ks in equation 1 are uncertain and require more investigation because these parameters vary between ocean sections lutz et al 2002 and are not a constant value as assumed in this study 4 conclusions we developed a new global model fate hg for predicting biogeochemical cycling of mercury in the ocean fate hg is an offline model that is not based on any gcms or air water quality models therefore it is flexible for selecting data concerning climate and reactants it was developed based on coupled atmosphere ocean chemical transport models and considers explicit methylated mercury cycles in the ocean it also considers for the first time spatial and temporal variations of the seawater pom partitioning coefficient which is highly sensitive to simulated hgt levels in the surface intermediate ocean this new model may be a useful tool for assessing the effectiveness of future efforts to control mercury pollution undertaken in response to the minamata convention on mercury on the other hand further development is required to develop a state of the art multimedia model one limitation of the current version of the fate hg model is that it employs conventional atmospheric o3 oh chemistry the other limitation is the lack of explicit consideration of soil vegetation compartments with river discharge improvements to these limitations are ongoing in this study we tested the available parameterizations for rate coefficients of mercury transformations in seawater this resulted in worse agreements between modeled and observed concentrations of specific mercury forms accumulation of additional observational data and improving parameterization for these rate coefficients are required for future model development declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments this research was partly supported by the environment research and technology development fund 5 1405 5 1702 of the ministry of the environment japan and jsps kakenhi grant number 16k00524 appendix a supplementary data the following is the supplementary data to this article multimedia component 1 multimedia component 1 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2019 104599 
26089,we developed a new global model to predict biogeochemical cycling of mercury in the ocean we describe and evaluate the model and discuss mercury levels distribution and budgets based on a simulation with a total time span of 260 years the model is based on a fully coupled atmosphere ocean chemical transport model and considers methylated mercury production in the water column followed by biotransfer to lower order marine organisms including spatial and temporal variations in partitioning properties model validation shows that we can simulate total dissolved mercury hgt concentrations in the surface ocean with model data differences at a maximum of one order of magnitude the simulated oceanic hgt content is currently 2010 1 6 16 9 times larger than previously modeled estimates the estimated overall turnover time of oceanic hgt determined by our model is 320 years which is shorter than suggested by previous modeling studies keywords mercury ocean biotransfer global model software availability name of software fate hg developer t kawai contact email kawai toru nies go jp year first available 2019 hardware required unix linux software required netcdf availability development version available for contact developers through github cost free language fortran90 program size 3 0 mb source files 1 introduction mercury especially in its methylated form is a notorious neurotoxin and numerous studies have documented its toxic effects on human health ha et al 2017 additionally exposure to mercury can induce a variety of adverse effects in wildlife such as birds scheuhammer et al 2007 mammals scheuhammer et al 2007 and fishes scheuhammer et al 2007 depew et al 2012 morcillo et al 2017 at physiologic histologic bio chemical enzymatic and genetic levels mercury is emitted from both anthropogenic and geogenic sources and cycles through the atmosphere ocean soil vegetation and biosphere taking elemental oxidized and methylated forms anthropogenic mercury emissions likely began around 2700 bc with the advent of mining precious metals lacerda 1997 and has increased since the industrial revolution with peaks around 1890 and 1970 streets et al 2017 north america and europe previously generated the highest levels of atmospheric emissions 75 in 1890 and 60 in 1970 if all source sectors are combined but asia has become the largest polluter in recent years approx 60 in 2010 mercury emitted to the atmosphere is transported to remote regions via atmospheric general circulation the residence time of gaseous elemental mercury gem i e the dominant form found in the atmosphere is approximately 0 5 1 year holmes et al 2010 which allows the dispersion of gem over the entire global atmosphere diffusive transport from continental areas and the oceans to the atmosphere is also significant because mercury is a highly volatile heavy metal the release of mercury from continental areas and the ocean is several times greater than anthropogenic emissions to the atmosphere amap unep 2013 amap un environment 2019 mercury cycles on a global scale and across diverse environmental media sets this element apart from other heavy metals in terms of environmental dynamics in october 2013 the minamata convention on mercury was finalized and enforced starting in august 2017 the convention was one of several international efforts devoted to reducing anthropogenic mercury emissions numerical modeling is useful for evaluating the effects that reduced anthropogenic mercury emissions which arise from such efforts have on the levels and distributions of mercury in environmental media and biota several 3 d dynamic global to regional mercury models have been developed based on the atmospheric general circulation or air quality models pirrone and keating 2010 on the other hand global ocean models are less developed and studied interfacial transport is often treated as secondary emissions the ocean plays a central role in mercury biogeochemical cycles and are a large reservoir for mercury which suggests the possibility that transport from the ocean to atmosphere is a primary source of atmospheric mercury the oceans also produce methylated mercury and accumulation is common in fish consumption of this food source is a primary pathway for human exposure to mercury mergler et al 2007 therefore ocean modeling is important not only for discussing the role of the mercury cycle in the environment but also to link dynamic modeling and risk assessments the geos chem selin et al 2008 strode et al 2007 implements a mixed layer slab ocean submodel soerensen et al 2010 referred to as so10 zhang et al 2014a referred to as zh14 for the first time developed a global mercury ocean model offtrac hg based on the ocean general circulation model ogcm using this model they analyzed and discussed preindustrial background concentrations based on a 10 000 year simulation without anthropogenic emissions they also coupled geos chem and offtrac hg and showed that the coupled atmosphere ocean model increases the air sea exchange flux by 12 compared to the uncoupled model zhang et al 2019 previous studies have observed methylated mercury in open ocean seawater with maximum concentrations typically near the oxycline sunderland et al 2009 mason et al 2012 kim et al 2017 although there is a lack of quantitative understanding of the source and pathways of methylated mercury more recent models have considered explicit methylated mercury cycles such as the arctic box model developed in soerensen et al 2016 referred to a so16 and the ogcm based global ocean model reported in semeniuk and dastoor 2017 referred to as called sd17 previously we developed a finely advanced transboundary environmental model known as fate to model chlorinated and brominated organic compounds such as polychlorinated biphenyls kawai et al 2014 handoh and kawai 2014 in this study we incorporated mercury processes into fate to be able to model mercury fate hg the fate hg model takes into account methylated mercury production in the water column followed by biotransfers to lower order marine organisms i e particle organic matter pom and the subsequent export to the deeper ocean via the biological pump the major differences in the fate hg model with respect to preceding models soerensen et al 2010 2016 zhang et al 2014a 2014b semeniuk and dastoor 2017 horowitz et al 2017 mason et al 1994 lamborg et al 2002 mason and sheu 2002 sunderland and mason 2007 streets et al 2011 amos et al 2013 2015 are twofold 1 the model is based on a fully coupled atmosphere ocean chemical transport model and 2 the model considers spatial and temporal variations in the seawater pom partitioning coefficient observational studies lamborg et al 2016 have indicated that this parameter varies significantly in both time and space the aim of this study is to describe and evaluate fate hg and to investigate the human impact on the fate of mercury with a particular focus on the ocean using fate hg we discuss long term 1850 2010 trends in mercury levels and the global distribution and budget of mercury based on a centurial simulation with changes in historical anthropogenic emissions 2 methods 2 1 model description fate hg is a global model capable of simulating the biogeochemical cycles of mercury in and across the troposphere continental areas ocean sediment and lower order marine organisms the transport and ecosystem sub models implemented by fate hg provide the basis for the physical and biological processes the coupled atmosphere ocean chemical transport model computes 3 d non steady advective and diffusive transport of gaseous elemental divalent and particle associated mercury in the troposphere as well as dissolved elemental hg0 hgii mmhg and dimethyl dmhg mercury in the ocean the spatial resolutions of the troposphere and ocean are 0 75 horizontal 35 œÉ layers vertical and 1 0 50 layers 0 5500 m respectively the governing equations and physical schemes are identical to those in fate kawai et al 2014 handoh and kawai 2014 but fate hg uses different climate forcing data the ecosystem model uses satellite data to estimate pom concentrations pom export flux via the biological pump pom remineralization flux in seawater pom burial flux to sediment and pom remineralization flux from sediments fig 1 summarizes the mercury processes that are considered in the model fate hg also takes into account transformations in air and cloud water interfacial transports transformations in seawater and sediment transport from sediment to seawater via diffusion and resuspension and the biotransfer to lower order marine organisms i e pom followed by deep sea export via the biologically driven carbon cycle processes for interfacial transport at land sea surfaces consist of dry and wet deposition and the bidirectional diffusive transport of elemental mercury dry deposition is further classified into the deposition of particle associated mercury and gaseous mercury atmospheric chemistry is based on o3 and oh chemistry as major gem oxidants shia et al 1999 seigneur et al 2006 the model does not explicitly consider terrestrial compartments but does calculate interfacial transport we assume that surface soil vegetation the cryosphere and seasonal snowpack are two dimensional layers with no volume the total depositional flux to the terrestrial surface is divided into a burial flux in the geological strata and a flux from the surface to atmosphere the global burial flux is assumed to be equivalent to global geogenic emissions the global flux from terrestrial surfaces to the atmosphere is distributed such that it is proportional to the total depositional flux i e at the terrestrial surface relative to the magnitude of the evasion between grids which is assumed to be identical to that of the total deposition between grids appendix a sections a1 a2 a3 and a4 describe in detail the transport model ecosystem model interfacial transport and atmospheric chemistry the ocean chemistry and ocean biotransfer are described in the following subsections 2 1 1 ocean chemistry in seawater the model considers dissolved hg0 hgii mmhg and dmhg and in sediment it considers the dissolved and particle associated hgii and mmhg fig 1 sediment is defined as an active layer with a height of 2 cm ikehara 1997 at which there are occurrences of substance exchange with the overlying seawater sediment burial to the deeper layer i e strata is assumed to be the final sink in the model table 1 provides a list of the mercury transformations in seawater and sediment the interfacial transport between seawater and sediment and sediment burial to strata that are considered in fate hg interfacial transport is considered only in coastal sediment because turbulence near the sea floor in the open ocean is assumed to be weak in this study the coastal ocean corresponds to depths shallower than 200 m the transformations and transports listed in table 1 are formulated by the first order equation the kinetic preprocessor kpp sandu and sander 2006 is used to create a module that numerically computes simultaneous differential equations for numerical solutions the model uses a second order rosenbrock method the time integration interval used to calculate seawater transformations is 1 h the rate coefficients listed in table 1 are the key model parameters for model performance we obtained these rate coefficients from previous studies that performed observational and modeling analyses with several modifications as described below the improvement of available parameterizations for rate coefficients or proposal of new parameterizations is not the focus of this study but we discuss how the available parameterizations work in our model the model considers seawater transformations as a redox between hg0 and hgii and as methylation and demethylation between hgii mmhg and dmhg photolytic reactions occur in the euphotic zone while only biotic reactions occur in deeper layers i e aphotic zone demethylation may occur from mmhg to hg0 however this reaction is relatively minor sharif et al 2014 and is therefore not considered in our model rate coefficients for the photolytic reactions of oxidation and reduction i e kpox and kpred in table 1 are parameterized as linear functions of net shortwave radiation at specific depths which is equivalent to that adopted in so10 zh15 so16 and sd17 attenuation of shortwave radiation within the water column is calculated using an empirical model developed in paulson and simpson 1977 dark oxidation with a rate coefficient kdox is also considered this reaction occurs in euphotic zones after sunset generated by the oxidants produced during the day rate coefficients for biotic reactions i e kbox and kbred for redox km1 km2 and km3 for methylation and kdm1 and kdm2 for demethylation are parameterized as linear functions of the poc remineralization rate ocrr for redox net primary production npp for methylation and demethylation in the euphotic zone and ocrr for methylation and demethylation in the deeper oceans i e aphotic zone respectively the parameterizations of kbox and kbred and km1 km2 km3 kdm1 and kdm2 are identical to those used in zh15 and sd17 respectively table a3 in appendix a provides a list of the observational studies that reported the rate coefficients for methylation and demethylation in seawater with available rate coefficients and the estimation method the observed rate coefficients and values adopted in previous models are summarized in fig 2 observations of methylation and demethylation rates between hgii and mmhg have increased in recent years on the other hand studies on other rate coefficients are scarce with only one or two data sources available in addition to transformations in seawater fate hg considers demethylation from dmhg to mmhg in the lower troposphere in other words dmhg in the upper ocean diffuses into the atmosphere where it rapidly transforms to mmhg niki et al 1983a 1983b or hgii sommar et al 1997 thomsen and egsgaard 1986 and is then deposited in the upper ocean in this study we assumed that mmhg is a dominant product of gaseous dmhg demethylation in the lower troposphere black et al 2009 indicated that the dmhg loss rate via diffusion is possibly on the same order of magnitude as photodecomposition rate coefficients for sediment transformations i e ksm ksdm and interfacial transport i e kdiff1 kdiff2 krs1 krs2 ksb1 ksb2 were obtained from a modeling study performed in the canadian coastal ocean sunderland et al 2010 2 1 2 oceanic biotransfer fate hg computes the mercury concentration in pom as steadily existing exporting and remineralizing fractions for each ocean grid using the seawater pom partitioning coefficient kd kg wet l the concentrations of mmhg in pom are explicitly calculated while the concentrations of inorganic mercury are calculated using the ratio of mmhg to hgt in pom van der velden et al 2013 on a wet weight basis the mercury in both remineralizing pom and exporting pom are subsequently converted to mercury fluxes via pom remineralization within the grid and pom removal from the grid respectively we assume that pom consists of both living and dead phytoplankton and zooplankton that phytoplankton in the euphotic zone can be either alive or dead all phytoplankton are dead in the aphotic zone and that all zooplankton are alive and dead in euphotic zone and aphotic zone respectively phytoplankton cell viability in the euphotic zone is set at a constant value of 0 8 which coincides with the reported range for natural surface waters agust√≠ and s√°nchez 2002 alonso laita and agust√≠ 2006 hayakawa et al 2008 we determined the mmhg bioconcentration factor bcf for living phytoplankton using a model developed in schartup et al 2018 in this model both the bcf dependency of living phytoplankton on dissolved organic carbon concentration and phytoplankton cell surface area to the volume ratio were taken into account the phytoplankton are divided into three size classes picoplankton diameter of 0 2 2 Œºm nanoplankton 2 20 Œºm and microplankton 20 200 Œºm and their fractions are determined by empirical formulas with chl see table a4 in appendix a for practical purposes we assumed that the bcf of living zooplankton is a constant value i e log10bcf 5 this value is an average of the observed values at the continental margin of the northwest atlantic ocean hammerschmidt et al 2013 and in the central pacific ocean gosnell and mason 2015 experimental studies have suggested that the bcf of living phytoplankton is larger than that of dead phytoplankton lee and fisher 2016 pickhardt and fisher 2007 tada and marumoto 2019 this is because mmhg uptake by phytoplankton consists of two pathways i e penetration into the cytoplasm active uptake and passive absorption onto the cell surface where the former pathway does not occur in dead cells we assume that the bcf of dead phytoplankton is 0 4 times smaller than that of living phytoplankton this scaling factor is based on the observed range for diverse marine microalgae 0 28 0 56 tada and marumoto 2019 and is consistent with the observation that 63 of mmhg in marine diatoms is found in the cytoplasm mason et al 1996 section a5 table a4 and fig a2 in appendix a provide full descriptions of the formulas and parameters used as well as a comparison of the modeled and observed mmhg kd fig 3 shows the modeled long term mean of the mean annual mmhg kd the modeled kd in the coastal ocean is generally smaller than that of the open ocean by several orders of magnitude in the open ocean this parameter had a range of approximately half an order of magnitude with relatively smaller values in tropical oceans and the northern part of the north atlantic the distribution of kd values is due to different productivities in the various regions in more productive areas of the oceans doc is higher larger phytoplankton dominate and thus the rate of mmhg uptake decreases the global average of the modeled log10kd is 5 66 which is slightly smaller than that of the mmhg in phytoplankton at log10bcf 5 78 fig a1b and larger than that used in sd17 5 3 2 2 simulation design and data simulations were performed for the period 1750 2010 the first 100 years were used as a spin up period and were not analyzed in this study fate hg is computationally expensive at a full spatial resolution and therefore its use with respect to long term simulations is impractical instead we used a low resolution model the horizontal resolution of the low resolution model is 3 the internal processes considered vertical resolution and data used are identical to those used in the full resolution model for initial background hgt concentrations in the ocean we used the results of zh14 and sd17 in which they performed 10 000 and 5000 year simulations at natural conditions we obtained concentrations at three depths i e the mixed layer depth as well as 1000 and 3000 m depths from figure 4 in zh14 and fiture 3 in sd17 with 10 of horizontal resolution we used the average of these two data sources which were then linearly interpolated into our model grids fig a3 in appendix a the initial concentration of gaseous hg0 is set to 0 22 ng hg m3 throughout the troposphere zhang et al 2014a all other initial concentrations were set to zero in fate hg we used four categories of global data i e emission climate reactant and satellite data fate hg is an offline model not based on any general circulation or air quality models therefore fate hg is flexible when selecting data concerning climate and reactants streets et al 2017 estimated long term 1850 2010 anthropogenic emissions in seven regions oceania asia africa mid east former ussr europe and south and north america we incorporated these estimates for the 10 year interval and linearly interpolated this to estimate yearly emissions we then distributed the yearly emissions from the seven regions onto the model grid by assuming that the spatial distribution i e the relative amount of emission between grids is equivalent to the available inventory for 2010 amap unep 2013 in a specific region i e similar patterns were applied to each specific region fig a4 in appendix a summarizes the prepared gridded inventory this inventory is used in simulations for the period from 1850 to 2010 for anthropogenic emissions from 1750 to 1850 we used a constant global emission of 342 5 mg year distributed to grids with historical silver and gold mining activities lacerda 1997 the value 342 5 mg year is an average of global anthropogenic emission for 1450 1850 used in a previous modeling study streets et al 2011 climate data were obtained from the european centre for medium range weather forecasting ecmwf era interim 1979 to present ecmwf 2017 and the geophysical fluid dynamics laboratory gfdl ocean data assimilation experiments oda 1970 2007 zhang et al 2007 the reactant data used in atmospheric chemistry such as the o3 1979 to present and so2 2003 2012 mixing ratios were obtained from the ecmwf era interim and monitoring atmospheric composition and climate macc ecmwf 2017 satellite data were obtained from nasa seawifs 1997 2002 and modis terra 2003 to present nasa 2017 to retain data consistency throughout the simulation period we prepared mean long term data for years with available data which was repeatedly used except for the anthropogenic emissions and wind velocities for wind velocities which were used for advection calculations in the atmosphere we repeatedly used data from 2010 the input and forcing data used in this study are described in more detail in appendix a section a6 3 results and discussion 3 1 modeled concentration evaluation to perform model validation observational data for the concentrations of dissolved hgt mmhg dmhg and hg0 dgm in the ocean and gaseous hg0 gem above the ocean were compiled from available databases see table a5 and fig a5 in appendix a for this validation we exclude data obtained near the coast where the maximum water depth is shallower than 1000 m because our model does not consider riverine mercury input fig 4 shows the validation results for concentrations of hgt mmhg and dmhg we excluded data obtained in years earlier than 2000 appendix a provides a statistical summary of the validation of individual cruises table a6 and the same correlation diagrams as shown in fig 4 but also includes data obtained earlier prior to 2000 fig a6 also fig a6 shows the results for the gem in the lower troposphere and dissolved hg0 results from recent observations are superior to those from earlier observations and earlier observed concentrations tend to be overestimated compared with modeled concentrations the ocean is classified into three depth ranges the surface 0 70 m intermediate 70 1000 m and deep 1000 5500 m oceans hereinafter we use this classification we used log transformed hg concentrations in the validations included in this section and in discussions of global distribution in section 3 2 this is because 1 seawater concentrations in our dataset can vary over 100 fold depending on the month and region although in the open ocean the degree of such variation is decreased and 2 uncertainty in some key model parameters such as kd produces order of magnitude differences in the simulated hgt concentrations as discussed in section 3 3 in the surface ocean fate hg generally simulated hgt concentrations the upper panels in fig 4 for most of the recent cruises where differences in the modeled and observed concentrations are within one order of magnitude in the intermediate ocean the modeled concentrations are slightly underestimated compared with the observed concentrations in the deep ocean the modeled concentrations spanned approximately two orders of magnitude on the other hand the observed concentrations ranged over approximately one order of magnitude this is shown as vertically long plots in fig 4 and is evident in the larger standard deviations of modeled concentrations shown in table a6 our model could not successfully simulate such uniform hgt distribution in the deep ocean the results for dmhg were worse than the results for hgt and mmhg for all depths as indicated by the considerable variation in the plots in the surface intermediate ocean and deep ocean the modeled mmhg concentrations tend to be underestimated and overestimated respectively compared with the observed concentrations as indicated by the positive and negative biases shown in table a6 we note that our model simulated gem concentrations in the atmospheric planetary boundary layer apbl with a much smaller variation in the plots see fig a6 in appendix a based on the results for the hgt concentrations in the surface ocean we can partly explain differences in model data by interlaboratory variability in the observed data interlaboratory variability could explain a maximum of 40 of these differences lamborg et al 2012 neglecting the interannual variability in concentrations induced by the interannual variability in climate i e the use of mean long term input data is possibly another reason for scatter in the plot underestimation of the modeled hgt concentrations in the intermediate and deep oceans is possibly due to overestimation of the seawater pom partitioning coefficient because this process is sensitive to simulated mercury levels the uncertainty in the initial background concentration used in the simulation is another possible cause these issues are discussed later with respect to fig 6 in section 3 3 the poor fit between the measured and modeled concentrations of methylated mercury may be caused by insufficient parameterization of the rate coefficients for seawater transformations in this study we adopted the parameterizations used in previous models without modification or used constant observed values there is uncertainty in the rate coefficients for biotic reductions for the rate coefficients of biotic oxidation and reduction i e kbox and kbred in table 1 we adopted the parameterization used in zh14 the constant proportionalities that relate kbox and kbred as well as ocrr are adjusted values determined to force the modeled hgii hg0 ratio to be consistent with the observed ratio therefore these parameterizations are not supported by observation but are model dependent uncertainty exists as to whether biotic oxidation occurs in the ocean for example so10 and so16 do not take into account this reaction however if this reaction is not considered simulated hg0 concentrations become too low compared with the observed concentrations not shown recent years have seen an increase in the number of observational studies on rate coefficients for methylation and demethylation in the water column using stable isotopes fig 2 table a3 even though studies performed in the open ocean are limited e g the mediterranean sea monperrus et al 2007 and canadian arctic archipelago lehnherr et al 2011 on the other hand there are only a handful of studies for the other rates earlier studies on the creation and decomposition of dmhg in seawater mason and sullivan 1999 are more uncertain because these studies are not based on stable isotope analysis estimating the npp and ocrr from satellite observations with satellite based empirical models is relatively straightforward however their use as relevant variables for biotic methylation and demethylation is not sufficiently supported by observation for example in the canadian arctic archipelago the rates of methylation and demethylation at the depths of surface chlorophyll maxima and oxycline are similar lehnherr et al 2011 as shown in fig 2 observed methylation and demethylation rates differed significantly these site to site differences could not be fully explained by the differences in the npp or ocrr the poor fit between the measured and modeled methylated mercury concentrations may also be caused by problems in the methods used for analysis dmhg is known to decompose to mmhg at low ph values black et al 2009 when acidification is used as a preservation technique this will yield a mmhg dmhg ratio that is too high in general the fit with the measured values is not as good when predicting the concentrations of specific forms of mercury as compared with predicting the hgt concentrations therefore in the following discussion on the ocean we focus on hgt concentrations the ability to improve the predictions of methylated mercury concentrations requires more observational studies on the rate coefficients of methylation and demethylation in seawater also we do not discuss hgt distribution in the deeper ocean because fate hg overpredicts the range of hgt concentrations in the deeper ocean 3 2 global distribution fig 5 shows the simulated distributions of hgt in the surface ocean along with the distributions of gem in the apbl and the dry and wet deposition of hgt mean annual concentrations or annual fluxes for 2010 are shown when available the observed concentrations are plotted on the modeled concentrations although the observed data are corrected for various years and months the gem is characterized by higher concentrations near terrestrial emission sources and is uniformly distributed in remote regions gem concentrations are generally higher in the northern hemisphere than in the southern hemisphere fig 5a the dry and wet deposition fluxes had higher values at low latitudes and correlate with higher concentrations of the major gem oxidants i e o3 and oh and the amount of precipitation fig 5b the distribution of hgt in the surface ocean fig 5c does not significantly correlate with the distribution of dry and wet deposition fluxes although atmospheric deposition is a principal source of oceanic mercury mason et al 2012 this indicates that not only deposition from the atmosphere but also several internal processes within the interior of the ocean such as physical transport and export associated with the biologically driven carbon cycle determine the hgt distribution in the surface ocean hgt concentrations tend to be higher in regions with higher deposition and upwelling flows fig a7 e g high latitude regions in the north pacific and north atlantic as well as near the antarctic circumpolar current so10 reported similar analyses for global distributions of hg concentrations and fluxes in the surface ocean in which the results are obtained from geos chem simulations figures 3 and 4 in soerensen et al 2010 the major difference between the results in so10 and our results can be found in the distribution of atmospheric deposition in so10 higher deposition occurs at higher latitudes which is likely due to the fact that so10 incorporates br chemistry in the atmosphere br chemistry shifts high deposition regions to higher latitudes holmes et al 2010 oxidation of gem by o3 and oh are thermodynamically unfavorable and no longer considered to be accurate the differences in hgt concentrations between this study and so10 are particularly apparent in the north atlantic in so10 inorganic hg concentrations in the north atlantic are higher than concentrations in other regions however inorganic hg concentrations are lower in our simulation except for off the coast of greenland where deep water formation occurs this difference may be caused by different settings in the initial background concentration and seawater pom partitioning coefficient the initial background concentrations of the intermediate and deep ocean in the north atlantic used in this study are lower than those in other sections of the northern hemisphere fig a3 as discussed later i e the discussion for fig 7 in section 3 3 with respect to the hgt levels in the ocean the north atlantic is the most sensitive region to historical anthropogenic emissions however the simulated hgt concentrations may still not reach their current levels in the north atlantic in fact hgt concentrations from recent exploratory cruises in the north atlantic positive biases in geotraces ga01 and ga03 shown in table a6 showed higher values than those simulated figs 4 and 5c also as shown in fig 3 estimates for the mmhg kd levels at mid latitudes in the north atlantic are relatively higher than the values in other sections therefore hgt in the surface ocean throughout the north atlantic is more likely to be exported by particle removal in our model calculations consequently we estimate that mercury export from the surface to deeper ocean via particle removal in this area is dominant as compared with other areas 3 3 long term variation fig 6 shows the simulated long term variation in the global concentrations of hgt at the apbl surface ocean intermediate ocean and deep ocean fig 6 also shows the annual global anthropogenic emissions into the atmosphere and the results from three sensitivity runs sr1 3 in addition to results from a reference run ref in sr1 the mmhg bcf for dead phytoplankton is assumed to be equivalent to that for living phytoplankton i e in sr1 the lack of active uptake in dead phytoplankton considered in ref is not considered in sr2 a prescribed constant value for the seawater pom partitioning coefficient log10kd 5 3 is used for both hgii and mmhg in sr3 initial background concentrations in the ocean are doubled for all depths prior to the simulation the objective with respect to sensitivity analyses is to investigate how uncertainties in the initial background concentration settings in the ocean and parameterization of the seawater pom partitioning coefficient affect the simulated hgt levels as we described in the preceding section section 2 2 the initial background concentrations used in this study are not based on simulations at natural conditions over a period of thousands of years but rather are based on results from previous modeling studies therefore uncertainty exists in the initial background concentration settings to discuss the impact of past anthropogenic emissions on present day mercury content herein we define the relative increase in mercury content as the 2010 mercury content normalized by the 1850 mercury content when anthropogenic emissions began to clearly increase in ref the relative increase of hgt in the apbl surface ocean intermediate ocean and deep ocean are 2 3 1 9 1 8 and 2 2 respectively the relative increase of the hgt in the apbl 2 3 is consistent with that of atmospheric deposition increased 2 5 fold as inferred by observations from lake sediments peat bogs and ice cores fitzgerald et al 2007 morel et al 1998 estimations for the increase in oceanic hgt due to historical anthropogenic emissions vary considerably in different models the result for the relative increase of hgt in the surface ocean 1 9 is consistent with increases from previous modeling studies covering the years from 1850 onwards 1 25 3 selin et al 2008 zhang et al 2014b mason et al 1994 lamborg et al 2002 mason and sheu 2002 sunderland and mason 2007 the relative increase of hgt in the intermediate and deep ocean from previous studies which incorporate pre 1850 emissions is 1 5 5 3 for the intermediate ocean and 1 2 2 1 for the deep ocean zhang et al 2014b streets et al 2011 amos et al 2013 by taking the influence of pre 1850 emissions into account our estimates for the intermediate 1 8 and deep oceans 2 2 are consistent with and larger than estimates from previous studies respectively the hgt concentration in the apbl rapidly tracks changing anthropogenic emissions due to its relatively short residence time in the atmosphere on the other hand the hgt contents in the ocean showed phase lags after anthropogenic emissions this is evident even in the surface ocean for example the local maximum for hgt contents in the surface ocean that correspond to emissions from 1970 roughly appeared around 1975 these phase lags are comparable in both the surface and intermediate ocean and longer in the deep ocean due to its larger inertia compared with the results of ref in sr1 and sr2 hgt contents in the apbl surface ocean and intermediate ocean showed significantly lower values despite a relative increase similar to that of the ref this may have been induced by an overly large kd in sr1 and sr2 if we do not consider the decrease of the bcf in dead phytoplankton mercury removal via particle removal becomes too large which causes the apparent depletion of mercury from the water column in our model simulated mercury levels in the lower troposphere and surface intermediate ocean are highly sensitive to the seawater pom partitioning coefficient given that particle removal is a major pathway by which mercury is removed from the ocean this is expected the uncertainty in the seawater pom partitioning modeling may be a cause of the disagreement between the modeled and observed concentrations in fig 4 and a6 in sr3 hgt contents had higher values than those in ref at all depths and this difference decreased with time at present 2010 hgt contents in the apbl surface ocean intermediate ocean and deep ocean in sr3 are 1 04 1 05 1 05 and 1 17 times larger than the concentrations in ref respectively based on this result we find that at present the uncertainty in the initial background concentrations in the ocean is insensitive to hgt concentrations in both the apbl and surface intermediate ocean on the other hand this may be a possible cause that produces model data disagreements in the hgt levels in the deep ocean fig 7 is similar to fig 6 but shows the results for ref in the seven global oceans north pacific south pacific north atlantic south atlantic indian ocean arctic ocean and antarctic ocean see fig a8 in appendix a for geographical classifications where we replace the y axis with the relative increase in hgt throughout the analyzed period 1850 2010 both the arctic ocean and north atlantic are sensitive to anthropogenic emissions i e the relative hgt increase in these oceans is larger than increases in other oceans on the other hand the estimated hgt concentrations tend to be underestimated in the surface intermediate ocean when compared with observed concentrations from recent cruises in the north atlantic positive biases in geotraces ga01 and ga03 in table a6 if the initial background concentrations for the north atlantic fig a3 are properly set the estimated relative hgt increase in the north atlantic is still underestimated compared with actual values the geographic proximity to emissions is one of the factors that produces differences in the relative increases between oceans in fact oceans in the southern hemisphere and indian ocean are characterized by a lower relative increase the well known characteristic that chemicals characterized by long range transport tend to accumulate in polar regions wania and mackay 1995 is possibly another factor for the relatively high sensitivity of the arctic ocean to anthropogenic emissions given the relatively low sensitivity of the north pacific to anthropogenic emissions compared with other oceans in the northern hemisphere water age may have a certain influence the high latitude north atlantic is a well known location for deep water formation water in the north atlantic and surface of the arctic ocean is relatively young semeniuk and dastoor 2017 whereas water in the north pacific is older oceans with younger water are more sensitive to historical anthropogenic emissions in our simulations the atmospheric gem rapidly follows changing anthropogenic emissions in magnitude and location historically emissions from north america and europe are greater than that from other regions fig a4 in appendix a therefore the increase in gem in the arctic ocean and north atlantic is larger than other oceans recent years have seen an increasing amount of anthropogenic emissions from asia which has rendered the observation of differences between oceans unclear the hgt contents in the oceans showed phase lags from the anthropogenic emissions as in fig 6 and the degree of these phase lags differed between the seven oceans the arctic ocean responds quickly to changing anthropogenic emissions as compared with the north atlantic oceans in the southern hemisphere also tend to respond slowly compared with oceans in the northern hemisphere zhang et al 2014b reported similar analyses of long term variations in oceanic hgt levels with changes in anthropogenic emissions they estimated that the contribution of historical emissions to present day concentrations are relatively smaller in the arctic ocean these results are opposite of this study s results on the other hand the contributions are relatively higher in the intermediate deep ocean in the north atlantic and relatively smaller in the pacific ocean in zhang et al 2014b these results are qualitatively consistent with the results of this study so16 also reported similar analyses for the arctic ocean they estimated that hgt in the polar mixed layer subsurface water and deep ocean have increased by 1 5 1 4 and 1 2 fold respectively from 1850 to the present these relative increases are smaller than those in this study the relative increase for the arctic ocean in this study is generally larger than that in previous studies which is possibly due to the fact that the fate hg does not consider sea ice cover 3 4 global budgets fig 8 shows the global hgt budget for 2010 simulated by the fate hg the simulated dry and wet deposition to the sea surface 3590 mg year 1 and hgt contents in the surface ocean 2900 mg are comparable with those in the amap unep amap un environment 2019 3800 mg year 1 and 2600 mg respectively on the other hand the simulated net evasion from the sea surface to the atmosphere 1730 mg year 1 and hgt contents in the intermediate deep ocean 406 7 gg are smaller and larger than those in the amap unep 3400 mg year 1 and 310 gg respectively the estimated mercury contents in the ocean from previous modeling studies varies significantly selin et al 2008 zhang et al 2014b mason et al 1994 lamborg et al 2002 mason and sheu 2002 sunderland and mason 2007 streets et al 2011 amos et al 2013 the estimated hgt content in the ocean in this study 409 6 gg is 1 6 16 9 times larger than previous estimates however on average the simulated hgt concentrations in the ocean in this study never overestimated large numbers of observed concentrations but rather slightly underestimated them fig 4 therefore the actual content is likely larger than the estimate in this study the estimated overall turnover time for hgt in the ocean is 320 years which is considerably shorter than estimates from previous modeling studies 3000 years in mason and sheu 2002 600 years in selin et al 2008 1400 years in amos et al 2013 2000 years in zh14 1082 years in sd17 the turnover time in this study is defined identically to sd17 i e as the amount of hgt in an ocean layer divided by the export flux from the base of the layer the turnover time in the surface ocean in this study is 0 6 years which is comparable with that reported in zh14 7 months and shorter than that in sd17 2 3 years results suggest that the turnover time is 13 and 320 years in the intermediate and the deep ocean respectively which are both shorter than those in zh14 120 and 2000 years and sd17 214 and 1476 years in the ocean vertical transport via the biological pump is the dominant removal pathway for mercury zhang et al 2014a semeniuk and dastoor 2017 therefore the estimated turnover time is highly dependent on the poc export flux different formulations for the poc export profile can possibly explain the different estimations for the hgt turnover time in the ocean between models in this study the non dimensional flux profile is calculated with the following equation 1 s z s 1 e k s z s 2 where s z is the poc export flux at depth z m normalized by the poc export flux at the depth of the euphotic zone and s1 1 39 s2 0 046 and ks 1 317 are empirical parameters determined by averaging regional estimates lutz et al 2002 formulas that assume a simple logarithmic decay for the poc export flux with depth such as the model developed in martin et al 1987 are commonly used in biogeochemical models however such formulations underestimate the poc export flux on the sea floor lutz et al 2002 and therefore underestimate the hgt export flux on the sea floor and overestimate the overall turnover time in the ocean in fact the hgt export flux on the sea floor estimated in this study 1190 mg year 1 is approximately 5 times larger than similar estimates from the offtrac hg 240 mg year 1 zhang et al 2014b which uses the formulation reported in martin et al 1987 for the poc export flux profile considering refractory material in the poc against remineralization i e including s2 in equation 1 will improve estimations of the poc export flux on the sea floor however the empirical parameters i e s1 s2 and ks in equation 1 are uncertain and require more investigation because these parameters vary between ocean sections lutz et al 2002 and are not a constant value as assumed in this study 4 conclusions we developed a new global model fate hg for predicting biogeochemical cycling of mercury in the ocean fate hg is an offline model that is not based on any gcms or air water quality models therefore it is flexible for selecting data concerning climate and reactants it was developed based on coupled atmosphere ocean chemical transport models and considers explicit methylated mercury cycles in the ocean it also considers for the first time spatial and temporal variations of the seawater pom partitioning coefficient which is highly sensitive to simulated hgt levels in the surface intermediate ocean this new model may be a useful tool for assessing the effectiveness of future efforts to control mercury pollution undertaken in response to the minamata convention on mercury on the other hand further development is required to develop a state of the art multimedia model one limitation of the current version of the fate hg model is that it employs conventional atmospheric o3 oh chemistry the other limitation is the lack of explicit consideration of soil vegetation compartments with river discharge improvements to these limitations are ongoing in this study we tested the available parameterizations for rate coefficients of mercury transformations in seawater this resulted in worse agreements between modeled and observed concentrations of specific mercury forms accumulation of additional observational data and improving parameterization for these rate coefficients are required for future model development declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments this research was partly supported by the environment research and technology development fund 5 1405 5 1702 of the ministry of the environment japan and jsps kakenhi grant number 16k00524 appendix a supplementary data the following is the supplementary data to this article multimedia component 1 multimedia component 1 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2019 104599 
