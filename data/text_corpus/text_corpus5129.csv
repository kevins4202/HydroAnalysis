index,text
25645,water quality wq models parameterization remains a challenging task as these models are typically characterized by a high number of parameters the objective of this study was to present a solution to the wq parameterization problem by the use of a fast sensitivity analysis sa method and a manual calibration for this purpose we applied the simple screening lh oat method to the conceptual wq model of the river dender belgium to evaluate the effectiveness of lh oat to identify the influential parameters the advanced pawn method was applied a manual calibration was done using the influential parameters lh oat provided a parameter ranking that was very similar to the one of pawn but in a much more efficient way the bayesian uncertainty assessment showed the effectiveness of the lh oat results to conclude a fast screening method is preferred over an advanced sa method to identify the influential parameters for the calibration keywords parameterization sensitivity analysis uncertainty analysis dream zs error model 1 introduction the water quality wq of watershed systems is affected by high anthropogenic pressures including domestic wastewater discharges and industrial and agricultural activities the effects of pollutant emissions into rivers can be evaluated using mathematical water quality models moreover wq models have been widely used to support environmental management decisions by providing a predictive link between management strategies and the future state of river water vandenberghe et al 2007 rode et al 2010 therefore they are useful tools to support the decision making processes in view of wq management leta et al 2015 in order to be able to make simulations or perform scenario analyses with the wq models they should represent the reality adequately for this purpose it is necessary to set the model parameters to appropriate values by calibrating the model the main goal of model calibration is to find a set of parameter values in the parameter hyperspace which yields the model output as closely and consistently as possible to the measured data points however parameterization of wq models is a challenging task as most wq models are typically characterized by a high number of parameters variables and long calculation time e g rwqm1 simulator reichert et al 2001 rode et al 2010 on the other hand only few water quality observations are available for the model calibration mannina and viviani 2010 khorashadi zadeh et al 2019 manual calibration of a model with a large number of parameters is very tedious and time consuming vrugt et al 2003 on the other hand the efficiency of automatic calibration algorithms is reduced when the number of parameters is large duan et al 1992 therefore it is not feasible to include all model parameters in the calibration process bekele and nicklow 2007 nossent et al 2011 in order to support the choice of which model parameters should be focused on during calibration and which ones could be instead excluded from calibration and set to default values global sensitivity analysis gsa is becoming popular in environmental modeling practices e g muleta and nicklow 2005 van werkhoven et al 2009 norton 2015 pianosi et al 2016 gsa indeed allows for the identification of those parameters that have negligible influences on the model output and therefore can be fixed to any value within their feasible range so called factor fixing ff moreover gsa is applied to identify the parameters that have the largest influence on the model output so called factor prioritization fp saltelli et al 2008 many different gsa methods have been developed from simple screening methods to advanced quantitative ones sobol 1990 saltelli et al 2000 van griensven et al 2006 borgonovo 2007 pianosi and wagener 2015 therefore the major issue for the gsa is the selection of an appropriate method which is a highly specialized task requiring detailed knowledge on the application and limitation of methods in addition gsa of wq models with long calculation time is hardly feasible as gsa depends on multiple simulations all these problems lead to serious calibration issues of which the many failures typically do not reach the literature for the limited number of successfully calibrated models e g wade et al 2001 little information on the model parameter uncertainty has been provided raat et al 2004 the uncertainty of hydrological models has been extensively discussed in literature e g montanari et al 2009 leta et al 2015 but examples of uncertainty assessment of water quality models are limited stow et al 2007 han and zheng 2016 the reasons are mainly related to the long computational time of wq models the complicated uncertainty structure and the large number of parameters and variables involved in general in comparison with hydrological simulations rode et al 2010 han and zheng 2016 the uncertainty in wq simulations is greater and more complicated for example wq modelling typically integrates pollution simulations with hydrological simulations chemical reactions and pollutant transport are often represented by simple equations therefore much higher model structure errors are incorporated into the wq modelling process as compared to the hydrological modelling process additionally regarding the input data wq modelling also involves much higher input uncertainty than pure hydrological modelling rode et al 2010 han and zheng 2016 measurements or estimates of the pollution loads of both point sources e g waste water treatment plant discharges and distributed sources e g fertilizer application are often highly inaccurate and limited furthermore water quality observations used during the parameter estimation procedure often have a limited measurement frequency e g only on a biweekly or even monthly interval zheng and keller 2007 and are poorly measured khorashadi zadeh et al 2019 therefore the ambiguity in the parameter estimation process i e the parameter uncertainty is further increased as compared to hydrological modelling where high frequency e g daily flow observations are both more available and more reliable raat et al 2004 franceschini and tsai 2010 han and zheng 2016 as wq models are in most cases ultimately developed to inform management decisions there is a strong need for a rigorous uncertainty assessment to provide information regarding the parameter uncertainty and the model output uncertainty in the context of wq modelling rode et al 2010 recommended the use of a bayesian uncertainty assessment framework bayesian uncertainty assessment using markov chain monte carlo mcmc sampling bates and campbell 2001 kuczera and parent 1998 is a well known solution to infer the parameter uncertainty and the model output uncertainty kavetski et al 2006a 2006b vrugt et al 2008 in the field of hydrological modelling various bayesian inference approaches have been proposed and applied e g schoups and vrugt 2010 leta et al 2015 but their applications to wq modelling are limited han and zheng 2016 khorashadi zadeh et al 2019 the main objective of this paper is therefore to present a solution to the parameterization problem of wq models by the use of a fast sensitivity analysis method and a manual calibration moreover a bayesian uncertainty assessment is performed to evaluate the success of our parameterization and to quantify the model output uncertainty to this end a conceptual wq model has been built for the river dender belgium to simulate dissolved oxygen o2 biological oxygen demand bod nitrate no3 and ammonia nh3 prior to the calibration a gsa is performed to identify the influential model parameters in this study a simple screening method called latin hypercube one factor at a time lh oat method van griensven et al 2006 is applied to rank the parameters with a limited computational cost in order to evaluate the use of the lh oat for identifying the influential parameters the results of the advanced gsa method called pawn pianosi and wagener 2015 khorashadi zadeh et al 2017 are served as benchmark the pawn method is computationally more expensive than the lh oat method quantitatively assessing the influence of the parameters the model calibration is performed manually to find a set of appropriate values for the influential parameters in the parameter hyperspace the differential evolution adaptive metropolis dream zs algorithm a state of the art mcmc sampler has been employed as a bayesian uncertainty assessment tool to evaluate the identifiability of the parameters and to quantify the parameter and the output uncertainty residual post processing is performed to validate the assumptions regarding the model error 2 methods and materials 2 1 conceptual integrated tool for water quality assessment citowa conceptual wq simulators use simplified equations with physical chemical meaning to represent the physical chemical processes in rivers in a lumped way many conceptual wq simulators have been used as alternatives to detailed simulators to support decision making processes and scenario investigation however in general they are not suitable for complex systems such as river flood plain systems tidal rivers and looped sewer systems also most of them are not able to simulate pollutants attached to fine sediments such as phosphate and trace metals furthermore the numerical solvers of most conceptual simulators may provide unreliable results when the computation time step is large these limitations challenge the usefulness of conceptual water quality simulators woldegiorgis 2017 in order to enhance the applicability of conceptual river water quality simulators woldegiorgis woldegiorgis 2017 woldegiorgis et al 2017 developed a conceptual integrated tool for water quality assessment citowa as an alternative to detailed wq simulators in citowa the river system is represented by reaches which are conceptual elements that divide the channel longitudinally into different parts the discharges and the velocities in these reaches must be provided by external simulation tools e g by the tool introduced by meert et al 2016 the reaches are modeled as continuously stirred tank reactors cstrs whereby complete mixing is assumed within each reach the citowa integrates several components the first component is the simulator of dissolved oxygen biochemical oxygen demand nitrogen compounds phosphorus compounds and algae the simulator applies a formulation based on ordinary differential equations of the physico chemical processes based on a qual 2e brown and barnwell 1987 type formulation of these processes it integrates the mass balance equation with the transformation decay processes and solves the resulting differential equation using a quasi analytical solution the variables which can be simulated in the model are nitrogen compounds phosphorus compounds algae biological oxygen demand and dissolved oxygen moreover citowa can be used to simulate the sediment sediment bound pollutants and any arbitrary pollutant along a river it can also handle tidal rivers and looped sewer systems 2 2 the case study as a case study a conceptual wq model for the river dender is built up using citowa the dender basin located in belgium is a part of the international scheldt basin the region is characterized by a temperate maritime climate with an average annual rainfall of 819 mm and an average annual temperature of 10 c climate data org 2012 in view of the available data as shown in fig 1 the study is limited to the flemish part of the basin the official basin area according to the flemish authorities is 704 km2 aubin and varone 2002 in this part the main watercourse of the river dender has a total length of 51 km an extensive monitoring network of water quantity physico chemical and biological quality diffuse emissions including industries agriculture and wastewater treatment effluents are handled by the flemish environmental agency vmm and the hydrological information centre hic the major anthropogenic pollutant sources include the wastewater generated from household activities agricultural activities and industrial emissions the model build up steps and sources of the input data and the boundary data are explained in the supplementary materials section a the time step of the citowa simulation is hourly there are 7 measuring stations along the flemish part of the river dender the available data provided by vmm is for 2010 2012 therefore the conceptual wq model is executed for these 3 years the first two years are used for model calibration and the last year is considered for model validation in this study we focus on the o2 bod no3 and nh3 simulations table 1 summaries the available data of the control stations along the river the temporal variations of o2 bod no3 and nh3 during the simulation period 2010 2012 at overboelare idegem zandbergen ninove and aalst are represented in suplementaly materials section b 2 3 the sensitivity analysis after building up the model the next step is to determine the values of the model parameters however the values of the parameters of the conceptual model cannot be measured directly therefore a model calibration is needed to adjust the parameter values by maximizing the agreement between the model simulations and historical measurements laloy et al 2010 vrugt et al 2013 mostly due to lack of calibration data only a few model parameters can be identified during the calibration procedure therefore prior to calibration a gsa can be essential to support the calibration procedure by identifying the influential and non influential parameters muleta and nicklow 2005 van werkhoven et al 2009 nossent and bauwens 2012 norton 2015 based on this identification sa allows for a reduction of the number of parameters incorporated in the calibration procedure using the results of the sa the set of parameters incorporated in the calibration is reduced to the influential parameters i e ff the non influential parameters are set to the default values saltelli et al 2008 in literature many gsa techniques are available from simple and easy to implement methods e g morris 1991 to advanced and quantitative methods e g sobol 1990 in this study two gsa methods are applied the latin hypercube one factor at a time lh oat method van griensven et al 2006 and the moment independent pawn method pianosi and wagener 2015 khorashadi zadeh et al 2017 the former is a simple so called screening method used for ranking the parameters with limited computational cost while the latter is a more advanced method that is computationally more expensive and is applied to quantitatively assess the influence of the parameters the pawn method will therefore serve as a benchmark to evaluate the use of the lh oat for identifying the influential parameters in the following the applications of the lh oat and the pawn methods to the conceptual wq model are explained 2 3 1 the lh oat method the lh oat method van griensven et al 2006 is an enhancement of the basic morris elementary effect method morris 1991 to achieve a better coverage of the parameter space it uses latin hypercube lh sampling to generate n l h random parameter sets then each lh sampled parameter set is varied p times p is the number of parameters by changing each of the parameters one at a time one factor at a time oat and keeping the other parameters at their current values the perturbation of the i th parameter is performed by a fixed fraction f i and the same fraction is used for all parameters the method combines the robustness of the lh sampling ensuring that the full range of all parameters has been sampled with the precision of an oat design assuring that the change in the output in each model run can be unambiguously attributed to the parameter that was changed the method operates by loops each loop starts with an lh point around each lh point j a partial effect s i j l h for each parameter x i is calculated as in percentage 1 s i j l h 100 f x 1 x i 1 f i x p f x 1 x i x p f x 1 x i 1 f i x p f x 1 x i x p 2 f i where f is the function that maps the model parameters into a scalar variable as model output therefore a loop requires p 1 runs the final effect s i l h is calculated by averaging these partial effects of each loop for all lh points thus for n l h loops as below 2 s i l h j 1 n l h s i j l h n l h the method is very efﬁcient as it only requires a total of n l h p 1 runs the final effects are ranked with the largest effect being given rank 1 and the smallest effect being given a rank equal to the total number of parameters analyzed for a non influential parameters s i l h should be 0 the lh oat method is applied to the conceptual wq model of the river dender to identify the influential and the non influential parameters parameter screening for this purpose 10 parameters p 10 which are mathematically relevant for o2 bod no3 and nh3 simulations are selected see table 2 the feasible ranges of these 10 parameters reported in table 2 are determined according to the qual2e manual brown and barnwell 1987 since there is no prior information on the parameter distributions parameter values are sampled from a uniform distribution dividing the parameter range in 200 equal intervals n l h 200 therefore the total number of model evaluations is 2200 n l h p 1 nossent 2012 suggested n l h 50 to have reliable and converging sa results thus n l h 200 is a conservative selection and fewer intervals e g n l h 100 could be considered this will results in less model evaluations the fraction f i for changing the parameters is set to 0 05 typically when the objective of the sa is to inform the calibration procedure screening a performance measure is used as a scalar variable to evaluate the model results e g van griensven et al 2006 pappenberger et al 2008 nossent et al 2011 in this study the mean absolute error mae is selected as performance measure 3 m a e i 1 m c i s i m c i o b s m where c i o b s is the ith observed concentration c i s i m is the simulated concentration corresponding to the ith observation and m is the total number of observed data points 2 3 2 the pawn method pawn pianosi and wagener 2015 is a density based gsa method where the entire model output distribution is used to quantify the relative influence of the parameters on the model output the pawn sensitivity indices measure the sensitivity of y to parameter x i by the distance between the unconditional cumulative distribution function cdf of y i e a scalar variable as model output which is obtained by varying all parameters simultaneously and the conditional cdfs of y which are obtained by varying all parameters but x i i e x i is fixed at a nominal value x i liu et al 2006 borgonovo 2007 pianosi and wagener 2015 proposed to measure the distance between the conditional and unconditional cdfs by the kolmogorov smirnov statistic ks kolmogorov 1933 smirnov 1939 as below 4 k s x i max f y y f y x i y y where f y y is the unconditional cdf of the output y and f y x i y is the conditional cdf when x i is fixed as f y x i y characterizes the output distribution when the variability due to x i is removed its distance from f y y indicates the effect of x i on y when f y x i y overlaps with f y y completely k s x i is equal to zero which means that removing the uncertainty about x i does not affect the output distribution i e x i has no influence on y a large distance instead indicates a high influence of the parameter since ks depends on the conditioning value of x i the pawn sensitivity index t i considers a statistic e g maximum or median over all possible values of x i 5 t i stat k s x i x i the pawn index varies between 0 and 1 the higher the value the more influential x i for complex and non linear models the analytical computation of the pawn index t i is usually impossible pianosi and wagener 2015 therefore suggested the following approximate numerical procedure first the ks statistic in eq 4 is approximated by using empirical unconditional and conditional distributions n u parameter sets are generated by varying all parameters simultaneously then n u model evaluations are used to estimate the empirical unconditional distribution of y to estimate the empirical conditional distributions of y n c parameter sets are generated by varying all parameters except x i which is kept to a fixed value secondly in eq 5 the statistic with respect to the conditioning value of x i is approximated by using n randomly sampled values for the fixed parameter x i therefore the total number of model evaluations required to calculate the pawn index t i for all the p parameters is n u n n c p in addition to lh oat the pawn method is applied to the conceptual wq model of the river dender the purpose is to identify the most influential parameters for different model output variables i e o2 bod no3 and nh3 and to compare the results with those of lh oat to perform the pawn analysis a sample size of 300 n u n c 300 is used to approximate the conditional and unconditional cumulative distribution functions of the model output the approximation of the conditional distribution is repeated for 10 different condition values for each parameter n 10 therefore the total number of model evaluations is 30300 n u n c p n as a scalar representation for the model output the mae see eq 3 is calculated by comparing the results of the conceptual model with the observed data points since numerical approximations rather than analytical solutions are utilized in the pawn method to calculate the sensitivity indices small but non zero indices may be obtained for the indices of non influential parameters to define a threshold for screening the parameters khorashadi zadeh et al 2017 proposed to calculate the sensitivity index of a dummy parameter this dummy parameter has no influence on the model output but will have a non zero sensitivity index representing the error due to the numerical approximation hence the parameters whose indices are above the sensitivity index of the dummy parameter can be classified as influential whereas the parameters whose indices are below this index are within the range of the numerical error and should be considered as non influential the readers are referred to khorashadi zadeh et al 2017 for the procedure to calculate the pawn sensitivity index of the dummy parameter 2 4 the parameter estimation procedure the main goal of model calibration is to find a set of parameter values in the parameter hyperspace which yields the model output as closely and consistently as possible to the measured data points considering the results of the sa the calibration is only performed for the parameters that are identified as influential calibration methods can be broadly grouped into manual and automatic calibration techniques sorooshian and gupta 1995 2 4 1 the manual calibration the manual calibration is performed by adjusting these influential parameters for simulating o2 bod no3 and nh3 the effect of changing the model parameters is assessed by comparing the relevant model output with the observed data due to the limited observed data points the comparison and the calibration results assessment are performed visually the simulation results of reaches 5 6 and 8 for 2010 and 2011 are used for the model calibration the corresponding measuring stations for these reaches are idegem zandbergen and ninove respectively the simulation results for 2012 are used for the model validation in time at these reaches and stations moreover the spatial validation is performed using the independent stations denderleeuw and aalst which are not used for the calibration 2 4 2 the model validation once the model is calibrated the model validation is performed using an independent data set not used in the calibration procedure the validation shows the capability and credibility of the calibrated model for the simulation and prediction the hydraulic input data i e flow velocity water depth and the pollutant load of the boundary conditions are updated for the validation period then the model is run using the calibrated parameters values as the conceptual wq model provides the concentrations at different locations along the river in addition to validation in time validation in space is performed using independent stations if acceptable results are obtained for the validation the procedure for the model calibration is finished otherwise it is necessary to go back to the initial stage of the model calibration and to reconsider the choice of the calibration parameters and or the model input data 2 5 the automatic calibration and the uncertainty analysis automatic calibration methods search for an optimal set of parameters in the parameter space although it is possible to obtain an optimum parameter set and an optimum model output during the calibration this calibrated model may perform poorly for a different data set during the validation in that case other parameter sets may be ranked as optimal brazier et al 2000 moreover a single best parameter set only minimizes the overall residuals but does not eliminate them completely nossent 2012 consequently the model results are made with a certain amount of uncertainty beven 2006 therefore instead of searching for a single optimal parameter set and for the best model simulation results some optimization algorithm propose alternative methods for finding a range a distribution or a probability function of model parameters i e posterior distributions of parameters and model output beven and binley 1992 bayesian inference using markov chain monte carlo mcmc sampling bates and campbell 2001 kuczera and parent 1998 is a well known solution to infer the posterior distribution of parameters 2 5 1 bayesian inference using dream zs dream zs vrugt et al 2008 2009a is a state of the art mcmc algorithm which applies a formal statistical bayesian approach to infer the posterior parameter distribution detailed explanation of this algorithm can be found elsewhere vrugt et al 2009b laloy and vrugt 2012 dream zs is an extension of dream vrugt et al 2008 2009a to simplify inference and to enhance the convergence rate like the original dream algorithm instead of searching for a single optimal parameter dream zs infers the posterior distribution of model parameters by using the information content in the calibration data according to bayes theorem the relationship between a model output y and its corresponding observation z can be expressed as 6 z y θ ε where θ is a vector of uncertain model parameters and ε is a lumped residual error term bayes rule can be adopted as 7 p θ z p θ l θ z where p θ z and p θ represent the posterior and prior distribution of θ respectively and l θ z represent the likelihood function which is mathematically determined by the error model of residual ε if we assume that the residuals are unrelated and normally distributed with zero mean and standard deviation σ then the log likelihood function can be written as follows vrugt et al 2013 8 l θ σ z m 2 log 2 π t 1 m log σ t 1 2 t 1 m z t y t θ σ t 2 where m is the total number of observations this formulation allows for homoscedastic i e constant variance and heteroscedastic errors i e variance dependent on the magnitude of the data for example for the hydrological modelling the heteroscedasticity can be explicitly considered by assuming that σ increases linearly with the simulated flow leta et al 2015 to extend the applicability of the algorithm to situations where the error residuals are correlated and or non gaussian schoups and vrugt 2010 proposed the following model for the residual errors 9 φ k b e t σ t a t with a t sep 0 1 ξ β where φ k b is an autoregressive polynomial with k autoregressive parameters to account for dependence and correlation between errors b is the backshift operator and a t is an independent and identically distributed random error with zero mean and unit standard deviation described by a skew exponential power sep density the skewness parameter ξ and the kurtosis parameter β are tunable hyper parameters to account for nonnormality ξ takes a positive value affecting the asymmetry the density is symmetric for ξ 1 positively skewed for ξ 1 and negatively skewed for ξ 1 the value of β is between 1 and 1 and determines the peakedness of the distribution in case of symmetric density ξ 1 β 0 results in a gaussian distribution and β 0 results in a more peaked density more details about the sep distribution and its mathematical formula can be found in schoups and vrugt 2010 therefore the formulated model error contains a number of hyper parameters e g ξ β hereafter denoted as a vector φ both the model parameters θ and the model error hyper parameters φ are estimated from the observed data based on the bayesian inference based on the error model defined in eq 9 the resulting expression for the log likelihood is 10 l θ σ φ z m log 2 σ ξ ω β ξ ξ 1 t 1 m log σ t c β t 1 m a ξ t 2 1 β where a ξ t ξ s i g n μ ξ σ ξ μ ξ σ ξ a t and μ ξ σ ξ ω β and c β are computed as a function of skewness parameter ξ and kurtosis parameter β as detailed in schoups and vrugt 2010 as suggested by schoups and vrugt 2010 the complexity of the error model should be gradually increased from a gaussian homoscedastic and non autocorrelated model to eq 9 until posterior checks verified the error model assumptions in the context of bayesian inference error models of water quality responses have been specifically discussed by a few studies wellen et al 2014 simply assumed independent and normally distributed error for sediment modelling han and zheng 2016 gradually increased the complexity of error models until posterior checks confirmed that the residual errors are consistent with the error model assumptions finally they adopted a non gaussian homoscedasticity and auto correlated error model for nitrate simulation for the multiple response calibration different error models are required for different responses if it is assumed that the residual errors of multiple responses are independent then the combined likelihood function l m u l t i p l e is the sum of individual log likelihood functions balin talamba et al 2010 han and zheng 2016 11 l m u l t i p l e i 1 l i θ σ φ z i according to the mcmc theory the dream algorithm is expected to eventually converge to a stationary distribution which should be the desired target distribution but in practice the question is how to actually assess if convergence has been reached without knowledge of the actual target distribution the most powerful formal way to evaluate the convergence of the dream zs algorithm is the r statistic gelman and rubin 1992 in practice a value of the r statistic smaller than 1 2 for all parameters points to convergence vrugt et al 2009a 2 5 2 setting up the numerical experiment the dream zs algorithm is adopted as an uncertainty analysis tool to perform simultaneously the automatic calibration and the uncertainty analysis for the conceptual wq model according to the results of draem zs the optimum range of the parameters and the parameter uncertainty ranges are obtained moreover it allows to infer the appropriate statistical distribution of the residual errors for the conceptual wq model which is a multi response model the sequential calibration approach is sound and useful however a more straight forward alternative is a multi response bayesian calibration han and zheng 2016 for example raat et al 2004 have shown that including multiple responses in the bayesian calibration can improve uncertainty assessment and the identifiability of model parameters in this study the uncertainty analysis is applied for o2 and bod simulations no3 and nh3 simulations are not included the measured o2 and bod at ninove reach 8 in the conceptual model are used as observations in the dream zs algorithm multi response calibration because the measured data is available for three years 2010 2012 at this station moreover according to the manual calibration where three stations are considered for the calibration there is no conflict between the stations see section 3 3 however there is no limitation for the dream zs algorithm to include more measured data points and it is possible to perform a multi site calibration according to the sensitivity analysis the most influential parameters for simulating o2 and bod are selected for the uncertainty analysis the other parameters of the model are set to their default values the simulation results for 2010 and 2011 are used for the model calibration the simulation results for 2012 are used for the model validation in time at this station the critical step in the dream zs application is to define an explicit statistical model for the residuals i e applying an approximated error model and to formulate the likelihood function as schoups and vrugt 2010 suggested the complexity of the error model is gradually increased from a simple gaussian homoscedastic and non autocorrelated type to the most complicated one i e correlated heteroscedastic and non gaussian until posterior checks confirm that the residuals are consistent with the error model assumptions eventually a non gaussian with a non zero mean non correlated and homoscedastic error model has been adopted for o2 and bod responses as below 12 e o 2 t σ ˆ o 2 a o 2 t μ o 2 with a o 2 t sep 0 1 ξ o 2 β o 2 13 e b o d t σ ˆ b o d a b o d t μ b o d with a b o d t sep 0 1 ξ b o d β b o d where e o 2 t and e b o d t are the residuals for o2 and bod μ o 2 and μ b o d are the mean of the residuals for o2 and bod respectively σ ˆ o 2 and σ ˆ b o d are the standard deviations of the residuals for o2 and bod respectively a o 2 t and a b o d t are random errors with zero mean and unit standard deviation described by a sep density ξ o 2 and ξ b o d are the skewness parameters for o2 and bod and β o 2 and β b o d are the kurtosis parameters for o2 and bod respectively therefore the formulated models for the residuals contain a number of hyper parameters i e ξ o 2 ξ b o d β o 2 and β b o d hereafter denoted as a vector φ in addition to the model parameters θ and the hyper parameters φ standard deviations and the means of the residuals for o2 and bod are estimated from the observed data based on the bayesian inference therefore in total 11 unknown variables summarized in table 3 are involved in the calibration and uncertainty analysis uniform distributions are considered for the prior distributions of the model parameters and error model parameters the ranges of the parameters are selected based on literature values brown and barnwell 1987 schoups and vrugt 2010 vrugt et al 2009b the maximum number of iterations is set to 12000 the convergence analysis based on the r statistic values is performed to evaluate the sufficiency of the maximum number of iterations then the residual post processing is performed to confirm the assumption regarding the error models for o2 and bod responses after evaluating the residuals assumptions the posterior density functions of the parameters are plotted and the 95 confidence intervals cis of the simulated o2 and bod due to the parameter uncertainty and due to the total uncertainty are calculated the generated parameter sets after the convergence are considered as posterior parameter sets considering the r statistic values for the parameters see section b in supplementary materials the last 25 of the generated parameter sets 9000 12000 are selected as posterior parameter set once the posterior distributions of the model parameters and the model error parameters are inferred the cis of the simulation can be estimated the 95 cis of the simulation due to the parameter uncertainty are obtained by running the model for the posterior samples of the model parameters then the simulation results are ranked for each time step and the 0 025th and 0 975th quantiles are obtained for every time step to calculate the 95 cis of the simulation due to the total uncertainty the total errors e o 2 t and e b o d t are added to the simulation results as follows 14 o 2 t o 2 t e o 2 t i f o 2 t 0 t h e n o 2 t 0 15 b o d t b o d t e b o d t i f b o d t 0 t h e n b o d t 0 where o 2 t and b o d t are the simulated o2 and bod using the posterior parameter sets and o 2 t and b o d t are the simulated o2 and bod considering the total uncertainty e o 2 t and e b o d t are the total error including observation error forcing and output data model structural error input error and boundary condition error similar to the cis of the simulation due to the parameter uncertainty for every time step the 0 025th and 0 975th quantiles of o 2 t and b o d t are calculated to obtained the 95 cis of the simulation due to the total uncertainty 2 6 the assessment of the water quality the assessment of the water quality can be done based on the ambient water quality standards the standards set by the vmm are listed in table 4 vlarem 1995 as a parameter of wq bod is an indicator of the amount of organic matter the water contains the content of organic matter in a river depends on several factors such as the loads of urban and industrial discharges the capacity of self purification of the river the purification infrastructure diffuse pressures etc bod is directly related to the o2 concentration because one of the main causes of the decrease of o2 is the degradation of the organic matter it is necessary to ensure that the oxygen levels in the river are above the acceptable levels as the latter affects aquatic life the oxygen in the aquatic system is produced by the photosynthesis of plants and removed by respirations of plants animals and bacteria ammonia compounds are found in water under reducing conditions in this environment ammonia is quickly converted to nitrite and then to nitrate this process is called nitrification and it is faster when the water is well aerated this decay of ammonia gives rise to an oxygen demand of 2 mol of oxygen per mole of nitrogen oxidized a high content of ammonia facilitates the multiplication of microbes and that is why the presence of free ammonia or ammonium ion in the water is an indicator of recent and dangerous contamination 3 results and discussion 3 1 the results of the sensitivity analysis the lh oat screening method is applied to analyze 10 parameters of the conceptual wq model for better comparison the lh oat sensitivity indices are scaled between 0 and 1 the sensitivity analyses are performed using the observed data at ninove considering the results of sa for simulating o2 see fig 2 a 4 parameters rk2 bc1 rk2 and rk3 see table 2 for the definition of these parameters are identified as important fig 2 b shows that two parameters rk3 and rk1 appear to be influential according to the lh oat analysis for simulating bod for simulating no3 four parameters are identified as important by lh oat kd ko2 bc1 and bc2 see fig 2 c and finally bc1 is the only influential parameter for simulating nh3 according to the lh oat analysis see fig 2 d to evaluate the results of the simple lh oat screening method the more advanced pawn method is applied to analyze the 10 parameters of the wq model the mae is hereby used as performance measure the sensitivity index of the dummy parameter is calculated to set a threshold for identifying the influential parameters fig 3 shows the pawn results for simulating o2 bod no3 and nh3 the dummy parameter is indicated in the figures with an arrow and the dashed line represents its sensitivity index as a threshold for the parameter screening the list of influential parameters based on the pawn method is given in table 5 table 5 summarizes the results of the lh oat and pawn methods according to the lh oat method bc2 is important for simulating o2 see fig 2 while the pawn index of bc2 is close to that of dummy parameter see fig 3 on the other hand bc3 is identified as important by the pawn method for simulating o2 while it is not as important based on the lh oat method however for simulating o2 bc3 and bc2 are less influential as compared to the other influential parameters based on the pawn and lh oat methods respectively for simulating bod rk1 and rk3 are the most influential parameters in both methods while bc1 and bc2 are identified less influential in pawn for simulating no3 and nh3 the lists of influential parameters based on lh oat and pawn are the same comparison of the results of the two sa methods illustrates that the identified influential parameters are almost the same since the lh oat requires less computation cost as compared to that of the pawn method 2200 vs 30300 model evaluations the lh oat method is an appropriate method for fast parameterization of the conceptual wq model it is also recommended for other wq models to identify the important parameters for the calibration based on the sa results 8 parameters are selected for the model calibration rk1 rk2 rk3 bc1 bc2 bc3 kd and ko2 the definitions of the parameters are given in table 2 these parameters were also important for the conceptual wq model of the river zenne belgium khorashadi zadeh et al 2019 3 2 the results of the manual calibration the manual calibration is performed by adjusting the multiplier factors of the 8 influential parameters identified by the sa see section 3 1 for the list of influential parameters the results of the calibrated multiplier factors for the considered parameter are reported in table 6 the multiplier factors of the parameters that are not considered for the calibration are set to 1 fig 4 shows the simulation results for o2 bod no3 and nh3 together with the corresponding observed data points at reach 5 corresponding to geraardsbergen 2 idegem the results of the model simulation at reach 6 corresponding to geraardsbergen 3 zandbergen and reach 8 corresponding to ninove are shown in supplementary materials section d as observed in fig 4 the simulated o2 and no3 concentrations capture the trends in the observations quite well moreover the results are satisfactory for both the calibration and the validation period nevertheless the model cannot simulate the over saturation of o2 during the summer the oxygen oversaturated condition in summer is related to the photosynthesis of algae due to limited data for the algae concentrations and for the phosphorus concentrations which is needed for the algae growth it is not possible to simulate the oversaturated oxygen in summers the results of the nh3 simulations follow the observed trend during the calibration but the model overestimates the nh3 concentrations during the validation period especially in summer as this pattern is observed for all reaches the revision of the nh3 emission for 2012 is required regarding the bod simulation except the underestimation in july 2011 the results are acceptable at reach 5 geraardsbergen 2 idegem also the bod results for the validation period at this reach are acceptable the reason of the unsatisfactory bod results in some durations is mainly related to the uncertainty involved in the diffuse pollution from agriculture moreover it should be noted that the measurement error of the bod concentration is high the results of the model simulations at reach 13 corresponding to denderleeuw and reach 16 corresponding to aalst illustrated in supplementary materials section d are used for the model validation 3 3 the results of the uncertainty analysis 3 3 1 the verification of the error model assumptions 3 3 1 1 the error model distribution normal at the beginning it was assumed that the model residuals are homoscedastic constant standard deviation non autocorrelated and have a normal distribution with zero mean this analysis involves 5 unknown parameters rk1 rk2 rk3 as the first three influential parameters for simulating o2 and bod σ ˆ o 2 and σ ˆ b o d standard deviations of o2 and bod residuals respectively the maximum number of iterations was set to 9000 the r statistic value smaller than 1 2 for all parameters points to convergence as shown in supplementary materials section b the values of r statistic are smaller than 1 2 for all parameters after 5000 iterations it indicates that the algorithm finds stationary posterior distributions for the parameters the generated parameter sets of the last 25 iterations 6750 9000 are used to derive the posterior distributions of the parameters after inferring the posterior ranges for the parameters the model residuals are post processed to assess the error model assumptions for this purpose one parameter set is randomly selected from the posterior parameter sets to run the wq model the model residuals for o2 and bod are calculated by comparing the simulated o2 and bod by observed o2 and bod respectively as shown in fig 5 the empirical distributions of the o2 and bod residuals blue circles do not match with a normal distribution dashed gray line therefore the model error assumption is not verified as observed in the figure the residuals of o2 and bod can be fit with a sep distribution red solid line moreover it is observed that the mean of the residuals are not zero therefore in the next step it is assumed that the error model for o2 and bod are homoscedastic constant standard deviation non autocorrelated and have a sep distribution with non zero mean 3 3 1 2 the error model distribution sep as shown in the previous analysis the assumption of a normal distribution for the error model is not valid therefore the complexity of the error model is increased by assuming a sep non gaussian distribution with non zero mean for the residuals the analysis involved 11 unknown parameters 3 model parameters and 8 error model parameters summarized in table 3 the preliminary results show that more iterations than in the previous analysis a normal distribution for the residuals are needed to reach the converged results when a sep distribution is assumed for the residuals this is due to the higher number of parameters involved in the likelihood function formulated with the non gaussian residuals the posterior parameter distributions would be more complex when more parameters are involved in the inference therefore the maximum number of iterations is set to 12000 the convergence analysis based on the r statistic values shows that acceptable results are obtained for most of the parameters using 12000 iterations then the residual post processing is performed to confirm the assumption regarding the error models for o2 and bod as shown in supplementary materials section b the values of the r statistic are smaller than 1 2 for most of the parameters after 5000 iterations however for a few parameters it slightly fluctuates around 1 2 when this criterion is not met the inferred posterior distribution of the parameters are only an approximation and the target distribution may be wider raat et al 2004 neglecting the r statistic values a bit higher than 1 2 for a few parameters it is assumed that the algorithm finds stationary posterior distributions for the parameters the generated parameter sets in the last 25 iterations 9000 12000 are used to drive the posterior distributions of the parameters after inferring the posterior parameter samples the next step is the residual analysis to assess the error model assumptions similar to previous analysis a random parameter set is selected from the inferred samples to calculate the residuals for o2 and bod as shown in fig 6 the residuals of o2 and bod perfectly match with a sep distribution verifying the error model assumptions in eqs 12 and 13 moreover it is assumed that the variances of the o2 and bod residuals are constant i e the residuals are homoscedastic to verify this assumption the residuals are plotted against the corresponding simulated values as scatter plots see fig 7 the residuals of o2 are randomly distributed and no specific trend is observed for bod however it seems that the residual variance is higher for bod concentrations over 3 5 mg l nevertheless no specific trend between simulated bod and residual is observed therefore the assumption of constant variance for the residuals is accepted however as observed data is limited it is difficult to draw strong conclusions about the variance of the residuals for example leta et al 2015 clearly found that the residual variance increases as a function of the simulated stream flows when the observed data is unlimited in surface water modeling since the frequency of the observed data is limited one per month the auto correlation between residuals is not concerned to be an issue however for modelling cases where the frequency of observed data is high e g hourly time step it is observed that the residuals can be significantly autocorrelated at lag one leta et al 2015 overall the residual diagnostic indicates that our assumptions regarding the error model are verified therefore the inferred posterior distributions of the model parameters and the error model parameters are valid and can be used to estimate the 95 cis of the simulations due to the parameter uncertainty and due to the total uncertainty 3 3 2 the parameter uncertainty the inferred posterior distributions of the parameters are illustrated in fig 8 the posterior distributions of the model parameters rk1 rk2 and rk3 are well identified within their prior ranges actually a narrower posterior distribution indicates a better identified posterior distribution the optimum range for σ ˆ o 2 is between 2 and 3 mg l and for σ ˆ b o d is between 1 5 and 2 5 mg l the posterior distribution of μ o 2 is between 0 and 1 with a peak close to zero for μ b o d the posterior range is between 0 and 1 with a peak close to 1 the latter indicates that the bod concentrations are underestimated the posterior distributions of ξ o 2 and ξ b o d show that the residuals of o2 and bod have positively skewed distributions since most of the generated sample values are higher than 1 the posterior distributions of the kurtosis parameters β o 2 and β b o d are wide between 0 5 and 0 5 the prior range although more samples have a value higher than 0 3 3 3 the model uncertainty the 95 confidence interval of the model simulations due to the parameter uncertainty is obtained by running the model using samples from the posterior distributions then the simulation results are ranked for each time step and the 0 025th and 0 975th quantiles are obtained for every time step accurate probabilistic modelling requires that the uncertainty bounds are statistically meaningful and show an appropriate measurement coverage blasone et al 2008 since the observed data points are limited in this study the percentage of the observation coverages and the trend of the uncertainty bounds are examined instead of focusing on the goodness of fit of the median simulated variables the black ranges in figs 9 and 10 show the 95 confidence intervals of the o2 and bod simulations at ninove during the calibration and validation periods due to the parameter uncertainty in order to calculate the uncertainty range due to the total uncertainty gray ranges in figs 9 and 10 the approximated residuals as total uncertainty are added to the simulation results the inferred parameters of the sep distributions for the residuals ξ o 2 β o 2 ξ b o d β b o d the inferred standard deviations σ ˆ o 2 σ ˆ b o d and the mean of the residuals μ o 2 μ b o d illustrated in fig 7 are used by eqs 12 and 13 to approximate the simulation residuals e o 2 t e b o d t then the 0 025th and 0 975th quantiles for every time step are considered for the uncertainty range due to the total uncertainty i e 95 total uncertainty range although the uncertainty ranges due to the parameters black ranges follow the trend of the o2 and bod observations they cannot totally cover the observations the observation coverages by the uncertainty ranges due to the parameters are summarized in table 7 they indicate that the parameter uncertainty explains some part of the total uncertainty but the other sources of uncertainty including uncertainty in the hydraulic data uncertainty in the pollutant loads data uncertainty in the calibration data and the model structural uncertainty also have an important contribution to the total uncertainty as expected the total uncertainty ranges cover almost all the measurements for o2 and bod however the uncertainty ranges are wide the wide bands imply the significant uncertainty involved in input data boundary conditions measurement and model structure two data points corresponding with the oversaturated oxygen concentrations in summer are not bounded by the total uncertainty range gray range in fig 9 as mentioned before the oxygen oversaturated condition in summer is related to the photosynthesis of algae fig 9 shows that the results of the bod simulation using the posterior parameter samples black range are satisfactory for low concentrations while the high bod concentrations are underestimated during calibration for the validation 2012 the results of bod using the posterior samples black range are acceptable if one is interested to perform the automatic calibration for other stations a multi site calibration is preferred over a sequential calibration the reason is that the computational cost of multi site calibration is less than that of sequential calibration moreover the interactions between the stations can be taken into account using multi site calibration 3 4 discussion according to the results of the sensitivity analysis the influential parameter set based on the lh oat method is almost the same as that based on the pawn method the total number of model evaluations for the lh oat method is 2200 while that for the pawn method is 30300 it indicates that the lh oat is more efficient than the pawn method the posterior distributions of the influential parameters are well identified by the bayesian uncertainty analysis it indicates the effectiveness of the lh oat to identify the influential parameters therefore to identify the influential parameters of the conceptual wq model for the model calibration the simple screening method is preferred over the advanced density based method by the manual calibration of the 8 identified important parameters the results of the conceptual model follow the trend of the observations for 4 wq variables including o2 bod no3 and nh3 therefore the use of lh oat as a fast sensitivity analysis method and a manual calibration is proposed as a solution to the wq parameterization problem in our uncertainty analysis the various error terms that make up the model residuals e g measurement model input and model structural error are not separated all the uncertainty sources are considered in a lumped way as a total uncertainty model residuals we focus on a correct statistical description of the total model error by post processing the model residuals without separating out various error sources this results in a pragmatic method for estimating parameter and simulation uncertainty without the need for explicit assumptions about various error contributions schoups and vrugt 2010 on the other hand some studies e g kuczera et al 2006 reichert and mieleitner 2009 have focused on separating the various error contributions such studies can be used to test hypotheses about possible causes for deviations between model predictions and data however complete disentangling of the various error sources can be quite challenging renard et al 2010 the results of the uncertainty analysis illustrate that the uncertainty range due to the total uncertainty is wide the total uncertainty can be reduced by improving the estimation of the pollutant loads to the river dender from wallonia point sources and agricultural runoff moreover the nh3 simulation could be improved by including the organic nitrogen simulation simulation of new variables such as phosphorous sediment and organic nitrogen requires the provision of the time series of their loads from the point source boundaries from the agricultural runoff and from wallonia as the upstream boundary furthermore as khorashadi zade et al 2019 stated measurements with higher frequency e g biweekly data instead of monthly data can significantly improve the performance of the model calibration and reduce the output uncertainty intervals last but not least despite all the uncertainties associated with limited uncertain input data and boundary conditions and measured in stream wq variables the conceptual wq model can be applied to support water management and policy makers in implementing integrated water recourse management rode et al 2010 actually it is impossible to evaluate the effectiveness of alternative watershed management plans without using modelling tools 4 conclusions wq models have been widely used to support decision making processes in view of wq management most wq simulators are characterized by the high number of parameters variables and processes and long calculation time in addition only few water quality observations are available for the calibration these lead to a serious problem for the model calibration and therefore limit the models applications furthermore due to above mentioned problems little information on the parameter and output uncertainty has been provided the objective of this study was to present a solution to the problem of wq parameterization by the use of a fast sensitivity analysis method and a manual calibration the success of our parameterization and the identifiability of the parameters were evaluated by the bayesian uncertainty assessment furthermore the parameter uncertainty and the output uncertainty were quantified major conclusions include the following a simple and fast screening method is preferred over an advanced computationally expensive sa method to identify the influential parameters for the calibration bayesian inference based on an mcmc algorithm is successfully applied to characterize the posterior parameter distributions and the simulation uncertainty for the conceptual wq model the simulation uncertainty range due to the parameters cannot cover the observations and the effect of other sources of uncertainty should be considered to explain the total uncertainty although the model calibration the sensitivity analysis and the uncertainty analysis are computationally demanding their applications are feasible thanks to the fast simulation of the conceptual wq model future studies may address the following issues including more wq variables such as organic matter algae and sediment to the simulation as this can improve o2 simulation in summer and nh3 simulation including more responses e g no3 and nh3 to the uncertainty analysis explicitly accounting for the input and model structural errors in the bayesian calibration software data availability the pawn method is implemented in the safe matlab octave toolbox for gsa pianosi et al 2015 safe is freely available for non commercial purposes at www bristol ac uk cabot resources safe toolbox the matlab toolbox of dream is available upon request from the developer jasper uci edu the citowa tool is available upon request from the developer befekadu woldegiorgis usask ca conflict of interest and authorship conformation form all authors have participated in a conception and design or analysis and interpretation of the data b drafting the article or revising it critically for important intellectual content and c approval of the final version this manuscript has not been submitted to nor is under review at another journal or other publishing venue the authors have no affiliation with any organization with a direct or indirect financial interest in the subject matter discussed in the manuscript declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgment the authors would like to thank the flanders hydraulics research for supporting and coordinating the project of development of conceptual models for an integrated river basin management we also thank dr peter meert for his help to build up the conceptual wq model for the river dender belgium appendix a supplementary data the following is the supplementary data to this article multimedia component 1 multimedia component 1 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2022 105331 
25645,water quality wq models parameterization remains a challenging task as these models are typically characterized by a high number of parameters the objective of this study was to present a solution to the wq parameterization problem by the use of a fast sensitivity analysis sa method and a manual calibration for this purpose we applied the simple screening lh oat method to the conceptual wq model of the river dender belgium to evaluate the effectiveness of lh oat to identify the influential parameters the advanced pawn method was applied a manual calibration was done using the influential parameters lh oat provided a parameter ranking that was very similar to the one of pawn but in a much more efficient way the bayesian uncertainty assessment showed the effectiveness of the lh oat results to conclude a fast screening method is preferred over an advanced sa method to identify the influential parameters for the calibration keywords parameterization sensitivity analysis uncertainty analysis dream zs error model 1 introduction the water quality wq of watershed systems is affected by high anthropogenic pressures including domestic wastewater discharges and industrial and agricultural activities the effects of pollutant emissions into rivers can be evaluated using mathematical water quality models moreover wq models have been widely used to support environmental management decisions by providing a predictive link between management strategies and the future state of river water vandenberghe et al 2007 rode et al 2010 therefore they are useful tools to support the decision making processes in view of wq management leta et al 2015 in order to be able to make simulations or perform scenario analyses with the wq models they should represent the reality adequately for this purpose it is necessary to set the model parameters to appropriate values by calibrating the model the main goal of model calibration is to find a set of parameter values in the parameter hyperspace which yields the model output as closely and consistently as possible to the measured data points however parameterization of wq models is a challenging task as most wq models are typically characterized by a high number of parameters variables and long calculation time e g rwqm1 simulator reichert et al 2001 rode et al 2010 on the other hand only few water quality observations are available for the model calibration mannina and viviani 2010 khorashadi zadeh et al 2019 manual calibration of a model with a large number of parameters is very tedious and time consuming vrugt et al 2003 on the other hand the efficiency of automatic calibration algorithms is reduced when the number of parameters is large duan et al 1992 therefore it is not feasible to include all model parameters in the calibration process bekele and nicklow 2007 nossent et al 2011 in order to support the choice of which model parameters should be focused on during calibration and which ones could be instead excluded from calibration and set to default values global sensitivity analysis gsa is becoming popular in environmental modeling practices e g muleta and nicklow 2005 van werkhoven et al 2009 norton 2015 pianosi et al 2016 gsa indeed allows for the identification of those parameters that have negligible influences on the model output and therefore can be fixed to any value within their feasible range so called factor fixing ff moreover gsa is applied to identify the parameters that have the largest influence on the model output so called factor prioritization fp saltelli et al 2008 many different gsa methods have been developed from simple screening methods to advanced quantitative ones sobol 1990 saltelli et al 2000 van griensven et al 2006 borgonovo 2007 pianosi and wagener 2015 therefore the major issue for the gsa is the selection of an appropriate method which is a highly specialized task requiring detailed knowledge on the application and limitation of methods in addition gsa of wq models with long calculation time is hardly feasible as gsa depends on multiple simulations all these problems lead to serious calibration issues of which the many failures typically do not reach the literature for the limited number of successfully calibrated models e g wade et al 2001 little information on the model parameter uncertainty has been provided raat et al 2004 the uncertainty of hydrological models has been extensively discussed in literature e g montanari et al 2009 leta et al 2015 but examples of uncertainty assessment of water quality models are limited stow et al 2007 han and zheng 2016 the reasons are mainly related to the long computational time of wq models the complicated uncertainty structure and the large number of parameters and variables involved in general in comparison with hydrological simulations rode et al 2010 han and zheng 2016 the uncertainty in wq simulations is greater and more complicated for example wq modelling typically integrates pollution simulations with hydrological simulations chemical reactions and pollutant transport are often represented by simple equations therefore much higher model structure errors are incorporated into the wq modelling process as compared to the hydrological modelling process additionally regarding the input data wq modelling also involves much higher input uncertainty than pure hydrological modelling rode et al 2010 han and zheng 2016 measurements or estimates of the pollution loads of both point sources e g waste water treatment plant discharges and distributed sources e g fertilizer application are often highly inaccurate and limited furthermore water quality observations used during the parameter estimation procedure often have a limited measurement frequency e g only on a biweekly or even monthly interval zheng and keller 2007 and are poorly measured khorashadi zadeh et al 2019 therefore the ambiguity in the parameter estimation process i e the parameter uncertainty is further increased as compared to hydrological modelling where high frequency e g daily flow observations are both more available and more reliable raat et al 2004 franceschini and tsai 2010 han and zheng 2016 as wq models are in most cases ultimately developed to inform management decisions there is a strong need for a rigorous uncertainty assessment to provide information regarding the parameter uncertainty and the model output uncertainty in the context of wq modelling rode et al 2010 recommended the use of a bayesian uncertainty assessment framework bayesian uncertainty assessment using markov chain monte carlo mcmc sampling bates and campbell 2001 kuczera and parent 1998 is a well known solution to infer the parameter uncertainty and the model output uncertainty kavetski et al 2006a 2006b vrugt et al 2008 in the field of hydrological modelling various bayesian inference approaches have been proposed and applied e g schoups and vrugt 2010 leta et al 2015 but their applications to wq modelling are limited han and zheng 2016 khorashadi zadeh et al 2019 the main objective of this paper is therefore to present a solution to the parameterization problem of wq models by the use of a fast sensitivity analysis method and a manual calibration moreover a bayesian uncertainty assessment is performed to evaluate the success of our parameterization and to quantify the model output uncertainty to this end a conceptual wq model has been built for the river dender belgium to simulate dissolved oxygen o2 biological oxygen demand bod nitrate no3 and ammonia nh3 prior to the calibration a gsa is performed to identify the influential model parameters in this study a simple screening method called latin hypercube one factor at a time lh oat method van griensven et al 2006 is applied to rank the parameters with a limited computational cost in order to evaluate the use of the lh oat for identifying the influential parameters the results of the advanced gsa method called pawn pianosi and wagener 2015 khorashadi zadeh et al 2017 are served as benchmark the pawn method is computationally more expensive than the lh oat method quantitatively assessing the influence of the parameters the model calibration is performed manually to find a set of appropriate values for the influential parameters in the parameter hyperspace the differential evolution adaptive metropolis dream zs algorithm a state of the art mcmc sampler has been employed as a bayesian uncertainty assessment tool to evaluate the identifiability of the parameters and to quantify the parameter and the output uncertainty residual post processing is performed to validate the assumptions regarding the model error 2 methods and materials 2 1 conceptual integrated tool for water quality assessment citowa conceptual wq simulators use simplified equations with physical chemical meaning to represent the physical chemical processes in rivers in a lumped way many conceptual wq simulators have been used as alternatives to detailed simulators to support decision making processes and scenario investigation however in general they are not suitable for complex systems such as river flood plain systems tidal rivers and looped sewer systems also most of them are not able to simulate pollutants attached to fine sediments such as phosphate and trace metals furthermore the numerical solvers of most conceptual simulators may provide unreliable results when the computation time step is large these limitations challenge the usefulness of conceptual water quality simulators woldegiorgis 2017 in order to enhance the applicability of conceptual river water quality simulators woldegiorgis woldegiorgis 2017 woldegiorgis et al 2017 developed a conceptual integrated tool for water quality assessment citowa as an alternative to detailed wq simulators in citowa the river system is represented by reaches which are conceptual elements that divide the channel longitudinally into different parts the discharges and the velocities in these reaches must be provided by external simulation tools e g by the tool introduced by meert et al 2016 the reaches are modeled as continuously stirred tank reactors cstrs whereby complete mixing is assumed within each reach the citowa integrates several components the first component is the simulator of dissolved oxygen biochemical oxygen demand nitrogen compounds phosphorus compounds and algae the simulator applies a formulation based on ordinary differential equations of the physico chemical processes based on a qual 2e brown and barnwell 1987 type formulation of these processes it integrates the mass balance equation with the transformation decay processes and solves the resulting differential equation using a quasi analytical solution the variables which can be simulated in the model are nitrogen compounds phosphorus compounds algae biological oxygen demand and dissolved oxygen moreover citowa can be used to simulate the sediment sediment bound pollutants and any arbitrary pollutant along a river it can also handle tidal rivers and looped sewer systems 2 2 the case study as a case study a conceptual wq model for the river dender is built up using citowa the dender basin located in belgium is a part of the international scheldt basin the region is characterized by a temperate maritime climate with an average annual rainfall of 819 mm and an average annual temperature of 10 c climate data org 2012 in view of the available data as shown in fig 1 the study is limited to the flemish part of the basin the official basin area according to the flemish authorities is 704 km2 aubin and varone 2002 in this part the main watercourse of the river dender has a total length of 51 km an extensive monitoring network of water quantity physico chemical and biological quality diffuse emissions including industries agriculture and wastewater treatment effluents are handled by the flemish environmental agency vmm and the hydrological information centre hic the major anthropogenic pollutant sources include the wastewater generated from household activities agricultural activities and industrial emissions the model build up steps and sources of the input data and the boundary data are explained in the supplementary materials section a the time step of the citowa simulation is hourly there are 7 measuring stations along the flemish part of the river dender the available data provided by vmm is for 2010 2012 therefore the conceptual wq model is executed for these 3 years the first two years are used for model calibration and the last year is considered for model validation in this study we focus on the o2 bod no3 and nh3 simulations table 1 summaries the available data of the control stations along the river the temporal variations of o2 bod no3 and nh3 during the simulation period 2010 2012 at overboelare idegem zandbergen ninove and aalst are represented in suplementaly materials section b 2 3 the sensitivity analysis after building up the model the next step is to determine the values of the model parameters however the values of the parameters of the conceptual model cannot be measured directly therefore a model calibration is needed to adjust the parameter values by maximizing the agreement between the model simulations and historical measurements laloy et al 2010 vrugt et al 2013 mostly due to lack of calibration data only a few model parameters can be identified during the calibration procedure therefore prior to calibration a gsa can be essential to support the calibration procedure by identifying the influential and non influential parameters muleta and nicklow 2005 van werkhoven et al 2009 nossent and bauwens 2012 norton 2015 based on this identification sa allows for a reduction of the number of parameters incorporated in the calibration procedure using the results of the sa the set of parameters incorporated in the calibration is reduced to the influential parameters i e ff the non influential parameters are set to the default values saltelli et al 2008 in literature many gsa techniques are available from simple and easy to implement methods e g morris 1991 to advanced and quantitative methods e g sobol 1990 in this study two gsa methods are applied the latin hypercube one factor at a time lh oat method van griensven et al 2006 and the moment independent pawn method pianosi and wagener 2015 khorashadi zadeh et al 2017 the former is a simple so called screening method used for ranking the parameters with limited computational cost while the latter is a more advanced method that is computationally more expensive and is applied to quantitatively assess the influence of the parameters the pawn method will therefore serve as a benchmark to evaluate the use of the lh oat for identifying the influential parameters in the following the applications of the lh oat and the pawn methods to the conceptual wq model are explained 2 3 1 the lh oat method the lh oat method van griensven et al 2006 is an enhancement of the basic morris elementary effect method morris 1991 to achieve a better coverage of the parameter space it uses latin hypercube lh sampling to generate n l h random parameter sets then each lh sampled parameter set is varied p times p is the number of parameters by changing each of the parameters one at a time one factor at a time oat and keeping the other parameters at their current values the perturbation of the i th parameter is performed by a fixed fraction f i and the same fraction is used for all parameters the method combines the robustness of the lh sampling ensuring that the full range of all parameters has been sampled with the precision of an oat design assuring that the change in the output in each model run can be unambiguously attributed to the parameter that was changed the method operates by loops each loop starts with an lh point around each lh point j a partial effect s i j l h for each parameter x i is calculated as in percentage 1 s i j l h 100 f x 1 x i 1 f i x p f x 1 x i x p f x 1 x i 1 f i x p f x 1 x i x p 2 f i where f is the function that maps the model parameters into a scalar variable as model output therefore a loop requires p 1 runs the final effect s i l h is calculated by averaging these partial effects of each loop for all lh points thus for n l h loops as below 2 s i l h j 1 n l h s i j l h n l h the method is very efﬁcient as it only requires a total of n l h p 1 runs the final effects are ranked with the largest effect being given rank 1 and the smallest effect being given a rank equal to the total number of parameters analyzed for a non influential parameters s i l h should be 0 the lh oat method is applied to the conceptual wq model of the river dender to identify the influential and the non influential parameters parameter screening for this purpose 10 parameters p 10 which are mathematically relevant for o2 bod no3 and nh3 simulations are selected see table 2 the feasible ranges of these 10 parameters reported in table 2 are determined according to the qual2e manual brown and barnwell 1987 since there is no prior information on the parameter distributions parameter values are sampled from a uniform distribution dividing the parameter range in 200 equal intervals n l h 200 therefore the total number of model evaluations is 2200 n l h p 1 nossent 2012 suggested n l h 50 to have reliable and converging sa results thus n l h 200 is a conservative selection and fewer intervals e g n l h 100 could be considered this will results in less model evaluations the fraction f i for changing the parameters is set to 0 05 typically when the objective of the sa is to inform the calibration procedure screening a performance measure is used as a scalar variable to evaluate the model results e g van griensven et al 2006 pappenberger et al 2008 nossent et al 2011 in this study the mean absolute error mae is selected as performance measure 3 m a e i 1 m c i s i m c i o b s m where c i o b s is the ith observed concentration c i s i m is the simulated concentration corresponding to the ith observation and m is the total number of observed data points 2 3 2 the pawn method pawn pianosi and wagener 2015 is a density based gsa method where the entire model output distribution is used to quantify the relative influence of the parameters on the model output the pawn sensitivity indices measure the sensitivity of y to parameter x i by the distance between the unconditional cumulative distribution function cdf of y i e a scalar variable as model output which is obtained by varying all parameters simultaneously and the conditional cdfs of y which are obtained by varying all parameters but x i i e x i is fixed at a nominal value x i liu et al 2006 borgonovo 2007 pianosi and wagener 2015 proposed to measure the distance between the conditional and unconditional cdfs by the kolmogorov smirnov statistic ks kolmogorov 1933 smirnov 1939 as below 4 k s x i max f y y f y x i y y where f y y is the unconditional cdf of the output y and f y x i y is the conditional cdf when x i is fixed as f y x i y characterizes the output distribution when the variability due to x i is removed its distance from f y y indicates the effect of x i on y when f y x i y overlaps with f y y completely k s x i is equal to zero which means that removing the uncertainty about x i does not affect the output distribution i e x i has no influence on y a large distance instead indicates a high influence of the parameter since ks depends on the conditioning value of x i the pawn sensitivity index t i considers a statistic e g maximum or median over all possible values of x i 5 t i stat k s x i x i the pawn index varies between 0 and 1 the higher the value the more influential x i for complex and non linear models the analytical computation of the pawn index t i is usually impossible pianosi and wagener 2015 therefore suggested the following approximate numerical procedure first the ks statistic in eq 4 is approximated by using empirical unconditional and conditional distributions n u parameter sets are generated by varying all parameters simultaneously then n u model evaluations are used to estimate the empirical unconditional distribution of y to estimate the empirical conditional distributions of y n c parameter sets are generated by varying all parameters except x i which is kept to a fixed value secondly in eq 5 the statistic with respect to the conditioning value of x i is approximated by using n randomly sampled values for the fixed parameter x i therefore the total number of model evaluations required to calculate the pawn index t i for all the p parameters is n u n n c p in addition to lh oat the pawn method is applied to the conceptual wq model of the river dender the purpose is to identify the most influential parameters for different model output variables i e o2 bod no3 and nh3 and to compare the results with those of lh oat to perform the pawn analysis a sample size of 300 n u n c 300 is used to approximate the conditional and unconditional cumulative distribution functions of the model output the approximation of the conditional distribution is repeated for 10 different condition values for each parameter n 10 therefore the total number of model evaluations is 30300 n u n c p n as a scalar representation for the model output the mae see eq 3 is calculated by comparing the results of the conceptual model with the observed data points since numerical approximations rather than analytical solutions are utilized in the pawn method to calculate the sensitivity indices small but non zero indices may be obtained for the indices of non influential parameters to define a threshold for screening the parameters khorashadi zadeh et al 2017 proposed to calculate the sensitivity index of a dummy parameter this dummy parameter has no influence on the model output but will have a non zero sensitivity index representing the error due to the numerical approximation hence the parameters whose indices are above the sensitivity index of the dummy parameter can be classified as influential whereas the parameters whose indices are below this index are within the range of the numerical error and should be considered as non influential the readers are referred to khorashadi zadeh et al 2017 for the procedure to calculate the pawn sensitivity index of the dummy parameter 2 4 the parameter estimation procedure the main goal of model calibration is to find a set of parameter values in the parameter hyperspace which yields the model output as closely and consistently as possible to the measured data points considering the results of the sa the calibration is only performed for the parameters that are identified as influential calibration methods can be broadly grouped into manual and automatic calibration techniques sorooshian and gupta 1995 2 4 1 the manual calibration the manual calibration is performed by adjusting these influential parameters for simulating o2 bod no3 and nh3 the effect of changing the model parameters is assessed by comparing the relevant model output with the observed data due to the limited observed data points the comparison and the calibration results assessment are performed visually the simulation results of reaches 5 6 and 8 for 2010 and 2011 are used for the model calibration the corresponding measuring stations for these reaches are idegem zandbergen and ninove respectively the simulation results for 2012 are used for the model validation in time at these reaches and stations moreover the spatial validation is performed using the independent stations denderleeuw and aalst which are not used for the calibration 2 4 2 the model validation once the model is calibrated the model validation is performed using an independent data set not used in the calibration procedure the validation shows the capability and credibility of the calibrated model for the simulation and prediction the hydraulic input data i e flow velocity water depth and the pollutant load of the boundary conditions are updated for the validation period then the model is run using the calibrated parameters values as the conceptual wq model provides the concentrations at different locations along the river in addition to validation in time validation in space is performed using independent stations if acceptable results are obtained for the validation the procedure for the model calibration is finished otherwise it is necessary to go back to the initial stage of the model calibration and to reconsider the choice of the calibration parameters and or the model input data 2 5 the automatic calibration and the uncertainty analysis automatic calibration methods search for an optimal set of parameters in the parameter space although it is possible to obtain an optimum parameter set and an optimum model output during the calibration this calibrated model may perform poorly for a different data set during the validation in that case other parameter sets may be ranked as optimal brazier et al 2000 moreover a single best parameter set only minimizes the overall residuals but does not eliminate them completely nossent 2012 consequently the model results are made with a certain amount of uncertainty beven 2006 therefore instead of searching for a single optimal parameter set and for the best model simulation results some optimization algorithm propose alternative methods for finding a range a distribution or a probability function of model parameters i e posterior distributions of parameters and model output beven and binley 1992 bayesian inference using markov chain monte carlo mcmc sampling bates and campbell 2001 kuczera and parent 1998 is a well known solution to infer the posterior distribution of parameters 2 5 1 bayesian inference using dream zs dream zs vrugt et al 2008 2009a is a state of the art mcmc algorithm which applies a formal statistical bayesian approach to infer the posterior parameter distribution detailed explanation of this algorithm can be found elsewhere vrugt et al 2009b laloy and vrugt 2012 dream zs is an extension of dream vrugt et al 2008 2009a to simplify inference and to enhance the convergence rate like the original dream algorithm instead of searching for a single optimal parameter dream zs infers the posterior distribution of model parameters by using the information content in the calibration data according to bayes theorem the relationship between a model output y and its corresponding observation z can be expressed as 6 z y θ ε where θ is a vector of uncertain model parameters and ε is a lumped residual error term bayes rule can be adopted as 7 p θ z p θ l θ z where p θ z and p θ represent the posterior and prior distribution of θ respectively and l θ z represent the likelihood function which is mathematically determined by the error model of residual ε if we assume that the residuals are unrelated and normally distributed with zero mean and standard deviation σ then the log likelihood function can be written as follows vrugt et al 2013 8 l θ σ z m 2 log 2 π t 1 m log σ t 1 2 t 1 m z t y t θ σ t 2 where m is the total number of observations this formulation allows for homoscedastic i e constant variance and heteroscedastic errors i e variance dependent on the magnitude of the data for example for the hydrological modelling the heteroscedasticity can be explicitly considered by assuming that σ increases linearly with the simulated flow leta et al 2015 to extend the applicability of the algorithm to situations where the error residuals are correlated and or non gaussian schoups and vrugt 2010 proposed the following model for the residual errors 9 φ k b e t σ t a t with a t sep 0 1 ξ β where φ k b is an autoregressive polynomial with k autoregressive parameters to account for dependence and correlation between errors b is the backshift operator and a t is an independent and identically distributed random error with zero mean and unit standard deviation described by a skew exponential power sep density the skewness parameter ξ and the kurtosis parameter β are tunable hyper parameters to account for nonnormality ξ takes a positive value affecting the asymmetry the density is symmetric for ξ 1 positively skewed for ξ 1 and negatively skewed for ξ 1 the value of β is between 1 and 1 and determines the peakedness of the distribution in case of symmetric density ξ 1 β 0 results in a gaussian distribution and β 0 results in a more peaked density more details about the sep distribution and its mathematical formula can be found in schoups and vrugt 2010 therefore the formulated model error contains a number of hyper parameters e g ξ β hereafter denoted as a vector φ both the model parameters θ and the model error hyper parameters φ are estimated from the observed data based on the bayesian inference based on the error model defined in eq 9 the resulting expression for the log likelihood is 10 l θ σ φ z m log 2 σ ξ ω β ξ ξ 1 t 1 m log σ t c β t 1 m a ξ t 2 1 β where a ξ t ξ s i g n μ ξ σ ξ μ ξ σ ξ a t and μ ξ σ ξ ω β and c β are computed as a function of skewness parameter ξ and kurtosis parameter β as detailed in schoups and vrugt 2010 as suggested by schoups and vrugt 2010 the complexity of the error model should be gradually increased from a gaussian homoscedastic and non autocorrelated model to eq 9 until posterior checks verified the error model assumptions in the context of bayesian inference error models of water quality responses have been specifically discussed by a few studies wellen et al 2014 simply assumed independent and normally distributed error for sediment modelling han and zheng 2016 gradually increased the complexity of error models until posterior checks confirmed that the residual errors are consistent with the error model assumptions finally they adopted a non gaussian homoscedasticity and auto correlated error model for nitrate simulation for the multiple response calibration different error models are required for different responses if it is assumed that the residual errors of multiple responses are independent then the combined likelihood function l m u l t i p l e is the sum of individual log likelihood functions balin talamba et al 2010 han and zheng 2016 11 l m u l t i p l e i 1 l i θ σ φ z i according to the mcmc theory the dream algorithm is expected to eventually converge to a stationary distribution which should be the desired target distribution but in practice the question is how to actually assess if convergence has been reached without knowledge of the actual target distribution the most powerful formal way to evaluate the convergence of the dream zs algorithm is the r statistic gelman and rubin 1992 in practice a value of the r statistic smaller than 1 2 for all parameters points to convergence vrugt et al 2009a 2 5 2 setting up the numerical experiment the dream zs algorithm is adopted as an uncertainty analysis tool to perform simultaneously the automatic calibration and the uncertainty analysis for the conceptual wq model according to the results of draem zs the optimum range of the parameters and the parameter uncertainty ranges are obtained moreover it allows to infer the appropriate statistical distribution of the residual errors for the conceptual wq model which is a multi response model the sequential calibration approach is sound and useful however a more straight forward alternative is a multi response bayesian calibration han and zheng 2016 for example raat et al 2004 have shown that including multiple responses in the bayesian calibration can improve uncertainty assessment and the identifiability of model parameters in this study the uncertainty analysis is applied for o2 and bod simulations no3 and nh3 simulations are not included the measured o2 and bod at ninove reach 8 in the conceptual model are used as observations in the dream zs algorithm multi response calibration because the measured data is available for three years 2010 2012 at this station moreover according to the manual calibration where three stations are considered for the calibration there is no conflict between the stations see section 3 3 however there is no limitation for the dream zs algorithm to include more measured data points and it is possible to perform a multi site calibration according to the sensitivity analysis the most influential parameters for simulating o2 and bod are selected for the uncertainty analysis the other parameters of the model are set to their default values the simulation results for 2010 and 2011 are used for the model calibration the simulation results for 2012 are used for the model validation in time at this station the critical step in the dream zs application is to define an explicit statistical model for the residuals i e applying an approximated error model and to formulate the likelihood function as schoups and vrugt 2010 suggested the complexity of the error model is gradually increased from a simple gaussian homoscedastic and non autocorrelated type to the most complicated one i e correlated heteroscedastic and non gaussian until posterior checks confirm that the residuals are consistent with the error model assumptions eventually a non gaussian with a non zero mean non correlated and homoscedastic error model has been adopted for o2 and bod responses as below 12 e o 2 t σ ˆ o 2 a o 2 t μ o 2 with a o 2 t sep 0 1 ξ o 2 β o 2 13 e b o d t σ ˆ b o d a b o d t μ b o d with a b o d t sep 0 1 ξ b o d β b o d where e o 2 t and e b o d t are the residuals for o2 and bod μ o 2 and μ b o d are the mean of the residuals for o2 and bod respectively σ ˆ o 2 and σ ˆ b o d are the standard deviations of the residuals for o2 and bod respectively a o 2 t and a b o d t are random errors with zero mean and unit standard deviation described by a sep density ξ o 2 and ξ b o d are the skewness parameters for o2 and bod and β o 2 and β b o d are the kurtosis parameters for o2 and bod respectively therefore the formulated models for the residuals contain a number of hyper parameters i e ξ o 2 ξ b o d β o 2 and β b o d hereafter denoted as a vector φ in addition to the model parameters θ and the hyper parameters φ standard deviations and the means of the residuals for o2 and bod are estimated from the observed data based on the bayesian inference therefore in total 11 unknown variables summarized in table 3 are involved in the calibration and uncertainty analysis uniform distributions are considered for the prior distributions of the model parameters and error model parameters the ranges of the parameters are selected based on literature values brown and barnwell 1987 schoups and vrugt 2010 vrugt et al 2009b the maximum number of iterations is set to 12000 the convergence analysis based on the r statistic values is performed to evaluate the sufficiency of the maximum number of iterations then the residual post processing is performed to confirm the assumption regarding the error models for o2 and bod responses after evaluating the residuals assumptions the posterior density functions of the parameters are plotted and the 95 confidence intervals cis of the simulated o2 and bod due to the parameter uncertainty and due to the total uncertainty are calculated the generated parameter sets after the convergence are considered as posterior parameter sets considering the r statistic values for the parameters see section b in supplementary materials the last 25 of the generated parameter sets 9000 12000 are selected as posterior parameter set once the posterior distributions of the model parameters and the model error parameters are inferred the cis of the simulation can be estimated the 95 cis of the simulation due to the parameter uncertainty are obtained by running the model for the posterior samples of the model parameters then the simulation results are ranked for each time step and the 0 025th and 0 975th quantiles are obtained for every time step to calculate the 95 cis of the simulation due to the total uncertainty the total errors e o 2 t and e b o d t are added to the simulation results as follows 14 o 2 t o 2 t e o 2 t i f o 2 t 0 t h e n o 2 t 0 15 b o d t b o d t e b o d t i f b o d t 0 t h e n b o d t 0 where o 2 t and b o d t are the simulated o2 and bod using the posterior parameter sets and o 2 t and b o d t are the simulated o2 and bod considering the total uncertainty e o 2 t and e b o d t are the total error including observation error forcing and output data model structural error input error and boundary condition error similar to the cis of the simulation due to the parameter uncertainty for every time step the 0 025th and 0 975th quantiles of o 2 t and b o d t are calculated to obtained the 95 cis of the simulation due to the total uncertainty 2 6 the assessment of the water quality the assessment of the water quality can be done based on the ambient water quality standards the standards set by the vmm are listed in table 4 vlarem 1995 as a parameter of wq bod is an indicator of the amount of organic matter the water contains the content of organic matter in a river depends on several factors such as the loads of urban and industrial discharges the capacity of self purification of the river the purification infrastructure diffuse pressures etc bod is directly related to the o2 concentration because one of the main causes of the decrease of o2 is the degradation of the organic matter it is necessary to ensure that the oxygen levels in the river are above the acceptable levels as the latter affects aquatic life the oxygen in the aquatic system is produced by the photosynthesis of plants and removed by respirations of plants animals and bacteria ammonia compounds are found in water under reducing conditions in this environment ammonia is quickly converted to nitrite and then to nitrate this process is called nitrification and it is faster when the water is well aerated this decay of ammonia gives rise to an oxygen demand of 2 mol of oxygen per mole of nitrogen oxidized a high content of ammonia facilitates the multiplication of microbes and that is why the presence of free ammonia or ammonium ion in the water is an indicator of recent and dangerous contamination 3 results and discussion 3 1 the results of the sensitivity analysis the lh oat screening method is applied to analyze 10 parameters of the conceptual wq model for better comparison the lh oat sensitivity indices are scaled between 0 and 1 the sensitivity analyses are performed using the observed data at ninove considering the results of sa for simulating o2 see fig 2 a 4 parameters rk2 bc1 rk2 and rk3 see table 2 for the definition of these parameters are identified as important fig 2 b shows that two parameters rk3 and rk1 appear to be influential according to the lh oat analysis for simulating bod for simulating no3 four parameters are identified as important by lh oat kd ko2 bc1 and bc2 see fig 2 c and finally bc1 is the only influential parameter for simulating nh3 according to the lh oat analysis see fig 2 d to evaluate the results of the simple lh oat screening method the more advanced pawn method is applied to analyze the 10 parameters of the wq model the mae is hereby used as performance measure the sensitivity index of the dummy parameter is calculated to set a threshold for identifying the influential parameters fig 3 shows the pawn results for simulating o2 bod no3 and nh3 the dummy parameter is indicated in the figures with an arrow and the dashed line represents its sensitivity index as a threshold for the parameter screening the list of influential parameters based on the pawn method is given in table 5 table 5 summarizes the results of the lh oat and pawn methods according to the lh oat method bc2 is important for simulating o2 see fig 2 while the pawn index of bc2 is close to that of dummy parameter see fig 3 on the other hand bc3 is identified as important by the pawn method for simulating o2 while it is not as important based on the lh oat method however for simulating o2 bc3 and bc2 are less influential as compared to the other influential parameters based on the pawn and lh oat methods respectively for simulating bod rk1 and rk3 are the most influential parameters in both methods while bc1 and bc2 are identified less influential in pawn for simulating no3 and nh3 the lists of influential parameters based on lh oat and pawn are the same comparison of the results of the two sa methods illustrates that the identified influential parameters are almost the same since the lh oat requires less computation cost as compared to that of the pawn method 2200 vs 30300 model evaluations the lh oat method is an appropriate method for fast parameterization of the conceptual wq model it is also recommended for other wq models to identify the important parameters for the calibration based on the sa results 8 parameters are selected for the model calibration rk1 rk2 rk3 bc1 bc2 bc3 kd and ko2 the definitions of the parameters are given in table 2 these parameters were also important for the conceptual wq model of the river zenne belgium khorashadi zadeh et al 2019 3 2 the results of the manual calibration the manual calibration is performed by adjusting the multiplier factors of the 8 influential parameters identified by the sa see section 3 1 for the list of influential parameters the results of the calibrated multiplier factors for the considered parameter are reported in table 6 the multiplier factors of the parameters that are not considered for the calibration are set to 1 fig 4 shows the simulation results for o2 bod no3 and nh3 together with the corresponding observed data points at reach 5 corresponding to geraardsbergen 2 idegem the results of the model simulation at reach 6 corresponding to geraardsbergen 3 zandbergen and reach 8 corresponding to ninove are shown in supplementary materials section d as observed in fig 4 the simulated o2 and no3 concentrations capture the trends in the observations quite well moreover the results are satisfactory for both the calibration and the validation period nevertheless the model cannot simulate the over saturation of o2 during the summer the oxygen oversaturated condition in summer is related to the photosynthesis of algae due to limited data for the algae concentrations and for the phosphorus concentrations which is needed for the algae growth it is not possible to simulate the oversaturated oxygen in summers the results of the nh3 simulations follow the observed trend during the calibration but the model overestimates the nh3 concentrations during the validation period especially in summer as this pattern is observed for all reaches the revision of the nh3 emission for 2012 is required regarding the bod simulation except the underestimation in july 2011 the results are acceptable at reach 5 geraardsbergen 2 idegem also the bod results for the validation period at this reach are acceptable the reason of the unsatisfactory bod results in some durations is mainly related to the uncertainty involved in the diffuse pollution from agriculture moreover it should be noted that the measurement error of the bod concentration is high the results of the model simulations at reach 13 corresponding to denderleeuw and reach 16 corresponding to aalst illustrated in supplementary materials section d are used for the model validation 3 3 the results of the uncertainty analysis 3 3 1 the verification of the error model assumptions 3 3 1 1 the error model distribution normal at the beginning it was assumed that the model residuals are homoscedastic constant standard deviation non autocorrelated and have a normal distribution with zero mean this analysis involves 5 unknown parameters rk1 rk2 rk3 as the first three influential parameters for simulating o2 and bod σ ˆ o 2 and σ ˆ b o d standard deviations of o2 and bod residuals respectively the maximum number of iterations was set to 9000 the r statistic value smaller than 1 2 for all parameters points to convergence as shown in supplementary materials section b the values of r statistic are smaller than 1 2 for all parameters after 5000 iterations it indicates that the algorithm finds stationary posterior distributions for the parameters the generated parameter sets of the last 25 iterations 6750 9000 are used to derive the posterior distributions of the parameters after inferring the posterior ranges for the parameters the model residuals are post processed to assess the error model assumptions for this purpose one parameter set is randomly selected from the posterior parameter sets to run the wq model the model residuals for o2 and bod are calculated by comparing the simulated o2 and bod by observed o2 and bod respectively as shown in fig 5 the empirical distributions of the o2 and bod residuals blue circles do not match with a normal distribution dashed gray line therefore the model error assumption is not verified as observed in the figure the residuals of o2 and bod can be fit with a sep distribution red solid line moreover it is observed that the mean of the residuals are not zero therefore in the next step it is assumed that the error model for o2 and bod are homoscedastic constant standard deviation non autocorrelated and have a sep distribution with non zero mean 3 3 1 2 the error model distribution sep as shown in the previous analysis the assumption of a normal distribution for the error model is not valid therefore the complexity of the error model is increased by assuming a sep non gaussian distribution with non zero mean for the residuals the analysis involved 11 unknown parameters 3 model parameters and 8 error model parameters summarized in table 3 the preliminary results show that more iterations than in the previous analysis a normal distribution for the residuals are needed to reach the converged results when a sep distribution is assumed for the residuals this is due to the higher number of parameters involved in the likelihood function formulated with the non gaussian residuals the posterior parameter distributions would be more complex when more parameters are involved in the inference therefore the maximum number of iterations is set to 12000 the convergence analysis based on the r statistic values shows that acceptable results are obtained for most of the parameters using 12000 iterations then the residual post processing is performed to confirm the assumption regarding the error models for o2 and bod as shown in supplementary materials section b the values of the r statistic are smaller than 1 2 for most of the parameters after 5000 iterations however for a few parameters it slightly fluctuates around 1 2 when this criterion is not met the inferred posterior distribution of the parameters are only an approximation and the target distribution may be wider raat et al 2004 neglecting the r statistic values a bit higher than 1 2 for a few parameters it is assumed that the algorithm finds stationary posterior distributions for the parameters the generated parameter sets in the last 25 iterations 9000 12000 are used to drive the posterior distributions of the parameters after inferring the posterior parameter samples the next step is the residual analysis to assess the error model assumptions similar to previous analysis a random parameter set is selected from the inferred samples to calculate the residuals for o2 and bod as shown in fig 6 the residuals of o2 and bod perfectly match with a sep distribution verifying the error model assumptions in eqs 12 and 13 moreover it is assumed that the variances of the o2 and bod residuals are constant i e the residuals are homoscedastic to verify this assumption the residuals are plotted against the corresponding simulated values as scatter plots see fig 7 the residuals of o2 are randomly distributed and no specific trend is observed for bod however it seems that the residual variance is higher for bod concentrations over 3 5 mg l nevertheless no specific trend between simulated bod and residual is observed therefore the assumption of constant variance for the residuals is accepted however as observed data is limited it is difficult to draw strong conclusions about the variance of the residuals for example leta et al 2015 clearly found that the residual variance increases as a function of the simulated stream flows when the observed data is unlimited in surface water modeling since the frequency of the observed data is limited one per month the auto correlation between residuals is not concerned to be an issue however for modelling cases where the frequency of observed data is high e g hourly time step it is observed that the residuals can be significantly autocorrelated at lag one leta et al 2015 overall the residual diagnostic indicates that our assumptions regarding the error model are verified therefore the inferred posterior distributions of the model parameters and the error model parameters are valid and can be used to estimate the 95 cis of the simulations due to the parameter uncertainty and due to the total uncertainty 3 3 2 the parameter uncertainty the inferred posterior distributions of the parameters are illustrated in fig 8 the posterior distributions of the model parameters rk1 rk2 and rk3 are well identified within their prior ranges actually a narrower posterior distribution indicates a better identified posterior distribution the optimum range for σ ˆ o 2 is between 2 and 3 mg l and for σ ˆ b o d is between 1 5 and 2 5 mg l the posterior distribution of μ o 2 is between 0 and 1 with a peak close to zero for μ b o d the posterior range is between 0 and 1 with a peak close to 1 the latter indicates that the bod concentrations are underestimated the posterior distributions of ξ o 2 and ξ b o d show that the residuals of o2 and bod have positively skewed distributions since most of the generated sample values are higher than 1 the posterior distributions of the kurtosis parameters β o 2 and β b o d are wide between 0 5 and 0 5 the prior range although more samples have a value higher than 0 3 3 3 the model uncertainty the 95 confidence interval of the model simulations due to the parameter uncertainty is obtained by running the model using samples from the posterior distributions then the simulation results are ranked for each time step and the 0 025th and 0 975th quantiles are obtained for every time step accurate probabilistic modelling requires that the uncertainty bounds are statistically meaningful and show an appropriate measurement coverage blasone et al 2008 since the observed data points are limited in this study the percentage of the observation coverages and the trend of the uncertainty bounds are examined instead of focusing on the goodness of fit of the median simulated variables the black ranges in figs 9 and 10 show the 95 confidence intervals of the o2 and bod simulations at ninove during the calibration and validation periods due to the parameter uncertainty in order to calculate the uncertainty range due to the total uncertainty gray ranges in figs 9 and 10 the approximated residuals as total uncertainty are added to the simulation results the inferred parameters of the sep distributions for the residuals ξ o 2 β o 2 ξ b o d β b o d the inferred standard deviations σ ˆ o 2 σ ˆ b o d and the mean of the residuals μ o 2 μ b o d illustrated in fig 7 are used by eqs 12 and 13 to approximate the simulation residuals e o 2 t e b o d t then the 0 025th and 0 975th quantiles for every time step are considered for the uncertainty range due to the total uncertainty i e 95 total uncertainty range although the uncertainty ranges due to the parameters black ranges follow the trend of the o2 and bod observations they cannot totally cover the observations the observation coverages by the uncertainty ranges due to the parameters are summarized in table 7 they indicate that the parameter uncertainty explains some part of the total uncertainty but the other sources of uncertainty including uncertainty in the hydraulic data uncertainty in the pollutant loads data uncertainty in the calibration data and the model structural uncertainty also have an important contribution to the total uncertainty as expected the total uncertainty ranges cover almost all the measurements for o2 and bod however the uncertainty ranges are wide the wide bands imply the significant uncertainty involved in input data boundary conditions measurement and model structure two data points corresponding with the oversaturated oxygen concentrations in summer are not bounded by the total uncertainty range gray range in fig 9 as mentioned before the oxygen oversaturated condition in summer is related to the photosynthesis of algae fig 9 shows that the results of the bod simulation using the posterior parameter samples black range are satisfactory for low concentrations while the high bod concentrations are underestimated during calibration for the validation 2012 the results of bod using the posterior samples black range are acceptable if one is interested to perform the automatic calibration for other stations a multi site calibration is preferred over a sequential calibration the reason is that the computational cost of multi site calibration is less than that of sequential calibration moreover the interactions between the stations can be taken into account using multi site calibration 3 4 discussion according to the results of the sensitivity analysis the influential parameter set based on the lh oat method is almost the same as that based on the pawn method the total number of model evaluations for the lh oat method is 2200 while that for the pawn method is 30300 it indicates that the lh oat is more efficient than the pawn method the posterior distributions of the influential parameters are well identified by the bayesian uncertainty analysis it indicates the effectiveness of the lh oat to identify the influential parameters therefore to identify the influential parameters of the conceptual wq model for the model calibration the simple screening method is preferred over the advanced density based method by the manual calibration of the 8 identified important parameters the results of the conceptual model follow the trend of the observations for 4 wq variables including o2 bod no3 and nh3 therefore the use of lh oat as a fast sensitivity analysis method and a manual calibration is proposed as a solution to the wq parameterization problem in our uncertainty analysis the various error terms that make up the model residuals e g measurement model input and model structural error are not separated all the uncertainty sources are considered in a lumped way as a total uncertainty model residuals we focus on a correct statistical description of the total model error by post processing the model residuals without separating out various error sources this results in a pragmatic method for estimating parameter and simulation uncertainty without the need for explicit assumptions about various error contributions schoups and vrugt 2010 on the other hand some studies e g kuczera et al 2006 reichert and mieleitner 2009 have focused on separating the various error contributions such studies can be used to test hypotheses about possible causes for deviations between model predictions and data however complete disentangling of the various error sources can be quite challenging renard et al 2010 the results of the uncertainty analysis illustrate that the uncertainty range due to the total uncertainty is wide the total uncertainty can be reduced by improving the estimation of the pollutant loads to the river dender from wallonia point sources and agricultural runoff moreover the nh3 simulation could be improved by including the organic nitrogen simulation simulation of new variables such as phosphorous sediment and organic nitrogen requires the provision of the time series of their loads from the point source boundaries from the agricultural runoff and from wallonia as the upstream boundary furthermore as khorashadi zade et al 2019 stated measurements with higher frequency e g biweekly data instead of monthly data can significantly improve the performance of the model calibration and reduce the output uncertainty intervals last but not least despite all the uncertainties associated with limited uncertain input data and boundary conditions and measured in stream wq variables the conceptual wq model can be applied to support water management and policy makers in implementing integrated water recourse management rode et al 2010 actually it is impossible to evaluate the effectiveness of alternative watershed management plans without using modelling tools 4 conclusions wq models have been widely used to support decision making processes in view of wq management most wq simulators are characterized by the high number of parameters variables and processes and long calculation time in addition only few water quality observations are available for the calibration these lead to a serious problem for the model calibration and therefore limit the models applications furthermore due to above mentioned problems little information on the parameter and output uncertainty has been provided the objective of this study was to present a solution to the problem of wq parameterization by the use of a fast sensitivity analysis method and a manual calibration the success of our parameterization and the identifiability of the parameters were evaluated by the bayesian uncertainty assessment furthermore the parameter uncertainty and the output uncertainty were quantified major conclusions include the following a simple and fast screening method is preferred over an advanced computationally expensive sa method to identify the influential parameters for the calibration bayesian inference based on an mcmc algorithm is successfully applied to characterize the posterior parameter distributions and the simulation uncertainty for the conceptual wq model the simulation uncertainty range due to the parameters cannot cover the observations and the effect of other sources of uncertainty should be considered to explain the total uncertainty although the model calibration the sensitivity analysis and the uncertainty analysis are computationally demanding their applications are feasible thanks to the fast simulation of the conceptual wq model future studies may address the following issues including more wq variables such as organic matter algae and sediment to the simulation as this can improve o2 simulation in summer and nh3 simulation including more responses e g no3 and nh3 to the uncertainty analysis explicitly accounting for the input and model structural errors in the bayesian calibration software data availability the pawn method is implemented in the safe matlab octave toolbox for gsa pianosi et al 2015 safe is freely available for non commercial purposes at www bristol ac uk cabot resources safe toolbox the matlab toolbox of dream is available upon request from the developer jasper uci edu the citowa tool is available upon request from the developer befekadu woldegiorgis usask ca conflict of interest and authorship conformation form all authors have participated in a conception and design or analysis and interpretation of the data b drafting the article or revising it critically for important intellectual content and c approval of the final version this manuscript has not been submitted to nor is under review at another journal or other publishing venue the authors have no affiliation with any organization with a direct or indirect financial interest in the subject matter discussed in the manuscript declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgment the authors would like to thank the flanders hydraulics research for supporting and coordinating the project of development of conceptual models for an integrated river basin management we also thank dr peter meert for his help to build up the conceptual wq model for the river dender belgium appendix a supplementary data the following is the supplementary data to this article multimedia component 1 multimedia component 1 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2022 105331 
25646,models help decision makers anticipate the consequences of policies for ecosystems and people for instance improving our ability to represent interactions between human activities and ecological systems is essential to identify pathways to meet the 2030 sustainable development goals however use of modeling outputs in decision making remains uncommon we share insights from a multidisciplinary national socio environmental synthesis center working group on technical communication and process related factors that facilitate or hamper uptake of model results we emphasize that it is not simply technical model improvements but active and iterative stakeholder involvement that can lead to more impactful outcomes in particular trust and relationship building with decision makers are key for knowledge based decision making in this respect nurturing knowledge exchange on the interpersonal e g through participatory processes and institutional level e g through science policy interfaces across scales represents a promising approach to this end we offer a generalized approach for linking modeling and decision making keywords biodiversity ecosystem function relationships co production ecological modeling policy relevance stakeholder engagement 1 introduction ecosystems depend on diverse biological attributes to ensure ecological functions and processes that are fundamental to life on earth improved ability to represent interactions between human activities and ecological systems is essential to identify pathways to meet the 2030 sustainable development goals kim et al 2018 therefore decision makers often want to anticipate the consequences of policies for ecosystems and human well being isbell et al 2017 models i e qualitative or quantitative descriptions of key components of a system and of relationships between those components are important tools to support such endeavors and can provide a useful way to examine possible outcomes ipbes 2016 however examples of modeling outputs that have directly influenced conservation practice and decision making are rare rapacciuolo 2019 while models are often created with the intention of informing decision makers a number of factors may hamper or facilitate the use of the models and research more broadly in decision making processes dilling and lemos 2011 ipbes 2016 wall et al 2017 here we focus on the uptake of biodiversity ecosystem function and ecosystem services models integrating biodiversity and ecosystem function bef models to improve projections of ecosystem services was one of the main challenges identified in the collective modeling effort that contributed to the ipbes global assessment rosa et al 2020 and the insights we share in this paper are drawn from an interdisciplinary sesync the national socio environmental synthesis center working group focused on this challenge to draw on our collective experience we informally interviewed working group members that have developed or work extensively with different ecological models about their model use experience working group members have experience with a variety of ecological models with varying levels of uptake in policy processes at different scales which provided a unique opportunity to compare factors hampering and facilitating model uptake models ranged from national fisheries management e g atlantis in australia fulton et al 2014 to regional water management and land use planning e g invest daily et al 2009 kareiva et al 2011 nelson et al 2009 see list of models in appendix 1 nearly half of the models discussed had not yet been used in a specific decision making process drawing on this expert opinion and reflection we present lessons learned for how to improve ecological model uptake for decision makers and go further to emphasize that it is not simply uptake but active and iterative involvement by stakeholders that can lead to more impactful outcomes ulibarri 2018 this is especially important as we move into the negotiation of the post 2020 global biodiversity framework for the convention on biological diversity cbd where reliance is being placed on the ability of models to identify challenges and to answer key questions especially in monitoring progress towards specific targets for example models are being used to explore possible pathways to reversing biodiversity loss by 2050 leclère et al 2020 here we present key considerations to ensure that models are better able to meet policy needs although many of the concepts we present are not new e g rose et al 2020 there is a persistent need to raise awareness of these issues and potential solutions among the modeling community addison et al 2013 rapacciuolo 2019 saltelli et al 2020 enhanced consideration of these issues by ecological modelers may improve the usefulness of modeling results in decision making allowing models to reach their full potential by providing decision makers with a stronger evidence basis for making decisions in often highly uncertain contexts 2 facilitating and hampering factors we categorized the factors facilitating and hampering the use of models into three main categories technical communication and process based i e related to the social context of decision making factors these mirror the characteristics of actionable information identified by cash et al 2003 credible salient and legitimate but are distinct in that they are specific factors related to modeling which may or may not lead to information uptake technical factors are often the focus of modeling work e g improving model accuracy precision and data processing techniques addressing data gaps from the technical perspective issues which can either facilitate or hamper model uptake span the entire modeling process including the thematic focus or scope of a model as well as model assumptions resolution and scale spatial and temporal model relevance is often more context dependent than discussed so it is important to assess whether the model is fit for purpose in each context where it is being used parker 2020 most models that had been used in a policy context had underlying data and or model results that matched the scale of the decision crucially the general model framing in terms of implicit or explicit value judgements by the modelers and model designers may underpin the additional technical barriers outlined below while most models did not incorporate the relationship between biodiversity and ecosystem function modelers deemed them to be important to include depending on the decision better incorporation of these relationships could improve model accuracy especially over longer time frames where it is important to capture all relevant processes in the model for example an australian ecosystem modeling exercise that ran more traditionally structured marine ecosystem models alongside models including species turnover found very different results under various potential levels of climate change fulton and gorton 2014 biodiversity productivity relationships have also been found to be important for both biodiversity conservation and climate mitigation mori et al 2021 in other cases the potential imbalance between model complexity and the perceived usefulness of output could act as a hampering technical barrier for instance incorporating bef relationships into invest models used for mapping water supply may not be useful because the policy driven question may not be directly impacted by this relationship and including it may make the model unnecessarily complicated a long history of work in fisheries shows that the complexity of models used in decision making must be compatible with the level of available information with more constrained models containing key relationships leading to more effective management decisions than more complicated models based on sparse information ludwig and walters 1985 however it is important to update models as new data and understanding of underlying relationships become available myers et al 2021 ultimately our interviews highlighted that the bef link is currently lacking in most models but would be especially important to better understand the consequences of climate change and other global changes for ecosystem services a key reason for this is because it is the change in biodiversity over time within a place that is expected to impact ecosystem services bef experiments assess this relationship which could otherwise be masked by only comparing ecosystem services between sites loreau 1998 tilman et al 2014 communication and accessibility related factors facilitating or hampering the uptake of models and their outputs included the accessibility of the model design e g how the model design and assumptions are communicated the accessibility of the modeling interface e g the steepness of the model interface learning curve as well as the accessibility of the modeling output e g how difficult the modeling products are to understand to be updated and to maintain accessibility in time models that lack flexibility or are difficult to use can discourage uptake by decision makers robinson and freebairn 2000 another key factor is transparency or clarity in addressing the uncertainties related to a model scientific uncertainty does not necessarily decrease trust van der bles et al 2020 but the way in which it is communicated is influential howe et al 2019 adding precise numbers to uncertain predictions can lead to a false sense of accuracy in the results saltelli et al 2020 on the decision maker side there is also a tendency to confuse uncertainty where the probability of a situation is unknown with risk where the confidence in probabilities is known this can lead to overconfidence in policy options and business as usual practices stirling 2019 all of these points make communication fraught and the situation is only exacerbated by the incompatibilities in the cultural and communication styles of scientists and decision makers cairney 2016 this is why knowledge brokers who can successfully interpret between the two groups are such a critical part of modern modeling teams cvitanovic et al 2015 while the majority of available studies on factors facilitating and hampering model uptake in decision making focus primarily on technical and communication factors this perspective illustrates the key role of process based factors these factors have been frequently underestimated or overlooked in different modeling communities however their importance has been increasingly recognized e g clark et al 2016 and warrants deeper consideration as they represent some of the key barriers to a larger scale model uptake in decision making the issues related to the social context of model uptake cover a broad range of process related aspects on the institutional level model uptake is determined by how decision making processes are embedded in institutional structures as well as by institutional arrangements bureaucracy and power distribution understanding how decisions are made and who is affected by decisions i e identifying stakeholders and understanding the decision context is a key part of problem framing in the decision making process and is important for modelers to understand as they define the modeling problem fig 1 runge 2020 on the level of individuals interpersonal relationships have proved influential in the process of knowledge transfer for example having established relationships with ngos was helpful for promoting the use of the invest model another role of the social setting lies in which and whose objectives define the objectives of the modeling process and which and whose values and priorities tend to be incorporated both on the level of individuals and social groups for example a multi species size spectrum model implemented in mizer was designed by a team that included scientists working in government agencies to address policy driven questions which facilitated its use in identifying large fish indicators for use in the north sea blanchard et al 2014 scott et al 2014 finally the skills and capacities of decision makers to use a particular model and its interface may present additional barriers both institutional and individual level factors can be exacerbated by the tendency to follow path dependencies and the lack of ability or willingness to change practices e g reliance on models already in use fulton 2021 process based factors can facilitate model uptake by building a sufficient level of understanding of a model and or its underlying concepts and mitigating the fear of excessive sophistication and complexity and thus nurturing trust in models and modeling however social context and process based factors can also underlie decision makers hesitation of modeling results e g fear of getting less funding in the light of modeling results or making potential management failures more apparent and mistrust of results that do not align with previously held beliefs pahl wostl 2009 conversely modelers highlighted that a robust participatory process with stakeholders identifying policy driven questions that underpinned model development and output and sufficient capacity to increase model usefulness facilitated model uptake identifying shared goals and designing a collaboration process up front can help address these issues hallett et al 2017 lawson et al 2017 iterative engagement with stakeholders during model development can lead to models that are viewed as more credible salient and legitimate by model users white et al 2010 while there are many benefits to participatory approaches modelers should also be aware that including decision makers in the process could lead to a perception of less objective science especially if only a small group of stakeholders are involved enquist et al 2017 2 1 lessons learnt for improving the process of model design and use based on our collective experience we reflected on what could have been done differently to improve uptake and identified three areas for consideration we discuss these lessons in light of literature that provides more detailed suggestions of ways to facilitate model uptake 1 identify and include decision makers from the beginning and during the entire process including policy and other decision makers throughout the research process i e the co production of knowledge enhances its legitimacy and relevance to decision making purposes meadow et al 2015 thus helping to reduce the impact of hampering process based factors in some cases active participation from stakeholders throughout the modeling process may not always be necessary there is a spectrum of stakeholder engagement approaches all of which can lead to actionable science bamzai dodson et al 2021 descriptions of and considerations for applying translational ecology approaches that include relevant decision makers and interdisciplinary perspectives have been described elsewhere e g enquist et al 2017 rose et al 2020 wall et al 2017 but applications in the modeling context remain uncommon rapacciuolo 2019 but see beier et al 2017 and miller et al 2017 for recommended practices for co producing science in general and models in particular respectively including users who understand the policy environment in the model design from the beginning means that the model inputs do not have to be retrofitted for a specific outcome later thereby saving time and effort for example setting up a participatory process with all interested parties from the beginning to get consensus on the technical aspects of the work data targets decision support systems helped facilitate the successful use of species distribution modeling for forest planning in new south wales australia ferrier et al 2002a 2002b finkel 1998 in instances where directly involving stakeholders in the design is not feasible e g when the model is too complicated another mechanism to bridge the model decision divide is through the use of a knowledge broker or boundary spanner chapman et al 2017 safford et al 2017 as a go between a knowledge broker will have sufficient knowledge of the model dynamics to be able to feed information and needs to from stakeholders fig 1 presents an example of how modeling and decision making processes can be linked although presented as a linear process engagement with stakeholders can be initiated at various parts of the modeling cycle depending on the decision making scale goals of the modeling exercise and overall understanding of the modeled processes for example when general understanding of our ability to model particular relationships e g bef relationships is low it may be appropriate to engage stakeholders after initial model development has been completed when preliminary outputs can be used to raise awareness of new or improved information and thus help reframe the decision making problem moreover decision making processes and relevant model outputs are different at the global scale which often focus on the evidence base for setting global targets versus regional and local scale where on the ground management decisions are made thus models or outputs that have been developed for one scale of decision making may need to be retrofitted for applications at other scales stakeholder mapping can be used to determine which decision making scale and stakeholder groups may be most interested in the model results 2 communication with decision makers communication with key decision makers whether they are local managers or government ministers is vital to any model uptake strategy first it is important to create a common understanding of language and culture between modelers and decision makers jackson et al 2017 morisette et al 2017 techniques like the persona method are useful to characterize user needs and design solutions cooper 1999 for models that are relatively easy to use seeking opportunities to introduce the model to decision makers investing in capacity building and letting them explore it for themselves is a good way to get a richer understanding of what the model can and cannot do and what it is best used for for more complex models it may be necessary to communicate higher level model structure in particular modular designs can facilitate model communication and customization regardless of model complexity communication should involve transparent descriptions of model limitations and uncertainty without belaboring the technical details it is important to be transparent about the assumptions and value judgements made by the modelers in model framing and highlighting how those decisions affect model output saltelli et al 2020 uncertainty can be communicated verbally rather than numerically scales such as the intergovernmental panel on climate change s seven point likelihood scale may be preferable for stakeholders rapacciuolo 2019 these facilitating factors are supported by ensuring that models or at least model outputs are open access 3 iterative learning and model evaluation continuously improving models as new data become available can lead to better decision outcomes myers et al 2021 in retrospect it is easy to see where research gaps could have been covered by the model if certain aspects had been included for example analyzing biodiversity impacts outside of protected areas to be able to appreciate the net outcome for biodiversity of interventions in the new south wales case ferrier et al 2002a 2002b finkel 1998 sometimes including different scenarios at an earlier stage in the model can be helpful for exploring a wider set of futures and extending the models to include multiple drivers of change can also be more relevant for decision makers incorporating diverse user values and objectives into modeled scenarios e g nature futures framework kim et al 2021 pereira et al 2020 and getting feedback on plausible alternative actions can make model outputs more relevant for users a critical aspect of the modeling process itself is also to evaluate the model the extra effort of evaluation is rewarded by increased transparency and credibility of the overall approach and sets up a learning process for later improvements dietze et al 2018 if decision makers use model output to select and implement a decision monitoring decision outcomes can provide useful data for model validation fig 1 6 conclusion models provide a useful way to assess how policy decisions may impact biodiversity and ecosystem services however no model can capture all aspects of a system as noted by george p box all models are wrong but some are useful the wrongness of any given model poses a big challenge to decision makers whose choices affect people s lives and environments acknowledging implications of the model being wrong is important can a robust finding be made even if the model is imperfect or could there be serious consequences of an incorrect prediction it is therefore important not just to communicate model uncertainties but to work actively with the intended users of the model outputs in unpacking assumptions and be open to reconfiguration so as to make sure that the model s wrongness does not stop it from being useful the recent ipbes global assessment highlighted that transformative changes are needed to halt biodiversity loss and sustain nature s contributions to people díaz et al 2019 as part of the transformative change it is important to move towards transdisciplinary work with the active participation of the different stakeholders clark et al 2016 grumbine and xu 2021 while our understanding of how biodiversity contributes to human well being has grown many models do not yet fully incorporate this relationship nevertheless focusing only on technical model improvements will not be enough to bring about transformative change for biodiversity while there is still an important role for traditional modeling approaches e g for improving scientific understanding investing in the process surrounding a modeling exercise and promoting two way interactions between modelers and stakeholders is vital for successfully contributing to decision making processes in particular trust building and establishing relationships with decision makers have proved key to increase buy in to modeling as a key element for knowledge based decision making in practice this means working closely with those involved in decision making on the ground and at community levels as well as at higher policy making levels in this respect nurturing knowledge exchange on the interpersonal level e g through participatory processes and on the institutional level e g through promoting science policy interfaces across scales represents a promising approach declaration of competing interest the authors declare the following financial interests personal relationships which may be considered as potential competing interests sarah weiskopf reports financial support was provided by national socio environmental synthesis center acknowledgements this work was supported by the national socio environmental synthesis center sesync under funding received from the national science foundation dbi 1639145 a portion of this research was supported by the u s geological survey national and north central climate adaptation science centers any use of trade firm or product names is for descriptive purposes only and does not imply endorsement by the u s government we thank a cravens for her helpful review comments appendix a supplementary data the following is the supplementary data to this article multimedia component 1 multimedia component 1 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2022 105318 
25646,models help decision makers anticipate the consequences of policies for ecosystems and people for instance improving our ability to represent interactions between human activities and ecological systems is essential to identify pathways to meet the 2030 sustainable development goals however use of modeling outputs in decision making remains uncommon we share insights from a multidisciplinary national socio environmental synthesis center working group on technical communication and process related factors that facilitate or hamper uptake of model results we emphasize that it is not simply technical model improvements but active and iterative stakeholder involvement that can lead to more impactful outcomes in particular trust and relationship building with decision makers are key for knowledge based decision making in this respect nurturing knowledge exchange on the interpersonal e g through participatory processes and institutional level e g through science policy interfaces across scales represents a promising approach to this end we offer a generalized approach for linking modeling and decision making keywords biodiversity ecosystem function relationships co production ecological modeling policy relevance stakeholder engagement 1 introduction ecosystems depend on diverse biological attributes to ensure ecological functions and processes that are fundamental to life on earth improved ability to represent interactions between human activities and ecological systems is essential to identify pathways to meet the 2030 sustainable development goals kim et al 2018 therefore decision makers often want to anticipate the consequences of policies for ecosystems and human well being isbell et al 2017 models i e qualitative or quantitative descriptions of key components of a system and of relationships between those components are important tools to support such endeavors and can provide a useful way to examine possible outcomes ipbes 2016 however examples of modeling outputs that have directly influenced conservation practice and decision making are rare rapacciuolo 2019 while models are often created with the intention of informing decision makers a number of factors may hamper or facilitate the use of the models and research more broadly in decision making processes dilling and lemos 2011 ipbes 2016 wall et al 2017 here we focus on the uptake of biodiversity ecosystem function and ecosystem services models integrating biodiversity and ecosystem function bef models to improve projections of ecosystem services was one of the main challenges identified in the collective modeling effort that contributed to the ipbes global assessment rosa et al 2020 and the insights we share in this paper are drawn from an interdisciplinary sesync the national socio environmental synthesis center working group focused on this challenge to draw on our collective experience we informally interviewed working group members that have developed or work extensively with different ecological models about their model use experience working group members have experience with a variety of ecological models with varying levels of uptake in policy processes at different scales which provided a unique opportunity to compare factors hampering and facilitating model uptake models ranged from national fisheries management e g atlantis in australia fulton et al 2014 to regional water management and land use planning e g invest daily et al 2009 kareiva et al 2011 nelson et al 2009 see list of models in appendix 1 nearly half of the models discussed had not yet been used in a specific decision making process drawing on this expert opinion and reflection we present lessons learned for how to improve ecological model uptake for decision makers and go further to emphasize that it is not simply uptake but active and iterative involvement by stakeholders that can lead to more impactful outcomes ulibarri 2018 this is especially important as we move into the negotiation of the post 2020 global biodiversity framework for the convention on biological diversity cbd where reliance is being placed on the ability of models to identify challenges and to answer key questions especially in monitoring progress towards specific targets for example models are being used to explore possible pathways to reversing biodiversity loss by 2050 leclère et al 2020 here we present key considerations to ensure that models are better able to meet policy needs although many of the concepts we present are not new e g rose et al 2020 there is a persistent need to raise awareness of these issues and potential solutions among the modeling community addison et al 2013 rapacciuolo 2019 saltelli et al 2020 enhanced consideration of these issues by ecological modelers may improve the usefulness of modeling results in decision making allowing models to reach their full potential by providing decision makers with a stronger evidence basis for making decisions in often highly uncertain contexts 2 facilitating and hampering factors we categorized the factors facilitating and hampering the use of models into three main categories technical communication and process based i e related to the social context of decision making factors these mirror the characteristics of actionable information identified by cash et al 2003 credible salient and legitimate but are distinct in that they are specific factors related to modeling which may or may not lead to information uptake technical factors are often the focus of modeling work e g improving model accuracy precision and data processing techniques addressing data gaps from the technical perspective issues which can either facilitate or hamper model uptake span the entire modeling process including the thematic focus or scope of a model as well as model assumptions resolution and scale spatial and temporal model relevance is often more context dependent than discussed so it is important to assess whether the model is fit for purpose in each context where it is being used parker 2020 most models that had been used in a policy context had underlying data and or model results that matched the scale of the decision crucially the general model framing in terms of implicit or explicit value judgements by the modelers and model designers may underpin the additional technical barriers outlined below while most models did not incorporate the relationship between biodiversity and ecosystem function modelers deemed them to be important to include depending on the decision better incorporation of these relationships could improve model accuracy especially over longer time frames where it is important to capture all relevant processes in the model for example an australian ecosystem modeling exercise that ran more traditionally structured marine ecosystem models alongside models including species turnover found very different results under various potential levels of climate change fulton and gorton 2014 biodiversity productivity relationships have also been found to be important for both biodiversity conservation and climate mitigation mori et al 2021 in other cases the potential imbalance between model complexity and the perceived usefulness of output could act as a hampering technical barrier for instance incorporating bef relationships into invest models used for mapping water supply may not be useful because the policy driven question may not be directly impacted by this relationship and including it may make the model unnecessarily complicated a long history of work in fisheries shows that the complexity of models used in decision making must be compatible with the level of available information with more constrained models containing key relationships leading to more effective management decisions than more complicated models based on sparse information ludwig and walters 1985 however it is important to update models as new data and understanding of underlying relationships become available myers et al 2021 ultimately our interviews highlighted that the bef link is currently lacking in most models but would be especially important to better understand the consequences of climate change and other global changes for ecosystem services a key reason for this is because it is the change in biodiversity over time within a place that is expected to impact ecosystem services bef experiments assess this relationship which could otherwise be masked by only comparing ecosystem services between sites loreau 1998 tilman et al 2014 communication and accessibility related factors facilitating or hampering the uptake of models and their outputs included the accessibility of the model design e g how the model design and assumptions are communicated the accessibility of the modeling interface e g the steepness of the model interface learning curve as well as the accessibility of the modeling output e g how difficult the modeling products are to understand to be updated and to maintain accessibility in time models that lack flexibility or are difficult to use can discourage uptake by decision makers robinson and freebairn 2000 another key factor is transparency or clarity in addressing the uncertainties related to a model scientific uncertainty does not necessarily decrease trust van der bles et al 2020 but the way in which it is communicated is influential howe et al 2019 adding precise numbers to uncertain predictions can lead to a false sense of accuracy in the results saltelli et al 2020 on the decision maker side there is also a tendency to confuse uncertainty where the probability of a situation is unknown with risk where the confidence in probabilities is known this can lead to overconfidence in policy options and business as usual practices stirling 2019 all of these points make communication fraught and the situation is only exacerbated by the incompatibilities in the cultural and communication styles of scientists and decision makers cairney 2016 this is why knowledge brokers who can successfully interpret between the two groups are such a critical part of modern modeling teams cvitanovic et al 2015 while the majority of available studies on factors facilitating and hampering model uptake in decision making focus primarily on technical and communication factors this perspective illustrates the key role of process based factors these factors have been frequently underestimated or overlooked in different modeling communities however their importance has been increasingly recognized e g clark et al 2016 and warrants deeper consideration as they represent some of the key barriers to a larger scale model uptake in decision making the issues related to the social context of model uptake cover a broad range of process related aspects on the institutional level model uptake is determined by how decision making processes are embedded in institutional structures as well as by institutional arrangements bureaucracy and power distribution understanding how decisions are made and who is affected by decisions i e identifying stakeholders and understanding the decision context is a key part of problem framing in the decision making process and is important for modelers to understand as they define the modeling problem fig 1 runge 2020 on the level of individuals interpersonal relationships have proved influential in the process of knowledge transfer for example having established relationships with ngos was helpful for promoting the use of the invest model another role of the social setting lies in which and whose objectives define the objectives of the modeling process and which and whose values and priorities tend to be incorporated both on the level of individuals and social groups for example a multi species size spectrum model implemented in mizer was designed by a team that included scientists working in government agencies to address policy driven questions which facilitated its use in identifying large fish indicators for use in the north sea blanchard et al 2014 scott et al 2014 finally the skills and capacities of decision makers to use a particular model and its interface may present additional barriers both institutional and individual level factors can be exacerbated by the tendency to follow path dependencies and the lack of ability or willingness to change practices e g reliance on models already in use fulton 2021 process based factors can facilitate model uptake by building a sufficient level of understanding of a model and or its underlying concepts and mitigating the fear of excessive sophistication and complexity and thus nurturing trust in models and modeling however social context and process based factors can also underlie decision makers hesitation of modeling results e g fear of getting less funding in the light of modeling results or making potential management failures more apparent and mistrust of results that do not align with previously held beliefs pahl wostl 2009 conversely modelers highlighted that a robust participatory process with stakeholders identifying policy driven questions that underpinned model development and output and sufficient capacity to increase model usefulness facilitated model uptake identifying shared goals and designing a collaboration process up front can help address these issues hallett et al 2017 lawson et al 2017 iterative engagement with stakeholders during model development can lead to models that are viewed as more credible salient and legitimate by model users white et al 2010 while there are many benefits to participatory approaches modelers should also be aware that including decision makers in the process could lead to a perception of less objective science especially if only a small group of stakeholders are involved enquist et al 2017 2 1 lessons learnt for improving the process of model design and use based on our collective experience we reflected on what could have been done differently to improve uptake and identified three areas for consideration we discuss these lessons in light of literature that provides more detailed suggestions of ways to facilitate model uptake 1 identify and include decision makers from the beginning and during the entire process including policy and other decision makers throughout the research process i e the co production of knowledge enhances its legitimacy and relevance to decision making purposes meadow et al 2015 thus helping to reduce the impact of hampering process based factors in some cases active participation from stakeholders throughout the modeling process may not always be necessary there is a spectrum of stakeholder engagement approaches all of which can lead to actionable science bamzai dodson et al 2021 descriptions of and considerations for applying translational ecology approaches that include relevant decision makers and interdisciplinary perspectives have been described elsewhere e g enquist et al 2017 rose et al 2020 wall et al 2017 but applications in the modeling context remain uncommon rapacciuolo 2019 but see beier et al 2017 and miller et al 2017 for recommended practices for co producing science in general and models in particular respectively including users who understand the policy environment in the model design from the beginning means that the model inputs do not have to be retrofitted for a specific outcome later thereby saving time and effort for example setting up a participatory process with all interested parties from the beginning to get consensus on the technical aspects of the work data targets decision support systems helped facilitate the successful use of species distribution modeling for forest planning in new south wales australia ferrier et al 2002a 2002b finkel 1998 in instances where directly involving stakeholders in the design is not feasible e g when the model is too complicated another mechanism to bridge the model decision divide is through the use of a knowledge broker or boundary spanner chapman et al 2017 safford et al 2017 as a go between a knowledge broker will have sufficient knowledge of the model dynamics to be able to feed information and needs to from stakeholders fig 1 presents an example of how modeling and decision making processes can be linked although presented as a linear process engagement with stakeholders can be initiated at various parts of the modeling cycle depending on the decision making scale goals of the modeling exercise and overall understanding of the modeled processes for example when general understanding of our ability to model particular relationships e g bef relationships is low it may be appropriate to engage stakeholders after initial model development has been completed when preliminary outputs can be used to raise awareness of new or improved information and thus help reframe the decision making problem moreover decision making processes and relevant model outputs are different at the global scale which often focus on the evidence base for setting global targets versus regional and local scale where on the ground management decisions are made thus models or outputs that have been developed for one scale of decision making may need to be retrofitted for applications at other scales stakeholder mapping can be used to determine which decision making scale and stakeholder groups may be most interested in the model results 2 communication with decision makers communication with key decision makers whether they are local managers or government ministers is vital to any model uptake strategy first it is important to create a common understanding of language and culture between modelers and decision makers jackson et al 2017 morisette et al 2017 techniques like the persona method are useful to characterize user needs and design solutions cooper 1999 for models that are relatively easy to use seeking opportunities to introduce the model to decision makers investing in capacity building and letting them explore it for themselves is a good way to get a richer understanding of what the model can and cannot do and what it is best used for for more complex models it may be necessary to communicate higher level model structure in particular modular designs can facilitate model communication and customization regardless of model complexity communication should involve transparent descriptions of model limitations and uncertainty without belaboring the technical details it is important to be transparent about the assumptions and value judgements made by the modelers in model framing and highlighting how those decisions affect model output saltelli et al 2020 uncertainty can be communicated verbally rather than numerically scales such as the intergovernmental panel on climate change s seven point likelihood scale may be preferable for stakeholders rapacciuolo 2019 these facilitating factors are supported by ensuring that models or at least model outputs are open access 3 iterative learning and model evaluation continuously improving models as new data become available can lead to better decision outcomes myers et al 2021 in retrospect it is easy to see where research gaps could have been covered by the model if certain aspects had been included for example analyzing biodiversity impacts outside of protected areas to be able to appreciate the net outcome for biodiversity of interventions in the new south wales case ferrier et al 2002a 2002b finkel 1998 sometimes including different scenarios at an earlier stage in the model can be helpful for exploring a wider set of futures and extending the models to include multiple drivers of change can also be more relevant for decision makers incorporating diverse user values and objectives into modeled scenarios e g nature futures framework kim et al 2021 pereira et al 2020 and getting feedback on plausible alternative actions can make model outputs more relevant for users a critical aspect of the modeling process itself is also to evaluate the model the extra effort of evaluation is rewarded by increased transparency and credibility of the overall approach and sets up a learning process for later improvements dietze et al 2018 if decision makers use model output to select and implement a decision monitoring decision outcomes can provide useful data for model validation fig 1 6 conclusion models provide a useful way to assess how policy decisions may impact biodiversity and ecosystem services however no model can capture all aspects of a system as noted by george p box all models are wrong but some are useful the wrongness of any given model poses a big challenge to decision makers whose choices affect people s lives and environments acknowledging implications of the model being wrong is important can a robust finding be made even if the model is imperfect or could there be serious consequences of an incorrect prediction it is therefore important not just to communicate model uncertainties but to work actively with the intended users of the model outputs in unpacking assumptions and be open to reconfiguration so as to make sure that the model s wrongness does not stop it from being useful the recent ipbes global assessment highlighted that transformative changes are needed to halt biodiversity loss and sustain nature s contributions to people díaz et al 2019 as part of the transformative change it is important to move towards transdisciplinary work with the active participation of the different stakeholders clark et al 2016 grumbine and xu 2021 while our understanding of how biodiversity contributes to human well being has grown many models do not yet fully incorporate this relationship nevertheless focusing only on technical model improvements will not be enough to bring about transformative change for biodiversity while there is still an important role for traditional modeling approaches e g for improving scientific understanding investing in the process surrounding a modeling exercise and promoting two way interactions between modelers and stakeholders is vital for successfully contributing to decision making processes in particular trust building and establishing relationships with decision makers have proved key to increase buy in to modeling as a key element for knowledge based decision making in practice this means working closely with those involved in decision making on the ground and at community levels as well as at higher policy making levels in this respect nurturing knowledge exchange on the interpersonal level e g through participatory processes and on the institutional level e g through promoting science policy interfaces across scales represents a promising approach declaration of competing interest the authors declare the following financial interests personal relationships which may be considered as potential competing interests sarah weiskopf reports financial support was provided by national socio environmental synthesis center acknowledgements this work was supported by the national socio environmental synthesis center sesync under funding received from the national science foundation dbi 1639145 a portion of this research was supported by the u s geological survey national and north central climate adaptation science centers any use of trade firm or product names is for descriptive purposes only and does not imply endorsement by the u s government we thank a cravens for her helpful review comments appendix a supplementary data the following is the supplementary data to this article multimedia component 1 multimedia component 1 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2022 105318 
25647,generating fine resolution weather data over the himalayan region is difficult due to complicated topography and harsh weather in this study we set up a multiple point statistics mps based statistical model to spatially extrapolate fine resolution rainfall and temperature data in five spatial domains and at ten point locations of glaciers over the northwest himalayas the training domain consists of thirteen years 2001 2013 of daily rainfall and temperature data at 30 and 10 km spatial resolution which generates multiple realizations of fine resolution information for the year 2014 in the larger domain small range of rmse 2 46 6 68 mm and mae 1 12 4 15 mm for rainfall suggests that mps performs well in spatial domains furthermore at the glacier sites we observe that the training data of smaller training domains are insufficient while data of larger domains perform well overall mps shows encouraging results in extrapolating fine resolution weather information over the complex region of northwest himalayas keywords himalayas multiple point statistics rainfall extrapolation glaciers direct sampling statistical downscaling 1 introduction the himalayan region is home to hundreds of glaciers which serve as the origin of major asian rivers such as the indus ganges and brahmaputra thus influencing various economic activities singhal and jha 2021a the glaciers are significantly influenced by the rainfall from the south west monsoon season since the peak melting of these glaciers coincides with the monsoon season thayyen et al 2005 moreover the presence of complex topography and atmospheric circulation patterns leads to the unique distribution of rainfall and temperature over the himalayas bookhagen and burbank 2010 even small variations in rainfall and temperature influence the glacio hydrology of the glaciers immerzeel et al 2010 previous studies suggest that the changing weather over the himalayas has caused a majority of these glaciers to retreat mukhopadhyay and khan 2014 wood et al 2020 in the himalayan catchments limited availability of fine resolution is a major obstacle the complex topography and harsh climate lead to an insufficient network of rain gauges in the region in setting up a weather research and forecasting wrf model the topographical complexity poses a major challenge in the case of existing wrf data in a smaller domain a novel cost effective alternative could be to spatially extrapolate the information to a larger domain having similar characteristics spatial extrapolation is a method to estimate data in a larger region by extending the pattern of known information of a related smaller region the concept of spatial extrapolation has been investigated in multiple areas of research such as soil mapping stockmann et al 2015 air pollution concentration ma et al 2019 wind speed xu et al 2018 local weather information lo et al 2011 bracho mujica et al 2019 temperature strauss et al 2013 etc however to the best of our knowledge limited studies have focused on the extrapolation of rainfall data for this purpose we set up a statistical downscaling model which can extend the coverage of fine resolution rainfall data to a larger domain to this end the model uses the coarse and fine resolution data of a smaller domain as training data and extrapolates the downscaled information to a larger domain the concept behind such extrapolation is that if detailed information about a known domain is available it may be possible to extrapolate the information to a relatively larger unknown domain in the context of rainfall the application of such extrapolation technique may be significant for the generation of fine resolution data in a feasible manner especially over complicated regions such as the himalayas where the availability of data is generally limited in the case of modelling spatially correlated variables geostatistics is a natural tool to apply multiple point statistics mps is a widely known geostatistical technique which has the potential to offer an approach for spatial extrapolation conceptually mps is a stochastic simulation technique which reproduces the patterns from a 2d or a 3d training image ti and fills a simulation grid sg when the patterns of sg match with those of ti in the past decade it has found several applications in various areas of research such as in geophysics comunian et al 2013 remote sensing mariethoz et al 2012 yin et al 2016 rasera et al 2020 climate sciences jha et al 2013a hydrological sciences jha et al 2015 benoit et al 2020 river morphology jha et al 2013b etc several studies have applied mps for spatial downscaling of weather data for instance jha et al 2015 used twenty years of precipitation and temperature data to successfully downscale them from 50 km to 10 km spatial resolution in southeastern australia recently singhal and jha 2021b successfully applied mps to downscale rainfall and temperature data from 30 km to 10 km spatial resolution over the highly complex northwest himalayas using 12 years of training data the authors also examined the efficacy of the model to generate fine resolution data in ten administrative districts of the region and at various point locations of extreme precipitation and avalanches more details about mps can be found in guardiano and srivastava 1993 and mariethoz et al 2010 the concept of mps has been successfully applied in downscaling of a wide range of variables however limited studies have examined the efficacy of mps for spatial extrapolation of data for instance malone et al 2016 applied mps to extrapolate fine resolution soil information at recipient site on the basis of information collected from a nearby donor site having similar characteristics similarly oriani et al 2020 extended the coverage of fine resolution multispectral satellite images to a region where similar fine resolution data was found using the concept of mps however to the best of our knowledge no study has utilized the concept of mps for spatial extrapolation of rainfall data by overcoming its high spatiotemporal variability various algorithms have been developed using the concept of mps however in this study we select the direct sampling ds algorithm developed by mariethoz et al 2010 the potential of ds algorithm has been examined in various research fields for the simulation of spatial data oriani et al 2016 dembélé et al 2019 zuo et al 2019 2020 bai and mariethoz 2021 hosseini et al 2021 the advantage of using ds is that it is able to handle both univariate and multivariate simulations of categorical and continuous variables straubhaar and renard 2021 moreover it does not require the storage of any database of spatial patterns prior to the simulations since it extracts the samples directly from the ti van der grijp et al 2021 the tis represent conceptual models that contain the spatial features of the variable to be estimated the idea here is to utilize a ti with the spatial information from a smaller domain in order to learn the spatial features of a larger domain without actually training the model for the larger region in this study we set up a statistical model based on our previous work presented in singhal and jha 2021b over the northwest himalayas the present study is an extension of the previous work and offers a broader understanding regarding the cost effective generation of fine resolution weather information over the region in singhal and jha 2021b the location and size of training and simulation domain were the same hence the model was used primarily for downscaling the coarse resolution weather data in this work however we train the model by providing training data only in a smaller domain training domain and extend the downscaled fine resolution data of rainfall and temperature to a larger domain simulation domain in other words the training domain contains both coarse and fine resolution data while the simulation domain contains only the coarse resolution information which is downscaled to obtain the corresponding fine resolution information in the larger domain the extrapolation approach allows researchers to study glaciers located in the simulation domain where wrf is not set up by borrowing information from the training domain where the wrf is actually set up here each grid in the training domain acts as the donor site while the point locations of glaciers in the simulation domain act as the recipient sites to this end we choose five spatial domains in this study with each succeeding domain larger than the previous one the ds algorithm if recognizes similar spatial patterns uses the coarse resolution information of the larger domain to generate fine resolution data by sampling the ti multiple realizations of data are generated for each larger domain which is subsequently verified with the available reference dataset it is important to note here that the algorithm does not require fine resolution information of variables for the larger domain to simulate the data the specific objectives of this study are a to set up a statistical model over the northwest himalayas which utilizes coarse and fine resolution information of rainfall and temperature in smaller domains in a common time period and estimates the fine resolution information in larger domains in another time period b to determine the appropriate size of the simulation domain to which the extrapolation of data is possible c to determine the smallest size of the training domain which can provide sufficient information for the extrapolation of data d to examine the performance of the model in generating extrapolated fine resolution weather information at ten selected glacier sites the rest of the paper is organized as follows section 2 describes the study area and dataset the methodology and experimental runs are explained in section 3 in section 4 results from the experimental runs are presented followed by the discussion of the results in section 5 section 6 deals with conclusions regarding the efficacy of the ds simulations in extrapolating the downscaled fine resolution weather information 2 study area and dataset the study area is located in the northwest himalayas including parts of the tibetan plateau spanning from 27on to 33o n latitude and 75o e to 81o e longitude fig 1 a shows the coarse resolution wrf domain covering the south central asian domain the fine resolution wrf domain is shown in fig 1b in which the five square boxes represent the spatial domains chosen for this study fig 1c depicts the expanded view of the five spatial boxes including the ten glacier sites involved in the study the largest domain has the dimensions of 61 61 grids having an equal spacing of 0 1o 10 km it is further divided into four equal domains with each domain having a different size geography and weather attributes see details in table 1 the domains are known to be highly sensitive to changes in climate and are prone to frequent natural hazards such as avalanches extreme rainfall flash flood glacial outburst etc moreover a number of glaciers are located in the region which serves as the origin of important rivers thus feeding millions of people in south asia the har dataset generated by dynamical downscaling of the wrf product is used in this study the data is available in both coarse 30 km and fine 10 km spatial resolution from october 2000 to september 2014 the coarse resolution domain has the size of 200 200 grid points fig 1a while the fine resolution domain has the size 270 180 grid points fig 1b the dataset is publicly available at http www klima tu berlin de har more details about the dataset are provided in maussion et al 2014 in this study thirteen years 2001 2013 of daily rainfall and temperature data at 30 and 10 km spatial resolution is used to construct the training image conditioning data cd is constructed using the 30 km resolution data for the year 2014 assuming that 10 km resolution data is not available for this year the 10 km data for the year 2014 is estimated using the model 3 methods in this section we describe the working of the ds algorithm followed by the details of the experimental design 3 1 the direct sampling algorithm the ds algorithm is one of the many algorithms developed under the broader concept of mps like any other mps algorithm the primary aim of the ds algorithm is to fill unknown information of a simulation grid by searching similar spatial patterns from a training image in the present study the simulation grid already consists of coarse resolution information called conditioning data which assists in the identification of similar patterns once similar patterns are identified in the ti the value from the ti is transferred to the unknown pixel of the simulation grid for detailed information about the working of the ds algorithm we refer to mariethoz et al 2010 straubhaar 2017 documentation http www randlab org here our objective is to apply and evaluate the performance of the ds algorithm in extrapolating the downscaled information of rainfall and temperature hence we only present the important points a schematic illustration of ds algorithm is presented in fig 2 the setup of ti and cd for this study is represented in fig 2a which shows a smaller domain containing ti training domain and a larger domain containing cd simulation domain please note that we use smaller training data to simulate results for a larger region in this study fig 2b represents the pixelated structure of ti and cd where each pixel is already informed of values a simulation grid is shown in fig 2c whose unknown pixel represented by x is required to be simulated and filled the 3 3 grid arrangement in the middle of sg represents a data event n x which comprises of the unknown pixel along with the conditioning data colored pixels as represented in fig 2d the aim of the simulation is to estimate the value of x by searching the pattern similar to n x in the ti under a certain distance threshold t to this end a simulation is run where the patterns of n x are matched with the patterns of four random tis fig 2e in fig 2e we see that the patterns of n x do not match with the patterns of ti 1 ti 2 and ti 3 the simulation continues matching the patterns until they match with each other within the distance threshold the patterns of n x match with the patterns of ti 4 upon successful matching the pixel value of y from ti 4 is transferred to the location of unknown pixel x in the simulation grid as depicted in fig 2f the cycle of simulation is repeated until each unknown pixel of sg is filled with values in the case of multivariate ti a combined distance function dist is used which is a linear combination of individual distances between the pixels of n x and n y the purpose of using a combined distance function is to find the matching neighborhood in the ti within a certain distance as prescribed by dist various distance functions such as manhattan euclidean hamming etc are used in a multivariate setup in this study we select the manhattan distance function to compute the distance threshold mariethoz et al 2012 jha et al 2013a singhal and jha 2021b consequently the ds algorithm is applied to generate fine resolution information of rainfall p10 and temperature t10 in a larger domain by using the corresponding training data ti of a smaller domain the following workflow provides more information about the working of the ds algorithm 1 select an unknown pixel x in the sg whose data is to simulated 2 assign the data event n x in the sg which consists of known neighbors of x cd within a radius corresponding to ¼ of the size of ti 3 search for similar patterns as n x in the tis and identify the corresponding data event n y 4 compute the distance dist nx n y and if the distance is smaller than the prescribed threshold t transfer the value of y to x otherwise repeat steps 3 and 4 5 repeat the steps until all unknown pixels are filled with values 3 2 arrangement of training images conditioning data and covariates a total of 14 years 2001 2014 of weather data is used in this study out of which the data of the first 13 years 2001 2013 is used to construct the ti cd is constructed using the data of the fourteenth year 2014 which is also the target year in this study ti is composed of both coarse p30 and t30 and fine resolution p10 and t10 information while cd contains only the coarse resolution information such a breakdown of ti and cd can be generalized to any simulation involving the ds algorithm such that suppose m total years of data 14 years in our case then ti m 1 13 years in our case and cd mth year year 2014 target year similar arrangement of ti and cd was used in singhal and jha 2021b however in the present study significant changes are necessary the most important change is regarding the size of ti and cd in the previous work the size of the training domain containing ti and the simulation domain containing cd were similar thus meaning that the amount of fine resolution data estimated for the target year 61 61 grids was the same to the amount used as input data in the form of ti 61 61 grids in this study however the size of the training domain is kept smaller e g 13 13 grids while the size of the simulation domain is larger e g 61 61 grids this implies that in this study we use a smaller amount of fine resolution data to estimate the corresponding fine resolution data in a larger domain the coarse resolution information is present in both smaller and larger domains to assist in the accurate reproduction of patterns furthermore covariates are also used in this study which is auxiliary information pertaining to the local environment of the study area they are included in both ti and cd to support better resampling of patterns especially rainfall in total 11 covariates are used in this study relating to weather vegetation topography and location subsequently rainfall and temperature data at 10 km spatial resolution are estimated in larger domains for the target year 2014 assuming that no such information is available for that year 3 3 ds parameters assigning proper values to parameters of the ds algorithm is critical to regulate the simulation accuracy and to obtain an efficient run time the important parameters of ds are maximum fraction of ti scanned f distance threshold t weights of variable w and maximum number of neighbouring pixels n generally the values of f t and w range between 0 and 1 while n is one fourth of the size of the ti in this study the input values of these parameters are similar to those in singhal and jha 2021b the parameter f is set to 0 5 t 0 01 and n 14 moreover weather variables p30 p10 t30 t10 are assigned a weight of 1 topographical elevation slope and aspect and vegetative variables leaf area index and vegetative fraction are given the weight of 0 1 while locational variables latitude and longitude are given 0 01 as the weight before finalizing a sensitivity analysis was performed to assess the relative influence of the different combinations of weights 1 0 1 and 0 01 over the simulation of variables results from the sensitivity analysis suggested no significant change in the quality of simulations 3 4 experimental design in this study we perform ten experimental runs to extrapolate the rainfall and temperature data in five spatial domains we denote the domains as dn where n 1 2 5 the smallest domain is denoted by d1 while the largest domain by d5 table 2 summarizes the details of each experimental run in each run thirteen years of ti years 2001 2013 and one year of cd year 2014 is provided as the input data to the model the algorithm uses the input data of each domain to extrapolate the fine resolution information of the year 2014 to a larger domain moreover in each run we also focus on estimating the extrapolated fine resolution data of rainfall and temperature at selected point locations of glaciers the results of extrapolation are examined using spatial maps and error statistics to this end three error statistics are applied such as root mean square error rmse mean square error mae and percentage bias pbias as shown in equations 1 3 1 rmse 1 n i 1 n obs i sim i 2 2 m a e 1 n i 1 n o b s i s i m i 3 p b i a s i 1 n s i m i o b s i 100 i 1 n o b s i in run 1 the input data of the smallest domain d1 training domain is provided to the ds algorithm which it simulates to estimate the fine resolution information of the largest domain d5 similarly in runs 2 4 the input data of training domains d2 d3 and d4 is utilized respectively to simulate the data for d5 simulation domain the motive behind keeping the simulation domain d5 constant and varying the size of the training domains is to evaluate the extent to which ds can satisfactorily estimate data in larger domains using the information from a smaller domain moreover we also focus on assessing the influence of local weather and elevation over the performance of extrapolation and hence we vary the size of the simulation domain in subsequent runs for instance in runs 5 7 d1 d2 and d3 respectively provide the input data for estimating the fine resolution data in the simulation domain d4 similarly in runs 8 9 d1 and d2 provide the input data to estimate the data for the next larger domain d3 while in run 10 input data from d1 is utilized to estimate the data for the simulation domain d2 of the total ten selected glaciers two glaciers belong to d3 five to d4 and three to d5 since there is no glacier present in domain d2 we didn t perform any point simulation for glacier sites in run 10 see table 2 it is important to note that in all the experimental runs the training which provides the input data and simulation domains are different generally the weather patterns of nearby domains are expected to be similar however estimating reliable information in larger domains by using the input data of a smaller domain is challenging especially in mountainous terrains of the northwest himalayas 4 results in this section we present the results from the various experimental runs undertaken in the study please note that although ten experimental runs are performed in the study we present the results for only the first four runs the first four runs satisfy the primary objective of evaluating the performance of ds in spatially extrapolating fine resolution weather data using different sizes of training domains the other runs are only important to understand the influence of different input sizes of data geographical attributes weather patterns etc over the quality of simulations in runs 1 4 the largest domain d5 is selected as the simulation domain which remains constant while the training information is provided by domains of different sizes domains 1 4 the focus in this section is to examine the performance of the ds algorithm in estimating the fine resolution information of rainfall and temperature among the five selected domains one training and four simulation domain in the northwest himalayas thirteen years of coarse and fine resolution monsoon rainfall and temperature data is used as input data to simulate daily data for the target year 2014 moreover we select ten glacier sites to assess the suitability of the ds algorithm in extrapolating weather data at point locations since weather data involves a considerable amount of uncertainty we also generate an ensemble of 25 realizations in all the runs 4 1 simulation results here we evaluate the efficacy of the ds algorithm to simulate similar patterns of rainfall and temperature in comparison to the reference in each of the runs 1 4 information of smaller training domains d1 d4 is used to simulate daily fine resolution data of rainfall and temperature for the largest simulation domain d5 for more details about the training and simulation domains see table 2 4 1 1 rainfall the simulated results of rainfall from experimental runs 1 4 are shown in fig 3 the first row in fig 3 a1 a4 represents the reference rainfall data of d5 having the size 61 61 grids the simulations from run 1 are represented by fig 3 b1 b4 where the input information from the domain of size 13 13 is used to simulate the results for the domain of size 61 61 the objective of these spatial maps is to examine the similarity between the spatial patterns of reference and the simulations from the figure it is observed that the ds algorithm is able to recognize the spatial patterns of rainfall for d5 however in comparison to the reference the patterns appear to be homogenized to a larger region in all months such homogenization could be attributed to the fact that the input information is provided in the smallest domain in run 1 while simulated results are estimated in the largest domain please note that although the results are simulated daily they are presented at a monthly timescale after summing up the daily simulations for each month furthermore the simulations from run 2 fig 3 c1 c4 are less homogenized as compared to those from run 1 however the input information is not sufficient to accurately reproduce the locations of heavier rainfall yellow patches in fig 3 in run 3 fig 3 d1 d4 size of the input information is increased domain 2 and consequently the algorithm is able to reproduce the spatial patterns well the locations depicting heavier rainfall are also better represented although not satisfactorily in run 4 the spatial patterns from simulations closely resemble those of the reference also regions of heavier rainfall are reproduced with greater accuracy run 4 provides the largest input information to the algorithm since d4 size 49 49 is used to simulate for the domain d5 size 61 61 overall we find that the runs 3 4 are reasonably able to reproduce low and moderate rainfall events observed in june and september however heavy rainfall events of july and august are better reproduced when the ds algorithm receives a larger amount of input data as in the case of run 4 the simulated rainfall results from experimental runs 5 7 and 8 10 are presented in figs s1 and s2 of the supplementary material respectively 4 1 2 temperature for temperature the simulated results for experimental runs 1 4 are shown in fig 4 the figure shows distinct spatial patterns for the low and high temperature low temperatures are observed in the higher elevated regions of the himalayas blue color while high temperatures are observed in the low lying regions orange color furthermore closer visual inspection of temperature patterns shows a diagonal yellow colored patch line located from northwest to the east direction this patch line represents a transition zone between low and high temperature results show that the simulated spatial patterns of temperature largely agree with those of the reference fig 4a1 a4 except in run 1 fig 4b1 b4 simulations in run 1 clearly indicate that the small amount of training information provided to the algorithm is insufficient to reproduce the patterns of low temperatures moreover in run 2 fig 4c1 c4 the spatial patterns of low temperature are reproduced however the patterns appear to be homogenized as compared to the reference in run 3 fig 4d1 d4 the homogenization of patterns reduces and the patterns appear similar to those in the reference simulations in run 4 provides the best results as the patterns of high and low temperature closely resemble fig 4e1 e4 to those in the reference in all four months we also observe that the transition zone is well preserved in the simulations especially in runs 3 and 4 which suggests that ds is reasonable in reproducing fine resolution weather patterns over complex elevational gradients such as in the northwest himalayas the simulated results for temperature in experimental runs 5 7 and 8 10 are presented in figs s3 and s4 of the supplementary material respectively 4 2 results from error analysis here the model simulations of p10 and t10 are compared to the reference p10 and t10 for the year 2014 to evaluate the accuracy of simulated results in each experimental run using three error statistics rmse mae and pbias rmse and mae inform about the average magnitude of difference between the reference and simulated data moreover pbias indicates the nature of simulations being either positive or negative simulated results are considered overestimated compared to the reference if the pbias is positive and underestimated if the pbias is negative 4 2 1 seasonal error analysis of rainfall simulations we show the spread and frequency of mean seasonal error values of rainfall calculated as rmse mae and pbias for the experimental runs 1 4 in fig 5 please note that here the simulations are done daily however the results are shown at a seasonal timescale also the error values are calculated for each realization and averaged thereafter from fig 5 a1 a4 we observe that the rmse values of rainfall do not show high variations among the experimental runs however the values decrease as the size of the training domain increases for instance in run 1 the rmse values are on the higher side both in terms of spread and frequency the scatter plot fig 5 a1a shows that rmse for the majority of the values is greater than 3 mm the histogram fig 5 a1b confirms the same showing that the frequency of values lies between 3 and 8 mm the values show a decreasing trend in runs 2 and 3 in run 4 the distribution of the majority of rmse values is in the range of 0 6 mm a similar trend is observed in the case of mae fig 5 b1 b4 where the values are higher in runs 1 and 2 while they are comparatively lower for runs 3 and 4 one obvious reason behind such a trend is the smaller amount of training data provided to the algorithm in the case of runs 1 and 2 which is increased in subsequent runs in the case of pbias fig 5 c1 c4 we observe considerable fluctuations in the values both within and among the experimental runs results show that the values are significantly high in run 1 which falls down considerably in run 5 particularly in runs 1 and 2 the range of pbias values is large which decreases in runs 3 and 4 in run 3 only a small number of grids show high pbias which is also suggested by the corresponding histogram furthermore the values in run 4 are relatively scattered within a relatively small range leading to a more accurate simulation of results in run 4 majority of the grids have negative pbias some are closer to zero while few are positive overall seasonal error results for rainfall show that the size of input domain has a direct relation with the accuracy of simulations with runs 3 and 4 performing reasonably well the mean seasonal error values of rainfall calculated as rmse mae and pbias for the experimental runs 5 7 and 8 10 are shown in figs s5 and s6 of the supplementary material respectively 4 2 2 seasonal error analysis of temperature simulations in the case of temperature the spread and frequency of rmse mae and pbias for experimental runs 1 4 are shown in fig 6 a1 a4 b1 b4 and c1 c4 respectively please note that the results are presented here at a seasonal timescale although the simulations are done daily the scatter plots show that the rmse values are highest in run 1 which significantly decreases in runs 2 4 in runs 2 and 3 the majority of the values are less than 10 mm while in run 4 the values drop to less than 6 mm the histograms indicate that most of the values are close to zero mm in all experimental run particularly in runs 3 and 4 thus indicating accurate results furthermore the spread and distribution of mae values largely follow the patterns of rmse with the highest values obtained in run 1 and the least values in run 5 in the case of pbias run 1 shows a larger spread of error values however the range of spread is not large 0 8 the spread becomes smaller with successive runs and reaches its lowest in run 5 where the majority of the values are close to zero the histograms corresponding to each run also indicate that pbias values are relatively high in run 1 and 2 while they are close to zero in runs 3 and 4 overall in the case of temperature run 3 and especially 4 provides the most accurate results the mean seasonal error values of temperature calculated as rmse mae and pbias for the experimental runs 5 7 and 8 10 are shown in figs s7 and s8 of the supplementary material respectively 4 2 3 monthly error analysis of rainfall and temperature the himalayan region shows high variation in the distribution of rainfall among the months of the monsoon season i e june july august and september it may be possible that different months show different magnitude of error depending upon the size of the training data provided to the model the mean monthly error values of rainfall and temperature obtained from each experimental run are presented in table 3 in the case of rainfall results indicate that the values of rmse and mae are quite low for each month unlike in the results of seasonal error values the monthly error values do not show a clear cut decreasing trend when we move from smaller to larger training domain for instance the rmse 2 46 mm and mae 1 12 mm values are lowest in run 3 for the month of june even though the largest training data is provided in run 4 in july rmse is lowest in run 2 5 26 mm while mae is lowest in run 3 2 85 mm moreover in both august and september the rmse and mae values are again found to be lowest in run 3 however it must be noted that except for run 1 there is a very slight difference in the error values among the experimental runs of the respective months for instance the difference between the highest and lowest rmse values among the experimental runs 2 4 for june is only 0 08 mm 2 54 2 46 0 08 mm for july is 0 26 mm 5 52 5 26 0 26 mm for august is 0 2 mm 5 49 5 29 0 2 mm and for september is 0 33 mm 4 25 3 92 0 33 mm error values in a similar range have been estimated by other studies over the region shi 2020 arshad et al 2021 jiang et al 2021 one of the reasons for obtaining such low rmse and mae values could be the averaging of individual days to one single value for a month however the values are low for all months and in all experimental runs thus suggesting that majority of the individual days in a month may also have low error values such small differences in error values indicate that the experimental runs except run 1 possess a reasonable ability to simulate rainfall values for individual months furthermore we observe higher rmse and mae values for july and august in comparison to june and september this can be attributed to the fact that july and august are considered the peak monsoon months hrudya et al 2021 and few extreme rainfall events have been reported to occur over the region in the two months houze et al 2017 in the case of pbias high positive values in run 1 in all four months indicates overestimation by simulations the values decrease considerably in subsequent runs as the training domain gets larger in run 2 pbias shows small negative values for june and july denoting underestimation and positive values for august and september similar trend is visible in run 3 with the exception that the value for september becomes negative furthermore in run 4 pbias values for all months are obtained as slightly negative indicating underestimation of values in the case of temperature we notice a clear cut decreasing trend in rmse and mae values with the increase in the size of the input domain in all four months see table 3 results show that the values in all the months are highest in run 1 while they are lowest in run 5 similar to the results in the case of rainfall the difference between the values in runs 2 4 is very small suggesting reasonable ability of the model to simulate temperature for larger domains furthermore unlike in the case of rainfall where we notice higher error values in july and august and lower values in june and september the error values are relatively similar for temperature across all months in each of the four experimental runs this is primarily because temperature generally does not show high variability in space and time and largely remains uniform within a season moreover the obtained error values are in line with other studies over the region srinivasan et al 2010 singh et al 2015 the monthly error values of rainfall and temperature for the experimental runs 5 10 are presented in table s1 of the supplementary material 4 3 qualitative performance of ds at glacier sites here we evaluate the qualitative performance of ds in estimating fine resolution information of rainfall and temperature at ten selected important glaciers sites hereafter gsites located in the domains d3 d4 and d5 as shown in fig 7 the reference data of daily rainfall the ensemble of 25 realizations and the mean of realizations is plotted in each subplot of fig 7 specifically gsite 1 2 lie in d3 gsite 3 to gsite 7 lie in d4 and gsite 8 to gsite 10 lie in d5 4 3 1 glaciers in domain 3 as mentioned earlier two glaciers gsite 1 and gsite 2 are located in d3 of the study area in the case of rainfall results from fig 7 show that ds is reasonably accurate in estimating rainfall time series at the glacier sites the mean of ensembles is observed to coincide with the reference dataset at most sites with large training domains moreover we observe that the quality of simulations does not deteriorate even when input data from smaller training domains are used at gsite 1 and gsite 2 we notice that both input domains d1 and d2 perform reasonably well in capturing the rainfall time series at gsite 2 however the simulations are not able to capture the peak of extreme rainfall previous study suggest that ds has limitations in reproducing the peaks of extreme rainfall over the region singhal and jha 2021b moreover in the case of temperature we find that the domains do influence the simulations in temperature unlike in the case of rainfall where the size of the training domain did not influence the quality of simulations much the simulated temperature at the gsites is closer to the reference when the largest training domain is used and begin to deviate as the domains become smaller at gsite 1 the gap between the reference and simulated datasets indicates that the input information from d1 is inadequate in simulating temperature of domain 3 d3 we notice that the simulations almost coincide with the reference when a larger training domain d2 is used similar inferences are observed for gsite 2 where the simulated temperature using a larger domain training domain of d2 performs better than that using a smaller training domain d1 4 3 2 glaciers in domain 4 five glaciers gsite 3 to gsite 7 are located in the next domain d4 results of rainfall at the gsite 3 show that the input domain d2 provides the best simulations although other domains d1 and d3 also perform well see fig 7 the simulations overestimate rainfall at gsite 4 for all input domains at the gsites 5 7 ds is able to successfully capture both high and low rainfall peaks using each of the input domains d1 d3 results show slight overestimation at gsite 7 however the simulated ensembles are able to capture the variations in the peak rainfall in the case of temperature we observe a similar trend in results where the largest training domain provides the best results as expected at all the gsites d3 is found to perform better than the other two smaller domains see fig 7 4 3 3 glaciers in domain 5 three glaciers gsite 8 to gsite 10 are located in d5 in the case of rainfall all the glacier sites show overestimation when smaller training domains are used and begin to coincide with the reference as the larger training domains are used however at the gsite 8 and gsite 9 we observe that the simulated rainfall using the training domain of d3 gives the best results despite being the second largest training domain see fig 7 at the gsite 10 the largest domain d4 gives the best result in the case of temperature results show that the largest training domain d4 estimates the most accurate results at all sites specifically at gsite 9 we notice that d3 also shows reasonable simulation accuracy while at gsite 10 both d2 shows good accuracy along with d4 4 4 quantitative performance of ds at glacier sites here we evaluate the performance of ds in simulating the rainfall and temperature time series using the error statistics of rmse and pbias the mean rmse and pbias values for rainfall and temperature calculated for all the gsites are shown in table 4 the errors are estimated on the basis of the deviation of the mean of ensembles from the reference data in the case of rainfall results show that the rmse values are consistently low at all gsites for all training domains the lowest rmse value is obtained for gsite 4 0 55 mm when the information from the training domain d2 is used to conduct the simulations for d4 while the highest value is obtained at gsite 2 when domain d2 is used to simulate for d3 as observed earlier the rmse values at the gsites are obtained on the higher side when the smallest training domain is used apart from that no other relationship is observed between the error values and domain size furthermore the pbias values show a high degree of fluctuations both within and among the glacier sites for instance at gsite 6 the training domain d1 shows pbias as 137 30 while d3 shows only 9 03 similarly at gsite 10 pbias is high for d1 129 78 decreases for d2 and d3 and again rises sharply to 162 95 for d4 the values are mostly positive indicating overestimation of results overall results show that ds is successfully able to simulate rainfall at point locations of glaciers using input domains of various sizes especially d3 and d4 in the case of temperature higher values of rmse for training domain d1 at each gsite clearly suggests that the input information is not enough for simulations rmse for all other training domains is comparatively very low showing a decreasing trend with the increase in size of the domain the range of rmse is largest for d1 among all the gsites 20 69 13 93 k decreases in d2 4 91 1 06 k and d3 1 02 2 73 k and becomes smallest in d4 1 02 1 27 k the pbias values follow a similar trend as in rmse where the smallest training domain estimates the largest range of pbias 7 62 5 0 k and the largest domain d4 estimates the smallest range 0 01 to 0 31 k overall the pbias values do not show high fluctuations as observed in rainfall with the majority of the values for higher training domains being closer to zero thus indicating good performance by ds in simulating fine resolution temperature data at point locations of glaciers 5 discussion we setup an mps based statistical downscaling model over the northwest himalayas to estimate fine resolution daily weather information in a larger domain simulation domain by utilizing the corresponding training information of a smaller domain training domain a total of ten experimental runs are performed among five spatial domains of varying sizes apart from the spatial domains fine resolution rainfall and temperature information is also simulated at ten point locations of glaciers results from the experimental runs show that ds is reasonably accurate in simulating the information of rainfall and temperature the input information of larger training domains d3 and d4 performs relatively better than those of the smaller domains d1 and d2 moreover results from the seasonal error analysis show that the values of rmse mae and pbias decrease as the size of the training domain increases for both rainfall and temperature one of the reasons for the decreasing trend of error values could be that the smaller training domains lack sufficient information to reproduce the fine resolution weather patterns while it is present in a sufficient amount in larger domains the results from the monthly error analyses do not show any such trend for rainfall as in some experimental runs smaller training domains d1 and d2 are observed to perform better than the larger domains d3 and d4 however in the case of temperature the error values begin to decrease with the increase in the domain size for each of the four monsoon months the absence of any trend in the monthly errors of rainfall could be attributed to the fact that rainfall shows significant variability even among the months of the monsoon season over the himalayas while the temperature is largely consistent and shows variations only within a smaller range the simulated results at the point locations of glaciers suggest that ds is able to reproduce the fine resolution time series of rainfall and temperature in the case of rainfall we find that simulations performed using the smaller sized training domains d1 and d2 are able to coincide with the reference however the input information from larger domains d3 and d4 performs reasonably well at the sites where high rainfall peaks are observed this implies that the information in the larger domain may be necessary for simulating extreme rainfall which themselves are rare events moreover the rmse values are observed to be consistently low at most glacier sites while pbias values are on the higher side for smaller domains while on the lower side for larger domains in the case of temperature results show that the quality of simulations is directly proportional to the size of the training domain simulations using larger domains perform well in comparison to those using the smaller domains values of rmse and pbias also suggests the same since the largest error values are obtained from the smallest domain d1 and vice versa similar to the experimental runs among the spatial domains the simulation at each glacier site is performed using the training domains other than the one in which the glaciers are located for instance if the glacier is located in d5 the simulations are performed using the input information from d1 d4 only overall ds shows great potential in estimating fine resolution weather information in a larger domain using training information of a smaller domain both among the spatial domains and at important glacier sites located over the northwest himalayas 6 conclusions in this work we set up an mps based statistical model over the northwest himalayas to extrapolate the fine resolution weather information of a larger domain simulation domain by using training information of a comparatively smaller domain training domain the training domain consists of rainfall and temperature data at both 10 and 30 km spatial resolution for the time period of 2001 2013 the simulation domain contains the information only at 30 km spatial resolution for the year 2014 the model searches for spatial patterns in the training domain similar to those in the simulation domain and generates the corresponding 10 km extrapolated information for the year 2014 moreover an ensemble of realizations is also generated by performing multiple stochastic simulations which helps in reducing the uncertainty involved with weather data the performance of ds to generate extrapolated weather information is evaluated both spatially and at point locations of glaciers the spatial evaluation is performed under ten experimental runs involving five domains of different sizes and characteristics results show that ds is able to replicate the spatial patterns of rainfall and temperature for the larger region specifically the simulations involving larger training domains d3 and d4 perform better than those involving smaller training domains d1 and d2 small range of rmse 2 46 6 68 mm and mae 1 12 4 15 mm for rainfall among all the experimental runs suggest that ds is reasonable in extrapolating downscaled information over complicated terrains of the northwest himalayas furthermore the daily simulated results of rainfall and temperature at ten point locations of glaciers are found to be comparable to the reference at most sites at the glacier sites smaller training domains are able to reproduce low and moderate peaks of rainfall however higher peaks require information from larger domains in the case of temperature smaller domains do not show satisfactory simulation ability while larger domains such as d3 and d4 mean rmse 1 68 k and 1 11 k respectively show good results compared to the reference moreover the pbias values are slightly positive with most of them being closer to zero indicating reasonable accuracy in summary ds is also capable of estimating fine resolution weather information at point locations of glaciers located over complex terrains such as of northwest himalayas thus creating greater avenues to study their behaviour under climate change the ds algorithm can be further applied to a wide range of hydrological research future studies will focus on using alternate fine resolution rainfall information to overcome the limited availability of satellite rainfall products moreover studies can be undertaken to find ways to reduce the dependency on a large amount of ti to generate fine resolution weather information declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements the ds code was developed by the randlab team at the university of neuchâtel switzerland http www randlab org we obtained the ds tool from dr gregoire mariethoz institute of earth surface dynamics university of lausanne for carrying out this work the present research is supported by the scheme for transformational and advanced research in sciences moe stars grant number stars apr2019 ds 391 fs awarded to sanjeev kumar jha we are also thankful to the department of science and technology government of india for providing financial support to mr akshay singhal for his doctoral work under the dst inspire scheme dst inspire 03 2019 001343 if 190257 appendix a supplementary data the following is the supplementary data to this article multimedia component 1 multimedia component 1 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2022 105317 
25647,generating fine resolution weather data over the himalayan region is difficult due to complicated topography and harsh weather in this study we set up a multiple point statistics mps based statistical model to spatially extrapolate fine resolution rainfall and temperature data in five spatial domains and at ten point locations of glaciers over the northwest himalayas the training domain consists of thirteen years 2001 2013 of daily rainfall and temperature data at 30 and 10 km spatial resolution which generates multiple realizations of fine resolution information for the year 2014 in the larger domain small range of rmse 2 46 6 68 mm and mae 1 12 4 15 mm for rainfall suggests that mps performs well in spatial domains furthermore at the glacier sites we observe that the training data of smaller training domains are insufficient while data of larger domains perform well overall mps shows encouraging results in extrapolating fine resolution weather information over the complex region of northwest himalayas keywords himalayas multiple point statistics rainfall extrapolation glaciers direct sampling statistical downscaling 1 introduction the himalayan region is home to hundreds of glaciers which serve as the origin of major asian rivers such as the indus ganges and brahmaputra thus influencing various economic activities singhal and jha 2021a the glaciers are significantly influenced by the rainfall from the south west monsoon season since the peak melting of these glaciers coincides with the monsoon season thayyen et al 2005 moreover the presence of complex topography and atmospheric circulation patterns leads to the unique distribution of rainfall and temperature over the himalayas bookhagen and burbank 2010 even small variations in rainfall and temperature influence the glacio hydrology of the glaciers immerzeel et al 2010 previous studies suggest that the changing weather over the himalayas has caused a majority of these glaciers to retreat mukhopadhyay and khan 2014 wood et al 2020 in the himalayan catchments limited availability of fine resolution is a major obstacle the complex topography and harsh climate lead to an insufficient network of rain gauges in the region in setting up a weather research and forecasting wrf model the topographical complexity poses a major challenge in the case of existing wrf data in a smaller domain a novel cost effective alternative could be to spatially extrapolate the information to a larger domain having similar characteristics spatial extrapolation is a method to estimate data in a larger region by extending the pattern of known information of a related smaller region the concept of spatial extrapolation has been investigated in multiple areas of research such as soil mapping stockmann et al 2015 air pollution concentration ma et al 2019 wind speed xu et al 2018 local weather information lo et al 2011 bracho mujica et al 2019 temperature strauss et al 2013 etc however to the best of our knowledge limited studies have focused on the extrapolation of rainfall data for this purpose we set up a statistical downscaling model which can extend the coverage of fine resolution rainfall data to a larger domain to this end the model uses the coarse and fine resolution data of a smaller domain as training data and extrapolates the downscaled information to a larger domain the concept behind such extrapolation is that if detailed information about a known domain is available it may be possible to extrapolate the information to a relatively larger unknown domain in the context of rainfall the application of such extrapolation technique may be significant for the generation of fine resolution data in a feasible manner especially over complicated regions such as the himalayas where the availability of data is generally limited in the case of modelling spatially correlated variables geostatistics is a natural tool to apply multiple point statistics mps is a widely known geostatistical technique which has the potential to offer an approach for spatial extrapolation conceptually mps is a stochastic simulation technique which reproduces the patterns from a 2d or a 3d training image ti and fills a simulation grid sg when the patterns of sg match with those of ti in the past decade it has found several applications in various areas of research such as in geophysics comunian et al 2013 remote sensing mariethoz et al 2012 yin et al 2016 rasera et al 2020 climate sciences jha et al 2013a hydrological sciences jha et al 2015 benoit et al 2020 river morphology jha et al 2013b etc several studies have applied mps for spatial downscaling of weather data for instance jha et al 2015 used twenty years of precipitation and temperature data to successfully downscale them from 50 km to 10 km spatial resolution in southeastern australia recently singhal and jha 2021b successfully applied mps to downscale rainfall and temperature data from 30 km to 10 km spatial resolution over the highly complex northwest himalayas using 12 years of training data the authors also examined the efficacy of the model to generate fine resolution data in ten administrative districts of the region and at various point locations of extreme precipitation and avalanches more details about mps can be found in guardiano and srivastava 1993 and mariethoz et al 2010 the concept of mps has been successfully applied in downscaling of a wide range of variables however limited studies have examined the efficacy of mps for spatial extrapolation of data for instance malone et al 2016 applied mps to extrapolate fine resolution soil information at recipient site on the basis of information collected from a nearby donor site having similar characteristics similarly oriani et al 2020 extended the coverage of fine resolution multispectral satellite images to a region where similar fine resolution data was found using the concept of mps however to the best of our knowledge no study has utilized the concept of mps for spatial extrapolation of rainfall data by overcoming its high spatiotemporal variability various algorithms have been developed using the concept of mps however in this study we select the direct sampling ds algorithm developed by mariethoz et al 2010 the potential of ds algorithm has been examined in various research fields for the simulation of spatial data oriani et al 2016 dembélé et al 2019 zuo et al 2019 2020 bai and mariethoz 2021 hosseini et al 2021 the advantage of using ds is that it is able to handle both univariate and multivariate simulations of categorical and continuous variables straubhaar and renard 2021 moreover it does not require the storage of any database of spatial patterns prior to the simulations since it extracts the samples directly from the ti van der grijp et al 2021 the tis represent conceptual models that contain the spatial features of the variable to be estimated the idea here is to utilize a ti with the spatial information from a smaller domain in order to learn the spatial features of a larger domain without actually training the model for the larger region in this study we set up a statistical model based on our previous work presented in singhal and jha 2021b over the northwest himalayas the present study is an extension of the previous work and offers a broader understanding regarding the cost effective generation of fine resolution weather information over the region in singhal and jha 2021b the location and size of training and simulation domain were the same hence the model was used primarily for downscaling the coarse resolution weather data in this work however we train the model by providing training data only in a smaller domain training domain and extend the downscaled fine resolution data of rainfall and temperature to a larger domain simulation domain in other words the training domain contains both coarse and fine resolution data while the simulation domain contains only the coarse resolution information which is downscaled to obtain the corresponding fine resolution information in the larger domain the extrapolation approach allows researchers to study glaciers located in the simulation domain where wrf is not set up by borrowing information from the training domain where the wrf is actually set up here each grid in the training domain acts as the donor site while the point locations of glaciers in the simulation domain act as the recipient sites to this end we choose five spatial domains in this study with each succeeding domain larger than the previous one the ds algorithm if recognizes similar spatial patterns uses the coarse resolution information of the larger domain to generate fine resolution data by sampling the ti multiple realizations of data are generated for each larger domain which is subsequently verified with the available reference dataset it is important to note here that the algorithm does not require fine resolution information of variables for the larger domain to simulate the data the specific objectives of this study are a to set up a statistical model over the northwest himalayas which utilizes coarse and fine resolution information of rainfall and temperature in smaller domains in a common time period and estimates the fine resolution information in larger domains in another time period b to determine the appropriate size of the simulation domain to which the extrapolation of data is possible c to determine the smallest size of the training domain which can provide sufficient information for the extrapolation of data d to examine the performance of the model in generating extrapolated fine resolution weather information at ten selected glacier sites the rest of the paper is organized as follows section 2 describes the study area and dataset the methodology and experimental runs are explained in section 3 in section 4 results from the experimental runs are presented followed by the discussion of the results in section 5 section 6 deals with conclusions regarding the efficacy of the ds simulations in extrapolating the downscaled fine resolution weather information 2 study area and dataset the study area is located in the northwest himalayas including parts of the tibetan plateau spanning from 27on to 33o n latitude and 75o e to 81o e longitude fig 1 a shows the coarse resolution wrf domain covering the south central asian domain the fine resolution wrf domain is shown in fig 1b in which the five square boxes represent the spatial domains chosen for this study fig 1c depicts the expanded view of the five spatial boxes including the ten glacier sites involved in the study the largest domain has the dimensions of 61 61 grids having an equal spacing of 0 1o 10 km it is further divided into four equal domains with each domain having a different size geography and weather attributes see details in table 1 the domains are known to be highly sensitive to changes in climate and are prone to frequent natural hazards such as avalanches extreme rainfall flash flood glacial outburst etc moreover a number of glaciers are located in the region which serves as the origin of important rivers thus feeding millions of people in south asia the har dataset generated by dynamical downscaling of the wrf product is used in this study the data is available in both coarse 30 km and fine 10 km spatial resolution from october 2000 to september 2014 the coarse resolution domain has the size of 200 200 grid points fig 1a while the fine resolution domain has the size 270 180 grid points fig 1b the dataset is publicly available at http www klima tu berlin de har more details about the dataset are provided in maussion et al 2014 in this study thirteen years 2001 2013 of daily rainfall and temperature data at 30 and 10 km spatial resolution is used to construct the training image conditioning data cd is constructed using the 30 km resolution data for the year 2014 assuming that 10 km resolution data is not available for this year the 10 km data for the year 2014 is estimated using the model 3 methods in this section we describe the working of the ds algorithm followed by the details of the experimental design 3 1 the direct sampling algorithm the ds algorithm is one of the many algorithms developed under the broader concept of mps like any other mps algorithm the primary aim of the ds algorithm is to fill unknown information of a simulation grid by searching similar spatial patterns from a training image in the present study the simulation grid already consists of coarse resolution information called conditioning data which assists in the identification of similar patterns once similar patterns are identified in the ti the value from the ti is transferred to the unknown pixel of the simulation grid for detailed information about the working of the ds algorithm we refer to mariethoz et al 2010 straubhaar 2017 documentation http www randlab org here our objective is to apply and evaluate the performance of the ds algorithm in extrapolating the downscaled information of rainfall and temperature hence we only present the important points a schematic illustration of ds algorithm is presented in fig 2 the setup of ti and cd for this study is represented in fig 2a which shows a smaller domain containing ti training domain and a larger domain containing cd simulation domain please note that we use smaller training data to simulate results for a larger region in this study fig 2b represents the pixelated structure of ti and cd where each pixel is already informed of values a simulation grid is shown in fig 2c whose unknown pixel represented by x is required to be simulated and filled the 3 3 grid arrangement in the middle of sg represents a data event n x which comprises of the unknown pixel along with the conditioning data colored pixels as represented in fig 2d the aim of the simulation is to estimate the value of x by searching the pattern similar to n x in the ti under a certain distance threshold t to this end a simulation is run where the patterns of n x are matched with the patterns of four random tis fig 2e in fig 2e we see that the patterns of n x do not match with the patterns of ti 1 ti 2 and ti 3 the simulation continues matching the patterns until they match with each other within the distance threshold the patterns of n x match with the patterns of ti 4 upon successful matching the pixel value of y from ti 4 is transferred to the location of unknown pixel x in the simulation grid as depicted in fig 2f the cycle of simulation is repeated until each unknown pixel of sg is filled with values in the case of multivariate ti a combined distance function dist is used which is a linear combination of individual distances between the pixels of n x and n y the purpose of using a combined distance function is to find the matching neighborhood in the ti within a certain distance as prescribed by dist various distance functions such as manhattan euclidean hamming etc are used in a multivariate setup in this study we select the manhattan distance function to compute the distance threshold mariethoz et al 2012 jha et al 2013a singhal and jha 2021b consequently the ds algorithm is applied to generate fine resolution information of rainfall p10 and temperature t10 in a larger domain by using the corresponding training data ti of a smaller domain the following workflow provides more information about the working of the ds algorithm 1 select an unknown pixel x in the sg whose data is to simulated 2 assign the data event n x in the sg which consists of known neighbors of x cd within a radius corresponding to ¼ of the size of ti 3 search for similar patterns as n x in the tis and identify the corresponding data event n y 4 compute the distance dist nx n y and if the distance is smaller than the prescribed threshold t transfer the value of y to x otherwise repeat steps 3 and 4 5 repeat the steps until all unknown pixels are filled with values 3 2 arrangement of training images conditioning data and covariates a total of 14 years 2001 2014 of weather data is used in this study out of which the data of the first 13 years 2001 2013 is used to construct the ti cd is constructed using the data of the fourteenth year 2014 which is also the target year in this study ti is composed of both coarse p30 and t30 and fine resolution p10 and t10 information while cd contains only the coarse resolution information such a breakdown of ti and cd can be generalized to any simulation involving the ds algorithm such that suppose m total years of data 14 years in our case then ti m 1 13 years in our case and cd mth year year 2014 target year similar arrangement of ti and cd was used in singhal and jha 2021b however in the present study significant changes are necessary the most important change is regarding the size of ti and cd in the previous work the size of the training domain containing ti and the simulation domain containing cd were similar thus meaning that the amount of fine resolution data estimated for the target year 61 61 grids was the same to the amount used as input data in the form of ti 61 61 grids in this study however the size of the training domain is kept smaller e g 13 13 grids while the size of the simulation domain is larger e g 61 61 grids this implies that in this study we use a smaller amount of fine resolution data to estimate the corresponding fine resolution data in a larger domain the coarse resolution information is present in both smaller and larger domains to assist in the accurate reproduction of patterns furthermore covariates are also used in this study which is auxiliary information pertaining to the local environment of the study area they are included in both ti and cd to support better resampling of patterns especially rainfall in total 11 covariates are used in this study relating to weather vegetation topography and location subsequently rainfall and temperature data at 10 km spatial resolution are estimated in larger domains for the target year 2014 assuming that no such information is available for that year 3 3 ds parameters assigning proper values to parameters of the ds algorithm is critical to regulate the simulation accuracy and to obtain an efficient run time the important parameters of ds are maximum fraction of ti scanned f distance threshold t weights of variable w and maximum number of neighbouring pixels n generally the values of f t and w range between 0 and 1 while n is one fourth of the size of the ti in this study the input values of these parameters are similar to those in singhal and jha 2021b the parameter f is set to 0 5 t 0 01 and n 14 moreover weather variables p30 p10 t30 t10 are assigned a weight of 1 topographical elevation slope and aspect and vegetative variables leaf area index and vegetative fraction are given the weight of 0 1 while locational variables latitude and longitude are given 0 01 as the weight before finalizing a sensitivity analysis was performed to assess the relative influence of the different combinations of weights 1 0 1 and 0 01 over the simulation of variables results from the sensitivity analysis suggested no significant change in the quality of simulations 3 4 experimental design in this study we perform ten experimental runs to extrapolate the rainfall and temperature data in five spatial domains we denote the domains as dn where n 1 2 5 the smallest domain is denoted by d1 while the largest domain by d5 table 2 summarizes the details of each experimental run in each run thirteen years of ti years 2001 2013 and one year of cd year 2014 is provided as the input data to the model the algorithm uses the input data of each domain to extrapolate the fine resolution information of the year 2014 to a larger domain moreover in each run we also focus on estimating the extrapolated fine resolution data of rainfall and temperature at selected point locations of glaciers the results of extrapolation are examined using spatial maps and error statistics to this end three error statistics are applied such as root mean square error rmse mean square error mae and percentage bias pbias as shown in equations 1 3 1 rmse 1 n i 1 n obs i sim i 2 2 m a e 1 n i 1 n o b s i s i m i 3 p b i a s i 1 n s i m i o b s i 100 i 1 n o b s i in run 1 the input data of the smallest domain d1 training domain is provided to the ds algorithm which it simulates to estimate the fine resolution information of the largest domain d5 similarly in runs 2 4 the input data of training domains d2 d3 and d4 is utilized respectively to simulate the data for d5 simulation domain the motive behind keeping the simulation domain d5 constant and varying the size of the training domains is to evaluate the extent to which ds can satisfactorily estimate data in larger domains using the information from a smaller domain moreover we also focus on assessing the influence of local weather and elevation over the performance of extrapolation and hence we vary the size of the simulation domain in subsequent runs for instance in runs 5 7 d1 d2 and d3 respectively provide the input data for estimating the fine resolution data in the simulation domain d4 similarly in runs 8 9 d1 and d2 provide the input data to estimate the data for the next larger domain d3 while in run 10 input data from d1 is utilized to estimate the data for the simulation domain d2 of the total ten selected glaciers two glaciers belong to d3 five to d4 and three to d5 since there is no glacier present in domain d2 we didn t perform any point simulation for glacier sites in run 10 see table 2 it is important to note that in all the experimental runs the training which provides the input data and simulation domains are different generally the weather patterns of nearby domains are expected to be similar however estimating reliable information in larger domains by using the input data of a smaller domain is challenging especially in mountainous terrains of the northwest himalayas 4 results in this section we present the results from the various experimental runs undertaken in the study please note that although ten experimental runs are performed in the study we present the results for only the first four runs the first four runs satisfy the primary objective of evaluating the performance of ds in spatially extrapolating fine resolution weather data using different sizes of training domains the other runs are only important to understand the influence of different input sizes of data geographical attributes weather patterns etc over the quality of simulations in runs 1 4 the largest domain d5 is selected as the simulation domain which remains constant while the training information is provided by domains of different sizes domains 1 4 the focus in this section is to examine the performance of the ds algorithm in estimating the fine resolution information of rainfall and temperature among the five selected domains one training and four simulation domain in the northwest himalayas thirteen years of coarse and fine resolution monsoon rainfall and temperature data is used as input data to simulate daily data for the target year 2014 moreover we select ten glacier sites to assess the suitability of the ds algorithm in extrapolating weather data at point locations since weather data involves a considerable amount of uncertainty we also generate an ensemble of 25 realizations in all the runs 4 1 simulation results here we evaluate the efficacy of the ds algorithm to simulate similar patterns of rainfall and temperature in comparison to the reference in each of the runs 1 4 information of smaller training domains d1 d4 is used to simulate daily fine resolution data of rainfall and temperature for the largest simulation domain d5 for more details about the training and simulation domains see table 2 4 1 1 rainfall the simulated results of rainfall from experimental runs 1 4 are shown in fig 3 the first row in fig 3 a1 a4 represents the reference rainfall data of d5 having the size 61 61 grids the simulations from run 1 are represented by fig 3 b1 b4 where the input information from the domain of size 13 13 is used to simulate the results for the domain of size 61 61 the objective of these spatial maps is to examine the similarity between the spatial patterns of reference and the simulations from the figure it is observed that the ds algorithm is able to recognize the spatial patterns of rainfall for d5 however in comparison to the reference the patterns appear to be homogenized to a larger region in all months such homogenization could be attributed to the fact that the input information is provided in the smallest domain in run 1 while simulated results are estimated in the largest domain please note that although the results are simulated daily they are presented at a monthly timescale after summing up the daily simulations for each month furthermore the simulations from run 2 fig 3 c1 c4 are less homogenized as compared to those from run 1 however the input information is not sufficient to accurately reproduce the locations of heavier rainfall yellow patches in fig 3 in run 3 fig 3 d1 d4 size of the input information is increased domain 2 and consequently the algorithm is able to reproduce the spatial patterns well the locations depicting heavier rainfall are also better represented although not satisfactorily in run 4 the spatial patterns from simulations closely resemble those of the reference also regions of heavier rainfall are reproduced with greater accuracy run 4 provides the largest input information to the algorithm since d4 size 49 49 is used to simulate for the domain d5 size 61 61 overall we find that the runs 3 4 are reasonably able to reproduce low and moderate rainfall events observed in june and september however heavy rainfall events of july and august are better reproduced when the ds algorithm receives a larger amount of input data as in the case of run 4 the simulated rainfall results from experimental runs 5 7 and 8 10 are presented in figs s1 and s2 of the supplementary material respectively 4 1 2 temperature for temperature the simulated results for experimental runs 1 4 are shown in fig 4 the figure shows distinct spatial patterns for the low and high temperature low temperatures are observed in the higher elevated regions of the himalayas blue color while high temperatures are observed in the low lying regions orange color furthermore closer visual inspection of temperature patterns shows a diagonal yellow colored patch line located from northwest to the east direction this patch line represents a transition zone between low and high temperature results show that the simulated spatial patterns of temperature largely agree with those of the reference fig 4a1 a4 except in run 1 fig 4b1 b4 simulations in run 1 clearly indicate that the small amount of training information provided to the algorithm is insufficient to reproduce the patterns of low temperatures moreover in run 2 fig 4c1 c4 the spatial patterns of low temperature are reproduced however the patterns appear to be homogenized as compared to the reference in run 3 fig 4d1 d4 the homogenization of patterns reduces and the patterns appear similar to those in the reference simulations in run 4 provides the best results as the patterns of high and low temperature closely resemble fig 4e1 e4 to those in the reference in all four months we also observe that the transition zone is well preserved in the simulations especially in runs 3 and 4 which suggests that ds is reasonable in reproducing fine resolution weather patterns over complex elevational gradients such as in the northwest himalayas the simulated results for temperature in experimental runs 5 7 and 8 10 are presented in figs s3 and s4 of the supplementary material respectively 4 2 results from error analysis here the model simulations of p10 and t10 are compared to the reference p10 and t10 for the year 2014 to evaluate the accuracy of simulated results in each experimental run using three error statistics rmse mae and pbias rmse and mae inform about the average magnitude of difference between the reference and simulated data moreover pbias indicates the nature of simulations being either positive or negative simulated results are considered overestimated compared to the reference if the pbias is positive and underestimated if the pbias is negative 4 2 1 seasonal error analysis of rainfall simulations we show the spread and frequency of mean seasonal error values of rainfall calculated as rmse mae and pbias for the experimental runs 1 4 in fig 5 please note that here the simulations are done daily however the results are shown at a seasonal timescale also the error values are calculated for each realization and averaged thereafter from fig 5 a1 a4 we observe that the rmse values of rainfall do not show high variations among the experimental runs however the values decrease as the size of the training domain increases for instance in run 1 the rmse values are on the higher side both in terms of spread and frequency the scatter plot fig 5 a1a shows that rmse for the majority of the values is greater than 3 mm the histogram fig 5 a1b confirms the same showing that the frequency of values lies between 3 and 8 mm the values show a decreasing trend in runs 2 and 3 in run 4 the distribution of the majority of rmse values is in the range of 0 6 mm a similar trend is observed in the case of mae fig 5 b1 b4 where the values are higher in runs 1 and 2 while they are comparatively lower for runs 3 and 4 one obvious reason behind such a trend is the smaller amount of training data provided to the algorithm in the case of runs 1 and 2 which is increased in subsequent runs in the case of pbias fig 5 c1 c4 we observe considerable fluctuations in the values both within and among the experimental runs results show that the values are significantly high in run 1 which falls down considerably in run 5 particularly in runs 1 and 2 the range of pbias values is large which decreases in runs 3 and 4 in run 3 only a small number of grids show high pbias which is also suggested by the corresponding histogram furthermore the values in run 4 are relatively scattered within a relatively small range leading to a more accurate simulation of results in run 4 majority of the grids have negative pbias some are closer to zero while few are positive overall seasonal error results for rainfall show that the size of input domain has a direct relation with the accuracy of simulations with runs 3 and 4 performing reasonably well the mean seasonal error values of rainfall calculated as rmse mae and pbias for the experimental runs 5 7 and 8 10 are shown in figs s5 and s6 of the supplementary material respectively 4 2 2 seasonal error analysis of temperature simulations in the case of temperature the spread and frequency of rmse mae and pbias for experimental runs 1 4 are shown in fig 6 a1 a4 b1 b4 and c1 c4 respectively please note that the results are presented here at a seasonal timescale although the simulations are done daily the scatter plots show that the rmse values are highest in run 1 which significantly decreases in runs 2 4 in runs 2 and 3 the majority of the values are less than 10 mm while in run 4 the values drop to less than 6 mm the histograms indicate that most of the values are close to zero mm in all experimental run particularly in runs 3 and 4 thus indicating accurate results furthermore the spread and distribution of mae values largely follow the patterns of rmse with the highest values obtained in run 1 and the least values in run 5 in the case of pbias run 1 shows a larger spread of error values however the range of spread is not large 0 8 the spread becomes smaller with successive runs and reaches its lowest in run 5 where the majority of the values are close to zero the histograms corresponding to each run also indicate that pbias values are relatively high in run 1 and 2 while they are close to zero in runs 3 and 4 overall in the case of temperature run 3 and especially 4 provides the most accurate results the mean seasonal error values of temperature calculated as rmse mae and pbias for the experimental runs 5 7 and 8 10 are shown in figs s7 and s8 of the supplementary material respectively 4 2 3 monthly error analysis of rainfall and temperature the himalayan region shows high variation in the distribution of rainfall among the months of the monsoon season i e june july august and september it may be possible that different months show different magnitude of error depending upon the size of the training data provided to the model the mean monthly error values of rainfall and temperature obtained from each experimental run are presented in table 3 in the case of rainfall results indicate that the values of rmse and mae are quite low for each month unlike in the results of seasonal error values the monthly error values do not show a clear cut decreasing trend when we move from smaller to larger training domain for instance the rmse 2 46 mm and mae 1 12 mm values are lowest in run 3 for the month of june even though the largest training data is provided in run 4 in july rmse is lowest in run 2 5 26 mm while mae is lowest in run 3 2 85 mm moreover in both august and september the rmse and mae values are again found to be lowest in run 3 however it must be noted that except for run 1 there is a very slight difference in the error values among the experimental runs of the respective months for instance the difference between the highest and lowest rmse values among the experimental runs 2 4 for june is only 0 08 mm 2 54 2 46 0 08 mm for july is 0 26 mm 5 52 5 26 0 26 mm for august is 0 2 mm 5 49 5 29 0 2 mm and for september is 0 33 mm 4 25 3 92 0 33 mm error values in a similar range have been estimated by other studies over the region shi 2020 arshad et al 2021 jiang et al 2021 one of the reasons for obtaining such low rmse and mae values could be the averaging of individual days to one single value for a month however the values are low for all months and in all experimental runs thus suggesting that majority of the individual days in a month may also have low error values such small differences in error values indicate that the experimental runs except run 1 possess a reasonable ability to simulate rainfall values for individual months furthermore we observe higher rmse and mae values for july and august in comparison to june and september this can be attributed to the fact that july and august are considered the peak monsoon months hrudya et al 2021 and few extreme rainfall events have been reported to occur over the region in the two months houze et al 2017 in the case of pbias high positive values in run 1 in all four months indicates overestimation by simulations the values decrease considerably in subsequent runs as the training domain gets larger in run 2 pbias shows small negative values for june and july denoting underestimation and positive values for august and september similar trend is visible in run 3 with the exception that the value for september becomes negative furthermore in run 4 pbias values for all months are obtained as slightly negative indicating underestimation of values in the case of temperature we notice a clear cut decreasing trend in rmse and mae values with the increase in the size of the input domain in all four months see table 3 results show that the values in all the months are highest in run 1 while they are lowest in run 5 similar to the results in the case of rainfall the difference between the values in runs 2 4 is very small suggesting reasonable ability of the model to simulate temperature for larger domains furthermore unlike in the case of rainfall where we notice higher error values in july and august and lower values in june and september the error values are relatively similar for temperature across all months in each of the four experimental runs this is primarily because temperature generally does not show high variability in space and time and largely remains uniform within a season moreover the obtained error values are in line with other studies over the region srinivasan et al 2010 singh et al 2015 the monthly error values of rainfall and temperature for the experimental runs 5 10 are presented in table s1 of the supplementary material 4 3 qualitative performance of ds at glacier sites here we evaluate the qualitative performance of ds in estimating fine resolution information of rainfall and temperature at ten selected important glaciers sites hereafter gsites located in the domains d3 d4 and d5 as shown in fig 7 the reference data of daily rainfall the ensemble of 25 realizations and the mean of realizations is plotted in each subplot of fig 7 specifically gsite 1 2 lie in d3 gsite 3 to gsite 7 lie in d4 and gsite 8 to gsite 10 lie in d5 4 3 1 glaciers in domain 3 as mentioned earlier two glaciers gsite 1 and gsite 2 are located in d3 of the study area in the case of rainfall results from fig 7 show that ds is reasonably accurate in estimating rainfall time series at the glacier sites the mean of ensembles is observed to coincide with the reference dataset at most sites with large training domains moreover we observe that the quality of simulations does not deteriorate even when input data from smaller training domains are used at gsite 1 and gsite 2 we notice that both input domains d1 and d2 perform reasonably well in capturing the rainfall time series at gsite 2 however the simulations are not able to capture the peak of extreme rainfall previous study suggest that ds has limitations in reproducing the peaks of extreme rainfall over the region singhal and jha 2021b moreover in the case of temperature we find that the domains do influence the simulations in temperature unlike in the case of rainfall where the size of the training domain did not influence the quality of simulations much the simulated temperature at the gsites is closer to the reference when the largest training domain is used and begin to deviate as the domains become smaller at gsite 1 the gap between the reference and simulated datasets indicates that the input information from d1 is inadequate in simulating temperature of domain 3 d3 we notice that the simulations almost coincide with the reference when a larger training domain d2 is used similar inferences are observed for gsite 2 where the simulated temperature using a larger domain training domain of d2 performs better than that using a smaller training domain d1 4 3 2 glaciers in domain 4 five glaciers gsite 3 to gsite 7 are located in the next domain d4 results of rainfall at the gsite 3 show that the input domain d2 provides the best simulations although other domains d1 and d3 also perform well see fig 7 the simulations overestimate rainfall at gsite 4 for all input domains at the gsites 5 7 ds is able to successfully capture both high and low rainfall peaks using each of the input domains d1 d3 results show slight overestimation at gsite 7 however the simulated ensembles are able to capture the variations in the peak rainfall in the case of temperature we observe a similar trend in results where the largest training domain provides the best results as expected at all the gsites d3 is found to perform better than the other two smaller domains see fig 7 4 3 3 glaciers in domain 5 three glaciers gsite 8 to gsite 10 are located in d5 in the case of rainfall all the glacier sites show overestimation when smaller training domains are used and begin to coincide with the reference as the larger training domains are used however at the gsite 8 and gsite 9 we observe that the simulated rainfall using the training domain of d3 gives the best results despite being the second largest training domain see fig 7 at the gsite 10 the largest domain d4 gives the best result in the case of temperature results show that the largest training domain d4 estimates the most accurate results at all sites specifically at gsite 9 we notice that d3 also shows reasonable simulation accuracy while at gsite 10 both d2 shows good accuracy along with d4 4 4 quantitative performance of ds at glacier sites here we evaluate the performance of ds in simulating the rainfall and temperature time series using the error statistics of rmse and pbias the mean rmse and pbias values for rainfall and temperature calculated for all the gsites are shown in table 4 the errors are estimated on the basis of the deviation of the mean of ensembles from the reference data in the case of rainfall results show that the rmse values are consistently low at all gsites for all training domains the lowest rmse value is obtained for gsite 4 0 55 mm when the information from the training domain d2 is used to conduct the simulations for d4 while the highest value is obtained at gsite 2 when domain d2 is used to simulate for d3 as observed earlier the rmse values at the gsites are obtained on the higher side when the smallest training domain is used apart from that no other relationship is observed between the error values and domain size furthermore the pbias values show a high degree of fluctuations both within and among the glacier sites for instance at gsite 6 the training domain d1 shows pbias as 137 30 while d3 shows only 9 03 similarly at gsite 10 pbias is high for d1 129 78 decreases for d2 and d3 and again rises sharply to 162 95 for d4 the values are mostly positive indicating overestimation of results overall results show that ds is successfully able to simulate rainfall at point locations of glaciers using input domains of various sizes especially d3 and d4 in the case of temperature higher values of rmse for training domain d1 at each gsite clearly suggests that the input information is not enough for simulations rmse for all other training domains is comparatively very low showing a decreasing trend with the increase in size of the domain the range of rmse is largest for d1 among all the gsites 20 69 13 93 k decreases in d2 4 91 1 06 k and d3 1 02 2 73 k and becomes smallest in d4 1 02 1 27 k the pbias values follow a similar trend as in rmse where the smallest training domain estimates the largest range of pbias 7 62 5 0 k and the largest domain d4 estimates the smallest range 0 01 to 0 31 k overall the pbias values do not show high fluctuations as observed in rainfall with the majority of the values for higher training domains being closer to zero thus indicating good performance by ds in simulating fine resolution temperature data at point locations of glaciers 5 discussion we setup an mps based statistical downscaling model over the northwest himalayas to estimate fine resolution daily weather information in a larger domain simulation domain by utilizing the corresponding training information of a smaller domain training domain a total of ten experimental runs are performed among five spatial domains of varying sizes apart from the spatial domains fine resolution rainfall and temperature information is also simulated at ten point locations of glaciers results from the experimental runs show that ds is reasonably accurate in simulating the information of rainfall and temperature the input information of larger training domains d3 and d4 performs relatively better than those of the smaller domains d1 and d2 moreover results from the seasonal error analysis show that the values of rmse mae and pbias decrease as the size of the training domain increases for both rainfall and temperature one of the reasons for the decreasing trend of error values could be that the smaller training domains lack sufficient information to reproduce the fine resolution weather patterns while it is present in a sufficient amount in larger domains the results from the monthly error analyses do not show any such trend for rainfall as in some experimental runs smaller training domains d1 and d2 are observed to perform better than the larger domains d3 and d4 however in the case of temperature the error values begin to decrease with the increase in the domain size for each of the four monsoon months the absence of any trend in the monthly errors of rainfall could be attributed to the fact that rainfall shows significant variability even among the months of the monsoon season over the himalayas while the temperature is largely consistent and shows variations only within a smaller range the simulated results at the point locations of glaciers suggest that ds is able to reproduce the fine resolution time series of rainfall and temperature in the case of rainfall we find that simulations performed using the smaller sized training domains d1 and d2 are able to coincide with the reference however the input information from larger domains d3 and d4 performs reasonably well at the sites where high rainfall peaks are observed this implies that the information in the larger domain may be necessary for simulating extreme rainfall which themselves are rare events moreover the rmse values are observed to be consistently low at most glacier sites while pbias values are on the higher side for smaller domains while on the lower side for larger domains in the case of temperature results show that the quality of simulations is directly proportional to the size of the training domain simulations using larger domains perform well in comparison to those using the smaller domains values of rmse and pbias also suggests the same since the largest error values are obtained from the smallest domain d1 and vice versa similar to the experimental runs among the spatial domains the simulation at each glacier site is performed using the training domains other than the one in which the glaciers are located for instance if the glacier is located in d5 the simulations are performed using the input information from d1 d4 only overall ds shows great potential in estimating fine resolution weather information in a larger domain using training information of a smaller domain both among the spatial domains and at important glacier sites located over the northwest himalayas 6 conclusions in this work we set up an mps based statistical model over the northwest himalayas to extrapolate the fine resolution weather information of a larger domain simulation domain by using training information of a comparatively smaller domain training domain the training domain consists of rainfall and temperature data at both 10 and 30 km spatial resolution for the time period of 2001 2013 the simulation domain contains the information only at 30 km spatial resolution for the year 2014 the model searches for spatial patterns in the training domain similar to those in the simulation domain and generates the corresponding 10 km extrapolated information for the year 2014 moreover an ensemble of realizations is also generated by performing multiple stochastic simulations which helps in reducing the uncertainty involved with weather data the performance of ds to generate extrapolated weather information is evaluated both spatially and at point locations of glaciers the spatial evaluation is performed under ten experimental runs involving five domains of different sizes and characteristics results show that ds is able to replicate the spatial patterns of rainfall and temperature for the larger region specifically the simulations involving larger training domains d3 and d4 perform better than those involving smaller training domains d1 and d2 small range of rmse 2 46 6 68 mm and mae 1 12 4 15 mm for rainfall among all the experimental runs suggest that ds is reasonable in extrapolating downscaled information over complicated terrains of the northwest himalayas furthermore the daily simulated results of rainfall and temperature at ten point locations of glaciers are found to be comparable to the reference at most sites at the glacier sites smaller training domains are able to reproduce low and moderate peaks of rainfall however higher peaks require information from larger domains in the case of temperature smaller domains do not show satisfactory simulation ability while larger domains such as d3 and d4 mean rmse 1 68 k and 1 11 k respectively show good results compared to the reference moreover the pbias values are slightly positive with most of them being closer to zero indicating reasonable accuracy in summary ds is also capable of estimating fine resolution weather information at point locations of glaciers located over complex terrains such as of northwest himalayas thus creating greater avenues to study their behaviour under climate change the ds algorithm can be further applied to a wide range of hydrological research future studies will focus on using alternate fine resolution rainfall information to overcome the limited availability of satellite rainfall products moreover studies can be undertaken to find ways to reduce the dependency on a large amount of ti to generate fine resolution weather information declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements the ds code was developed by the randlab team at the university of neuchâtel switzerland http www randlab org we obtained the ds tool from dr gregoire mariethoz institute of earth surface dynamics university of lausanne for carrying out this work the present research is supported by the scheme for transformational and advanced research in sciences moe stars grant number stars apr2019 ds 391 fs awarded to sanjeev kumar jha we are also thankful to the department of science and technology government of india for providing financial support to mr akshay singhal for his doctoral work under the dst inspire scheme dst inspire 03 2019 001343 if 190257 appendix a supplementary data the following is the supplementary data to this article multimedia component 1 multimedia component 1 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2022 105317 
25648,data assimilation is an effective approach to reduce the propagation and cumulative errors of land use models however as the discrete categorical outputs of land use models land use data assimilation requires a novel approach distinguished from the traditional assimilation for continuous variables here bayesian inference for categorical distribution is introduced into a land use cellular automata model to update multiple discrete state variables the accuracies with data assimilation outperform those of simulation only by 2009 kappa coefficient and figure of merit in the entire area increase by 0 34 and 1 78 respectively and in fine scale areas where drastic and representative land use changes occurred increase by 23 88 and 38 39 respectively the assimilation performance is associated with the landscape patch size and the length of the assimilation cycle this study innovatively introduces the conjugate prior features of dirichlet distribution into land use assimilation providing insights and references for discrete variable data assimilation keywords land use data assimilation cellular automata categorical variable dirichlet distribution 1 introduction land use change has a substantial impact on the earth s land surface system newbold et al 2015 verburg et al 2013a turner et al 2007 foley et al 2005 lambin et al 2001 its importance in climate hydrology and ecosystem studies has been increasingly recognized prestele et al 2017 quesada et al 2017 alexander et al 2017 to represent land use dynamics cellular automata so called land use cellular automata luca models have been increasingly used for land use change modeling because they are tractable and generate complex dynamics according to a set of simple rules santé et al 2010 li and yeh 2000 luca models involve many input factors including variables parameters and structural elements e g transition rules the quality of the results depends highly on these input factors moreover land use change processes can be considered complex processes yielding nonlinear developments under the combined influence of natural and social factors at different scales koomen et al 2007 therefore like other land surface process models luca models also have uncertainties and cannot be expected to generate results that are perfectly accurate van vliet et al 2016 soares filho et al 2013 verburg et al 2013b these uncertainties can propagate and accumulate over the model runtime koo et al 2020 to improve fidelity with reality conditions model calibration is required newland et al 2018 brown et al 2013 at present a number of calibration approaches have been proposed such as expert knowledge manual calibration machine learning neural networks and genetic algorithms statistical analysis mainly regression analysis and weights of evidence and other methods van vliet j et al 2016 soares filho et al 2013 kityuttachai et al 2013 aljoufie et al 2013 mas et al 2012 however data assimilation also known as model data fusion has received limited attention compared to other calibration approaches although it can explicitly address uncertainty issues that arise in predicting land use change levy et al 2018 van der kwast et al 2011 2012 data assimilation is an approach to determine the best unbiased estimate of model state variables or parameters by combining different source observations and dynamical models li et al 2007 2010 2014 2021 liu et al 2020 most importantly data assimilation can quantify the uncertainties of various errors levy et al 2018 liu et al 2017 verstegen et al 2016 bai et al 2013 liu and gupta 2007 it has been successfully applied to many fields such as meteorology fang and li 2016 houtekamer and zhang 2016 oceanography carton and giese 2008 and land surface process modeling li et al 2020 huang et al 2016 rodell et al 2004 unlike these fields in which data assimilation has been successfully applied land use data assimilation is not easy to implement for land use models because the outputs of land use models are discrete categorical variables e g land use type and there is a lack of state variables that can be directly used for assimilation therefore there are few studies on land use data assimilation in existing studies particle filter pf and ensemble kalman filter enkf two sequential assimilation algorithms were used verstegen et al 2014 van der kwast et al 2011 zhang et al 2011 however these two assimilation algorithms are for continuous variables and cannot be used directly to optimize categorical variables zhang et al 2011 first attempted to establish a relationship between simulated and observed land use maps with a patch based aggregated urban development density based on the observation information that was incorporated into the modeling process using an enkf however this approach is not feasible to implement for the assimilation of multiple land use types because establishing precise mapping between multiple discrete land use types and their densities is difficult our aim here is to develop an assimilation strategy for considering categorical variables and introduce it into a luca model to obtain the best estimate of land use change the highlight of this work is that based on the conjugate prior of dirichlet distribution the state variables with a discrete categorical distribution can be updated in a data assimilation system this assimilation approach keeps the analysis step from experiencing the posterior probability nonlinear recurrence problem in a case study the approach is applied to land use change modeling in ganzhou district of gansu province china over the period 1992 2009 at the same time through different assimilation schemes with combinations of different landscape patch sizes and assimilation cycles the sensitivities of these two factors to assimilation performance are investigated 2 method and data 2 1 bayesian inference using conjugate prior for assimilating categorical distribution the proposed land use assimilation framework integrates the model assimilation algorithm and data driving data parameters and observation data to improve the estimate of model state variables by reducing uncertainties the framework can be expressed as 1 y 0 t h x t ε t where y 0 t is the observation at time t 1 2 n h is the observation operator which transfers the model state into the estimated observation x t is the model state at time t and ε represents the observation error here the input x of the model is a categorical variable taking values 1 2 k which is different from the corresponding continuous variable in a traditional data assimilation method in this study x represents different land use types to study the evolution of x a categorical distribution should be introduced a categorical distribution is a discrete probability distribution which describes the possible results of a random variable it is also a special case of the multinomial distribution the probability function p of this variable is 2 p x π i 1 k π i δ x t i where π π 1 π k π k represents the probability of category k and i 1 k π i 1 in this study the probability parameter π denotes the density of land use type δ x t i is an indicator function and evaluates to 1 if x t i and 0 otherwise and k represents the dimension of the categorical variable x bayesian inference treats categorical variable to probability parameter π and the posterior distributions of this parameter π can be expressed by 3 p π y 0 p y 0 π p π p y 0 p y 0 π p π where p π and p π y 0 denote the prior and posterior parameter distributions respectively p y 0 π signifies the model likelihood for observed data y 0 and p y 0 is the normalization constant so that the posterior parameter distribution integrates to unity in bayesian statistics the conjugate prior distribution of a categorical distribution is dirichlet distribution denoted dir α the prior distribution of this parameter π is therefore 4 p π π 1 π k d i r α where α α 1 α k is a concentration hyperparameter and α k represents the virtual number of category k similarly observation data y 0 is a categorical distribution and a multinomial distribution denoted cat n c therefore the likelihood function is a categorical distribution 5 p y 0 π y 1 0 y k 0 cat ν c where c c 1 c k and c k j 1 n y j 0 k is the observed number of category k in n observation sample points according to the bayesian formula and the conjugation of dirichlet distribution minka 2003 the posterior distribution of the parameter π is also dirichlet distribution after incorporating the information gained from the observed data that is 6 p π y 0 p y 0 π p π d i r α c where α c α 1 c 1 α k c k intuitively the posterior probability has the same form as the expected value of the posterior distribution minka 2003 7 p π i y 0 e π i y 0 c i α i n i 1 k α i where c i is the observed number of category i α i represents the virtual number of category i before observation n is the number of sampling points that is the observed number for all categories and k α k is a virtual number for all categories in summary bayesian inference for categorical distribution can be used to estimate the density parameter π of discrete land use types given a collection of n samples however the output of the land use model is a discrete land use type e g cropland or built up land the land use data assimilation algorithm is used to calculate the posterior distribution probability of discrete land use types then based on the posterior distribution the difference between the observed and simulated land use types can be compared and reduced adjusting the simulation results therefore the assimilation framework of bayesian inference for categorical distribution needs to transform discrete land use type into continuous probability density which is an intermediate variable linking the observed and simulated land use types a previous study also indicated that patch based aggregation of land use to a macroscale with a continuous field is a feasible method to implement state assimilation of land use models zhang et al 2011 in this study the whole study area is divided into adjacent landscape patches the probability density of each land use type in each landscape patch is calculated as follows 8 π k l w w c o n s i j k w w where π kl is the probability density of land use type k in landscape patch l s ij is the state of the cell i j and w is the window size of each landscape patch that is each landscape patch is assigned k values representing the percentage of k land use types within the patch based on the probability density of the observed and simulated land use types a posterior distribution is derived from the conjugate prior features of the categorical distribution equation 7 the simulation results of the land use model can be updated based on the assimilated probability density of land use type i e the posterior distribution from bayesian inference for categorical distribution most noteworthy is that the sum of the assimilated probability density for all land use types is 1 in each patch therefore the assimilation method of land use based on bayesian inference for categorical distribution is easy to implement the land use assimilation flow diagram is shown in fig 1 2 2 luca model the model selected in this study is an integrated luca model based on the techniques of a markov chain logistic regression and cellular automata hu et al 2018 in the luca model the markov chain is used to control the number of land use types while logistic regression and cellular automata are used to extract transition rules and manage the spatial pattern of land use change a markov chain is a discrete random process from one state to another state at each time step and is widely used to study land use change at different scales arsanjani et al 2013 it can predict all multidirectional area changes among all land use types by a series of transition probabilities or transition areas from one state to another at a specified period of time a markov forecast model can be expressed as follows 9 s t 1 p s t where s t and s t 1 are land use statuses at times t and t 1 respectively the annual transition probability matrix p p t 1 t t t t is the interval time in period t and p t is the transition probability matrix of land use type in period t expressed as follows 10 p t p 11 p 12 p 1 n p 21 p 22 p 2 n p n 1 p n 2 p n n in the above matrix p ij is the transition probability from the ith type into the jth type during the years from the start point to the target simulation period and n is the number of land use types logistic regression is a simple and practical statistical methodology it is used to establish the empirical relationships between the dependent variables and independent variables arsanjani et al 2013 verburg et al 2002 serneels and lambin 2001 in the luca model the dependent variable represents land use change and is a binary presence 1 or absence 0 event the independent variables are spatial driving variables while employing logistic regression to simulate land use change the spatial dependence of spatial data should be considered to remove spatial autocorrelation a stratified random sampling technique can effectively represent populations with a smaller sample size and can also reduce spatial dependence arsanjani et al 2013 xie et al 2005 cellular automata have strong capabilities for simulating the spatiotemporal characteristics of complex systems li and yeh 2002 due to their ability to simulate dynamic spatial processes from a bottom up perspective and their ease of implementation cellular automata are regarded as one of most effective methods in simulating and predicting land use change santé et al 2010 jantz et al 2004 based on logistic regression and cellular automata the transition probability p i j t of the luca model is calculated by 11 p i j t r a p g k i j c o n s i j t where the stochastic perturbation r a l n γ α in which γ is a random number within the range of 0 1 and α is a constant to control the size of the stochastic perturbation the global transition potential of a cell p g 1 1 exp z i j where z i j a k b k x k a is constant to be estimated and b k is the coefficient weight of the spatial driving variable both of which are calculated using logistic regression and x k is a spatial driving variable such as the distance to roads or the neighborhood influence of land use type the inheritance coefficient k ij is defined as a constant between 0 and 1 according to the inheritance ability of land use greater k values indicate a stronger likelihood of maintaining the original state the constraint factors of a cell c o n s i j t is a constant range from 0 to 1 for example if water source protection areas and basic cropland are forbidden to be developed as built up land the value of c o n s i j t is 0 s i j t is the state of cell i j at time t in each iteration loop the transition probabilities of each type of land use are calculated using equation 11 finally the land use type is determined according to the cellular automata transition rules and the number of land use types predicted by the markov chain in this study the cellular automata transition rule mainly uses the maximum transition probability and follows particular priority sequence rules depending on preliminary research as well as expert knowledge during the simulation a cell can only be allocated to a land use type and according to the maximum value of the transition probability the future type of land use in a cell is decided that is a land use type is successively allocated to cells as a descending sequence of transition probabilities of this land use type in all cells until the total demand for this land use type is satisfied in addition a cell will not be changed to another land use type if the cell is allocated to a land use type 2 3 study area and data the ganzhou district of zhangye prefecture is selected as the study area fig 2 ganzhou district is in gansu province china and is located in the zhangye oasis of the middle reaches of the heihe river basin it is a typical irrigated agricultural region and water resources are scarce its total area is 3603 83 km2 in the past five decades major changes in land use patterns have been observed in ganzhou district driven by changes in the water environment and human activities cheng et al 2014 the most significant land use changes in the study area are substantial urban expansion and the continued reclamation of cropland for economic interests hu et al 2015 these factors increase the stress on water resources and the environment threatening sustainable regional development a long term annual land use dataset from 1992 to 2009 is used to implement the land use assimilation scheme the land use dataset is derived using visual interpretation of 30 m resolution landsat tm etm images the overall accuracies for land use classification are greater than 90 based on field investigation and verification of high resolution images spot in typical areas for particular years following the national land use database of china nlud c classification system and land use characteristics in the study area the classification system in this study includes seven primary land use types cropland forestland grassland water body built up land wetland and desert the vector land use maps for all years are separately converted into raster format with a resolution of 30 m 30 m other data are spatial driving variables including distance based variables socioeconomic variables and physical attributes these spatial driving variables are obtained using various calculation methods table 1 and are closely related to land use changes based on preliminary logistic regression analysis all the spatial driving variables are unified to the same projection and resolution as the land use maps normalized values from 0 to 1 are then calculated using the maximum and minimum values after scaling the original spatial driving variable datasets 2 4 performance assessment criteria the performance of the land use assimilation system is evaluated using the root mean square error rmse which is calculated as follows 12 r m s e 1 n i 1 n 1 n j 1 n x i j o b s i j 2 where n is the number of land use types n is the dimension of assimilated state variables i e the patch number x ij is an assimilated or simulated value and obs i is an observed value in addition the accuracy of the simulated and observed land use maps is compared on a pixel by pixel basis in this study two accuracy assessment techniques are used the first is an error matrix using the predicted and observed maps the error matrix measures the overall accuracy oa and kappa coefficient the oa is the total number of correctly classified samples diagonal cells of the matrix divided by the total number of samples it measures the accuracy of the entire prediction map the kappa coefficient is a measure of agreement between model the predictions and observations congalton 1991 and is calculated using equation 13 the second technique is the figure of merit fm assessment which is used to calculate the model performance for the changed regions the fm is the ratio of the intersection of an observed change and a predicted change to the union of the observed change and predicted change its range is from 0 meaning no overlap between the observed and predicted change to 100 meaning perfect overlap between the observed and predicted change i e a perfectly accurate prediction pontius et al 2008 the fm is evaluated using equation 14 13 k a p p a m i 1 n x i i i 1 n x i x i m 2 i 1 n x i x i 100 where m is the total number of observations n is the number of rows in the error matrix i e the number of land use types x ii is the number of observations in row i and column i i e the diagonal elements and x i and x i are the marginal totals of row i and column i respectively 14 f m b a b c d 100 where a is the error area due to an observed change predicted as persistence b is the correct area due to an observed change predicted as change c is the error area due to an observed change predicted as the wrong gaining class and d is the error area due to an observed persistence predicted as change 3 experimental design and results 3 1 experimental design to evaluate the performance of the assimilation algorithm for categorical distribution a pair of parallel experiments a luca model without bayesian inference for categorical distribution assimilation algorithm and a luca model with bayesian inference for categorical distribution assimilation algorithm are designed first the luca model parameters are obtained using model calibration based on the historical change dataset during 1992 1999 second land use assimilation is implemented using bayesian inference for categorical distribution algorithm from 1999 to 2009 in addition the performance of land use assimilation may be affected by the landscape patch size and assimilation cycle employed the landscape patch size describes the spatial unit of the assimilated dataset while the assimilation cycle determines how often the assimilation should be implemented therefore the influences of the landscape patch size and assimilation cycle on the assimilation system are also analyzed finally the accuracy of the assimilation results is evaluated and analyzed based on the observation dataset in this study nine schemes are designed to explore the sensitivity to the landscape patch size and the assimilation cycle table 2 considering the size of the study area and the data accuracy three landscape patch scales are designed with spatial resolutions of 150 m 300 m and 900 m since the temporal range of modeling is from 1999 to 2009 three assimilation cycles including n a no assimilation 2 assimilation every 2 years and 5 assimilation every 5 years are designed assimilation performance is analyzed by different combinations of patch sizes and assimilation cycles in a pair based way 3 2 assimilation results and verification during the modeling period of 1999 2009 land use types in the study area remained basically stable the area proportion of land use change is as small as approximately 6 of the entire study area therefore on the whole the spatial distribution of various simulated and assimilated land use types is similar to the actual distribution fig 3 especially in 2001 which is the first year of assimilation the simulation and assimilation maps are closer to the actual maps however as the modeling progressed errors of the luca model continuously accumulate continuing until 2009 which is the end year of the modeling there are some discrepancies in the simulated map for example in the center of the study area urban sprawl is not simulated by the luca model moreover the simulation results of some patches for wetlands in the north of the city forestland in the east of the city and cropland on the edge of the oasis are different from the actual situation after the incorporation of observations the running trajectory of the luca model is adjusted the errors of the luca model are controlled and the assimilation results are improved aside from the qualitative comparison of the maps we also quantitatively compare the oa kappa coefficient and fm of the simulation and assimilation table 3 the table shows that the oa and kappa coefficient of the luca model in 2001 2005 and 2009 are 97 71 and 96 50 95 53 and 93 18 92 65 and 88 85 respectively the accuracy of the luca model shows a decreasing trend over time because of the accumulation of model errors compared with the luca model bayesian inference for categorical distribution assimilation algorithm can adjust the simulation results by incorporating observations into the simulation process therefore the accuracy is improved by relieving the accumulation of errors and the corresponding results are more reliable than those of the luca model the oa kappa coefficient and fm of bayesian inference for categorical distribution assimilation algorithm are improved by 0 05 0 07 and 1 72 in 2001 and are improved by 0 22 0 34 and 1 78 by 2009 compared with those of the luca model the rmse results also show that the assimilation effect is significant and the rmse is improved overall in 2001 and 2009 the rmse decrease by 0 058 and 0 139 respectively table 4 to further evaluate the performance of assimilation three fine scale areas of a b and c in fig 3 are selected within the study area the selection principle of fine scale areas is that land use type changes drastically and these changes are representative within the study area the three fine scale areas represent the changes in built up land water body and cropland affected by human behaviors such as wetland conservation policy or socioeconomic development the simulations of the luca model are poor in the three fine scale areas fig 4 and table 5 due to the insufficient expressions of national policy and socioeconomic activities in the model hu et al 2018 for example in the fine scale area a the development speed and extent of the built up land simulated by the luca model are lower than those of the actual land by the end year of the modeling in 2009 however in the simulation process the simulation results are constantly revised by incorporating observations and the expansion degree of built up land is significantly improved after assimilation the kappa coefficient ranges from 53 75 to 77 63 with an increase of 23 88 the fm ranges from 36 85 to 75 24 with an increase of 38 39 similarly there are also some obvious errors in the simulation of fine scale areas b and c in the fine scale area b the water body location and scope are affected by wetland conservation policy namely construction of a national wetland park the water body in the simulation is larger than that in the actual situation due to insufficient consideration of the impact of national policy in the luca model in fine scale area c on the edge of the oasis the pursuit of economic benefits led to a substantial increase in cropland however the inadequate expression of socioeconomic factors in the luca model leads to the limited expansion of cropland after incorporating observations using bayesian inference for categorical distribution assimilation algorithm the kappa coefficients in the fine scale areas b and c increase by 8 17 and 35 84 respectively the fm of the two fine scale areas increase 18 82 and 35 57 bayesian inference for categorical distribution assimilation algorithm can significantly improve the accuracy of the luca model and the assimilation results can better represent the actual situations 4 discussion the above analysis of the assimilation results reveals that data assimilation is an effective way to alleviate the propagation and accumulation of model errors because the revised model output serves as a more accurate input to the next simulation as a result the model with assimilation clearly outperforms that without assimilation however from the calculation process of bayesian inference for categorical distribution assimilation algorithm land use data assimilation needed to be a priori given the landscape patch size and assimilation cycle therefore is the assimilation performance affected by various landscape patch sizes and assimilation cycles here we analyze the sensitivity to landscape patch size and assimilation cycle 4 1 sensitivity analysis to landscape patch size fig 5 presents the rmse changes with a 2 year assimilation cycle at different landscape patch sizes from the simulation results of the luca model without data assimilation the rmses at the 900 m patch size are the smallest while those at the 150 m patch size are the largest the rmses at the three landscape patch sizes show an increasing trend as the modeling progresses this is because over time the model errors will be transmitted and accumulated resulting in a decrease in luca model performance this is consistent with previous research results chen et al 2016 li and gong 2016 garcía et al 2011 yeh and li 2006 after implementing bayesian inference for categorical distribution assimilation algorithm the rmses at the three landscape patch sizes are significantly reduced this is because observations are absorbed into the modeling process which weakens the errors of the luca model in addition similar to the simulation results of the luca model without data assimilation at different landscape patch sizes the rmses of finer patches are larger than those of coarser patches which is related to the scale effect of aggregated landscape patches with a finer patch scale there are fewer land use types especially in extreme cases and there may be only one type of land use enlarging the gaps between simulated and observed land use type densities however a coarser patch can obscure the spatial details of land use change within it which will weaken the characteristics of land use type changes resulting in smaller rmse values in addition the fm change shows that a finer patch scale is associated with a better performance of data assimilation fig 6 the fm curve with a patch size of 150 m is above the curves of the other two patch sizes for the target year of 2009 the fm value with a patch size of 150 m is approximately 0 52 higher than that with a patch size of 900 m although the rmse is larger for finer patch sizes the performance of bayesian inference assimilation algorithm is better than that with coarser patch sizes 4 2 sensitivity analysis to assimilation cycle in addition to landscape patch size the assimilation cycle is also a critical factor in land use data assimilation from the rmse changes at 150 m patch size with different assimilation cycles fig 7 rmses with assimilation cycles of 2 and 5 years are smaller than those of the luca model without assimilation achieving the corrections of land use types the rmses with a 5 year assimilation cycle are significantly larger than those with a 2 year assimilation cycle with the prolongation of the assimilation cycle the rmses increase and the performance of bayesian inference for categorical distribution assimilation algorithm decreases from changes of the fm values with a 150 m patch size but different assimilation cycles fig 8 the fm values after implementing bayesian inference for categorical distribution assimilation algorithm are larger than those of the luca model without data assimilation for the target year of 2009 the fm value with a 2 year assimilation cycle is improved by approximately 1 78 compared to that without data assimilation by comparing different assimilation cycles it can be found that the curve of the 2 year assimilation cycle is higher than that of the 5 year assimilation cycle specifically for the target year of 2009 the fm value with a 2 year assimilation cycle is approximately 0 30 higher than that with a 5 year assimilation cycle however this does not mean that the shorter the assimilation cycle is the better the performance of bayesian inference for categorical distribution assimilation algorithm due to the limitation of available observations the determination of an optimum assimilation cycle needs further study 4 3 problems and challenges this study attempts to update the multiple discrete state variables of a land use dynamic model by introducing the conjugate prior of dirichlet distribution into bayesian inference the proposed assimilation framework can significantly improve the simulation accuracy of the luca model by compensating for the insufficient expression of human behaviors in the luca model at three fine scale areas a b and c where dramatic and typical changes in land use type were affected by national policy and socioeconomic development the simulation accuracy of the luca model is poor because the model does not take full account of the effect of national policy and socioeconomic activities however the proposed assimilation framework makes the simulation results closer to the actual situation by combining observations compensating for the impact of human factors on land use types in addition the proposed assimilation framework can be extended not only to assimilate grid based remote sensing data and other socioeconomic indicators but also to assimilate pixel based landscape matrices as shown by relevant studies van der kwast et al 2011 the performance of bayesian inference for categorical distribution assimilation algorithm can be improved by simultaneously adjusting the assimilation strategy with varying spatial patch sizes and assimilation cycles however it does not imply that assimilation should be implemented with a finer patch size and a shorter assimilation cycle the optimum spatial patch size and assimilation cycle need further study due to the limitation of available observations additionally error is an important part of a data assimilation system thanks to the conjugate prior theory in bayesian inference we did not need to perform any nonlinear approximate approach such as an ensemble kalman filter and in theory the corresponding algorithm errors are therefore not introduced into the assimilation system in this study the assimilation technique reduces the errors caused by the stochastic perturbation in the luca model while ignoring the systemic uncertainties in the model structure and parameterization systemic uncertainty needs to be considered in future land use simulation research and it is also a large challenge for further research 5 conclusions bayesian inference for categorical distribution assimilation algorithm is introduced into a land use cellular automata luca model to update discrete land use type variables from the assimilation results the proposed assimilation approach can significantly improve the accuracy of the luca model compared with the results without data assimilation the oa kappa coefficient and fm with data assimilation increase by 0 22 0 34 and 1 78 respectively at the end year of the modeling in 2009 additionally in fine scale areas where drastic and representative changes in land use types were affected by wetland conservation policy or socioeconomic development the assimilation effect is more significant at fine scale area a the oa kappa coefficient and fm can increase by 14 10 23 88 and 38 39 respectively overall bayesian inference for categorical distribution assimilation algorithm can fuse the observations and simulation values of discrete and multiple land use types alleviating the propagation and accumulation of model errors the assimilation performance is affected by the landscape patch size and the assimilation cycle better assimilation performance corresponds to a finer landscape patch size and a shorter assimilation cycle however it does not imply that assimilation should be implemented with a finer patch size and a higher frequency the determination of the optimum landscape patch size and assimilation cycle needs further study due to the limitation of available observations additionally the systemic error in the luca model structure and parameterization needs to be considered in future land use simulation research and it is also a large challenge for further research in summary it is feasible to use bayesian inference for categorical distribution method to assimilate the discrete and multiple state variables of land use type and the results of assimilation are improved compared with those of modeling without assimilation the study contributes to a better understanding of the assimilation mechanisms of discrete variables and provides a general framework for assessing land use change by combining observations with a multiple land use dynamic model although the assimilation strategy was tested only with a land use model it can be easily applied to other models involving discrete variables such as economic and social models data availability all the data used in this study including land use channel population density and dem are available at the national tibetan plateau third role environment data center http data tpdc ac cn declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements the study was supported by the strategic priority research program of the chinese academy of sciences china grants xda20100104 the national science and technology major project of china s high resolution earth observation system china 21 y20b01 9001 19 22 the national natural science foundation of china china grant no 41801270 and gansu province science and technology program china grant no 21jr7ra053 
25648,data assimilation is an effective approach to reduce the propagation and cumulative errors of land use models however as the discrete categorical outputs of land use models land use data assimilation requires a novel approach distinguished from the traditional assimilation for continuous variables here bayesian inference for categorical distribution is introduced into a land use cellular automata model to update multiple discrete state variables the accuracies with data assimilation outperform those of simulation only by 2009 kappa coefficient and figure of merit in the entire area increase by 0 34 and 1 78 respectively and in fine scale areas where drastic and representative land use changes occurred increase by 23 88 and 38 39 respectively the assimilation performance is associated with the landscape patch size and the length of the assimilation cycle this study innovatively introduces the conjugate prior features of dirichlet distribution into land use assimilation providing insights and references for discrete variable data assimilation keywords land use data assimilation cellular automata categorical variable dirichlet distribution 1 introduction land use change has a substantial impact on the earth s land surface system newbold et al 2015 verburg et al 2013a turner et al 2007 foley et al 2005 lambin et al 2001 its importance in climate hydrology and ecosystem studies has been increasingly recognized prestele et al 2017 quesada et al 2017 alexander et al 2017 to represent land use dynamics cellular automata so called land use cellular automata luca models have been increasingly used for land use change modeling because they are tractable and generate complex dynamics according to a set of simple rules santé et al 2010 li and yeh 2000 luca models involve many input factors including variables parameters and structural elements e g transition rules the quality of the results depends highly on these input factors moreover land use change processes can be considered complex processes yielding nonlinear developments under the combined influence of natural and social factors at different scales koomen et al 2007 therefore like other land surface process models luca models also have uncertainties and cannot be expected to generate results that are perfectly accurate van vliet et al 2016 soares filho et al 2013 verburg et al 2013b these uncertainties can propagate and accumulate over the model runtime koo et al 2020 to improve fidelity with reality conditions model calibration is required newland et al 2018 brown et al 2013 at present a number of calibration approaches have been proposed such as expert knowledge manual calibration machine learning neural networks and genetic algorithms statistical analysis mainly regression analysis and weights of evidence and other methods van vliet j et al 2016 soares filho et al 2013 kityuttachai et al 2013 aljoufie et al 2013 mas et al 2012 however data assimilation also known as model data fusion has received limited attention compared to other calibration approaches although it can explicitly address uncertainty issues that arise in predicting land use change levy et al 2018 van der kwast et al 2011 2012 data assimilation is an approach to determine the best unbiased estimate of model state variables or parameters by combining different source observations and dynamical models li et al 2007 2010 2014 2021 liu et al 2020 most importantly data assimilation can quantify the uncertainties of various errors levy et al 2018 liu et al 2017 verstegen et al 2016 bai et al 2013 liu and gupta 2007 it has been successfully applied to many fields such as meteorology fang and li 2016 houtekamer and zhang 2016 oceanography carton and giese 2008 and land surface process modeling li et al 2020 huang et al 2016 rodell et al 2004 unlike these fields in which data assimilation has been successfully applied land use data assimilation is not easy to implement for land use models because the outputs of land use models are discrete categorical variables e g land use type and there is a lack of state variables that can be directly used for assimilation therefore there are few studies on land use data assimilation in existing studies particle filter pf and ensemble kalman filter enkf two sequential assimilation algorithms were used verstegen et al 2014 van der kwast et al 2011 zhang et al 2011 however these two assimilation algorithms are for continuous variables and cannot be used directly to optimize categorical variables zhang et al 2011 first attempted to establish a relationship between simulated and observed land use maps with a patch based aggregated urban development density based on the observation information that was incorporated into the modeling process using an enkf however this approach is not feasible to implement for the assimilation of multiple land use types because establishing precise mapping between multiple discrete land use types and their densities is difficult our aim here is to develop an assimilation strategy for considering categorical variables and introduce it into a luca model to obtain the best estimate of land use change the highlight of this work is that based on the conjugate prior of dirichlet distribution the state variables with a discrete categorical distribution can be updated in a data assimilation system this assimilation approach keeps the analysis step from experiencing the posterior probability nonlinear recurrence problem in a case study the approach is applied to land use change modeling in ganzhou district of gansu province china over the period 1992 2009 at the same time through different assimilation schemes with combinations of different landscape patch sizes and assimilation cycles the sensitivities of these two factors to assimilation performance are investigated 2 method and data 2 1 bayesian inference using conjugate prior for assimilating categorical distribution the proposed land use assimilation framework integrates the model assimilation algorithm and data driving data parameters and observation data to improve the estimate of model state variables by reducing uncertainties the framework can be expressed as 1 y 0 t h x t ε t where y 0 t is the observation at time t 1 2 n h is the observation operator which transfers the model state into the estimated observation x t is the model state at time t and ε represents the observation error here the input x of the model is a categorical variable taking values 1 2 k which is different from the corresponding continuous variable in a traditional data assimilation method in this study x represents different land use types to study the evolution of x a categorical distribution should be introduced a categorical distribution is a discrete probability distribution which describes the possible results of a random variable it is also a special case of the multinomial distribution the probability function p of this variable is 2 p x π i 1 k π i δ x t i where π π 1 π k π k represents the probability of category k and i 1 k π i 1 in this study the probability parameter π denotes the density of land use type δ x t i is an indicator function and evaluates to 1 if x t i and 0 otherwise and k represents the dimension of the categorical variable x bayesian inference treats categorical variable to probability parameter π and the posterior distributions of this parameter π can be expressed by 3 p π y 0 p y 0 π p π p y 0 p y 0 π p π where p π and p π y 0 denote the prior and posterior parameter distributions respectively p y 0 π signifies the model likelihood for observed data y 0 and p y 0 is the normalization constant so that the posterior parameter distribution integrates to unity in bayesian statistics the conjugate prior distribution of a categorical distribution is dirichlet distribution denoted dir α the prior distribution of this parameter π is therefore 4 p π π 1 π k d i r α where α α 1 α k is a concentration hyperparameter and α k represents the virtual number of category k similarly observation data y 0 is a categorical distribution and a multinomial distribution denoted cat n c therefore the likelihood function is a categorical distribution 5 p y 0 π y 1 0 y k 0 cat ν c where c c 1 c k and c k j 1 n y j 0 k is the observed number of category k in n observation sample points according to the bayesian formula and the conjugation of dirichlet distribution minka 2003 the posterior distribution of the parameter π is also dirichlet distribution after incorporating the information gained from the observed data that is 6 p π y 0 p y 0 π p π d i r α c where α c α 1 c 1 α k c k intuitively the posterior probability has the same form as the expected value of the posterior distribution minka 2003 7 p π i y 0 e π i y 0 c i α i n i 1 k α i where c i is the observed number of category i α i represents the virtual number of category i before observation n is the number of sampling points that is the observed number for all categories and k α k is a virtual number for all categories in summary bayesian inference for categorical distribution can be used to estimate the density parameter π of discrete land use types given a collection of n samples however the output of the land use model is a discrete land use type e g cropland or built up land the land use data assimilation algorithm is used to calculate the posterior distribution probability of discrete land use types then based on the posterior distribution the difference between the observed and simulated land use types can be compared and reduced adjusting the simulation results therefore the assimilation framework of bayesian inference for categorical distribution needs to transform discrete land use type into continuous probability density which is an intermediate variable linking the observed and simulated land use types a previous study also indicated that patch based aggregation of land use to a macroscale with a continuous field is a feasible method to implement state assimilation of land use models zhang et al 2011 in this study the whole study area is divided into adjacent landscape patches the probability density of each land use type in each landscape patch is calculated as follows 8 π k l w w c o n s i j k w w where π kl is the probability density of land use type k in landscape patch l s ij is the state of the cell i j and w is the window size of each landscape patch that is each landscape patch is assigned k values representing the percentage of k land use types within the patch based on the probability density of the observed and simulated land use types a posterior distribution is derived from the conjugate prior features of the categorical distribution equation 7 the simulation results of the land use model can be updated based on the assimilated probability density of land use type i e the posterior distribution from bayesian inference for categorical distribution most noteworthy is that the sum of the assimilated probability density for all land use types is 1 in each patch therefore the assimilation method of land use based on bayesian inference for categorical distribution is easy to implement the land use assimilation flow diagram is shown in fig 1 2 2 luca model the model selected in this study is an integrated luca model based on the techniques of a markov chain logistic regression and cellular automata hu et al 2018 in the luca model the markov chain is used to control the number of land use types while logistic regression and cellular automata are used to extract transition rules and manage the spatial pattern of land use change a markov chain is a discrete random process from one state to another state at each time step and is widely used to study land use change at different scales arsanjani et al 2013 it can predict all multidirectional area changes among all land use types by a series of transition probabilities or transition areas from one state to another at a specified period of time a markov forecast model can be expressed as follows 9 s t 1 p s t where s t and s t 1 are land use statuses at times t and t 1 respectively the annual transition probability matrix p p t 1 t t t t is the interval time in period t and p t is the transition probability matrix of land use type in period t expressed as follows 10 p t p 11 p 12 p 1 n p 21 p 22 p 2 n p n 1 p n 2 p n n in the above matrix p ij is the transition probability from the ith type into the jth type during the years from the start point to the target simulation period and n is the number of land use types logistic regression is a simple and practical statistical methodology it is used to establish the empirical relationships between the dependent variables and independent variables arsanjani et al 2013 verburg et al 2002 serneels and lambin 2001 in the luca model the dependent variable represents land use change and is a binary presence 1 or absence 0 event the independent variables are spatial driving variables while employing logistic regression to simulate land use change the spatial dependence of spatial data should be considered to remove spatial autocorrelation a stratified random sampling technique can effectively represent populations with a smaller sample size and can also reduce spatial dependence arsanjani et al 2013 xie et al 2005 cellular automata have strong capabilities for simulating the spatiotemporal characteristics of complex systems li and yeh 2002 due to their ability to simulate dynamic spatial processes from a bottom up perspective and their ease of implementation cellular automata are regarded as one of most effective methods in simulating and predicting land use change santé et al 2010 jantz et al 2004 based on logistic regression and cellular automata the transition probability p i j t of the luca model is calculated by 11 p i j t r a p g k i j c o n s i j t where the stochastic perturbation r a l n γ α in which γ is a random number within the range of 0 1 and α is a constant to control the size of the stochastic perturbation the global transition potential of a cell p g 1 1 exp z i j where z i j a k b k x k a is constant to be estimated and b k is the coefficient weight of the spatial driving variable both of which are calculated using logistic regression and x k is a spatial driving variable such as the distance to roads or the neighborhood influence of land use type the inheritance coefficient k ij is defined as a constant between 0 and 1 according to the inheritance ability of land use greater k values indicate a stronger likelihood of maintaining the original state the constraint factors of a cell c o n s i j t is a constant range from 0 to 1 for example if water source protection areas and basic cropland are forbidden to be developed as built up land the value of c o n s i j t is 0 s i j t is the state of cell i j at time t in each iteration loop the transition probabilities of each type of land use are calculated using equation 11 finally the land use type is determined according to the cellular automata transition rules and the number of land use types predicted by the markov chain in this study the cellular automata transition rule mainly uses the maximum transition probability and follows particular priority sequence rules depending on preliminary research as well as expert knowledge during the simulation a cell can only be allocated to a land use type and according to the maximum value of the transition probability the future type of land use in a cell is decided that is a land use type is successively allocated to cells as a descending sequence of transition probabilities of this land use type in all cells until the total demand for this land use type is satisfied in addition a cell will not be changed to another land use type if the cell is allocated to a land use type 2 3 study area and data the ganzhou district of zhangye prefecture is selected as the study area fig 2 ganzhou district is in gansu province china and is located in the zhangye oasis of the middle reaches of the heihe river basin it is a typical irrigated agricultural region and water resources are scarce its total area is 3603 83 km2 in the past five decades major changes in land use patterns have been observed in ganzhou district driven by changes in the water environment and human activities cheng et al 2014 the most significant land use changes in the study area are substantial urban expansion and the continued reclamation of cropland for economic interests hu et al 2015 these factors increase the stress on water resources and the environment threatening sustainable regional development a long term annual land use dataset from 1992 to 2009 is used to implement the land use assimilation scheme the land use dataset is derived using visual interpretation of 30 m resolution landsat tm etm images the overall accuracies for land use classification are greater than 90 based on field investigation and verification of high resolution images spot in typical areas for particular years following the national land use database of china nlud c classification system and land use characteristics in the study area the classification system in this study includes seven primary land use types cropland forestland grassland water body built up land wetland and desert the vector land use maps for all years are separately converted into raster format with a resolution of 30 m 30 m other data are spatial driving variables including distance based variables socioeconomic variables and physical attributes these spatial driving variables are obtained using various calculation methods table 1 and are closely related to land use changes based on preliminary logistic regression analysis all the spatial driving variables are unified to the same projection and resolution as the land use maps normalized values from 0 to 1 are then calculated using the maximum and minimum values after scaling the original spatial driving variable datasets 2 4 performance assessment criteria the performance of the land use assimilation system is evaluated using the root mean square error rmse which is calculated as follows 12 r m s e 1 n i 1 n 1 n j 1 n x i j o b s i j 2 where n is the number of land use types n is the dimension of assimilated state variables i e the patch number x ij is an assimilated or simulated value and obs i is an observed value in addition the accuracy of the simulated and observed land use maps is compared on a pixel by pixel basis in this study two accuracy assessment techniques are used the first is an error matrix using the predicted and observed maps the error matrix measures the overall accuracy oa and kappa coefficient the oa is the total number of correctly classified samples diagonal cells of the matrix divided by the total number of samples it measures the accuracy of the entire prediction map the kappa coefficient is a measure of agreement between model the predictions and observations congalton 1991 and is calculated using equation 13 the second technique is the figure of merit fm assessment which is used to calculate the model performance for the changed regions the fm is the ratio of the intersection of an observed change and a predicted change to the union of the observed change and predicted change its range is from 0 meaning no overlap between the observed and predicted change to 100 meaning perfect overlap between the observed and predicted change i e a perfectly accurate prediction pontius et al 2008 the fm is evaluated using equation 14 13 k a p p a m i 1 n x i i i 1 n x i x i m 2 i 1 n x i x i 100 where m is the total number of observations n is the number of rows in the error matrix i e the number of land use types x ii is the number of observations in row i and column i i e the diagonal elements and x i and x i are the marginal totals of row i and column i respectively 14 f m b a b c d 100 where a is the error area due to an observed change predicted as persistence b is the correct area due to an observed change predicted as change c is the error area due to an observed change predicted as the wrong gaining class and d is the error area due to an observed persistence predicted as change 3 experimental design and results 3 1 experimental design to evaluate the performance of the assimilation algorithm for categorical distribution a pair of parallel experiments a luca model without bayesian inference for categorical distribution assimilation algorithm and a luca model with bayesian inference for categorical distribution assimilation algorithm are designed first the luca model parameters are obtained using model calibration based on the historical change dataset during 1992 1999 second land use assimilation is implemented using bayesian inference for categorical distribution algorithm from 1999 to 2009 in addition the performance of land use assimilation may be affected by the landscape patch size and assimilation cycle employed the landscape patch size describes the spatial unit of the assimilated dataset while the assimilation cycle determines how often the assimilation should be implemented therefore the influences of the landscape patch size and assimilation cycle on the assimilation system are also analyzed finally the accuracy of the assimilation results is evaluated and analyzed based on the observation dataset in this study nine schemes are designed to explore the sensitivity to the landscape patch size and the assimilation cycle table 2 considering the size of the study area and the data accuracy three landscape patch scales are designed with spatial resolutions of 150 m 300 m and 900 m since the temporal range of modeling is from 1999 to 2009 three assimilation cycles including n a no assimilation 2 assimilation every 2 years and 5 assimilation every 5 years are designed assimilation performance is analyzed by different combinations of patch sizes and assimilation cycles in a pair based way 3 2 assimilation results and verification during the modeling period of 1999 2009 land use types in the study area remained basically stable the area proportion of land use change is as small as approximately 6 of the entire study area therefore on the whole the spatial distribution of various simulated and assimilated land use types is similar to the actual distribution fig 3 especially in 2001 which is the first year of assimilation the simulation and assimilation maps are closer to the actual maps however as the modeling progressed errors of the luca model continuously accumulate continuing until 2009 which is the end year of the modeling there are some discrepancies in the simulated map for example in the center of the study area urban sprawl is not simulated by the luca model moreover the simulation results of some patches for wetlands in the north of the city forestland in the east of the city and cropland on the edge of the oasis are different from the actual situation after the incorporation of observations the running trajectory of the luca model is adjusted the errors of the luca model are controlled and the assimilation results are improved aside from the qualitative comparison of the maps we also quantitatively compare the oa kappa coefficient and fm of the simulation and assimilation table 3 the table shows that the oa and kappa coefficient of the luca model in 2001 2005 and 2009 are 97 71 and 96 50 95 53 and 93 18 92 65 and 88 85 respectively the accuracy of the luca model shows a decreasing trend over time because of the accumulation of model errors compared with the luca model bayesian inference for categorical distribution assimilation algorithm can adjust the simulation results by incorporating observations into the simulation process therefore the accuracy is improved by relieving the accumulation of errors and the corresponding results are more reliable than those of the luca model the oa kappa coefficient and fm of bayesian inference for categorical distribution assimilation algorithm are improved by 0 05 0 07 and 1 72 in 2001 and are improved by 0 22 0 34 and 1 78 by 2009 compared with those of the luca model the rmse results also show that the assimilation effect is significant and the rmse is improved overall in 2001 and 2009 the rmse decrease by 0 058 and 0 139 respectively table 4 to further evaluate the performance of assimilation three fine scale areas of a b and c in fig 3 are selected within the study area the selection principle of fine scale areas is that land use type changes drastically and these changes are representative within the study area the three fine scale areas represent the changes in built up land water body and cropland affected by human behaviors such as wetland conservation policy or socioeconomic development the simulations of the luca model are poor in the three fine scale areas fig 4 and table 5 due to the insufficient expressions of national policy and socioeconomic activities in the model hu et al 2018 for example in the fine scale area a the development speed and extent of the built up land simulated by the luca model are lower than those of the actual land by the end year of the modeling in 2009 however in the simulation process the simulation results are constantly revised by incorporating observations and the expansion degree of built up land is significantly improved after assimilation the kappa coefficient ranges from 53 75 to 77 63 with an increase of 23 88 the fm ranges from 36 85 to 75 24 with an increase of 38 39 similarly there are also some obvious errors in the simulation of fine scale areas b and c in the fine scale area b the water body location and scope are affected by wetland conservation policy namely construction of a national wetland park the water body in the simulation is larger than that in the actual situation due to insufficient consideration of the impact of national policy in the luca model in fine scale area c on the edge of the oasis the pursuit of economic benefits led to a substantial increase in cropland however the inadequate expression of socioeconomic factors in the luca model leads to the limited expansion of cropland after incorporating observations using bayesian inference for categorical distribution assimilation algorithm the kappa coefficients in the fine scale areas b and c increase by 8 17 and 35 84 respectively the fm of the two fine scale areas increase 18 82 and 35 57 bayesian inference for categorical distribution assimilation algorithm can significantly improve the accuracy of the luca model and the assimilation results can better represent the actual situations 4 discussion the above analysis of the assimilation results reveals that data assimilation is an effective way to alleviate the propagation and accumulation of model errors because the revised model output serves as a more accurate input to the next simulation as a result the model with assimilation clearly outperforms that without assimilation however from the calculation process of bayesian inference for categorical distribution assimilation algorithm land use data assimilation needed to be a priori given the landscape patch size and assimilation cycle therefore is the assimilation performance affected by various landscape patch sizes and assimilation cycles here we analyze the sensitivity to landscape patch size and assimilation cycle 4 1 sensitivity analysis to landscape patch size fig 5 presents the rmse changes with a 2 year assimilation cycle at different landscape patch sizes from the simulation results of the luca model without data assimilation the rmses at the 900 m patch size are the smallest while those at the 150 m patch size are the largest the rmses at the three landscape patch sizes show an increasing trend as the modeling progresses this is because over time the model errors will be transmitted and accumulated resulting in a decrease in luca model performance this is consistent with previous research results chen et al 2016 li and gong 2016 garcía et al 2011 yeh and li 2006 after implementing bayesian inference for categorical distribution assimilation algorithm the rmses at the three landscape patch sizes are significantly reduced this is because observations are absorbed into the modeling process which weakens the errors of the luca model in addition similar to the simulation results of the luca model without data assimilation at different landscape patch sizes the rmses of finer patches are larger than those of coarser patches which is related to the scale effect of aggregated landscape patches with a finer patch scale there are fewer land use types especially in extreme cases and there may be only one type of land use enlarging the gaps between simulated and observed land use type densities however a coarser patch can obscure the spatial details of land use change within it which will weaken the characteristics of land use type changes resulting in smaller rmse values in addition the fm change shows that a finer patch scale is associated with a better performance of data assimilation fig 6 the fm curve with a patch size of 150 m is above the curves of the other two patch sizes for the target year of 2009 the fm value with a patch size of 150 m is approximately 0 52 higher than that with a patch size of 900 m although the rmse is larger for finer patch sizes the performance of bayesian inference assimilation algorithm is better than that with coarser patch sizes 4 2 sensitivity analysis to assimilation cycle in addition to landscape patch size the assimilation cycle is also a critical factor in land use data assimilation from the rmse changes at 150 m patch size with different assimilation cycles fig 7 rmses with assimilation cycles of 2 and 5 years are smaller than those of the luca model without assimilation achieving the corrections of land use types the rmses with a 5 year assimilation cycle are significantly larger than those with a 2 year assimilation cycle with the prolongation of the assimilation cycle the rmses increase and the performance of bayesian inference for categorical distribution assimilation algorithm decreases from changes of the fm values with a 150 m patch size but different assimilation cycles fig 8 the fm values after implementing bayesian inference for categorical distribution assimilation algorithm are larger than those of the luca model without data assimilation for the target year of 2009 the fm value with a 2 year assimilation cycle is improved by approximately 1 78 compared to that without data assimilation by comparing different assimilation cycles it can be found that the curve of the 2 year assimilation cycle is higher than that of the 5 year assimilation cycle specifically for the target year of 2009 the fm value with a 2 year assimilation cycle is approximately 0 30 higher than that with a 5 year assimilation cycle however this does not mean that the shorter the assimilation cycle is the better the performance of bayesian inference for categorical distribution assimilation algorithm due to the limitation of available observations the determination of an optimum assimilation cycle needs further study 4 3 problems and challenges this study attempts to update the multiple discrete state variables of a land use dynamic model by introducing the conjugate prior of dirichlet distribution into bayesian inference the proposed assimilation framework can significantly improve the simulation accuracy of the luca model by compensating for the insufficient expression of human behaviors in the luca model at three fine scale areas a b and c where dramatic and typical changes in land use type were affected by national policy and socioeconomic development the simulation accuracy of the luca model is poor because the model does not take full account of the effect of national policy and socioeconomic activities however the proposed assimilation framework makes the simulation results closer to the actual situation by combining observations compensating for the impact of human factors on land use types in addition the proposed assimilation framework can be extended not only to assimilate grid based remote sensing data and other socioeconomic indicators but also to assimilate pixel based landscape matrices as shown by relevant studies van der kwast et al 2011 the performance of bayesian inference for categorical distribution assimilation algorithm can be improved by simultaneously adjusting the assimilation strategy with varying spatial patch sizes and assimilation cycles however it does not imply that assimilation should be implemented with a finer patch size and a shorter assimilation cycle the optimum spatial patch size and assimilation cycle need further study due to the limitation of available observations additionally error is an important part of a data assimilation system thanks to the conjugate prior theory in bayesian inference we did not need to perform any nonlinear approximate approach such as an ensemble kalman filter and in theory the corresponding algorithm errors are therefore not introduced into the assimilation system in this study the assimilation technique reduces the errors caused by the stochastic perturbation in the luca model while ignoring the systemic uncertainties in the model structure and parameterization systemic uncertainty needs to be considered in future land use simulation research and it is also a large challenge for further research 5 conclusions bayesian inference for categorical distribution assimilation algorithm is introduced into a land use cellular automata luca model to update discrete land use type variables from the assimilation results the proposed assimilation approach can significantly improve the accuracy of the luca model compared with the results without data assimilation the oa kappa coefficient and fm with data assimilation increase by 0 22 0 34 and 1 78 respectively at the end year of the modeling in 2009 additionally in fine scale areas where drastic and representative changes in land use types were affected by wetland conservation policy or socioeconomic development the assimilation effect is more significant at fine scale area a the oa kappa coefficient and fm can increase by 14 10 23 88 and 38 39 respectively overall bayesian inference for categorical distribution assimilation algorithm can fuse the observations and simulation values of discrete and multiple land use types alleviating the propagation and accumulation of model errors the assimilation performance is affected by the landscape patch size and the assimilation cycle better assimilation performance corresponds to a finer landscape patch size and a shorter assimilation cycle however it does not imply that assimilation should be implemented with a finer patch size and a higher frequency the determination of the optimum landscape patch size and assimilation cycle needs further study due to the limitation of available observations additionally the systemic error in the luca model structure and parameterization needs to be considered in future land use simulation research and it is also a large challenge for further research in summary it is feasible to use bayesian inference for categorical distribution method to assimilate the discrete and multiple state variables of land use type and the results of assimilation are improved compared with those of modeling without assimilation the study contributes to a better understanding of the assimilation mechanisms of discrete variables and provides a general framework for assessing land use change by combining observations with a multiple land use dynamic model although the assimilation strategy was tested only with a land use model it can be easily applied to other models involving discrete variables such as economic and social models data availability all the data used in this study including land use channel population density and dem are available at the national tibetan plateau third role environment data center http data tpdc ac cn declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements the study was supported by the strategic priority research program of the chinese academy of sciences china grants xda20100104 the national science and technology major project of china s high resolution earth observation system china 21 y20b01 9001 19 22 the national natural science foundation of china china grant no 41801270 and gansu province science and technology program china grant no 21jr7ra053 
25649,geological modelling is an essential aspect of underground investigations with cross sections being one of the key aspects this modelling can be done by experienced geologists or using mathematical methods we present geopropy an open source decision making algorithm implemented in python that generates 3d cross sections the boreholes do not have to be aligned it performs as an intelligent agent that simulates the steps taken by the geologist in the process of creating the cross section coupled with data driven decisions the algorithm detects zones with more than one possible outcome and based on the level of complexity or user preference proceeds to automatic semiautomatic or manual stages geopropy could be the basis of a new simpler more comprehensible way of looking at geological models in industry and academia while at the same time creating the potential for using novel machine learning algorithms in geological modelling keywords decision making algorithm 3d geological modelling cross section open source python 1 introduction obtaining 3d subsurface geological models is of great importance in a wide variety of geoscience studies these models represent superficial and underground geological structures and the distribution of geological units and their purpose is to illustrate the existing underground conditions subsurface geological understanding and models are essential aspects of earth related industrial projects and academic investigations from mining and petroleum to hydrogeology and environmental studies hawie et al 2021 kerrou et al 2017 pasculli et al 2014 yang et al 2021 in general geological models can be described from implicit or explicit points of view implicit modelling includes data driven methods that use datasets derived from measured features and algorithms explicit modelling mostly relies on expert opinion experience and interpretation where the expert usually builds cross sections surfaces or volumes by interpolating the accessible data randle et al 2018 to date various works have presented and discussed the use of implicit methods including cokriging based modelling calcagno et al 2008 gonçalves et al 2017 hillier et al 2014 lajaunie et al 1997 volume based modelling iskenova et al 2016 souche et al 2013 kinematic modelling brandes and tanner 2014 caumon 2010 and others lemon and jones 2003 muzik et al 2015 in addition open source software such as noddy pynoddy florian wellmann et al 2016 jessell and valenta 1996 and gempy de la varga et al 2019 schaaf et al 2020 and commercial software such as gdm suite brgm 2020 move petroleum experts 2019 vulcan 3d eureka maptek 2021 leapfrog seequent 2021a and oasis montaj seequent 2021b are powered by these methods most of these computer programs contain more than one method and although they are known as implicit modelers they benefit from explicit modelling in some stages of geological model evaluation for explicit modelling there are computer programs that support experts in generating cross sections some examples of open source software are heros velasco et al 2013 and heros 3d alcaraz et al 2016b the midvatten qgis plugin källgården and spångmyr 2020 grass gis grass development team 2020 and geomodelr serrano 2019 there are also commercial software programs that support explicit modelling such as rockware gis rockware 2020 surpac dassault systèmes 2021 datamine 2021 geoscene3d i gis 2020 bgs groundhog desktop british geological survey 2020 and gsi3d cullen et al 2010 one can also use other characteristics to categorize these programs the amount or type of information used to build a model the proportion of knowledge or data that drives the model the level of automation the focused scope the computational performance or the degree of specialty needed to use them bryant and flint 2009 despite the advancements in implicit geological modelling tools approaching 3d modelling and cross section generation by relying on geologists experience is an acceptable outcome in many scenarios which is why it is still used in numerous ongoing projects in industry and the academic community alike despite the diversity of methods and computer programs available we believe that there are some shortcomings that have not been fully addressed to date 1 one common key aspect of generating geological models is database integration modifying databases from well known formats to a specifically structured shape could be time consuming specifically in complex projects with a considerable amount of data velasco et al 2013 2 projects normally need various cross sections so generating them could be a time consuming procedure 3 in explicit modelling the lack of geologist experience could result in outcomes that are far from the reality of the region kumar 2021 4 there are many scenarios with more than one possible outcome freedom in the style and bias of each expert could play a role in determining the final cross sections which will result in nonunique solutions bowden 2004 points three and four may result in inconsistent models especially in projects where more than one geologist is working on cross sections randle et al 2019 which could potentially result in decreasing the feasibility of using the explicit models 5 the lack of experience of the geologist with mathematical modelling methods specifically in moderately complex scenarios could make it challenging to follow the steps and interpret the cross sections 6 some advanced packages do not provide extensive information about the procedures and methods undertaken without purchasing a licence e g petrel schlumberger 2015 geomodeller intrepid geophysics 2020 go cad paradigm 2015 7 the rapid development of artificial intelligence and machine learning libraries e g tensorflow abadi et al scikit learn pedregosa et al 2011 pytorch paszke et al 2019 pymc salvatier et al 2016 suggests a potential for expandability in geological modelling software however some existing computer programs are limited in this matter e g earthvision dynamic graphics 2009 rockworks strater golden software 2021 and the developments are mostly focused on reservoir properties chai et al 2021 chaikine and gates 2021 idrees et al 2021 or environmental modelling criollo et al 2019 hörning and haese 2021 volk and turner 2019 white et al 2021 developing open source software programs based on widely used programming languages brings the possibility of i understanding and replicating the undertaken procedures ii customizing the code for further analysis or different purposes in case of need and iii dynamically improving the software by implementing the latest version of machine learning and decision making methods that have already been developed by third parties heron et al 2013 moreover using a standardized known database could ease the process of introducing data to the software allen et al 2008 more importantly to the best of our knowledge there is no algorithm that aims to reproduce the steps of creating a cross section by the same approach used by a geologist which we think is worth considering trying and developing for research and industry in this work we address some of the shortcomings described above by developing geopropy an open source python based van rossum 1995 library to generate consistent 3d geological cross sections by hybrid implicit explicit methods geopropy completely or partially automates the workflow of the geologist to support the decision making process accompanied by data driven decisions the objectives of geopropy are as follows 1 it aims to identify the uncertainty caused by complex structures or a lack of data by detecting zones with more than one possible outcome and facilitating the decision making process in these zones for the user 2 the algorithm tries to act as an intelligent agent that emulates the decision making steps of a geologist by following the geological complexity with three different degrees of freedom a if the algorithm detects that the available information results in a unique outcome the cross sections will be generated automatically b if the algorithm determines that there is more than one possible outcome for the cross section it proceeds to the semiautomatic stage in which it asks for decisions on how to complete the geological unit contacts c if new geospatial information or more complex decisions are needed to complete the cross section the algorithm enters the manual stage to complete the cross section although fundamental uncertainties in geological and environmental models and various studies around them are of great importance bond 2015 jessell et al 2016 refsgaard et al 2007 wellmann and caumon 2018 wellmann and regenauer lieb 2012 white et al 2016 uncertainty is currently not reflected in geopropy in the next sections we will describe the methods applied in this work followed by the details of the geopropy code and the database modifications to evaluate the algorithm we will demonstrate the application of the algorithm on three synthetic datasets and compare it with explicit methods 2 methods in this section we will clarify geological unit contact types to unify the definitions for the machine and the geologist and then we will explain the database that is used in geopropy we will discuss the extraction of the logical steps for cross section generation based on the geologist s decision making steps 2 1 general definitions to translate the thinking of the geologist to a machine language unified assumptions are essential with that in mind various geological structures are defined so that they are distinguishable by the geologist and geopropy simultaneously i a conformity conformable contact represents continual uninterrupted deposition and accumulation of sedimentary rocks without a gap in the geologic record ii an unconformity unconformable contact is a type of stratigraphic contact between a specific sedimentary material in the top layer and other older units that are not chronologically correlative as the bottom layer this contact may cut previous surfaces unconformity contacts include disconformity nonconformity angular unconformity and paraconformity contact types geopropy recognizes all unconformity contact types as the same group iii an intrusion is an intrusive body of rock that solidifies inside existing geological units an intrusion has priority with respect to the units it intrudes into since it forms after them iv faults are defined as surfaces bounding any kind of geological unit that cuts any other previous unit and they imply a relative movement between the two blocks at each side of the surface davis et al 2011 using these definitions geopropy determines the zones that contain complex structures such as reverse faults folds and repetition of the same geological unit in a borehole we name these regions critical zones and the respective points critical points these critical zones could be the source of multiple cross sectional outcomes for a specific dataset 2 2 database an increasing volume of data in earth science demands a database with the ability to integrate various types of information for further practice vázquez suñé et al 2016 to ensure a dynamic workflow and the replicability of the analysis it is advised to manage the data collected in a standard database in addition flexibility in use with several tools and user friendliness are other properties of an adequate database we use the hydor data platform as the main geopropy database to continue the developments made by velasco 2013 alcaraz 2016 and criollo 2019 hydor was built as a personal geodatabase esri that can be used as a relational microsoft database which facilitates data management with a user friendly interface such as microsoft access software microsoft 2019 the arcgis platform esri 2021 or qgis qgis development team 2021 hydor has been widely used and tested vázquez suñé et al 2016 its data tables are homogenized and it cross analyses different types of information such as geology hydrogeology and environmental information which could improve the validation of conceptual models in addition there are tools such as heros heros 3d quimet velasco et al 2014 arcaraz alcaraz 2016 metrogeother alcaraz et al 2016a mj pumpit and hyyh criollo et al 2016 that have already been designed based on this database the hydor design follows established guidelines such as onegeology 2012 ogc standards 2012 ogc waterml 2 0 2012 and inspire 2013 2 3 steps of drawing a cross section to map out the automation of cross section construction by following the same strategy a geologist would we performed an exercise in which several geologists of diverse levels of expertise 4 university professors in the geological modelling field 5 bachelor s students of geology and 2 phd candidates in the geoscience field drew a cross section based on the same dataset refer to section 4 1 so that they could describe their thinking steps and considerations while performing a geological interpretation in general they proceeded with the following steps 1 identification of the geological units present in the area and their relative ages and interpretation of the types of contacts that separate the units in the boreholes 2 consideration of the outcroppings and ground surface data and flagging the critical points that can affect the layer contacts 3 focusing on complex structured zones such as folds and reverse faults keeping in mind the geological maps relative age of the layers and critical ground surface points at this stage the geologist may have one or more possible outcomes to complete the contacts respecting all boundary conditions the contacts will be determined by considering all the information the experience of the geologist and personal preference then the geologist completes the cross section by depicting relatively simple unit contacts there are some simple and logical assumptions extracted from the steps that a geologist considers in constructing the cross section if one contact of two specific geological units for example the a b contact in boreholes 1 and 2 exists in two consecutive boreholes the boundary between the two geological units orange and black lines must be defined by either a straight or a curved line in the case of the intersection of two geological contacts following hutton s cross cutting relation principle hutton 1795 the younger contact has priority over the older contact since it is derived from a newer geological event fig 1 blue dot if there is no unit contact information that forces the position of the contact the geologist obtains the apparent dip angle and tries to complete the contact using it or tries to continue the unit contact with the same angle if this contact unit already exists in the previous boreholes it is possible that using these two options will result in an inconsistent unit contact that cannot be verified by other available information or angular data that are not available the dashed red line and dashed brown line in fig 1 are examples of drawing a line by using the apparent dip angle and by using the same angle as the yellow solid line respectively and none of them can be validated by existing geological units in this case the personal preference of the geologist plays an important role in determining the unit contact the geologist could connect it to the younger contact at any point in the form of a curved or a straight line the blue line in fig 1 shows an example of a contact that could be validated by existing information in the next sections we will discuss the different aspects of the geopropy code structure followed by an application of the algorithm on three synthetic datasets to evaluate the functionality of the code by comparing it to the explicit cross sections drawn by the geologist 3 the geopropy code the program is a hybrid implicit explicit type as it is designed to proceed automatically whenever possible that is whenever there is only one possible solution with the data available however it switches to a manual mode or asks for help from a geologist when there is more than one outcome in this section we will describe the architecture of geopropy followed by explaining the modifications of the hydor database and required input data we will discuss the workflow of the algorithm in cross sections containing simple and complex geological structures with an example and we will show the available outputs and visualization options 3 1 geopropy architecture as an intelligent agent that tries to simulate the same stages as a skilled geologist geopropy is programmed to proceed as follows first it extracts the geological units contact points then it examines the ground surface points and outcroppings to generate the ground surface contacts and to determine critical ground surface points afterwards it analyses the contact points to detect repetitions of units in the same borehole polarities faults the possibility of existing folds and the locations of faults and marks the critical zones then for each critical zone the algorithm proceeds to the automatic stage if there is just one possible solution if there is more than one potential outcome the algorithm asks the user in two substages to identify the desirable scenario in the critical zone primarily by asking the user to choose the preferable solution between the existing options if this information does not result in a unique outcome the algorithm proceeds to the manual stage where the user has to introduce more information the workflow of the critical zones will be explained in the next sections in detail after analysing and determining the state of the contacts in the critical zones and accounting for the temporal sequence of geological events the rest of the unit contacts will be completed based on the mentioned guidelines it should be noted that it is possible to modify the cross section in a postprocessing stage with any software that is compatible with 3d shape files 3 2 geological input data and hydor database modifications the geological dataset in hydor includes information related to borehole properties samples lithology and definitions of geological units and subunits geopropy uses lithological and borehole data the lithology data table contains the borehole sample depth and geological units whereas the borehole data table contains information about the locations of the boreholes fig 2 one of the requirements of processing the geological data similarly to using the criteria of a geologist is to have the same level of information available to the user in the decision making procedure therefore chronological data fault data and ground surface data tables are added to the geological section of hydor to fulfill this need fig 2 green tables 3 2 1 chronological fault and ground surface data the chronological data table is designed to contain information regarding the priority of geological event occurrence table 1 based on the type of geological contact data must be introduced taking into account the concepts explained in section 2 1 the information in this table can follow four scenarios i if the contact is a conformity the top and bottom units are known and must be introduced in the top layer and bottom layer fields ii for an unconformity the top layer is known but the bottom layer s could be dissimilar units thus information about the bottom unit does not need to be added iii in the case of faults apart from the priority no information about the top or bottom unit is needed additional information about faults is located in the fault data table iv in the case of intrusions the top layer field has to be filled by the intruding unit and the bottom layer field does not need any information a priority number is defined to determine the chronological sequence of the geological contacts a priority number of one indicates the oldest contact and the highest prority number is the youngest the angle field is optional information that determines the apparent dip of the contact with 180 degree variation table 1 illustrates the various examples of contact types in the chronological data table note that chronological data can be extracted from geological maps or other regional studies tuned by user experience the fault data table table 2 must include the priority number that is already assigned in the chronological data table and the borehole id and the elevation of the fault inside the borehole in the borehole id and elevation fields respectively it is also optional to add an apparent fault dip in case of availability the ground surface data table table 3 consists of two groups of points the points with only geospatial information table 3 grey row and the outcroppings or unit contacts that are extracted from geological maps or field surveys in the latter the priority number of the unit contact point must be identified fields x y and z define the geospatial information of the sample polarity is an optional field that can be normal or reverse reverse polarity occurs when the unit contact defined in priority number is observed but the sequence of the unit is reversed angle is an optional field to add the apparent dip of the unit contact 3 3 geopropy workflow the programming language used for the code presented is python geopropy can be executed on windows and linux os based computers the code will interact with databases in personal spatial geodatabase esri format the first step of running geopropy is to complete the specific hydor database as mentioned in section 3 2 once the main function is executed geopropy reads the input information extracting the outcroppings ground surface lithological unit contacts and chronological information then the tool identifies 2d planes between every two consecutive boreholes and defines the boundary conditions respecting the arguments chosen by the user fig 3 shows the simplified schematic decision making algorithm of geopropy next accounting for the relative age of the unit contacts geopropy identifies the zones defined as critical with more than one possible scenario in the ground surface and subsurface to complete the geological unit contact lines the state of the critical zones determines the complexity of the process the yellow box in fig 3 shows the procedure for each critical zone if there is no identified critical zone geopropy passes directly to the automatic stage if there are critical zones geopropy first processes each zone in the semiautomatic stage some information will be shown to the user contact unit info critical zone boundaries faults that are related to the unit contacts or exist within the critical zone the point table that contains the specific point ids for each critical point and the instruction to choose between points or to proceed to the manual stage in the semiautomatic stage the user can choose the sequence of points in the critical zone that have to be connected in the form of a python list if the contact in the critical zone has more than one connected piece each piece can be separated by determining the separate keyword in the list see listing c 1 in the appendices the semiautomatic stage is accompanied by a visual representation of the completed parts of the cross section and the new information see fig a 4 in the appendies this semiautomatic stage does not require any new additional input the new information added in this stage by the user is a decision if the new decision does not result in a unique outcome for that zone or if the user wants to determine the points related to the unit contacts manually the zone will be passed to the third manual stage listing c 2 in the appendices here geopropy can accept more geospatial information with the purpose of either singling out the outcome or accounting for specific manual considerations such as adding a contact point in a space between boreholes that is not based on the data in the database different options are available in the third stage 1 connect existing points using the point ids 2 define new contact points and connect them to other new points or an existing point 3 connect points in the same borehole from the left or right sides which is normally useful if there is a change in the polarity of the unit in a borehole due to folds for example and 4 skip making a decision for some points and let the algorithm complete the critical zone automatically the new information added in the manual stage by the user can be geospatial information a group of decisions and or a determination that the algorithm has to complete the third stage by itself finally geopropy completes the cross section respecting all the geological units to evaluate the contacts the resulting geological areas in the cross section will be compared to the raw data to determine whether all the raw data points are correctly situated in their respective zones or whether there are areas containing more than one geological unit moreover if there is an area in which the geological unit is not identified by geopropy it will be flagged for further analysis by the user at this point a 3d visualization of the preliminary result will be displayed an example of this visualization can be found in fig a 1 in the appendices some stages of the algorithm are described by a simple synthetic cross section fig 4 demonstrates the steps that geopropy follows to generate it and the results this cross section contains conformable geological unit contacts an unconformity contact and a normal fault as shown in the coloured bars in fig 4 1 tables b1 to b4 in the appendices show the input data this database is designed to visualize the simple decision making process of geopropy with a limited number of geological layers and boreholes after preprocessing the input data the algorithm extracts the unit contact points coloured dots in fig 4 2 and 4 3 based on the chronological data table table b3 in the appendices then it calculates the 3d properties of the cross section defines the boundary conditions and processes the ground surface data the side boundaries of the cross section are the first and last introduced boreholes the top boundary is defined based on the borehole and ground surface data the preliminary bottom boundary is defined by a specific ratio of the maximum depth of the deepest borehole the dark blue and violet rectangles in fig 4 2 show the top and preliminary bottom boundary points respectively note that after creating all unit contacts the bottom boundary will be modified by finding the maximum depth of the critical points and units between each pair of consecutive boreholes and reducing the area of the units at the bottom to decrease the areas without available information the difference between the preliminary and final bottom boundaries is shown by the violet line in fig 4 3 and the cross sectional bottom boundary line in fig 4 this cross section does not contain any critical zones so the algorithm directly passes to completing the cross section automatically fig 4 3 shows how geopropy completes the cross section in 2 steps first determining the unit contacts that have to be connected because they have just one other equal contact point on each side green lines then determining the yellow contact lines which the algorithm decides how to generate based on the relative age of the layers orientation data and other information introduced in the database after the completion of the cross section each geological body is determined in the form of a polygon or multipart polygons each colour and polygon name determines a specific geological material the resulting geological bodies polygons are validated by comparing the polygons with the input geological unit data that are inside the polygon if the resulting geological bodies cannot be validated by the input data they will be flagged so that the user can analyse them in postprocessing finally the outputs will be generated as shown in fig 4 4 although the top view of the boreholes does not have to be on a straight line the general orientation of the boreholes has to follow a line in such a way that the same apparent dip angle for a contact through the whole cross section results in negligible error this is because for each unit contact the apparent dip angle does not change between borehole locations since geopropy draws a line between the holes based on the needs the user is able to ask for desired output shape files the available options are unit contact lines 3d vertical polygons for each geological unit a 2d projection of the 3d cross section and a curved version of the unit contact lines to facilitate geopropy usage one main method function in the python language is designed to run the tool the main method is executed with just two mandatory parameters the input hydor database directory which stores the geological information and the desired borehole ids the other variables are optional i precision related adjustments ii user preferences and iii visualization preferences the developer mode allows the user a more powerful error handling option refer to section 6 1 for access to the library help for information about the method arguments 3 4 output and visualization for each critical zone in the semiautomatic stage there is a 3d visual guide to illustrate the information graphically geopropy shows the final cross section in a 3d environment these two sets of graphs are based on the matplotlib python library hunter 2007 refer to appendix a for some examples various visualization options are available on demand for each cross section in addition to 3d shape files containing interpreted geological units it is possible to generate 2d projections of the cross section this is beneficial for illustration purposes especially in the absence of a computer program that can visualize 3d shape files each visualization option generates a database that contains geological unit polygons as shape files unit contacts as polylines and key coordinates as points depending on the visualization platform in use it is also possible to modify the graphical details such as the colour boundaries and line type additionally there is a built in option to smooth the geological unit contacts using curved lines on demand this option is designed for use in 2d and is only an aesthetic tool smoothed cross sections are not recommended since they result in decreased model accuracy which could cause potential difficulties in the subsequent processing stages such as fence diagrams and 3d volumes the results can be displayed in any computer program that supports 3d shape files such as arcgis or qgis although there are certain shortcomings in 3d data processing and visualization editing polygons in 3d arcmap documentation we used the arcpy python library to configure and optimize the output shape files 4 application to synthetic datasets three different synthetic datasets with varied properties are designed in 2d for better visualization each dataset was input to geopropy and given to a geologist to generate the cross section the geologist used heros to visualize and create the cross sections the first dataset is described in section 3 3 to demonstrate the functionality of the algorithm in this database there are geological units that do not exist in all boreholes so they have to terminate according to the guidelines and assumptions the second synthetic dataset contains conformities unconformities normal faults and an intrusion the third dataset contains an intrusion a reverse fault in conformable contacts and another reverse fault that crosses through a fold this dataset focuses on testing the semiautomatic and manual stages of geopropy when there is more than one possible correct explanation for the same dataset in each zone whether it is an intrusion zone or fold fault zone 4 1 synthetic database 1 4 1 1 dataset overview and input the database directory and borehole ids that are mandatory arguments to execute geopropy correspond to tables b1 to b4 in the appendices which describe the geological specifications of the example it is assumed that the only available data in this example correspond to boreholes that is there are no ground surface data available 4 1 2 results and comparison fig 5 shows the cross sections completed by a geologist and geopropy for the most part the two cross sections are similar although there are some differences 1 units e and d have different forms the termination of a unit contact in the absence of information can be done in various ways in this case the geologist chooses a smoother line whereas geopropy draws a straight line into the middle of two boreholes 2 similarly the intersection of units a b and c red boxes in fig 5 is slightly different and again is not smoothed by geopropy 3 layer f which appears only on the boreholes on the left side of the fault is not continued by geopropy on the footwall since there is no information in the database to verify that this layer continues on the right side of the fault whereas it is drawn there by the geologist fig 5 1 4 regarding the depth of the cross sections geopropy has diverse ways of interpreting the deepest boundary of the cross sections refer to section 6 1 for access to the geopropy documentation the first two differences can be considered as simply dissimilarities in styles 4 2 synthetic database 2 4 2 1 dataset overview and input this dataset is inspired by earth science student exercises it includes conformable and unconformable contacts with older faults and an intrusion of relatively complicated geometry the coloured bars in fig a 2 in the appendices illustrate the raw borehole data tables b5 to b8 in the appendices show the data used for this example 4 2 2 results and comparison the cross sections generated by the geologist and geopropy are compared in fig 6 similar to the first example the general geometry of the two cross sections is very similar the intrusion unit a in yellow in fig 6 contacts are smoother in the geologist s cross section than in geopropy s cross section in addition in the two contact zones between the intrusion and unit n coloured dark blue in fig 6 the geologist softened it to draw a zone contact in contrast to the singular contact points in geopropy this is the major difference between the two and it could also be considered a personal style related decision 4 3 synthetic database 3 4 3 1 dataset overview and input the goal of this example is to challenge geopropy with scenarios with critical zones that can lead to several possible outcomes fig 7 illustrates borehole and ground surface data which in turn contain two sets of ground surface information at some points only the altitude is available black points whereas at others there is information about the unit contact fig 7 zone i coloured points each point indicates a unit contact and each colour indicates the unit that has been observed zone ii includes a reverse fault that causes repetition of units a b and c in all boreholes of the zone zone iii is a region with complex geometry that could be interpreted in different ways there must be folds and a reverse fault fig 8 1 and 8 2 zone iv illustrates an intrusion that can have more than one possible geometry refer to section 6 2 for the details of how to reproduce the example geopropy detects that the dataset does not result in a unique outcome immediately after introducing the dataset directory and the borehole ids thus it starts an interaction with the user asking for complementary information this interaction is discussed below tables b9 to b13 in the appendices show the tables in the dataset list 1 shows how to execute the function list 1 executing synthetic dataset 3 using the geopropy library note that the default arguments of the cross section method are used for more information refer to section 6 1 list 1 4 3 2 results and comparison the interpretations of zone i by the geologist and geopropy are fairly similar fig 9 the geopropy contacts are more angular whereas the geologist s contacts are more rounded or smoothed as in the previous examples in zone ii fig 7 where units a b and c are repeated in the boreholes the existence of the reverse fault shapes the geometry of the zone therefore although geopropy identifies this region as a critical zone it does not ask for more information since the outcome is unique fig 9 the results obtained in critical zone iii fig 7 contain repetitions of the geological units but the unit contacts can be interpreted in more than one way in addition there is a reverse fault crossing this critical region fig 8 1 and 8 2 show two of the possible outcomes which were drawn by the geologist to reach a unique solution in this critical zone geopropy proceeds to the semiautomatic stage where the user has to choose one of the possible scenarios based on preference or experience listing c 2 in the appendices fig a4 in the appendices shows the visual 3d helper of geopropy used to facilitate the procedure since the information provided by the user in the semiautomatic stage results in a unique solution in the critical zone geopropy verifies the uniqueness of the critical zone and does not enter the manual stage fig 8 1 shows the geometry of the units generated by the geologist in critical zone iii which is also introduced to geopropy the comparison between fig 9 1 and 9 2 in zone iii illustrates the difference between them which is the lack of smoothness especially for the contact points between unit b and the fault and respecting the thickness of unit b in the geologist s interpretation the intrusion in critical zone iv could be interpreted in various forms fig 8 3 and 8 4 according to the geologist geopropy detects the nonuniqueness of the zone and proceeds to the semiautomatic stage to ask for more information from the user which could lead to different cross sectional outcomes based on different choices of the user as shown in fig 8 5 8 6 and 8 7 fig 8 6 and 8 7 are generated in the semiautomatic stage whereas fig 8 5 is generated in the manual stage in addition it is possible to introduce new sample points to the zone using spatial data depending on the user needs and available data listing c 3 in the appendices shows the geopropy user interaction in the semiautomatic and manual stages there are some differences between the interpretations of the geologist fig 9 1 and geopropy fig 9 2 smoothness is the first difference especially around folds and intrusions the unit contact angles also differ in the absence of angular data the geologist relies on experience and personal style whereas geopropy calculates the apparent dip in the unit contacts based on the contact lines across the whole cross section to preserve the orientation behaviour of the unit it must be mentioned that identifying the available options and being able to apply additional information offers freedom to the user to obtain results that fit expectations figs a3 and a 5 in the appendices illustrate two other possible outcomes created by the geologist and geopropy respectively 5 discussion and conclusion geopropy is an open source python library that performs as an intelligent agent to create 3d geological cross sections based on a knowledge and data driven approach as seen in the three presented examples figs 5 6 and 9 geopropy s cross sections preserve the overall characteristics of a real geologist s cross sections although small differences mostly in the smoothness of contacts can be observed the application of geopropy to synthetic profiles validates the functionality and the decision making procedure making it a useful support tool for geologists the use of geopropy outputs in a gis platform may aid users in modifying and customizing the results with the capabilities available on each gis platform for instance using a gis fence diagrams or geological volumes can be easily generated which can be used later in numerical modelling software nevertheless geopropy has some limitations first it is highly sensitive to the data geopropy always respects the input data whereas in some scenarios a geologist may decide that an observation is not reliable or not compatible with the properties of the region and may ignore or modify the data in geopropy this step has to be carried out by the user second although geopropy is an open source library it is not completely free to use since it depends partially on the arcpy library to generate the shape files this means that to execute geopropy the user needs to have access to an arcgis licence the reason that other freely available libraries were not used to create shape files is that there was no free and open source library that could reliably support 3d shape file generation even with the shortcomings of the arcpy library we considered it the most suitable library for geopropy in addition there is a vast range of tools developed and available in arcgis that could be coupled with geopropy or used in the postprocessing of the results third in the case of high variation of the angles among the vertical planes between each consecutive borehole pair in a cross section the assumption of the same apparent dip angles for one contact along the whole cross section in 3d could result in considerable errors fourth geopropy does not necessarily preserve the thickness of the geological units it is assumed that by preserving the unit contacts and the unit angles the output unit thickness will be accurate but thickness is not considered an ascertainable factor in generating the cross sections developing geopropy in a widely used programming language and making the source code available create the possibility for further development of the library and coupling the program with other existing decision making tools in addition the use of a standard and well known database with various tools depending on it facilitates the integration of different aspects of the study and saves the time needed to prepare databases for each tool moreover by following specific guidelines that are derived from the thought process of a geologist i the process of generating the cross section and the results can be easily interpreted since the algorithm acts similarly to the user s thinking steps ii the code controls inconsistencies related to personal style and bias which are important in studies that involve more than one person or that continue for a long period and iii the code can help inexperienced users avoid the unrealistic results that could occur when complicated mathematical modelling techniques are used geopropy is easy to implement and keeping in mind that creating large numbers of cross sections is time consuming it could be used to speed up the process of creating geological cross sections overall this tool could be of great help when geological modelling must be done explicitly as it can avoid inconsistent decision making in addition to the time benefits and avoidance of inconsistencies another valuable asset of geopropy is that it detects zones with several possible outcomes in a cross section this could potentially help users analyse different plausible geological scenarios based on the available data this also holds in the absence of orientation data which brings flexibility to the tool at the cost of precision geopropy does not replace implicit models but it would help in generating cross sections explicitly as an intelligent agent 6 software and data availability 6 1 geopropy library information available to download freely in https github com idaea evs geopropy under agpl 3 0 license year first available 2021 dependencies arcpy arcgis 10 5 or higher matplotlib pypyodbc programming language python developed by ashkan hassanzadeh contact information ashkan hassanzadeh csic es refer to https github com idaea evs geopropy wiki for additional information about the installation default values of the arguments the explanation and the usage 6 2 synthetic examples databases of 3 synthetic examples in this article are available free of charge in mdb format alongside the jupyter notebook in https github com idaea evs geopropy declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements the authors acknowledge miguel lópez blanco patricia cabello carlos ayora and two anonymous reviewers that helped us to improve this article this research did not receive any specific grant from funding agencies in the public commercial or not for profit sectors idaea csic is centre of excellence severo ochoa project cex2018 000794 s r criollo gratefully acknowledges the financial support from the balearic island government through the margalida comas postdoctoral fellowship programme pd 036 2020 appendices appendix a additional figures fig a 1 a 3d cross section based on synthetic database 2 note that the geospatial properties of the boreholes change from synthetic dataset 2 to demonstrate the 3d capabilities of geopropy fig a 1 fig a 2 raw data of synthetic dataset 2 fig a 2 fig a 3 cross sections of synthetic dataset 3 created by the geologist based on the available data the geological units in critical zones iii and iv could be interpreted in various ways fig a 3 fig a 4 screenshot of the semiautomatic stage visualization of synthetic dataset 3 the interactive visualization tool helps the user identify the faults contact points and respective point ids the provisional scheme of unit contacts that are already created is shown by lines in different colours fig a 4 fig a 5 cross sections of synthetic dataset 3 created by geopropy the user decided to choose the semiautomatic stage preferences in critical zones iii and iv fig a 5 appendix b synthetic datasets tables table b 1 borehole data table of the synthetic dataset 1 table b 1 borehole id x y elevation 1 0 0 2 2 21 21 1 3 27 27 0 4 40 40 0 5 46 46 0 6 53 53 0 7 62 62 0 table b 2 borehole unit data table of the synthetic dataset 1 table b 2 borehole id top depth bottom depth units 1 0 2 e 1 2 3 a 1 3 9 b 1 9 11 3 f 2 0 2 a 2 2 2 fault 2 2 4 a 2 4 9 3 b 2 9 3 11 f 3 0 5 a 3 5 8 b 3 8 9 5 b 3 8 8 fault 3 9 5 11 f 4 0 6 a 4 6 13 8 b 5 0 5 a 5 5 7 c 5 7 10 3 b 6 0 5 a 6 5 8 c 6 8 12 b 7 0 5 a 7 5 6 2 d 7 6 2 9 1 c 7 9 1 12 1 b 7 12 1 14 1 b table b 3 chronological data table of the synthetic dataset 1 table b 3 prority number bottom layer top layer type preferred angle 1 f b conformity 2 b c conformity 3 c d conformity 4 a unconformity 5 fault 6 a e conformity table b 4 fault data table of the synthetic dataset 1 table b 4 priority number borehole id elevation preferred angle type 5 2 2 45 fault 5 3 8 45 fault table b 5 borehole data table of the synthetic dataset 2 table b 5 borehole id x y elevation 111 0 0 1 112 5 5 1 5 113 10 10 2 114 15 15 2 5 115 20 20 2 25 116 25 25 2 117 32 5 32 5 1 8 118 37 5 37 5 1 5 119 42 5 42 5 0 1110 47 5 47 5 1 1111 53 75 53 75 1 15 1112 60 60 1 3 1113 66 25 66 25 1 3 1114 72 5 72 5 1 3 1115 77 5 77 5 1 3 1116 81 25 81 25 1 3 1117 85 85 1 3 1118 88 75 88 75 1 2 1119 92 5 92 5 1 1 1120 97 5 97 5 1 1 table b 6 borehole unit data table of the synthetic dataset 2 table b 6 borehole id top depth bottom depth units 111 0 1 f 111 1 5 16 j 111 5 16 11 4 d 111 11 4 15 56 v 111 15 56 17 4 l 111 17 4 20 15 g 111 20 15 21 e 112 0 1 5 f 112 1 5 6 2 j 112 6 2 12 44 d 112 12 44 16 6 v 112 16 6 21 7 l 112 21 7 24 2 g 112 24 2 41 5 e 112 41 5 41 5 fault 112 41 5 43 e 113 0 2 f 113 2 3 n 113 3 8 24 j 113 8 24 14 48 d 113 14 48 18 v 113 18 18 5 c 113 18 5 27 64 l 113 27 64 30 39 g 113 30 39 35 39 e 113 35 39 35 39 fault 113 35 39 38 e 114 0 2 5 f 114 2 5 4 5 n 114 4 5 9 74 j 114 9 74 15 98 d 114 15 98 20 14 v 114 20 14 22 5 c 114 22 5 28 64 l 114 28 64 30 e 114 28 64 28 64 fault 115 0 2 25 f 115 2 25 5 6 n 115 5 6 10 84 j 115 10 84 17 08 d 115 17 08 21 24 v 115 21 24 22 75 c 115 22 75 22 75 fault 115 22 75 27 4 l 115 27 4 30 15 g 115 30 15 32 e 116 0 2 f 116 2 5 62 n 116 5 62 11 36 j 116 11 36 17 6 d 116 17 6 21 76 v 116 21 76 25 8 c 116 25 8 31 26 l 116 31 26 34 01 g 116 34 01 36 e 117 0 1 8 f 117 1 8 6 68 n 117 6 68 12 72 j 117 12 72 19 5 d 117 19 5 24 8 v 117 24 8 30 12 c 117 30 12 37 62 l 117 37 62 40 37 g 117 40 37 42 e 118 0 1 5 f 118 1 5 7 14 n 118 7 14 12 86 j 118 12 86 20 1 d 118 20 1 25 5 v 118 25 5 32 84 c 118 32 84 40 l 118 40 41 a 119 0 1 k 119 1 7 4 n 119 7 4 13 12 j 119 13 12 19 36 d 119 19 36 25 52 v 119 25 52 33 c 119 33 35 a 1110 0 1 f 1110 1 3 6 k 1110 3 6 10 n 1110 10 15 08 j 1110 15 08 21 32 d 1110 21 32 25 v 1110 25 27 a 1111 0 1 15 f 1111 1 15 5 05 k 1111 5 05 10 5 n 1111 10 5 13 04 j 1111 13 04 15 a 1112 0 1 3 f 1112 1 3 6 5 k 1112 6 5 11 n 1112 11 12 a 1113 0 1 3 f 1113 1 3 7 8 k 1113 7 8 13 75 n 1113 13 75 15 67 j 1113 15 67 17 a 1114 0 1 3 f 1114 1 3 9 1 k 1114 9 1 16 5 n 1114 16 5 20 34 j 1114 20 34 23 a 1115 0 1 3 f 1115 1 3 10 5 k 1115 10 5 17 5 n 1115 17 5 20 24 j 1115 20 24 23 a 1116 0 1 3 f 1116 1 3 11 1 k 1116 11 1 17 n 1116 17 18 37 j 1116 18 37 20 a 1117 0 1 3 f 1117 1 3 1 8 b 1117 1 8 12 2 k 1117 12 2 17 n 1117 17 20 a 1118 0 1 2 f 1118 1 2 2 7 b 1118 2 7 13 1 k 1118 13 1 18 57 n 1118 18 57 25 5 j 1118 25 5 26 01 d 1118 26 01 28 a 1119 0 1 1 f 1119 1 1 3 6 b 1119 3 6 14 k 1119 14 20 24 n 1119 20 24 26 48 j 1119 26 48 28 d 1119 28 30 a 1120 0 1 1 f 1120 1 1 4 1 b 1120 4 1 14 5 k 1120 14 5 20 74 n 1120 20 74 26 98 j 1120 26 98 30 d 1120 30 32 a table b 7 chronological data table of the synthetic dataset 2 table b 7 prority number bottom layer top layer type preferred angle 2 e g conformity 3 g l conformity 4 l c conformity 5 fault 6 v unconformity 7 v d conformity 8 d j conformity 9 a intrusion 10 n unconformity 11 n k conformity 12 k b conformity 13 f unconformity table b 8 fault data table of the synthetic dataset 2 table b 8 priority number borehole id elevation preferred angle type 5 113 35 39 135 fault 5 114 28 64 135 fault 5 112 41 5 135 fault 5 115 22 75 135 fault table b 9 borehole data table of the synthetic dataset 3 table b 9 borehole id x y elevation 111 70 70 0 112 90 90 0 113 110 110 0 114 130 130 0 115 140 140 0 116 160 160 0 117 180 180 0 118 230 230 3 119 250 250 0 table b 10 borehole unit data table of the synthetic dataset 3 table b 10 borehole id top depth bottom depth units 111 0 50 a 111 50 56 b 111 56 60 c 112 0 10 a 112 10 15 b 112 15 20 c 112 20 25 b 112 25 47 a 112 47 55 a 112 47 47 fault 112 55 60 b 112 60 61 c 113 0 10 a 113 10 15 b 113 15 20 c 113 20 30 c 113 20 20 fault 113 30 35 b 113 35 55 a 113 55 60 b 113 60 61 c 114 0 13 a 114 13 17 b 114 17 61 c 115 0 13 a 115 13 17 b 115 17 55 c 116 0 10 a 116 10 15 b 116 15 47 c 116 47 50 a 116 47 47 fault 116 50 55 b 116 55 61 c 117 0 10 a 117 10 15 b 117 15 18 c 117 18 45 a 117 18 18 fault 117 45 50 b 117 50 55 c 117 55 63 d 117 63 65 c 118 0 10 n 118 10 15 m 118 15 35 a 118 35 60 d 118 60 61 c 119 0 50 a 119 50 55 b 119 55 61 c table b 11 chronological data table of the synthetic dataset 3 table b 11 prority number bottom layer top layer type preferred angle 1 c b conformity 5 2 b a conformity 5 3 d intrusion 4 fault 5 fault 6 a m conformity 7 m n conformity table b 12 fault data table of the synthetic dataset 3 table b 12 priority number borehole id elevation preferred angle type 5 116 45 135 fault 5 117 18 135 fault 6 112 47 135 fault 6 113 20 135 fault table b 13 ground surface data table of the synthetic dataset 3 table b 13 x y z priority num type polarity angle 80 80 10 topography 175 175 4 topography normal 215 215 0 7 topography normal 219 219 0 8 topography normal 235 235 0 8 topography normal 239 239 0 7 topography normal 245 245 4 topography appendix c listings listing c 1 geopropy semi automatic guided stage of synthetic dataset 3 listing c 1 listing c 2 geopropy third manual stage of synthetic dataset 3 listing c 2 
25649,geological modelling is an essential aspect of underground investigations with cross sections being one of the key aspects this modelling can be done by experienced geologists or using mathematical methods we present geopropy an open source decision making algorithm implemented in python that generates 3d cross sections the boreholes do not have to be aligned it performs as an intelligent agent that simulates the steps taken by the geologist in the process of creating the cross section coupled with data driven decisions the algorithm detects zones with more than one possible outcome and based on the level of complexity or user preference proceeds to automatic semiautomatic or manual stages geopropy could be the basis of a new simpler more comprehensible way of looking at geological models in industry and academia while at the same time creating the potential for using novel machine learning algorithms in geological modelling keywords decision making algorithm 3d geological modelling cross section open source python 1 introduction obtaining 3d subsurface geological models is of great importance in a wide variety of geoscience studies these models represent superficial and underground geological structures and the distribution of geological units and their purpose is to illustrate the existing underground conditions subsurface geological understanding and models are essential aspects of earth related industrial projects and academic investigations from mining and petroleum to hydrogeology and environmental studies hawie et al 2021 kerrou et al 2017 pasculli et al 2014 yang et al 2021 in general geological models can be described from implicit or explicit points of view implicit modelling includes data driven methods that use datasets derived from measured features and algorithms explicit modelling mostly relies on expert opinion experience and interpretation where the expert usually builds cross sections surfaces or volumes by interpolating the accessible data randle et al 2018 to date various works have presented and discussed the use of implicit methods including cokriging based modelling calcagno et al 2008 gonçalves et al 2017 hillier et al 2014 lajaunie et al 1997 volume based modelling iskenova et al 2016 souche et al 2013 kinematic modelling brandes and tanner 2014 caumon 2010 and others lemon and jones 2003 muzik et al 2015 in addition open source software such as noddy pynoddy florian wellmann et al 2016 jessell and valenta 1996 and gempy de la varga et al 2019 schaaf et al 2020 and commercial software such as gdm suite brgm 2020 move petroleum experts 2019 vulcan 3d eureka maptek 2021 leapfrog seequent 2021a and oasis montaj seequent 2021b are powered by these methods most of these computer programs contain more than one method and although they are known as implicit modelers they benefit from explicit modelling in some stages of geological model evaluation for explicit modelling there are computer programs that support experts in generating cross sections some examples of open source software are heros velasco et al 2013 and heros 3d alcaraz et al 2016b the midvatten qgis plugin källgården and spångmyr 2020 grass gis grass development team 2020 and geomodelr serrano 2019 there are also commercial software programs that support explicit modelling such as rockware gis rockware 2020 surpac dassault systèmes 2021 datamine 2021 geoscene3d i gis 2020 bgs groundhog desktop british geological survey 2020 and gsi3d cullen et al 2010 one can also use other characteristics to categorize these programs the amount or type of information used to build a model the proportion of knowledge or data that drives the model the level of automation the focused scope the computational performance or the degree of specialty needed to use them bryant and flint 2009 despite the advancements in implicit geological modelling tools approaching 3d modelling and cross section generation by relying on geologists experience is an acceptable outcome in many scenarios which is why it is still used in numerous ongoing projects in industry and the academic community alike despite the diversity of methods and computer programs available we believe that there are some shortcomings that have not been fully addressed to date 1 one common key aspect of generating geological models is database integration modifying databases from well known formats to a specifically structured shape could be time consuming specifically in complex projects with a considerable amount of data velasco et al 2013 2 projects normally need various cross sections so generating them could be a time consuming procedure 3 in explicit modelling the lack of geologist experience could result in outcomes that are far from the reality of the region kumar 2021 4 there are many scenarios with more than one possible outcome freedom in the style and bias of each expert could play a role in determining the final cross sections which will result in nonunique solutions bowden 2004 points three and four may result in inconsistent models especially in projects where more than one geologist is working on cross sections randle et al 2019 which could potentially result in decreasing the feasibility of using the explicit models 5 the lack of experience of the geologist with mathematical modelling methods specifically in moderately complex scenarios could make it challenging to follow the steps and interpret the cross sections 6 some advanced packages do not provide extensive information about the procedures and methods undertaken without purchasing a licence e g petrel schlumberger 2015 geomodeller intrepid geophysics 2020 go cad paradigm 2015 7 the rapid development of artificial intelligence and machine learning libraries e g tensorflow abadi et al scikit learn pedregosa et al 2011 pytorch paszke et al 2019 pymc salvatier et al 2016 suggests a potential for expandability in geological modelling software however some existing computer programs are limited in this matter e g earthvision dynamic graphics 2009 rockworks strater golden software 2021 and the developments are mostly focused on reservoir properties chai et al 2021 chaikine and gates 2021 idrees et al 2021 or environmental modelling criollo et al 2019 hörning and haese 2021 volk and turner 2019 white et al 2021 developing open source software programs based on widely used programming languages brings the possibility of i understanding and replicating the undertaken procedures ii customizing the code for further analysis or different purposes in case of need and iii dynamically improving the software by implementing the latest version of machine learning and decision making methods that have already been developed by third parties heron et al 2013 moreover using a standardized known database could ease the process of introducing data to the software allen et al 2008 more importantly to the best of our knowledge there is no algorithm that aims to reproduce the steps of creating a cross section by the same approach used by a geologist which we think is worth considering trying and developing for research and industry in this work we address some of the shortcomings described above by developing geopropy an open source python based van rossum 1995 library to generate consistent 3d geological cross sections by hybrid implicit explicit methods geopropy completely or partially automates the workflow of the geologist to support the decision making process accompanied by data driven decisions the objectives of geopropy are as follows 1 it aims to identify the uncertainty caused by complex structures or a lack of data by detecting zones with more than one possible outcome and facilitating the decision making process in these zones for the user 2 the algorithm tries to act as an intelligent agent that emulates the decision making steps of a geologist by following the geological complexity with three different degrees of freedom a if the algorithm detects that the available information results in a unique outcome the cross sections will be generated automatically b if the algorithm determines that there is more than one possible outcome for the cross section it proceeds to the semiautomatic stage in which it asks for decisions on how to complete the geological unit contacts c if new geospatial information or more complex decisions are needed to complete the cross section the algorithm enters the manual stage to complete the cross section although fundamental uncertainties in geological and environmental models and various studies around them are of great importance bond 2015 jessell et al 2016 refsgaard et al 2007 wellmann and caumon 2018 wellmann and regenauer lieb 2012 white et al 2016 uncertainty is currently not reflected in geopropy in the next sections we will describe the methods applied in this work followed by the details of the geopropy code and the database modifications to evaluate the algorithm we will demonstrate the application of the algorithm on three synthetic datasets and compare it with explicit methods 2 methods in this section we will clarify geological unit contact types to unify the definitions for the machine and the geologist and then we will explain the database that is used in geopropy we will discuss the extraction of the logical steps for cross section generation based on the geologist s decision making steps 2 1 general definitions to translate the thinking of the geologist to a machine language unified assumptions are essential with that in mind various geological structures are defined so that they are distinguishable by the geologist and geopropy simultaneously i a conformity conformable contact represents continual uninterrupted deposition and accumulation of sedimentary rocks without a gap in the geologic record ii an unconformity unconformable contact is a type of stratigraphic contact between a specific sedimentary material in the top layer and other older units that are not chronologically correlative as the bottom layer this contact may cut previous surfaces unconformity contacts include disconformity nonconformity angular unconformity and paraconformity contact types geopropy recognizes all unconformity contact types as the same group iii an intrusion is an intrusive body of rock that solidifies inside existing geological units an intrusion has priority with respect to the units it intrudes into since it forms after them iv faults are defined as surfaces bounding any kind of geological unit that cuts any other previous unit and they imply a relative movement between the two blocks at each side of the surface davis et al 2011 using these definitions geopropy determines the zones that contain complex structures such as reverse faults folds and repetition of the same geological unit in a borehole we name these regions critical zones and the respective points critical points these critical zones could be the source of multiple cross sectional outcomes for a specific dataset 2 2 database an increasing volume of data in earth science demands a database with the ability to integrate various types of information for further practice vázquez suñé et al 2016 to ensure a dynamic workflow and the replicability of the analysis it is advised to manage the data collected in a standard database in addition flexibility in use with several tools and user friendliness are other properties of an adequate database we use the hydor data platform as the main geopropy database to continue the developments made by velasco 2013 alcaraz 2016 and criollo 2019 hydor was built as a personal geodatabase esri that can be used as a relational microsoft database which facilitates data management with a user friendly interface such as microsoft access software microsoft 2019 the arcgis platform esri 2021 or qgis qgis development team 2021 hydor has been widely used and tested vázquez suñé et al 2016 its data tables are homogenized and it cross analyses different types of information such as geology hydrogeology and environmental information which could improve the validation of conceptual models in addition there are tools such as heros heros 3d quimet velasco et al 2014 arcaraz alcaraz 2016 metrogeother alcaraz et al 2016a mj pumpit and hyyh criollo et al 2016 that have already been designed based on this database the hydor design follows established guidelines such as onegeology 2012 ogc standards 2012 ogc waterml 2 0 2012 and inspire 2013 2 3 steps of drawing a cross section to map out the automation of cross section construction by following the same strategy a geologist would we performed an exercise in which several geologists of diverse levels of expertise 4 university professors in the geological modelling field 5 bachelor s students of geology and 2 phd candidates in the geoscience field drew a cross section based on the same dataset refer to section 4 1 so that they could describe their thinking steps and considerations while performing a geological interpretation in general they proceeded with the following steps 1 identification of the geological units present in the area and their relative ages and interpretation of the types of contacts that separate the units in the boreholes 2 consideration of the outcroppings and ground surface data and flagging the critical points that can affect the layer contacts 3 focusing on complex structured zones such as folds and reverse faults keeping in mind the geological maps relative age of the layers and critical ground surface points at this stage the geologist may have one or more possible outcomes to complete the contacts respecting all boundary conditions the contacts will be determined by considering all the information the experience of the geologist and personal preference then the geologist completes the cross section by depicting relatively simple unit contacts there are some simple and logical assumptions extracted from the steps that a geologist considers in constructing the cross section if one contact of two specific geological units for example the a b contact in boreholes 1 and 2 exists in two consecutive boreholes the boundary between the two geological units orange and black lines must be defined by either a straight or a curved line in the case of the intersection of two geological contacts following hutton s cross cutting relation principle hutton 1795 the younger contact has priority over the older contact since it is derived from a newer geological event fig 1 blue dot if there is no unit contact information that forces the position of the contact the geologist obtains the apparent dip angle and tries to complete the contact using it or tries to continue the unit contact with the same angle if this contact unit already exists in the previous boreholes it is possible that using these two options will result in an inconsistent unit contact that cannot be verified by other available information or angular data that are not available the dashed red line and dashed brown line in fig 1 are examples of drawing a line by using the apparent dip angle and by using the same angle as the yellow solid line respectively and none of them can be validated by existing geological units in this case the personal preference of the geologist plays an important role in determining the unit contact the geologist could connect it to the younger contact at any point in the form of a curved or a straight line the blue line in fig 1 shows an example of a contact that could be validated by existing information in the next sections we will discuss the different aspects of the geopropy code structure followed by an application of the algorithm on three synthetic datasets to evaluate the functionality of the code by comparing it to the explicit cross sections drawn by the geologist 3 the geopropy code the program is a hybrid implicit explicit type as it is designed to proceed automatically whenever possible that is whenever there is only one possible solution with the data available however it switches to a manual mode or asks for help from a geologist when there is more than one outcome in this section we will describe the architecture of geopropy followed by explaining the modifications of the hydor database and required input data we will discuss the workflow of the algorithm in cross sections containing simple and complex geological structures with an example and we will show the available outputs and visualization options 3 1 geopropy architecture as an intelligent agent that tries to simulate the same stages as a skilled geologist geopropy is programmed to proceed as follows first it extracts the geological units contact points then it examines the ground surface points and outcroppings to generate the ground surface contacts and to determine critical ground surface points afterwards it analyses the contact points to detect repetitions of units in the same borehole polarities faults the possibility of existing folds and the locations of faults and marks the critical zones then for each critical zone the algorithm proceeds to the automatic stage if there is just one possible solution if there is more than one potential outcome the algorithm asks the user in two substages to identify the desirable scenario in the critical zone primarily by asking the user to choose the preferable solution between the existing options if this information does not result in a unique outcome the algorithm proceeds to the manual stage where the user has to introduce more information the workflow of the critical zones will be explained in the next sections in detail after analysing and determining the state of the contacts in the critical zones and accounting for the temporal sequence of geological events the rest of the unit contacts will be completed based on the mentioned guidelines it should be noted that it is possible to modify the cross section in a postprocessing stage with any software that is compatible with 3d shape files 3 2 geological input data and hydor database modifications the geological dataset in hydor includes information related to borehole properties samples lithology and definitions of geological units and subunits geopropy uses lithological and borehole data the lithology data table contains the borehole sample depth and geological units whereas the borehole data table contains information about the locations of the boreholes fig 2 one of the requirements of processing the geological data similarly to using the criteria of a geologist is to have the same level of information available to the user in the decision making procedure therefore chronological data fault data and ground surface data tables are added to the geological section of hydor to fulfill this need fig 2 green tables 3 2 1 chronological fault and ground surface data the chronological data table is designed to contain information regarding the priority of geological event occurrence table 1 based on the type of geological contact data must be introduced taking into account the concepts explained in section 2 1 the information in this table can follow four scenarios i if the contact is a conformity the top and bottom units are known and must be introduced in the top layer and bottom layer fields ii for an unconformity the top layer is known but the bottom layer s could be dissimilar units thus information about the bottom unit does not need to be added iii in the case of faults apart from the priority no information about the top or bottom unit is needed additional information about faults is located in the fault data table iv in the case of intrusions the top layer field has to be filled by the intruding unit and the bottom layer field does not need any information a priority number is defined to determine the chronological sequence of the geological contacts a priority number of one indicates the oldest contact and the highest prority number is the youngest the angle field is optional information that determines the apparent dip of the contact with 180 degree variation table 1 illustrates the various examples of contact types in the chronological data table note that chronological data can be extracted from geological maps or other regional studies tuned by user experience the fault data table table 2 must include the priority number that is already assigned in the chronological data table and the borehole id and the elevation of the fault inside the borehole in the borehole id and elevation fields respectively it is also optional to add an apparent fault dip in case of availability the ground surface data table table 3 consists of two groups of points the points with only geospatial information table 3 grey row and the outcroppings or unit contacts that are extracted from geological maps or field surveys in the latter the priority number of the unit contact point must be identified fields x y and z define the geospatial information of the sample polarity is an optional field that can be normal or reverse reverse polarity occurs when the unit contact defined in priority number is observed but the sequence of the unit is reversed angle is an optional field to add the apparent dip of the unit contact 3 3 geopropy workflow the programming language used for the code presented is python geopropy can be executed on windows and linux os based computers the code will interact with databases in personal spatial geodatabase esri format the first step of running geopropy is to complete the specific hydor database as mentioned in section 3 2 once the main function is executed geopropy reads the input information extracting the outcroppings ground surface lithological unit contacts and chronological information then the tool identifies 2d planes between every two consecutive boreholes and defines the boundary conditions respecting the arguments chosen by the user fig 3 shows the simplified schematic decision making algorithm of geopropy next accounting for the relative age of the unit contacts geopropy identifies the zones defined as critical with more than one possible scenario in the ground surface and subsurface to complete the geological unit contact lines the state of the critical zones determines the complexity of the process the yellow box in fig 3 shows the procedure for each critical zone if there is no identified critical zone geopropy passes directly to the automatic stage if there are critical zones geopropy first processes each zone in the semiautomatic stage some information will be shown to the user contact unit info critical zone boundaries faults that are related to the unit contacts or exist within the critical zone the point table that contains the specific point ids for each critical point and the instruction to choose between points or to proceed to the manual stage in the semiautomatic stage the user can choose the sequence of points in the critical zone that have to be connected in the form of a python list if the contact in the critical zone has more than one connected piece each piece can be separated by determining the separate keyword in the list see listing c 1 in the appendices the semiautomatic stage is accompanied by a visual representation of the completed parts of the cross section and the new information see fig a 4 in the appendies this semiautomatic stage does not require any new additional input the new information added in this stage by the user is a decision if the new decision does not result in a unique outcome for that zone or if the user wants to determine the points related to the unit contacts manually the zone will be passed to the third manual stage listing c 2 in the appendices here geopropy can accept more geospatial information with the purpose of either singling out the outcome or accounting for specific manual considerations such as adding a contact point in a space between boreholes that is not based on the data in the database different options are available in the third stage 1 connect existing points using the point ids 2 define new contact points and connect them to other new points or an existing point 3 connect points in the same borehole from the left or right sides which is normally useful if there is a change in the polarity of the unit in a borehole due to folds for example and 4 skip making a decision for some points and let the algorithm complete the critical zone automatically the new information added in the manual stage by the user can be geospatial information a group of decisions and or a determination that the algorithm has to complete the third stage by itself finally geopropy completes the cross section respecting all the geological units to evaluate the contacts the resulting geological areas in the cross section will be compared to the raw data to determine whether all the raw data points are correctly situated in their respective zones or whether there are areas containing more than one geological unit moreover if there is an area in which the geological unit is not identified by geopropy it will be flagged for further analysis by the user at this point a 3d visualization of the preliminary result will be displayed an example of this visualization can be found in fig a 1 in the appendices some stages of the algorithm are described by a simple synthetic cross section fig 4 demonstrates the steps that geopropy follows to generate it and the results this cross section contains conformable geological unit contacts an unconformity contact and a normal fault as shown in the coloured bars in fig 4 1 tables b1 to b4 in the appendices show the input data this database is designed to visualize the simple decision making process of geopropy with a limited number of geological layers and boreholes after preprocessing the input data the algorithm extracts the unit contact points coloured dots in fig 4 2 and 4 3 based on the chronological data table table b3 in the appendices then it calculates the 3d properties of the cross section defines the boundary conditions and processes the ground surface data the side boundaries of the cross section are the first and last introduced boreholes the top boundary is defined based on the borehole and ground surface data the preliminary bottom boundary is defined by a specific ratio of the maximum depth of the deepest borehole the dark blue and violet rectangles in fig 4 2 show the top and preliminary bottom boundary points respectively note that after creating all unit contacts the bottom boundary will be modified by finding the maximum depth of the critical points and units between each pair of consecutive boreholes and reducing the area of the units at the bottom to decrease the areas without available information the difference between the preliminary and final bottom boundaries is shown by the violet line in fig 4 3 and the cross sectional bottom boundary line in fig 4 this cross section does not contain any critical zones so the algorithm directly passes to completing the cross section automatically fig 4 3 shows how geopropy completes the cross section in 2 steps first determining the unit contacts that have to be connected because they have just one other equal contact point on each side green lines then determining the yellow contact lines which the algorithm decides how to generate based on the relative age of the layers orientation data and other information introduced in the database after the completion of the cross section each geological body is determined in the form of a polygon or multipart polygons each colour and polygon name determines a specific geological material the resulting geological bodies polygons are validated by comparing the polygons with the input geological unit data that are inside the polygon if the resulting geological bodies cannot be validated by the input data they will be flagged so that the user can analyse them in postprocessing finally the outputs will be generated as shown in fig 4 4 although the top view of the boreholes does not have to be on a straight line the general orientation of the boreholes has to follow a line in such a way that the same apparent dip angle for a contact through the whole cross section results in negligible error this is because for each unit contact the apparent dip angle does not change between borehole locations since geopropy draws a line between the holes based on the needs the user is able to ask for desired output shape files the available options are unit contact lines 3d vertical polygons for each geological unit a 2d projection of the 3d cross section and a curved version of the unit contact lines to facilitate geopropy usage one main method function in the python language is designed to run the tool the main method is executed with just two mandatory parameters the input hydor database directory which stores the geological information and the desired borehole ids the other variables are optional i precision related adjustments ii user preferences and iii visualization preferences the developer mode allows the user a more powerful error handling option refer to section 6 1 for access to the library help for information about the method arguments 3 4 output and visualization for each critical zone in the semiautomatic stage there is a 3d visual guide to illustrate the information graphically geopropy shows the final cross section in a 3d environment these two sets of graphs are based on the matplotlib python library hunter 2007 refer to appendix a for some examples various visualization options are available on demand for each cross section in addition to 3d shape files containing interpreted geological units it is possible to generate 2d projections of the cross section this is beneficial for illustration purposes especially in the absence of a computer program that can visualize 3d shape files each visualization option generates a database that contains geological unit polygons as shape files unit contacts as polylines and key coordinates as points depending on the visualization platform in use it is also possible to modify the graphical details such as the colour boundaries and line type additionally there is a built in option to smooth the geological unit contacts using curved lines on demand this option is designed for use in 2d and is only an aesthetic tool smoothed cross sections are not recommended since they result in decreased model accuracy which could cause potential difficulties in the subsequent processing stages such as fence diagrams and 3d volumes the results can be displayed in any computer program that supports 3d shape files such as arcgis or qgis although there are certain shortcomings in 3d data processing and visualization editing polygons in 3d arcmap documentation we used the arcpy python library to configure and optimize the output shape files 4 application to synthetic datasets three different synthetic datasets with varied properties are designed in 2d for better visualization each dataset was input to geopropy and given to a geologist to generate the cross section the geologist used heros to visualize and create the cross sections the first dataset is described in section 3 3 to demonstrate the functionality of the algorithm in this database there are geological units that do not exist in all boreholes so they have to terminate according to the guidelines and assumptions the second synthetic dataset contains conformities unconformities normal faults and an intrusion the third dataset contains an intrusion a reverse fault in conformable contacts and another reverse fault that crosses through a fold this dataset focuses on testing the semiautomatic and manual stages of geopropy when there is more than one possible correct explanation for the same dataset in each zone whether it is an intrusion zone or fold fault zone 4 1 synthetic database 1 4 1 1 dataset overview and input the database directory and borehole ids that are mandatory arguments to execute geopropy correspond to tables b1 to b4 in the appendices which describe the geological specifications of the example it is assumed that the only available data in this example correspond to boreholes that is there are no ground surface data available 4 1 2 results and comparison fig 5 shows the cross sections completed by a geologist and geopropy for the most part the two cross sections are similar although there are some differences 1 units e and d have different forms the termination of a unit contact in the absence of information can be done in various ways in this case the geologist chooses a smoother line whereas geopropy draws a straight line into the middle of two boreholes 2 similarly the intersection of units a b and c red boxes in fig 5 is slightly different and again is not smoothed by geopropy 3 layer f which appears only on the boreholes on the left side of the fault is not continued by geopropy on the footwall since there is no information in the database to verify that this layer continues on the right side of the fault whereas it is drawn there by the geologist fig 5 1 4 regarding the depth of the cross sections geopropy has diverse ways of interpreting the deepest boundary of the cross sections refer to section 6 1 for access to the geopropy documentation the first two differences can be considered as simply dissimilarities in styles 4 2 synthetic database 2 4 2 1 dataset overview and input this dataset is inspired by earth science student exercises it includes conformable and unconformable contacts with older faults and an intrusion of relatively complicated geometry the coloured bars in fig a 2 in the appendices illustrate the raw borehole data tables b5 to b8 in the appendices show the data used for this example 4 2 2 results and comparison the cross sections generated by the geologist and geopropy are compared in fig 6 similar to the first example the general geometry of the two cross sections is very similar the intrusion unit a in yellow in fig 6 contacts are smoother in the geologist s cross section than in geopropy s cross section in addition in the two contact zones between the intrusion and unit n coloured dark blue in fig 6 the geologist softened it to draw a zone contact in contrast to the singular contact points in geopropy this is the major difference between the two and it could also be considered a personal style related decision 4 3 synthetic database 3 4 3 1 dataset overview and input the goal of this example is to challenge geopropy with scenarios with critical zones that can lead to several possible outcomes fig 7 illustrates borehole and ground surface data which in turn contain two sets of ground surface information at some points only the altitude is available black points whereas at others there is information about the unit contact fig 7 zone i coloured points each point indicates a unit contact and each colour indicates the unit that has been observed zone ii includes a reverse fault that causes repetition of units a b and c in all boreholes of the zone zone iii is a region with complex geometry that could be interpreted in different ways there must be folds and a reverse fault fig 8 1 and 8 2 zone iv illustrates an intrusion that can have more than one possible geometry refer to section 6 2 for the details of how to reproduce the example geopropy detects that the dataset does not result in a unique outcome immediately after introducing the dataset directory and the borehole ids thus it starts an interaction with the user asking for complementary information this interaction is discussed below tables b9 to b13 in the appendices show the tables in the dataset list 1 shows how to execute the function list 1 executing synthetic dataset 3 using the geopropy library note that the default arguments of the cross section method are used for more information refer to section 6 1 list 1 4 3 2 results and comparison the interpretations of zone i by the geologist and geopropy are fairly similar fig 9 the geopropy contacts are more angular whereas the geologist s contacts are more rounded or smoothed as in the previous examples in zone ii fig 7 where units a b and c are repeated in the boreholes the existence of the reverse fault shapes the geometry of the zone therefore although geopropy identifies this region as a critical zone it does not ask for more information since the outcome is unique fig 9 the results obtained in critical zone iii fig 7 contain repetitions of the geological units but the unit contacts can be interpreted in more than one way in addition there is a reverse fault crossing this critical region fig 8 1 and 8 2 show two of the possible outcomes which were drawn by the geologist to reach a unique solution in this critical zone geopropy proceeds to the semiautomatic stage where the user has to choose one of the possible scenarios based on preference or experience listing c 2 in the appendices fig a4 in the appendices shows the visual 3d helper of geopropy used to facilitate the procedure since the information provided by the user in the semiautomatic stage results in a unique solution in the critical zone geopropy verifies the uniqueness of the critical zone and does not enter the manual stage fig 8 1 shows the geometry of the units generated by the geologist in critical zone iii which is also introduced to geopropy the comparison between fig 9 1 and 9 2 in zone iii illustrates the difference between them which is the lack of smoothness especially for the contact points between unit b and the fault and respecting the thickness of unit b in the geologist s interpretation the intrusion in critical zone iv could be interpreted in various forms fig 8 3 and 8 4 according to the geologist geopropy detects the nonuniqueness of the zone and proceeds to the semiautomatic stage to ask for more information from the user which could lead to different cross sectional outcomes based on different choices of the user as shown in fig 8 5 8 6 and 8 7 fig 8 6 and 8 7 are generated in the semiautomatic stage whereas fig 8 5 is generated in the manual stage in addition it is possible to introduce new sample points to the zone using spatial data depending on the user needs and available data listing c 3 in the appendices shows the geopropy user interaction in the semiautomatic and manual stages there are some differences between the interpretations of the geologist fig 9 1 and geopropy fig 9 2 smoothness is the first difference especially around folds and intrusions the unit contact angles also differ in the absence of angular data the geologist relies on experience and personal style whereas geopropy calculates the apparent dip in the unit contacts based on the contact lines across the whole cross section to preserve the orientation behaviour of the unit it must be mentioned that identifying the available options and being able to apply additional information offers freedom to the user to obtain results that fit expectations figs a3 and a 5 in the appendices illustrate two other possible outcomes created by the geologist and geopropy respectively 5 discussion and conclusion geopropy is an open source python library that performs as an intelligent agent to create 3d geological cross sections based on a knowledge and data driven approach as seen in the three presented examples figs 5 6 and 9 geopropy s cross sections preserve the overall characteristics of a real geologist s cross sections although small differences mostly in the smoothness of contacts can be observed the application of geopropy to synthetic profiles validates the functionality and the decision making procedure making it a useful support tool for geologists the use of geopropy outputs in a gis platform may aid users in modifying and customizing the results with the capabilities available on each gis platform for instance using a gis fence diagrams or geological volumes can be easily generated which can be used later in numerical modelling software nevertheless geopropy has some limitations first it is highly sensitive to the data geopropy always respects the input data whereas in some scenarios a geologist may decide that an observation is not reliable or not compatible with the properties of the region and may ignore or modify the data in geopropy this step has to be carried out by the user second although geopropy is an open source library it is not completely free to use since it depends partially on the arcpy library to generate the shape files this means that to execute geopropy the user needs to have access to an arcgis licence the reason that other freely available libraries were not used to create shape files is that there was no free and open source library that could reliably support 3d shape file generation even with the shortcomings of the arcpy library we considered it the most suitable library for geopropy in addition there is a vast range of tools developed and available in arcgis that could be coupled with geopropy or used in the postprocessing of the results third in the case of high variation of the angles among the vertical planes between each consecutive borehole pair in a cross section the assumption of the same apparent dip angles for one contact along the whole cross section in 3d could result in considerable errors fourth geopropy does not necessarily preserve the thickness of the geological units it is assumed that by preserving the unit contacts and the unit angles the output unit thickness will be accurate but thickness is not considered an ascertainable factor in generating the cross sections developing geopropy in a widely used programming language and making the source code available create the possibility for further development of the library and coupling the program with other existing decision making tools in addition the use of a standard and well known database with various tools depending on it facilitates the integration of different aspects of the study and saves the time needed to prepare databases for each tool moreover by following specific guidelines that are derived from the thought process of a geologist i the process of generating the cross section and the results can be easily interpreted since the algorithm acts similarly to the user s thinking steps ii the code controls inconsistencies related to personal style and bias which are important in studies that involve more than one person or that continue for a long period and iii the code can help inexperienced users avoid the unrealistic results that could occur when complicated mathematical modelling techniques are used geopropy is easy to implement and keeping in mind that creating large numbers of cross sections is time consuming it could be used to speed up the process of creating geological cross sections overall this tool could be of great help when geological modelling must be done explicitly as it can avoid inconsistent decision making in addition to the time benefits and avoidance of inconsistencies another valuable asset of geopropy is that it detects zones with several possible outcomes in a cross section this could potentially help users analyse different plausible geological scenarios based on the available data this also holds in the absence of orientation data which brings flexibility to the tool at the cost of precision geopropy does not replace implicit models but it would help in generating cross sections explicitly as an intelligent agent 6 software and data availability 6 1 geopropy library information available to download freely in https github com idaea evs geopropy under agpl 3 0 license year first available 2021 dependencies arcpy arcgis 10 5 or higher matplotlib pypyodbc programming language python developed by ashkan hassanzadeh contact information ashkan hassanzadeh csic es refer to https github com idaea evs geopropy wiki for additional information about the installation default values of the arguments the explanation and the usage 6 2 synthetic examples databases of 3 synthetic examples in this article are available free of charge in mdb format alongside the jupyter notebook in https github com idaea evs geopropy declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements the authors acknowledge miguel lópez blanco patricia cabello carlos ayora and two anonymous reviewers that helped us to improve this article this research did not receive any specific grant from funding agencies in the public commercial or not for profit sectors idaea csic is centre of excellence severo ochoa project cex2018 000794 s r criollo gratefully acknowledges the financial support from the balearic island government through the margalida comas postdoctoral fellowship programme pd 036 2020 appendices appendix a additional figures fig a 1 a 3d cross section based on synthetic database 2 note that the geospatial properties of the boreholes change from synthetic dataset 2 to demonstrate the 3d capabilities of geopropy fig a 1 fig a 2 raw data of synthetic dataset 2 fig a 2 fig a 3 cross sections of synthetic dataset 3 created by the geologist based on the available data the geological units in critical zones iii and iv could be interpreted in various ways fig a 3 fig a 4 screenshot of the semiautomatic stage visualization of synthetic dataset 3 the interactive visualization tool helps the user identify the faults contact points and respective point ids the provisional scheme of unit contacts that are already created is shown by lines in different colours fig a 4 fig a 5 cross sections of synthetic dataset 3 created by geopropy the user decided to choose the semiautomatic stage preferences in critical zones iii and iv fig a 5 appendix b synthetic datasets tables table b 1 borehole data table of the synthetic dataset 1 table b 1 borehole id x y elevation 1 0 0 2 2 21 21 1 3 27 27 0 4 40 40 0 5 46 46 0 6 53 53 0 7 62 62 0 table b 2 borehole unit data table of the synthetic dataset 1 table b 2 borehole id top depth bottom depth units 1 0 2 e 1 2 3 a 1 3 9 b 1 9 11 3 f 2 0 2 a 2 2 2 fault 2 2 4 a 2 4 9 3 b 2 9 3 11 f 3 0 5 a 3 5 8 b 3 8 9 5 b 3 8 8 fault 3 9 5 11 f 4 0 6 a 4 6 13 8 b 5 0 5 a 5 5 7 c 5 7 10 3 b 6 0 5 a 6 5 8 c 6 8 12 b 7 0 5 a 7 5 6 2 d 7 6 2 9 1 c 7 9 1 12 1 b 7 12 1 14 1 b table b 3 chronological data table of the synthetic dataset 1 table b 3 prority number bottom layer top layer type preferred angle 1 f b conformity 2 b c conformity 3 c d conformity 4 a unconformity 5 fault 6 a e conformity table b 4 fault data table of the synthetic dataset 1 table b 4 priority number borehole id elevation preferred angle type 5 2 2 45 fault 5 3 8 45 fault table b 5 borehole data table of the synthetic dataset 2 table b 5 borehole id x y elevation 111 0 0 1 112 5 5 1 5 113 10 10 2 114 15 15 2 5 115 20 20 2 25 116 25 25 2 117 32 5 32 5 1 8 118 37 5 37 5 1 5 119 42 5 42 5 0 1110 47 5 47 5 1 1111 53 75 53 75 1 15 1112 60 60 1 3 1113 66 25 66 25 1 3 1114 72 5 72 5 1 3 1115 77 5 77 5 1 3 1116 81 25 81 25 1 3 1117 85 85 1 3 1118 88 75 88 75 1 2 1119 92 5 92 5 1 1 1120 97 5 97 5 1 1 table b 6 borehole unit data table of the synthetic dataset 2 table b 6 borehole id top depth bottom depth units 111 0 1 f 111 1 5 16 j 111 5 16 11 4 d 111 11 4 15 56 v 111 15 56 17 4 l 111 17 4 20 15 g 111 20 15 21 e 112 0 1 5 f 112 1 5 6 2 j 112 6 2 12 44 d 112 12 44 16 6 v 112 16 6 21 7 l 112 21 7 24 2 g 112 24 2 41 5 e 112 41 5 41 5 fault 112 41 5 43 e 113 0 2 f 113 2 3 n 113 3 8 24 j 113 8 24 14 48 d 113 14 48 18 v 113 18 18 5 c 113 18 5 27 64 l 113 27 64 30 39 g 113 30 39 35 39 e 113 35 39 35 39 fault 113 35 39 38 e 114 0 2 5 f 114 2 5 4 5 n 114 4 5 9 74 j 114 9 74 15 98 d 114 15 98 20 14 v 114 20 14 22 5 c 114 22 5 28 64 l 114 28 64 30 e 114 28 64 28 64 fault 115 0 2 25 f 115 2 25 5 6 n 115 5 6 10 84 j 115 10 84 17 08 d 115 17 08 21 24 v 115 21 24 22 75 c 115 22 75 22 75 fault 115 22 75 27 4 l 115 27 4 30 15 g 115 30 15 32 e 116 0 2 f 116 2 5 62 n 116 5 62 11 36 j 116 11 36 17 6 d 116 17 6 21 76 v 116 21 76 25 8 c 116 25 8 31 26 l 116 31 26 34 01 g 116 34 01 36 e 117 0 1 8 f 117 1 8 6 68 n 117 6 68 12 72 j 117 12 72 19 5 d 117 19 5 24 8 v 117 24 8 30 12 c 117 30 12 37 62 l 117 37 62 40 37 g 117 40 37 42 e 118 0 1 5 f 118 1 5 7 14 n 118 7 14 12 86 j 118 12 86 20 1 d 118 20 1 25 5 v 118 25 5 32 84 c 118 32 84 40 l 118 40 41 a 119 0 1 k 119 1 7 4 n 119 7 4 13 12 j 119 13 12 19 36 d 119 19 36 25 52 v 119 25 52 33 c 119 33 35 a 1110 0 1 f 1110 1 3 6 k 1110 3 6 10 n 1110 10 15 08 j 1110 15 08 21 32 d 1110 21 32 25 v 1110 25 27 a 1111 0 1 15 f 1111 1 15 5 05 k 1111 5 05 10 5 n 1111 10 5 13 04 j 1111 13 04 15 a 1112 0 1 3 f 1112 1 3 6 5 k 1112 6 5 11 n 1112 11 12 a 1113 0 1 3 f 1113 1 3 7 8 k 1113 7 8 13 75 n 1113 13 75 15 67 j 1113 15 67 17 a 1114 0 1 3 f 1114 1 3 9 1 k 1114 9 1 16 5 n 1114 16 5 20 34 j 1114 20 34 23 a 1115 0 1 3 f 1115 1 3 10 5 k 1115 10 5 17 5 n 1115 17 5 20 24 j 1115 20 24 23 a 1116 0 1 3 f 1116 1 3 11 1 k 1116 11 1 17 n 1116 17 18 37 j 1116 18 37 20 a 1117 0 1 3 f 1117 1 3 1 8 b 1117 1 8 12 2 k 1117 12 2 17 n 1117 17 20 a 1118 0 1 2 f 1118 1 2 2 7 b 1118 2 7 13 1 k 1118 13 1 18 57 n 1118 18 57 25 5 j 1118 25 5 26 01 d 1118 26 01 28 a 1119 0 1 1 f 1119 1 1 3 6 b 1119 3 6 14 k 1119 14 20 24 n 1119 20 24 26 48 j 1119 26 48 28 d 1119 28 30 a 1120 0 1 1 f 1120 1 1 4 1 b 1120 4 1 14 5 k 1120 14 5 20 74 n 1120 20 74 26 98 j 1120 26 98 30 d 1120 30 32 a table b 7 chronological data table of the synthetic dataset 2 table b 7 prority number bottom layer top layer type preferred angle 2 e g conformity 3 g l conformity 4 l c conformity 5 fault 6 v unconformity 7 v d conformity 8 d j conformity 9 a intrusion 10 n unconformity 11 n k conformity 12 k b conformity 13 f unconformity table b 8 fault data table of the synthetic dataset 2 table b 8 priority number borehole id elevation preferred angle type 5 113 35 39 135 fault 5 114 28 64 135 fault 5 112 41 5 135 fault 5 115 22 75 135 fault table b 9 borehole data table of the synthetic dataset 3 table b 9 borehole id x y elevation 111 70 70 0 112 90 90 0 113 110 110 0 114 130 130 0 115 140 140 0 116 160 160 0 117 180 180 0 118 230 230 3 119 250 250 0 table b 10 borehole unit data table of the synthetic dataset 3 table b 10 borehole id top depth bottom depth units 111 0 50 a 111 50 56 b 111 56 60 c 112 0 10 a 112 10 15 b 112 15 20 c 112 20 25 b 112 25 47 a 112 47 55 a 112 47 47 fault 112 55 60 b 112 60 61 c 113 0 10 a 113 10 15 b 113 15 20 c 113 20 30 c 113 20 20 fault 113 30 35 b 113 35 55 a 113 55 60 b 113 60 61 c 114 0 13 a 114 13 17 b 114 17 61 c 115 0 13 a 115 13 17 b 115 17 55 c 116 0 10 a 116 10 15 b 116 15 47 c 116 47 50 a 116 47 47 fault 116 50 55 b 116 55 61 c 117 0 10 a 117 10 15 b 117 15 18 c 117 18 45 a 117 18 18 fault 117 45 50 b 117 50 55 c 117 55 63 d 117 63 65 c 118 0 10 n 118 10 15 m 118 15 35 a 118 35 60 d 118 60 61 c 119 0 50 a 119 50 55 b 119 55 61 c table b 11 chronological data table of the synthetic dataset 3 table b 11 prority number bottom layer top layer type preferred angle 1 c b conformity 5 2 b a conformity 5 3 d intrusion 4 fault 5 fault 6 a m conformity 7 m n conformity table b 12 fault data table of the synthetic dataset 3 table b 12 priority number borehole id elevation preferred angle type 5 116 45 135 fault 5 117 18 135 fault 6 112 47 135 fault 6 113 20 135 fault table b 13 ground surface data table of the synthetic dataset 3 table b 13 x y z priority num type polarity angle 80 80 10 topography 175 175 4 topography normal 215 215 0 7 topography normal 219 219 0 8 topography normal 235 235 0 8 topography normal 239 239 0 7 topography normal 245 245 4 topography appendix c listings listing c 1 geopropy semi automatic guided stage of synthetic dataset 3 listing c 1 listing c 2 geopropy third manual stage of synthetic dataset 3 listing c 2 
