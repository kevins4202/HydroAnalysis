index,text
25660,water resource management is of crucial societal and economic importance requiring a strong capacity for anticipating environmental change progress in physical process knowledge numerical methods and computational power allows us to address hydro environmental problems of growing complexity modeling of river and marine flows is no exception with the increase in it resources environmental modeling is evolving to meet the challenges of complex real world problems this paper presents a new distributed application programming interface api of the open source telemac mascaret system to run hydro environmental simulations with the help of the interoperability concept use of the api encourages and facilitates the combination of worldwide reference environmental libraries with the hydro informatic system consequently the objective of the paper is to promote the interoperability concept for studies dealing with such issues as uncertainty propagation global sensitivity analysis optimization multi physics or multi dimensional coupling to illustrate the capability of the api an operational problem for improving the navigation capacity of the gironde estuary is presented the api potential is demonstrated in a re calibration context the api is used for a multivariate sensitivity analysis to quickly reveal the most influential parameters which can then be optimally calibrated with the help of a data assimilation technique keywords application programing interface api environmental modeling physical based data driven modeling sensitivity analysis 1 introduction water availability and quality are vital to human health increasing global scarcity makes anticipating the evolution of this limited natural resource even more essential this ability relies on the understanding and prediction of hydrodynamic flows hydraulic simulation codes such as hec ras dyhouse et al 2007 mohid mateus and neves 2013 delft 3d deltares 2014 telemac mascaret hervouet 2007 have been developed for many years and applied to a wide range of hydro environmental cases such as prevention of flood risks teng et al 2017 development of management plans ran and nedovic budic 2016 or design schemes for flood alleviation ogie et al 2020 hydro environmental modeling of river and marine flows is increasingly complex and requires a better knowledge of physical and biochemical processes advanced numerical methods and high performance computing resources for many applications engineers have to deal with complex modeling systems for the representation of subparts of a physical system braunschweig et al 2004 harpham and danovaro 2014 increasing a numerical model conceptual complexity in terms of how many aspects of the underlying system are included in it has the potential to improve the accuracy of the results in term of descriptive and predictive capacities however adding each new aspect to the model can induce an increase of the error brought about by the uncertainty in the description of the features accumulates and increases the overall uncertainty in the output cea et al 2011 lees et al 2000 this trade off between model completeness and propagation error saltelli 2019 is known as the conjecture of o neill o neill and rust 1979 in maritime or fluvial cases numerical modeling tools allow replicating the past as well as predicting a future with an uncertainty range that is strongly linked to the approximate description of hydraulic parameters pappenberger et al 2008 meteorological data aronica et al 2012 and geographical data abily et al 2016 in spite of the significant improvement in computational resources and the accuracy of numerical models the turbulent nature of fluid mechanics equations limits the performance of hydraulic models the need for realistic hydrodynamic flow simulation is beyond the abilities of deterministic forecast as a consequence model uncertainties have to be quantified recent advances in simulation and optimization of environmental systems have relied on increasingly detailed models running many scenarios in order to quantify these uncertainties herman et al 2015 teng et al 2017 according to solomatine and ostfeld 2008 significant advances in the means of observing continental waters especially through orbital equipment swot satellite mission morrow et al 2019 sentinel satellite missions copernicus program malenovsky et al 2012 allows use of various types of data with their associated analysis for water resources management knox et al 2019 thus environmental modeling is increasingly complemented by data driven models kim et al 2010 moreover by taking into account advances in information processing and data management it becomes possible to reduce uncertainties and better understand the modeling process data driven modeling is a paradigm shift for addressing many problems in science and engineering montáns et al 2019 to summarize with the increase in computer resources environmental modeling is evolving with the need to solve increasingly complex real world problems involving the environment and its relationship to human systems and activities if it is to break down research silos and bring scientists stakeholders and decision makers together to solve social daloglu et al 2014 economic harou et al 2009 and environmental problems an environmental modeling system should support it tasks such as aggregation of model components into functional units component interaction and communication parallel computing and cross language interoperability david et al 2013 laniak et al 2013 these technical requirements are closely related to interoperability aspects namely the capacity of the software to run and share information with other codes environmental system development requires both scientific understanding of the phenomena involved and software development proficiency this paper presents a new application programming interface api of the open source and interoperable telemac mascaret system https gitlab pam retd fr otm telemac mascaret to solve environmental problems lacombe et al 2013 goeury et al 2017 since 2017 the telemac mascaret system has included in its distribution a generic api that allows it to expand its range of applications audouin et al 2017 with this newly implemented feature the telemac mascaret system can be easily coupled or integrated into higher level platforms to model study or simulate complex problems goeury et al 2015 to our knowledge apis for an environmental modeling system with hydraulic mascaret 1d telemac 2d telemac 3d water quality tracer waqtel sediment transport courlis gaia and wave hydrodynamics tomawac and artemis are not yet widespread in the environmental community and are an original innovation the present work uses as an example france s gironde estuary to demonstrate the operational applicability and outcomes of apis the port of bordeaux in the gironde estuary faces many development challenges klein et al 2018 to satisfy demand for larger ships while ensuring navigational safety the evolution of water depth over time in the estuary needs to be predicted with a high degree of accuracy to address this issue the numerical results must be consistent with past observational data among other things this process relies on calibration to determine empirical adequacy oreskes et al 1994 in particular the calibration aims at simulating a series of reference events by adjusting some uncertain physically based parameters until the comparison is as accurate as possible dung et al 2011 the objective of this work is to implement an efficient calibration algorithm capable of processing measurement optimally in order to estimate the partially known or missing parameters nagel et al 2020 often performed calibrating a hydrodynamic model is nevertheless a difficult task owing to the complexity of the flows and their interaction with the shoreline the bathymetry islands etc it is essential to understand in depth the relationship between the calibration of modeling parameters and the simulated state variables which are compared to the observations in this context a multivariate sensitivity analysis lamboni et al 2011 gamboa et al 2013 succeeded in identifying the most influential input parameters on the model results sensitivity analysis is associated to define the parameters to be calibrated using a physically based data driven technique carrassi et al 2018 this paper is organized as follows section 2 includes a description of the main concepts of the telemac mascaret system api section 3 presents the architecture of implemented apis section 4 describes the application case section 5 deals with the calibration problem section 6 is the discussion and section 7 is the conclusion 2 method the telemac mascaret hydro informatic system created in 1987 is an open source www opentelemac org integrated suite of solvers for use in the field of free surface flows hervouet 2007 as displayed on fig 1 it can carry out simulations of flows mascaret telemac 2d and telemac 3d gray circle sediment transports courlis and gaia orange circle waves artemis and tomawac green circle and water quality tracer and waqtel purple circle historically from the original software solving physics equations saint venant mascaret and telemac 2d navier stokes telemac 3d elliptic mild slope equation artemis simplified equation for the spectro angular density of wave action tomawac exner courlis and gaia water quality processes tracer and waqtel the system has gradually evolved to the notion of hydro informatics with a set of solvers dealing with large heterogeneous data and solving complex dependent problems overlapping physical components in fig 1 the various simulation components use high end algorithms based on the finite element or finite volume method the ability to study global regional or local scale problems by using the same system makes the telemac mascaret a useful tool for assessing the environmental state in the sea estuaries coastline and rivers according to laniak et al 2013 integrated environmental modeling iem highlights the importance of software development and sharing in their role as pluggable components of a larger ecosystem the work presented here is part of this trend it shows the implementation procedure for the component based solver interoperability of the telemac mascaret system the api architecture described in this paper is directly applicable for any software regardless of its language since it is an easy task to build a wrapper in other programming language as presented in the following for fortran and python the main difficult part is to split the software main program when a solver is an interoperable component it can be part of an assembly of elements in permanent interaction that work in a coordinated way to be considered as an elementary reusable component a computation model must possess apis which can be viewed as new functionalities or entry points in the computational model to provide new services on demand have a resolution algorithm comprising three main functionalities problem initialization simulation and finalization of the computation allow instantiation of several clearly separated and identifiable problems be useable in dynamic library form in the following the four points listed above are addressed 2 1 design a computer programming model like the object oriented programming oop aims to organize the software design around data grouped into objects with attributes and methods this help the developer to focus on the data rather than the logic required to manipulate them most of the oop concepts encapsulation interfaces polymorphism modularity instantiation are derived from software engineering patterns but relate to the real world as shown in fig 2 the first concept used here is encapsulation with a classic design pattern facade gamma et al 1994 since the telemac mascaret system is written in fortran 77 and 90 the design pattern has been adapted to this non object oriented language as oop is well suited for the design of large and complex programs the main benefits of this approach are that it is easier to use and understand limits the library user dependencies includes integrity constraints and validation rules and avoids some unnecessary calculations lacombe et al 2013 internal data has been accessed in order to expand the capabilities of the api it is possible to examine all the information embedded in the calculation code and modify its structure specifically the values some metadata and properties at runtime pointers were used to access the simulation data as fortran lacks the pointer definition of other high level languages such as c or c a combination of the pointer and target fortran keywords with adequate updates were employed for instance a manual update must be done if a pointer on an allocatable variable is set before the dynamic allocation since the pointer is no longer valid this ensures the correct state of the controlled variable a component based approach was adopted by having the same structure as all of the telemac mascaret physical dedicated solver to allow them to be viewed as linkable the execution of a simulation was partitioned to allow modification of a given parameter at runtime for example each time step the flexibility that comes with interoperability allows deployment of the telemac mascaret system in specific industrial applications and distribution of its solvers into complex modeling systems applications blue part in fig 3 as mentioned by gil et al 2018 geosciences will lead to a new generation of knowledge rich intelligent systems that contain rich knowledge and context in addition to data enabling fundamentally new forms of reasoning autonomy learning and interaction as presented in fig 3 the telemac mascaret physical dedicated components can be easily connected together to build multi physics or multi dimensional interactions harpham and danovaro 2014 coupling orange part inserted in optimization or uncertainty quantification processes studies yellow part see section 5 wrapped in advanced technology such as cloud based solutions or embedded systems with data management openmi gregersen et al 2007 or pynsim knox et al 2018 cloud services green part and integrated in platforms salome ribes and caremoli 2007 or palm buis et al 2006 for example numeric computing environment pink part 2 2 technology to maintain efficient execution a native code compiler for example gnu compiler collection licensed under gnu gpl running on most systems and hardware architectures and supporting fortran is used and recommended moreover the calculation code of the telemac mascaret system numerically solves a model based on a system of partial differential equations pde the computational solution relies on an implicit time scheme removing stability constraints on the time frame but leading to a linear system solution of considerable size the resulting increased complexity of the model structures poses strong limitations in terms of practical implementation and computational requirements consequently high performance computing hpc is necessary to tackle realistic applications the solution deployed in the hydro informatic system is to decompose the domain of computation this idea of domain decomposition is to assign to each processor one part of the global domain over which it solves the fluid mechanics problem the results of the other processors help in defining artificial boundary conditions arising from the partition the assignments given to each processor are the same but the data differ therefore the telemac mascaret system requires the use of an mpi library for communication between processors clarke et al 1994 it is possible to use mpi technology for the instantiation of several clearly separated and identifiable problems this is because mpi is adapted for distributed memory systems this method has been followed for the learning step of sensitivity analysis presented in subsection 5 1 3 architecture of api the architecture is generic for all telemac mascaret system components fig 1 for the sake of clarity only an api example with the telemac 2d component is shown in the following sections 3 1 fortran structure api s main goal is to control the simulation while running a case for example it must allow the user to suspend the simulation at any time step retrieve some variable values and possibly change them before resuming the calculation to make this possible a fortran structure called instance is used in the api the instance structure gives direct access to the physical memory of variables and therefore allows control of the variable furthermore based on decomposition of the main telemac mascaret system subroutines in three main functionalities initialization simulation of one time step and finalization it is possible to run a hydraulic case for an indefinite sequence of time steps fig 4 describes an example of api workflow where x are input parameters and o t is information extracted at given times for instance in the framework of the gironde estuary application case x represents parameters to be estimated calibrated friction coefficients and tidal boundary condition parameters and o t is the free surface elevation every minute at the measurement stations see section 4 for each action defined above the identity number of the instance is used as an input argument allowing all computation variables to be linked with the corresponding instance pointers these actions must be done in chronological sequence in order to ensure proper execution of the computation in the api main program in fortran and for the shallow water code telemac 2d it will begin with the call of an initialization subroutine call run set config t2d id lu lng comm ierr where all parameters are scalars of type integer and id is an output value giving the instantiation number the input parameter lu is used to redirect the standard output lng has only two possible values for the choice of the english or french language and comm is the mpi communicator for the distributed memory parallelism the output ierr has to be considered with care as a non null value stands for the occurrence of errors a systematic test on this return value is recommended for instance one may want to stop the program accordingly if ierr ne 0 exit ierr after the initialization phase the model has to be read from files with the following instruction call run read case t2d id cas file dico file init ierr the name of the steering file is given with cas file as a character string it stores several input data of the model with key value pairs each possible key is set with a dictionary file whose name is a character string hold by dico file a true value for the logical input init will initialize a common computational kernel of all the modules after this step a dynamic allocation of arrays in memory is necessary followed by a state initialization of the physics call run allocation t2d id ierr call run init t2d id ierr all accessible parameters are listed in predefined keyword lists see appendix a for telemac 2d for example the number of time steps in the simulation is named model ntimesteps and its value is obtained and stored in the variable nbsteps with the following call call get integer id t2d model ntimesteps nbsteps 0 0 0 ierr input parameter can be defined here using the following call call set double id t2d model sealevel sealevel 0 0 0 ierr where the sea level value in the computation accessed through the keyword model sealevel is set to the value of the variable sealevel now the simulation can be done with a loop on the number of time steps while storing all the results of an evolution of the water depth do i 1 nbsteps call run timestep t2d id ierr call get double id t2d model waterdepth waterdepth i 42 0 0 ierr enddo waterdepth is a one dimensional array of size nbsteps it is used here to store the water depth values at node number 42 of the telemac 2d triangular mesh finally the fortran program can end with the instance deletion thus freeing the memory used call run finalize t2d id ierr additional functions are available to handle parallelism see subsection 2 2 for more details on parallelism in the telemac system the data structure in the current version official version 8 2 does not yet allow for multiple instantiations as all the instances point to the same memory area this constitutes a future development to improve api capacity however instantiation associated with the processor communication function overcomes this limitation by ensuring the possibility of several clearly separated and identifiable problems 3 2 python wrapping the use of api is not limited to fortran programing but can also be used by a scripting language to this end a python wrapper is also available it is relatively easy to use the fortran api routines directly in python using the f2py tool of the python scipy library peterson 2009 this tool will make it possible to compile fortran code for use in python the only limitations are on the type of arguments of the functions to wrap python is a portable dynamic extensible free language which allows without requiring a modular approach and object oriented programming van rossum and drake 2009 in addition to the benefits of this programming language python offers a large number of interoperable libraries for scientific computing image processing data processing machine learning and deep learning the link between various interoperable libraries with telemac mascaret system apis allows the creation of an ever more efficient calculation chain capable of responding finely to various complex problems such as the application case presented in this article see section 4 moreover the python scripting language makes it possible to implement a wrapper in order to provide user friendly functioning of the fortran api thus a python overlay was developed to encapsulate and simplify the different api calls for instance a python get function can encapsulate all api fortran type dedicated function get type where type can be replaced by double integer boolean and string file management is also made easier with python copy move and reducing the number of arguments this python wrapping of telemac mascaret system api constitutes a package called telapy distributed in the official version of the hydro informatic system fig 5 the telapy package is provided with a tutorial for people who want to run the telemac mascaret system physical components in an interactive mode with the help of the python scripting language the script corresponding to the application case of section 4 is presented in appendix b 3 3 summary a single coherent fortran structure has been deployed in telemac mascaret in order to ensure interoperability of the system for each of its solvers this is based on three main functionalities instantiation variable values and computation control firstly instantiation associated with the processor communication function ensures the possibility of a simultaneous existence of several clearly separated and identifiable problems secondly based on variable and computation control memory access is permitted during simulation allowing new services to be provided on demand finally the technology allows dynamic compilation of each physical based component model all of these features enable the telemac mascaret system to operate with other existing or future codes without restricting access or implementation it therefore becomes natural to drive these apis using python scripting language as mentioned by knox et al 2018 this scripting language offers numerical and scientific libraries and is already a recognized tool in environmental resource modeling finally the python wrapping allows use of the telemac mascaret system for a wide range of studies such as optimization coupling and uncertainty quantification the application programing interface api framework described in this work fortran apis and its python wrapper is available for download and distributed in telemac mascaret system www opentelemac org 4 gironde estuary application case the hydrodynamic model telemac 2d was used to solve the shallow water equations see section 4 2 in the real case of the gironde estuary in southwestern france fig 6 this important large scale estuary involves many economic and environmental considerations several hydrodynamic and morphodynamic telemac 2d studies of the estuary have been carried out for the last decade huybrechts et al 2012 villaret et al 2010 in these studies the importance of the calibration phase was systematically emphasized to achieve operational performance model calibration aims at simulating a series of reference events by adjusting some uncertain physically based parameters in order to minimize deviation between measured and computed values of variables of interest this process of parameter estimation a subset of the so called inverse problems consists in evaluating the underlying input data of a problem from its solution for free surface flow hydraulics parameters that are often unknown or difficult to assess include initial state bathymetry bed friction and model boundary conditions in this work the bathymetry and initial state are considered as known a spin up of 12 hours was used to generate realistic initial condition bathymetry uncertainty was not considered in this study and could constitute an approximation in particular on current speed cea and french 2012 a primary challenge of integrating bathymetry in the calibration process is the spatial structure characterization of its uncertainty in relation to channel morphology legleiter et al 2011 this was not studied here and will be the topic of a future study consequently both bed roughness and model boundary conditions are considered in the calibration process as mentioned by williams and esteves 2017 the hydrodynamic bed friction is a primary calibration variable for all coastal and estuarine models it is also essential for modeling other processes accurately such as sediment transport and wave attenuation concerning the model boundary conditions only offshore boundary condition is considered in the calibration process as it is derived from a coarser scale model and upstream boundary conditions is coming from measurement the transfer of information between a large scale model and the boundaries of a more local model generally requires empirical adequacy sea level could need to be corrected to account for seasonal variability effect of thermal expansion salinity variations air pressure etc in addition to long term sea level rise resulting from climate change and differences in tidal amplitude could be attributable to meteorological effects storm and surge atmospheric and wave setup idier et al 2019 although this paper focuses on friction and tidal parameters all telemac 2d variables can be changed by the api as presented in table a1 4 1 numerical configuration and available data the gironde is a navigable estuary in southwest france formed by the confluence of the garonne and dordogne rivers just downstream of the center of bordeaux the gironde estuary is the largest estuary in western europe the hydraulic model covers approximately 195 km between the fluvial upstream and the maritime downstream boundary conditions for an area of around 635 km2 the finite element mesh is composed of 173 781 nodes fig 6 the mesh size varies from 40 m within the area of interest the navigation channel to 750 m offshore western and northern sectors of the model as shown in fig 6 six friction areas are considered in the hydraulic model the boundary condition along the marine border of the model was set using depth averaged velocities and water levels from the legos numerical model tugo dataset 46 harmonic constants surge data describing the difference between the tidal signal and the observed water level are taken into account using a data file generated from the hycom2d model of the shom chassignet et al 2007 surface wind data are also considered to simulate the flow under windy conditions time evolution flow discharge hydrographs are imposed upstream of the gironde estuary model on the garonne and dordogne rivers the ability of the numerical model to reproduce complex physical processes such as sediment transport is presented in orseau et al 2020 the free surface flow was continuously measured every minute at the verdon richard lamena pauillac fort médoc ambes the marquis bassens and bordeaux locations fig 6 for this study observation results are used over a 36 hour period from august 12 to 14 2015 4 2 hydrodynamic model the telemac 2d code solves the 2d depth averaged free surface flow equations also known as shallow water equations eqs 1 3 1 h t x h u y h v 0 2 h u t x h u u y h u v g h z s x h f x h ν e u 3 h v t x h u v y h v v g h z s y h f y h ν e v where x and y are the horizontal cartesian coordinates t the time u and v the components of the depth averaged velocity h the water depth ν e an effective diffusion representing depth averaged turbulent viscosity ν t and dispersion z s the free surface elevation g the gravitational acceleration f x and f y refer to the friction force see section 4 2 1 telemac 2d solves the previous equation system using the finite element method on a triangular element mesh the main results at each node of the computational mesh are the water depth and the depth averaged velocity components telemac 2d can take into account propagation of long waves including non linear effects bed friction effect of the coriolis force effects of meteorological phenomena e g atmospheric pressure rain or evaporation and wind turbulence supercritical and subcritical flows influence of horizontal temperature and salinity gradients on density and dry area in the computational domain amongst other processes hervouet 2007 4 2 1 friction coefficient the friction term in the momentum part of the shallow water equations is treated in a semi implicit form within telemac 2d the two components of friction force are given in eq 4 4 f x u 2 h c f u 2 v 2 f y v 2 h c f u 2 v 2 where c f is a dimensionless friction coefficient the roughness coefficient often takes into account friction caused by walls on the fluid or other phenomena such as turbulence empirical or semi empirical formulas are used for calculating c f morvan et al 2008 in the present work c f is given by the widely used strickler formulas eq 5 the strickler coefficient m1 3s 1 is a calibration parameter of the modeling system to be adjusted according to field data e g usual measured water levels 5 c f 2 g h 1 3 k s 2 generally the friction coefficient is contained in an interval bounded by physical values as it results from different contributions e g skin friction bed form dissipation etc in this study the strickler bounds are taken as large as possible in fact the lower bound of the strickler coefficient is set to 5 according to u s f h a united states federal hig 1984 rough bed surface and the upper one is set to 115 extreme smooth bed surface in the present api context the friction coefficient is called model chestr and in the study corresponds to the strickler coefficient k si where i denotes the considered friction area 4 2 2 tidal amplification parameter tidal characteristics are imposed using a database of harmonic constituents to force the open boundary conditions for each harmonic constituent the water depth h and components of velocity u and v are calculated at point p and time t by eq 6 6 f p t i f i p t f i p t f i t a f i p c o s 2 π t t i φ f i p φ i 0 l i t where f is either the water depth h or one of the components of velocity u or v i refers to the considered constituent t i is the period of the constituent a f i is the amplitude of water depth or one of the horizontal components of velocity φ f i is the phase f i t and l i t are the nodal factors and φ i 0 is the phase at the original time of the simulation the water level and velocities of each constituent are then submitted to obtain the water depths and velocities for the open boundary conditions eq 7 7 h α i h i z b z r e f γ u β i u i v β i v i where z b is the bottom elevation and z ref the mean reference level in eq 7 the tidal amplitude multiplier coefficients of tidal range and velocity respectively α and β at boundary locations and the sea level correction γ are assumed to be the tidal calibration parameters pham and lyard 2012 the sea level correction parameter is assumed following expert opinion to be contained in an interval of plus or minus a meter the interval of the weighting coefficients of tidal range and velocity is set to 0 8 1 2 corresponding to a 20 margin of the initial value in the api framework the sea level correction variable γ is designated as model sealevel and the weighting coefficients of tidal range α and velocity β are denoted model tidalrange and model velocityrange respectively 4 3 summary of parameter range variation all the model input parameters and their associated probability distribution are summarized in table 1 5 calibration problem the inverse problem of calibration can be understood as the computation of the posterior distribution π x y model calibration algorithm step fig 7 as presented in appendix c where model parameters constitute the p components of the parameter control vector x x 1 x p r p composed of independent variables defined on some probability space ω a p with ω as a sample space a the σ algebra of events and p the probability measure y r m is the observation vector also defined on a probability space around the unknown parameter vector x and r m is the observation space defined by eq 8 8 y g x ε m where g r p r m is a vector valued function of vector x and ε m r m is an observable measurement noise such as e ε m 0 and r c o v ε m e ε m ε m r m m and identified as a multivariate normal distribution ε m n 0 r as a reminder in the gironde estuary application case the parameter control vector x is composed of the friction and tidal parameters and the observation parameter y is composed of water levels measured every minute at an observation station note that the operator g enabling the passage from the parameter space where vector x lives to the observation space where vector y lives represents a call to the api study workflow defined in fig 4 as telemac 2d is a discrete time dynamic model the output o is composed of scalar output water elevation at observation stations interpolated from telemac 2d computational nodes given at t t 1 t finally the maximum a posteriori is equivalent to the optimal search for the control vector minimizing the objective function j x 1 2 x x 0 b 1 x x 0 1 2 y g x r 1 y g x where b is the prior covariance matrix this is known as the traditional variational data assimilation cost function called 3d var carrassi et al 2018 mathematical methods can be used to solve optimization problems the former can vary significantly according to the form of the cost function convex quadratic nonlinear etc its regularity and the dimension of the space many deterministic optimization methods are known as gradient descent methods among which is the broyden fletcher goldfarb shanno bfgs quasi newton method used in this work morales and nocedal 2011 using this constrained optimization method makes it possible to impose boundaries during the research process of the model parameters guaranteeing their physical values because the inverse problem is often ill posed and unstable with available data corresponding to more than one solution small changes in model results can lead to very different estimates for the input calibration parameters these problems are related to the issue of parameter identifiability navon 1998 still the chosen optimization method involves computing the adjoint of the observation operator g or the partial derivatives of the operator with respect to its input parameters in this work the partial derivatives are approximated using a classical finite difference method this is a simple solution numerically sensitive and computationally time consuming but the observation operator can be written to make use of parallel computing thus providing a fast automatic calibration algorithm relevant to real world applications as the inverse problem is defined a relevant question is what are the effects of the modeling calibration parameters x on the simulated state variables o t m x t which are compared to the observations this question can be addressed by multivariate sensitivity analysis 5 1 multivariate sensitivity analysis the sensitivity analysis aims at quantifying the relative importance of each input model parameter the variance based methods aim at decomposing the variance of the output to quantify the participation of variables considered as independent conventional approaches to global sensitivity analysis gsa compute sensitivity indices called sobol indices and assume that the output variable is scalar the definition of sobol indices is a result of the anova analysis of variance variance decomposition sobol 2001 saltelli 2002 however as reported by many authors campbell et al 2006 lamboni et al 2011 garcia cabrejo and valocchi 2014 when this approach is applied to each variable of a functional output it leads to a high degree of redundancy because of the strong relationship between responses from one time step to another moreover it also misses important features of the output dynamic because many features cannot be efficiently detected through single time measurements thus the methodology used in this work to compute gsa is a multivariate sensitivity analysis computing generalized sensitivity indices lamboni et al 2011 which are similar to aggregated sensitivity indices gamboa et al 2013 as demonstrated in garcia cabrejo and valocchi 2014 marrel et al 2017 5 1 1 analysis of covariance to perform sensitivity analysis some parameters and input variables are identified as uncertain while the others are fixed at given nominal values telemac 2d denoted m thereafter ensures the relationship between the vector of model inputs x and the output quantity of interest o t whereby o t m x t direct model evaluation fig 7 since telemac 2d is a discrete time dynamic model for one interest point the model output o is composed of scalar output given at t t 1 t such as o o 1 o t with o i o t i m x t i the covariance of the model vector output o o 1 o t can be partitioned into summands of increasing dimensions single pairs triplets and so on of input variables as shown in eq 9 9 c o v o 1 o t i 1 p c o v i o 1 o t 1 i j p p c o v i j o 1 o t c o v 1 p o 1 o t from this equation some sensitivity indices of multivariate case can be defined by projection of the covariance matrix into scalar using the trace operator gamboa et al 2013 present the following aggregated indices 10 s 1 i o 1 o t t r c o v i o 1 o t t r c o v o 1 o t s t i o 1 o t t r c o v i o 1 o t 1 i j p p t r c o v i j o 1 o t t r c o v 1 p o 1 o t t r c o v o 1 o t 5 1 2 computation of the multivariate sensitivity indices to handle dynamic system behavior under parameter uncertainty a sample set of configurations is generated using latin hypercube sampling lhs in the sampling procedure the parameter uncertainties are taken from uniform distribution whose limits are defined by the parameter bounds see section 4 3 for a fixed number of desired configurations n n p lhss are feasible it is therefore possible to select the plan optimizing a uniformity criterion discrepancy or a statistical criterion entropy in this study the uniformity criterion relying on l 2 discrepancies was considered and optimized as described in damblin et al 2013 in fact according to these autors the l 2 discrepancy optimized lhs have shown some good properties to 2d projections in high dimension the full set of output dynamics over the complete latin hypercube sampling gives a matrix of size n t 11 o o 1 o t o 1 1 o 1 2 o 1 t o 2 1 o 2 2 o 2 t o n 1 o n 2 o n t each row of the matrix o corresponds to a temporal trajectory for a fixed scenario of uncertain input parameter as proposed by lamboni et al 2011 a principal component analysis pca can be carried out on the matrix o c which is the matrix obtained by centering each column of o the pca decomposition consists in a small number determination of variable h 1 h t to transcribe the maximum information in the sense of variance contained in the initial variables o c o 1 c o t c with o i c o i e o i where e is the mean operator and o i the discrete form of o i in order to remove redundancies with the least possible loss of information the variables h i are constructed to be uncorrelated from each other based on an iterative process that adds complementary information not contained in the previously constructed variables this methodology can be summarized by resolution of the system described by eq 12 12 h i o c w i i 1 t with v a r h 1 v a r h 2 v a r h t 0 under the constraint w i 2 1 in practice w 1 w t is a set of normalised and mutually orthogonal eigenvectors associated with eigenvalues λ 1 λ t of the covariance matrix σ 1 n e o c t o c once the eigenvectors resulting from the diagonalization of σ are determined the matrix o c can be decomposed as written in eq 13 13 o c i 1 t h i w i h i variables are uncorrelated and the principal component matrix h h 1 h t has the same total inertia as o but is mostly concentrated in its first columns thus the expression of covariance decomposition can be expressed as a sum of variance of h i on which the sobol decomposition into summands of increasing dimension can be carried out eq 14 14 t r c o v o 1 o m k 1 t i 1 p v i k 1 i j p p v i j k v 1 p k with v ε k v a r e h k x ε w w ε v w k where v a r e h k x ε is the variance of the conditional expectation of h k to the input variable x ε thus based on an identification process between the formulation that is described in eq 10 and eq 14 the expression of multivariate sensitivity indices can be given as described in eq 15 15 s 1 i o 1 o m k 1 t s i k o b λ k k 1 t λ k s t i o 1 o m k 1 t s o b t i k λ k k 1 t λ k where s ε k o b denotes sobol sensitivity indices such as s ε k o b v ε k v a r h k and s o b t i k represents the total sobol sensitivity index such as s o b t i k i ε v ε k v a r h k as expressed by eq 15 the multivariate sensitivity analysis indices involve sobol index computation the next paragraph is dedicated to the computation process of this mathematical expression based on a combined pca and polynomial chaos expansion pce emulator 5 1 3 reduced order model pca pce as already mentioned python is a language that can be used in many scientific contexts and can be adapted to any type of use based on dedicated libraries the multivariate sensitivity analysis explained in the previous section was carried out based on an open source library for uncertainty treatment named openturns standing for open source initiative to treat uncertainties risks n statistics www openturns org baudin et al 2017 a design of experiment doe of size 1000 n 1000 is constructed for the pca pce learning step this number of model evaluation was determined based on a convergence study carried on the sensitivity analysis see section 5 1 4 the mpi library was used for launching and managing the telemac 2d solver computations at the end of the calculation a telemac 2d result file was composed of 288 time records corresponding to a physical variable record every 10 minutes t 288 in the present study the discrete version of the pca singular value decomposition svd of the matrix o c is considered this means that h i considered is expressed as h i n λ i a i with a i orthonormal in order to have an efficient sensitivity index computation in terms of computational cost the k main eigenvectors which explained more than 99 95 of the original variance are considered k 10 t it is important to note that owing to truncation the equivalence between aggregated sensitivity indices gamboa et al 2013 and generalized sensitivity indices lamboni et al 2011 no longer exists the generalized sensitivity indices represent an approximation of the aggregated sensitivity indices for each eigenvector a learning sample of realizations of the decomposition coefficient h i is available as proposed by garcia cabrejo and valocchi 2014 a polynomial chaos expansion can be used as learning step of each decomposition coefficient thus the sobol indices needed in eq 15 are obtained based on post treatment of each mode pce as proposed in sudret 2008 here the construction of the pce is carried out based on least angle regression stage wise method lars in order to construct an adaptive sparse pce in this approach a collection of possible pces ordered by sparsity is provided and an optimum can be chosen with an accuracy estimate it was performed in this study using corrected leave one out error blatman and sudret 2011 at this stage a reduced order pca pce model is produced for each observation station of the gironde estuary fig 6 the validation of these emulators was carried out based on a validation sample generated with the same procedure optimized lhs as for the learning step this sample was composed of 100 validation points n val 100 and allows after the telemac 2d unit computing a temporal predictivity criterion defined by eq 16 16 q 2 t 1 j 1 n v a l m k x j t m x j t 2 j 1 n v a l m k x j t 1 n v a l i 1 n v a l m x j t 2 where m k x j t is the estimated pca pce evolution fig 8 shows from top to bottom a comparison on a validation configuration between an estimated result from the reduced order pca pce model and the telemac 2d computation the difference between these two water elevations is then given and finally the predictivity criterion is presented the reduced order model is validated according to the performance displayed in fig 8 indeed the results provided by the emulator and the telemac 2d solver are fairly close the deviation between these results varies respectively from 10 to 20 cm at verdon and bordeaux observation stations this is confirmed by the average predictivity criterion close to 1 for verdon and bordeaux during the study period a coefficient close to 1 shows a good fit between the validation database and the result estimated by the reduced order model however a less satisfactory performance of q 2 criteria can be noticed at the bordeaux observation station compared to the verdon station the confluence of the dordogne and garonne rivers occurs just downstream from bordeaux and the influence of the hydrological forcing of these two rivers is not studied here consequently the upstream fluvial discharges are not considered as parameters in construction of the pca pce emulator the missing interactions can explain the observed performance gap in the predictivity criteria to conclude the reduced pca pce model shows good agreement with the telemac 2d computation the high score of predictivity criteria allows confidence in the sensitivity results post treated from the reduced order model 5 1 4 multivariate sensitivity analysis results a reduced order model based on pca pce is used to identify influential input parameters as reported by pianosi et al 2016 when applying sampling based sensitivity analysis sensitivity indices are not computed exactly but they are approximated from the available samples the robustness and convergence of such sensitivity estimates should therefore be assessed thus three elements are of interest in the following analysis i the convergence of sensitivity indices ii the robustness of the estimates and iii relevant sensitivity analysis visualization convergence to handle this issue the convergence rates of generalized sensitivity indices are assessed as the sample size increases fig 9 presents the evolution of sensitivity indices obtained at bordeaux observation station as displayed on fig 9 the number of samples needed to reach stable sensitivity estimates can vary from one input factor to another however from a sample size of 1000 the sensitivity indices are stabilized thus this number of model evaluation is considered in the following investigations robustness a robustness analysis is carried out in order to evaluate the sensitivity of the estimates to the design of experiment first the robustness of the sensitivity indices is analysed through confidence intervals they are obtained by repeating 30 times the methodology described above see section 5 1 3 table 2 presents the result of these confidence intervals which have finally required n 30 n 3 104 model calls as shown in table 2 the bounds of the min max confidence intervals are relatively close and demonstrate the capacity of the reduced order pca pce model constructed from a l 2 discrepancy optimized lhs sample to produce precise estimates an alternative sampling method based on low discrepancy sequence of sobol is also considered the obtained results are presented in table d2 most of the time the sensitivity indices obtained with the low discrepancy sequence are included in the min max confidence intervals obtained from l 2 discrepancy optimized lhs sample when out of range values stay close to the bounds consequently the estimates can be considered satisfactory in terms of accuracy and robustness results visualization visualization can significantly improve the interpretation of the sensitivity analysis results for this purpose radial convergence diagrams also called chord graphs are used to simultaneously visualize some computed generalized sensitivity indices fig 10 these diagrams plot the main effect of each input variable first order multivariate sensitivity analysis proportional to the size of the inner circle its total influence including interactions proportional to the size of the outer circle existence and extent of second order effects second order multivariate sensitivity indices proportional to width butler et al 2014 fig 10 displays sensitivity analysis estimates from one l 2 discrepancy optimized lhs sample used to compute the min max confidence intervals presented in table 2 as shown in the figure the output variance at each observation station is mainly explained by the upstream friction coefficients areas one and two of the gironde estuary model and the sea water level as expected downstream from the estuary bordeaux station the friction coefficient of area 3 containing the bordeaux station has a more significant influence it is noticeable from the visualization of variable interactions that even if the tidal range variable contribution is not considerable its interaction with other parameters should not be neglected this enhances the utility of simultaneously visualized interactions with main and total effects of sensitivity analysis to conclude the calibration problem initially composed of nine input variables can be reduced to five the first three being friction area sea level correction and tidal range coefficients after considering the results of multivariate sensitivity analysis 5 2 calibration results the calibration algorithm fig 7 is performed by coupling telemac 2d and the data assimilation library adao in python context through the component telapy of the telemac mascaret system adao a module for data assimilation and optimization provides modular data assimilation and optimization features in python https pypi org project adao argaud 2019 it can be coupled with other modules or external codes while providing a number of standard and advanced data assimilation or optimization methods the adao library also covers a wide variety of practical applications from real engineering to experimental methodologies its architecture and numerical scalability adapt to the field of application as shown by eq 18 the optimal search for the control vector x takes a minimization form of an objective or cost function given by the expression inside the exponential term which must satisfy the background error statistics prior term and the equivalent observation error likelihood term this minimization process equivalent to the maximum a posteriori search is carried out using the 3d var algorithm the control parameter is composed of the five most influential variables identified by the multivariate sensitivity analysis performed previously the initial guess x 0 is set to random values inside the constrained search space x 0 k s 1 73 76 k s 2 83 62 k s 3 83 62 α 0 9729 γ 0 8611 the observation vector y is the free surface flow evolution extracted every 60 seconds at the verdon lamena pauillac fort médoc bassens and bordeaux locations from noon august 12 to midnight august 14 2015 the chosen optimization method involves computing the partial derivatives of the observation operator g with respect to x a classical finite differences method with a differential increment set to 10 4 the error background and observation covariance matrices respectively identified by b and r are token diagonals meaning they have no error correlations a small variance value for r justifying great confidence in the observation value is considered such as σ m 2 0 1 y on the contrary little confidence is given to a prior part such as σ b 2 10 x 0 as shown by figs 11 and 12 the automatic calibration algorithm finds an optimal solution in about 30 iterations with the following set of parameters x m a p k s 1 47 99 k s 2 59 63 k s 3 67 485 α 0 9114 γ 0 5344 a rapid decrease in cost function can be observed for the first algorithm iterations the cost function curve behavior is smooth until iteration number 20 where a slight decrease can be observed as expected a similar behavior is observed for the parameter to be calibrated fig 13 displays the results of automatic calibration over the computation period as expected the water surface profiles calculated from the calibrated parameter configuration are much closer to measurements than the ones computed from the background knowledge parameters the final results emphasize the efficiency of the automatic calibration tool in the framework of a real configuration for most of the studied period the difference between observations and the calibrated configuration is less than 20 cm about 5 of the tidal range at the verdon and bordeaux stations however at the bordeaux observation station the calibrated configuration presents some error peaks at low tide this phenomenon can be induced by a greater effect of the fluvial part of the estuary which is not well represented in the model where discharge is expressed as an hourly average 6 discussion the concept of interoperability is a generic solution for gathering and exchanging information from various multidisciplinary knowledge the application presented in this article is a case of 2d hydrodynamics and is not representative of all the possibilities offered by the interoperability of telemac mascaret system for example the estuarine sediment transport could be taken into account in order to better model and understand the evolution of the bed with the api of the gaia module more generally the approach presented here can be easily applied to different geoscience problems where telemac mascaret is relevant all apis of the different modules are freely available as they are part of the telemac mascaret system applying interoperability criteria on an old and large code is not an easy task because of the transformation effort it requires for the telemac mascaret system the transformation towards the concept of interoperability required several years of development a recommendation is to take into account these criteria at the early stage of development considering the fact that the code will probably have to interact with the outside world the implementation of apis makes it possible to extend the scope of the software by facilitating the use for different types of applications for instance based on fluid exchange of information apis can be used to couple the telemac mascaret system with a computational fluid dynamics cfd type software for example code saturne archambeau et al 2004 www code saturne org to take into account atmospheric and groundwater flows the development of standards for publishing interoperable softwares in forms suitable for community interactions remains a major issue laniak et al 2013 the present work allows the telemac mascaret system to be integrated in different environments however the wrapper for each specific environment must be maintained a lean standard has been proposed with the openmi environment gregersen et al 2007 the integration of the telemac mascaret system in this environment could constitute an outlook to this work since python is really easy to pick up and learn a wrapper in this language is distributed in the telemac mascaret system official version telapy this choice aims to favorise the dissemination of the hydro informatic system as an environmental modeling tool however the system compilation is a cumbersome process which can slow down its dissemination to overcome this issue and based on the python package index pypi https pypi org a compiled version of telapy might be envisaged to provide all dynamic libraries needed to run telemac mascaret system apis to demonstrate that telapy is a functional system an example of hydraulic model calibration is presented this case deals with a series of reference events by adjusting some uncertain physically based parameters until the comparison with observations achieves sufficient accuracy if performed manually the model calibration is time consuming fortunately the process can be largely automated to significantly reduce human workload as shown in this paper a reduced order model based on pca pce is used to identify influential input parameters this emulator has been created and validated on the basis of learning and validation solver computations a major issue arising from this methodology concerns the optimization of the number of computational runs needed for sufficient results in the sensitivity analysis approximation with the surrogate models must become sufficiently accurate with just limited data available for the learning step there have been some recent advances in this area based on adaptive sampling steiner et al 2019 after most influential parameters have been identified they are then calibrated using a data driven technique the chosen algorithm is based on a minimization process requiring derivatives of the telemac 2d solver with respect to the parameters to be calibrated several options exist to obtain the derivatives the finite difference method used in this work is easily implemented but returns approximate derivatives whose poor accuracy can degrade the performance of the complete application a much better option is to create a new program that computes the exact analytical derivatives of the model algorithmic differentiation griewank and walther 2008 is a way of automating creation of the derivative program thus providing accurate derivatives for a minimal development effort gradient based methods are very useful to find local extreme values within a reasonable time but they cannot pretend to find a global solution in the search domain on the other hand metaheuristic optimization algorithms are useful in finding the area of a global solution minimum or maximum but the convergence becomes much slower due to the large number of required simulations one way to improve the efficiency of the calibration would be to combine a derivative free algorithm like a metaheuristic with a gradient based method like bfgs to obtain better solutions within a reasonable time hybridization 7 conclusion the application programming interface api of the open source telemac mascaret system was developed to convert a heterogeneous set of open source user contributed models into a suite of plug and play modeling components that can be reused in many different contexts the apis provide a user friendly development framework that can be easily understood allowing seamless integration of base codes and do not invalidate existing institutional software development practices the api development framework does not compel the use of or supply specific environmental modeling standards as its services standardize at a more basic level of internal communication this gives researchers and model developers more freedom to customize services for the problems they are facing the telemac mascaret system api seeks to assist the hydrodynamic community in resolving new challenges such as uncertainties real time data assimilation and multi physics simulations many model developers have limited skills in software development and architecture recognizing this and seeking to promote the widest possible adoption of the telemac mascaret system api by the user community a scripting feature of modeling is privileged with the python language as recommended by knox et al 2018 this forms a new module of the system called telapy work performed with python can be transposed to other scripting languages such as r and julia python is a language that can be used in many contexts and adapted to any type of program based on dedicated libraries however it is particularly used to automate tedious tasks such as the calibration process of a numerical model saving a significant amount of time in realizing a project use of python is widespread in the scientific community and it has many libraries optimized and intended for numerical computation this flexibility was demonstrated in the gironde estuary case first a sensitivity analysis was carried out to identify sensitive parameters to calibrate the most influential variables on water depth variation were the upstream friction coefficients k s1 k s2 k s3 the tidal amplitudes multiplier coefficient of tidal range α and the sea level correction γ this was achieved by linking a telemac 2d physical based component to an uncertainty quantification library to achieve better model accuracy a calibration process was realized through a physical based data driven technique using a data assimilation library to promote and facilitate the dissemination of the deployed approach to the telemac mascaret community the development of a graphical user interface gui can be useful the major benefits are user friendliness efficiency and enhancing the quality of hydraulic studies the calibration process deployed here can be extended to other solvers of the telemac mascaret system water quality sediment transport wave propagation and so on moreover there are key potentially available sources of information on continental water bodies in situ and remote sensing data for instance data assimilation algorithms for integrating observation data into real cases are now increasingly applied to hydraulic problems with two main objectives optimizing model parameters and improving stream flow simulation and forecasting therefore the ability to exchange data as computational results has become a growing need facilitated by interoperability software availability the application programing interface api framework described in this article fortran apis and its python wrapper is available for download in telemac mascaret system www opentelemac org telemac mascaret is an integrated suite of solvers for use in the field of free surface flow available under the gnu general public license version 3 declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments the authors gratefully acknowledge contributions from the open source community especially that of openturns open source initiative for the treatment of uncertainties risks n statistics and adao a module for data assimilation and optimization in particular we would like to thank angelique ponçot and jean philippe argaud from edf r d for their constructive discussion on data assimilation regis lebrun from airbus for the pod construction based on openturns and all of the edf r d openturns team the authors also would like to address special thanks to kamal el kadi abderrezzak for his support and deep reading that greatly enriched this paper finally we would like to thank the anonymous reviewer and arnald puy whose comments and suggestions helped improve the manuscript appendix a telemac 2d list of api variables table a 1 telemac 2d variables accessible through api table a 1 variable name definition model at current time model dt time step model bcfile boundary condition file name model bnd tide options for tidal boundary condition model bottomelevation bottom elevation model chestr roughness coefficient model fair wind influence coefficient model cote water level on boundary conditions model cpl period coupling period with telemac mascaret solvers model debit flowrate discharge on boundary conditions model debug activation debug mode keyword model flux boundaries flux on the boundary conditions model geometryfile geometry file name model meteofile binary atmospheric file name model fo2file formatted data file name model liqbcfile liquid boundary file name model prefile previous computation file name model graph period graphical output period model hbor value of prescribed water depth on boundary conditions model ikle mesh connectivity table model nachb number of processors sharing a given node model knolg pointer to get the global numbering of a node in the initial mesh model kp1bor boundary node connectivity model lihbor boundary type for the water depth model listin period listing output period model liubor boundary type for horizontal velocity model livbor boundary type for vertical velocity model lt current time step model compleo graphical output counter model ptinig number of the first graphical printout time step model nptir number of border nodes for subdomains model nbor boundary node global number model nelem number of elements in the discretization mesh model nelmax maximum possible number of elements model npoin number of nodes in the computational mesh model nptfr number of boundary nodes model ntimesteps number of time steps model numliq liquid boundary number model porosity porosity model resultfile result file name model sealevel sea level calibration coefficient model tidalrange tidal range calibration coefficient model velocityrange velocity range calibration coefficient model ubor value of prescribed horizontal velocity on boundary conditions model vbor value of prescribed vertical velocity on boundary conditions model velocityu horizontal velocity model velocityv vertical velocity model waterdepth water depth model x horizontal coordinates of mesh points model xnebor horizontal component of the normal at boundary nodes model y vertical coordinates of mesh nodes model ynebor vertical component of the normal at boundary nodes model equation name of solved equation model ak turbulent kinetic energy term of k ε turbulence model model ep dissipation term of k ε turbulence model model iturb turbulence model model init depth initial value of water depth model tracer tracer value on mesh nodes model ntrac number of tracers model flowrateq solid transport flowrate model dcla median grain size value model shields critical shields parameter model xwc settling velocity model z free surface elevation model qbor value of prescribed discharge flowrate on boundary nodes model ebor value of prescribed turbulence dissipation term on boundary nodes model flbor value of prescribed bottom elevation on boundary nodes model tob shear stress model liqbor boundary type for the discharge model nsicla number of bed material size classes model nomblay number of layers in the bed model concentration concentration at the current time step model evolution evolution of bed model partheniades parthenaides erosion coefficient of each bed layer model volu2d basis integral appendix b image 2 appendix c bayesian inference formulation of calibration problem the posterior distribution π x y can be determined through the well known bayes rule eq 17 17 π x y π y x π x π y x π x d x the term π y x called the likelihood can be interpreted as the probability density function of the observed data conditional upon a set of parameter values considered as random variables and as a measure of the information provided by the observations on the parameter values from eq 8 the likelihood is expressed as π y x exp 1 2 y g x r 1 y g x the term π x represents a priori knowledge of the unknown parameters x this term is classically taken as a multivariate normal distribution with known mean x 0 derived from measurement data or previous computation and covariance matrix b r p p positive definite such as π x exp 1 2 x x 0 b 1 x x 0 from the previous expressions of a priori and likelihood the posterior distribution is given by the following equation 18 π x y exp 1 2 y g x r 1 y g x 1 2 x x 0 b 1 x x 0 the maximum a posteriori map is equivalent to the formulation of the optimal search of control vector x which must satisfy the a priori error statistics j b 1 2 x x 0 b 1 x x 0 and the equivalent observation error statistics j o b s 1 2 y g x r 1 y g x appendix d generalized sensitivity analysis based on low discrepancy sequence of sobol table d 2 generalized sensitivity indices gsi based on 1024 210 low discrepancy sobol points at bordeaux and verdon observation stations table d 2 inputs gsi verdon bordeaux first order total order first order total order k s1 0 126 0 133 0 110 0 156 k s2 0 01 0 0153 0 358 0 437 k s3 2 29 10 5 7 45 10 5 0 0816 0 118 k s4 0 1 33 10 5 9 69 10 5 3 44 10 4 k s5 2 53 10 5 6 50 10 5 2 06 10 3 3 54 10 3 k s6 1 65 10 7 8 13 10 6 4 17 10 5 1 86 10 4 α 0 0464 0 0492 0 0135 0 019 β 4 82 10 4 5 07 10 4 1 81 10 4 2 70 10 4 γ 0 809 0 809 0 341 0 364 
25660,water resource management is of crucial societal and economic importance requiring a strong capacity for anticipating environmental change progress in physical process knowledge numerical methods and computational power allows us to address hydro environmental problems of growing complexity modeling of river and marine flows is no exception with the increase in it resources environmental modeling is evolving to meet the challenges of complex real world problems this paper presents a new distributed application programming interface api of the open source telemac mascaret system to run hydro environmental simulations with the help of the interoperability concept use of the api encourages and facilitates the combination of worldwide reference environmental libraries with the hydro informatic system consequently the objective of the paper is to promote the interoperability concept for studies dealing with such issues as uncertainty propagation global sensitivity analysis optimization multi physics or multi dimensional coupling to illustrate the capability of the api an operational problem for improving the navigation capacity of the gironde estuary is presented the api potential is demonstrated in a re calibration context the api is used for a multivariate sensitivity analysis to quickly reveal the most influential parameters which can then be optimally calibrated with the help of a data assimilation technique keywords application programing interface api environmental modeling physical based data driven modeling sensitivity analysis 1 introduction water availability and quality are vital to human health increasing global scarcity makes anticipating the evolution of this limited natural resource even more essential this ability relies on the understanding and prediction of hydrodynamic flows hydraulic simulation codes such as hec ras dyhouse et al 2007 mohid mateus and neves 2013 delft 3d deltares 2014 telemac mascaret hervouet 2007 have been developed for many years and applied to a wide range of hydro environmental cases such as prevention of flood risks teng et al 2017 development of management plans ran and nedovic budic 2016 or design schemes for flood alleviation ogie et al 2020 hydro environmental modeling of river and marine flows is increasingly complex and requires a better knowledge of physical and biochemical processes advanced numerical methods and high performance computing resources for many applications engineers have to deal with complex modeling systems for the representation of subparts of a physical system braunschweig et al 2004 harpham and danovaro 2014 increasing a numerical model conceptual complexity in terms of how many aspects of the underlying system are included in it has the potential to improve the accuracy of the results in term of descriptive and predictive capacities however adding each new aspect to the model can induce an increase of the error brought about by the uncertainty in the description of the features accumulates and increases the overall uncertainty in the output cea et al 2011 lees et al 2000 this trade off between model completeness and propagation error saltelli 2019 is known as the conjecture of o neill o neill and rust 1979 in maritime or fluvial cases numerical modeling tools allow replicating the past as well as predicting a future with an uncertainty range that is strongly linked to the approximate description of hydraulic parameters pappenberger et al 2008 meteorological data aronica et al 2012 and geographical data abily et al 2016 in spite of the significant improvement in computational resources and the accuracy of numerical models the turbulent nature of fluid mechanics equations limits the performance of hydraulic models the need for realistic hydrodynamic flow simulation is beyond the abilities of deterministic forecast as a consequence model uncertainties have to be quantified recent advances in simulation and optimization of environmental systems have relied on increasingly detailed models running many scenarios in order to quantify these uncertainties herman et al 2015 teng et al 2017 according to solomatine and ostfeld 2008 significant advances in the means of observing continental waters especially through orbital equipment swot satellite mission morrow et al 2019 sentinel satellite missions copernicus program malenovsky et al 2012 allows use of various types of data with their associated analysis for water resources management knox et al 2019 thus environmental modeling is increasingly complemented by data driven models kim et al 2010 moreover by taking into account advances in information processing and data management it becomes possible to reduce uncertainties and better understand the modeling process data driven modeling is a paradigm shift for addressing many problems in science and engineering montáns et al 2019 to summarize with the increase in computer resources environmental modeling is evolving with the need to solve increasingly complex real world problems involving the environment and its relationship to human systems and activities if it is to break down research silos and bring scientists stakeholders and decision makers together to solve social daloglu et al 2014 economic harou et al 2009 and environmental problems an environmental modeling system should support it tasks such as aggregation of model components into functional units component interaction and communication parallel computing and cross language interoperability david et al 2013 laniak et al 2013 these technical requirements are closely related to interoperability aspects namely the capacity of the software to run and share information with other codes environmental system development requires both scientific understanding of the phenomena involved and software development proficiency this paper presents a new application programming interface api of the open source and interoperable telemac mascaret system https gitlab pam retd fr otm telemac mascaret to solve environmental problems lacombe et al 2013 goeury et al 2017 since 2017 the telemac mascaret system has included in its distribution a generic api that allows it to expand its range of applications audouin et al 2017 with this newly implemented feature the telemac mascaret system can be easily coupled or integrated into higher level platforms to model study or simulate complex problems goeury et al 2015 to our knowledge apis for an environmental modeling system with hydraulic mascaret 1d telemac 2d telemac 3d water quality tracer waqtel sediment transport courlis gaia and wave hydrodynamics tomawac and artemis are not yet widespread in the environmental community and are an original innovation the present work uses as an example france s gironde estuary to demonstrate the operational applicability and outcomes of apis the port of bordeaux in the gironde estuary faces many development challenges klein et al 2018 to satisfy demand for larger ships while ensuring navigational safety the evolution of water depth over time in the estuary needs to be predicted with a high degree of accuracy to address this issue the numerical results must be consistent with past observational data among other things this process relies on calibration to determine empirical adequacy oreskes et al 1994 in particular the calibration aims at simulating a series of reference events by adjusting some uncertain physically based parameters until the comparison is as accurate as possible dung et al 2011 the objective of this work is to implement an efficient calibration algorithm capable of processing measurement optimally in order to estimate the partially known or missing parameters nagel et al 2020 often performed calibrating a hydrodynamic model is nevertheless a difficult task owing to the complexity of the flows and their interaction with the shoreline the bathymetry islands etc it is essential to understand in depth the relationship between the calibration of modeling parameters and the simulated state variables which are compared to the observations in this context a multivariate sensitivity analysis lamboni et al 2011 gamboa et al 2013 succeeded in identifying the most influential input parameters on the model results sensitivity analysis is associated to define the parameters to be calibrated using a physically based data driven technique carrassi et al 2018 this paper is organized as follows section 2 includes a description of the main concepts of the telemac mascaret system api section 3 presents the architecture of implemented apis section 4 describes the application case section 5 deals with the calibration problem section 6 is the discussion and section 7 is the conclusion 2 method the telemac mascaret hydro informatic system created in 1987 is an open source www opentelemac org integrated suite of solvers for use in the field of free surface flows hervouet 2007 as displayed on fig 1 it can carry out simulations of flows mascaret telemac 2d and telemac 3d gray circle sediment transports courlis and gaia orange circle waves artemis and tomawac green circle and water quality tracer and waqtel purple circle historically from the original software solving physics equations saint venant mascaret and telemac 2d navier stokes telemac 3d elliptic mild slope equation artemis simplified equation for the spectro angular density of wave action tomawac exner courlis and gaia water quality processes tracer and waqtel the system has gradually evolved to the notion of hydro informatics with a set of solvers dealing with large heterogeneous data and solving complex dependent problems overlapping physical components in fig 1 the various simulation components use high end algorithms based on the finite element or finite volume method the ability to study global regional or local scale problems by using the same system makes the telemac mascaret a useful tool for assessing the environmental state in the sea estuaries coastline and rivers according to laniak et al 2013 integrated environmental modeling iem highlights the importance of software development and sharing in their role as pluggable components of a larger ecosystem the work presented here is part of this trend it shows the implementation procedure for the component based solver interoperability of the telemac mascaret system the api architecture described in this paper is directly applicable for any software regardless of its language since it is an easy task to build a wrapper in other programming language as presented in the following for fortran and python the main difficult part is to split the software main program when a solver is an interoperable component it can be part of an assembly of elements in permanent interaction that work in a coordinated way to be considered as an elementary reusable component a computation model must possess apis which can be viewed as new functionalities or entry points in the computational model to provide new services on demand have a resolution algorithm comprising three main functionalities problem initialization simulation and finalization of the computation allow instantiation of several clearly separated and identifiable problems be useable in dynamic library form in the following the four points listed above are addressed 2 1 design a computer programming model like the object oriented programming oop aims to organize the software design around data grouped into objects with attributes and methods this help the developer to focus on the data rather than the logic required to manipulate them most of the oop concepts encapsulation interfaces polymorphism modularity instantiation are derived from software engineering patterns but relate to the real world as shown in fig 2 the first concept used here is encapsulation with a classic design pattern facade gamma et al 1994 since the telemac mascaret system is written in fortran 77 and 90 the design pattern has been adapted to this non object oriented language as oop is well suited for the design of large and complex programs the main benefits of this approach are that it is easier to use and understand limits the library user dependencies includes integrity constraints and validation rules and avoids some unnecessary calculations lacombe et al 2013 internal data has been accessed in order to expand the capabilities of the api it is possible to examine all the information embedded in the calculation code and modify its structure specifically the values some metadata and properties at runtime pointers were used to access the simulation data as fortran lacks the pointer definition of other high level languages such as c or c a combination of the pointer and target fortran keywords with adequate updates were employed for instance a manual update must be done if a pointer on an allocatable variable is set before the dynamic allocation since the pointer is no longer valid this ensures the correct state of the controlled variable a component based approach was adopted by having the same structure as all of the telemac mascaret physical dedicated solver to allow them to be viewed as linkable the execution of a simulation was partitioned to allow modification of a given parameter at runtime for example each time step the flexibility that comes with interoperability allows deployment of the telemac mascaret system in specific industrial applications and distribution of its solvers into complex modeling systems applications blue part in fig 3 as mentioned by gil et al 2018 geosciences will lead to a new generation of knowledge rich intelligent systems that contain rich knowledge and context in addition to data enabling fundamentally new forms of reasoning autonomy learning and interaction as presented in fig 3 the telemac mascaret physical dedicated components can be easily connected together to build multi physics or multi dimensional interactions harpham and danovaro 2014 coupling orange part inserted in optimization or uncertainty quantification processes studies yellow part see section 5 wrapped in advanced technology such as cloud based solutions or embedded systems with data management openmi gregersen et al 2007 or pynsim knox et al 2018 cloud services green part and integrated in platforms salome ribes and caremoli 2007 or palm buis et al 2006 for example numeric computing environment pink part 2 2 technology to maintain efficient execution a native code compiler for example gnu compiler collection licensed under gnu gpl running on most systems and hardware architectures and supporting fortran is used and recommended moreover the calculation code of the telemac mascaret system numerically solves a model based on a system of partial differential equations pde the computational solution relies on an implicit time scheme removing stability constraints on the time frame but leading to a linear system solution of considerable size the resulting increased complexity of the model structures poses strong limitations in terms of practical implementation and computational requirements consequently high performance computing hpc is necessary to tackle realistic applications the solution deployed in the hydro informatic system is to decompose the domain of computation this idea of domain decomposition is to assign to each processor one part of the global domain over which it solves the fluid mechanics problem the results of the other processors help in defining artificial boundary conditions arising from the partition the assignments given to each processor are the same but the data differ therefore the telemac mascaret system requires the use of an mpi library for communication between processors clarke et al 1994 it is possible to use mpi technology for the instantiation of several clearly separated and identifiable problems this is because mpi is adapted for distributed memory systems this method has been followed for the learning step of sensitivity analysis presented in subsection 5 1 3 architecture of api the architecture is generic for all telemac mascaret system components fig 1 for the sake of clarity only an api example with the telemac 2d component is shown in the following sections 3 1 fortran structure api s main goal is to control the simulation while running a case for example it must allow the user to suspend the simulation at any time step retrieve some variable values and possibly change them before resuming the calculation to make this possible a fortran structure called instance is used in the api the instance structure gives direct access to the physical memory of variables and therefore allows control of the variable furthermore based on decomposition of the main telemac mascaret system subroutines in three main functionalities initialization simulation of one time step and finalization it is possible to run a hydraulic case for an indefinite sequence of time steps fig 4 describes an example of api workflow where x are input parameters and o t is information extracted at given times for instance in the framework of the gironde estuary application case x represents parameters to be estimated calibrated friction coefficients and tidal boundary condition parameters and o t is the free surface elevation every minute at the measurement stations see section 4 for each action defined above the identity number of the instance is used as an input argument allowing all computation variables to be linked with the corresponding instance pointers these actions must be done in chronological sequence in order to ensure proper execution of the computation in the api main program in fortran and for the shallow water code telemac 2d it will begin with the call of an initialization subroutine call run set config t2d id lu lng comm ierr where all parameters are scalars of type integer and id is an output value giving the instantiation number the input parameter lu is used to redirect the standard output lng has only two possible values for the choice of the english or french language and comm is the mpi communicator for the distributed memory parallelism the output ierr has to be considered with care as a non null value stands for the occurrence of errors a systematic test on this return value is recommended for instance one may want to stop the program accordingly if ierr ne 0 exit ierr after the initialization phase the model has to be read from files with the following instruction call run read case t2d id cas file dico file init ierr the name of the steering file is given with cas file as a character string it stores several input data of the model with key value pairs each possible key is set with a dictionary file whose name is a character string hold by dico file a true value for the logical input init will initialize a common computational kernel of all the modules after this step a dynamic allocation of arrays in memory is necessary followed by a state initialization of the physics call run allocation t2d id ierr call run init t2d id ierr all accessible parameters are listed in predefined keyword lists see appendix a for telemac 2d for example the number of time steps in the simulation is named model ntimesteps and its value is obtained and stored in the variable nbsteps with the following call call get integer id t2d model ntimesteps nbsteps 0 0 0 ierr input parameter can be defined here using the following call call set double id t2d model sealevel sealevel 0 0 0 ierr where the sea level value in the computation accessed through the keyword model sealevel is set to the value of the variable sealevel now the simulation can be done with a loop on the number of time steps while storing all the results of an evolution of the water depth do i 1 nbsteps call run timestep t2d id ierr call get double id t2d model waterdepth waterdepth i 42 0 0 ierr enddo waterdepth is a one dimensional array of size nbsteps it is used here to store the water depth values at node number 42 of the telemac 2d triangular mesh finally the fortran program can end with the instance deletion thus freeing the memory used call run finalize t2d id ierr additional functions are available to handle parallelism see subsection 2 2 for more details on parallelism in the telemac system the data structure in the current version official version 8 2 does not yet allow for multiple instantiations as all the instances point to the same memory area this constitutes a future development to improve api capacity however instantiation associated with the processor communication function overcomes this limitation by ensuring the possibility of several clearly separated and identifiable problems 3 2 python wrapping the use of api is not limited to fortran programing but can also be used by a scripting language to this end a python wrapper is also available it is relatively easy to use the fortran api routines directly in python using the f2py tool of the python scipy library peterson 2009 this tool will make it possible to compile fortran code for use in python the only limitations are on the type of arguments of the functions to wrap python is a portable dynamic extensible free language which allows without requiring a modular approach and object oriented programming van rossum and drake 2009 in addition to the benefits of this programming language python offers a large number of interoperable libraries for scientific computing image processing data processing machine learning and deep learning the link between various interoperable libraries with telemac mascaret system apis allows the creation of an ever more efficient calculation chain capable of responding finely to various complex problems such as the application case presented in this article see section 4 moreover the python scripting language makes it possible to implement a wrapper in order to provide user friendly functioning of the fortran api thus a python overlay was developed to encapsulate and simplify the different api calls for instance a python get function can encapsulate all api fortran type dedicated function get type where type can be replaced by double integer boolean and string file management is also made easier with python copy move and reducing the number of arguments this python wrapping of telemac mascaret system api constitutes a package called telapy distributed in the official version of the hydro informatic system fig 5 the telapy package is provided with a tutorial for people who want to run the telemac mascaret system physical components in an interactive mode with the help of the python scripting language the script corresponding to the application case of section 4 is presented in appendix b 3 3 summary a single coherent fortran structure has been deployed in telemac mascaret in order to ensure interoperability of the system for each of its solvers this is based on three main functionalities instantiation variable values and computation control firstly instantiation associated with the processor communication function ensures the possibility of a simultaneous existence of several clearly separated and identifiable problems secondly based on variable and computation control memory access is permitted during simulation allowing new services to be provided on demand finally the technology allows dynamic compilation of each physical based component model all of these features enable the telemac mascaret system to operate with other existing or future codes without restricting access or implementation it therefore becomes natural to drive these apis using python scripting language as mentioned by knox et al 2018 this scripting language offers numerical and scientific libraries and is already a recognized tool in environmental resource modeling finally the python wrapping allows use of the telemac mascaret system for a wide range of studies such as optimization coupling and uncertainty quantification the application programing interface api framework described in this work fortran apis and its python wrapper is available for download and distributed in telemac mascaret system www opentelemac org 4 gironde estuary application case the hydrodynamic model telemac 2d was used to solve the shallow water equations see section 4 2 in the real case of the gironde estuary in southwestern france fig 6 this important large scale estuary involves many economic and environmental considerations several hydrodynamic and morphodynamic telemac 2d studies of the estuary have been carried out for the last decade huybrechts et al 2012 villaret et al 2010 in these studies the importance of the calibration phase was systematically emphasized to achieve operational performance model calibration aims at simulating a series of reference events by adjusting some uncertain physically based parameters in order to minimize deviation between measured and computed values of variables of interest this process of parameter estimation a subset of the so called inverse problems consists in evaluating the underlying input data of a problem from its solution for free surface flow hydraulics parameters that are often unknown or difficult to assess include initial state bathymetry bed friction and model boundary conditions in this work the bathymetry and initial state are considered as known a spin up of 12 hours was used to generate realistic initial condition bathymetry uncertainty was not considered in this study and could constitute an approximation in particular on current speed cea and french 2012 a primary challenge of integrating bathymetry in the calibration process is the spatial structure characterization of its uncertainty in relation to channel morphology legleiter et al 2011 this was not studied here and will be the topic of a future study consequently both bed roughness and model boundary conditions are considered in the calibration process as mentioned by williams and esteves 2017 the hydrodynamic bed friction is a primary calibration variable for all coastal and estuarine models it is also essential for modeling other processes accurately such as sediment transport and wave attenuation concerning the model boundary conditions only offshore boundary condition is considered in the calibration process as it is derived from a coarser scale model and upstream boundary conditions is coming from measurement the transfer of information between a large scale model and the boundaries of a more local model generally requires empirical adequacy sea level could need to be corrected to account for seasonal variability effect of thermal expansion salinity variations air pressure etc in addition to long term sea level rise resulting from climate change and differences in tidal amplitude could be attributable to meteorological effects storm and surge atmospheric and wave setup idier et al 2019 although this paper focuses on friction and tidal parameters all telemac 2d variables can be changed by the api as presented in table a1 4 1 numerical configuration and available data the gironde is a navigable estuary in southwest france formed by the confluence of the garonne and dordogne rivers just downstream of the center of bordeaux the gironde estuary is the largest estuary in western europe the hydraulic model covers approximately 195 km between the fluvial upstream and the maritime downstream boundary conditions for an area of around 635 km2 the finite element mesh is composed of 173 781 nodes fig 6 the mesh size varies from 40 m within the area of interest the navigation channel to 750 m offshore western and northern sectors of the model as shown in fig 6 six friction areas are considered in the hydraulic model the boundary condition along the marine border of the model was set using depth averaged velocities and water levels from the legos numerical model tugo dataset 46 harmonic constants surge data describing the difference between the tidal signal and the observed water level are taken into account using a data file generated from the hycom2d model of the shom chassignet et al 2007 surface wind data are also considered to simulate the flow under windy conditions time evolution flow discharge hydrographs are imposed upstream of the gironde estuary model on the garonne and dordogne rivers the ability of the numerical model to reproduce complex physical processes such as sediment transport is presented in orseau et al 2020 the free surface flow was continuously measured every minute at the verdon richard lamena pauillac fort médoc ambes the marquis bassens and bordeaux locations fig 6 for this study observation results are used over a 36 hour period from august 12 to 14 2015 4 2 hydrodynamic model the telemac 2d code solves the 2d depth averaged free surface flow equations also known as shallow water equations eqs 1 3 1 h t x h u y h v 0 2 h u t x h u u y h u v g h z s x h f x h ν e u 3 h v t x h u v y h v v g h z s y h f y h ν e v where x and y are the horizontal cartesian coordinates t the time u and v the components of the depth averaged velocity h the water depth ν e an effective diffusion representing depth averaged turbulent viscosity ν t and dispersion z s the free surface elevation g the gravitational acceleration f x and f y refer to the friction force see section 4 2 1 telemac 2d solves the previous equation system using the finite element method on a triangular element mesh the main results at each node of the computational mesh are the water depth and the depth averaged velocity components telemac 2d can take into account propagation of long waves including non linear effects bed friction effect of the coriolis force effects of meteorological phenomena e g atmospheric pressure rain or evaporation and wind turbulence supercritical and subcritical flows influence of horizontal temperature and salinity gradients on density and dry area in the computational domain amongst other processes hervouet 2007 4 2 1 friction coefficient the friction term in the momentum part of the shallow water equations is treated in a semi implicit form within telemac 2d the two components of friction force are given in eq 4 4 f x u 2 h c f u 2 v 2 f y v 2 h c f u 2 v 2 where c f is a dimensionless friction coefficient the roughness coefficient often takes into account friction caused by walls on the fluid or other phenomena such as turbulence empirical or semi empirical formulas are used for calculating c f morvan et al 2008 in the present work c f is given by the widely used strickler formulas eq 5 the strickler coefficient m1 3s 1 is a calibration parameter of the modeling system to be adjusted according to field data e g usual measured water levels 5 c f 2 g h 1 3 k s 2 generally the friction coefficient is contained in an interval bounded by physical values as it results from different contributions e g skin friction bed form dissipation etc in this study the strickler bounds are taken as large as possible in fact the lower bound of the strickler coefficient is set to 5 according to u s f h a united states federal hig 1984 rough bed surface and the upper one is set to 115 extreme smooth bed surface in the present api context the friction coefficient is called model chestr and in the study corresponds to the strickler coefficient k si where i denotes the considered friction area 4 2 2 tidal amplification parameter tidal characteristics are imposed using a database of harmonic constituents to force the open boundary conditions for each harmonic constituent the water depth h and components of velocity u and v are calculated at point p and time t by eq 6 6 f p t i f i p t f i p t f i t a f i p c o s 2 π t t i φ f i p φ i 0 l i t where f is either the water depth h or one of the components of velocity u or v i refers to the considered constituent t i is the period of the constituent a f i is the amplitude of water depth or one of the horizontal components of velocity φ f i is the phase f i t and l i t are the nodal factors and φ i 0 is the phase at the original time of the simulation the water level and velocities of each constituent are then submitted to obtain the water depths and velocities for the open boundary conditions eq 7 7 h α i h i z b z r e f γ u β i u i v β i v i where z b is the bottom elevation and z ref the mean reference level in eq 7 the tidal amplitude multiplier coefficients of tidal range and velocity respectively α and β at boundary locations and the sea level correction γ are assumed to be the tidal calibration parameters pham and lyard 2012 the sea level correction parameter is assumed following expert opinion to be contained in an interval of plus or minus a meter the interval of the weighting coefficients of tidal range and velocity is set to 0 8 1 2 corresponding to a 20 margin of the initial value in the api framework the sea level correction variable γ is designated as model sealevel and the weighting coefficients of tidal range α and velocity β are denoted model tidalrange and model velocityrange respectively 4 3 summary of parameter range variation all the model input parameters and their associated probability distribution are summarized in table 1 5 calibration problem the inverse problem of calibration can be understood as the computation of the posterior distribution π x y model calibration algorithm step fig 7 as presented in appendix c where model parameters constitute the p components of the parameter control vector x x 1 x p r p composed of independent variables defined on some probability space ω a p with ω as a sample space a the σ algebra of events and p the probability measure y r m is the observation vector also defined on a probability space around the unknown parameter vector x and r m is the observation space defined by eq 8 8 y g x ε m where g r p r m is a vector valued function of vector x and ε m r m is an observable measurement noise such as e ε m 0 and r c o v ε m e ε m ε m r m m and identified as a multivariate normal distribution ε m n 0 r as a reminder in the gironde estuary application case the parameter control vector x is composed of the friction and tidal parameters and the observation parameter y is composed of water levels measured every minute at an observation station note that the operator g enabling the passage from the parameter space where vector x lives to the observation space where vector y lives represents a call to the api study workflow defined in fig 4 as telemac 2d is a discrete time dynamic model the output o is composed of scalar output water elevation at observation stations interpolated from telemac 2d computational nodes given at t t 1 t finally the maximum a posteriori is equivalent to the optimal search for the control vector minimizing the objective function j x 1 2 x x 0 b 1 x x 0 1 2 y g x r 1 y g x where b is the prior covariance matrix this is known as the traditional variational data assimilation cost function called 3d var carrassi et al 2018 mathematical methods can be used to solve optimization problems the former can vary significantly according to the form of the cost function convex quadratic nonlinear etc its regularity and the dimension of the space many deterministic optimization methods are known as gradient descent methods among which is the broyden fletcher goldfarb shanno bfgs quasi newton method used in this work morales and nocedal 2011 using this constrained optimization method makes it possible to impose boundaries during the research process of the model parameters guaranteeing their physical values because the inverse problem is often ill posed and unstable with available data corresponding to more than one solution small changes in model results can lead to very different estimates for the input calibration parameters these problems are related to the issue of parameter identifiability navon 1998 still the chosen optimization method involves computing the adjoint of the observation operator g or the partial derivatives of the operator with respect to its input parameters in this work the partial derivatives are approximated using a classical finite difference method this is a simple solution numerically sensitive and computationally time consuming but the observation operator can be written to make use of parallel computing thus providing a fast automatic calibration algorithm relevant to real world applications as the inverse problem is defined a relevant question is what are the effects of the modeling calibration parameters x on the simulated state variables o t m x t which are compared to the observations this question can be addressed by multivariate sensitivity analysis 5 1 multivariate sensitivity analysis the sensitivity analysis aims at quantifying the relative importance of each input model parameter the variance based methods aim at decomposing the variance of the output to quantify the participation of variables considered as independent conventional approaches to global sensitivity analysis gsa compute sensitivity indices called sobol indices and assume that the output variable is scalar the definition of sobol indices is a result of the anova analysis of variance variance decomposition sobol 2001 saltelli 2002 however as reported by many authors campbell et al 2006 lamboni et al 2011 garcia cabrejo and valocchi 2014 when this approach is applied to each variable of a functional output it leads to a high degree of redundancy because of the strong relationship between responses from one time step to another moreover it also misses important features of the output dynamic because many features cannot be efficiently detected through single time measurements thus the methodology used in this work to compute gsa is a multivariate sensitivity analysis computing generalized sensitivity indices lamboni et al 2011 which are similar to aggregated sensitivity indices gamboa et al 2013 as demonstrated in garcia cabrejo and valocchi 2014 marrel et al 2017 5 1 1 analysis of covariance to perform sensitivity analysis some parameters and input variables are identified as uncertain while the others are fixed at given nominal values telemac 2d denoted m thereafter ensures the relationship between the vector of model inputs x and the output quantity of interest o t whereby o t m x t direct model evaluation fig 7 since telemac 2d is a discrete time dynamic model for one interest point the model output o is composed of scalar output given at t t 1 t such as o o 1 o t with o i o t i m x t i the covariance of the model vector output o o 1 o t can be partitioned into summands of increasing dimensions single pairs triplets and so on of input variables as shown in eq 9 9 c o v o 1 o t i 1 p c o v i o 1 o t 1 i j p p c o v i j o 1 o t c o v 1 p o 1 o t from this equation some sensitivity indices of multivariate case can be defined by projection of the covariance matrix into scalar using the trace operator gamboa et al 2013 present the following aggregated indices 10 s 1 i o 1 o t t r c o v i o 1 o t t r c o v o 1 o t s t i o 1 o t t r c o v i o 1 o t 1 i j p p t r c o v i j o 1 o t t r c o v 1 p o 1 o t t r c o v o 1 o t 5 1 2 computation of the multivariate sensitivity indices to handle dynamic system behavior under parameter uncertainty a sample set of configurations is generated using latin hypercube sampling lhs in the sampling procedure the parameter uncertainties are taken from uniform distribution whose limits are defined by the parameter bounds see section 4 3 for a fixed number of desired configurations n n p lhss are feasible it is therefore possible to select the plan optimizing a uniformity criterion discrepancy or a statistical criterion entropy in this study the uniformity criterion relying on l 2 discrepancies was considered and optimized as described in damblin et al 2013 in fact according to these autors the l 2 discrepancy optimized lhs have shown some good properties to 2d projections in high dimension the full set of output dynamics over the complete latin hypercube sampling gives a matrix of size n t 11 o o 1 o t o 1 1 o 1 2 o 1 t o 2 1 o 2 2 o 2 t o n 1 o n 2 o n t each row of the matrix o corresponds to a temporal trajectory for a fixed scenario of uncertain input parameter as proposed by lamboni et al 2011 a principal component analysis pca can be carried out on the matrix o c which is the matrix obtained by centering each column of o the pca decomposition consists in a small number determination of variable h 1 h t to transcribe the maximum information in the sense of variance contained in the initial variables o c o 1 c o t c with o i c o i e o i where e is the mean operator and o i the discrete form of o i in order to remove redundancies with the least possible loss of information the variables h i are constructed to be uncorrelated from each other based on an iterative process that adds complementary information not contained in the previously constructed variables this methodology can be summarized by resolution of the system described by eq 12 12 h i o c w i i 1 t with v a r h 1 v a r h 2 v a r h t 0 under the constraint w i 2 1 in practice w 1 w t is a set of normalised and mutually orthogonal eigenvectors associated with eigenvalues λ 1 λ t of the covariance matrix σ 1 n e o c t o c once the eigenvectors resulting from the diagonalization of σ are determined the matrix o c can be decomposed as written in eq 13 13 o c i 1 t h i w i h i variables are uncorrelated and the principal component matrix h h 1 h t has the same total inertia as o but is mostly concentrated in its first columns thus the expression of covariance decomposition can be expressed as a sum of variance of h i on which the sobol decomposition into summands of increasing dimension can be carried out eq 14 14 t r c o v o 1 o m k 1 t i 1 p v i k 1 i j p p v i j k v 1 p k with v ε k v a r e h k x ε w w ε v w k where v a r e h k x ε is the variance of the conditional expectation of h k to the input variable x ε thus based on an identification process between the formulation that is described in eq 10 and eq 14 the expression of multivariate sensitivity indices can be given as described in eq 15 15 s 1 i o 1 o m k 1 t s i k o b λ k k 1 t λ k s t i o 1 o m k 1 t s o b t i k λ k k 1 t λ k where s ε k o b denotes sobol sensitivity indices such as s ε k o b v ε k v a r h k and s o b t i k represents the total sobol sensitivity index such as s o b t i k i ε v ε k v a r h k as expressed by eq 15 the multivariate sensitivity analysis indices involve sobol index computation the next paragraph is dedicated to the computation process of this mathematical expression based on a combined pca and polynomial chaos expansion pce emulator 5 1 3 reduced order model pca pce as already mentioned python is a language that can be used in many scientific contexts and can be adapted to any type of use based on dedicated libraries the multivariate sensitivity analysis explained in the previous section was carried out based on an open source library for uncertainty treatment named openturns standing for open source initiative to treat uncertainties risks n statistics www openturns org baudin et al 2017 a design of experiment doe of size 1000 n 1000 is constructed for the pca pce learning step this number of model evaluation was determined based on a convergence study carried on the sensitivity analysis see section 5 1 4 the mpi library was used for launching and managing the telemac 2d solver computations at the end of the calculation a telemac 2d result file was composed of 288 time records corresponding to a physical variable record every 10 minutes t 288 in the present study the discrete version of the pca singular value decomposition svd of the matrix o c is considered this means that h i considered is expressed as h i n λ i a i with a i orthonormal in order to have an efficient sensitivity index computation in terms of computational cost the k main eigenvectors which explained more than 99 95 of the original variance are considered k 10 t it is important to note that owing to truncation the equivalence between aggregated sensitivity indices gamboa et al 2013 and generalized sensitivity indices lamboni et al 2011 no longer exists the generalized sensitivity indices represent an approximation of the aggregated sensitivity indices for each eigenvector a learning sample of realizations of the decomposition coefficient h i is available as proposed by garcia cabrejo and valocchi 2014 a polynomial chaos expansion can be used as learning step of each decomposition coefficient thus the sobol indices needed in eq 15 are obtained based on post treatment of each mode pce as proposed in sudret 2008 here the construction of the pce is carried out based on least angle regression stage wise method lars in order to construct an adaptive sparse pce in this approach a collection of possible pces ordered by sparsity is provided and an optimum can be chosen with an accuracy estimate it was performed in this study using corrected leave one out error blatman and sudret 2011 at this stage a reduced order pca pce model is produced for each observation station of the gironde estuary fig 6 the validation of these emulators was carried out based on a validation sample generated with the same procedure optimized lhs as for the learning step this sample was composed of 100 validation points n val 100 and allows after the telemac 2d unit computing a temporal predictivity criterion defined by eq 16 16 q 2 t 1 j 1 n v a l m k x j t m x j t 2 j 1 n v a l m k x j t 1 n v a l i 1 n v a l m x j t 2 where m k x j t is the estimated pca pce evolution fig 8 shows from top to bottom a comparison on a validation configuration between an estimated result from the reduced order pca pce model and the telemac 2d computation the difference between these two water elevations is then given and finally the predictivity criterion is presented the reduced order model is validated according to the performance displayed in fig 8 indeed the results provided by the emulator and the telemac 2d solver are fairly close the deviation between these results varies respectively from 10 to 20 cm at verdon and bordeaux observation stations this is confirmed by the average predictivity criterion close to 1 for verdon and bordeaux during the study period a coefficient close to 1 shows a good fit between the validation database and the result estimated by the reduced order model however a less satisfactory performance of q 2 criteria can be noticed at the bordeaux observation station compared to the verdon station the confluence of the dordogne and garonne rivers occurs just downstream from bordeaux and the influence of the hydrological forcing of these two rivers is not studied here consequently the upstream fluvial discharges are not considered as parameters in construction of the pca pce emulator the missing interactions can explain the observed performance gap in the predictivity criteria to conclude the reduced pca pce model shows good agreement with the telemac 2d computation the high score of predictivity criteria allows confidence in the sensitivity results post treated from the reduced order model 5 1 4 multivariate sensitivity analysis results a reduced order model based on pca pce is used to identify influential input parameters as reported by pianosi et al 2016 when applying sampling based sensitivity analysis sensitivity indices are not computed exactly but they are approximated from the available samples the robustness and convergence of such sensitivity estimates should therefore be assessed thus three elements are of interest in the following analysis i the convergence of sensitivity indices ii the robustness of the estimates and iii relevant sensitivity analysis visualization convergence to handle this issue the convergence rates of generalized sensitivity indices are assessed as the sample size increases fig 9 presents the evolution of sensitivity indices obtained at bordeaux observation station as displayed on fig 9 the number of samples needed to reach stable sensitivity estimates can vary from one input factor to another however from a sample size of 1000 the sensitivity indices are stabilized thus this number of model evaluation is considered in the following investigations robustness a robustness analysis is carried out in order to evaluate the sensitivity of the estimates to the design of experiment first the robustness of the sensitivity indices is analysed through confidence intervals they are obtained by repeating 30 times the methodology described above see section 5 1 3 table 2 presents the result of these confidence intervals which have finally required n 30 n 3 104 model calls as shown in table 2 the bounds of the min max confidence intervals are relatively close and demonstrate the capacity of the reduced order pca pce model constructed from a l 2 discrepancy optimized lhs sample to produce precise estimates an alternative sampling method based on low discrepancy sequence of sobol is also considered the obtained results are presented in table d2 most of the time the sensitivity indices obtained with the low discrepancy sequence are included in the min max confidence intervals obtained from l 2 discrepancy optimized lhs sample when out of range values stay close to the bounds consequently the estimates can be considered satisfactory in terms of accuracy and robustness results visualization visualization can significantly improve the interpretation of the sensitivity analysis results for this purpose radial convergence diagrams also called chord graphs are used to simultaneously visualize some computed generalized sensitivity indices fig 10 these diagrams plot the main effect of each input variable first order multivariate sensitivity analysis proportional to the size of the inner circle its total influence including interactions proportional to the size of the outer circle existence and extent of second order effects second order multivariate sensitivity indices proportional to width butler et al 2014 fig 10 displays sensitivity analysis estimates from one l 2 discrepancy optimized lhs sample used to compute the min max confidence intervals presented in table 2 as shown in the figure the output variance at each observation station is mainly explained by the upstream friction coefficients areas one and two of the gironde estuary model and the sea water level as expected downstream from the estuary bordeaux station the friction coefficient of area 3 containing the bordeaux station has a more significant influence it is noticeable from the visualization of variable interactions that even if the tidal range variable contribution is not considerable its interaction with other parameters should not be neglected this enhances the utility of simultaneously visualized interactions with main and total effects of sensitivity analysis to conclude the calibration problem initially composed of nine input variables can be reduced to five the first three being friction area sea level correction and tidal range coefficients after considering the results of multivariate sensitivity analysis 5 2 calibration results the calibration algorithm fig 7 is performed by coupling telemac 2d and the data assimilation library adao in python context through the component telapy of the telemac mascaret system adao a module for data assimilation and optimization provides modular data assimilation and optimization features in python https pypi org project adao argaud 2019 it can be coupled with other modules or external codes while providing a number of standard and advanced data assimilation or optimization methods the adao library also covers a wide variety of practical applications from real engineering to experimental methodologies its architecture and numerical scalability adapt to the field of application as shown by eq 18 the optimal search for the control vector x takes a minimization form of an objective or cost function given by the expression inside the exponential term which must satisfy the background error statistics prior term and the equivalent observation error likelihood term this minimization process equivalent to the maximum a posteriori search is carried out using the 3d var algorithm the control parameter is composed of the five most influential variables identified by the multivariate sensitivity analysis performed previously the initial guess x 0 is set to random values inside the constrained search space x 0 k s 1 73 76 k s 2 83 62 k s 3 83 62 α 0 9729 γ 0 8611 the observation vector y is the free surface flow evolution extracted every 60 seconds at the verdon lamena pauillac fort médoc bassens and bordeaux locations from noon august 12 to midnight august 14 2015 the chosen optimization method involves computing the partial derivatives of the observation operator g with respect to x a classical finite differences method with a differential increment set to 10 4 the error background and observation covariance matrices respectively identified by b and r are token diagonals meaning they have no error correlations a small variance value for r justifying great confidence in the observation value is considered such as σ m 2 0 1 y on the contrary little confidence is given to a prior part such as σ b 2 10 x 0 as shown by figs 11 and 12 the automatic calibration algorithm finds an optimal solution in about 30 iterations with the following set of parameters x m a p k s 1 47 99 k s 2 59 63 k s 3 67 485 α 0 9114 γ 0 5344 a rapid decrease in cost function can be observed for the first algorithm iterations the cost function curve behavior is smooth until iteration number 20 where a slight decrease can be observed as expected a similar behavior is observed for the parameter to be calibrated fig 13 displays the results of automatic calibration over the computation period as expected the water surface profiles calculated from the calibrated parameter configuration are much closer to measurements than the ones computed from the background knowledge parameters the final results emphasize the efficiency of the automatic calibration tool in the framework of a real configuration for most of the studied period the difference between observations and the calibrated configuration is less than 20 cm about 5 of the tidal range at the verdon and bordeaux stations however at the bordeaux observation station the calibrated configuration presents some error peaks at low tide this phenomenon can be induced by a greater effect of the fluvial part of the estuary which is not well represented in the model where discharge is expressed as an hourly average 6 discussion the concept of interoperability is a generic solution for gathering and exchanging information from various multidisciplinary knowledge the application presented in this article is a case of 2d hydrodynamics and is not representative of all the possibilities offered by the interoperability of telemac mascaret system for example the estuarine sediment transport could be taken into account in order to better model and understand the evolution of the bed with the api of the gaia module more generally the approach presented here can be easily applied to different geoscience problems where telemac mascaret is relevant all apis of the different modules are freely available as they are part of the telemac mascaret system applying interoperability criteria on an old and large code is not an easy task because of the transformation effort it requires for the telemac mascaret system the transformation towards the concept of interoperability required several years of development a recommendation is to take into account these criteria at the early stage of development considering the fact that the code will probably have to interact with the outside world the implementation of apis makes it possible to extend the scope of the software by facilitating the use for different types of applications for instance based on fluid exchange of information apis can be used to couple the telemac mascaret system with a computational fluid dynamics cfd type software for example code saturne archambeau et al 2004 www code saturne org to take into account atmospheric and groundwater flows the development of standards for publishing interoperable softwares in forms suitable for community interactions remains a major issue laniak et al 2013 the present work allows the telemac mascaret system to be integrated in different environments however the wrapper for each specific environment must be maintained a lean standard has been proposed with the openmi environment gregersen et al 2007 the integration of the telemac mascaret system in this environment could constitute an outlook to this work since python is really easy to pick up and learn a wrapper in this language is distributed in the telemac mascaret system official version telapy this choice aims to favorise the dissemination of the hydro informatic system as an environmental modeling tool however the system compilation is a cumbersome process which can slow down its dissemination to overcome this issue and based on the python package index pypi https pypi org a compiled version of telapy might be envisaged to provide all dynamic libraries needed to run telemac mascaret system apis to demonstrate that telapy is a functional system an example of hydraulic model calibration is presented this case deals with a series of reference events by adjusting some uncertain physically based parameters until the comparison with observations achieves sufficient accuracy if performed manually the model calibration is time consuming fortunately the process can be largely automated to significantly reduce human workload as shown in this paper a reduced order model based on pca pce is used to identify influential input parameters this emulator has been created and validated on the basis of learning and validation solver computations a major issue arising from this methodology concerns the optimization of the number of computational runs needed for sufficient results in the sensitivity analysis approximation with the surrogate models must become sufficiently accurate with just limited data available for the learning step there have been some recent advances in this area based on adaptive sampling steiner et al 2019 after most influential parameters have been identified they are then calibrated using a data driven technique the chosen algorithm is based on a minimization process requiring derivatives of the telemac 2d solver with respect to the parameters to be calibrated several options exist to obtain the derivatives the finite difference method used in this work is easily implemented but returns approximate derivatives whose poor accuracy can degrade the performance of the complete application a much better option is to create a new program that computes the exact analytical derivatives of the model algorithmic differentiation griewank and walther 2008 is a way of automating creation of the derivative program thus providing accurate derivatives for a minimal development effort gradient based methods are very useful to find local extreme values within a reasonable time but they cannot pretend to find a global solution in the search domain on the other hand metaheuristic optimization algorithms are useful in finding the area of a global solution minimum or maximum but the convergence becomes much slower due to the large number of required simulations one way to improve the efficiency of the calibration would be to combine a derivative free algorithm like a metaheuristic with a gradient based method like bfgs to obtain better solutions within a reasonable time hybridization 7 conclusion the application programming interface api of the open source telemac mascaret system was developed to convert a heterogeneous set of open source user contributed models into a suite of plug and play modeling components that can be reused in many different contexts the apis provide a user friendly development framework that can be easily understood allowing seamless integration of base codes and do not invalidate existing institutional software development practices the api development framework does not compel the use of or supply specific environmental modeling standards as its services standardize at a more basic level of internal communication this gives researchers and model developers more freedom to customize services for the problems they are facing the telemac mascaret system api seeks to assist the hydrodynamic community in resolving new challenges such as uncertainties real time data assimilation and multi physics simulations many model developers have limited skills in software development and architecture recognizing this and seeking to promote the widest possible adoption of the telemac mascaret system api by the user community a scripting feature of modeling is privileged with the python language as recommended by knox et al 2018 this forms a new module of the system called telapy work performed with python can be transposed to other scripting languages such as r and julia python is a language that can be used in many contexts and adapted to any type of program based on dedicated libraries however it is particularly used to automate tedious tasks such as the calibration process of a numerical model saving a significant amount of time in realizing a project use of python is widespread in the scientific community and it has many libraries optimized and intended for numerical computation this flexibility was demonstrated in the gironde estuary case first a sensitivity analysis was carried out to identify sensitive parameters to calibrate the most influential variables on water depth variation were the upstream friction coefficients k s1 k s2 k s3 the tidal amplitudes multiplier coefficient of tidal range α and the sea level correction γ this was achieved by linking a telemac 2d physical based component to an uncertainty quantification library to achieve better model accuracy a calibration process was realized through a physical based data driven technique using a data assimilation library to promote and facilitate the dissemination of the deployed approach to the telemac mascaret community the development of a graphical user interface gui can be useful the major benefits are user friendliness efficiency and enhancing the quality of hydraulic studies the calibration process deployed here can be extended to other solvers of the telemac mascaret system water quality sediment transport wave propagation and so on moreover there are key potentially available sources of information on continental water bodies in situ and remote sensing data for instance data assimilation algorithms for integrating observation data into real cases are now increasingly applied to hydraulic problems with two main objectives optimizing model parameters and improving stream flow simulation and forecasting therefore the ability to exchange data as computational results has become a growing need facilitated by interoperability software availability the application programing interface api framework described in this article fortran apis and its python wrapper is available for download in telemac mascaret system www opentelemac org telemac mascaret is an integrated suite of solvers for use in the field of free surface flow available under the gnu general public license version 3 declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments the authors gratefully acknowledge contributions from the open source community especially that of openturns open source initiative for the treatment of uncertainties risks n statistics and adao a module for data assimilation and optimization in particular we would like to thank angelique ponçot and jean philippe argaud from edf r d for their constructive discussion on data assimilation regis lebrun from airbus for the pod construction based on openturns and all of the edf r d openturns team the authors also would like to address special thanks to kamal el kadi abderrezzak for his support and deep reading that greatly enriched this paper finally we would like to thank the anonymous reviewer and arnald puy whose comments and suggestions helped improve the manuscript appendix a telemac 2d list of api variables table a 1 telemac 2d variables accessible through api table a 1 variable name definition model at current time model dt time step model bcfile boundary condition file name model bnd tide options for tidal boundary condition model bottomelevation bottom elevation model chestr roughness coefficient model fair wind influence coefficient model cote water level on boundary conditions model cpl period coupling period with telemac mascaret solvers model debit flowrate discharge on boundary conditions model debug activation debug mode keyword model flux boundaries flux on the boundary conditions model geometryfile geometry file name model meteofile binary atmospheric file name model fo2file formatted data file name model liqbcfile liquid boundary file name model prefile previous computation file name model graph period graphical output period model hbor value of prescribed water depth on boundary conditions model ikle mesh connectivity table model nachb number of processors sharing a given node model knolg pointer to get the global numbering of a node in the initial mesh model kp1bor boundary node connectivity model lihbor boundary type for the water depth model listin period listing output period model liubor boundary type for horizontal velocity model livbor boundary type for vertical velocity model lt current time step model compleo graphical output counter model ptinig number of the first graphical printout time step model nptir number of border nodes for subdomains model nbor boundary node global number model nelem number of elements in the discretization mesh model nelmax maximum possible number of elements model npoin number of nodes in the computational mesh model nptfr number of boundary nodes model ntimesteps number of time steps model numliq liquid boundary number model porosity porosity model resultfile result file name model sealevel sea level calibration coefficient model tidalrange tidal range calibration coefficient model velocityrange velocity range calibration coefficient model ubor value of prescribed horizontal velocity on boundary conditions model vbor value of prescribed vertical velocity on boundary conditions model velocityu horizontal velocity model velocityv vertical velocity model waterdepth water depth model x horizontal coordinates of mesh points model xnebor horizontal component of the normal at boundary nodes model y vertical coordinates of mesh nodes model ynebor vertical component of the normal at boundary nodes model equation name of solved equation model ak turbulent kinetic energy term of k ε turbulence model model ep dissipation term of k ε turbulence model model iturb turbulence model model init depth initial value of water depth model tracer tracer value on mesh nodes model ntrac number of tracers model flowrateq solid transport flowrate model dcla median grain size value model shields critical shields parameter model xwc settling velocity model z free surface elevation model qbor value of prescribed discharge flowrate on boundary nodes model ebor value of prescribed turbulence dissipation term on boundary nodes model flbor value of prescribed bottom elevation on boundary nodes model tob shear stress model liqbor boundary type for the discharge model nsicla number of bed material size classes model nomblay number of layers in the bed model concentration concentration at the current time step model evolution evolution of bed model partheniades parthenaides erosion coefficient of each bed layer model volu2d basis integral appendix b image 2 appendix c bayesian inference formulation of calibration problem the posterior distribution π x y can be determined through the well known bayes rule eq 17 17 π x y π y x π x π y x π x d x the term π y x called the likelihood can be interpreted as the probability density function of the observed data conditional upon a set of parameter values considered as random variables and as a measure of the information provided by the observations on the parameter values from eq 8 the likelihood is expressed as π y x exp 1 2 y g x r 1 y g x the term π x represents a priori knowledge of the unknown parameters x this term is classically taken as a multivariate normal distribution with known mean x 0 derived from measurement data or previous computation and covariance matrix b r p p positive definite such as π x exp 1 2 x x 0 b 1 x x 0 from the previous expressions of a priori and likelihood the posterior distribution is given by the following equation 18 π x y exp 1 2 y g x r 1 y g x 1 2 x x 0 b 1 x x 0 the maximum a posteriori map is equivalent to the formulation of the optimal search of control vector x which must satisfy the a priori error statistics j b 1 2 x x 0 b 1 x x 0 and the equivalent observation error statistics j o b s 1 2 y g x r 1 y g x appendix d generalized sensitivity analysis based on low discrepancy sequence of sobol table d 2 generalized sensitivity indices gsi based on 1024 210 low discrepancy sobol points at bordeaux and verdon observation stations table d 2 inputs gsi verdon bordeaux first order total order first order total order k s1 0 126 0 133 0 110 0 156 k s2 0 01 0 0153 0 358 0 437 k s3 2 29 10 5 7 45 10 5 0 0816 0 118 k s4 0 1 33 10 5 9 69 10 5 3 44 10 4 k s5 2 53 10 5 6 50 10 5 2 06 10 3 3 54 10 3 k s6 1 65 10 7 8 13 10 6 4 17 10 5 1 86 10 4 α 0 0464 0 0492 0 0135 0 019 β 4 82 10 4 5 07 10 4 1 81 10 4 2 70 10 4 γ 0 809 0 809 0 341 0 364 
25661,satellite remote sensing of river discharge rsq algorithms provide a useful source of observations to supplement river gauge records rsq algorithms have existed for over a decade yet their widespread use has been impeded by a lack of operational usability and quantitative characterization of uncertainty here we present rodeo an algorithm for estimating river discharge using landsat observations in near real time rodeo is validated with 456 gauges median kling gupta efficiency 0 3 and uses a novel quantile rating curve technique that pairs landsat river widths with discharge estimates from a global hydrologic model rodeo also characterizes the uncertainty of rsq estimates estimated root mean square error 7 enabling rsq retrievals to be used for data assimilation into hydrologic models with the goal of expanding the rsq user base the rodeo algorithm is implemented as a freely available off the shelf cloud based google earth engine application that provides rsq estimates across north america from 1984 present keywords remote sensing of discharge landsat google earth engine river width rating curve discharge uncertainty rsq 1 introduction effective management of river water resources necessitates careful monitoring of river discharge through space and time barrow 1998 murphy 1902 the river gauge is the most commonly used tool to measure river discharge and is considered the gold standard for river discharge monitoring dobriyal et al 2017 however at the global scale most river basins are poorly gauged ungauged or have gauges with data records that are not publicly available hannah et al 2011 sheffield et al 2018 additionally gauges provide discharge measurements only at single points along river reaches rather than whole network measurements this sparsity of gauge data has led to the rise of alternative methods for measuring river discharge satellite remote sensing of discharge rsq is an increasingly popular approach for supplementing gauge networks gleason and durand 2020 lettenmaier et al 2015 smith 1997 satellite remote sensing enables rivers to be monitored holistically as an interconnected system from the reach to the global scale several studies have effectively estimated discharge by pairing gauge measurements with satellite observations of river widths e g feng et al 2019 smith and pavelsky 2008 river stage e g paris et al 2016 tourian et al 2013 and surface reflectance tarpanelli and domeneghetti 2021 to generate classic rating curve relationships with r2 values ranging from 0 80 to 0 99 along ungauged reaches similar approaches have paired modeled discharge with moderate resolution imaging spectroradiometer modis satellite imagery to estimate daily to weekly discharge dynamics at the basin to global scale brakenridge et al 2012 hou et al 2018 2020 tarpanelli et al 2013 while these modis studies often successfully estimate changes in relative discharge pearson correlation coefficient r typically ranging from 0 4 to 0 9 they do not focus on estimating absolute discharge a critical quantity in water management applications these studies also do not estimate discharge uncertainty in ungauged basins which is an essential metric for assimilating rsq estimates into hydrologic models in addition to rsq algorithms that are based on rating curve techniques several rsq algorithms follow the mass conserved flow law inversion mcfli approach which can estimate discharge using river observations and bayesian priors e g modeled or gauged discharge gleason et al 2017 examples of mcfli algorithms include at many stations hydraulic geometry amhg gleason and smith 2014 gamo garambois and monnier 2015 metroman durand et al 2014 and several others e g andreadis et al 2020 bonnema et al 2016 durand et al 2016 hagemann et al 2017 see gleason and durand 2020 for a more complete list of rsq algorithms though mcfli algorithms often provide accurate rsq they are sensitive to their bayesian priors and often cannot overcome inaccurate priors durand et al 2016 unlike gauge data satellite rsq measurements are temporally discontinuous limited to the intermittent timing of contamination free satellite overpasses allen et al 2020 to fill in the temporal gaps in rsq measurements significant work has been done to assimilate rsq data into hydrologic models which can simulate discharge continuously through time and space e g andreadis et al 2007 emery et al 2020 fisher et al 2020 oubanas et al 2018 data assimilation techniques require reasonable estimates of rsq uncertainty pan and wood 2010 reichle 2008 while rsq data have been shown to improve model performance through data assimilation the efficacy of these demonstrations is limited by misestimated uncertainty causing an unrealistic confidence in rsq measurements and in turn creating errors in the assimilation step e g ishitsuka et al 2021 pathiraja et al 2018 several mcfli studies discuss uncertainty estimation in detail e g durand et al 2014 hagemann et al 2017 but few if any studies have quantitatively assessed the accuracy of their uncertainty estimates for example brinkerhoff et al 2020 provides uncertainty estimates based on ensembles of bayesian outputs but they do not assess how well these uncertainties represent actual error past studies have implemented rsq algorithms on local machines or on supercomputers typically requiring a relatively complex setup limiting their useability among non expert users sheffield et al 2018 even the more user friendly rsq algorithms like the one presented by hagemann et al 2017 which is implemented as an r package require some familiarity with the coding language and require the user to provide algorithm inputs e g a priori discharge estimates and satellite river observations additionally many of the existing rsq studies use synthetic data for assessing algorithms e g bonnema et al 2016 domeneghetti et al 2018 frasson et al 2021 garambois and monnier 2015 which may fail to represent the full complexity of satellite data the rsq studies that use extant satellite data are often limited to individual rivers e g bjerklie et al 2018 gleason et al 2018 sichangi et al 2018 tarpanelli et al 2020 2019 tuozzolo et al 2019 or basins e g ishitsuka et al 2021 sichangi et al 2016 tourian et al 2017 making it difficult to assess their validity at the global scale in this study we present remotely observed discharge from effective width occurrence rodeo a computationally efficient empirical algorithm for estimating discharge and discharge uncertainty from landsat rodeo integrates historical landsat data and modeled discharge to produce reach level rating curves allowing for past and near real time absolute discharge and discharge uncertainty estimates to be made from landsat observations the rodeo algorithm is incorporated into a freely available google earth engine gee gorelick et al 2017 application which can be run on any internet connected device and produces on the fly discharge fig 1 and uncertainty estimates at the reach scale across north america from 1984 present 2 methods to demonstrate rodeo across a variety of environments we implement the algorithm to 28 409 river reaches across north america we focus on north america because of the high prevalence of in situ width and discharge measurements that can be used for validation we validate rodeo at 456 gauge stations using data from the united states geological survey usgs u s geological survey 2021 and the global runoff data center grdc fig 1 the global runoff data centre 2021 all of the validation data and results are publicly available see acknowledgements 2 1 remote sensing of river widths the rodeo algorithm relies on a river width based rating curve technique to estimate discharge to calculate river widths we use gee to process landsat 5 7 and 8 imagery throughout north america from march 1 1984 to december 31 2020 note that the rodeo gee application is designed to estimate discharge at near real time using the most recently available landsat data and thus this end date pertains to this study s validation purposes only we use the global river widths from landsat grwl database v01 01 allen and pavelsky 2018 to identify the centerline locations of rivers that are on average 100 m or wider which typically exhibit enough width variability to be measurable by 30 m resolution landsat imagery e g feng et al 2019 we found not shown here that calculating effective width e g sichangi et al 2016 smith et al 1995 yielded more accurate results compared to measuring width along individual cross sections e g yang et al 2020 effective width hereinafter referred to as width is calculated along a reach by dividing the river surface area by the river centerline length producing a reach averaged width measurement fig 2 specifically the reach boundary is defined by creating a circular buffer around the center of each reach with a diameter equal to three times the mean grwl width gray circle fig 2a inside this circular buffer a second polygonal buffer around the grwl centerline is created with a width equal to three times the mean grwl width extent of the landsat scene fig 2a the grwl centerline buffer width is based on the approach of yang et al 2020 we use this inner buffer to calculate river widths from landsat imagery we clip all landsat scenes that overlap with the grwl centerline buffer and apply the function of mask fmask classification algorithm to flag pixels containing clouds cloud shadows and or snow ice zhu et al 2015 we then filter out any landsat scenes with a flagged pixel area greater than 10 of the grwl centerline buffer area since this landsat filtering is done for each reach a single landsat scene may be filtered out for one reach and considered contamination free for another after filtering out contaminated landsat scenes we classify surface water using the dynamic surface water extent dswe classification algorithm jones 2015 2019 following the approach from yang et al 2020 we classify water pixels as river or non river pixels based on 8 way connectivity of water pixels to the grwl centerline fig 2b to ensure that we are only considering instances where rodeo accurately captures widths we use a 1 landsat pixel buffer around the grwl centerline buffer blue geometry fig 2c and remove any scenes that contain river pixels within the 1 pixel buffer finally we calculate width by converting the number of river pixels within the grwl centerline buffer into geographic area and dividing this area by the length of the grwl centerline to produce an effective river width fig 2d the processed rodeo widths are then used to develop width based rating curves we validate our width measurements by comparing 1 926 rodeo width observations with same day in situ width measurements from 318 usgs gauges a subset of the gauges shown in fig 1 we exclude any usgs width data equal to 0 m as this is likely an inaccurate measurement for rivers with mean widths 100 m in addition we exclude usgs width data with the quality classification of poor following the method of yang et al 2020 we assess the accuracy of rodeo using mean absolute error m a e b i a s and root mean square error r m s e equations 1 3 in table 1 2 2 rating curve development along each river reach we develop a width based rating curve by pairing rodeo widths with same day modeled discharge fig 3 the source of this modeled discharge is the global reach level a priori discharge estimates grades dataset which contains daily reach level estimates of discharge from 1979 to 2013 lin et al 2019 grades was validated using discharge records from over 14 000 global gauges and displays good results for reaches with widths 100 m with 44 76 containing a daily percentage bias within 20 50 note that grades discharge is not directly calibrated using gauge data lin et al 2019 to build the rating curves we only include modeled discharge estimates that occur on the same day as a rodeo width measurement using these same day data we calculate percentile values of both variables and then use these width and modeled discharge percentiles to build a rating curve by pairing each percentile e g the 5th percentile of modeled discharge is paired with the 5th percentile of rodeo width to create the colorful curve in fig 3 this approach is similar to those taken by tourian et al 2013 and tarpanelli and domeneghetti 2021 we validate the grades data used in this study by comparing 87 290 grades discharge estimates with same day in situ discharge measurements from 439 usgs and grdc gauges the accuracy of grades is described by the error metrics m a e b i a s and r m s e equations 1 3 in table 1 we only use data from 1984 to 2013 to develop the rating curves since landsat 5 was launched march 1 1984 and grades data ends december 31 2013 we extend the rating curves from the 5th to the 95th percentile of flows because this is the range of hydrologic conditions that the landsat archive typically represents flow frequency allen et al 2020 any landsat widths that lie outside the rating curve boundaries are removed from the analysis and the rodeo application to validate the rodeo rating curve technique we compare rodeo rsq with same day in situ discharge measurements at 456 usgs and grdc gauge locations because the rating curves are built with data from 1984 to 2013 we only use river widths from 2014 to 2020 for discharge validation to ensure independence of gauge data in the rodeo validation in line with similar studies dijk et al 2016 hou et al 2018 2020 we use pearson correlation coefficient r to determine rodeo performance in estimating discharge dynamics since the r value does not fully characterize the accuracy of discharge estimates we follow the approach of several other rsq studies e g durand et al 2016 feng et al 2019 ishitsuka et al 2021 paris et al 2016 and use the following metrics to characterize the accuracy kling gupta efficiency k g e normalized root mean square error n r m s e relative bias r b i a s and relative root mean square error r r m s e equations 4 7 2 3 estimating discharge uncertainty to assimilate rodeo discharge data into hydrologic models it is necessary to accurately estimate the uncertainty associated with rodeo rsq estimates we use the term uncertainty to describe estimated error and the term error to describe the actual error of the rodeo rsq estimates to characterize uncertainty for each rodeo reach in north america we calculate the mean difference between the grades discharge q m i and rodeo discharge q ˆ i for each width used to build the rating curves fig 3 the approach used here provides uncertainty estimates for all reach locations without the need for in situ measurements and can be validated where in situ measurements exist following the approach of david et al 2019 we validate rodeo uncertainty by calculating three metrics absolute bias b i a s u standard error s t d e u and r m s e u equations 8 10 in addition we calculate the error metrics b i a s e s t d e e and r m s e e at gauge stations by comparing rodeo discharge q ˆ i to in situ discharge q i equations 11 13 we then compare the uncertainty and error metrics to assess the accuracy of the estimated uncertainty approach b i a s and s t d e are selected because they capture two different uncertainty characteristics and combine to describe r m s e via the pythagorean theorem equation 14 to assess the validity of the rodeo rsq uncertainty we compare the uncertainty to error metrics across all 456 gauges we then calibrate rodeo uncertainty following a method similar to that presented by david et al 2019 specifically using a randomly selected 60 of the gauges n 275 we calculate least squares linear regressions with a y intercept fixed at 0 for the b i a s and s t d e comparisons both the b i a s u and s t d e u are calibrated by 1 slope from their respective linear regressions and are used in equation 14 to develop a corrected r m s e u estimate this step is independently validated using the remaining 40 of gauges n 181 3 results 3 1 rodeo rating curve inputs river width observations from landsat form the empirical basis of rodeo as described in section 2 1 we validate the rodeo width measurements by comparing 1 926 rodeo widths with same day in situ river width measurements from 318 usgs gauges mean river width of 206 m across all 318 gauges we find that the rodeo widths reasonably match in situ measurements m a e 43 8 m b i a s 26 5 m r m s e 83 6 m applying the non parametric theil sen median estimator sen 1968 we derive a robust linear regression between rodeo widths and in situ width measurements fig 4 a the regression produces a slope that deviates from unity by 3 4 indicating that on average rodeo provides accurate observations of river widths and can be used to develop width based rating curves we find that discrepancies between landsat and in situ river width occur at reservoirs or during flood events when the temporal discrepancy between landsat overpasses and in situ river width measurements causes rodeo widths to deviate from the in situ observations modeled discharge from grades directly influences rodeo s rsq performance we validate grades discharge by comparing 87 290 grades discharge estimates used in the development of rodeo with same day in situ discharge measurements from 439 usgs and grdc gauges we determine that grades discharge match in situ measurements across all gauge locations fairly accurately m a e 327 9 cms b i a s 123 5 cms r m s e 951 7 cms in addition we use the theil sen median estimator sen 1968 to derive a robust regression between grades discharge and in situ discharge which produces a slope that deviates from unity by 4 3 fig 4b the cluster of underestimated grades discharge seen in fig 4b occur at 2 gauges in semi arid regions and 6 gauges located near dams reservoirs 3 2 remote sensing of discharge applied across north america rodeo produces 5 835 367 discharge estimates across all 28 409 reach locations with a reach averaged mean of 5 1 observations per year from 1984 to 2020 and 7 0 observations per year since the launch of landsat 8 february 11 2013 rodeo discharge observations vary across the continent with the canadian and alaskan reaches being observed less frequently mean of 4 2 observations per year across 19 796 reaches compared with the remaining southern portion of the continent mean of 9 2 observations per year across 8 613 reaches summer months tend to have more observations than winter months with july being the most commonly observed month narrow rivers widths 400 m tend to be observed nearly as frequently mean of 5 7 observations per year across 21 111 reaches as wide rivers mean of 5 8 observations per year across 7 298 reaches to validate rodeo discharge estimates we compare rodeo rsq from 2014 to 2020 with same day in situ discharge measurements from 456 gauges there is a reach averaged mean of 8 observations per year during the validation period and the total number of observations used in validation is 22 585 comparing rodeo discharge estimates to in situ discharge we find that rodeo typically captures the variability and absolute discharge fig 5 a however at some gauge locations it produces overestimates of discharge magnitude at high flows fig 5b the error bars in fig 5 represent the calibrated r m s e u which are discussed in section 3 3 across all gauges the median n r m s e r b i a s r r m s e k g e and r values are 69 1 81 0 3 and 0 6 respectively fig 6 median statistics 56 18 62 0 2 0 7 for wide rivers with grwl widths 400 m improves n r m s e r r m s e and r indicating that rodeo performance in most error metrics is better or relatively similar along wider rivers however the r b i a s for wide rivers shows a relatively strong positive bias which is a decrease in performance compared to the near zero r b i a s for all gauges the half violin plots in fig 6 show that r r m s e k g e and r have a skewed distribution towards the negative performance direction indicating that rodeo performs well at most gauge locations but sometimes exhibits very poor performance along certain reaches running rodeo on all observable rivers in north america from 1984 to 2020 produces patterns of mean discharge that follows expected spatial variability fig 7 a we find that 64 of high discharge reaches defined here as having a mean rodeo discharge 1000 cms are located in the canada and alaska region in the western usa rodeo is largely limited in application due to a lack of rivers wide enough to be observable by landsat we describe gauge performance spatially using the k g e metric fig 7b and determine that rodeo produces improvements from using the mean discharge k g e 0 41 knoben et al 2019 at 412 of the 456 gauge locations poor performance tends to occur in the western usa along the colorado missouri and platte rivers but is not limited to this region 3 3 estimated discharge uncertainty we assess the accuracy of uncalibrated rodeo rsq uncertainty by comparing uncertainty metrics of b i a s u s t d e u and r m s e u with the error metrics of b i a s e s t d e e and r m s e e at all 456 gauges as described in section 2 3 we quantify misestimations of uncertainty by calculating a least squares linear regression y intercept of 0 between the uncertainty and the error metrics for each of the three statistics we find that our uncalibrated uncertainty estimation technique produces underestimates of b i a s u 96 underestimation r 2 0 65 and r m s e u 24 underestimation r 2 0 81 while overestimating s t d e u 39 overestimation r 2 0 89 fig 8 these substantial misestimates of uncertainty can be improved through a calibration step because of the relatively high r 2 values which indicate that the uncertainty well captures the spatial variability of the error to correct for these inaccurate uncertainties we use 60 of the gauges to develop least squares linear regressions for b i a s 95 underestimation r 2 0 70 and s t d e 30 overestimation r 2 0 87 we then use the inverse of the over underestimations as a scaling factor to calibrate the uncertainty estimates of b i a s u and s t d e u and use equation 14 to calculate a calibrated r m s e u next we validate these calibrated uncertainties with the remaining 40 of gauges that were not used in the calibration step post calibration uncertainty b i a s u improves by 59 s t d e u improves by 21 and r m s e u improves by 17 indicating that the technique works well on each of the uncertainty metrics examples of calibrated r m s e u are shown in fig 5 as the error bars note that this calibration technique can also be applied on rodeo data in ungauged reaches to produce relatively accurate discharge uncertainty fig 9 a shows the spatial distribution of normalized rodeo uncertainty n r m s e u calculated by dividing the calibrated r m s e u with mean rodeo discharge the rodeo uncertainties tend to systematically transition from high to low with increasing river size e g mississippi river network fig 9a in addition rodeo estimates high uncertainties along rivers in mexico and along the northern coastlines of canada and alaska where rivers tend to be small we describe the spatial distribution of rodeo uncertainty validation by using calibrated r m s e u r m s e e to assess over underestimations of uncertainty we find that rodeo produces r m s e u within 50 of r m s e e in 322 of the 456 gauge locations fig 9b including 80 of the gauges with poor discharge performance k g e 0 41 3 4 rodeo gee application the workflow for estimating discharge from rodeo widths is implemented into gee as a user friendly publicly available web application fig 10 the source code is on github https github com ryan riggs rodeo and the javascript source code along with detailed instructions for the gee application can be found at this link https rriggs users earthengine app view rodeo using the application involves the following basic steps 1 define the landsat filtering for cloud cover percentage within the grwl centerline buffer a maximum of 10 cloud cover is recommended 2 select the start and end year for the analysis 3 zoom into the map and click a reach of interest a 5 s delay may initially occur while gee processes the landsat data a 1 min delay may occur before the rsq estimates are fully compiled 4 individual figures of the uncertainties rating curve widths and discharge will be presented as they are processed 5 estimated discharge with uncertainties and widths can be exported as a csv file we follow the methods described in section 2 3 to generate calibrated uncertainty for each reach of interest and provide these uncertainties in the gee application the gee application contains over 28 409 reach locations throughout north america at approximately 5 km spacing downstream gaps in rodeo reaches occur when mean river widths decrease below 100 m requiring only an internet connection and a web browser the application can quickly 1 min per location with default options estimate discharge and calibrated uncertainty throughout north america as landsat 9 is planned for launch in 2021 we design the workflow for the easy inclusion of these landsat 9 observations which will improve rodeo rsq temporal frequency 4 discussion 4 1 comparisons between rodeo and other rsq algorithms we find that the performance of rodeo is similar or better than other comparable rsq algorithms to make this comparison we identify locations of overlapping measurements given the high number of gauges used in the validation of rodeo we identified relatively few sites available for comparison we found two overlapping gauge locations from hou et al 2020 which uses modis derived water occurrence values and the w3 hydrological model van dijk et al 2018 to estimate discharge globally other modis based studies are validated outside of north america and thus were not included in our comparison rodeo uses same day comparisons with in situ measurements to calculate statistics while hou et al 2020 uses monthly averaged discharge for assessment even without the exclusion of daily outliers rodeo outperforms the modis based approach at gauges along the mississippi and churchill rivers fig 11 a potential causes of rodeo s higher performance include the increased spatial resolution of landsat compared to modis using grades discharge as opposed to the w3 hydrological model and using effective river widths rather than water occurrence in the development of rating curves we also compare rodeo performance to results presented in feng et al 2019 which uses a mcfli algorithm bayesian amhg manning bam hagemann et al 2017 to compare landsat rsq with planet and sentinel 2 rsq in arctic rivers using both gauge prior information and modeled runoff data we are unable to directly compare rodeo performance to other mcfli algorithms because they do not validate over the same gauges or because they use synthetic rsq data the feng et al study uses four usgs gauges three of which overlap with the rodeo validation to validate landsat rsq from may september for the years 2016 2018 as rodeo rsq is calculated independent of the gauge record we compare rodeo with the modeled prior bam rsq as these estimates constitute truly ungauged scenarios comparing r r m s e at three gauges along the yukon river and tanana river we find that rodeo exhibits higher performance in eight out of the nine bam rsq estimates fig 11b in addition to bam feng et al 2019 follows the approach of pavelsky et al 2014 to develop gauge landsat rating curves for rsq estimation we find that rodeo r r m s e is within 10 percentage points of the gauge landsat rating curve approach at two gauge locations indicating that rodeo s results are near the performance ceiling of landsat rsq at these locations 4 2 sources of rodeo error sources of rodeo error can be attributed to three primary sources the river width measurements the grades discharge estimates and the rodeo rating curve method rodeo s width retrieval method shows a good ability to capture river width over a variety of river sizes fig 4a but there are outliers present as rodeo widths are dependent on the accuracy of the water classification the dswe algorithm is a potential source of error additionally the static grwl centerline and width data used in the extraction of landsat river widths may potentially cause errors during high flows since grwl is built from landsat scenes captured at approximately mean annual discharge allen and pavelsky 2018 rodeo s discharge estimates post 2013 are based on average width discharge relationships established from 1984 to 2013 and this assumption may introduce some error into rodeo rsq estimates during later dates rodeo error can also be traced to the grades modeled discharge data and the rating curve technique used while rodeo s quantile based rating curves are not heavily influenced by grades outliers rodeo will perform poorly on reaches where grades consistently shows bias or poor hydrologic skill for example the positive relative bias seen in rodeo discharge along wide rivers fig 6 is likely related to the positive bias in the grades dataset in large rivers lin et al 2019 further the grades dataset does not perform well in arid regions which may be a source of rodeo s poor performance in some western usgs gauges lin et al 2019 while alternative hydrological model discharge outputs are available such as copernicus glofas alfieri et al 2013 hirpa et al 2018 or noaa nwm national water model 2016 their use in rodeo is beyond the scope of this study in addition to errors associated with landsat width observations and grades simulated discharge rodeo error can also be attributable to the quantile based rating curve method itself inherent to rating curve techniques is the assumption that width and discharge scale perfectly through time and across all hydrologic conditions which is not hydraulically realistic e g kiang et al 2018 more complex analytical approaches could be explored to improve rodeo s rating curve method including machine learning approaches kim et al 2019 along the same lines rodeo s uncertainty estimation is limited to a single reach averaged value rather than estimating uncertainty for individual rodeo rsq estimates e g durand et al 2014 while rodeo uncertainty is shown to be relatively accurate fig 8 using a single reach averaged value is an oversimplification which warrants further refinement e g kratzert et al 2019 shortridge et al 2016 despite these limitations rodeo is one of the first algorithm to quantitatively assess and calibrate rsq uncertainty and it provides a path forward for improved rsq uncertainty estimation 4 3 limitations and future work although rodeo provides a user friendly and efficient method for estimating rsq and uncertainty over large areas it does have some intrinsic limitations for instance the poor temporal resolution of landsat diminishes rodeo s widespread applicability in the near real time future efforts could integrate river width observations from additional sensors including sentinel 1 sentinel 2 or planet which have shorter revisit times and finer spatial resolutions than landsat e g feng et al 2019 shi et al 2020 in addition landsat is inherently limited by cloud cover making rodeo unlikely to produce frequent discharge estimates in regions such as the tropics however this limitation could be diminished with radar observations of river width biancamaria et al 2016 king et al 2013 alternative techniques such as altimetry derived stage based rating curves e g tourian et al 2017 tuozzolo et al 2019 could also be incorporated into the rodeo framework as could cotemporal width and stage observations from the forthcoming surface water and ocean topography swot mission biancamaria et al 2016 despite these limitations the data produced by rodeo can be used in the future to advance the understanding of a wide range of applications and fields of scientific inquiry for example quickly derived rodeo rsq estimates could be used as ancillary information in mcfli algorithms which could further improve rsq accuracy durand et al 2016 additionally future efforts could incorporate rodeo rsq into hydrologic models to improve performance and better understand drivers of change in the global water cycle e g ishitsuka et al 2021 li et al 2020 oubanas et al 2018 for instance rodeo rsq may improve model performance during unpredictable flows e g water withdrawals reservoir releases as rodeo widths are converted to discharge without the need for meteorological inputs e g munier et al 2015 finally as the rodeo application is easily scalable it could be expanded to the planetary level to provide users quick access to near real time discharge and uncertainty estimates worldwide 5 conclusions rodeo provides easily accessible discharge estimates through a gee application while still producing comparable accuracy to other rsq algorithms rodeo can be used to produce efficient discharge estimates in near real time across north america grwl river widths 100 m providing accessible rsq data for use in a variety of fields and producing a large amount of data for improving global hydrologic model performance through data assimilation our analysis at a limited number of sites suggests that rodeo outperforms a modis based rating curve technique and produces comparable results to a hydraulic based mcfli algorithm from our extensive validation it is clear that rodeo performs well on a variety of reaches with most accuracy metrics improving along wide rivers width 400 m while rodeo s performance varies geographically fig 7b rodeo clearly captures discharge dynamics for both narrow and wide rivers rodeo is one of the first algorithms to provide quantitative assessments of rsq uncertainty with potential uses for hydrological model data assimilation through calibration using in situ data we show that estimated rsq uncertainty accuracy can be significantly improved rodeo is largely restricted by the accuracy of the grades dataset and the inability to integrate river parameters other than width into the workflow however when only using river widths rodeo produces comparable accuracy to a mcfli algorithm and could be used in tandem with these more complex algorithms to further improve rsq estimates as the gauge data availability has diminished over the last few decades rsq is increasingly important for fields that rely on in situ data e g hydrologic modeling water management ecology the publicly available rodeo application is uniquely suited to fill this gap as it produces quick and accurate rsq and uncertainty estimates throughout north america from 1984 present software availability software name remotely observed discharge from effective width occurrence rodeo year first official release 2021 hardware requirements pc system requirements windows linux mac program language javascript python availability https rriggs users earthengine app view rodeo https github com ryan riggs rodeo declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements this work was supported by nasa s terrestrial hydrology program nnh17zda001n thp and the president s excellence fund x grants program at texas a m university c h david was supported by the jet propulsion laboratory california institute of technology under a contract with the u s national aeronautics and space administration nasa the data produced in this study are openly available at https zenodo org record 4751885 ymd3zgdkj g rodeo code is shared under an open source berkeley software distribution 3 clause license hosted on github https github com ryan riggs rodeo the validation data used for this study can be found at https zenodo org record 4751885 ymd3zgdkj g appendix a supplementary data the following is the supplementary data to this article multimedia component 1 multimedia component 1 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2021 105254 
25661,satellite remote sensing of river discharge rsq algorithms provide a useful source of observations to supplement river gauge records rsq algorithms have existed for over a decade yet their widespread use has been impeded by a lack of operational usability and quantitative characterization of uncertainty here we present rodeo an algorithm for estimating river discharge using landsat observations in near real time rodeo is validated with 456 gauges median kling gupta efficiency 0 3 and uses a novel quantile rating curve technique that pairs landsat river widths with discharge estimates from a global hydrologic model rodeo also characterizes the uncertainty of rsq estimates estimated root mean square error 7 enabling rsq retrievals to be used for data assimilation into hydrologic models with the goal of expanding the rsq user base the rodeo algorithm is implemented as a freely available off the shelf cloud based google earth engine application that provides rsq estimates across north america from 1984 present keywords remote sensing of discharge landsat google earth engine river width rating curve discharge uncertainty rsq 1 introduction effective management of river water resources necessitates careful monitoring of river discharge through space and time barrow 1998 murphy 1902 the river gauge is the most commonly used tool to measure river discharge and is considered the gold standard for river discharge monitoring dobriyal et al 2017 however at the global scale most river basins are poorly gauged ungauged or have gauges with data records that are not publicly available hannah et al 2011 sheffield et al 2018 additionally gauges provide discharge measurements only at single points along river reaches rather than whole network measurements this sparsity of gauge data has led to the rise of alternative methods for measuring river discharge satellite remote sensing of discharge rsq is an increasingly popular approach for supplementing gauge networks gleason and durand 2020 lettenmaier et al 2015 smith 1997 satellite remote sensing enables rivers to be monitored holistically as an interconnected system from the reach to the global scale several studies have effectively estimated discharge by pairing gauge measurements with satellite observations of river widths e g feng et al 2019 smith and pavelsky 2008 river stage e g paris et al 2016 tourian et al 2013 and surface reflectance tarpanelli and domeneghetti 2021 to generate classic rating curve relationships with r2 values ranging from 0 80 to 0 99 along ungauged reaches similar approaches have paired modeled discharge with moderate resolution imaging spectroradiometer modis satellite imagery to estimate daily to weekly discharge dynamics at the basin to global scale brakenridge et al 2012 hou et al 2018 2020 tarpanelli et al 2013 while these modis studies often successfully estimate changes in relative discharge pearson correlation coefficient r typically ranging from 0 4 to 0 9 they do not focus on estimating absolute discharge a critical quantity in water management applications these studies also do not estimate discharge uncertainty in ungauged basins which is an essential metric for assimilating rsq estimates into hydrologic models in addition to rsq algorithms that are based on rating curve techniques several rsq algorithms follow the mass conserved flow law inversion mcfli approach which can estimate discharge using river observations and bayesian priors e g modeled or gauged discharge gleason et al 2017 examples of mcfli algorithms include at many stations hydraulic geometry amhg gleason and smith 2014 gamo garambois and monnier 2015 metroman durand et al 2014 and several others e g andreadis et al 2020 bonnema et al 2016 durand et al 2016 hagemann et al 2017 see gleason and durand 2020 for a more complete list of rsq algorithms though mcfli algorithms often provide accurate rsq they are sensitive to their bayesian priors and often cannot overcome inaccurate priors durand et al 2016 unlike gauge data satellite rsq measurements are temporally discontinuous limited to the intermittent timing of contamination free satellite overpasses allen et al 2020 to fill in the temporal gaps in rsq measurements significant work has been done to assimilate rsq data into hydrologic models which can simulate discharge continuously through time and space e g andreadis et al 2007 emery et al 2020 fisher et al 2020 oubanas et al 2018 data assimilation techniques require reasonable estimates of rsq uncertainty pan and wood 2010 reichle 2008 while rsq data have been shown to improve model performance through data assimilation the efficacy of these demonstrations is limited by misestimated uncertainty causing an unrealistic confidence in rsq measurements and in turn creating errors in the assimilation step e g ishitsuka et al 2021 pathiraja et al 2018 several mcfli studies discuss uncertainty estimation in detail e g durand et al 2014 hagemann et al 2017 but few if any studies have quantitatively assessed the accuracy of their uncertainty estimates for example brinkerhoff et al 2020 provides uncertainty estimates based on ensembles of bayesian outputs but they do not assess how well these uncertainties represent actual error past studies have implemented rsq algorithms on local machines or on supercomputers typically requiring a relatively complex setup limiting their useability among non expert users sheffield et al 2018 even the more user friendly rsq algorithms like the one presented by hagemann et al 2017 which is implemented as an r package require some familiarity with the coding language and require the user to provide algorithm inputs e g a priori discharge estimates and satellite river observations additionally many of the existing rsq studies use synthetic data for assessing algorithms e g bonnema et al 2016 domeneghetti et al 2018 frasson et al 2021 garambois and monnier 2015 which may fail to represent the full complexity of satellite data the rsq studies that use extant satellite data are often limited to individual rivers e g bjerklie et al 2018 gleason et al 2018 sichangi et al 2018 tarpanelli et al 2020 2019 tuozzolo et al 2019 or basins e g ishitsuka et al 2021 sichangi et al 2016 tourian et al 2017 making it difficult to assess their validity at the global scale in this study we present remotely observed discharge from effective width occurrence rodeo a computationally efficient empirical algorithm for estimating discharge and discharge uncertainty from landsat rodeo integrates historical landsat data and modeled discharge to produce reach level rating curves allowing for past and near real time absolute discharge and discharge uncertainty estimates to be made from landsat observations the rodeo algorithm is incorporated into a freely available google earth engine gee gorelick et al 2017 application which can be run on any internet connected device and produces on the fly discharge fig 1 and uncertainty estimates at the reach scale across north america from 1984 present 2 methods to demonstrate rodeo across a variety of environments we implement the algorithm to 28 409 river reaches across north america we focus on north america because of the high prevalence of in situ width and discharge measurements that can be used for validation we validate rodeo at 456 gauge stations using data from the united states geological survey usgs u s geological survey 2021 and the global runoff data center grdc fig 1 the global runoff data centre 2021 all of the validation data and results are publicly available see acknowledgements 2 1 remote sensing of river widths the rodeo algorithm relies on a river width based rating curve technique to estimate discharge to calculate river widths we use gee to process landsat 5 7 and 8 imagery throughout north america from march 1 1984 to december 31 2020 note that the rodeo gee application is designed to estimate discharge at near real time using the most recently available landsat data and thus this end date pertains to this study s validation purposes only we use the global river widths from landsat grwl database v01 01 allen and pavelsky 2018 to identify the centerline locations of rivers that are on average 100 m or wider which typically exhibit enough width variability to be measurable by 30 m resolution landsat imagery e g feng et al 2019 we found not shown here that calculating effective width e g sichangi et al 2016 smith et al 1995 yielded more accurate results compared to measuring width along individual cross sections e g yang et al 2020 effective width hereinafter referred to as width is calculated along a reach by dividing the river surface area by the river centerline length producing a reach averaged width measurement fig 2 specifically the reach boundary is defined by creating a circular buffer around the center of each reach with a diameter equal to three times the mean grwl width gray circle fig 2a inside this circular buffer a second polygonal buffer around the grwl centerline is created with a width equal to three times the mean grwl width extent of the landsat scene fig 2a the grwl centerline buffer width is based on the approach of yang et al 2020 we use this inner buffer to calculate river widths from landsat imagery we clip all landsat scenes that overlap with the grwl centerline buffer and apply the function of mask fmask classification algorithm to flag pixels containing clouds cloud shadows and or snow ice zhu et al 2015 we then filter out any landsat scenes with a flagged pixel area greater than 10 of the grwl centerline buffer area since this landsat filtering is done for each reach a single landsat scene may be filtered out for one reach and considered contamination free for another after filtering out contaminated landsat scenes we classify surface water using the dynamic surface water extent dswe classification algorithm jones 2015 2019 following the approach from yang et al 2020 we classify water pixels as river or non river pixels based on 8 way connectivity of water pixels to the grwl centerline fig 2b to ensure that we are only considering instances where rodeo accurately captures widths we use a 1 landsat pixel buffer around the grwl centerline buffer blue geometry fig 2c and remove any scenes that contain river pixels within the 1 pixel buffer finally we calculate width by converting the number of river pixels within the grwl centerline buffer into geographic area and dividing this area by the length of the grwl centerline to produce an effective river width fig 2d the processed rodeo widths are then used to develop width based rating curves we validate our width measurements by comparing 1 926 rodeo width observations with same day in situ width measurements from 318 usgs gauges a subset of the gauges shown in fig 1 we exclude any usgs width data equal to 0 m as this is likely an inaccurate measurement for rivers with mean widths 100 m in addition we exclude usgs width data with the quality classification of poor following the method of yang et al 2020 we assess the accuracy of rodeo using mean absolute error m a e b i a s and root mean square error r m s e equations 1 3 in table 1 2 2 rating curve development along each river reach we develop a width based rating curve by pairing rodeo widths with same day modeled discharge fig 3 the source of this modeled discharge is the global reach level a priori discharge estimates grades dataset which contains daily reach level estimates of discharge from 1979 to 2013 lin et al 2019 grades was validated using discharge records from over 14 000 global gauges and displays good results for reaches with widths 100 m with 44 76 containing a daily percentage bias within 20 50 note that grades discharge is not directly calibrated using gauge data lin et al 2019 to build the rating curves we only include modeled discharge estimates that occur on the same day as a rodeo width measurement using these same day data we calculate percentile values of both variables and then use these width and modeled discharge percentiles to build a rating curve by pairing each percentile e g the 5th percentile of modeled discharge is paired with the 5th percentile of rodeo width to create the colorful curve in fig 3 this approach is similar to those taken by tourian et al 2013 and tarpanelli and domeneghetti 2021 we validate the grades data used in this study by comparing 87 290 grades discharge estimates with same day in situ discharge measurements from 439 usgs and grdc gauges the accuracy of grades is described by the error metrics m a e b i a s and r m s e equations 1 3 in table 1 we only use data from 1984 to 2013 to develop the rating curves since landsat 5 was launched march 1 1984 and grades data ends december 31 2013 we extend the rating curves from the 5th to the 95th percentile of flows because this is the range of hydrologic conditions that the landsat archive typically represents flow frequency allen et al 2020 any landsat widths that lie outside the rating curve boundaries are removed from the analysis and the rodeo application to validate the rodeo rating curve technique we compare rodeo rsq with same day in situ discharge measurements at 456 usgs and grdc gauge locations because the rating curves are built with data from 1984 to 2013 we only use river widths from 2014 to 2020 for discharge validation to ensure independence of gauge data in the rodeo validation in line with similar studies dijk et al 2016 hou et al 2018 2020 we use pearson correlation coefficient r to determine rodeo performance in estimating discharge dynamics since the r value does not fully characterize the accuracy of discharge estimates we follow the approach of several other rsq studies e g durand et al 2016 feng et al 2019 ishitsuka et al 2021 paris et al 2016 and use the following metrics to characterize the accuracy kling gupta efficiency k g e normalized root mean square error n r m s e relative bias r b i a s and relative root mean square error r r m s e equations 4 7 2 3 estimating discharge uncertainty to assimilate rodeo discharge data into hydrologic models it is necessary to accurately estimate the uncertainty associated with rodeo rsq estimates we use the term uncertainty to describe estimated error and the term error to describe the actual error of the rodeo rsq estimates to characterize uncertainty for each rodeo reach in north america we calculate the mean difference between the grades discharge q m i and rodeo discharge q ˆ i for each width used to build the rating curves fig 3 the approach used here provides uncertainty estimates for all reach locations without the need for in situ measurements and can be validated where in situ measurements exist following the approach of david et al 2019 we validate rodeo uncertainty by calculating three metrics absolute bias b i a s u standard error s t d e u and r m s e u equations 8 10 in addition we calculate the error metrics b i a s e s t d e e and r m s e e at gauge stations by comparing rodeo discharge q ˆ i to in situ discharge q i equations 11 13 we then compare the uncertainty and error metrics to assess the accuracy of the estimated uncertainty approach b i a s and s t d e are selected because they capture two different uncertainty characteristics and combine to describe r m s e via the pythagorean theorem equation 14 to assess the validity of the rodeo rsq uncertainty we compare the uncertainty to error metrics across all 456 gauges we then calibrate rodeo uncertainty following a method similar to that presented by david et al 2019 specifically using a randomly selected 60 of the gauges n 275 we calculate least squares linear regressions with a y intercept fixed at 0 for the b i a s and s t d e comparisons both the b i a s u and s t d e u are calibrated by 1 slope from their respective linear regressions and are used in equation 14 to develop a corrected r m s e u estimate this step is independently validated using the remaining 40 of gauges n 181 3 results 3 1 rodeo rating curve inputs river width observations from landsat form the empirical basis of rodeo as described in section 2 1 we validate the rodeo width measurements by comparing 1 926 rodeo widths with same day in situ river width measurements from 318 usgs gauges mean river width of 206 m across all 318 gauges we find that the rodeo widths reasonably match in situ measurements m a e 43 8 m b i a s 26 5 m r m s e 83 6 m applying the non parametric theil sen median estimator sen 1968 we derive a robust linear regression between rodeo widths and in situ width measurements fig 4 a the regression produces a slope that deviates from unity by 3 4 indicating that on average rodeo provides accurate observations of river widths and can be used to develop width based rating curves we find that discrepancies between landsat and in situ river width occur at reservoirs or during flood events when the temporal discrepancy between landsat overpasses and in situ river width measurements causes rodeo widths to deviate from the in situ observations modeled discharge from grades directly influences rodeo s rsq performance we validate grades discharge by comparing 87 290 grades discharge estimates used in the development of rodeo with same day in situ discharge measurements from 439 usgs and grdc gauges we determine that grades discharge match in situ measurements across all gauge locations fairly accurately m a e 327 9 cms b i a s 123 5 cms r m s e 951 7 cms in addition we use the theil sen median estimator sen 1968 to derive a robust regression between grades discharge and in situ discharge which produces a slope that deviates from unity by 4 3 fig 4b the cluster of underestimated grades discharge seen in fig 4b occur at 2 gauges in semi arid regions and 6 gauges located near dams reservoirs 3 2 remote sensing of discharge applied across north america rodeo produces 5 835 367 discharge estimates across all 28 409 reach locations with a reach averaged mean of 5 1 observations per year from 1984 to 2020 and 7 0 observations per year since the launch of landsat 8 february 11 2013 rodeo discharge observations vary across the continent with the canadian and alaskan reaches being observed less frequently mean of 4 2 observations per year across 19 796 reaches compared with the remaining southern portion of the continent mean of 9 2 observations per year across 8 613 reaches summer months tend to have more observations than winter months with july being the most commonly observed month narrow rivers widths 400 m tend to be observed nearly as frequently mean of 5 7 observations per year across 21 111 reaches as wide rivers mean of 5 8 observations per year across 7 298 reaches to validate rodeo discharge estimates we compare rodeo rsq from 2014 to 2020 with same day in situ discharge measurements from 456 gauges there is a reach averaged mean of 8 observations per year during the validation period and the total number of observations used in validation is 22 585 comparing rodeo discharge estimates to in situ discharge we find that rodeo typically captures the variability and absolute discharge fig 5 a however at some gauge locations it produces overestimates of discharge magnitude at high flows fig 5b the error bars in fig 5 represent the calibrated r m s e u which are discussed in section 3 3 across all gauges the median n r m s e r b i a s r r m s e k g e and r values are 69 1 81 0 3 and 0 6 respectively fig 6 median statistics 56 18 62 0 2 0 7 for wide rivers with grwl widths 400 m improves n r m s e r r m s e and r indicating that rodeo performance in most error metrics is better or relatively similar along wider rivers however the r b i a s for wide rivers shows a relatively strong positive bias which is a decrease in performance compared to the near zero r b i a s for all gauges the half violin plots in fig 6 show that r r m s e k g e and r have a skewed distribution towards the negative performance direction indicating that rodeo performs well at most gauge locations but sometimes exhibits very poor performance along certain reaches running rodeo on all observable rivers in north america from 1984 to 2020 produces patterns of mean discharge that follows expected spatial variability fig 7 a we find that 64 of high discharge reaches defined here as having a mean rodeo discharge 1000 cms are located in the canada and alaska region in the western usa rodeo is largely limited in application due to a lack of rivers wide enough to be observable by landsat we describe gauge performance spatially using the k g e metric fig 7b and determine that rodeo produces improvements from using the mean discharge k g e 0 41 knoben et al 2019 at 412 of the 456 gauge locations poor performance tends to occur in the western usa along the colorado missouri and platte rivers but is not limited to this region 3 3 estimated discharge uncertainty we assess the accuracy of uncalibrated rodeo rsq uncertainty by comparing uncertainty metrics of b i a s u s t d e u and r m s e u with the error metrics of b i a s e s t d e e and r m s e e at all 456 gauges as described in section 2 3 we quantify misestimations of uncertainty by calculating a least squares linear regression y intercept of 0 between the uncertainty and the error metrics for each of the three statistics we find that our uncalibrated uncertainty estimation technique produces underestimates of b i a s u 96 underestimation r 2 0 65 and r m s e u 24 underestimation r 2 0 81 while overestimating s t d e u 39 overestimation r 2 0 89 fig 8 these substantial misestimates of uncertainty can be improved through a calibration step because of the relatively high r 2 values which indicate that the uncertainty well captures the spatial variability of the error to correct for these inaccurate uncertainties we use 60 of the gauges to develop least squares linear regressions for b i a s 95 underestimation r 2 0 70 and s t d e 30 overestimation r 2 0 87 we then use the inverse of the over underestimations as a scaling factor to calibrate the uncertainty estimates of b i a s u and s t d e u and use equation 14 to calculate a calibrated r m s e u next we validate these calibrated uncertainties with the remaining 40 of gauges that were not used in the calibration step post calibration uncertainty b i a s u improves by 59 s t d e u improves by 21 and r m s e u improves by 17 indicating that the technique works well on each of the uncertainty metrics examples of calibrated r m s e u are shown in fig 5 as the error bars note that this calibration technique can also be applied on rodeo data in ungauged reaches to produce relatively accurate discharge uncertainty fig 9 a shows the spatial distribution of normalized rodeo uncertainty n r m s e u calculated by dividing the calibrated r m s e u with mean rodeo discharge the rodeo uncertainties tend to systematically transition from high to low with increasing river size e g mississippi river network fig 9a in addition rodeo estimates high uncertainties along rivers in mexico and along the northern coastlines of canada and alaska where rivers tend to be small we describe the spatial distribution of rodeo uncertainty validation by using calibrated r m s e u r m s e e to assess over underestimations of uncertainty we find that rodeo produces r m s e u within 50 of r m s e e in 322 of the 456 gauge locations fig 9b including 80 of the gauges with poor discharge performance k g e 0 41 3 4 rodeo gee application the workflow for estimating discharge from rodeo widths is implemented into gee as a user friendly publicly available web application fig 10 the source code is on github https github com ryan riggs rodeo and the javascript source code along with detailed instructions for the gee application can be found at this link https rriggs users earthengine app view rodeo using the application involves the following basic steps 1 define the landsat filtering for cloud cover percentage within the grwl centerline buffer a maximum of 10 cloud cover is recommended 2 select the start and end year for the analysis 3 zoom into the map and click a reach of interest a 5 s delay may initially occur while gee processes the landsat data a 1 min delay may occur before the rsq estimates are fully compiled 4 individual figures of the uncertainties rating curve widths and discharge will be presented as they are processed 5 estimated discharge with uncertainties and widths can be exported as a csv file we follow the methods described in section 2 3 to generate calibrated uncertainty for each reach of interest and provide these uncertainties in the gee application the gee application contains over 28 409 reach locations throughout north america at approximately 5 km spacing downstream gaps in rodeo reaches occur when mean river widths decrease below 100 m requiring only an internet connection and a web browser the application can quickly 1 min per location with default options estimate discharge and calibrated uncertainty throughout north america as landsat 9 is planned for launch in 2021 we design the workflow for the easy inclusion of these landsat 9 observations which will improve rodeo rsq temporal frequency 4 discussion 4 1 comparisons between rodeo and other rsq algorithms we find that the performance of rodeo is similar or better than other comparable rsq algorithms to make this comparison we identify locations of overlapping measurements given the high number of gauges used in the validation of rodeo we identified relatively few sites available for comparison we found two overlapping gauge locations from hou et al 2020 which uses modis derived water occurrence values and the w3 hydrological model van dijk et al 2018 to estimate discharge globally other modis based studies are validated outside of north america and thus were not included in our comparison rodeo uses same day comparisons with in situ measurements to calculate statistics while hou et al 2020 uses monthly averaged discharge for assessment even without the exclusion of daily outliers rodeo outperforms the modis based approach at gauges along the mississippi and churchill rivers fig 11 a potential causes of rodeo s higher performance include the increased spatial resolution of landsat compared to modis using grades discharge as opposed to the w3 hydrological model and using effective river widths rather than water occurrence in the development of rating curves we also compare rodeo performance to results presented in feng et al 2019 which uses a mcfli algorithm bayesian amhg manning bam hagemann et al 2017 to compare landsat rsq with planet and sentinel 2 rsq in arctic rivers using both gauge prior information and modeled runoff data we are unable to directly compare rodeo performance to other mcfli algorithms because they do not validate over the same gauges or because they use synthetic rsq data the feng et al study uses four usgs gauges three of which overlap with the rodeo validation to validate landsat rsq from may september for the years 2016 2018 as rodeo rsq is calculated independent of the gauge record we compare rodeo with the modeled prior bam rsq as these estimates constitute truly ungauged scenarios comparing r r m s e at three gauges along the yukon river and tanana river we find that rodeo exhibits higher performance in eight out of the nine bam rsq estimates fig 11b in addition to bam feng et al 2019 follows the approach of pavelsky et al 2014 to develop gauge landsat rating curves for rsq estimation we find that rodeo r r m s e is within 10 percentage points of the gauge landsat rating curve approach at two gauge locations indicating that rodeo s results are near the performance ceiling of landsat rsq at these locations 4 2 sources of rodeo error sources of rodeo error can be attributed to three primary sources the river width measurements the grades discharge estimates and the rodeo rating curve method rodeo s width retrieval method shows a good ability to capture river width over a variety of river sizes fig 4a but there are outliers present as rodeo widths are dependent on the accuracy of the water classification the dswe algorithm is a potential source of error additionally the static grwl centerline and width data used in the extraction of landsat river widths may potentially cause errors during high flows since grwl is built from landsat scenes captured at approximately mean annual discharge allen and pavelsky 2018 rodeo s discharge estimates post 2013 are based on average width discharge relationships established from 1984 to 2013 and this assumption may introduce some error into rodeo rsq estimates during later dates rodeo error can also be traced to the grades modeled discharge data and the rating curve technique used while rodeo s quantile based rating curves are not heavily influenced by grades outliers rodeo will perform poorly on reaches where grades consistently shows bias or poor hydrologic skill for example the positive relative bias seen in rodeo discharge along wide rivers fig 6 is likely related to the positive bias in the grades dataset in large rivers lin et al 2019 further the grades dataset does not perform well in arid regions which may be a source of rodeo s poor performance in some western usgs gauges lin et al 2019 while alternative hydrological model discharge outputs are available such as copernicus glofas alfieri et al 2013 hirpa et al 2018 or noaa nwm national water model 2016 their use in rodeo is beyond the scope of this study in addition to errors associated with landsat width observations and grades simulated discharge rodeo error can also be attributable to the quantile based rating curve method itself inherent to rating curve techniques is the assumption that width and discharge scale perfectly through time and across all hydrologic conditions which is not hydraulically realistic e g kiang et al 2018 more complex analytical approaches could be explored to improve rodeo s rating curve method including machine learning approaches kim et al 2019 along the same lines rodeo s uncertainty estimation is limited to a single reach averaged value rather than estimating uncertainty for individual rodeo rsq estimates e g durand et al 2014 while rodeo uncertainty is shown to be relatively accurate fig 8 using a single reach averaged value is an oversimplification which warrants further refinement e g kratzert et al 2019 shortridge et al 2016 despite these limitations rodeo is one of the first algorithm to quantitatively assess and calibrate rsq uncertainty and it provides a path forward for improved rsq uncertainty estimation 4 3 limitations and future work although rodeo provides a user friendly and efficient method for estimating rsq and uncertainty over large areas it does have some intrinsic limitations for instance the poor temporal resolution of landsat diminishes rodeo s widespread applicability in the near real time future efforts could integrate river width observations from additional sensors including sentinel 1 sentinel 2 or planet which have shorter revisit times and finer spatial resolutions than landsat e g feng et al 2019 shi et al 2020 in addition landsat is inherently limited by cloud cover making rodeo unlikely to produce frequent discharge estimates in regions such as the tropics however this limitation could be diminished with radar observations of river width biancamaria et al 2016 king et al 2013 alternative techniques such as altimetry derived stage based rating curves e g tourian et al 2017 tuozzolo et al 2019 could also be incorporated into the rodeo framework as could cotemporal width and stage observations from the forthcoming surface water and ocean topography swot mission biancamaria et al 2016 despite these limitations the data produced by rodeo can be used in the future to advance the understanding of a wide range of applications and fields of scientific inquiry for example quickly derived rodeo rsq estimates could be used as ancillary information in mcfli algorithms which could further improve rsq accuracy durand et al 2016 additionally future efforts could incorporate rodeo rsq into hydrologic models to improve performance and better understand drivers of change in the global water cycle e g ishitsuka et al 2021 li et al 2020 oubanas et al 2018 for instance rodeo rsq may improve model performance during unpredictable flows e g water withdrawals reservoir releases as rodeo widths are converted to discharge without the need for meteorological inputs e g munier et al 2015 finally as the rodeo application is easily scalable it could be expanded to the planetary level to provide users quick access to near real time discharge and uncertainty estimates worldwide 5 conclusions rodeo provides easily accessible discharge estimates through a gee application while still producing comparable accuracy to other rsq algorithms rodeo can be used to produce efficient discharge estimates in near real time across north america grwl river widths 100 m providing accessible rsq data for use in a variety of fields and producing a large amount of data for improving global hydrologic model performance through data assimilation our analysis at a limited number of sites suggests that rodeo outperforms a modis based rating curve technique and produces comparable results to a hydraulic based mcfli algorithm from our extensive validation it is clear that rodeo performs well on a variety of reaches with most accuracy metrics improving along wide rivers width 400 m while rodeo s performance varies geographically fig 7b rodeo clearly captures discharge dynamics for both narrow and wide rivers rodeo is one of the first algorithms to provide quantitative assessments of rsq uncertainty with potential uses for hydrological model data assimilation through calibration using in situ data we show that estimated rsq uncertainty accuracy can be significantly improved rodeo is largely restricted by the accuracy of the grades dataset and the inability to integrate river parameters other than width into the workflow however when only using river widths rodeo produces comparable accuracy to a mcfli algorithm and could be used in tandem with these more complex algorithms to further improve rsq estimates as the gauge data availability has diminished over the last few decades rsq is increasingly important for fields that rely on in situ data e g hydrologic modeling water management ecology the publicly available rodeo application is uniquely suited to fill this gap as it produces quick and accurate rsq and uncertainty estimates throughout north america from 1984 present software availability software name remotely observed discharge from effective width occurrence rodeo year first official release 2021 hardware requirements pc system requirements windows linux mac program language javascript python availability https rriggs users earthengine app view rodeo https github com ryan riggs rodeo declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements this work was supported by nasa s terrestrial hydrology program nnh17zda001n thp and the president s excellence fund x grants program at texas a m university c h david was supported by the jet propulsion laboratory california institute of technology under a contract with the u s national aeronautics and space administration nasa the data produced in this study are openly available at https zenodo org record 4751885 ymd3zgdkj g rodeo code is shared under an open source berkeley software distribution 3 clause license hosted on github https github com ryan riggs rodeo the validation data used for this study can be found at https zenodo org record 4751885 ymd3zgdkj g appendix a supplementary data the following is the supplementary data to this article multimedia component 1 multimedia component 1 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2021 105254 
25662,social values for ecosystem services solves 1 5 scope and objectives 2 software design and capabilities 2 1 ppgis value and preference survey data 2 2 environmental data 2 3 solves generalized process flow 2 4 maxent maximum entropy modeling software 2 5 the analyze survey data tool 2 6 the transfer values tool 2 7 the view results tool 3 solves applications 4 discussion 4 1 considerations for solves analyses 4 1 1 survey data 4 1 2 environmental data 4 1 3 appropriateness of social value transfer 4 2 implications for resource management and stakeholders 4 3 future directions for development and analysis 5 conclusion software availability acknowledgements appendix a database schema sample output and survey points example alessa 2008 27 39 l bagstad 2016 2005 2018 k bagstad 2017 77 97 k braunisch 2011 955 967 v brown 2015 119 133 g brown 2009 166 182 g brown 2002 49 76 g chan 2016 1462 1465 k chan 2012 8 18 k clement 2006 j spatiallyexplicitvaluespikesanisabelnationalforestsincolorado clemente 2019 59 68 p coffin 2012 a aspatialanalysisculturalecosystemservicevaluationbyregionalstakeholdersinfloridaacoastalapplicationsocialvaluesforecosystemservicessolvestool cole 2013 511 523 z daniel 2012 8812 8819 t darvill 2016 533 545 r dorning 2017 73 88 m drechsler 2020 104892 m elith 2006 129 151 j elith 2010 1 15 j fagerholm 2021 n goodbody 2021 109377 t gould 2020 1093 1107 r holtslag 2017 69 m citizenperceptionnaturesocialmedia hosmer 2000 392 d appliedlogisticalregression jenks 2012 826 833 k katzgerro 2015 28 t kenter 2019 1439 1461 j kronenberg 2019 1283 1295 j makovnikova 2016 44 52 j mavrommati 2020 22 g meng 2020 101156 s muenchow 2019 e12441 j petway 2020 699 j phillips 2017 887 893 s phillips 2006 231 259 s phillips 2008 161 175 s phillips 2004 655 662 s internationalconferencemachinelearning21stacmpress amaximumentropyapproachspeciesdistributionmodeling qin 2019 3062 3074 k raymond 2009 1301 1315 c rolston 1991 35 40 h semmens 2019 100945 d sherrouse 2011 748 760 b sherrouse 2014 166 177 b sherrouse b sherrouse 2017 431 444 b sherrouse 2014 68 79 b smart 2021 102209 l sun 2019 105 113 f swets 1988 1285 1293 j tadaki 2017 7 m turner 2016 190 207 k tyrvainen 2007 5 19 l vanriper 2012 164 173 c voinov 2016 196 220 a whitehead 2014 992 1003 a wolf 2017 470 495 i zhao 2020 418 q sherrousex2022x105259 sherrousex2022x105259xb 2022 12 01t00 00 00 000z http www elsevier com open access userlicense 1 0 https vtw elsevier com content oragreement 10138 chu doa publishacceptedmanuscriptindexable 2022 12 01t00 00 00 000z http creativecommons org licenses by nc nd 4 0 published by elsevier ltd 2021 12 03t23 34 03 715z http vtw elsevier com data voc addontypes 50 7 aggregated refined usda usda u s department of agriculture http data elsevier com vocabulary scivalfunders 100000199 http sws geonames org 6252001 u s geological survey land change science program u s government this research was supported by the u s geological survey land change science program we would like to thank the journal s anonymous peer reviewers and dr alisa w coffin of the usda agricultural research service for their review comments any use of trade product or firm names is for descriptive purposes only and does not imply endorsement by the u s government 0 item s1364 8152 21 00301 7 s1364815221003017 1 s2 0 s1364815221003017 10 1016 j envsoft 2021 105259 271872 2022 03 02t23 37 07 833874z 2022 02 01 2022 02 28 1 s2 0 s1364815221003017 main pdf https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s1364815221003017 main application pdf 71e299afa295220d292083795ebf6109 main pdf main pdf pdf true 15929773 main 16 1 s2 0 s1364815221003017 main 1 png https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s1364815221003017 preview image png 44d3194d260ec39ff92ab5f990e5a932 main 1 png main 1 png png 58295 849 656 image web pdf 1 1 s2 0 s1364815221003017 gr9 jpg https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s1364815221003017 gr9 downsampled image jpeg 2854f007318098d1dacb0f0df6420eb1 gr9 jpg gr9 gr9 jpg jpg 108498 220 390 image downsampled 1 s2 0 s1364815221003017 gr8 jpg https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s1364815221003017 gr8 downsampled image jpeg 22cd1c5c8db2a1e72074eccd7a2d9a5d gr8 jpg gr8 gr8 jpg jpg 271638 854 667 image downsampled 1 s2 0 s1364815221003017 gr7 jpg https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s1364815221003017 gr7 downsampled image jpeg 69a27ba9cffdd31bf8d28feea95a76e7 gr7 jpg gr7 gr7 jpg jpg 97804 276 389 image downsampled 1 s2 0 s1364815221003017 gr6 jpg https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s1364815221003017 gr6 downsampled image jpeg f71794cf62128ecacbbefb4ff748be75 gr6 jpg gr6 gr6 jpg jpg 107816 282 390 image downsampled 1 s2 0 s1364815221003017 gr5 jpg https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s1364815221003017 gr5 downsampled image jpeg 75e534be50ade263970e0aab959df850 gr5 jpg gr5 gr5 jpg jpg 106039 291 579 image downsampled 1 s2 0 s1364815221003017 gr4 jpg https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s1364815221003017 gr4 downsampled image jpeg 5ba7bd6c4cb9f872ea3fae175f87cade gr4 jpg gr4 gr4 jpg jpg 141142 352 579 image downsampled 1 s2 0 s1364815221003017 gr3 jpg https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s1364815221003017 gr3 downsampled image jpeg 082a3017447efdd9069ef5eb6e99399c gr3 jpg gr3 gr3 jpg jpg 114402 308 556 image downsampled 1 s2 0 s1364815221003017 gr2 jpg https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s1364815221003017 gr2 downsampled image jpeg d3400172463230643893800cd5466b5c gr2 jpg gr2 gr2 jpg jpg 153893 501 667 image downsampled 1 s2 0 s1364815221003017 gr1 jpg https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s1364815221003017 gr1 downsampled image jpeg 13ebc9186588faaf60e7462fe3778dbb gr1 jpg gr1 gr1 jpg jpg 130643 430 579 image downsampled 1 s2 0 s1364815221003017 fx4 jpg https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s1364815221003017 fx4 downsampled image jpeg c7c9213d55e342a20d7a201a69031c6d fx4 jpg fx4 fx4 jpg jpg 175868 967 758 image downsampled 1 s2 0 s1364815221003017 fx3 jpg https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s1364815221003017 fx3 downsampled image jpeg 70992e41a320979d3e9f6ee0a04e526c fx3 jpg fx3 fx3 jpg jpg 399357 967 758 image downsampled 1 s2 0 s1364815221003017 fx2 jpg https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s1364815221003017 fx2 downsampled image jpeg 305b61f24e568265b9939794d967a551 fx2 jpg fx2 fx2 jpg jpg 323349 968 758 image downsampled 1 s2 0 s1364815221003017 fx1 jpg https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s1364815221003017 fx1 downsampled image jpeg 86d9251a9cb8c67bd823559a9830f6df fx1 jpg fx1 fx1 jpg jpg 188798 586 807 image downsampled 1 s2 0 s1364815221003017 gr9 sml https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s1364815221003017 gr9 thumbnail image gif 01f8c86c48eae67723978283f36bff5f gr9 sml gr9 gr9 sml sml 80500 123 219 image thumbnail 1 s2 0 s1364815221003017 gr8 sml https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s1364815221003017 gr8 thumbnail image gif d5a15d13810412adcb0eba0d5cefc2c9 gr8 sml gr8 gr8 sml sml 81351 164 128 image thumbnail 1 s2 0 s1364815221003017 gr7 sml https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s1364815221003017 gr7 thumbnail image gif 9655a30b43d311289284176130e12c34 gr7 sml gr7 gr7 sml sml 77727 155 219 image thumbnail 1 s2 0 s1364815221003017 gr6 sml https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s1364815221003017 gr6 thumbnail image gif 6d3f91dfb15d47219cb6bf41b6c848f8 gr6 sml gr6 gr6 sml sml 79505 159 219 image thumbnail 1 s2 0 s1364815221003017 gr5 sml https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s1364815221003017 gr5 thumbnail image gif 4c8cb047da356ecb701c98a2a0a7216b gr5 sml gr5 gr5 sml sml 73070 110 219 image thumbnail 1 s2 0 s1364815221003017 gr4 sml https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s1364815221003017 gr4 thumbnail image gif 710d1498d8f6958c804a72acaad1861c gr4 sml gr4 gr4 sml sml 79991 133 219 image thumbnail 1 s2 0 s1364815221003017 gr3 sml https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s1364815221003017 gr3 thumbnail image gif 744c2a9843c156ab782dea0211fc64c3 gr3 sml gr3 gr3 sml sml 76239 121 219 image thumbnail 1 s2 0 s1364815221003017 gr2 sml https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s1364815221003017 gr2 thumbnail image gif 3424d19d9c7c93a9311fc8027663347e gr2 sml gr2 gr2 sml sml 80544 164 218 image thumbnail 1 s2 0 s1364815221003017 gr1 sml https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s1364815221003017 gr1 thumbnail image gif 891ab1fcdf8068b8b930275158879d12 gr1 sml gr1 gr1 sml sml 80418 162 219 image thumbnail 1 s2 0 s1364815221003017 fx4 sml https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s1364815221003017 fx4 thumbnail image gif a041412b40bd6d9c80093480d08f3149 fx4 sml fx4 fx4 sml sml 73623 163 128 image thumbnail 1 s2 0 s1364815221003017 fx3 sml https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s1364815221003017 fx3 thumbnail image gif e193381bcc5fb1d768b49f8e5000e967 fx3 sml fx3 fx3 sml sml 85066 163 128 image thumbnail 1 s2 0 s1364815221003017 fx2 sml https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s1364815221003017 fx2 thumbnail image gif a24c0d2156c7f96791dad7fb9ad561de fx2 sml fx2 fx2 sml sml 81471 164 128 image thumbnail 1 s2 0 s1364815221003017 fx1 sml https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s1364815221003017 fx1 thumbnail image gif d45178b17b3c802afad3369733abca84 fx1 sml fx1 fx1 sml sml 80402 159 219 image thumbnail 1 s2 0 s1364815221003017 gr9 lrg jpg https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s1364815221003017 gr9 highres image jpeg 1b708b3f32a2d449f32de3c62cdd0115 gr9 lrg jpg gr9 gr9 lrg jpg jpg 385618 972 1726 image high res 1 s2 0 s1364815221003017 gr8 lrg jpg https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s1364815221003017 gr8 highres image jpeg 0443216ae0156e6a79dd4bc69d50a63f gr8 lrg jpg gr8 gr8 lrg jpg jpg 1863055 3780 2954 image high res 1 s2 0 s1364815221003017 gr7 lrg jpg https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s1364815221003017 gr7 highres image jpeg bc6543a2b0aef359958a9ca5e01fbd8a gr7 lrg jpg gr7 gr7 lrg jpg jpg 267990 1224 1724 image high res 1 s2 0 s1364815221003017 gr6 lrg jpg https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s1364815221003017 gr6 highres image jpeg f49ad72c5b23c7ee093d8864ab629a1d gr6 lrg jpg gr6 gr6 lrg jpg jpg 316202 1250 1726 image high res 1 s2 0 s1364815221003017 gr5 lrg jpg https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s1364815221003017 gr5 highres image jpeg 929338e73751e25093c78c254d50134a gr5 lrg jpg gr5 gr5 lrg jpg jpg 329085 1288 2562 image high res 1 s2 0 s1364815221003017 gr4 lrg jpg https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s1364815221003017 gr4 highres image jpeg c2d794a54410c659a1c941b0bc96781f gr4 lrg jpg gr4 gr4 lrg jpg jpg 572126 1556 2562 image high res 1 s2 0 s1364815221003017 gr3 lrg jpg https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s1364815221003017 gr3 highres image jpeg c39741ecb7a8e5abd53fa334d1ed50bb gr3 lrg jpg gr3 gr3 lrg jpg jpg 399672 1364 2462 image high res 1 s2 0 s1364815221003017 gr2 lrg jpg https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s1364815221003017 gr2 highres image jpeg 7c06a60a62bf05c041e56186522ed8b1 gr2 lrg jpg gr2 gr2 lrg jpg jpg 701639 2216 2953 image high res 1 s2 0 s1364815221003017 gr1 lrg jpg https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s1364815221003017 gr1 highres image jpeg 81aa97000c292f9500b58dc82adebb20 gr1 lrg jpg gr1 gr1 lrg jpg jpg 508236 1901 2562 image high res 1 s2 0 s1364815221003017 fx4 lrg jpg https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s1364815221003017 fx4 highres image jpeg 9d9b37b7bbddb979f9e1102b3c1a65c4 fx4 lrg jpg fx4 fx4 lrg jpg jpg 879119 4279 3354 image high res 1 s2 0 s1364815221003017 fx3 lrg jpg https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s1364815221003017 fx3 highres image jpeg 7629614e3dad4f7ba4d6c86ce71ff93f fx3 lrg jpg fx3 fx3 lrg jpg jpg 2798450 4277 3354 image high res 1 s2 0 s1364815221003017 fx2 lrg jpg https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s1364815221003017 fx2 highres image jpeg ea4363d34b348f23343095f0336e73e8 fx2 lrg jpg fx2 fx2 lrg jpg jpg 2489726 4285 3354 image high res 1 s2 0 s1364815221003017 fx1 lrg jpg https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s1364815221003017 fx1 highres image jpeg ebb6c1b5e82ac1f7f49141798db6007c fx1 lrg jpg fx1 fx1 lrg jpg jpg 866837 2594 3571 image high res 1 s2 0 s1364815221003017 am pdf am am pdf pdf 2940819 aam pdf https s3 eu west 1 amazonaws com prod ucs content store eu west content egi 1036qwvjs7l main application pdf 881203e67529fe3e245eb3e8b6b2dc34 am pdf enso 105259 105259 s1364 8152 21 00301 7 10 1016 j envsoft 2021 105259 fig 1 generalized process flow of the solves 4 0 modeling framework the ecosystem services social values model essm and the value mapping model vmm are accessed by the analyze survey data tool the value transfer mapping model vtmm is accessed by the transfer values tool both tools use maxent maximum entropy modeling software to generate their results fig 1 fig 2 the essm and the vmm accessed with the analyze survey data tool fig 2 fig 3 the select model option screen of the analyze survey data tool after user selection of desired parameters fig 3 fig 4 the select models to generate screen of the analyze survey data tool after user selection of social value types to be analyzed by maxent fig 4 fig 5 the vtmm accessed with the transfer values tool fig 5 fig 6 the transfer values tool after user selection of parameters based on model metadata fig 6 fig 7 the view results tool after user selection of required and optional parameters for generating a social value map layout fig 7 fig 8 analyze survey data tool results for the pike and san isabel national forests generated by the view results tool the social value map is accompanied by graphs reporting the relationship between the value index and each environmental variable as well as auc average nearest neighbor statistics and the maximum attained on the value index scale see appendix a figs a2 and a3 for additional examples fig 8 fig 9 locations where solves studies or studies referencing portions of solves methodology have been conducted fig 9 table 1 definitions of social value types frequently included in typologies to elicit information from stakeholders regarding values they assign to locations table 1 social value type social value definition aesthetic i value these locations because i enjoy the scenery sights sounds smells etc biodiversity i value these locations because they provide a variety of fish wildlife plant life etc cultural i value these locations because they are a place for me to continue to pass down the wisdom and knowledge traditions and way of life of my ancestors economic i value these locations because they provide timber fisheries minerals and or tourism opportunities such as outfitting and guiding future i value these locations because they allow future generations to know and experience the forests as they are now historic i value these locations because they have places and things of natural and human history that matter to me others or the nation intrinsic i value these locations in and of themselves whether people are present or not learning i value these locations because we can learn about the environment through scientific observation or experimentation life sustaining i value these locations because they help produce preserve clean and renew air soil and water recreation i value these locations because they provide a place for my favorite outdoor recreation activities spiritual i value these locations because they are a sacred religious or spiritually special place to me or because i feel reverence and respect for nature there subsistence i value these locations because they provide necessary food and supplies to sustain my life therapeutic i value these locations because they make me feel better physically and or mentally table 2 example of environmental data layers used to generate social value models from the pike and san isabel national forests survey data table 2 layer name description source distance to roads dtr horizontal distance to nearest road in meters u s geological survey usgs the road indicator project trip norm ed datasets http rmgsc cr usgs gov trip data distance to water dtw horizontal distance to nearest water body in meters derived from usgs national hydrography dataset nhd http viewer nationalmap gov viewer nhd html p nhd using arcgis euclidian distance tool elevation elev digital elevation model dem in meters usgs national elevation dataset ned http seamless usgs gov website seamless viewer htm land cover lulc 16 class categorical land cover data usgs national land cover database nlcd 2006 land cover http www mrlc gov nlcd06 data php land surface forms landform 10 class categorical land surface form data usgs global ecosystems http rmgsc cr usgs gov ecosystems data shtml table 3 count of solves studies by type and ecosystem applied locations mapped in fig 9 each study is counted for all applicable categories table 3 study type agricultural coastal forest mountain riparian urban wetland primary study 5 9 13 12 1 9 4 value transfer 2 3 1 scenarios 1 1 3 social media data 1 2 1 3 social biophysical hotspots 2 2 social values for ecosystem services solves open source spatial modeling of cultural services benson c sherrouse a darius j semmens b zachary h ancona b a u s geological survey 12100 beech forest road laurel md 20708 usa u s geological survey 12100 beech forest road laurel md 20708 usa u s geological survey 12100 beech forest road laurel md 20708 usa b u s geological survey p o box 25046 dfc ms 980 denver co 80225 usa u s geological survey dfc p o box 25046 ms 980 denver co 80225 usa u s geological survey p o box 25046 dfc ms 980 denver co 80225 usa corresponding author social values for ecosystem services solves version 4 0 is a fully open source gis based tool designed to aid in the creation of quantitative spatially explicit models of the nonmonetary values attributed to cultural ecosystem services such as aesthetics and recreation specifically to facilitate their incorporation into larger ecosystem service assessments newly redeveloped for qgis solves can be applied in a wide variety of biophysical and social contexts including mountain forest coastal riparian agricultural and urban settings worldwide redeveloping solves for an open source platform was intended to expand its user base by eliminating the cost of proprietary gis software licenses and to remove the impact of proprietary software changes on solves development providing additional options would enable users to delineate relevant stakeholder groups to better assess how differing preferences impact the intensity and spatial distribution of perceived social values keywords cultural ecosystem services nonmonetary valuation public participation gis value transfer scenarios 1 introduction 1 1 adopting open source value modeling increasing the use of open source software in geographic information system gis research areas including public participation gis ppgis and ecosystem services has been recommended to expand the potential for collaborative research and development efforts as well as to increase the number of users who can apply gis to planning and decision making processes across the globe muenchow et al 2019 although open source software comes with its own challenges and limitations it offers benefits that we have chosen to leverage by redeveloping social values for ecosystem services solves sherrouse et al 2011 2014 as an open source tool for the spatially explicit modeling of relationships between human values and environmental characteristics a recent study by petway et al 2020 provides a clear example of how open source development can assist users in modeling these relationships the lack of an available license led the authors to forego the arcgis version of solves and instead replicate its methodology using the r software environment https www r project org and qgis https www qgis org to model cultural ecosystem service values in taiwan ending the dependency of solves on proprietary software provides an opportunity to expand its user base and to define a path of future development to improve value modeling efforts that is not impacted by proprietary software release schedules or license availability 1 2 value perspectives valuation of nature is a process requiring the use of various methods to elicit information from different stakeholders which leads to results that can be qualitatively different and inconsistently represented kronenberg and andersson 2019 kronenberg and andersson 2019 describe the complexity of relationships between social monetary and ecological values that necessitates a social ecological approach to valuation that is more comprehensive such approaches can provide a more explicit accounting for the social value of various aspects of nature a diversity of conflicting values that are grounded in a wide array of cultural experiences and research disciplines from religion and social psychology to indigenous knowledge and philosophy kenter et al 2019 consideration of social values as noted by katz gerro and orenstein 2015 necessarily broadens our perspective on the value of ecosystem services to humans this is particularly beneficial when evaluating cultural ecosystem services that due to their non materiality katz gerro and orenstein 2015 tend to be a relatively neglected component of ecosystem service assessments and decision making chan et al 2016 daniel et al 2012 gould et al 2020 we have previously defined social values as nonmarket values perceived by stakeholders often corresponding to cultural ecosystem services such as aesthetics and recreation sherrouse et al 2014 and continue to use it as the operationalized definition of social values for solves analyses this definition presents value as a magnitude of preference one of four concepts of value in a values typology described by tadaki et al 2017 which also includes the value concepts of contribution to a goal individual priorities and relations values representing magnitudes of preference or contributions to a goal are often operationalized by valuation tools serving to measure human environment relations tadaki et al 2017 information regarding perceived social values representing magnitudes of preference across a value typology when elicited from the public in a spatially explicit format can help provide a basis for robust modeling of the relationships between human valuation and the environmental factors with which values are associated modeling that remains underrepresented to date drechsler 2020 and would assist with addressing the growing demand that stakeholders be more engaged in environmental modeling voinov et al 2016 1 3 social value information sources the collection of data from the public to inform social values modeling takes various forms photo based surveys of landscape perception have been the most common method dorning et al 2017 volunteered geographic information garnered from social media are an increasingly frequent source of data as well e g clemente et al 2019 holtslag 2017 the interest in ppgis which is based on the spatially explicit collection and use of stakeholder data through participatory planning processes has grown significantly over the past 20 years fagerholm et al 2021 a component of many ppgis data collections e g alessa et al 2008 brown et al 2002 raymond et al 2009 wolf et al 2017 is a value typology originally suggested by rolston and coufal 1991 for forest values over time the typology has been modified and adapted and came to be the basis for the social values typology originally implemented in solves sherrouse et al 2011 2014 although ppgis data collection most commonly via surveys can be challenging due to resource institutional and legal constraints brown and reed 2009 the resulting data provide great utility for operationalizing the spatial modeling of relationships between human value perception and underlying environmental characteristics which is recognized as one of the research frontiers of cultural ecosystem services gould et al 2020 furthermore the ability to crosswalk between social value types and ecosystem services particularly cultural services sherrouse et al 2011 2014 facilitates the development of solves as an effective tool to explicitly account for social values on par with economic and ecological values when assessing ecosystem services it should be noted however that this crosswalk is not a perfect one to one relation and certain elements of commonly used typologies are seemingly more akin to economic or ecological values e g economic life sustaining as opposed to social values or cultural services these do however still provide quantitative information regarding publicly perceived values that are not otherwise captured by economic markets or by traditional ecological measures 1 4 social values for ecosystem services solves solves was originally developed as a custom toolbar for arcgis to assess map and quantify the social values of ecosystem services sherrouse et al 2011 2014 although it was documented upon its initial release sherrouse et al 2011 a full description of the solves tool as it has evolved over time has not been published certain elements of its design have remained constant whereas others have changed substantially the relative intensity and spatial distribution of a social value are rendered by solves as a 10 point value index map derived from modeling the relationship between value and preference data collected from survey respondents and potentially explanatory environmental variables individual social value maps can be generated for survey respondents as a whole or for specific survey subgroups stakeholder groups defined by any number of distinguishing characteristics such as demographic groups preferred recreational activities or means of accessing the area points of origin season of use or attitudes and preferences regarding topics of concern for a study area additionally social value models developed for a primary study area can be transferred by solves to areas of similar biophysical and social context that lack their own survey data the appropriateness of these benefit function transfers hereafter referred to as social value transfers can be evaluated by consulting user provided metadata for each model describing the environmental variables and the socio economic and demographic composition of survey respondents which facilitates the assessment of site similarities sherrouse and semmens 2014 semmens et al 2019 this however is but one factor when considering social value transfer and should not be considered sufficient on its own a discussion of various caveats and problems associated with such transfers such as the compounding of errors in original study data can be found in sherrouse et al 2014 finally the processing framework implemented with solves allows it to be applied in almost any social or ecological context based on data availability and without the need for site specific recoding of the software each of these existing elements capabilities and features of solves now exists in a fully open source environment with the development of solves 4 0 1 5 scope and objectives this paper describes the recently developed solves 4 0 to distinguish it as an important open source gis tool for social value modeling solves 4 0 uses the same modeling framework implemented with the previous version of solves version 3 0 however it offers this framework to an expanded audience of potential users by eliminating reliance on proprietary software our objectives are to provide a description of the following the design capabilities and data requirements of solves 4 0 the relationship between the solves 4 0 user interface modeling parameters and underlying methodology the geographic and topical range of previous solves applications to demonstrate the utility of solves 4 0 important considerations for planning and conducting an analysis with solves 4 0 potential directions for developing future versions and refining analyses 2 software design and capabilities solves 4 0 was developed with python as a plugin for qgis https www qgis org a free open source gis the qgis plugin serves as the solves 4 0 user interface for analyzing public value and preference survey data collected to identify the locations and intensities at which survey respondents assign value to individual elements of a typology of social values perceived within a study site these data are analyzed with respect to user selected environmental data layers that are believed to explain the distribution of social values across an area of interest postgresql https www postgresql org in conjunction with its spatially enabling extension postgis https postgis net stores the survey and environmental data required for a solves analysis in the solves source database appendix a fig a1 solves uses the maxent maximum entropy modeling software elith et al 2010 phillips and dudík 2008 phillips et al 2004 2006 2017 to generate models describing the relationship between the survey and environmental data these models can be used by solves to transfer social values to similar locations as determined through consultation of model metadata and other considerations where primary ppgis survey data are not available and to generate environmentally driven scenarios of social value change 2 1 ppgis value and preference survey data solves is designed to work with ppgis value and preference survey data that can be collected in formats including hardcopy surveys sent by mail online desktop and mobile surveys and visitor intercept surveys existing survey data requirements are based on the original development of solves which relied on results from a mail survey of 2000 residents within 45 miles of the pike and san isabel national forests psi in colorado clement 2006 and are further described with respect to solves in sherrouse et al 2011 2014 and sherrouse and semmens 2020 specifically solves modeling is based on the following survey data format optional elements indicated allocations of a theoretical 100 dollars points percent among elements of a social value typology table 1 used as weighting values for each social value type value allocation exercise optional points marked on a study area map e g see appendix a fig a4 indicating locations where allocated values are perceived e g if 20 dollars points percent are allocated to recreation some number of locations associated with recreation should be marked study area map scale dictating scale of final solves output based on respondents ability to spatially resolve points to specific location e g as a rule of thumb data collected at a 1 500 000 scale is analyzed at an output resolution of no finer than 500 m which assumes a point width of 1 mm digital survey maps which allow for data collection at multiple scales require determination of a single output resolution for analysis attitudes and preferences regarding public uses of a study area such as motorized recreation indicating relative level of support or opposition based on a 5 point likert scale see sherrouse and semmens 2020 for adapting likert scales optional socio economic and demographic information which is useful for assessing similarities in social context as a precursor to conducting solves social value transfer semmens et al 2019 although solves requires this general format to be followed it allows users to define their own social value typologies and public uses or other topics of concern such as management actions within the surveys developed for their own research as well as to analyze point data lacking the allocation component of the survey i e weights if a user will not be attempting to segment respondents into distinct subgroups the attitude and preference data are not necessary when solves is used for social value transfer survey data are not required but justifying a transfer will likely require a comparison of the demographic and biophysical similarities between the study and receiving sites sherrouse and semmens 2014 semmens et al 2019 determining a quantifiable measure of contextual similarities between sites which could increase the likelihood of a meaningful transfer is a matter of ongoing inquiry with some findings suggesting promise in comparing widely available demographic variables but also indicating that further work is needed to develop a combined biophysical social indicator to better predict transfer performance semmens et al 2019 2 2 environmental data the environmental data required for a solves analysis are determined by the user and selected according to their judgment regarding specific environmental characteristics that potentially explain how the survey point data and associated social values are distributed across a study area the data selected are relevant to the specific social and biophysical context of the study site some data layers found useful in the environment of the southern rocky mountains are listed in table 2 as an example all environmental data must be provided in a raster format with all data layers aligned to an identical extent although the native resolution of individual data layers may differ solves resamples them all to the resolution of the coarsest spatial data layer based on the output resolution determined by the user often this output resolution is dictated by the scale of the maps used to collect the survey data rather than the environmental data environmental data must be included for both survey data analysis and social value transfer in the case of social value transfer the user must include all environmental layers used to generate the original model and request an output resolution no higher than what is noted in the model metadata 2 3 solves generalized process flow solves implements three separate models which interface with maxent fig 1 two models operate in sequence to perform analyses of survey data the ecosystem services social values model essm allows users to analyze data of all survey respondents or to select respondents belonging to specific stakeholder groups as defined by their attitudes and preferences based upon a user s input the essm retrieves all survey data or a selection thereof and evaluates all social value types e g aesthetic biodiversity etc to identify the single maximum numeric value from among all the social value types once this maximum value has been identified the value mapping model vmm uses it to normalize each social value type selected by the user to determine the maximum it attains on the value index scale the vmm then calls maxent to generate a value index map for each social value type it also generates statistics summarizing the mean or dominant value of each environmental variable along the value index range of each social value map maxent output in the form of a model describing the relationship between the ppgis data and user selected environmental data is also generated this model in turn can be used by the value transfer mapping model vtmm to generate predicted social value maps for biophysically and socially similar sites 2 4 maxent maximum entropy modeling software maxent was originally developed to model the geographic distribution of species but it has been applied to a wide variety of spatial modeling problems e g braunisch et al 2011 goodbody et al 2021 jenks et al 2012 and is well suited to mapping the social values of ecosystem services maxent relies on point data representing observations of species presence without true absence data points where species are observed to be absent available maxent generates randomly selected background points using these point data along with environmental variables that are judged to affect the suitability of the environment for a selected species maxent applies a machine learning method to estimate a probability distribution of maximum entropy closest to uniform while satisfying constraints represented by the environmental variables the logistic surfaces maxent generates are most pertinent to its use with solves each cell contains a value from 0 to 1 with higher values indicating locations more suitable as habitat for a species given the environmental conditions and the known presence of that species in a social values mapping context the logistic output represents the relative intensity that survey respondents assign to a social value type at a location given the underlying environmental characteristics and the respondents identification of such locations as representing a social value type maxent also enhances the functionality of solves by generating statistical models describing the relationship between mapped points and environmental variables or features as operationalized by maxent additionally maxent calculates area under the curve auc statistics for each model models with auc values of 0 5 or less perform at the level of random prediction phillips et al 2006 or worse models with auc values beginning at 0 70 hosmer and lemeshow 2000 swets 1988 to 0 75 elith et al 2006 and above are considered potentially useful solves instructs maxent to use 75 percent of the points from each user selected social value type or survey subgroup as training points and the remaining 25 percent as test points from which to calculate auc statistics the training auc statistics indicate how well the model fits the primary study area whereas the test auc statistics indicate the potential performance of the model in a predictive capacity i e transferring social values to a similar area maxent output includes jackknife statistics that can help solves users improve models by adjusting the environmental variables included in their analysis jackknife tests provide information regarding how the removal of individual environmental variables from the model could impact the predictive potential of the model through an iterative process of consulting the resulting jackknife data solves users can repeat their selected analyses to generate models best suited for their purposes 2 5 the analyze survey data tool the essm and the vmm fig 2 are accessible to users through the analyze survey data tool interface using the select model options screen fig 3 which accesses the essm users provide parameters as described in fig 2 a solves is instructed to analyze a specific survey subgroup by selecting the desired public use and attitude or preference parameters to analyze all survey respondents as a single group neither parameter is selected users also have the option of analyzing a single social value type across all possible survey subgroups by instead selecting the social value type parameter a study area can also be buffered to a designated width when survey data fall outside the formal study area boundary but are of interest to a user for example if another public land unit is adjacent to the study area the weighting of survey points according to the total amounts allocated to each social value type allocation exercise in the survey can also be excluded from an analysis based on user preference or in cases where allocation data are not available threshold features which are one of several feature types i e transformations of the environmental variables included in a solves analysis used by maxent to attempt to find better fitting models can be excluded when they result in social value maps containing spatial artifacts indicating abrupt changes in value intensity that a user judges to be potentially unrealistic the output cell size of a social value map is provided by the user and is most often determined from the scale of the map used during ppgis data collection all spatial data layers are resampled to the output cell size which represents the coarsest data resolution included in the analysis the search radius defaults to 10 times the entered output cell size however it can be adjusted to a higher or lower value if users desire the kernel density function to generate either smoother or coarser surfaces fig 2 c the maximum value is identified from among all cells in all kernel density surfaces the desired user supplied environmental layers are individually selected for inclusion in an analysis fig 2 b and can be changed in subsequent analyses to identify the combination of environmental variables that provide the best fit for the study area or potentially improved predictive power for social value transfer finally the selected environmental layers and survey points are converted to the required formats for input to maxent upon completion of the processing directed by the select model options screen users are presented with the select models to generate screen which accesses the vmm fig 4 here fig 2 d users can select the specific social value type or survey subgroup in cases where they have chosen to analyze a single value type for which they would like social value maps and models generated average nearest neighbor statistics i e r ratios and associated z scores generated by the essm indicate the relative clustering or dispersion of the points mapped by survey respondents providing additional information regarding the social value types that are likely the most highly valued r ratios less than 1 indicate clustering which tends to be related to higher values once value types are selected for modeling the maximum value identified by the essm model is used to normalize all selected kernel density surfaces and the value index maximum of each value type is calculated fig 2 e at this point maxent processes the selected data and returns logistic surfaces that are multiplied by the corresponding value index maximum to produce each final value index map fig 2 f finally zonal statistics are generated summarizing the mean continuous data and dominant categorical data value of each environmental variable along the range of the value index these statistics are included as line continuous and scatterplot categorical graphs in the final solves output to assist with characterizing the relationship between the environmental variables and the value index 2 6 the transfer values tool when a user desires to apply an existing social value model to a site lacking primary survey data the vtmm fig 5 can be accessed with the transfer values tool fig 6 sample models generated by maxent from analysis of data from the pike and san isabel national forests sherrouse et al 2014 are included with the solves 4 0 installation package these models are accompanied by metadata rtf files with the same name as the model describing all processing parameters required by the transfer values tool as well as additional information regarding the necessary environmental layers and the social context of the psi survey as summarized from selected socio economic and demographic survey responses including age gender education and income from the metadata fig 5 a users can enter the transferring site social value type or survey subgroup model value index maximum and output cell size of the model fig 5 b the social value model is then applied to the corresponding environmental data layers fig 5 c of the receiving site to generate a predicted social value map the final value index map resulting from multiplying the maxent logistic surface by the user provided value index maximum fig 5 d like the analyze survey data tool zonal statistics are generated for all environmental data layers included in the model the transfer values tool can also be used to analyze scenarios within a study area by applying value models generated at that site to environmental data layers that have been modified to represent potential changes in the environment at a future point in time i e alternative future scenarios sherrouse et al 2017 2 7 the view results tool outputs from the analyze survey data tool and the transfer values tool can be viewed using the view results tool fig 7 it is not part of the solves modeling framework but a utility for quickly displaying the results of a solves analysis users provide the directory location and project name of their analysis results and the specific social value map layout they wish to view additionally users can optionally select a value map background and locator map boundary that will be included in the final social value map layout fig 8 appendix a fig a2 the final layout includes the selected social value map along with a descriptive title including the project name survey subgroup and social value type along with auc values average nearest neighbor statistics and the maximum score on the value index continuous environmental variables are graphed as lines showing the response of the value index to changes in the mean value of the underlying environmental variable e g after an initial decrease in aesthetic value as dtr increases survey respondents opposed to motorized recreation perceive higher aesthetic value as dtr increases fig 8 categorical environmental variables are graphed as scatterplots indicating the dominant category at each point along the value index e g moderately dry steep slopes category 7 is the dominant landform across most of the value index range fig 8 results generated by the transfer values tool do not include auc or average nearest neighbor statistics reflecting the fact that point data are not used in such analyses appendix a fig a3 3 solves applications since the original version of solves sherrouse et al 2011 was released the tool or studies citing methods drawn from it has been applied on nearly every continent fig 9 to assess the geographic distribution of social values in a wide variety of biophysical and social contexts table 3 initial studies involved the assessment of social values in national forests in colorado and wyoming for different stakeholder subgroups with varying attitudes regarding public uses of these forests such as motorized recreation and oil and gas drilling sherrouse et al 2011 2014 subsequent analysis of these forests showed that the transfer of social value models between forests with more biophysical and social similarity demonstrated higher predictive performance within the receiving forest sherrouse and semmens 2014 semmens et al 2019 additional solves studies examined the social values of coastal areas a study of visitors to hinchinbrook island national park australia showed results differing between consumptive and non consumptive recreationists van riper et al 2012 another coastal study in sarasota bay florida demonstrated how an electronic delphi method could be used to leverage stakeholder engagement to develop the value typology cole et al 2013 that was in turn used in the collection of the survey data for a solves analysis of that site coffin et al 2012 a solves analysis of a slovakian agricultural ecosystem measured the recreational capacity of nonproductive grasslands makovníková et al 2016 while the analysis of another agricultural ecosystem in taiwan suggested that farmers valuations of the landscape were better understood when accounting for cultural places in addition to environmental characteristics petway et al 2020 solves studies have also been conducted in urban and urbanizing areas an assessment of urban green spaces in shanghai china used solves in conjunction with visitor photographs to identify locations where historic and educational values could be increased through additional attention to design of certain park elements sun et al 2019 in anticipation of the 2022 winter olympics in the chongli district of china meng et al 2020 compared solves results with social media reviews to identify mismatches between the supply and demand of cultural ecosystem services within a rapidly urbanizing watershed scenario analysis is another area of research in which solves has been used sherrouse et al 2017 demonstrated the applicability of solves to scenario analysis by developing a road network expansion scenario for two national forests in the southern rocky mountains to consider future trade offs between aesthetic and recreation values driven by land use change a study in the guanzhong tianshui economic region of china explicitly considered aesthetic and recreation values calculated by solves with various scenarios of provisioning and regulating services to select a priority conservation area for protection against soil loss qin et al 2019 in another example of scenario analysis zhao et al 2020 included aesthetic historical and recreation values calculated by solves for xi an china into four economic development scenarios to predict the average value of cultural ecosystem services across each district of the city in the year 2030 results from studies using solves output in conjunction with the biophysical models of the artificial intelligence for ecosystem services aries modeling platform identified hotspots and coldspots indicating where social and ecological values either did or did not coincide within several national forests in the southern rocky mountains bagstad et al 2016 2017 this approach to combining spatial information on cultural and biophysical ecosystem services can inform conservation planning and management by distinguishing a continuum of areas from those that should be prioritized for protection to those where development could be allowed 4 discussion 4 1 considerations for solves analyses considerations are numerous when planning a solves analysis they span all stages from formulating the research question to survey design to evaluating results based on lessons learned from various solves analyses to date a number of these considerations are discussed here 4 1 1 survey data the collection of value and preference survey data through ppgis is essential to the application of solves the ability to collect these data can be limited by costs and other resource constraints as well as institutional and cultural barriers brown and reed 2009 the methods used to collect these data vary and different survey implementation approaches have implications for solves results collecting data through a hardcopy mail survey could be more time and cost effective on the front end but require additional time and costs to digitize the collected response data including the task of sometimes deciphering free hand mapping responses that do not quite follow survey instructions the technical resources to develop an online survey if available provide the ability for a finer scale collection of spatial data with the ability to zoom in when marking valued locations but some respondents lack the technical resources to access or complete the survey in any format the survey could fatigue respondents if too long but miss important information if too streamlined and as with most any type of public survey the ability to obtain representative samples is a challenge selection bias can be further compounded by the mapping requirements of a ppgis survey and the level of spatial literacy of respondents this suggests that sufficient socio economic and demographic information be collected to allow for the evaluation of how far the survey sample diverges from the surveyed population e g brown and reed 2009 the availability of such data also enhances the ability to assess social value models for their appropriateness for social value transfer using solves previous work has shown that social value types having greater than 200 collected value points result more consistently in social value models with test auc scores of 0 70 or higher semmens et al 2019 4 1 2 environmental data compared to survey data the environmental data necessary for a solves analysis are much more readily available however it is important to consider early in the solves study design which environmental variables would potentially be the most useful for explaining the spatial distribution of social values given the specific biophysical context and if these data are available at an appropriate scale for the study area additionally if a social value transfer is anticipated it is important to consider if the same or similar environmental data are available for any potential receiving sites for more common data do continuous data ranges overlap e g elevation and do category definitions align e g land cover for less common data is it possible to locate or derive data that fulfill the requirements of a model developed for the study site compromises between site specific variables that produce models with a very high fit for a study area and more general variables that provide greater predictive potential for social value transfer may need consideration 4 1 3 appropriateness of social value transfer the primary consideration for social value transfer is to assess whether a transfer is appropriate based on biophysical and social similarities between the study and receiving sites biophysical similarity requires that the range of environmental values present at a study location are enough like those at a receiving location to minimize out of range predictions sherrouse and semmens 2014 social similarity measured using demographic information requires that the demography of survey respondents for a study location be representative of the sampled population in a study area as well as the equivalent population at a receiving area semmens et al 2019 as more primary surveys of social values are conducted over time there is the potential for curating collections of social value models that can provide numerous possible candidates of appropriate biophysical and social context for social value transfer 4 2 implications for resource management and stakeholders although solves has always been free and open source it was developed as a toolbar within a proprietary gis requiring an expensive software license to apply the tool by redeveloping solves in an open source gis environment we have removed an important obstacle to more widespread application of the tool for mapping social values and associated cultural ecosystem services although numerous studies across six continents and myriad environmental contexts have already demonstrated the portability and adaptability of solves many have been conducted by students academics with free or reduced cost access to the old proprietary environment we anticipate that the new fully open source version will increase accessibility globally for land and resource managers at the local and regional level who desire or are mandated to incorporate social values and or cultural ecosystem services into their resource assessments solves 4 0 in combination with a social value and preference survey also increases the ability of interested stakeholders to communicate to decision makers the values their communities attribute to shared environments as well as how those values could be impacted positively or negatively by different management alternatives such functionality is responsive to many previous calls for greater inclusion of social values e g mavrommati et al 2020 raymond et al 2009 tyrväinen et al 2007 whitehead et al 2014 cultural ecosystem services e g chan et al 2012 darvill and lindo 2016 smart et al 2021 and stakeholder perspectives e g brown and fagerholm 2015 turner et al 2016 into resource management and planning 4 3 future directions for development and analysis the open source environment of solves 4 0 expands the availability of solves to a broader user audience further enhancements to its design and analysis capabilities could provide new and existing users with a more convenient and robust tool for assessing social values and promote more explicit social value considerations in ecosystem service assessments some of these enhancements will be introduced sooner than others with the initial priority being on simplifying installation and database creation while the development of solves 4 0 to work in a free and open source gis eliminates reliance on proprietary software it also sacrifices some of the streamlining that such software can provide this is primarily evident with the initial installation and setup of solves 4 0 near term priorities for future development include the addition of a bundled installer that would walk users through the installation of all the required software components and reduce the number of steps required from users another addition would involve the development of a data loader tool that will streamline the process of creating the source database both for the included sample data and user provided data longer term possibilities include an additional tool to assist with social context matching based on methods and results described in semmens et al 2019 increasing the flexibility of respondent subgroup delineation by providing options to partition them by factors such as demographics or type of engagement with the study area and considering alternatives to maxent for modeling the spatial relationships between survey and environmental data 5 conclusion solves 4 0 provides a fully open source alternative for assessing mapping and quantifying the social values of ecosystem services it improves upon previous versions of solves which have demonstrated their portability and adaptability across numerous biophysical and social context by expanding the audience of potential users and allowing for future development without regard to proprietary software concerns further enhancing its capabilities would assist resource managers decision makers and stakeholders in the explicit inclusion of social values in ecosystem service assessments software availability software name social values for ecosystem services solves version 4 0 contact address bcsherrouse usgs gov year first official release 2020 hardware requirements pc system requirements windows 10 software requirements qgis 3 8 2 postgresql 11 7 postgis 2 5 3 maxent 3 4 1 java runtime or amazon corretto program language python availability https solves cr usgs gov documentation https doi org 10 3133 tm7c25 declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements this research was supported by the u s geological survey land change science program we would like to thank the journal s anonymous peer reviewers and dr alisa w coffin of the usda agricultural research service for their review comments any use of trade product or firm names is for descriptive purposes only and does not imply endorsement by the u s government appendix a database schema sample output and survey points example fig a 1 the solves source database schema with one to one and one to many relationships indicated between tabular data gray vector data yellow and raster data green fig a 1 fig a 2 analyze survey data tool results as generated by the view results tool fig a 2 fig a 3 transfer values tool results as generated by the view results tool fig a 3 fig a 4 survey points collected for the pike and san isabel national forests fig a 4 
25662,social values for ecosystem services solves 1 5 scope and objectives 2 software design and capabilities 2 1 ppgis value and preference survey data 2 2 environmental data 2 3 solves generalized process flow 2 4 maxent maximum entropy modeling software 2 5 the analyze survey data tool 2 6 the transfer values tool 2 7 the view results tool 3 solves applications 4 discussion 4 1 considerations for solves analyses 4 1 1 survey data 4 1 2 environmental data 4 1 3 appropriateness of social value transfer 4 2 implications for resource management and stakeholders 4 3 future directions for development and analysis 5 conclusion software availability acknowledgements appendix a database schema sample output and survey points example alessa 2008 27 39 l bagstad 2016 2005 2018 k bagstad 2017 77 97 k braunisch 2011 955 967 v brown 2015 119 133 g brown 2009 166 182 g brown 2002 49 76 g chan 2016 1462 1465 k chan 2012 8 18 k clement 2006 j spatiallyexplicitvaluespikesanisabelnationalforestsincolorado clemente 2019 59 68 p coffin 2012 a aspatialanalysisculturalecosystemservicevaluationbyregionalstakeholdersinfloridaacoastalapplicationsocialvaluesforecosystemservicessolvestool cole 2013 511 523 z daniel 2012 8812 8819 t darvill 2016 533 545 r dorning 2017 73 88 m drechsler 2020 104892 m elith 2006 129 151 j elith 2010 1 15 j fagerholm 2021 n goodbody 2021 109377 t gould 2020 1093 1107 r holtslag 2017 69 m citizenperceptionnaturesocialmedia hosmer 2000 392 d appliedlogisticalregression jenks 2012 826 833 k katzgerro 2015 28 t kenter 2019 1439 1461 j kronenberg 2019 1283 1295 j makovnikova 2016 44 52 j mavrommati 2020 22 g meng 2020 101156 s muenchow 2019 e12441 j petway 2020 699 j phillips 2017 887 893 s phillips 2006 231 259 s phillips 2008 161 175 s phillips 2004 655 662 s internationalconferencemachinelearning21stacmpress amaximumentropyapproachspeciesdistributionmodeling qin 2019 3062 3074 k raymond 2009 1301 1315 c rolston 1991 35 40 h semmens 2019 100945 d sherrouse 2011 748 760 b sherrouse 2014 166 177 b sherrouse b sherrouse 2017 431 444 b sherrouse 2014 68 79 b smart 2021 102209 l sun 2019 105 113 f swets 1988 1285 1293 j tadaki 2017 7 m turner 2016 190 207 k tyrvainen 2007 5 19 l vanriper 2012 164 173 c voinov 2016 196 220 a whitehead 2014 992 1003 a wolf 2017 470 495 i zhao 2020 418 q sherrousex2022x105259 sherrousex2022x105259xb 2022 12 01t00 00 00 000z http www elsevier com open access userlicense 1 0 https vtw elsevier com content oragreement 10138 chu doa publishacceptedmanuscriptindexable 2022 12 01t00 00 00 000z http creativecommons org licenses by nc nd 4 0 published by elsevier ltd 2021 12 03t23 34 03 715z http vtw elsevier com data voc addontypes 50 7 aggregated refined usda usda u s department of agriculture http data elsevier com vocabulary scivalfunders 100000199 http sws geonames org 6252001 u s geological survey land change science program u s government this research was supported by the u s geological survey land change science program we would like to thank the journal s anonymous peer reviewers and dr alisa w coffin of the usda agricultural research service for their review comments any use of trade product or firm names is for descriptive purposes only and does not imply endorsement by the u s government 0 item s1364 8152 21 00301 7 s1364815221003017 1 s2 0 s1364815221003017 10 1016 j envsoft 2021 105259 271872 2022 03 02t23 37 07 833874z 2022 02 01 2022 02 28 1 s2 0 s1364815221003017 main pdf https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s1364815221003017 main application pdf 71e299afa295220d292083795ebf6109 main pdf main pdf pdf true 15929773 main 16 1 s2 0 s1364815221003017 main 1 png https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s1364815221003017 preview image png 44d3194d260ec39ff92ab5f990e5a932 main 1 png main 1 png png 58295 849 656 image web pdf 1 1 s2 0 s1364815221003017 gr9 jpg https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s1364815221003017 gr9 downsampled image jpeg 2854f007318098d1dacb0f0df6420eb1 gr9 jpg gr9 gr9 jpg jpg 108498 220 390 image downsampled 1 s2 0 s1364815221003017 gr8 jpg https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s1364815221003017 gr8 downsampled image jpeg 22cd1c5c8db2a1e72074eccd7a2d9a5d gr8 jpg gr8 gr8 jpg jpg 271638 854 667 image downsampled 1 s2 0 s1364815221003017 gr7 jpg https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s1364815221003017 gr7 downsampled image jpeg 69a27ba9cffdd31bf8d28feea95a76e7 gr7 jpg gr7 gr7 jpg jpg 97804 276 389 image downsampled 1 s2 0 s1364815221003017 gr6 jpg https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s1364815221003017 gr6 downsampled image jpeg f71794cf62128ecacbbefb4ff748be75 gr6 jpg gr6 gr6 jpg jpg 107816 282 390 image downsampled 1 s2 0 s1364815221003017 gr5 jpg https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s1364815221003017 gr5 downsampled image jpeg 75e534be50ade263970e0aab959df850 gr5 jpg gr5 gr5 jpg jpg 106039 291 579 image downsampled 1 s2 0 s1364815221003017 gr4 jpg https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s1364815221003017 gr4 downsampled image jpeg 5ba7bd6c4cb9f872ea3fae175f87cade gr4 jpg gr4 gr4 jpg jpg 141142 352 579 image downsampled 1 s2 0 s1364815221003017 gr3 jpg https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s1364815221003017 gr3 downsampled image jpeg 082a3017447efdd9069ef5eb6e99399c gr3 jpg gr3 gr3 jpg jpg 114402 308 556 image downsampled 1 s2 0 s1364815221003017 gr2 jpg https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s1364815221003017 gr2 downsampled image jpeg d3400172463230643893800cd5466b5c gr2 jpg gr2 gr2 jpg jpg 153893 501 667 image downsampled 1 s2 0 s1364815221003017 gr1 jpg https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s1364815221003017 gr1 downsampled image jpeg 13ebc9186588faaf60e7462fe3778dbb gr1 jpg gr1 gr1 jpg jpg 130643 430 579 image downsampled 1 s2 0 s1364815221003017 fx4 jpg https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s1364815221003017 fx4 downsampled image jpeg c7c9213d55e342a20d7a201a69031c6d fx4 jpg fx4 fx4 jpg jpg 175868 967 758 image downsampled 1 s2 0 s1364815221003017 fx3 jpg https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s1364815221003017 fx3 downsampled image jpeg 70992e41a320979d3e9f6ee0a04e526c fx3 jpg fx3 fx3 jpg jpg 399357 967 758 image downsampled 1 s2 0 s1364815221003017 fx2 jpg https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s1364815221003017 fx2 downsampled image jpeg 305b61f24e568265b9939794d967a551 fx2 jpg fx2 fx2 jpg jpg 323349 968 758 image downsampled 1 s2 0 s1364815221003017 fx1 jpg https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s1364815221003017 fx1 downsampled image jpeg 86d9251a9cb8c67bd823559a9830f6df fx1 jpg fx1 fx1 jpg jpg 188798 586 807 image downsampled 1 s2 0 s1364815221003017 gr9 sml https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s1364815221003017 gr9 thumbnail image gif 01f8c86c48eae67723978283f36bff5f gr9 sml gr9 gr9 sml sml 80500 123 219 image thumbnail 1 s2 0 s1364815221003017 gr8 sml https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s1364815221003017 gr8 thumbnail image gif d5a15d13810412adcb0eba0d5cefc2c9 gr8 sml gr8 gr8 sml sml 81351 164 128 image thumbnail 1 s2 0 s1364815221003017 gr7 sml https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s1364815221003017 gr7 thumbnail image gif 9655a30b43d311289284176130e12c34 gr7 sml gr7 gr7 sml sml 77727 155 219 image thumbnail 1 s2 0 s1364815221003017 gr6 sml https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s1364815221003017 gr6 thumbnail image gif 6d3f91dfb15d47219cb6bf41b6c848f8 gr6 sml gr6 gr6 sml sml 79505 159 219 image thumbnail 1 s2 0 s1364815221003017 gr5 sml https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s1364815221003017 gr5 thumbnail image gif 4c8cb047da356ecb701c98a2a0a7216b gr5 sml gr5 gr5 sml sml 73070 110 219 image thumbnail 1 s2 0 s1364815221003017 gr4 sml https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s1364815221003017 gr4 thumbnail image gif 710d1498d8f6958c804a72acaad1861c gr4 sml gr4 gr4 sml sml 79991 133 219 image thumbnail 1 s2 0 s1364815221003017 gr3 sml https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s1364815221003017 gr3 thumbnail image gif 744c2a9843c156ab782dea0211fc64c3 gr3 sml gr3 gr3 sml sml 76239 121 219 image thumbnail 1 s2 0 s1364815221003017 gr2 sml https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s1364815221003017 gr2 thumbnail image gif 3424d19d9c7c93a9311fc8027663347e gr2 sml gr2 gr2 sml sml 80544 164 218 image thumbnail 1 s2 0 s1364815221003017 gr1 sml https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s1364815221003017 gr1 thumbnail image gif 891ab1fcdf8068b8b930275158879d12 gr1 sml gr1 gr1 sml sml 80418 162 219 image thumbnail 1 s2 0 s1364815221003017 fx4 sml https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s1364815221003017 fx4 thumbnail image gif a041412b40bd6d9c80093480d08f3149 fx4 sml fx4 fx4 sml sml 73623 163 128 image thumbnail 1 s2 0 s1364815221003017 fx3 sml https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s1364815221003017 fx3 thumbnail image gif e193381bcc5fb1d768b49f8e5000e967 fx3 sml fx3 fx3 sml sml 85066 163 128 image thumbnail 1 s2 0 s1364815221003017 fx2 sml https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s1364815221003017 fx2 thumbnail image gif a24c0d2156c7f96791dad7fb9ad561de fx2 sml fx2 fx2 sml sml 81471 164 128 image thumbnail 1 s2 0 s1364815221003017 fx1 sml https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s1364815221003017 fx1 thumbnail image gif d45178b17b3c802afad3369733abca84 fx1 sml fx1 fx1 sml sml 80402 159 219 image thumbnail 1 s2 0 s1364815221003017 gr9 lrg jpg https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s1364815221003017 gr9 highres image jpeg 1b708b3f32a2d449f32de3c62cdd0115 gr9 lrg jpg gr9 gr9 lrg jpg jpg 385618 972 1726 image high res 1 s2 0 s1364815221003017 gr8 lrg jpg https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s1364815221003017 gr8 highres image jpeg 0443216ae0156e6a79dd4bc69d50a63f gr8 lrg jpg gr8 gr8 lrg jpg jpg 1863055 3780 2954 image high res 1 s2 0 s1364815221003017 gr7 lrg jpg https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s1364815221003017 gr7 highres image jpeg bc6543a2b0aef359958a9ca5e01fbd8a gr7 lrg jpg gr7 gr7 lrg jpg jpg 267990 1224 1724 image high res 1 s2 0 s1364815221003017 gr6 lrg jpg https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s1364815221003017 gr6 highres image jpeg f49ad72c5b23c7ee093d8864ab629a1d gr6 lrg jpg gr6 gr6 lrg jpg jpg 316202 1250 1726 image high res 1 s2 0 s1364815221003017 gr5 lrg jpg https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s1364815221003017 gr5 highres image jpeg 929338e73751e25093c78c254d50134a gr5 lrg jpg gr5 gr5 lrg jpg jpg 329085 1288 2562 image high res 1 s2 0 s1364815221003017 gr4 lrg jpg https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s1364815221003017 gr4 highres image jpeg c2d794a54410c659a1c941b0bc96781f gr4 lrg jpg gr4 gr4 lrg jpg jpg 572126 1556 2562 image high res 1 s2 0 s1364815221003017 gr3 lrg jpg https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s1364815221003017 gr3 highres image jpeg c39741ecb7a8e5abd53fa334d1ed50bb gr3 lrg jpg gr3 gr3 lrg jpg jpg 399672 1364 2462 image high res 1 s2 0 s1364815221003017 gr2 lrg jpg https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s1364815221003017 gr2 highres image jpeg 7c06a60a62bf05c041e56186522ed8b1 gr2 lrg jpg gr2 gr2 lrg jpg jpg 701639 2216 2953 image high res 1 s2 0 s1364815221003017 gr1 lrg jpg https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s1364815221003017 gr1 highres image jpeg 81aa97000c292f9500b58dc82adebb20 gr1 lrg jpg gr1 gr1 lrg jpg jpg 508236 1901 2562 image high res 1 s2 0 s1364815221003017 fx4 lrg jpg https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s1364815221003017 fx4 highres image jpeg 9d9b37b7bbddb979f9e1102b3c1a65c4 fx4 lrg jpg fx4 fx4 lrg jpg jpg 879119 4279 3354 image high res 1 s2 0 s1364815221003017 fx3 lrg jpg https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s1364815221003017 fx3 highres image jpeg 7629614e3dad4f7ba4d6c86ce71ff93f fx3 lrg jpg fx3 fx3 lrg jpg jpg 2798450 4277 3354 image high res 1 s2 0 s1364815221003017 fx2 lrg jpg https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s1364815221003017 fx2 highres image jpeg ea4363d34b348f23343095f0336e73e8 fx2 lrg jpg fx2 fx2 lrg jpg jpg 2489726 4285 3354 image high res 1 s2 0 s1364815221003017 fx1 lrg jpg https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s1364815221003017 fx1 highres image jpeg ebb6c1b5e82ac1f7f49141798db6007c fx1 lrg jpg fx1 fx1 lrg jpg jpg 866837 2594 3571 image high res 1 s2 0 s1364815221003017 am pdf am am pdf pdf 2940819 aam pdf https s3 eu west 1 amazonaws com prod ucs content store eu west content egi 1036qwvjs7l main application pdf 881203e67529fe3e245eb3e8b6b2dc34 am pdf enso 105259 105259 s1364 8152 21 00301 7 10 1016 j envsoft 2021 105259 fig 1 generalized process flow of the solves 4 0 modeling framework the ecosystem services social values model essm and the value mapping model vmm are accessed by the analyze survey data tool the value transfer mapping model vtmm is accessed by the transfer values tool both tools use maxent maximum entropy modeling software to generate their results fig 1 fig 2 the essm and the vmm accessed with the analyze survey data tool fig 2 fig 3 the select model option screen of the analyze survey data tool after user selection of desired parameters fig 3 fig 4 the select models to generate screen of the analyze survey data tool after user selection of social value types to be analyzed by maxent fig 4 fig 5 the vtmm accessed with the transfer values tool fig 5 fig 6 the transfer values tool after user selection of parameters based on model metadata fig 6 fig 7 the view results tool after user selection of required and optional parameters for generating a social value map layout fig 7 fig 8 analyze survey data tool results for the pike and san isabel national forests generated by the view results tool the social value map is accompanied by graphs reporting the relationship between the value index and each environmental variable as well as auc average nearest neighbor statistics and the maximum attained on the value index scale see appendix a figs a2 and a3 for additional examples fig 8 fig 9 locations where solves studies or studies referencing portions of solves methodology have been conducted fig 9 table 1 definitions of social value types frequently included in typologies to elicit information from stakeholders regarding values they assign to locations table 1 social value type social value definition aesthetic i value these locations because i enjoy the scenery sights sounds smells etc biodiversity i value these locations because they provide a variety of fish wildlife plant life etc cultural i value these locations because they are a place for me to continue to pass down the wisdom and knowledge traditions and way of life of my ancestors economic i value these locations because they provide timber fisheries minerals and or tourism opportunities such as outfitting and guiding future i value these locations because they allow future generations to know and experience the forests as they are now historic i value these locations because they have places and things of natural and human history that matter to me others or the nation intrinsic i value these locations in and of themselves whether people are present or not learning i value these locations because we can learn about the environment through scientific observation or experimentation life sustaining i value these locations because they help produce preserve clean and renew air soil and water recreation i value these locations because they provide a place for my favorite outdoor recreation activities spiritual i value these locations because they are a sacred religious or spiritually special place to me or because i feel reverence and respect for nature there subsistence i value these locations because they provide necessary food and supplies to sustain my life therapeutic i value these locations because they make me feel better physically and or mentally table 2 example of environmental data layers used to generate social value models from the pike and san isabel national forests survey data table 2 layer name description source distance to roads dtr horizontal distance to nearest road in meters u s geological survey usgs the road indicator project trip norm ed datasets http rmgsc cr usgs gov trip data distance to water dtw horizontal distance to nearest water body in meters derived from usgs national hydrography dataset nhd http viewer nationalmap gov viewer nhd html p nhd using arcgis euclidian distance tool elevation elev digital elevation model dem in meters usgs national elevation dataset ned http seamless usgs gov website seamless viewer htm land cover lulc 16 class categorical land cover data usgs national land cover database nlcd 2006 land cover http www mrlc gov nlcd06 data php land surface forms landform 10 class categorical land surface form data usgs global ecosystems http rmgsc cr usgs gov ecosystems data shtml table 3 count of solves studies by type and ecosystem applied locations mapped in fig 9 each study is counted for all applicable categories table 3 study type agricultural coastal forest mountain riparian urban wetland primary study 5 9 13 12 1 9 4 value transfer 2 3 1 scenarios 1 1 3 social media data 1 2 1 3 social biophysical hotspots 2 2 social values for ecosystem services solves open source spatial modeling of cultural services benson c sherrouse a darius j semmens b zachary h ancona b a u s geological survey 12100 beech forest road laurel md 20708 usa u s geological survey 12100 beech forest road laurel md 20708 usa u s geological survey 12100 beech forest road laurel md 20708 usa b u s geological survey p o box 25046 dfc ms 980 denver co 80225 usa u s geological survey dfc p o box 25046 ms 980 denver co 80225 usa u s geological survey p o box 25046 dfc ms 980 denver co 80225 usa corresponding author social values for ecosystem services solves version 4 0 is a fully open source gis based tool designed to aid in the creation of quantitative spatially explicit models of the nonmonetary values attributed to cultural ecosystem services such as aesthetics and recreation specifically to facilitate their incorporation into larger ecosystem service assessments newly redeveloped for qgis solves can be applied in a wide variety of biophysical and social contexts including mountain forest coastal riparian agricultural and urban settings worldwide redeveloping solves for an open source platform was intended to expand its user base by eliminating the cost of proprietary gis software licenses and to remove the impact of proprietary software changes on solves development providing additional options would enable users to delineate relevant stakeholder groups to better assess how differing preferences impact the intensity and spatial distribution of perceived social values keywords cultural ecosystem services nonmonetary valuation public participation gis value transfer scenarios 1 introduction 1 1 adopting open source value modeling increasing the use of open source software in geographic information system gis research areas including public participation gis ppgis and ecosystem services has been recommended to expand the potential for collaborative research and development efforts as well as to increase the number of users who can apply gis to planning and decision making processes across the globe muenchow et al 2019 although open source software comes with its own challenges and limitations it offers benefits that we have chosen to leverage by redeveloping social values for ecosystem services solves sherrouse et al 2011 2014 as an open source tool for the spatially explicit modeling of relationships between human values and environmental characteristics a recent study by petway et al 2020 provides a clear example of how open source development can assist users in modeling these relationships the lack of an available license led the authors to forego the arcgis version of solves and instead replicate its methodology using the r software environment https www r project org and qgis https www qgis org to model cultural ecosystem service values in taiwan ending the dependency of solves on proprietary software provides an opportunity to expand its user base and to define a path of future development to improve value modeling efforts that is not impacted by proprietary software release schedules or license availability 1 2 value perspectives valuation of nature is a process requiring the use of various methods to elicit information from different stakeholders which leads to results that can be qualitatively different and inconsistently represented kronenberg and andersson 2019 kronenberg and andersson 2019 describe the complexity of relationships between social monetary and ecological values that necessitates a social ecological approach to valuation that is more comprehensive such approaches can provide a more explicit accounting for the social value of various aspects of nature a diversity of conflicting values that are grounded in a wide array of cultural experiences and research disciplines from religion and social psychology to indigenous knowledge and philosophy kenter et al 2019 consideration of social values as noted by katz gerro and orenstein 2015 necessarily broadens our perspective on the value of ecosystem services to humans this is particularly beneficial when evaluating cultural ecosystem services that due to their non materiality katz gerro and orenstein 2015 tend to be a relatively neglected component of ecosystem service assessments and decision making chan et al 2016 daniel et al 2012 gould et al 2020 we have previously defined social values as nonmarket values perceived by stakeholders often corresponding to cultural ecosystem services such as aesthetics and recreation sherrouse et al 2014 and continue to use it as the operationalized definition of social values for solves analyses this definition presents value as a magnitude of preference one of four concepts of value in a values typology described by tadaki et al 2017 which also includes the value concepts of contribution to a goal individual priorities and relations values representing magnitudes of preference or contributions to a goal are often operationalized by valuation tools serving to measure human environment relations tadaki et al 2017 information regarding perceived social values representing magnitudes of preference across a value typology when elicited from the public in a spatially explicit format can help provide a basis for robust modeling of the relationships between human valuation and the environmental factors with which values are associated modeling that remains underrepresented to date drechsler 2020 and would assist with addressing the growing demand that stakeholders be more engaged in environmental modeling voinov et al 2016 1 3 social value information sources the collection of data from the public to inform social values modeling takes various forms photo based surveys of landscape perception have been the most common method dorning et al 2017 volunteered geographic information garnered from social media are an increasingly frequent source of data as well e g clemente et al 2019 holtslag 2017 the interest in ppgis which is based on the spatially explicit collection and use of stakeholder data through participatory planning processes has grown significantly over the past 20 years fagerholm et al 2021 a component of many ppgis data collections e g alessa et al 2008 brown et al 2002 raymond et al 2009 wolf et al 2017 is a value typology originally suggested by rolston and coufal 1991 for forest values over time the typology has been modified and adapted and came to be the basis for the social values typology originally implemented in solves sherrouse et al 2011 2014 although ppgis data collection most commonly via surveys can be challenging due to resource institutional and legal constraints brown and reed 2009 the resulting data provide great utility for operationalizing the spatial modeling of relationships between human value perception and underlying environmental characteristics which is recognized as one of the research frontiers of cultural ecosystem services gould et al 2020 furthermore the ability to crosswalk between social value types and ecosystem services particularly cultural services sherrouse et al 2011 2014 facilitates the development of solves as an effective tool to explicitly account for social values on par with economic and ecological values when assessing ecosystem services it should be noted however that this crosswalk is not a perfect one to one relation and certain elements of commonly used typologies are seemingly more akin to economic or ecological values e g economic life sustaining as opposed to social values or cultural services these do however still provide quantitative information regarding publicly perceived values that are not otherwise captured by economic markets or by traditional ecological measures 1 4 social values for ecosystem services solves solves was originally developed as a custom toolbar for arcgis to assess map and quantify the social values of ecosystem services sherrouse et al 2011 2014 although it was documented upon its initial release sherrouse et al 2011 a full description of the solves tool as it has evolved over time has not been published certain elements of its design have remained constant whereas others have changed substantially the relative intensity and spatial distribution of a social value are rendered by solves as a 10 point value index map derived from modeling the relationship between value and preference data collected from survey respondents and potentially explanatory environmental variables individual social value maps can be generated for survey respondents as a whole or for specific survey subgroups stakeholder groups defined by any number of distinguishing characteristics such as demographic groups preferred recreational activities or means of accessing the area points of origin season of use or attitudes and preferences regarding topics of concern for a study area additionally social value models developed for a primary study area can be transferred by solves to areas of similar biophysical and social context that lack their own survey data the appropriateness of these benefit function transfers hereafter referred to as social value transfers can be evaluated by consulting user provided metadata for each model describing the environmental variables and the socio economic and demographic composition of survey respondents which facilitates the assessment of site similarities sherrouse and semmens 2014 semmens et al 2019 this however is but one factor when considering social value transfer and should not be considered sufficient on its own a discussion of various caveats and problems associated with such transfers such as the compounding of errors in original study data can be found in sherrouse et al 2014 finally the processing framework implemented with solves allows it to be applied in almost any social or ecological context based on data availability and without the need for site specific recoding of the software each of these existing elements capabilities and features of solves now exists in a fully open source environment with the development of solves 4 0 1 5 scope and objectives this paper describes the recently developed solves 4 0 to distinguish it as an important open source gis tool for social value modeling solves 4 0 uses the same modeling framework implemented with the previous version of solves version 3 0 however it offers this framework to an expanded audience of potential users by eliminating reliance on proprietary software our objectives are to provide a description of the following the design capabilities and data requirements of solves 4 0 the relationship between the solves 4 0 user interface modeling parameters and underlying methodology the geographic and topical range of previous solves applications to demonstrate the utility of solves 4 0 important considerations for planning and conducting an analysis with solves 4 0 potential directions for developing future versions and refining analyses 2 software design and capabilities solves 4 0 was developed with python as a plugin for qgis https www qgis org a free open source gis the qgis plugin serves as the solves 4 0 user interface for analyzing public value and preference survey data collected to identify the locations and intensities at which survey respondents assign value to individual elements of a typology of social values perceived within a study site these data are analyzed with respect to user selected environmental data layers that are believed to explain the distribution of social values across an area of interest postgresql https www postgresql org in conjunction with its spatially enabling extension postgis https postgis net stores the survey and environmental data required for a solves analysis in the solves source database appendix a fig a1 solves uses the maxent maximum entropy modeling software elith et al 2010 phillips and dudík 2008 phillips et al 2004 2006 2017 to generate models describing the relationship between the survey and environmental data these models can be used by solves to transfer social values to similar locations as determined through consultation of model metadata and other considerations where primary ppgis survey data are not available and to generate environmentally driven scenarios of social value change 2 1 ppgis value and preference survey data solves is designed to work with ppgis value and preference survey data that can be collected in formats including hardcopy surveys sent by mail online desktop and mobile surveys and visitor intercept surveys existing survey data requirements are based on the original development of solves which relied on results from a mail survey of 2000 residents within 45 miles of the pike and san isabel national forests psi in colorado clement 2006 and are further described with respect to solves in sherrouse et al 2011 2014 and sherrouse and semmens 2020 specifically solves modeling is based on the following survey data format optional elements indicated allocations of a theoretical 100 dollars points percent among elements of a social value typology table 1 used as weighting values for each social value type value allocation exercise optional points marked on a study area map e g see appendix a fig a4 indicating locations where allocated values are perceived e g if 20 dollars points percent are allocated to recreation some number of locations associated with recreation should be marked study area map scale dictating scale of final solves output based on respondents ability to spatially resolve points to specific location e g as a rule of thumb data collected at a 1 500 000 scale is analyzed at an output resolution of no finer than 500 m which assumes a point width of 1 mm digital survey maps which allow for data collection at multiple scales require determination of a single output resolution for analysis attitudes and preferences regarding public uses of a study area such as motorized recreation indicating relative level of support or opposition based on a 5 point likert scale see sherrouse and semmens 2020 for adapting likert scales optional socio economic and demographic information which is useful for assessing similarities in social context as a precursor to conducting solves social value transfer semmens et al 2019 although solves requires this general format to be followed it allows users to define their own social value typologies and public uses or other topics of concern such as management actions within the surveys developed for their own research as well as to analyze point data lacking the allocation component of the survey i e weights if a user will not be attempting to segment respondents into distinct subgroups the attitude and preference data are not necessary when solves is used for social value transfer survey data are not required but justifying a transfer will likely require a comparison of the demographic and biophysical similarities between the study and receiving sites sherrouse and semmens 2014 semmens et al 2019 determining a quantifiable measure of contextual similarities between sites which could increase the likelihood of a meaningful transfer is a matter of ongoing inquiry with some findings suggesting promise in comparing widely available demographic variables but also indicating that further work is needed to develop a combined biophysical social indicator to better predict transfer performance semmens et al 2019 2 2 environmental data the environmental data required for a solves analysis are determined by the user and selected according to their judgment regarding specific environmental characteristics that potentially explain how the survey point data and associated social values are distributed across a study area the data selected are relevant to the specific social and biophysical context of the study site some data layers found useful in the environment of the southern rocky mountains are listed in table 2 as an example all environmental data must be provided in a raster format with all data layers aligned to an identical extent although the native resolution of individual data layers may differ solves resamples them all to the resolution of the coarsest spatial data layer based on the output resolution determined by the user often this output resolution is dictated by the scale of the maps used to collect the survey data rather than the environmental data environmental data must be included for both survey data analysis and social value transfer in the case of social value transfer the user must include all environmental layers used to generate the original model and request an output resolution no higher than what is noted in the model metadata 2 3 solves generalized process flow solves implements three separate models which interface with maxent fig 1 two models operate in sequence to perform analyses of survey data the ecosystem services social values model essm allows users to analyze data of all survey respondents or to select respondents belonging to specific stakeholder groups as defined by their attitudes and preferences based upon a user s input the essm retrieves all survey data or a selection thereof and evaluates all social value types e g aesthetic biodiversity etc to identify the single maximum numeric value from among all the social value types once this maximum value has been identified the value mapping model vmm uses it to normalize each social value type selected by the user to determine the maximum it attains on the value index scale the vmm then calls maxent to generate a value index map for each social value type it also generates statistics summarizing the mean or dominant value of each environmental variable along the value index range of each social value map maxent output in the form of a model describing the relationship between the ppgis data and user selected environmental data is also generated this model in turn can be used by the value transfer mapping model vtmm to generate predicted social value maps for biophysically and socially similar sites 2 4 maxent maximum entropy modeling software maxent was originally developed to model the geographic distribution of species but it has been applied to a wide variety of spatial modeling problems e g braunisch et al 2011 goodbody et al 2021 jenks et al 2012 and is well suited to mapping the social values of ecosystem services maxent relies on point data representing observations of species presence without true absence data points where species are observed to be absent available maxent generates randomly selected background points using these point data along with environmental variables that are judged to affect the suitability of the environment for a selected species maxent applies a machine learning method to estimate a probability distribution of maximum entropy closest to uniform while satisfying constraints represented by the environmental variables the logistic surfaces maxent generates are most pertinent to its use with solves each cell contains a value from 0 to 1 with higher values indicating locations more suitable as habitat for a species given the environmental conditions and the known presence of that species in a social values mapping context the logistic output represents the relative intensity that survey respondents assign to a social value type at a location given the underlying environmental characteristics and the respondents identification of such locations as representing a social value type maxent also enhances the functionality of solves by generating statistical models describing the relationship between mapped points and environmental variables or features as operationalized by maxent additionally maxent calculates area under the curve auc statistics for each model models with auc values of 0 5 or less perform at the level of random prediction phillips et al 2006 or worse models with auc values beginning at 0 70 hosmer and lemeshow 2000 swets 1988 to 0 75 elith et al 2006 and above are considered potentially useful solves instructs maxent to use 75 percent of the points from each user selected social value type or survey subgroup as training points and the remaining 25 percent as test points from which to calculate auc statistics the training auc statistics indicate how well the model fits the primary study area whereas the test auc statistics indicate the potential performance of the model in a predictive capacity i e transferring social values to a similar area maxent output includes jackknife statistics that can help solves users improve models by adjusting the environmental variables included in their analysis jackknife tests provide information regarding how the removal of individual environmental variables from the model could impact the predictive potential of the model through an iterative process of consulting the resulting jackknife data solves users can repeat their selected analyses to generate models best suited for their purposes 2 5 the analyze survey data tool the essm and the vmm fig 2 are accessible to users through the analyze survey data tool interface using the select model options screen fig 3 which accesses the essm users provide parameters as described in fig 2 a solves is instructed to analyze a specific survey subgroup by selecting the desired public use and attitude or preference parameters to analyze all survey respondents as a single group neither parameter is selected users also have the option of analyzing a single social value type across all possible survey subgroups by instead selecting the social value type parameter a study area can also be buffered to a designated width when survey data fall outside the formal study area boundary but are of interest to a user for example if another public land unit is adjacent to the study area the weighting of survey points according to the total amounts allocated to each social value type allocation exercise in the survey can also be excluded from an analysis based on user preference or in cases where allocation data are not available threshold features which are one of several feature types i e transformations of the environmental variables included in a solves analysis used by maxent to attempt to find better fitting models can be excluded when they result in social value maps containing spatial artifacts indicating abrupt changes in value intensity that a user judges to be potentially unrealistic the output cell size of a social value map is provided by the user and is most often determined from the scale of the map used during ppgis data collection all spatial data layers are resampled to the output cell size which represents the coarsest data resolution included in the analysis the search radius defaults to 10 times the entered output cell size however it can be adjusted to a higher or lower value if users desire the kernel density function to generate either smoother or coarser surfaces fig 2 c the maximum value is identified from among all cells in all kernel density surfaces the desired user supplied environmental layers are individually selected for inclusion in an analysis fig 2 b and can be changed in subsequent analyses to identify the combination of environmental variables that provide the best fit for the study area or potentially improved predictive power for social value transfer finally the selected environmental layers and survey points are converted to the required formats for input to maxent upon completion of the processing directed by the select model options screen users are presented with the select models to generate screen which accesses the vmm fig 4 here fig 2 d users can select the specific social value type or survey subgroup in cases where they have chosen to analyze a single value type for which they would like social value maps and models generated average nearest neighbor statistics i e r ratios and associated z scores generated by the essm indicate the relative clustering or dispersion of the points mapped by survey respondents providing additional information regarding the social value types that are likely the most highly valued r ratios less than 1 indicate clustering which tends to be related to higher values once value types are selected for modeling the maximum value identified by the essm model is used to normalize all selected kernel density surfaces and the value index maximum of each value type is calculated fig 2 e at this point maxent processes the selected data and returns logistic surfaces that are multiplied by the corresponding value index maximum to produce each final value index map fig 2 f finally zonal statistics are generated summarizing the mean continuous data and dominant categorical data value of each environmental variable along the range of the value index these statistics are included as line continuous and scatterplot categorical graphs in the final solves output to assist with characterizing the relationship between the environmental variables and the value index 2 6 the transfer values tool when a user desires to apply an existing social value model to a site lacking primary survey data the vtmm fig 5 can be accessed with the transfer values tool fig 6 sample models generated by maxent from analysis of data from the pike and san isabel national forests sherrouse et al 2014 are included with the solves 4 0 installation package these models are accompanied by metadata rtf files with the same name as the model describing all processing parameters required by the transfer values tool as well as additional information regarding the necessary environmental layers and the social context of the psi survey as summarized from selected socio economic and demographic survey responses including age gender education and income from the metadata fig 5 a users can enter the transferring site social value type or survey subgroup model value index maximum and output cell size of the model fig 5 b the social value model is then applied to the corresponding environmental data layers fig 5 c of the receiving site to generate a predicted social value map the final value index map resulting from multiplying the maxent logistic surface by the user provided value index maximum fig 5 d like the analyze survey data tool zonal statistics are generated for all environmental data layers included in the model the transfer values tool can also be used to analyze scenarios within a study area by applying value models generated at that site to environmental data layers that have been modified to represent potential changes in the environment at a future point in time i e alternative future scenarios sherrouse et al 2017 2 7 the view results tool outputs from the analyze survey data tool and the transfer values tool can be viewed using the view results tool fig 7 it is not part of the solves modeling framework but a utility for quickly displaying the results of a solves analysis users provide the directory location and project name of their analysis results and the specific social value map layout they wish to view additionally users can optionally select a value map background and locator map boundary that will be included in the final social value map layout fig 8 appendix a fig a2 the final layout includes the selected social value map along with a descriptive title including the project name survey subgroup and social value type along with auc values average nearest neighbor statistics and the maximum score on the value index continuous environmental variables are graphed as lines showing the response of the value index to changes in the mean value of the underlying environmental variable e g after an initial decrease in aesthetic value as dtr increases survey respondents opposed to motorized recreation perceive higher aesthetic value as dtr increases fig 8 categorical environmental variables are graphed as scatterplots indicating the dominant category at each point along the value index e g moderately dry steep slopes category 7 is the dominant landform across most of the value index range fig 8 results generated by the transfer values tool do not include auc or average nearest neighbor statistics reflecting the fact that point data are not used in such analyses appendix a fig a3 3 solves applications since the original version of solves sherrouse et al 2011 was released the tool or studies citing methods drawn from it has been applied on nearly every continent fig 9 to assess the geographic distribution of social values in a wide variety of biophysical and social contexts table 3 initial studies involved the assessment of social values in national forests in colorado and wyoming for different stakeholder subgroups with varying attitudes regarding public uses of these forests such as motorized recreation and oil and gas drilling sherrouse et al 2011 2014 subsequent analysis of these forests showed that the transfer of social value models between forests with more biophysical and social similarity demonstrated higher predictive performance within the receiving forest sherrouse and semmens 2014 semmens et al 2019 additional solves studies examined the social values of coastal areas a study of visitors to hinchinbrook island national park australia showed results differing between consumptive and non consumptive recreationists van riper et al 2012 another coastal study in sarasota bay florida demonstrated how an electronic delphi method could be used to leverage stakeholder engagement to develop the value typology cole et al 2013 that was in turn used in the collection of the survey data for a solves analysis of that site coffin et al 2012 a solves analysis of a slovakian agricultural ecosystem measured the recreational capacity of nonproductive grasslands makovníková et al 2016 while the analysis of another agricultural ecosystem in taiwan suggested that farmers valuations of the landscape were better understood when accounting for cultural places in addition to environmental characteristics petway et al 2020 solves studies have also been conducted in urban and urbanizing areas an assessment of urban green spaces in shanghai china used solves in conjunction with visitor photographs to identify locations where historic and educational values could be increased through additional attention to design of certain park elements sun et al 2019 in anticipation of the 2022 winter olympics in the chongli district of china meng et al 2020 compared solves results with social media reviews to identify mismatches between the supply and demand of cultural ecosystem services within a rapidly urbanizing watershed scenario analysis is another area of research in which solves has been used sherrouse et al 2017 demonstrated the applicability of solves to scenario analysis by developing a road network expansion scenario for two national forests in the southern rocky mountains to consider future trade offs between aesthetic and recreation values driven by land use change a study in the guanzhong tianshui economic region of china explicitly considered aesthetic and recreation values calculated by solves with various scenarios of provisioning and regulating services to select a priority conservation area for protection against soil loss qin et al 2019 in another example of scenario analysis zhao et al 2020 included aesthetic historical and recreation values calculated by solves for xi an china into four economic development scenarios to predict the average value of cultural ecosystem services across each district of the city in the year 2030 results from studies using solves output in conjunction with the biophysical models of the artificial intelligence for ecosystem services aries modeling platform identified hotspots and coldspots indicating where social and ecological values either did or did not coincide within several national forests in the southern rocky mountains bagstad et al 2016 2017 this approach to combining spatial information on cultural and biophysical ecosystem services can inform conservation planning and management by distinguishing a continuum of areas from those that should be prioritized for protection to those where development could be allowed 4 discussion 4 1 considerations for solves analyses considerations are numerous when planning a solves analysis they span all stages from formulating the research question to survey design to evaluating results based on lessons learned from various solves analyses to date a number of these considerations are discussed here 4 1 1 survey data the collection of value and preference survey data through ppgis is essential to the application of solves the ability to collect these data can be limited by costs and other resource constraints as well as institutional and cultural barriers brown and reed 2009 the methods used to collect these data vary and different survey implementation approaches have implications for solves results collecting data through a hardcopy mail survey could be more time and cost effective on the front end but require additional time and costs to digitize the collected response data including the task of sometimes deciphering free hand mapping responses that do not quite follow survey instructions the technical resources to develop an online survey if available provide the ability for a finer scale collection of spatial data with the ability to zoom in when marking valued locations but some respondents lack the technical resources to access or complete the survey in any format the survey could fatigue respondents if too long but miss important information if too streamlined and as with most any type of public survey the ability to obtain representative samples is a challenge selection bias can be further compounded by the mapping requirements of a ppgis survey and the level of spatial literacy of respondents this suggests that sufficient socio economic and demographic information be collected to allow for the evaluation of how far the survey sample diverges from the surveyed population e g brown and reed 2009 the availability of such data also enhances the ability to assess social value models for their appropriateness for social value transfer using solves previous work has shown that social value types having greater than 200 collected value points result more consistently in social value models with test auc scores of 0 70 or higher semmens et al 2019 4 1 2 environmental data compared to survey data the environmental data necessary for a solves analysis are much more readily available however it is important to consider early in the solves study design which environmental variables would potentially be the most useful for explaining the spatial distribution of social values given the specific biophysical context and if these data are available at an appropriate scale for the study area additionally if a social value transfer is anticipated it is important to consider if the same or similar environmental data are available for any potential receiving sites for more common data do continuous data ranges overlap e g elevation and do category definitions align e g land cover for less common data is it possible to locate or derive data that fulfill the requirements of a model developed for the study site compromises between site specific variables that produce models with a very high fit for a study area and more general variables that provide greater predictive potential for social value transfer may need consideration 4 1 3 appropriateness of social value transfer the primary consideration for social value transfer is to assess whether a transfer is appropriate based on biophysical and social similarities between the study and receiving sites biophysical similarity requires that the range of environmental values present at a study location are enough like those at a receiving location to minimize out of range predictions sherrouse and semmens 2014 social similarity measured using demographic information requires that the demography of survey respondents for a study location be representative of the sampled population in a study area as well as the equivalent population at a receiving area semmens et al 2019 as more primary surveys of social values are conducted over time there is the potential for curating collections of social value models that can provide numerous possible candidates of appropriate biophysical and social context for social value transfer 4 2 implications for resource management and stakeholders although solves has always been free and open source it was developed as a toolbar within a proprietary gis requiring an expensive software license to apply the tool by redeveloping solves in an open source gis environment we have removed an important obstacle to more widespread application of the tool for mapping social values and associated cultural ecosystem services although numerous studies across six continents and myriad environmental contexts have already demonstrated the portability and adaptability of solves many have been conducted by students academics with free or reduced cost access to the old proprietary environment we anticipate that the new fully open source version will increase accessibility globally for land and resource managers at the local and regional level who desire or are mandated to incorporate social values and or cultural ecosystem services into their resource assessments solves 4 0 in combination with a social value and preference survey also increases the ability of interested stakeholders to communicate to decision makers the values their communities attribute to shared environments as well as how those values could be impacted positively or negatively by different management alternatives such functionality is responsive to many previous calls for greater inclusion of social values e g mavrommati et al 2020 raymond et al 2009 tyrväinen et al 2007 whitehead et al 2014 cultural ecosystem services e g chan et al 2012 darvill and lindo 2016 smart et al 2021 and stakeholder perspectives e g brown and fagerholm 2015 turner et al 2016 into resource management and planning 4 3 future directions for development and analysis the open source environment of solves 4 0 expands the availability of solves to a broader user audience further enhancements to its design and analysis capabilities could provide new and existing users with a more convenient and robust tool for assessing social values and promote more explicit social value considerations in ecosystem service assessments some of these enhancements will be introduced sooner than others with the initial priority being on simplifying installation and database creation while the development of solves 4 0 to work in a free and open source gis eliminates reliance on proprietary software it also sacrifices some of the streamlining that such software can provide this is primarily evident with the initial installation and setup of solves 4 0 near term priorities for future development include the addition of a bundled installer that would walk users through the installation of all the required software components and reduce the number of steps required from users another addition would involve the development of a data loader tool that will streamline the process of creating the source database both for the included sample data and user provided data longer term possibilities include an additional tool to assist with social context matching based on methods and results described in semmens et al 2019 increasing the flexibility of respondent subgroup delineation by providing options to partition them by factors such as demographics or type of engagement with the study area and considering alternatives to maxent for modeling the spatial relationships between survey and environmental data 5 conclusion solves 4 0 provides a fully open source alternative for assessing mapping and quantifying the social values of ecosystem services it improves upon previous versions of solves which have demonstrated their portability and adaptability across numerous biophysical and social context by expanding the audience of potential users and allowing for future development without regard to proprietary software concerns further enhancing its capabilities would assist resource managers decision makers and stakeholders in the explicit inclusion of social values in ecosystem service assessments software availability software name social values for ecosystem services solves version 4 0 contact address bcsherrouse usgs gov year first official release 2020 hardware requirements pc system requirements windows 10 software requirements qgis 3 8 2 postgresql 11 7 postgis 2 5 3 maxent 3 4 1 java runtime or amazon corretto program language python availability https solves cr usgs gov documentation https doi org 10 3133 tm7c25 declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements this research was supported by the u s geological survey land change science program we would like to thank the journal s anonymous peer reviewers and dr alisa w coffin of the usda agricultural research service for their review comments any use of trade product or firm names is for descriptive purposes only and does not imply endorsement by the u s government appendix a database schema sample output and survey points example fig a 1 the solves source database schema with one to one and one to many relationships indicated between tabular data gray vector data yellow and raster data green fig a 1 fig a 2 analyze survey data tool results as generated by the view results tool fig a 2 fig a 3 transfer values tool results as generated by the view results tool fig a 3 fig a 4 survey points collected for the pike and san isabel national forests fig a 4 
25663,the modflow api allows other programs to control modflow and interactively change variables without having to modify the source code the modflow api is based on the basic model interface bmi which is a set of conventions that define how to initialize a simulation update the model state by advancing in time and finalize the run for many existing modflow coupling applications the information provided to modflow must be updated multiple times in a time step as this capability to modify variables within a time step is not defined by the bmi an extension to bmi was developed this extended model interface is part of the modflow api and allows such a tight coupling to other models examples are included for a variety of use cases including new flexibility for users to develop custom packages without modifying the modflow source code and coupling modflow with other models and optimization libraries abbreviations api application programming interface bmi basic model interface cca common component architecture csdms community surface dynamics modeling system esmf earth system modeling framework evt evapotranspiration gwf groundwater flow gwt groundwater transport hru hydrologic response unit openmi open modeling interface prms precipitation runoff modeling system sfr streamflow routing uzf unsaturated zone flow xmi extended model interface keywords modflow 6 modflow api basic model interface metaswap prms modsim 1 introduction for over 30 years the modflow program has been widely used by academics private consultants and government scientists to accurately reliably and efficiently simulate groundwater flow and related processes due to its widespread popularity modular structure and thorough documentation modflow has been successfully coupled with many other physical process models and programs for example modflow has been coupled with watershed models including prms markstrom et al 2008 swat kim et al 2008 and hspf davis 2001 modflow has been combined with solute transport models including moc3d winston et al 2018 and mt3d langevin et al 2008 optimization routines have been coupled with modflow to maximize groundwater extraction subject to various constraints ahlfeld et al 2005 and to identify optimum surface water deliveries in order to meet water demands morway et al 2016 modflow has been extended to include pipe flow models to represent karst conditions shoemaker et al 2008 and flow in the unsaturated zone twarakavi et al 2008 modflow has also been coupled to hydrodynamic surface water models including hec ras rodriguez et al 2008 swift2d wang et al 2007 and branch swain and wexler 1996 the majority of previous coupling approaches used with modflow have been to combine the codes into a single program this single program coupling approach has often led to coupled programs that could not keep up with advances of the individual codes as a result many of these custom modflow variants have become stale over time and do not support recent modflow advances or advances to the other process model efforts to couple modflow with other process models requires extensive knowledge of both models an understanding of the types of information needed for the coupling performance objectives and a clear purpose for the resulting coupled modflow program these broader model integration issues are described by belete et al 2017 in a comprehensive overview the overview outlines phases of the model integration effort summarizes capabilities of existing frameworks for coupling models and describes the challenges of designing a generalized integration framework overcoming these challenges can allow for new and innovative ways for coupling models including sharing models and data over the web chen et al 2020 modflow6 is the most recent version of modflow and is currently the core version of modflow distributed and released by the u s geological survey modflow6 is an object oriented program and framework written in fortran the modflow6 software provides a platform for supporting multiple models and multiple types of models within the same simulation langevin et al 2017 hughes et al 2017 these models can be independent of one another with no interaction they can exchange information or they can be tightly coupled at the matrix level by adding them to the same numerical solution transfer of information between models is isolated to exchange objects which contain the data and code needed to couple models this design feature allow models to be developed maintained and used independently without having custom information about other models in the framework within this new framework a regional scale groundwater model may be coupled with multiple local scale groundwater models or a groundwater transport model may be coupled with a groundwater flow model langevin et al 2020 modflow6 currently includes the groundwater flow gwf model and the groundwater transport gwt model each with packages to represent surface water processes groundwater extraction external boundaries mass sources and sinks and mass sorption and reactions morway et al 2021 show how the advanced packages in modflow 6 can be connected to represent watershed processes in managed basins modflow6 also includes advanced capabilities to simulate three dimensional anisotropy and dispersion and correct grid errors for cell connections that violate generalized control volume finite difference assumptions panday et al 2013 provost et al 2017 to facilitate coupling with other models including those written in another language an application programming interface api was developed for modflow the api was developed using the established basic model interface bmi standard the bmi was developed by community surface dynamics modeling system csdms to provide a standard component based interface for earth science models peckham et al 2013 the common component architecture cca armstrong et al 1999 the earth system modeling framework esmf collins et al 2005 and the open modeling interface openmi gregersen et al 2007 are examples of other coupling standards the bmi standard was selected for the modflow api because it was developed specifically for earth science models and it was clear how the bmi could be used to couple modflow with the types of process models needed by the community some example applications of the bmi with earth science models include the coupling of 1 a river sediment transport model to a delta model that distributes the sediment ratliff et al 2018 and 2 a hydrologic model to a hydrodynamic model to improve flood inundation simulations hoch et al 2019 since its initial release development of the bmi has continued and updates have included functions for accessing variable metadata and for working with structured and unstructured grids hutton et al 2020 although the bmi provides the foundation for the modflow api additional functionality was required to allow for a tighter coupling with other process models than is possible with the standard bmi functions specifically there was a need to allow other components to be coupled with modflow within the non linear picard iteration loop see for example ferziger and peric 1996 for a treatment of this type of coupling and the picard iteration for example gsflow markstrom et al 2008 is a coupled version of modflow and prms markstrom et al 2015 a key feature of gsflow is the tight coupling of the prms soil zone and overland flow components with the modflow groundwater flow component it is implemented in gsflow by allowing the soil zone component to be calculated as part of the modflow picard iteration until convergence is achieved in the modflow api the bmi was extended to support this same type of tight component coupling that was used in gsflow but in a generic way that allows other models to be solved simultaneously with modflow the purpose of this paper is to describe the new modflow api which is implemented in modflow6 subsequent modflow references in this paper refer specifically to the modflow6 version although the focus of the paper is on coupling modflow with other process models the api is general and will allow modflow to be called by a wide variety of software programs such as by plotting programs or geographical information systems for example the paper first shows how the bmi was implemented in modflow including an explanation of how a modflow simulation can be controlled by an external program written in another language e g python c etc using model control and time functions and the way modflow variables can be accessed during the simulation the paper then describes extensions to the bmi that allow modflow to be tightly coupled with other process models finally five examples are presented to demonstrate a variety of uses for the modflow api including new flexibility for users to develop custom packages without modifying the modflow source code and coupling modflow with other process models and optimization libraries 2 development of the modflow api development of the modflow api required refactoring the source code into the core component this core component can be compiled into an executable program fig 1 which is provided as part of the standard distribution released by the u s geological survey the core component can also be compiled with the api routines into a shared library that can be used for interoperable applications the refactoring effort focused on ensuring that the routines and calls for the executable corresponded exactly to calls made to the shared library through the api and that none of the routines were duplicated extensive testing was performed and continues to be performed through continuous integration practices to ensure that the executable and library versions give identical numerical results and have equivalent run times with multiple compilers on the supported operating systems windows linux and macos as shown in fig 1 the api layer builds on the modflow core modflow is considered to be either the executable version or the shared library version as the latter can be initiated and run in a manner that gives the same results as the executable fig 1 shows three different implementations of an interoperable layer that sits between the modflow api and applications based on it an interoperable layer such as the modflowapi python package makes it easier to control modflow and transfer information the purpose of this section is to describe the api layer shown in fig 1 including both the bmi and the extended model interface xmi the subsequent section contains examples on how to use the modflow api in practice as shown in fig 1 the api layer builds on the modflow core it currently contains the extended model interface xmi which is the basic model interface bmi and the extensions described below plus supporting functionality however it is expected that it will continue to grow exposing more internal modflow functionality for use in other applications modflow is considered to be either the executable version or the shared library version as the latter can be initiated and run in a manner that gives the same results as the executable fig 1 shows three different implementations of an interoperable layer that sits between the modflow api and applications based on it an interoperable layer such as the modflowapi python package makes it easier to control modflow and transfer information the purpose of this section is to describe the api layer shown in fig 1 including both the bmi and the extended model interface xmi the subsequent section contains examples on how to use the modflow api in practice 2 1 implementation of the basic model interface in order to expose the modflow functionality with the main bmi control functions initialize update finalize the source code has been refactored a high level api is developed to aggregate the traditional modflow subroutines into functional units that can be mapped directly to the bmi initialize update and finalize functions as shown by the diagram in fig 2 the main reason for the code refactoring is the requirement that the same program code is executed regardless of whether the simulation is run with the executable or with the shared library containing the newly developed bmi as implied by the control flow in fig 2 the caller of the bmi control functions is also responsible for implementing the time step loop the simulation time inside modflow is divided into stress periods which are subdivided into time steps stress periods are intervals during which external stresses can be redefined and are provided as a convenience for users the time steps constitute the potentially non equidistant discretization of time used to solve the numerical models in the simulation note that as opposed to previous versions of the code there is no explicit loop over stress periods anymore in modflow which complies with the interpretation of the time loop as prescribed by the bmi a call to the update function advances the simulation by a single time step the start time of the simulation get start time is fixed at 0 0 for all simulations the end time get end time is set to the total simulation time which is equal to the summed lengths of the individual time steps the current time can be queried with a call to get current time but it is important to realize that its value is not updated until the internal time update tu is called the current time step length can be queried with a call to get time step the other part of the bmi implementation is its ability to access internal model component data nearly all array and scalar variables in modflow are declared as fortran pointers metcalf et al 2018 and managed by a dedicated module called the memory manager which allocates variables and tags them with a unique address string composed of the variable s name and a memory path identifying the unique name of the modflow component type and for modflow package variables the unique name of the package it belongs to examples of the address string for a number of characteristic variables is shown in table 1 modflow component types include timing solution model exchange and utility components see hughes et al 2017 for more information for user convenience and to assure future compatibility with the library the get var address utility function is available to construct this string based on the syntax used internally by the memory manager an inventory of all accessible variables can be retrieved with the bmi function get input var names or get output var names as no distinction has been made between input or output variables in the modflow api or by setting memory print option to all in the options block of the simulation namefile although in principle it is possible to modify all variables inside the memory manager this should be used with caution for instance the bmi could be used to change horizontal hydraulic conductivity k11 inside the node property flow npf package after the initialization phase in an attempt to simulate time varying hydraulic properties however the saturated conductance condsat is used to calculate terms in the coefficient matrix and although it depends on k11 it is calculated only once at the beginning of the simulation during initialize therefore modifying k11 after initialization would have no effect on the simulation results other parameters such as those related to the dimensionality of the coefficient matrices or to the time discretization should probably not be modified to avoid undefined behavior and possible program failure section 3 contains multiple validated use cases of reading and writing data using the modflow api procedures in general the user should consult the source code to confirm the validity of a specific application the implementation of the interface functions follows the csdms fortran 2003 bmi specification piper 2020 however with the goal of developing a universally accessible library in mind small modifications had to be made mostly avoiding the use of fortran specific data types the resulting api is ensured to have standard c kernighan and ritchie 1988 interoperability 2 2 development of the extended model interface bmi only allows a sequential loose coupling of process models to enable a tighter coupling at the iteration level but also to provide more fine grained control of the simulation we have developed the extended model interface xmi which consists of all bmi functionality plus the necessary extensions a first extension is the subdivision of the update function into smaller units to allow the interactive modification of time varying data these data are read from file for each time step in a dedicated read and prepare rp step as shown in fig 3 in the xmi function prepare time step the data are read and before an external call to do time step is made they can still be changed using the bmi functions a second extension is the explicit exposure of the non linear convergence loop of a numerical solution this is shown schematically in fig 4 with such fine grained control it is now possible to tightly couple modflow to other model components and have the solution converge at the outer iteration level i e within a single time step this is often preferable to a loosely coupled sequential alternative where one model finishes a time step and only then provides data that is used as boundary conditions for the other model the latter can cause the solution to oscillate in time and result in a simulation that is sensitive to the order in which the components are executed the value of having this extra functionality in the modflow api is clearly demonstrated by the fact that most of the example applications presented in section 3 require this tight coupling scheme it is important to realize that the flow chart shown in fig 3 corresponds to a simulation with a single numerical solution although this might be the most frequent use case modflow accommodates a simulation with multiple solutions for example when running groundwater flow and transport simultaneously to replicate the behavior of do time step in this case an enveloping loop running over the total number of solutions should be implemented in the calling program or script one of the key benefits of having the xmi available as an standard c compliant library is the capability to control the simulation from programming environments other than fortran this design makes the modflow api straightforward to integrate into programs written in for example c python java and c net or to run simulations interactively from a jupyter notebook pérez and granger 2007 kluyver et al 2016 because python is such a widespread and highly valued scripting language the xmipy russcher et al 2020 and modflowapi hughes et al 2021 packages have been developed to facilitate use of the modflow api the xmipy package is a key part of the interoperable layer shown in fig 1 as it contains the complete set of python bindings for the native bmi and xmi functions in modflow it fully encapsulates the complexity of type marshaling between python and fortran and the memory management of the data which allows users to work with comprehensible functions and standard python data types the modflowapi package extends the xmipy package and includes the python binding for the get var address memory manager convenience function in the future it will be extended with additional functionality that is made available through the modflow api 3 example applications we present a few examples below that demonstrate use of the modflow api the first example simulates evapotranspiration using different approaches and represents an example of how the api can be used to rapidly prototype new modflow functionality the second example shows how the modflow api can be used to optimize groundwater pumpage with standard optimization methods available in the python scipy package virtanen et al 2020 the third example is a tight coupling with metaswap van walsum and groenendijk 2008 van walsum and veldhuizen 2011 to simulate unsaturated zone flow processes and groundwater recharge the fourth example demonstrates the sequential loose coupling of prms using only bmi functionality in the modflow api and includes mapping data from non rectangular prms control volumes to grid based modflow control volumes the fifth example demonstrates use of the modflow api in the river operations model modsim labadie et al 2000 which is coded in c to optimize irrigation diversions represented with the modflow streamflow routing sfr package to meet prior appropriation constraints jupyter notebooks are available for all of the examples and include further details on the model setup the coupling and the software required to reproduce the examples see section 4 3 1 development of a custom modflow package using python modflow has simplified equations for dynamically calculating evapotranspiration as a function of the simulated water table depth this example shows how the api can be used to implement alternative evapotranspiration functions without modifying the modflow source code a python routine was written to simulate evapotranspiration that varies from a maximum value at or above defined elevation and decays to zero at a defined extinction depth using constant linear and exponential functions fig 5 for this simple example the modflow model has 10 layers 1 row and 1 column a constant grid spacing of 10 m was used in the row and column directions the top and bottom were set to 0 and 5 m respectively and the layers were discretized using a constant 0 5 m thickness a constant horizontal conductivity of 1 m d 1 specific storage of 1 5 10 5 m 1 specific yield of 0 2 and initial head of 1 m above land surface were specified in each cell the model was transient and ran for 1000 days using 1000 variable length time steps and a time step multiplier of 1 05 time step lengths ranged from 3 234 10 20 d at the beginning of the simulation and increased to 47 619 d at the end of the simulation to improve model convergence the newton raphson formulation was used a maximum evapotranspiration rate q max of 6 10 4 m d 1 an evapotranspiration surface elevation of 0 25 m depth of 0 25 m below land surface and a evapotranspiration extinction elevation of 3 m depth of 3 m below land surface were specified to simulate evapotranspiration in the model fig 5 the linear evapotranspiration function is identical to the modflow evapotranspiration evt package langevin et al 2017 the constant and exponential evapotranspiration functions are not directly available in the evt package the constant evapotranspiration function was implemented in python using 1 q e t q m a x h z e 0 h z e where q et is the volumetric evapotranspiration rate m3 d 1 q max is the maximum volumetric evapotranspiration rate m3 d 1 h is the simulated groundwater head m and z e is the evapotranspiration extinction elevation m the volumetric evapotranspiration rate q max is the product of the horizontal cell area a and q max the linear evapotranspiration function was implemented in python using 2 q e t q m a x h z s q m a x 1 d z s h z e 0 h z e where z s is the evapotranspiration surface elevation m d is the depth of the simulated head below z s normalized by the thickness of the interval over which q et varies from q max to 0 unitless d is calculated as 3 d z s h z s z e the normalized depth varies from 0 at z s to 1 at z e the exponential evapotranspiration function was implemented in python using 4 q e t q m a x h z s q m a x e 3 π d z s h z e 0 h z e the newton raphson method was used to linearize equations 1 2 and 4 and calculate the hcof and rhs variables for the api package which was used to represent evapotranspiration in the model the api package was developed to provide a generic way to add terms to the modflow system of equations when using the modflow api the modflow api for the evapotranspiration example is accessed in python by first instantiating a modflowapi object mf6 which is imported from modflowapi using image 1 the mf6 object has access to all of the methods exposed in the modflow api next the base modflow model is initialized from the mfsim nam file in the working directory using image 2 program memory is allocated and static data are read when initialize is called the current time 0 d when called right after initialize and simulation time at the end of the simulation are used to control the modflow time step loop and are determined using image 3 the maximum number of non linear iterations specified in the iterative model solution ims are determined using image 4 an array pointing to simulated heads for the evapotranspiration model is set using image 5 in this statement head is the name of the python variable of type numpy ndarray a data type in the python numpy package harris et al 2020 that points to the head array in modflow and gwfname is the user specified name of the groundwater flow model which is etmodel in this example the evapotranspiration variables hcof and rhs and the model layer containing the water table are both calculated using the values in head the variable addresses to access the data in the api package are image 6 node addr hcof addr and rhs addr are used in the set value method to modify the cell evapotranspiration is extracted from and the coefficient matrix and right hand side terms used to represent evapotranspiration in the system of equations respectively node numbers are calculated from user specified cell ids which are layer row column in this example and range from 1 to the number of cells in the groundwater flow model the python code shown in fig 6 determines the node evapotranspiration is extracted from calculates the evapotranspiration terms and solves the gwf model for a time step until convergence is achieved and then proceeds to the next time step until the entire simulation is complete the get node function returns the one based model node number for the cell containing the water table the et terms function returns the hcof and rhs variables that are used to calculate the current head dependent volumetric evapotranspiration rate q et based on equation 1 2 or 4 the hcof and rhs variables are added by modflow to the diagonal of the coefficient matrix and the right hand side respectively when the system of equations are formulated for the current outer iteration of a time step for more information see hughes et al 2017 langevin et al 2017 simulated results for the three evapotranspiration functions are shown in fig 7 simulated water levels decline from the initial value of 1 m for all three evapotranspiration functions fig 7 a the cumulative evapotranspiration for all three functions is shown in fig 7 b as expected the cumulative evapotranspiration is highest for the constant evapotranspiration function 60 0 m3 and results in the lowest water level 3 0 m at the end of the simulation the cumulative evapotranspiration is smallest for the exponential evapotranspiration function 18 6 m3 and results in the highest water level 0 9 m at the end of the simulation the cumulative evapotranspiration and water level for the linear evapotranspiration function were 39 5 m3 and 2 0 m respectively and are identical to model results using the evt package neither the constant nor the exponential evapotranspiration functions could be exactly represented using the standard evt package the modflow api allows a user to implement these functions without having to modify the modflow program 3 2 optimization of groundwater withdrawals groundwater models are often used within an optimization context to maximize groundwater withdrawals subject to drawdown constraints or to optimize mitigation strategies for contaminant plume containment for example optimization strategies require many forward runs with the groundwater model to calculate response coefficients and recalculate them as necessary for non linear problems the modflow api allows a single model instance to be solved as many times as necessary without reloading the model from files each model solution can correspond to altered model variables such as pumping rates this approach is efficient for optimization because the overhead associated with loading a model can be restricted to just the very first model solution the modflow api was used to optimize groundwater pumping rates for a hypothetical steady state groundwater flow model patterned after the example described by hill et al 1998 the original model was based on a regular grid for the application here however a 5 layer triangular mesh with 20 400 cells per layer was created to increase spatial resolution around the shoreline of the lake along the river and around three groundwater pumping wells fig 8 for this simple example the lake is represented as a specified head boundary in layer 1 the river is specified as a head dependent flow boundary in layer 1 and the wells are represented as specified flows in layer 5 net recharge is uniformly applied to the top of the model the goal of the optimization example is to determine the maximum pumping rate for each well subject to a minimum allowable head constraint for each well as shown in table 2 pumping rates cannot be optimized individually for each well because the effect of pumping at one well has an effect on the simulated head at the other wells specialized versions of modflow have been written to solve these types of optimization problems for example modflow gwm couples an earlier version of modflow harbaugh 2005 with optimization routines into a single program ahlfeld et al 2005 modflow gwm has not been kept up to date with new modflow versions and therefore it cannot be used with new features in modflow such as support for unstructured grids for example alternatively the model agnostic pestpp opt software white et al 2018 could be used for this problem but without the computational advantage of repeatedly solving the flow model without having to initialize the model from input files the modflow api allows any general optimization routine such as the simplex method available in the python scipy package to be used with a groundwater model as described by ahlfeld et al 2005 the simplex method can be used to solve the following linearized form of the optimization equations 5 m i n i m i z e z c t x s u b j e c t t o a x b w h e r e 0 x u where z is the value of the objective function c t is a transposed column vector of coefficients associated with the decision variables x is a column vector of pumping rates a is the response coefficients calculated here through perturbation of groundwater withdrawal rates and forward runs with the modflow model through the run model function fig 9 b is the column vector of constraints equal to the head at each well cell without pumping minus the head at the well cell with pumping and u is a vector containing the maximum pumping rate for each well for non linear problems in which the response coefficient matrix depends on the rates of groundwater withdrawals the optimization problem can be solved repeatedly until some acceptable solution tolerance is achieved the run model function shown in fig 9 is an important part of the optimization routine implemented to demonstrate the modflow api in this function the mf6 object is of type modflowapi and interfaces between python and the initialized and running modflow model well pumping rates stored in the wel package bound array are accessed through a pointer wrapped inside a numpy ndarray which is used by run model to change the pumping rates the maximum number of modflow iterations is also accessed and used as part of a loop to make repeated calls to mf6 solve until convergence is achieved finally the solution vector containing simulated groundwater head for the provided well pumping rates wellq are accessed as a pointer and returned to the calling program for those applications tasked with running a model repeatedly the run model function is intuitively appealing because it shows how a python script or any other program can send a running modflow instance one or more new parameter values and receive back the resulting heads and this can all be done through memory without any file access the python code used to solve this optimization problem formulates the terms shown in equation 5 and then uses the revised simplex method in scipy optimize to solve for the well withdrawal rates using image 7 for this problem the maximum pumping rates were determined to be 44 975 33 656 and 90 191 m3d 1 for wells a b and c respectively although this example problem used simple head constraints the scripting approach presented here is highly customizable and could be used with many other types of constraints such as baseflow to the river minimum head within a model subregion and so forth likewise pumping rates are used as the decision variable here however there are many other model decision variables that could be optimized in order to meet problem constraints 3 3 coupling modflow to metaswap accurate simulation of the flow of water in the unsaturated zone is important in many water quality and quantity studies in most cases the preferred approach involves numerical solution of the richards equation which is computationally intensive and challenging in terms of robustness see farthing and ogden 2017 for a recent overview on application of the richards equation to unsaturated flow problems the unsaturated zone flow uzf package which solves a simplified form of richards equation based on kinematic waves is available in modflow however this approximation affects the ability to simulate capillary effects and the accuracy of simulated soil moisture dynamics these dynamics are particularly important in lowland regions such as the netherlands where the groundwater levels are within 2 m of the soil surface in most of the country as a result the current national scale hydrological model of the netherlands de lange et al 2014 relies on a dedicated coupling between a previous customized version of modflow vermeulen et al 2020 and metaswap a quasi steady state simulation of the unsaturated zone based on richards equation van walsum and groenendijk 2008 a new implementation of this model is being developed and will now build on the modflow api avoiding the need to develop a proprietary version of modflow as was done in the past this example presents a hypothetical model with characteristics common to hydrologic conditions in a large part of the netherlands the software driving this example is the python package imod coupler which uses xmipy to control both modflow and metaswap components using their xmi enabled shared libraries in short after every solution of the groundwater heads within the non linear convergence loop see section 2 2 metaswap determines the unsaturated zone flux and the associated primary storage coefficients while at the same time ensuring mass balance for the shared control volume both variables groundwater recharge and storage coefficients are then communicated to modflow and this sequence is repeated until the modflow convergence criteria are met see van walsum and veldhuizen 2011 for more detail on the shared control volume approach used to couple modflow and metaswap the groundwater model consists of a rectangular grid consisting of 3 layers and 81 9 9 cells per layer covering an area of 8100 m2 with the soil surface located at an elevation of 0 0 m a general head boundary condition ghb is assigned to the outer columns and the active cells in the top layer are all coupled to a corresponding cell in metaswap the simulation is run for an exceptionally dry year 2018 using daily stress periods and 1 time step per stress period precipitation and reference evapotranspiration makkink 1957 both forcing data for metaswap are from the dutch national weather service de bilt station the land use type in the model domain is agricultural and the crop type is potatoes which have a relatively large seasonal transpiration rate and require groundwater irrigation the metaswap irrigation process is enabled and extracts water from model layer 3 of the modflow model component using the wel package the soil type for the metaswap model component is specified as peat which is type number 1 in the metaswap soil database precipitation and actual evapotranspiration et act are shown in fig 10 a actual evapotranspiration is greatest during the growing season with peaks appearing at weekly intervals resulting from increased soil water evaporation during the sprinkler irrigation which is modeled to occur on weekly basis the simulated recharge from the unsaturated zone to the groundwater model q rch for the centermost cell in the top layer is shown in fig 10b the inset in fig 10b shows how the system transitions from the unsaturated zone being a source of water for the groundwater system to a sink for the groundwater system as a result of capillary rise during the dry summer period simulated groundwater head in the centermost cell in the top layer is shown in fig 10c and shows the response of the water table to groundwater recharge q rch irrigation effects are not noticeable in fig 10c because the well extracts water from model layer 3 although this example is hypothetical and oversimplified it does illustrate how the modflow api enables seamless coupling of two existing component models without needing to modify the source code of either component model in this example metaswap makes it possible to simulate the unsaturated zone in more detail than possible with the modflow uzf package and to simulate groundwater irrigation that is a function of soil moisture the explicit control of the outer iteration loop a specific feature of xmi is required for this example and can serve as a blueprint for other applications that require tight coupling with modflow 3 4 coupling modflow to prms the modflow api and bmi for prms were used to simulate integrated surface water and groundwater processes in the sagehen creek watershed located on the east slope of the northern sierra nevada mountains in california the sagehen creek application described here is based on the gsflow model of the same area markstrom et al 2008 some key differences between this sagehen creek application and the gsflow sagehen creek model application include 1 128 watershed based hydrologic response units hrus were used instead of cell based hrus 2 the prms cascade module was not used to route surface runoff and interflow from upslope hrus to downslope hrus and 3 the prms soil component is only solved once per time step instead of being recalculated each modflow outer non linear iteration as is done in gsflow as part of a separate effort the bmi was recently implemented in prms with the program being split into separate surface soil groundwater and streamflow domain components the surface and soil domain components were the only prms domain components used in this example the prms surface domain component was used to simulate processes above the soil surface including 1 rain and snow 2 potential evapotranspiration 3 snow sublimation 4 canopy interception storage evaporation and throughfall and 5 surface runoff the prms soil domain component was used to simulate storage inflow and outflow within the soil zone reservoir soil zone inflows include infiltration and outflows include 1 evapotranspiration 2 interflow and preferential flow and 3 groundwater recharge the modflow uzf package was used to simulate vertical unsaturated groundwater flow below the prms soil domain component soil zone and subsurface reservoirs the modflow sfr package was used to simulate streamflow in a total of 201 connected reaches the modflow drain drn package was used to simulate groundwater seepage to land surface in areas where the groundwater levels exceed land surface prms hrus and parameters are from markstrom et al 2006 the hrus simulated in this model application are shown in fig 11 modflow parameters are from gsflow sagehen creek model application the groundwater domain component has a total of 2 layers 73 rows and 81 columns and was discretized using a constant grid cell size of 90 m in the row and column directions the active model domain is restricted to the lateral extent of the sagehen creek watershed with a total of 3392 active cells per layer and covering a 27 475 200 m2 area uzf package cells were included in active cells in both model layers and in cases where the water level in cells in model layer 1 is below the bottom of the cell unsaturated zone flow at the bottom of the cell is routed to the underlying uzf cell in layer 2 lateral groundwater discharge out of the watershed was simulated using the time varying constant head chd package and constant head cells at the downstream end of the watershed a constant head of 1915 m and 1900 m was specified in 3 cells in model layer 1 and 3 cells in model layer 2 respectively see markstrom et al 2006 and markstrom et al 2008 for additional information on prms and modflow parameters modflow and prms were sequentially coupled during a time step by first running the update functions for the prms surface and soil domain components extracting results from prms and mapping prms hru results to modflow grid cells and finally running the modflow update function groundwater recharge calculated by the prms soil domain component for the soil zone and subsurface reservoirs for each of the 128 hrus was applied as infiltration to 3386 underlying uzf package cells in model layer 1 unsatisfied potential evapotranspiration calculated as the difference between the potential evapotranspiration calculated in the prms surface domain component and the actual evapotranspiration calculated by the prms surface and soil domain components was applied as potential evapotranspiration to underlying the uzf cells in model layer 1 the hrus were intersected with the modflow grid to calculate the area based hru uzf weights used to map prms results to uzf cells the hrus were also intersected with the stream network to map prms runoff and interflow from 128 hrus as runoff to 201 sfr reaches equal weighting rather than reach length based weighting was applied to calculate hru sfr weights in addition to mapping prms hru data to modflow uzf cells and sfr reaches unit conversions were made to convert prms inch and acre units to modflow m and m2 units the modflow mover mvr package was used to route rejected infiltration calculated by the uzf package and groundwater seepage to the surface calculated by the drn package to the reaches in the sfr package the same approach used to develop the hru sfr weights was used to connect uzf cells to sfr reaches using the mvr package uzf cells in model layer 2 were also routed to the same sfr reach as uzf cells in model layer 1 using the mvr package to route excess unsaturated zone flow from the overlying uzf cell rejected infiltration to the surface water network the model application simulation period extends from october 1 1980 through september 30 1996 and included 5844 daily time steps the sagehen creek watershed model application used daily precipitation and air temperature data from the gsflow sagehen creek model application precipitation in the sagehen creek watershed is highly variable in form and intensity and generally increases with altitude initial groundwater levels for the simulation were based on steady state heads calculated by a stand alone modflow model with specified uzf package infiltration rates from the gsflow sagehen creek model application and an initial moisture content of 0 08 in each uzf cell although model application parameters are based on previous sagehen creek model applications this model application is considered uncalibrated because of the previously stated differences from the gsflow sagehen creek model application significant storage changes were observed in the unsaturated and saturated zones in modflow results during the first two years of the simulation as a result the period from october 1980 to october 1982 is considered a warm up period and has been excluded from model result analyses the average infiltration which is calculated as the difference between the groundwater recharge calculated by the prms soil domain component and rejected infiltration calculated by the uzf package is shown in fig 11a rejected infiltration occurs near the surface water network and is shown in fig 11a as areas where average infiltration rates are different from the groundwater recharge rate calculated by prms for a hru average groundwater recharge rates exceed average infiltration rates near topographic divides for several hrus and in cells with elevations much higher than surrounding cells as a result of drainage of initial soil moisture in the unsaturated zone simulated and observed streamflow is shown in fig 12 a the model application over simulates streamflow especially during high flow events the contribution of runoff interflow groundwater seepage and baseflow to streamflow is shown in 12b interflow was the largest contributor to streamflow at the end of the evaluation period followed by baseflow groundwater seepage and runoff the contribution of prms unsaturated zone and saturated zone evapotranspiration to the total evapotranspiration is shown in 12c evapotranspiration from the surface and soil prms component accounted for more the 99 of the total evapotranspiration 3 5 coupling modflow to modsim three recent publications showcased the integration of a river operation model with a physically based distributed parameter hydrologic model brookfield et al 2017 dogrul et al 2016 morway et al 2016 in all three integrations reservoir release and ditch diversion s as calculated by the river reservoir operation model override values within the hydrologic model and specified by a user prior to running the model the groundwater surface water exchange rates resulting from those operational decisions i e releases and diversions are then re calculated by the hydrologic model for updating the appropriate values within the operations model in summary the respective specialty of each model complements a known weakness of the other resulting in a modeling platform that better equips water resource managers to conjunctively manage groundwater and surface water as a single resource winter et al 1998 unlike the previous river operation and hydrologic model integration efforts no customization of the hydrologic model source code was necessary for this integration instead the selected river operation model modsim labadie et al 2000 repeatedly calls modflow as needed unlike the other included examples modsim uses the microsoft net platform to run the initialize and finalize functions shown in fig 2 the prepare time step and finalize time step functions shown in fig 3 and the prepare solve solve and finalize solve functions shown in fig 4 as necessary the only new code that was written to establish communication between modsim and modflow was through the custom code interface provided with the standard distribution of modsim use of the modflow api with a river operations model is demonstrated with a hypothetical model that first appeared in morway et al 2016 and is shown in schematic form in fig 13 the model is patterned after an irrigated river valley governed by the prior appropriation doctrine i e first in time first in right for this example the net streamflow accretions and depletions as calculated by modflow along every simulated stream reach are added to or subtracted from the corresponding modsim link with this information modsim recalculates the reservoir release and diversion rates based on the updated groundwater and surface water exchanges next the modflow time step is rerun in order to update the accretions and depletions associated with the latest modsim operational decisions the two models continue iterating within a time step until all the changes in shared values reservoir releases diversions and surface water groundwater interactions satisfy the convergence criteria in this way the respective hydrologic and river operation solutions synchronize in time and space before moving on to the next time step an important advancement in simulating conjunctive use systems the impact of integrating modsim with modflow as opposed to running modflow by itself is shown in fig 14 for an abbreviated simulation period of one year reservoir storage is drawn down and delivered to specific ditches that own the stored water fig 14a for this example the three junior ditches d1 d2 and d3 fig 13 are assigned storage accounts that provide water during the summer months when natural flows as opposed to stored water generally taper off the first storage account to run out of water is d1 and without access to other sources of water the diversion amounts are reduced to zero by modsim without any further input adjustments by the user fig 14b in the modflow only run however total monthly diversions remain inappropriately high in d1 throughout the year by virtue of its upstream location within the system on its own modflow cannot simulate prior appropriations within the surface water irrigation system and is therefore limited to a simplified set of rules available with the sfr package langevin et al 2017 for determining diversion rates that may or may not reflect management practices d2 shows relatively good agreement between modsim modflow and modflow only total monthly diversion amounts owing to the fact that its reservoir storage persists throughout the relatively short one year simulation period fig 14c however differences between the two simulations similar to fig 14b would occur were d2 to run out of storage water fig 14d and e further highlight the difficulty in simulating river operations without a tool like modsim informing river diversions recognizing that the next downstream diversion d4 fig 13 has a senior water right an appropriate rule was selected from the four rules available within the sfr package that attempts to allow enough water by to satisfy the senior downstream water right however in so doing no water is diverted by d3 even though it should have access to stored water that was released from the reservoir integration with modsim through the bmi correctly diverts the water released from the d3 storage account into the d3 ditch finally because natural flows are the only source of water available to d4 modsim diverts a full allotment for d4 during the spring snowmelt season commonly april june fig 14e but reduces diversions in the other months although this is hypothetical example it highlights the value of the modflow api without it it would be impossible to properly simulate the surface water operations and in connected stream aquifer systems if the surface hydrology is wrong then accurate simulation of the groundwater system remains unattained 4 software and data availability the modflow api xmi xmipy and the modflowapi see fig 1 were collaboratively developed by the u s geological survey and deltares modflow is available from the u s geologic survey at https www usgs gov software modflow 6 usgs modular hydrologic model and as an open source code repository from https github com modflow usgs modflow6 the xmipy and modflowapi python packages are available as open source code repositories from https github com deltares xmipy and https github com modflow usgs modflowapi respectively and both can be installed from pypi the prms soil and surface components with the basic model interface are available from the csdms stack conda channel or from pypi jupyter notebooks and model application datasets that are not built within the notebook using flopy bakker et al 2016 are available as a release asset from https github com modflow usgs modflowapi releases tag esmdatasets each jupyter notebook includes software installation instructions and use limitations several additional jupyter notebooks that demonstrate use of the modflowapi python package are available in the modflowapi repository 5 discussion and conclusions the modflow api was developed to allow external programs control of a modflow simulation and access to internal variables while the simulation is running these external programs may consist of another physical process model to be coupled with modflow a machine learning model representing a physical process or a python script that needs control and access of modflow while it is running the modflow api is based on the established bmi convention for software interoperability there are many benefits to be gained from the modflow api as demonstrated in the five examples and described in the following list the modflow api provides a sustainable way to design and maintain software that relies on modflow as a component software designed in this manner can take advantage of new modflow features without having to modify and update the source code the modflow api allows other process models to be coupled with modflow this coupling can be performed sequentially for each time step or the models can be tightly coupled at the matrix solution level using the xmi extension to the bmi this tight coupling approach may allow for larger time steps and more accurate mass conserved solutions for complex problems users have new flexibility to make custom modflow packages and prototype new capabilities without having to compile modflow or make any changes to the underlying source code these new user packages can be developed with popular languages such as python which are familiar and easy to use there are many different modflow variants including those that represent parameter estimation adjoint state calculations farm processes and chemical reactions for example including all possible relevant processes into modflow is not sustainable or practical the api allows the modflow program to remain focused on its intended scope as a hydrologic simulator while enabling extension to other domains as necessary modflow reads and writes text files and binary files that are well documented but not familiar to scientists in other domains through the data access routines afforded by the modflow api users can inject data read from alternative files such as hdf and netcdf files similarly model output could also be written to alternative file formats at runtime software capabilities are rapidly evolving and being made available to scientists and engineers in the form of new packages and modules for example sophisticated machine learning tools are being developed and released faster than they could be implemented in modflow the modflow api allows users to innovate with new software tools that may not be accessible otherwise modflow is supported by several commercial graphical user interfaces guis that allow users to develop run and post process groundwater models the modflow api provides a new way for these guis to run a simulation monitor its progress even at the iteration level and provide instantaneous feedback to the user this paper describes the initial release of the modflow api and there are several limitations to mention although the api allows the user to interact with time steps individually they must be executed sequentially from the first to the last time step i e a time step that has been finalized using update or finalize time step can not be rerun development of restart capability for modflow is a high priority but being able to rerun a time step and correctly synchronize file based input and output during run time is challenging another limitation is the current version of the modflow api requires that the user has intimate knowledge of the underlying modflow program variables and order of operations for coupling another model with modflow the user must be familiar with both programs have a firm understanding of variable names which variables to exchange when to exchange information the scientific units used in the models and so forth work is ongoing to better document the internal functioning of modflow in order to make it easier to use the modflow api for example it may be possible to use standard names for common variables as suggested by peckham et al 2013 and peckham 2014 and allow users to access data using these standard names there is also ongoing work to allow model simulations to be created in memory rather than being created from input files as is required in the current version these limitations will continue to be addressed by the modflow development team as the modflow api is used in practice and the latest developments will be available at the urls listed in section 4 declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements the authors would like to acknowledge mark piper and eric hutton from csdms for their help in implementing the bmi standard in prms and for discussions during the development of the modflow api the authors would also like to acknowledge michael fienen from the u s geological survey for early discussions about the development of the modflow api and for his review of the manuscript laura schachter from the u s geological survey for her review of the manuscript joeri van engelen from deltares for his review of the manuscript and four anonymous reviewers at the journal any use of trade firm or product names is for descriptive purposes only and does not imply endorsement by the u s government 
25663,the modflow api allows other programs to control modflow and interactively change variables without having to modify the source code the modflow api is based on the basic model interface bmi which is a set of conventions that define how to initialize a simulation update the model state by advancing in time and finalize the run for many existing modflow coupling applications the information provided to modflow must be updated multiple times in a time step as this capability to modify variables within a time step is not defined by the bmi an extension to bmi was developed this extended model interface is part of the modflow api and allows such a tight coupling to other models examples are included for a variety of use cases including new flexibility for users to develop custom packages without modifying the modflow source code and coupling modflow with other models and optimization libraries abbreviations api application programming interface bmi basic model interface cca common component architecture csdms community surface dynamics modeling system esmf earth system modeling framework evt evapotranspiration gwf groundwater flow gwt groundwater transport hru hydrologic response unit openmi open modeling interface prms precipitation runoff modeling system sfr streamflow routing uzf unsaturated zone flow xmi extended model interface keywords modflow 6 modflow api basic model interface metaswap prms modsim 1 introduction for over 30 years the modflow program has been widely used by academics private consultants and government scientists to accurately reliably and efficiently simulate groundwater flow and related processes due to its widespread popularity modular structure and thorough documentation modflow has been successfully coupled with many other physical process models and programs for example modflow has been coupled with watershed models including prms markstrom et al 2008 swat kim et al 2008 and hspf davis 2001 modflow has been combined with solute transport models including moc3d winston et al 2018 and mt3d langevin et al 2008 optimization routines have been coupled with modflow to maximize groundwater extraction subject to various constraints ahlfeld et al 2005 and to identify optimum surface water deliveries in order to meet water demands morway et al 2016 modflow has been extended to include pipe flow models to represent karst conditions shoemaker et al 2008 and flow in the unsaturated zone twarakavi et al 2008 modflow has also been coupled to hydrodynamic surface water models including hec ras rodriguez et al 2008 swift2d wang et al 2007 and branch swain and wexler 1996 the majority of previous coupling approaches used with modflow have been to combine the codes into a single program this single program coupling approach has often led to coupled programs that could not keep up with advances of the individual codes as a result many of these custom modflow variants have become stale over time and do not support recent modflow advances or advances to the other process model efforts to couple modflow with other process models requires extensive knowledge of both models an understanding of the types of information needed for the coupling performance objectives and a clear purpose for the resulting coupled modflow program these broader model integration issues are described by belete et al 2017 in a comprehensive overview the overview outlines phases of the model integration effort summarizes capabilities of existing frameworks for coupling models and describes the challenges of designing a generalized integration framework overcoming these challenges can allow for new and innovative ways for coupling models including sharing models and data over the web chen et al 2020 modflow6 is the most recent version of modflow and is currently the core version of modflow distributed and released by the u s geological survey modflow6 is an object oriented program and framework written in fortran the modflow6 software provides a platform for supporting multiple models and multiple types of models within the same simulation langevin et al 2017 hughes et al 2017 these models can be independent of one another with no interaction they can exchange information or they can be tightly coupled at the matrix level by adding them to the same numerical solution transfer of information between models is isolated to exchange objects which contain the data and code needed to couple models this design feature allow models to be developed maintained and used independently without having custom information about other models in the framework within this new framework a regional scale groundwater model may be coupled with multiple local scale groundwater models or a groundwater transport model may be coupled with a groundwater flow model langevin et al 2020 modflow6 currently includes the groundwater flow gwf model and the groundwater transport gwt model each with packages to represent surface water processes groundwater extraction external boundaries mass sources and sinks and mass sorption and reactions morway et al 2021 show how the advanced packages in modflow 6 can be connected to represent watershed processes in managed basins modflow6 also includes advanced capabilities to simulate three dimensional anisotropy and dispersion and correct grid errors for cell connections that violate generalized control volume finite difference assumptions panday et al 2013 provost et al 2017 to facilitate coupling with other models including those written in another language an application programming interface api was developed for modflow the api was developed using the established basic model interface bmi standard the bmi was developed by community surface dynamics modeling system csdms to provide a standard component based interface for earth science models peckham et al 2013 the common component architecture cca armstrong et al 1999 the earth system modeling framework esmf collins et al 2005 and the open modeling interface openmi gregersen et al 2007 are examples of other coupling standards the bmi standard was selected for the modflow api because it was developed specifically for earth science models and it was clear how the bmi could be used to couple modflow with the types of process models needed by the community some example applications of the bmi with earth science models include the coupling of 1 a river sediment transport model to a delta model that distributes the sediment ratliff et al 2018 and 2 a hydrologic model to a hydrodynamic model to improve flood inundation simulations hoch et al 2019 since its initial release development of the bmi has continued and updates have included functions for accessing variable metadata and for working with structured and unstructured grids hutton et al 2020 although the bmi provides the foundation for the modflow api additional functionality was required to allow for a tighter coupling with other process models than is possible with the standard bmi functions specifically there was a need to allow other components to be coupled with modflow within the non linear picard iteration loop see for example ferziger and peric 1996 for a treatment of this type of coupling and the picard iteration for example gsflow markstrom et al 2008 is a coupled version of modflow and prms markstrom et al 2015 a key feature of gsflow is the tight coupling of the prms soil zone and overland flow components with the modflow groundwater flow component it is implemented in gsflow by allowing the soil zone component to be calculated as part of the modflow picard iteration until convergence is achieved in the modflow api the bmi was extended to support this same type of tight component coupling that was used in gsflow but in a generic way that allows other models to be solved simultaneously with modflow the purpose of this paper is to describe the new modflow api which is implemented in modflow6 subsequent modflow references in this paper refer specifically to the modflow6 version although the focus of the paper is on coupling modflow with other process models the api is general and will allow modflow to be called by a wide variety of software programs such as by plotting programs or geographical information systems for example the paper first shows how the bmi was implemented in modflow including an explanation of how a modflow simulation can be controlled by an external program written in another language e g python c etc using model control and time functions and the way modflow variables can be accessed during the simulation the paper then describes extensions to the bmi that allow modflow to be tightly coupled with other process models finally five examples are presented to demonstrate a variety of uses for the modflow api including new flexibility for users to develop custom packages without modifying the modflow source code and coupling modflow with other process models and optimization libraries 2 development of the modflow api development of the modflow api required refactoring the source code into the core component this core component can be compiled into an executable program fig 1 which is provided as part of the standard distribution released by the u s geological survey the core component can also be compiled with the api routines into a shared library that can be used for interoperable applications the refactoring effort focused on ensuring that the routines and calls for the executable corresponded exactly to calls made to the shared library through the api and that none of the routines were duplicated extensive testing was performed and continues to be performed through continuous integration practices to ensure that the executable and library versions give identical numerical results and have equivalent run times with multiple compilers on the supported operating systems windows linux and macos as shown in fig 1 the api layer builds on the modflow core modflow is considered to be either the executable version or the shared library version as the latter can be initiated and run in a manner that gives the same results as the executable fig 1 shows three different implementations of an interoperable layer that sits between the modflow api and applications based on it an interoperable layer such as the modflowapi python package makes it easier to control modflow and transfer information the purpose of this section is to describe the api layer shown in fig 1 including both the bmi and the extended model interface xmi the subsequent section contains examples on how to use the modflow api in practice as shown in fig 1 the api layer builds on the modflow core it currently contains the extended model interface xmi which is the basic model interface bmi and the extensions described below plus supporting functionality however it is expected that it will continue to grow exposing more internal modflow functionality for use in other applications modflow is considered to be either the executable version or the shared library version as the latter can be initiated and run in a manner that gives the same results as the executable fig 1 shows three different implementations of an interoperable layer that sits between the modflow api and applications based on it an interoperable layer such as the modflowapi python package makes it easier to control modflow and transfer information the purpose of this section is to describe the api layer shown in fig 1 including both the bmi and the extended model interface xmi the subsequent section contains examples on how to use the modflow api in practice 2 1 implementation of the basic model interface in order to expose the modflow functionality with the main bmi control functions initialize update finalize the source code has been refactored a high level api is developed to aggregate the traditional modflow subroutines into functional units that can be mapped directly to the bmi initialize update and finalize functions as shown by the diagram in fig 2 the main reason for the code refactoring is the requirement that the same program code is executed regardless of whether the simulation is run with the executable or with the shared library containing the newly developed bmi as implied by the control flow in fig 2 the caller of the bmi control functions is also responsible for implementing the time step loop the simulation time inside modflow is divided into stress periods which are subdivided into time steps stress periods are intervals during which external stresses can be redefined and are provided as a convenience for users the time steps constitute the potentially non equidistant discretization of time used to solve the numerical models in the simulation note that as opposed to previous versions of the code there is no explicit loop over stress periods anymore in modflow which complies with the interpretation of the time loop as prescribed by the bmi a call to the update function advances the simulation by a single time step the start time of the simulation get start time is fixed at 0 0 for all simulations the end time get end time is set to the total simulation time which is equal to the summed lengths of the individual time steps the current time can be queried with a call to get current time but it is important to realize that its value is not updated until the internal time update tu is called the current time step length can be queried with a call to get time step the other part of the bmi implementation is its ability to access internal model component data nearly all array and scalar variables in modflow are declared as fortran pointers metcalf et al 2018 and managed by a dedicated module called the memory manager which allocates variables and tags them with a unique address string composed of the variable s name and a memory path identifying the unique name of the modflow component type and for modflow package variables the unique name of the package it belongs to examples of the address string for a number of characteristic variables is shown in table 1 modflow component types include timing solution model exchange and utility components see hughes et al 2017 for more information for user convenience and to assure future compatibility with the library the get var address utility function is available to construct this string based on the syntax used internally by the memory manager an inventory of all accessible variables can be retrieved with the bmi function get input var names or get output var names as no distinction has been made between input or output variables in the modflow api or by setting memory print option to all in the options block of the simulation namefile although in principle it is possible to modify all variables inside the memory manager this should be used with caution for instance the bmi could be used to change horizontal hydraulic conductivity k11 inside the node property flow npf package after the initialization phase in an attempt to simulate time varying hydraulic properties however the saturated conductance condsat is used to calculate terms in the coefficient matrix and although it depends on k11 it is calculated only once at the beginning of the simulation during initialize therefore modifying k11 after initialization would have no effect on the simulation results other parameters such as those related to the dimensionality of the coefficient matrices or to the time discretization should probably not be modified to avoid undefined behavior and possible program failure section 3 contains multiple validated use cases of reading and writing data using the modflow api procedures in general the user should consult the source code to confirm the validity of a specific application the implementation of the interface functions follows the csdms fortran 2003 bmi specification piper 2020 however with the goal of developing a universally accessible library in mind small modifications had to be made mostly avoiding the use of fortran specific data types the resulting api is ensured to have standard c kernighan and ritchie 1988 interoperability 2 2 development of the extended model interface bmi only allows a sequential loose coupling of process models to enable a tighter coupling at the iteration level but also to provide more fine grained control of the simulation we have developed the extended model interface xmi which consists of all bmi functionality plus the necessary extensions a first extension is the subdivision of the update function into smaller units to allow the interactive modification of time varying data these data are read from file for each time step in a dedicated read and prepare rp step as shown in fig 3 in the xmi function prepare time step the data are read and before an external call to do time step is made they can still be changed using the bmi functions a second extension is the explicit exposure of the non linear convergence loop of a numerical solution this is shown schematically in fig 4 with such fine grained control it is now possible to tightly couple modflow to other model components and have the solution converge at the outer iteration level i e within a single time step this is often preferable to a loosely coupled sequential alternative where one model finishes a time step and only then provides data that is used as boundary conditions for the other model the latter can cause the solution to oscillate in time and result in a simulation that is sensitive to the order in which the components are executed the value of having this extra functionality in the modflow api is clearly demonstrated by the fact that most of the example applications presented in section 3 require this tight coupling scheme it is important to realize that the flow chart shown in fig 3 corresponds to a simulation with a single numerical solution although this might be the most frequent use case modflow accommodates a simulation with multiple solutions for example when running groundwater flow and transport simultaneously to replicate the behavior of do time step in this case an enveloping loop running over the total number of solutions should be implemented in the calling program or script one of the key benefits of having the xmi available as an standard c compliant library is the capability to control the simulation from programming environments other than fortran this design makes the modflow api straightforward to integrate into programs written in for example c python java and c net or to run simulations interactively from a jupyter notebook pérez and granger 2007 kluyver et al 2016 because python is such a widespread and highly valued scripting language the xmipy russcher et al 2020 and modflowapi hughes et al 2021 packages have been developed to facilitate use of the modflow api the xmipy package is a key part of the interoperable layer shown in fig 1 as it contains the complete set of python bindings for the native bmi and xmi functions in modflow it fully encapsulates the complexity of type marshaling between python and fortran and the memory management of the data which allows users to work with comprehensible functions and standard python data types the modflowapi package extends the xmipy package and includes the python binding for the get var address memory manager convenience function in the future it will be extended with additional functionality that is made available through the modflow api 3 example applications we present a few examples below that demonstrate use of the modflow api the first example simulates evapotranspiration using different approaches and represents an example of how the api can be used to rapidly prototype new modflow functionality the second example shows how the modflow api can be used to optimize groundwater pumpage with standard optimization methods available in the python scipy package virtanen et al 2020 the third example is a tight coupling with metaswap van walsum and groenendijk 2008 van walsum and veldhuizen 2011 to simulate unsaturated zone flow processes and groundwater recharge the fourth example demonstrates the sequential loose coupling of prms using only bmi functionality in the modflow api and includes mapping data from non rectangular prms control volumes to grid based modflow control volumes the fifth example demonstrates use of the modflow api in the river operations model modsim labadie et al 2000 which is coded in c to optimize irrigation diversions represented with the modflow streamflow routing sfr package to meet prior appropriation constraints jupyter notebooks are available for all of the examples and include further details on the model setup the coupling and the software required to reproduce the examples see section 4 3 1 development of a custom modflow package using python modflow has simplified equations for dynamically calculating evapotranspiration as a function of the simulated water table depth this example shows how the api can be used to implement alternative evapotranspiration functions without modifying the modflow source code a python routine was written to simulate evapotranspiration that varies from a maximum value at or above defined elevation and decays to zero at a defined extinction depth using constant linear and exponential functions fig 5 for this simple example the modflow model has 10 layers 1 row and 1 column a constant grid spacing of 10 m was used in the row and column directions the top and bottom were set to 0 and 5 m respectively and the layers were discretized using a constant 0 5 m thickness a constant horizontal conductivity of 1 m d 1 specific storage of 1 5 10 5 m 1 specific yield of 0 2 and initial head of 1 m above land surface were specified in each cell the model was transient and ran for 1000 days using 1000 variable length time steps and a time step multiplier of 1 05 time step lengths ranged from 3 234 10 20 d at the beginning of the simulation and increased to 47 619 d at the end of the simulation to improve model convergence the newton raphson formulation was used a maximum evapotranspiration rate q max of 6 10 4 m d 1 an evapotranspiration surface elevation of 0 25 m depth of 0 25 m below land surface and a evapotranspiration extinction elevation of 3 m depth of 3 m below land surface were specified to simulate evapotranspiration in the model fig 5 the linear evapotranspiration function is identical to the modflow evapotranspiration evt package langevin et al 2017 the constant and exponential evapotranspiration functions are not directly available in the evt package the constant evapotranspiration function was implemented in python using 1 q e t q m a x h z e 0 h z e where q et is the volumetric evapotranspiration rate m3 d 1 q max is the maximum volumetric evapotranspiration rate m3 d 1 h is the simulated groundwater head m and z e is the evapotranspiration extinction elevation m the volumetric evapotranspiration rate q max is the product of the horizontal cell area a and q max the linear evapotranspiration function was implemented in python using 2 q e t q m a x h z s q m a x 1 d z s h z e 0 h z e where z s is the evapotranspiration surface elevation m d is the depth of the simulated head below z s normalized by the thickness of the interval over which q et varies from q max to 0 unitless d is calculated as 3 d z s h z s z e the normalized depth varies from 0 at z s to 1 at z e the exponential evapotranspiration function was implemented in python using 4 q e t q m a x h z s q m a x e 3 π d z s h z e 0 h z e the newton raphson method was used to linearize equations 1 2 and 4 and calculate the hcof and rhs variables for the api package which was used to represent evapotranspiration in the model the api package was developed to provide a generic way to add terms to the modflow system of equations when using the modflow api the modflow api for the evapotranspiration example is accessed in python by first instantiating a modflowapi object mf6 which is imported from modflowapi using image 1 the mf6 object has access to all of the methods exposed in the modflow api next the base modflow model is initialized from the mfsim nam file in the working directory using image 2 program memory is allocated and static data are read when initialize is called the current time 0 d when called right after initialize and simulation time at the end of the simulation are used to control the modflow time step loop and are determined using image 3 the maximum number of non linear iterations specified in the iterative model solution ims are determined using image 4 an array pointing to simulated heads for the evapotranspiration model is set using image 5 in this statement head is the name of the python variable of type numpy ndarray a data type in the python numpy package harris et al 2020 that points to the head array in modflow and gwfname is the user specified name of the groundwater flow model which is etmodel in this example the evapotranspiration variables hcof and rhs and the model layer containing the water table are both calculated using the values in head the variable addresses to access the data in the api package are image 6 node addr hcof addr and rhs addr are used in the set value method to modify the cell evapotranspiration is extracted from and the coefficient matrix and right hand side terms used to represent evapotranspiration in the system of equations respectively node numbers are calculated from user specified cell ids which are layer row column in this example and range from 1 to the number of cells in the groundwater flow model the python code shown in fig 6 determines the node evapotranspiration is extracted from calculates the evapotranspiration terms and solves the gwf model for a time step until convergence is achieved and then proceeds to the next time step until the entire simulation is complete the get node function returns the one based model node number for the cell containing the water table the et terms function returns the hcof and rhs variables that are used to calculate the current head dependent volumetric evapotranspiration rate q et based on equation 1 2 or 4 the hcof and rhs variables are added by modflow to the diagonal of the coefficient matrix and the right hand side respectively when the system of equations are formulated for the current outer iteration of a time step for more information see hughes et al 2017 langevin et al 2017 simulated results for the three evapotranspiration functions are shown in fig 7 simulated water levels decline from the initial value of 1 m for all three evapotranspiration functions fig 7 a the cumulative evapotranspiration for all three functions is shown in fig 7 b as expected the cumulative evapotranspiration is highest for the constant evapotranspiration function 60 0 m3 and results in the lowest water level 3 0 m at the end of the simulation the cumulative evapotranspiration is smallest for the exponential evapotranspiration function 18 6 m3 and results in the highest water level 0 9 m at the end of the simulation the cumulative evapotranspiration and water level for the linear evapotranspiration function were 39 5 m3 and 2 0 m respectively and are identical to model results using the evt package neither the constant nor the exponential evapotranspiration functions could be exactly represented using the standard evt package the modflow api allows a user to implement these functions without having to modify the modflow program 3 2 optimization of groundwater withdrawals groundwater models are often used within an optimization context to maximize groundwater withdrawals subject to drawdown constraints or to optimize mitigation strategies for contaminant plume containment for example optimization strategies require many forward runs with the groundwater model to calculate response coefficients and recalculate them as necessary for non linear problems the modflow api allows a single model instance to be solved as many times as necessary without reloading the model from files each model solution can correspond to altered model variables such as pumping rates this approach is efficient for optimization because the overhead associated with loading a model can be restricted to just the very first model solution the modflow api was used to optimize groundwater pumping rates for a hypothetical steady state groundwater flow model patterned after the example described by hill et al 1998 the original model was based on a regular grid for the application here however a 5 layer triangular mesh with 20 400 cells per layer was created to increase spatial resolution around the shoreline of the lake along the river and around three groundwater pumping wells fig 8 for this simple example the lake is represented as a specified head boundary in layer 1 the river is specified as a head dependent flow boundary in layer 1 and the wells are represented as specified flows in layer 5 net recharge is uniformly applied to the top of the model the goal of the optimization example is to determine the maximum pumping rate for each well subject to a minimum allowable head constraint for each well as shown in table 2 pumping rates cannot be optimized individually for each well because the effect of pumping at one well has an effect on the simulated head at the other wells specialized versions of modflow have been written to solve these types of optimization problems for example modflow gwm couples an earlier version of modflow harbaugh 2005 with optimization routines into a single program ahlfeld et al 2005 modflow gwm has not been kept up to date with new modflow versions and therefore it cannot be used with new features in modflow such as support for unstructured grids for example alternatively the model agnostic pestpp opt software white et al 2018 could be used for this problem but without the computational advantage of repeatedly solving the flow model without having to initialize the model from input files the modflow api allows any general optimization routine such as the simplex method available in the python scipy package to be used with a groundwater model as described by ahlfeld et al 2005 the simplex method can be used to solve the following linearized form of the optimization equations 5 m i n i m i z e z c t x s u b j e c t t o a x b w h e r e 0 x u where z is the value of the objective function c t is a transposed column vector of coefficients associated with the decision variables x is a column vector of pumping rates a is the response coefficients calculated here through perturbation of groundwater withdrawal rates and forward runs with the modflow model through the run model function fig 9 b is the column vector of constraints equal to the head at each well cell without pumping minus the head at the well cell with pumping and u is a vector containing the maximum pumping rate for each well for non linear problems in which the response coefficient matrix depends on the rates of groundwater withdrawals the optimization problem can be solved repeatedly until some acceptable solution tolerance is achieved the run model function shown in fig 9 is an important part of the optimization routine implemented to demonstrate the modflow api in this function the mf6 object is of type modflowapi and interfaces between python and the initialized and running modflow model well pumping rates stored in the wel package bound array are accessed through a pointer wrapped inside a numpy ndarray which is used by run model to change the pumping rates the maximum number of modflow iterations is also accessed and used as part of a loop to make repeated calls to mf6 solve until convergence is achieved finally the solution vector containing simulated groundwater head for the provided well pumping rates wellq are accessed as a pointer and returned to the calling program for those applications tasked with running a model repeatedly the run model function is intuitively appealing because it shows how a python script or any other program can send a running modflow instance one or more new parameter values and receive back the resulting heads and this can all be done through memory without any file access the python code used to solve this optimization problem formulates the terms shown in equation 5 and then uses the revised simplex method in scipy optimize to solve for the well withdrawal rates using image 7 for this problem the maximum pumping rates were determined to be 44 975 33 656 and 90 191 m3d 1 for wells a b and c respectively although this example problem used simple head constraints the scripting approach presented here is highly customizable and could be used with many other types of constraints such as baseflow to the river minimum head within a model subregion and so forth likewise pumping rates are used as the decision variable here however there are many other model decision variables that could be optimized in order to meet problem constraints 3 3 coupling modflow to metaswap accurate simulation of the flow of water in the unsaturated zone is important in many water quality and quantity studies in most cases the preferred approach involves numerical solution of the richards equation which is computationally intensive and challenging in terms of robustness see farthing and ogden 2017 for a recent overview on application of the richards equation to unsaturated flow problems the unsaturated zone flow uzf package which solves a simplified form of richards equation based on kinematic waves is available in modflow however this approximation affects the ability to simulate capillary effects and the accuracy of simulated soil moisture dynamics these dynamics are particularly important in lowland regions such as the netherlands where the groundwater levels are within 2 m of the soil surface in most of the country as a result the current national scale hydrological model of the netherlands de lange et al 2014 relies on a dedicated coupling between a previous customized version of modflow vermeulen et al 2020 and metaswap a quasi steady state simulation of the unsaturated zone based on richards equation van walsum and groenendijk 2008 a new implementation of this model is being developed and will now build on the modflow api avoiding the need to develop a proprietary version of modflow as was done in the past this example presents a hypothetical model with characteristics common to hydrologic conditions in a large part of the netherlands the software driving this example is the python package imod coupler which uses xmipy to control both modflow and metaswap components using their xmi enabled shared libraries in short after every solution of the groundwater heads within the non linear convergence loop see section 2 2 metaswap determines the unsaturated zone flux and the associated primary storage coefficients while at the same time ensuring mass balance for the shared control volume both variables groundwater recharge and storage coefficients are then communicated to modflow and this sequence is repeated until the modflow convergence criteria are met see van walsum and veldhuizen 2011 for more detail on the shared control volume approach used to couple modflow and metaswap the groundwater model consists of a rectangular grid consisting of 3 layers and 81 9 9 cells per layer covering an area of 8100 m2 with the soil surface located at an elevation of 0 0 m a general head boundary condition ghb is assigned to the outer columns and the active cells in the top layer are all coupled to a corresponding cell in metaswap the simulation is run for an exceptionally dry year 2018 using daily stress periods and 1 time step per stress period precipitation and reference evapotranspiration makkink 1957 both forcing data for metaswap are from the dutch national weather service de bilt station the land use type in the model domain is agricultural and the crop type is potatoes which have a relatively large seasonal transpiration rate and require groundwater irrigation the metaswap irrigation process is enabled and extracts water from model layer 3 of the modflow model component using the wel package the soil type for the metaswap model component is specified as peat which is type number 1 in the metaswap soil database precipitation and actual evapotranspiration et act are shown in fig 10 a actual evapotranspiration is greatest during the growing season with peaks appearing at weekly intervals resulting from increased soil water evaporation during the sprinkler irrigation which is modeled to occur on weekly basis the simulated recharge from the unsaturated zone to the groundwater model q rch for the centermost cell in the top layer is shown in fig 10b the inset in fig 10b shows how the system transitions from the unsaturated zone being a source of water for the groundwater system to a sink for the groundwater system as a result of capillary rise during the dry summer period simulated groundwater head in the centermost cell in the top layer is shown in fig 10c and shows the response of the water table to groundwater recharge q rch irrigation effects are not noticeable in fig 10c because the well extracts water from model layer 3 although this example is hypothetical and oversimplified it does illustrate how the modflow api enables seamless coupling of two existing component models without needing to modify the source code of either component model in this example metaswap makes it possible to simulate the unsaturated zone in more detail than possible with the modflow uzf package and to simulate groundwater irrigation that is a function of soil moisture the explicit control of the outer iteration loop a specific feature of xmi is required for this example and can serve as a blueprint for other applications that require tight coupling with modflow 3 4 coupling modflow to prms the modflow api and bmi for prms were used to simulate integrated surface water and groundwater processes in the sagehen creek watershed located on the east slope of the northern sierra nevada mountains in california the sagehen creek application described here is based on the gsflow model of the same area markstrom et al 2008 some key differences between this sagehen creek application and the gsflow sagehen creek model application include 1 128 watershed based hydrologic response units hrus were used instead of cell based hrus 2 the prms cascade module was not used to route surface runoff and interflow from upslope hrus to downslope hrus and 3 the prms soil component is only solved once per time step instead of being recalculated each modflow outer non linear iteration as is done in gsflow as part of a separate effort the bmi was recently implemented in prms with the program being split into separate surface soil groundwater and streamflow domain components the surface and soil domain components were the only prms domain components used in this example the prms surface domain component was used to simulate processes above the soil surface including 1 rain and snow 2 potential evapotranspiration 3 snow sublimation 4 canopy interception storage evaporation and throughfall and 5 surface runoff the prms soil domain component was used to simulate storage inflow and outflow within the soil zone reservoir soil zone inflows include infiltration and outflows include 1 evapotranspiration 2 interflow and preferential flow and 3 groundwater recharge the modflow uzf package was used to simulate vertical unsaturated groundwater flow below the prms soil domain component soil zone and subsurface reservoirs the modflow sfr package was used to simulate streamflow in a total of 201 connected reaches the modflow drain drn package was used to simulate groundwater seepage to land surface in areas where the groundwater levels exceed land surface prms hrus and parameters are from markstrom et al 2006 the hrus simulated in this model application are shown in fig 11 modflow parameters are from gsflow sagehen creek model application the groundwater domain component has a total of 2 layers 73 rows and 81 columns and was discretized using a constant grid cell size of 90 m in the row and column directions the active model domain is restricted to the lateral extent of the sagehen creek watershed with a total of 3392 active cells per layer and covering a 27 475 200 m2 area uzf package cells were included in active cells in both model layers and in cases where the water level in cells in model layer 1 is below the bottom of the cell unsaturated zone flow at the bottom of the cell is routed to the underlying uzf cell in layer 2 lateral groundwater discharge out of the watershed was simulated using the time varying constant head chd package and constant head cells at the downstream end of the watershed a constant head of 1915 m and 1900 m was specified in 3 cells in model layer 1 and 3 cells in model layer 2 respectively see markstrom et al 2006 and markstrom et al 2008 for additional information on prms and modflow parameters modflow and prms were sequentially coupled during a time step by first running the update functions for the prms surface and soil domain components extracting results from prms and mapping prms hru results to modflow grid cells and finally running the modflow update function groundwater recharge calculated by the prms soil domain component for the soil zone and subsurface reservoirs for each of the 128 hrus was applied as infiltration to 3386 underlying uzf package cells in model layer 1 unsatisfied potential evapotranspiration calculated as the difference between the potential evapotranspiration calculated in the prms surface domain component and the actual evapotranspiration calculated by the prms surface and soil domain components was applied as potential evapotranspiration to underlying the uzf cells in model layer 1 the hrus were intersected with the modflow grid to calculate the area based hru uzf weights used to map prms results to uzf cells the hrus were also intersected with the stream network to map prms runoff and interflow from 128 hrus as runoff to 201 sfr reaches equal weighting rather than reach length based weighting was applied to calculate hru sfr weights in addition to mapping prms hru data to modflow uzf cells and sfr reaches unit conversions were made to convert prms inch and acre units to modflow m and m2 units the modflow mover mvr package was used to route rejected infiltration calculated by the uzf package and groundwater seepage to the surface calculated by the drn package to the reaches in the sfr package the same approach used to develop the hru sfr weights was used to connect uzf cells to sfr reaches using the mvr package uzf cells in model layer 2 were also routed to the same sfr reach as uzf cells in model layer 1 using the mvr package to route excess unsaturated zone flow from the overlying uzf cell rejected infiltration to the surface water network the model application simulation period extends from october 1 1980 through september 30 1996 and included 5844 daily time steps the sagehen creek watershed model application used daily precipitation and air temperature data from the gsflow sagehen creek model application precipitation in the sagehen creek watershed is highly variable in form and intensity and generally increases with altitude initial groundwater levels for the simulation were based on steady state heads calculated by a stand alone modflow model with specified uzf package infiltration rates from the gsflow sagehen creek model application and an initial moisture content of 0 08 in each uzf cell although model application parameters are based on previous sagehen creek model applications this model application is considered uncalibrated because of the previously stated differences from the gsflow sagehen creek model application significant storage changes were observed in the unsaturated and saturated zones in modflow results during the first two years of the simulation as a result the period from october 1980 to october 1982 is considered a warm up period and has been excluded from model result analyses the average infiltration which is calculated as the difference between the groundwater recharge calculated by the prms soil domain component and rejected infiltration calculated by the uzf package is shown in fig 11a rejected infiltration occurs near the surface water network and is shown in fig 11a as areas where average infiltration rates are different from the groundwater recharge rate calculated by prms for a hru average groundwater recharge rates exceed average infiltration rates near topographic divides for several hrus and in cells with elevations much higher than surrounding cells as a result of drainage of initial soil moisture in the unsaturated zone simulated and observed streamflow is shown in fig 12 a the model application over simulates streamflow especially during high flow events the contribution of runoff interflow groundwater seepage and baseflow to streamflow is shown in 12b interflow was the largest contributor to streamflow at the end of the evaluation period followed by baseflow groundwater seepage and runoff the contribution of prms unsaturated zone and saturated zone evapotranspiration to the total evapotranspiration is shown in 12c evapotranspiration from the surface and soil prms component accounted for more the 99 of the total evapotranspiration 3 5 coupling modflow to modsim three recent publications showcased the integration of a river operation model with a physically based distributed parameter hydrologic model brookfield et al 2017 dogrul et al 2016 morway et al 2016 in all three integrations reservoir release and ditch diversion s as calculated by the river reservoir operation model override values within the hydrologic model and specified by a user prior to running the model the groundwater surface water exchange rates resulting from those operational decisions i e releases and diversions are then re calculated by the hydrologic model for updating the appropriate values within the operations model in summary the respective specialty of each model complements a known weakness of the other resulting in a modeling platform that better equips water resource managers to conjunctively manage groundwater and surface water as a single resource winter et al 1998 unlike the previous river operation and hydrologic model integration efforts no customization of the hydrologic model source code was necessary for this integration instead the selected river operation model modsim labadie et al 2000 repeatedly calls modflow as needed unlike the other included examples modsim uses the microsoft net platform to run the initialize and finalize functions shown in fig 2 the prepare time step and finalize time step functions shown in fig 3 and the prepare solve solve and finalize solve functions shown in fig 4 as necessary the only new code that was written to establish communication between modsim and modflow was through the custom code interface provided with the standard distribution of modsim use of the modflow api with a river operations model is demonstrated with a hypothetical model that first appeared in morway et al 2016 and is shown in schematic form in fig 13 the model is patterned after an irrigated river valley governed by the prior appropriation doctrine i e first in time first in right for this example the net streamflow accretions and depletions as calculated by modflow along every simulated stream reach are added to or subtracted from the corresponding modsim link with this information modsim recalculates the reservoir release and diversion rates based on the updated groundwater and surface water exchanges next the modflow time step is rerun in order to update the accretions and depletions associated with the latest modsim operational decisions the two models continue iterating within a time step until all the changes in shared values reservoir releases diversions and surface water groundwater interactions satisfy the convergence criteria in this way the respective hydrologic and river operation solutions synchronize in time and space before moving on to the next time step an important advancement in simulating conjunctive use systems the impact of integrating modsim with modflow as opposed to running modflow by itself is shown in fig 14 for an abbreviated simulation period of one year reservoir storage is drawn down and delivered to specific ditches that own the stored water fig 14a for this example the three junior ditches d1 d2 and d3 fig 13 are assigned storage accounts that provide water during the summer months when natural flows as opposed to stored water generally taper off the first storage account to run out of water is d1 and without access to other sources of water the diversion amounts are reduced to zero by modsim without any further input adjustments by the user fig 14b in the modflow only run however total monthly diversions remain inappropriately high in d1 throughout the year by virtue of its upstream location within the system on its own modflow cannot simulate prior appropriations within the surface water irrigation system and is therefore limited to a simplified set of rules available with the sfr package langevin et al 2017 for determining diversion rates that may or may not reflect management practices d2 shows relatively good agreement between modsim modflow and modflow only total monthly diversion amounts owing to the fact that its reservoir storage persists throughout the relatively short one year simulation period fig 14c however differences between the two simulations similar to fig 14b would occur were d2 to run out of storage water fig 14d and e further highlight the difficulty in simulating river operations without a tool like modsim informing river diversions recognizing that the next downstream diversion d4 fig 13 has a senior water right an appropriate rule was selected from the four rules available within the sfr package that attempts to allow enough water by to satisfy the senior downstream water right however in so doing no water is diverted by d3 even though it should have access to stored water that was released from the reservoir integration with modsim through the bmi correctly diverts the water released from the d3 storage account into the d3 ditch finally because natural flows are the only source of water available to d4 modsim diverts a full allotment for d4 during the spring snowmelt season commonly april june fig 14e but reduces diversions in the other months although this is hypothetical example it highlights the value of the modflow api without it it would be impossible to properly simulate the surface water operations and in connected stream aquifer systems if the surface hydrology is wrong then accurate simulation of the groundwater system remains unattained 4 software and data availability the modflow api xmi xmipy and the modflowapi see fig 1 were collaboratively developed by the u s geological survey and deltares modflow is available from the u s geologic survey at https www usgs gov software modflow 6 usgs modular hydrologic model and as an open source code repository from https github com modflow usgs modflow6 the xmipy and modflowapi python packages are available as open source code repositories from https github com deltares xmipy and https github com modflow usgs modflowapi respectively and both can be installed from pypi the prms soil and surface components with the basic model interface are available from the csdms stack conda channel or from pypi jupyter notebooks and model application datasets that are not built within the notebook using flopy bakker et al 2016 are available as a release asset from https github com modflow usgs modflowapi releases tag esmdatasets each jupyter notebook includes software installation instructions and use limitations several additional jupyter notebooks that demonstrate use of the modflowapi python package are available in the modflowapi repository 5 discussion and conclusions the modflow api was developed to allow external programs control of a modflow simulation and access to internal variables while the simulation is running these external programs may consist of another physical process model to be coupled with modflow a machine learning model representing a physical process or a python script that needs control and access of modflow while it is running the modflow api is based on the established bmi convention for software interoperability there are many benefits to be gained from the modflow api as demonstrated in the five examples and described in the following list the modflow api provides a sustainable way to design and maintain software that relies on modflow as a component software designed in this manner can take advantage of new modflow features without having to modify and update the source code the modflow api allows other process models to be coupled with modflow this coupling can be performed sequentially for each time step or the models can be tightly coupled at the matrix solution level using the xmi extension to the bmi this tight coupling approach may allow for larger time steps and more accurate mass conserved solutions for complex problems users have new flexibility to make custom modflow packages and prototype new capabilities without having to compile modflow or make any changes to the underlying source code these new user packages can be developed with popular languages such as python which are familiar and easy to use there are many different modflow variants including those that represent parameter estimation adjoint state calculations farm processes and chemical reactions for example including all possible relevant processes into modflow is not sustainable or practical the api allows the modflow program to remain focused on its intended scope as a hydrologic simulator while enabling extension to other domains as necessary modflow reads and writes text files and binary files that are well documented but not familiar to scientists in other domains through the data access routines afforded by the modflow api users can inject data read from alternative files such as hdf and netcdf files similarly model output could also be written to alternative file formats at runtime software capabilities are rapidly evolving and being made available to scientists and engineers in the form of new packages and modules for example sophisticated machine learning tools are being developed and released faster than they could be implemented in modflow the modflow api allows users to innovate with new software tools that may not be accessible otherwise modflow is supported by several commercial graphical user interfaces guis that allow users to develop run and post process groundwater models the modflow api provides a new way for these guis to run a simulation monitor its progress even at the iteration level and provide instantaneous feedback to the user this paper describes the initial release of the modflow api and there are several limitations to mention although the api allows the user to interact with time steps individually they must be executed sequentially from the first to the last time step i e a time step that has been finalized using update or finalize time step can not be rerun development of restart capability for modflow is a high priority but being able to rerun a time step and correctly synchronize file based input and output during run time is challenging another limitation is the current version of the modflow api requires that the user has intimate knowledge of the underlying modflow program variables and order of operations for coupling another model with modflow the user must be familiar with both programs have a firm understanding of variable names which variables to exchange when to exchange information the scientific units used in the models and so forth work is ongoing to better document the internal functioning of modflow in order to make it easier to use the modflow api for example it may be possible to use standard names for common variables as suggested by peckham et al 2013 and peckham 2014 and allow users to access data using these standard names there is also ongoing work to allow model simulations to be created in memory rather than being created from input files as is required in the current version these limitations will continue to be addressed by the modflow development team as the modflow api is used in practice and the latest developments will be available at the urls listed in section 4 declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements the authors would like to acknowledge mark piper and eric hutton from csdms for their help in implementing the bmi standard in prms and for discussions during the development of the modflow api the authors would also like to acknowledge michael fienen from the u s geological survey for early discussions about the development of the modflow api and for his review of the manuscript laura schachter from the u s geological survey for her review of the manuscript joeri van engelen from deltares for his review of the manuscript and four anonymous reviewers at the journal any use of trade firm or product names is for descriptive purposes only and does not imply endorsement by the u s government 
25664,total alkalinity ta is one of the important parameters to show the intensity of seawater buffer against ocean acidification ta dynamics in the northern gulf of mexico n gom is significantly affected by the mississippi river an empirical ta algorithm is offered here which accounts for the local effects of coastal processes in situ data collected during numerous research cruises in the n gom were compiled and used to develop ta algorithms using sea surface temperature sst and sea surface salinity sss as explanatory variables after improving the coefficients and functional form of this algorithm chlorophyll a chl a was included as an additional explanatory variable which worked as a proxy for addressing the pronounced effects of biological forcing on coastal waters finally a geographically weighted regression algorithm was developed in the form ta exp xo x1 sss 35 2 x2 sssxsst 1 2 x3chl a to address spatial non stationarity which produced improved estimates of ta in the n gom keywords total alkalinity northern gulf of mexico ocean acidification geographic weighted regression 1 introduction about 25 of the anthropologically generated atmospheric co2 is absorbed by the world oceans le quéré et al 2016 this has resulted in an increase in the concentration of dissolved inorganic carbon and decrease in average ocean ph also known as ocean acidification oa sun et al 2012 a gradual decrease in carbonate ion concentrations aragonite saturation states ωar and calcite saturation states ωca leads to reduced calcification rates and ultimately calcium carbonate dissolution cooley 2009 gledhill et al 2009 a key concerning aspect of ocean acidification reduced calcification rates negatively impact the calcifying organisms in the ocean and also have important implications for global carbon cycle raven et al 2005 thirty five years of observation concluded that the rate of alteration in seawater carbonate system parameters varies geographically with the greatest variability observed in the north pacific ocean region takahashi et al 2006 this variability likely results from local oceanographic processes which contribute additional carbon into the system by lateral mixing of waters from marginal seas input from coastal areas and rivers upwelling of subsurface waters and biological activities takahashi et al 2006 similarly there are a myriad of factors that affect coastal acidification in the northern gulf of mexico n gom primarily owing to heterotrophic respiration of terrestrial and autotrophically produced organic carbon cai 2003 hu et al 2017 mainly due to nutrient loading by the mississippi river as well as outflow from other rivers cai et al 2011 lohrenz and cai 2006 hence the variability in the carbonate system parameters in the n gom coastal waters is notably higher than the pelagic central gulf of mexico due to pronounced coastal biogeochemical cycling the partial pressure of co2 pco2 total alkalinity ta ph and dissolved inorganic carbon dic are the principal carbonate chemistry parameters used to document the status and trends of ocean acidification the carbonate system is bivariate and can be fully constrained if any two of the four measurable carbonate system parameters are known ta coupled with pco2 are frequently used e g noaa s ocean acidification product suite gledhill et al 2008 to estimate ph ωar and ωca using the co2sys program lewis and wallace 1998 employing the carbonate equilibrium equations e g mehrbach et al 1973 thus ta is one of the important parameters that aids in deriving other carbonate system parameters bever et al 2021 ta is one of the important parameters to show the intensity of seawater buffer against ocean acidification and is defined dickson 1981 as the number of moles of hydrogen ion equivalent to the excess of proton acceptors in 1 kg of sample μmol kg 1 due to the prevalence of carbonate rocks in the drainage basin of the mississippi river the ta values of the mississippi river water are usually high raymond and cole 2003 which has increased significantly in the past half century as a result of increased flow from higher rainfall and land use changes that have altered vegetation cover keul et al 2010 keul et al 2010 observed a non conservative behavior of ta in the mississippi river plume region which they attributed to changes in microbial recycling across salinity gradients additionally non conservative mixing with waters from local estuaries such as terrebonne bay and smaller inputs from other estuaries decrease the ta values in the coastal waters this decrease is considerably less due to the high alkalinity content of the mississippi river and other rivers draining to the gulf of mexico keul et al 2010 millero et al 1998 developed a set of algorithms for estimating salinity normalized ta in global oceans using historical ta measurements and ta measured during global carbon surveys during 1990s lee et al 2006 updated those algorithms with five polynomial algorithms for estimating ta in five major oceanographic regimes using sea surface salinity sss and sea surface temperature sst as explanatory variables these oceanographic regimes include the subtropics the equatorial upwelling pacific the north atlantic the north pacific and the southern ocean the n gom region is close to zone 1 i e the subtropics as described by lee et al 2006 however the calibration dataset of the ta algorithm for zone 1 lacked in situ data from the n gom region which likely explains the bias of the algorithm for the n gom region the mississippi river is the largest river influencing n gom with an average discharge of about 1 35 0 2 104 m3 s based on 64 years of united states geological survey discharge data hu et al 2005 at the mississippi river outfall and shelf area high net pco2 is evidenced due to heterotrophic production of co2 from land derived organic carbon cai 2003 it is further influenced by autotrophically produced organic carbon caused by high nutrient loading leading to co2 drawdown in shelf surface water ternon et al 2000 huang et al 2015 and heterotrophy of autotrophically produced carbon cai and lohrenz 2006 which acts as an additional pco2 source lohrenz and cai 2006 guo et al 2012 reported that the net autotrophic production of carbon at the mississippi river plume is even higher than the amazon system the nutrient rich mississippi river water coupled with outflow from other rivers influences carbon dynamics of the n gom coastal ecosystem lohrenz and cai 2006 causes bottom water hypoxia in the region rabalais et al 2002 and impacts the marine ecosystem affecting fisheries and coral reefs in the n gom hu et al 2003 salisbury et al 2001 walker 1996 inoue et al 2008 thus the n gom coastal region is biologically and chemically complex and is significantly different from the rest of the subtropical ocean in past research chlorophyll a chl a has been used as a proxy for primary production in estimating pco2 lohrenz and cai 2006 thus the incorporation of chl a as an explanatory variable has great potential to increase ta algorithm performance it is difficult to account for the high degree of spatial and temporal variability in carbonate system parameters in the coastal waters using an algorithm that was developed for the open ocean due to the biological and chemical complexities of the coastal waters as well as the variable freshwater influence spatial variability in the relationships between parameters is referred to as spatial nonstationarity brunsdon et al 1996 geographically weighted regression gwr is a statistical modeling technique that attempts to address the spatial nonstationarity brunsdon et al 2010 the spatial variation of complexities in ocean chemistry and acidification processes in coastal waters the gwr approach generates a set of coefficients for each point in space empirically using the provided functional form for the regression equations in the case of estimating carbonate system parameters the gwr approach has great potential in accounting for the spatial variability of the influence of biogeochemical processes on the explanatory variables the objective of this effort was to develop a revised empirical algorithm for estimating ta in the coastal waters of the n gom by proposing an improved functional form incorporating chl a as an additional explanatory variable and utilizing gwr to address spatial nonstationarity in the n gom domain 2 materials and methods in situ ta sst and sss data and satellite derived chl a were used for the development and testing of the algorithms in this study the algorithms were later applied to satellite derived sst sss and chl a imagery to illustrate the efficacy of the algorithms in showing the spatial distribution of ta using the derived algorithm a total of 247 observations of ta sst and sss each were assimilated from the in situ data collected during numerous research cruises from july 2007 to september 2013 in the n gom table 1 fig 1 the areal extent of these observations is about 474 219 sq km rendering an average of one observation per 41 km 41 km area in situ sss and sst values were used to estimate ta using each of the five algorithms given by lee et al 2006 and estimated ta values were compared with in situ measured ta values as the n gom is closer to the zone 1 it was expected that the ta estimation error would be lowest with this algorithm however the lowest error was found when the algorithm that was developed for zone 4 was used with zone 1 s algorithm performing second best table 2 while error was smallest with the zone 4 algorithm the zone 1 algorithm was selected due to its geographic proximity to the study region and its simpler functional form than the zone 4 algorithm the zone 4 algorithm incorporates longitude as an exploratory variable which would be redundant in the presented gwr algorithm the zone 1 algorithm coefficients were modified to investigate potential improvement in ta estimations in the n gom this was done by maintaining the functional form of the algorithm but updating the algorithm coefficients by cross validation next the functional form of the algorithm was modified to lower the standard errors of the coefficients in the algorithm by introducing chl a as an additional explanatory variable chl a data were derived from a time series of modis satellite imagery processed using the seadas software package v7 4 with the default atmospheric correction out of the 247 locations with ta observations modis derived chl a were available for only 78 locations fig 1 due to cloud cover and atmospheric correction failure at certain coastal locations this reduction in observations led to a study area reduction to 359 397 sq km with an average of one observation per 67 km 67 km area using the data from these 78 locations the ta algorithm was modified by adding a linear term for chl a along with the modified functional forms for sss and sst finally geographically weighted regression gwr was carried out with sss sst and chl a as explanatory variables and ta as the dependent variable while the 78 data points constituted a limited dataset the distribution of observations was still relatively uniform due to the paucity of data gwr was chosen to improve the algorithm since gwr allows coefficients to vary continuously over the study area and a set of coefficients are estimated for each location typically on a grid using a weighted approach so that the coefficient rasters account for the spatial heterogeneity the open source gwr4 semiparametric gwr gwgl modeling tool version 4 09 2016 developed by the national centre for geo computation ireland and ritsumeikan university japan was used to carry out gwr with an adaptive gaussian kernel agk with cross validation as the selection criteria brunsdon et al 1996 during the gwr algorithm development weights for each data point were computed based on the relative distance between the point under consideration and the rest of the observations using agk charlton and fotheringham 2009 the rate of decrease in the weights was calculated using the agk this kernel function unlike fixed kernel uses dynamic bandwidth distance where the bandwidth distance is automatically chosen with a smaller dynamic bandwidth distance for the regions with a higher number of observations and higher dynamic bandwidth distance for the regions with a lower number of observations the agk was chosen because of non uniform spatial distribution density of observations in the dataset all these algorithms were developed using all available in situ data and validated using a 10 fold cross validation approach for validation the dataset was randomly divided into ten subgroups an algorithm was developed using the data from the nine subgroups and validated using the 10th subgroup this was repeated nine times so that each subgroup couldbe used for validation and in the calculation of the residuals subsequently mean absolute error mae and root mean square error rmse were calculated for the estimated ta and the median rmses and maes were compared between the algorithms this approach allowed the use of all the data for algorithm development and validation finally ta images were generated by using the developed algorithms with sss from the soil moisture active passive smap sensor and sst and chl a from the aqua modis sensor 3 results a set of algorithms are given by lee et al 2006 for estimating global distribution of surface alkalinity using sss and sst as explanatory variables for the purpose of evaluating the existing algorithm performance in n gom the algorithm developed for zone 1 was tested to estimate ta using in situ sss and sst data the lee et al 2006 algorithm for zone 1 is given by 1 ta 2305 58 66 sss 35 2 32 sss 35 2 1 41 sst 20 0 04 sst 20 2 estimated ta values were compared with in situ measured ta for quantifying the algorithm accuracy within the n gom domain this algorithm produced a median rmse of 272 6 μmol kg 1 with a rmse of 11 4 and a mae of 183 9 μmol kg 1 with a mae of 7 7 tables 1 and 3 the distribution of the residuals as percent error calculated as in situ ta estimated ta in situ ta x 100 illustrates that the ta values were mostly underestimated by the lee et al 2006 zone 1 algorithm fig 2 a and b fig 3 a shows the residual percentages on a map revealing the highest ta estimate deviations range 10 80 were primarily located in coastal waters in the central gom the deviation of estimated values was less 2 from the in situ measured values hence the lee et al 2006 algorithm seems well suited for the central gom but performs poorly near the coast to address this bias the lee et al 2006 zone 1 algorithm was updated using the coastal in situ data to modify the algorithm coefficients the resulting modified algorithm was 2 ta 3299 394 7 449 sss 35 0 083 sss 35 2 250 062 sst 20 15 784 sst 20 2 a comparison of this algorithm with the zone 1 algorithm of lee et al 2006 revealed that the effect of sss on ta is different in the n gom region relative to the subtropical region as the magnitude of sss coefficients were smaller in eq 2 compared to eq 1 additionally the effect of sst on ta was more pronounced in the n gom region than the subtropics as evidenced by the larger sst coefficients in the updated algorithm the modification produced notable improvements as the median rmse was 94 5 μmol kg 1 with a range from 76 5 to 112 4 μmol kg 1 for this algorithm table 3 which is a 65 improvement over the zone 1 algorithm provided by lee et al 2006 moreover the residuals and comparison of in situ and estimated ta showed significant improvements fig 2c and d however the coefficient for the third term sss 35 2 had a very high standard error error for x2 91 table 3 suggesting high algorithm instability with this coefficient initially a regression without including the salinity parameter with only two terms of sst was carried out this resulted in an algorithm with high coefficient standard errors 30 for the first sst term and 33 for the second sst term and a lower adjusted r2 0 13 similarly a regression with sss alone also did not yield a good algorithm to address the issue of higher coefficient standard errors and lower adjusted r2 several functional forms were tested after including both sss and sst ultimately an exponential equation with modification in the parameters comprising one sss and two sst terms produced the best algorithm with lower rmse lower standard error and higher adjusted r2 which is given by 3 ta exp 8 053 0 000156 sss 35 2 0 0562 sst 20 2 0 000272 sst 20 3 modification in the functional form decreased the standard error for the coefficient of the second and third terms from 27 to 10 and 91 8 respectively table 3 however there was a decrease in r2 and an increase in the rmse table 3 as such chlorophyll a chl a was added as an additional explanatory variable to the algorithm the correlation between chl a and ta was found to be 0 74 which indicates it may be a confounding variable that requires addressing after adding chl a various modifications to the functional form were tested and it was found that an algorithm with sss and sst interaction term produced the most efficient algorithm when chl a was included in the algorithm among all the algorithms the one with the highest r2 0 61 lowest rmse 22 6 μmol kg 1 and lowest standard error for the coefficients table 3 was chosen which is given by 4 ta exp 7 727 0 000385 sss 35 2 0 00162 sss x sst 1 2 0 00212 chl a the inclusion of chl a as an explanatory variable improved the algorithm considerably however the adjusted r2 was still less than that of eq 2 table 3 next a gwr algorithm was developed using the above functional form that includes chl a as the additional explanatory variable the output from the gwr approach was the value of the coefficients at each point however the coefficients were noisy so the cubic spline surface interpolation was performed to develop smooth raster surfaces of the coefficients over the spatial domain the raster images for the four coefficients showing the range of values in the n gom are shown in fig 4 and given as an arc gis map package as supplementary material for example a resulting raster surface for the second coefficient x1 the sss 35 2 term ranged between 0 000350 and 0 000410 fig 4 the coefficients in coastal locations differed significantly from those in the central gom these differences suggest the spatial variation in the importance of the explanatory variables in ta estimations which likely arise from local effects of biogeochemical processes that affect carbonate chemistry an adjusted r2 of 0 69 was obtained for the gwr algorithm table 3 which is the highest adjusted r2 obtained in this study indicating that the algorithm best fits the data the residuals were also randomly distributed for this algorithm fig 2e notably most points were closer to the 1 1 line fig 2f for the gwr algorithm relative to that of other algorithms fig 2b and d suggesting the gwr algorithm is in good agreement with the in situ ta values additionally there is an absence of a spatial trend in the gwr residuals fig 3b when compared to the residuals of lee et al 2006 zone 1 algorithm fig 3a clearly the gwr approach significantly reduced spatial nonstationarity in the ta estimates deeming this approach more valid all the algorithms were applied to modis derived sst and chl a and smap derived sss images of february 10 2016 to obtain ta images by multiplying the coefficient rasters with the sst sss and chl a images as per the functional form of the algorithm provided in eq 4 the higher sss coefficient magnitudes in the lee et al 2006 zone 1 algorithm suggest ta estimated by that algorithm was highly influenced by sss the resulting ta image fig 5 d appears very similar to the sss image fig 5a and is strongly correlated at 0 97 with the improved gwr algorithm the resulting ta image displays the influence of sst sss and chl a on ta fig 4e specifically due to high chl a concentrations near the mississippi river outfall red color in fig 5c the ta values are estimated to be low blue in fig 5e thus the coastal region showed improved estimates differences between the gwr algorithm and the lee et al 2006 zone 1 algorithm eq 1 with chl a is displayed in fig 5f and the improvement offered by gwr is clearly demonstrated the lee et al 2006 algorithm underestimated ta in the coastal waters shown in deep blue in fig 5d and red in fig 5f and over estimated in the areas away from the coastal region green color in fig 5f compared to the image generated using the gwr algorithm 4 discussion when the lee et al 2006 algorithm which was developed for subtropical open ocean waters was applied to the n gom dataset the residuals showed a clear trend and the ta was mostly underestimated fig 2a and b the highest residuals were located in coastal waters fig 3a likely a result of the lack of in situ data from the n gom in the lee et al 2006 algorithm calibration errors were reduced by 65 when the algorithm coefficients were updated using in situ data from n gom but the algorithm coefficients had high standard errors after the update to alleviate this issue an exponential functional form was used which decreased the standard errors in the coefficients significantly table 3 ocean chemistry is significantly different in the n gom relative to the subtropical atlantic owing to the pronounced effects of coastal biogeochemistry heavily influenced by the mississippi river and outflow from other rivers which supply nutrients and alkalinity cai et al 2011 lohrenz and cai 2006 schneider et al 2007 was able to obtain a working algorithm for ta estimation in the mediterranean sea using sss alone however for the n gom region use of only sss or only sst or even sss and sst together resulted in high rmses possibly because of the influence of the mississippi river and outflow from other rivers thus chl a was introduced as an additional explanatory variable in the ta algorithm to serve as a proxy for primary production along with an interaction term between sst and sss more recently fine et al 2017 investigated global ta derived using lee et al 2006 algorithms and reported that the errors in estimated ta in north atlantic and pacific were due to higher observed sst and sss in large percentages than the ranges given for lee et al 2006 algorithms an underestimation of ta was observed in this study for the same reason in the n gom hence a regional parameterization of the algorithm using observed values in the n gom improved the algorithm considerably by reducing the median rmse from 272 6 to 22 6 μmol kg 1 however the standard error in the coefficients increased slightly as the sample sizes were significantly reduced 247 78 observations when chl a data were included while the introduction of chl a offered improvements in the ta estimates in the coastal region gwr was tested to minimize spatial nonstationarity the resulting gwr algorithm provided several interesting observations one clear observation was the difference in coefficient magnitudes between coastal areas and the central gom these differences resulted from the biological and chemical complexities of the coastal waters as compared to the central gom and was not revealed without using the gwr approach figs 4 and 5 in the development of the algorithms it was clear that the lee et al 2006 zone 1 algorithm relied heavily upon sss contrarily the improvements based on the lee et al 2006 algorithm offered here incorporated the effects of sss sst and chl a in its calculations of ta these results support the work by keul et al 2010 which showed that the ta salinity relationships from lee et al 2006 do not apply to shelf waters effectively adding the chl a variable produced more accurate ta estimates relative to lee et al 2006 in the n gom the adjustments offered by the gwr algorithm in the coastal regions of the study domain were also very apparent when comparing it side by side with the image generated by applying the lee et al 2006 zone 1 algorithm fig 4d and e overall the estimated ta obtained by the gwr algorithm corresponded well to the findings of keul et al 2010 and the expected ta in the n gom nevertheless a denser observation distribution would address local spatial effects more accurately while observation density was low the distribution of observations was relatively uniform suggesting the algorithm coefficients derived from the gwr approach are able to produce reasonable estimates of ta in the n gom mississippi river and other small rivers supply ta into the n gom and there is significant seasonal variability in ta and nutrient levels in the n gom the relationship of ta with sss sst and chl a is dependent on mixing thus the gwr approach and the use of three variables sss sst and chl a in the algorithm account for this variability to a great extent making this approach potentially applicable to other coastal systems around the world 5 conclusions the lee et al 2006 ta algorithm which was developed for the subtropical oceans did not perform efficiently in estimating ta in the n gom coastal regions likely owing to the biological and chemical complexities in the coastal waters caused by the outflow of the mississippi river and other smaller rivers an improved algorithm was formulated by using in situ data from the n gom region and by introducing satellite derived chl a in the algorithm prior to including chl a the coefficients and the functional form of the lee et al 2006 ta algorithm were modified with decreased standard errors the addition of an interaction term between sss and sst and the addition of chl a as an additional explanatory variable resulted in an increase in adjusted r2 with least rmse and mae finally the gwr approach offered the most reliable and valid algorithm with highest adjusted r2 and least error lee et al 2006 global ta algorithm was improved by introducing satellite derived chl a as an additional explanatory and by addressing spatial nonstationarity using gwr notable improvement in estimated ta was observed in the coastal n gom region with this approach as it considered both biological complexities and spatial heterogeneity an arc gis map package has been provided with four rasters with coefficients for the gwr algorithm as supplementary material to create ta images one would simply multiply these raster images to satellite derived sst and chl a images and either satellite or model e g hycom derived sss as per the provided functional form of the algorithm which can easily be performed using arc gis or any other image processing software or program although the algorithm was successfully improved future research utilizing a larger dataset from a longer period covering more locations is still essential to continue to improve ta estimates in n gom or other complex coastal ecosystems especially over the river influenced coastal margins declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements this research was partially supported by a noaa grant na11oar4320199 to dr padmanava dash we sincerely thank dr dwight gledhill deputy director noaa ocean acidification program and dr xinping hu associate professor department of physical and environmental sciences texas a m university corpus christi for reviewing the manuscript and suggesting substantial improvements in situ data used in this study was provided to the community by noaa aoml and noaa ncei through pangaea the satellite data products used in this study were provided to the community by nasa through the ocean biology processing group gsfc nasa and were processed using nasa s seawifs data analysis system seadas 
25664,total alkalinity ta is one of the important parameters to show the intensity of seawater buffer against ocean acidification ta dynamics in the northern gulf of mexico n gom is significantly affected by the mississippi river an empirical ta algorithm is offered here which accounts for the local effects of coastal processes in situ data collected during numerous research cruises in the n gom were compiled and used to develop ta algorithms using sea surface temperature sst and sea surface salinity sss as explanatory variables after improving the coefficients and functional form of this algorithm chlorophyll a chl a was included as an additional explanatory variable which worked as a proxy for addressing the pronounced effects of biological forcing on coastal waters finally a geographically weighted regression algorithm was developed in the form ta exp xo x1 sss 35 2 x2 sssxsst 1 2 x3chl a to address spatial non stationarity which produced improved estimates of ta in the n gom keywords total alkalinity northern gulf of mexico ocean acidification geographic weighted regression 1 introduction about 25 of the anthropologically generated atmospheric co2 is absorbed by the world oceans le quéré et al 2016 this has resulted in an increase in the concentration of dissolved inorganic carbon and decrease in average ocean ph also known as ocean acidification oa sun et al 2012 a gradual decrease in carbonate ion concentrations aragonite saturation states ωar and calcite saturation states ωca leads to reduced calcification rates and ultimately calcium carbonate dissolution cooley 2009 gledhill et al 2009 a key concerning aspect of ocean acidification reduced calcification rates negatively impact the calcifying organisms in the ocean and also have important implications for global carbon cycle raven et al 2005 thirty five years of observation concluded that the rate of alteration in seawater carbonate system parameters varies geographically with the greatest variability observed in the north pacific ocean region takahashi et al 2006 this variability likely results from local oceanographic processes which contribute additional carbon into the system by lateral mixing of waters from marginal seas input from coastal areas and rivers upwelling of subsurface waters and biological activities takahashi et al 2006 similarly there are a myriad of factors that affect coastal acidification in the northern gulf of mexico n gom primarily owing to heterotrophic respiration of terrestrial and autotrophically produced organic carbon cai 2003 hu et al 2017 mainly due to nutrient loading by the mississippi river as well as outflow from other rivers cai et al 2011 lohrenz and cai 2006 hence the variability in the carbonate system parameters in the n gom coastal waters is notably higher than the pelagic central gulf of mexico due to pronounced coastal biogeochemical cycling the partial pressure of co2 pco2 total alkalinity ta ph and dissolved inorganic carbon dic are the principal carbonate chemistry parameters used to document the status and trends of ocean acidification the carbonate system is bivariate and can be fully constrained if any two of the four measurable carbonate system parameters are known ta coupled with pco2 are frequently used e g noaa s ocean acidification product suite gledhill et al 2008 to estimate ph ωar and ωca using the co2sys program lewis and wallace 1998 employing the carbonate equilibrium equations e g mehrbach et al 1973 thus ta is one of the important parameters that aids in deriving other carbonate system parameters bever et al 2021 ta is one of the important parameters to show the intensity of seawater buffer against ocean acidification and is defined dickson 1981 as the number of moles of hydrogen ion equivalent to the excess of proton acceptors in 1 kg of sample μmol kg 1 due to the prevalence of carbonate rocks in the drainage basin of the mississippi river the ta values of the mississippi river water are usually high raymond and cole 2003 which has increased significantly in the past half century as a result of increased flow from higher rainfall and land use changes that have altered vegetation cover keul et al 2010 keul et al 2010 observed a non conservative behavior of ta in the mississippi river plume region which they attributed to changes in microbial recycling across salinity gradients additionally non conservative mixing with waters from local estuaries such as terrebonne bay and smaller inputs from other estuaries decrease the ta values in the coastal waters this decrease is considerably less due to the high alkalinity content of the mississippi river and other rivers draining to the gulf of mexico keul et al 2010 millero et al 1998 developed a set of algorithms for estimating salinity normalized ta in global oceans using historical ta measurements and ta measured during global carbon surveys during 1990s lee et al 2006 updated those algorithms with five polynomial algorithms for estimating ta in five major oceanographic regimes using sea surface salinity sss and sea surface temperature sst as explanatory variables these oceanographic regimes include the subtropics the equatorial upwelling pacific the north atlantic the north pacific and the southern ocean the n gom region is close to zone 1 i e the subtropics as described by lee et al 2006 however the calibration dataset of the ta algorithm for zone 1 lacked in situ data from the n gom region which likely explains the bias of the algorithm for the n gom region the mississippi river is the largest river influencing n gom with an average discharge of about 1 35 0 2 104 m3 s based on 64 years of united states geological survey discharge data hu et al 2005 at the mississippi river outfall and shelf area high net pco2 is evidenced due to heterotrophic production of co2 from land derived organic carbon cai 2003 it is further influenced by autotrophically produced organic carbon caused by high nutrient loading leading to co2 drawdown in shelf surface water ternon et al 2000 huang et al 2015 and heterotrophy of autotrophically produced carbon cai and lohrenz 2006 which acts as an additional pco2 source lohrenz and cai 2006 guo et al 2012 reported that the net autotrophic production of carbon at the mississippi river plume is even higher than the amazon system the nutrient rich mississippi river water coupled with outflow from other rivers influences carbon dynamics of the n gom coastal ecosystem lohrenz and cai 2006 causes bottom water hypoxia in the region rabalais et al 2002 and impacts the marine ecosystem affecting fisheries and coral reefs in the n gom hu et al 2003 salisbury et al 2001 walker 1996 inoue et al 2008 thus the n gom coastal region is biologically and chemically complex and is significantly different from the rest of the subtropical ocean in past research chlorophyll a chl a has been used as a proxy for primary production in estimating pco2 lohrenz and cai 2006 thus the incorporation of chl a as an explanatory variable has great potential to increase ta algorithm performance it is difficult to account for the high degree of spatial and temporal variability in carbonate system parameters in the coastal waters using an algorithm that was developed for the open ocean due to the biological and chemical complexities of the coastal waters as well as the variable freshwater influence spatial variability in the relationships between parameters is referred to as spatial nonstationarity brunsdon et al 1996 geographically weighted regression gwr is a statistical modeling technique that attempts to address the spatial nonstationarity brunsdon et al 2010 the spatial variation of complexities in ocean chemistry and acidification processes in coastal waters the gwr approach generates a set of coefficients for each point in space empirically using the provided functional form for the regression equations in the case of estimating carbonate system parameters the gwr approach has great potential in accounting for the spatial variability of the influence of biogeochemical processes on the explanatory variables the objective of this effort was to develop a revised empirical algorithm for estimating ta in the coastal waters of the n gom by proposing an improved functional form incorporating chl a as an additional explanatory variable and utilizing gwr to address spatial nonstationarity in the n gom domain 2 materials and methods in situ ta sst and sss data and satellite derived chl a were used for the development and testing of the algorithms in this study the algorithms were later applied to satellite derived sst sss and chl a imagery to illustrate the efficacy of the algorithms in showing the spatial distribution of ta using the derived algorithm a total of 247 observations of ta sst and sss each were assimilated from the in situ data collected during numerous research cruises from july 2007 to september 2013 in the n gom table 1 fig 1 the areal extent of these observations is about 474 219 sq km rendering an average of one observation per 41 km 41 km area in situ sss and sst values were used to estimate ta using each of the five algorithms given by lee et al 2006 and estimated ta values were compared with in situ measured ta values as the n gom is closer to the zone 1 it was expected that the ta estimation error would be lowest with this algorithm however the lowest error was found when the algorithm that was developed for zone 4 was used with zone 1 s algorithm performing second best table 2 while error was smallest with the zone 4 algorithm the zone 1 algorithm was selected due to its geographic proximity to the study region and its simpler functional form than the zone 4 algorithm the zone 4 algorithm incorporates longitude as an exploratory variable which would be redundant in the presented gwr algorithm the zone 1 algorithm coefficients were modified to investigate potential improvement in ta estimations in the n gom this was done by maintaining the functional form of the algorithm but updating the algorithm coefficients by cross validation next the functional form of the algorithm was modified to lower the standard errors of the coefficients in the algorithm by introducing chl a as an additional explanatory variable chl a data were derived from a time series of modis satellite imagery processed using the seadas software package v7 4 with the default atmospheric correction out of the 247 locations with ta observations modis derived chl a were available for only 78 locations fig 1 due to cloud cover and atmospheric correction failure at certain coastal locations this reduction in observations led to a study area reduction to 359 397 sq km with an average of one observation per 67 km 67 km area using the data from these 78 locations the ta algorithm was modified by adding a linear term for chl a along with the modified functional forms for sss and sst finally geographically weighted regression gwr was carried out with sss sst and chl a as explanatory variables and ta as the dependent variable while the 78 data points constituted a limited dataset the distribution of observations was still relatively uniform due to the paucity of data gwr was chosen to improve the algorithm since gwr allows coefficients to vary continuously over the study area and a set of coefficients are estimated for each location typically on a grid using a weighted approach so that the coefficient rasters account for the spatial heterogeneity the open source gwr4 semiparametric gwr gwgl modeling tool version 4 09 2016 developed by the national centre for geo computation ireland and ritsumeikan university japan was used to carry out gwr with an adaptive gaussian kernel agk with cross validation as the selection criteria brunsdon et al 1996 during the gwr algorithm development weights for each data point were computed based on the relative distance between the point under consideration and the rest of the observations using agk charlton and fotheringham 2009 the rate of decrease in the weights was calculated using the agk this kernel function unlike fixed kernel uses dynamic bandwidth distance where the bandwidth distance is automatically chosen with a smaller dynamic bandwidth distance for the regions with a higher number of observations and higher dynamic bandwidth distance for the regions with a lower number of observations the agk was chosen because of non uniform spatial distribution density of observations in the dataset all these algorithms were developed using all available in situ data and validated using a 10 fold cross validation approach for validation the dataset was randomly divided into ten subgroups an algorithm was developed using the data from the nine subgroups and validated using the 10th subgroup this was repeated nine times so that each subgroup couldbe used for validation and in the calculation of the residuals subsequently mean absolute error mae and root mean square error rmse were calculated for the estimated ta and the median rmses and maes were compared between the algorithms this approach allowed the use of all the data for algorithm development and validation finally ta images were generated by using the developed algorithms with sss from the soil moisture active passive smap sensor and sst and chl a from the aqua modis sensor 3 results a set of algorithms are given by lee et al 2006 for estimating global distribution of surface alkalinity using sss and sst as explanatory variables for the purpose of evaluating the existing algorithm performance in n gom the algorithm developed for zone 1 was tested to estimate ta using in situ sss and sst data the lee et al 2006 algorithm for zone 1 is given by 1 ta 2305 58 66 sss 35 2 32 sss 35 2 1 41 sst 20 0 04 sst 20 2 estimated ta values were compared with in situ measured ta for quantifying the algorithm accuracy within the n gom domain this algorithm produced a median rmse of 272 6 μmol kg 1 with a rmse of 11 4 and a mae of 183 9 μmol kg 1 with a mae of 7 7 tables 1 and 3 the distribution of the residuals as percent error calculated as in situ ta estimated ta in situ ta x 100 illustrates that the ta values were mostly underestimated by the lee et al 2006 zone 1 algorithm fig 2 a and b fig 3 a shows the residual percentages on a map revealing the highest ta estimate deviations range 10 80 were primarily located in coastal waters in the central gom the deviation of estimated values was less 2 from the in situ measured values hence the lee et al 2006 algorithm seems well suited for the central gom but performs poorly near the coast to address this bias the lee et al 2006 zone 1 algorithm was updated using the coastal in situ data to modify the algorithm coefficients the resulting modified algorithm was 2 ta 3299 394 7 449 sss 35 0 083 sss 35 2 250 062 sst 20 15 784 sst 20 2 a comparison of this algorithm with the zone 1 algorithm of lee et al 2006 revealed that the effect of sss on ta is different in the n gom region relative to the subtropical region as the magnitude of sss coefficients were smaller in eq 2 compared to eq 1 additionally the effect of sst on ta was more pronounced in the n gom region than the subtropics as evidenced by the larger sst coefficients in the updated algorithm the modification produced notable improvements as the median rmse was 94 5 μmol kg 1 with a range from 76 5 to 112 4 μmol kg 1 for this algorithm table 3 which is a 65 improvement over the zone 1 algorithm provided by lee et al 2006 moreover the residuals and comparison of in situ and estimated ta showed significant improvements fig 2c and d however the coefficient for the third term sss 35 2 had a very high standard error error for x2 91 table 3 suggesting high algorithm instability with this coefficient initially a regression without including the salinity parameter with only two terms of sst was carried out this resulted in an algorithm with high coefficient standard errors 30 for the first sst term and 33 for the second sst term and a lower adjusted r2 0 13 similarly a regression with sss alone also did not yield a good algorithm to address the issue of higher coefficient standard errors and lower adjusted r2 several functional forms were tested after including both sss and sst ultimately an exponential equation with modification in the parameters comprising one sss and two sst terms produced the best algorithm with lower rmse lower standard error and higher adjusted r2 which is given by 3 ta exp 8 053 0 000156 sss 35 2 0 0562 sst 20 2 0 000272 sst 20 3 modification in the functional form decreased the standard error for the coefficient of the second and third terms from 27 to 10 and 91 8 respectively table 3 however there was a decrease in r2 and an increase in the rmse table 3 as such chlorophyll a chl a was added as an additional explanatory variable to the algorithm the correlation between chl a and ta was found to be 0 74 which indicates it may be a confounding variable that requires addressing after adding chl a various modifications to the functional form were tested and it was found that an algorithm with sss and sst interaction term produced the most efficient algorithm when chl a was included in the algorithm among all the algorithms the one with the highest r2 0 61 lowest rmse 22 6 μmol kg 1 and lowest standard error for the coefficients table 3 was chosen which is given by 4 ta exp 7 727 0 000385 sss 35 2 0 00162 sss x sst 1 2 0 00212 chl a the inclusion of chl a as an explanatory variable improved the algorithm considerably however the adjusted r2 was still less than that of eq 2 table 3 next a gwr algorithm was developed using the above functional form that includes chl a as the additional explanatory variable the output from the gwr approach was the value of the coefficients at each point however the coefficients were noisy so the cubic spline surface interpolation was performed to develop smooth raster surfaces of the coefficients over the spatial domain the raster images for the four coefficients showing the range of values in the n gom are shown in fig 4 and given as an arc gis map package as supplementary material for example a resulting raster surface for the second coefficient x1 the sss 35 2 term ranged between 0 000350 and 0 000410 fig 4 the coefficients in coastal locations differed significantly from those in the central gom these differences suggest the spatial variation in the importance of the explanatory variables in ta estimations which likely arise from local effects of biogeochemical processes that affect carbonate chemistry an adjusted r2 of 0 69 was obtained for the gwr algorithm table 3 which is the highest adjusted r2 obtained in this study indicating that the algorithm best fits the data the residuals were also randomly distributed for this algorithm fig 2e notably most points were closer to the 1 1 line fig 2f for the gwr algorithm relative to that of other algorithms fig 2b and d suggesting the gwr algorithm is in good agreement with the in situ ta values additionally there is an absence of a spatial trend in the gwr residuals fig 3b when compared to the residuals of lee et al 2006 zone 1 algorithm fig 3a clearly the gwr approach significantly reduced spatial nonstationarity in the ta estimates deeming this approach more valid all the algorithms were applied to modis derived sst and chl a and smap derived sss images of february 10 2016 to obtain ta images by multiplying the coefficient rasters with the sst sss and chl a images as per the functional form of the algorithm provided in eq 4 the higher sss coefficient magnitudes in the lee et al 2006 zone 1 algorithm suggest ta estimated by that algorithm was highly influenced by sss the resulting ta image fig 5 d appears very similar to the sss image fig 5a and is strongly correlated at 0 97 with the improved gwr algorithm the resulting ta image displays the influence of sst sss and chl a on ta fig 4e specifically due to high chl a concentrations near the mississippi river outfall red color in fig 5c the ta values are estimated to be low blue in fig 5e thus the coastal region showed improved estimates differences between the gwr algorithm and the lee et al 2006 zone 1 algorithm eq 1 with chl a is displayed in fig 5f and the improvement offered by gwr is clearly demonstrated the lee et al 2006 algorithm underestimated ta in the coastal waters shown in deep blue in fig 5d and red in fig 5f and over estimated in the areas away from the coastal region green color in fig 5f compared to the image generated using the gwr algorithm 4 discussion when the lee et al 2006 algorithm which was developed for subtropical open ocean waters was applied to the n gom dataset the residuals showed a clear trend and the ta was mostly underestimated fig 2a and b the highest residuals were located in coastal waters fig 3a likely a result of the lack of in situ data from the n gom in the lee et al 2006 algorithm calibration errors were reduced by 65 when the algorithm coefficients were updated using in situ data from n gom but the algorithm coefficients had high standard errors after the update to alleviate this issue an exponential functional form was used which decreased the standard errors in the coefficients significantly table 3 ocean chemistry is significantly different in the n gom relative to the subtropical atlantic owing to the pronounced effects of coastal biogeochemistry heavily influenced by the mississippi river and outflow from other rivers which supply nutrients and alkalinity cai et al 2011 lohrenz and cai 2006 schneider et al 2007 was able to obtain a working algorithm for ta estimation in the mediterranean sea using sss alone however for the n gom region use of only sss or only sst or even sss and sst together resulted in high rmses possibly because of the influence of the mississippi river and outflow from other rivers thus chl a was introduced as an additional explanatory variable in the ta algorithm to serve as a proxy for primary production along with an interaction term between sst and sss more recently fine et al 2017 investigated global ta derived using lee et al 2006 algorithms and reported that the errors in estimated ta in north atlantic and pacific were due to higher observed sst and sss in large percentages than the ranges given for lee et al 2006 algorithms an underestimation of ta was observed in this study for the same reason in the n gom hence a regional parameterization of the algorithm using observed values in the n gom improved the algorithm considerably by reducing the median rmse from 272 6 to 22 6 μmol kg 1 however the standard error in the coefficients increased slightly as the sample sizes were significantly reduced 247 78 observations when chl a data were included while the introduction of chl a offered improvements in the ta estimates in the coastal region gwr was tested to minimize spatial nonstationarity the resulting gwr algorithm provided several interesting observations one clear observation was the difference in coefficient magnitudes between coastal areas and the central gom these differences resulted from the biological and chemical complexities of the coastal waters as compared to the central gom and was not revealed without using the gwr approach figs 4 and 5 in the development of the algorithms it was clear that the lee et al 2006 zone 1 algorithm relied heavily upon sss contrarily the improvements based on the lee et al 2006 algorithm offered here incorporated the effects of sss sst and chl a in its calculations of ta these results support the work by keul et al 2010 which showed that the ta salinity relationships from lee et al 2006 do not apply to shelf waters effectively adding the chl a variable produced more accurate ta estimates relative to lee et al 2006 in the n gom the adjustments offered by the gwr algorithm in the coastal regions of the study domain were also very apparent when comparing it side by side with the image generated by applying the lee et al 2006 zone 1 algorithm fig 4d and e overall the estimated ta obtained by the gwr algorithm corresponded well to the findings of keul et al 2010 and the expected ta in the n gom nevertheless a denser observation distribution would address local spatial effects more accurately while observation density was low the distribution of observations was relatively uniform suggesting the algorithm coefficients derived from the gwr approach are able to produce reasonable estimates of ta in the n gom mississippi river and other small rivers supply ta into the n gom and there is significant seasonal variability in ta and nutrient levels in the n gom the relationship of ta with sss sst and chl a is dependent on mixing thus the gwr approach and the use of three variables sss sst and chl a in the algorithm account for this variability to a great extent making this approach potentially applicable to other coastal systems around the world 5 conclusions the lee et al 2006 ta algorithm which was developed for the subtropical oceans did not perform efficiently in estimating ta in the n gom coastal regions likely owing to the biological and chemical complexities in the coastal waters caused by the outflow of the mississippi river and other smaller rivers an improved algorithm was formulated by using in situ data from the n gom region and by introducing satellite derived chl a in the algorithm prior to including chl a the coefficients and the functional form of the lee et al 2006 ta algorithm were modified with decreased standard errors the addition of an interaction term between sss and sst and the addition of chl a as an additional explanatory variable resulted in an increase in adjusted r2 with least rmse and mae finally the gwr approach offered the most reliable and valid algorithm with highest adjusted r2 and least error lee et al 2006 global ta algorithm was improved by introducing satellite derived chl a as an additional explanatory and by addressing spatial nonstationarity using gwr notable improvement in estimated ta was observed in the coastal n gom region with this approach as it considered both biological complexities and spatial heterogeneity an arc gis map package has been provided with four rasters with coefficients for the gwr algorithm as supplementary material to create ta images one would simply multiply these raster images to satellite derived sst and chl a images and either satellite or model e g hycom derived sss as per the provided functional form of the algorithm which can easily be performed using arc gis or any other image processing software or program although the algorithm was successfully improved future research utilizing a larger dataset from a longer period covering more locations is still essential to continue to improve ta estimates in n gom or other complex coastal ecosystems especially over the river influenced coastal margins declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements this research was partially supported by a noaa grant na11oar4320199 to dr padmanava dash we sincerely thank dr dwight gledhill deputy director noaa ocean acidification program and dr xinping hu associate professor department of physical and environmental sciences texas a m university corpus christi for reviewing the manuscript and suggesting substantial improvements in situ data used in this study was provided to the community by noaa aoml and noaa ncei through pangaea the satellite data products used in this study were provided to the community by nasa through the ocean biology processing group gsfc nasa and were processed using nasa s seawifs data analysis system seadas 
